/scratch/07428/ygqin/Aeolus/Fast_code/ML_dataset/ROM_NN_PF/train
Mon Feb 21 16:57:40 CST 2022
the mode is ini
device cuda
(input data) train, test 1800 100
dataset dir ../../twoD/*.h5  and size 1800
xmin 0.0 xmax 19.968 ymin 0.0 ymax 59.904003
nx,ny 391 1171
(1800, 24, 8)
maximum abs change 0.11508951149880886
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.004755982412981986
number of weird sim 0
[]
renaissance 0
renaissance points (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
min and max of training data 0.0 1.0
max diff from 1 4.470348358154297e-08
all the summation of grain fractions are 1 0.00036462442949414253
all the summation of grain fractions are 1 4.691802502065912e-13
mean and std of last y 32.63640903102027 12.170353870345611
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 1800
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 37800
total number of trained parameters  28945
Epoch:0, Train loss:0.434117, valid loss:0.411533
Epoch:1, Train loss:0.021249, valid loss:0.006072
Epoch:2, Train loss:0.005945, valid loss:0.003107
Epoch:3, Train loss:0.004232, valid loss:0.002959
Epoch:4, Train loss:0.003599, valid loss:0.002565
Epoch:5, Train loss:0.003167, valid loss:0.002487
Epoch:6, Train loss:0.002888, valid loss:0.002248
Epoch:7, Train loss:0.002726, valid loss:0.001880
Epoch:8, Train loss:0.002623, valid loss:0.001790
Epoch:9, Train loss:0.002332, valid loss:0.001989
Epoch:10, Train loss:0.002361, valid loss:0.001935
Epoch:11, Train loss:0.001704, valid loss:0.001368
Epoch:12, Train loss:0.001653, valid loss:0.001440
Epoch:13, Train loss:0.001595, valid loss:0.001429
Epoch:14, Train loss:0.001614, valid loss:0.001208
Epoch:15, Train loss:0.001502, valid loss:0.001261
Epoch:16, Train loss:0.001472, valid loss:0.001175
Epoch:17, Train loss:0.001455, valid loss:0.001174
Epoch:18, Train loss:0.001422, valid loss:0.001214
Epoch:19, Train loss:0.001404, valid loss:0.001188
Epoch:20, Train loss:0.001422, valid loss:0.001253
Epoch:21, Train loss:0.001082, valid loss:0.001021
Epoch:22, Train loss:0.001060, valid loss:0.000998
Epoch:23, Train loss:0.001067, valid loss:0.001005
Epoch:24, Train loss:0.001062, valid loss:0.000937
Epoch:25, Train loss:0.001025, valid loss:0.001066
Epoch:26, Train loss:0.001058, valid loss:0.001013
Epoch:27, Train loss:0.000995, valid loss:0.000953
Epoch:28, Train loss:0.000986, valid loss:0.001041
Epoch:29, Train loss:0.001007, valid loss:0.000954
Epoch:30, Train loss:0.001001, valid loss:0.000904
Epoch:31, Train loss:0.000845, valid loss:0.000792
Epoch:32, Train loss:0.000851, valid loss:0.000953
Epoch:33, Train loss:0.000846, valid loss:0.000792
Epoch:34, Train loss:0.000834, valid loss:0.000799
Epoch:35, Train loss:0.000827, valid loss:0.000836
Epoch:36, Train loss:0.000828, valid loss:0.000801
Epoch:37, Train loss:0.000815, valid loss:0.000820
Epoch:38, Train loss:0.000810, valid loss:0.000793
Epoch:39, Train loss:0.000805, valid loss:0.000783
Epoch:40, Train loss:0.000801, valid loss:0.000788
Epoch:41, Train loss:0.000743, valid loss:0.000737
Epoch:42, Train loss:0.000738, valid loss:0.000725
Epoch:43, Train loss:0.000741, valid loss:0.000717
Epoch:44, Train loss:0.000738, valid loss:0.000736
Epoch:45, Train loss:0.000730, valid loss:0.000782
Epoch:46, Train loss:0.000734, valid loss:0.000719
Epoch:47, Train loss:0.000723, valid loss:0.000723
Epoch:48, Train loss:0.000724, valid loss:0.000710
Epoch:49, Train loss:0.000728, valid loss:0.000731
Epoch:50, Train loss:0.000720, valid loss:0.000714
Epoch:51, Train loss:0.000691, valid loss:0.000695
Epoch:52, Train loss:0.000686, valid loss:0.000691
Epoch:53, Train loss:0.000689, valid loss:0.000704
Epoch:54, Train loss:0.000683, valid loss:0.000693
Epoch:55, Train loss:0.000683, valid loss:0.000702
Epoch:56, Train loss:0.000682, valid loss:0.000685
Epoch:57, Train loss:0.000684, valid loss:0.000715
Epoch:58, Train loss:0.000680, valid loss:0.000692
Epoch:59, Train loss:0.000680, valid loss:0.000698
Epoch:60, Train loss:0.000676, valid loss:0.000699
training time 2753.624086380005
total number of trained parameters for initialize model 28945
sample [0.12020461 0.17902814 0.15089513 0.13043478 0.10485934 0.05370844
 0.12787724 0.13299233 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02173398014535074
plot_id,batch_id 0 1 miss% 0.04414504969338061
plot_id,batch_id 0 2 miss% 0.04361180037645158
plot_id,batch_id 0 3 miss% 0.032355830621942326
plot_id,batch_id 0 4 miss% 0.04154107246279177
plot_id,batch_id 0 5 miss% 0.07115719064820394
plot_id,batch_id 0 6 miss% 0.03232193070875936
plot_id,batch_id 0 7 miss% 0.03593969582895465
plot_id,batch_id 0 8 miss% 0.034102254929683884
plot_id,batch_id 0 9 miss% 0.040743375110754446
plot_id,batch_id 0 10 miss% 0.05400685350406907
plot_id,batch_id 0 11 miss% 0.05694927907580056
plot_id,batch_id 0 12 miss% 0.0494725127232741
plot_id,batch_id 0 13 miss% 0.049944183718558674
plot_id,batch_id 0 14 miss% 0.04551863241266282
plot_id,batch_id 0 15 miss% 0.06465746377966189
plot_id,batch_id 0 16 miss% 0.03789235554338055
plot_id,batch_id 0 17 miss% 0.03878702482107865
plot_id,batch_id 0 18 miss% 0.03728195309591184
plot_id,batch_id 0 19 miss% 0.04892897279399831
plot_id,batch_id 0 20 miss% 0.04377407956677462
plot_id,batch_id 0 21 miss% 0.02317438828077361
plot_id,batch_id 0 22 miss% 0.022379463721717365
plot_id,batch_id 0 23 miss% 0.03339865208846321
plot_id,batch_id 0 24 miss% 0.021277444705982116
plot_id,batch_id 0 25 miss% 0.0389728635140005
plot_id,batch_id 0 26 miss% 0.04116981756042447
plot_id,batch_id 0 27 miss% 0.04906562057797471
plot_id,batch_id 0 28 miss% 0.044878933002299906
plot_id,batch_id 0 29 miss% 0.029246459321198383
plot_id,batch_id 0 30 miss% 0.04346259627476094
plot_id,batch_id 0 31 miss% 0.04829335442262932
plot_id,batch_id 0 32 miss% 0.03252976925318808
plot_id,batch_id 0 33 miss% 0.05316761904415159
plot_id,batch_id 0 34 miss% 0.04080343573649098
plot_id,batch_id 0 35 miss% 0.04933916014791101
plot_id,batch_id 0 36 miss% 0.06977660270373494
plot_id,batch_id 0 37 miss% 0.048157642119795285
plot_id,batch_id 0 38 miss% 0.04560074206744605
plot_id,batch_id 0 39 miss% 0.044599191574394875
plot_id,batch_id 0 40 miss% 0.04939588566445572
plot_id,batch_id 0 41 miss% 0.04346964707159813
plot_id,batch_id 0 42 miss% 0.03659050948700831
plot_id,batch_id 0 43 miss% 0.047241092149100404
plot_id,batch_id 0 44 miss% 0.03534027596071303
plot_id,batch_id 0 45 miss% 0.06403587035880555
plot_id,batch_id 0 46 miss% 0.02640278284277808
plot_id,batch_id 0 47 miss% 0.021070008807099447
plot_id,batch_id 0 48 miss% 0.033718986092835974
plot_id,batch_id 0 49 miss% 0.02026814555036639
plot_id,batch_id 0 50 miss% 0.031351467152239194
plot_id,batch_id 0 51 miss% 0.03606533237657496
plot_id,batch_id 0 52 miss% 0.03996889212681896
plot_id,batch_id 0 53 miss% 0.04281094037450402
plot_id,batch_id 0 54 miss% 0.041313643612360486
plot_id,batch_id 0 55 miss% 0.059100042934083025
plot_id,batch_id 0 56 miss% 0.05271516956309206
plot_id,batch_id 0 57 miss% 0.03413803013065251
plot_id,batch_id 0 58 miss% 0.0635877740839861
plot_id,batch_id 0 59 miss% 0.02742167068387986
plot_id,batch_id 0 60 miss% 0.06737569444661567
plot_id,batch_id 0 61 miss% 0.05616194012212078
plot_id,batch_id 0 62 miss% 0.049874804970677984
plot_id,batch_id 0 63 miss% 0.036511748888379714
plot_id,batch_id 0 64 miss% 0.022096359030095474
plot_id,batch_id 0 65 miss% 0.06867784197497409
plot_id,batch_id 0 66 miss% 0.04887471407220877
plot_id,batch_id 0 67 miss% 0.04917392081470606
plot_id,batch_id 0 68 miss% 0.0459077499750036
plot_id,batch_id 0 69 miss% 0.02159728405509104
plot_id,batch_id 0 70 miss% 0.027285930425941986
plot_id,batch_id 0 71 miss% 0.023945927691911225
plot_id,batch_id 0 72 miss% 0.03734670249681746
plot_id,batch_id 0 73 miss% 0.02043121175748624
plot_id,batch_id 0 74 miss% 0.02217147369706358
plot_id,batch_id 0 75 miss% 0.03295171465853838
plot_id,batch_id 0 76 miss% 0.022130485328782666
plot_id,batch_id 0 77 miss% 0.016746301782665254
plot_id,batch_id 0 78 miss% 0.0436573669430685
plot_id,batch_id 0 79 miss% 0.021938303514384413
plot_id,batch_id 0 80 miss% 0.05662833969583905
plot_id,batch_id 0 81 miss% 0.045572529302855665
plot_id,batch_id 0 82 miss% 0.04655852812592717
plot_id,batch_id 0 83 miss% 0.06212198758320414
plot_id,batch_id 0 84 miss% 0.06249795617101911
plot_id,batch_id 0 85 miss% 0.06891510196059936
plot_id,batch_id 0 86 miss% 0.06924081810301437
plot_id,batch_id 0 87 miss% 0.04902631712854141
plot_id,batch_id 0 88 miss% 0.03349857509708914
plot_id,batch_id 0 89 miss% 0.02146111902000428
plot_id,batch_id 0 90 miss% 0.042054233192442045
plot_id,batch_id 0 91 miss% 0.05461203773776752
plot_id,batch_id 0 92 miss% 0.048472143019950495
plot_id,batch_id 0 93 miss% 0.03315192944277977
plot_id,batch_id 0 94 miss% 0.03563675507696207
plot_id,batch_id 0 95 miss% 0.04613986312198382
plot_id,batch_id 0 96 miss% 0.02493215373815116
plot_id,batch_id 0 97 miss% 0.022473988563220703
plot_id,batch_id 0 98 miss% 0.019323639783989103
plot_id,batch_id 0 99 miss% 0.032698544490267115
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[1. 1. 1. ... 8. 8. 8.]
[0.1  0.2  0.3  ... 0.96 0.98 1.  ]
[0.02173398 0.04414505 0.0436118  0.03235583 0.04154107 0.07115719
 0.03232193 0.0359397  0.03410225 0.04074338 0.05400685 0.05694928
 0.04947251 0.04994418 0.04551863 0.06465746 0.03789236 0.03878702
 0.03728195 0.04892897 0.04377408 0.02317439 0.02237946 0.03339865
 0.02127744 0.03897286 0.04116982 0.04906562 0.04487893 0.02924646
 0.0434626  0.04829335 0.03252977 0.05316762 0.04080344 0.04933916
 0.0697766  0.04815764 0.04560074 0.04459919 0.04939589 0.04346965
 0.03659051 0.04724109 0.03534028 0.06403587 0.02640278 0.02107001
 0.03371899 0.02026815 0.03135147 0.03606533 0.03996889 0.04281094
 0.04131364 0.05910004 0.05271517 0.03413803 0.06358777 0.02742167
 0.06737569 0.05616194 0.0498748  0.03651175 0.02209636 0.06867784
 0.04887471 0.04917392 0.04590775 0.02159728 0.02728593 0.02394593
 0.0373467  0.02043121 0.02217147 0.03295171 0.02213049 0.0167463
 0.04365737 0.0219383  0.05662834 0.04557253 0.04655853 0.06212199
 0.06249796 0.0689151  0.06924082 0.04902632 0.03349858 0.02146112
 0.04205423 0.05461204 0.04847214 0.03315193 0.03563676 0.04613986
 0.02493215 0.02247399 0.01932364 0.03269854]
for model  0 the mean error 0.04120039451703874
