/scratch/07428/ygqin/Aeolus/Fast_code/ML_dataset/ROM_NN_PF/train
Mon Feb 21 18:26:38 CST 2022
the mode is test
device cuda
(input data) train, test 100 100
dataset dir ../../old_validation/*.h5  and size 100
xmin 0.0 xmax 19.968 ymin 0.0 ymax 59.904003
nx,ny 391 1171
(100, 24, 8)
maximum abs change 0.09207160770893097
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.003965792848709195
number of weird sim 0
[]
renaissance 0
renaissance points (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
min and max of training data 0.0 0.7314578294754028
max diff from 1 4.470348358154297e-08
all the summation of grain fractions are 1 1.9412953406572342e-05
all the summation of grain fractions are 1 2.90878432451791e-14
mean and std of last y 29.107048778533937 12.951323978090386
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 100
total frames 25 in_win 5 out_win 5
epoch 40 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 1600
total number of trained parameters  55697
total number of trained parameters for initialize model 28945
sample [0.16112532 0.1483376  0.04347826 0.08439898 0.12276215 0.13810742
 0.11253197 0.18925831 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.08099580562440695
plot_id,batch_id 0 1 miss% 0.06535714328761343
plot_id,batch_id 0 2 miss% 0.08596317138747361
plot_id,batch_id 0 3 miss% 0.07292893386582204
plot_id,batch_id 0 4 miss% 0.027044344022766505
plot_id,batch_id 0 5 miss% 0.052437003025060866
plot_id,batch_id 0 6 miss% 0.03181186976499042
plot_id,batch_id 0 7 miss% 0.0340461695979367
plot_id,batch_id 0 8 miss% 0.03869705456200649
plot_id,batch_id 0 9 miss% 0.02875041310112565
plot_id,batch_id 0 10 miss% 0.019889060704620754
plot_id,batch_id 0 11 miss% 0.024690740631006375
plot_id,batch_id 0 12 miss% 0.027556368010083848
plot_id,batch_id 0 13 miss% 0.04063703002250092
plot_id,batch_id 0 14 miss% 0.04111257461465888
plot_id,batch_id 0 15 miss% 0.027275749450678538
plot_id,batch_id 0 16 miss% 0.02397516495081318
plot_id,batch_id 0 17 miss% 0.025715812846818755
plot_id,batch_id 0 18 miss% 0.028685779150740623
plot_id,batch_id 0 19 miss% 0.03296800650607848
plot_id,batch_id 0 20 miss% 0.04042340421153505
plot_id,batch_id 0 21 miss% 0.03226805787250947
plot_id,batch_id 0 22 miss% 0.02630986308616903
plot_id,batch_id 0 23 miss% 0.027068948457757955
plot_id,batch_id 0 24 miss% 0.03128355380286867
plot_id,batch_id 0 25 miss% 0.07380617909209813
plot_id,batch_id 0 26 miss% 0.07626201330339046
plot_id,batch_id 0 27 miss% 0.07691123799798269
plot_id,batch_id 0 28 miss% 0.041339520803412765
plot_id,batch_id 0 29 miss% 0.06549095885540973
plot_id,batch_id 0 30 miss% 0.02157314461041254
plot_id,batch_id 0 31 miss% 0.024765139888707634
plot_id,batch_id 0 32 miss% 0.043218854545011266
plot_id,batch_id 0 33 miss% 0.029767355599707307
plot_id,batch_id 0 34 miss% 0.032530486417291365
plot_id,batch_id 0 35 miss% 0.029340822077488293
plot_id,batch_id 0 36 miss% 0.024262040131280253
plot_id,batch_id 0 37 miss% 0.044473142161267014
plot_id,batch_id 0 38 miss% 0.03730022128315674
plot_id,batch_id 0 39 miss% 0.041167237872814434
plot_id,batch_id 0 40 miss% 0.03308539100977355
plot_id,batch_id 0 41 miss% 0.022963402836286496
plot_id,batch_id 0 42 miss% 0.03305839615631451
plot_id,batch_id 0 43 miss% 0.04228122151370888
plot_id,batch_id 0 44 miss% 0.04861754207588237
plot_id,batch_id 0 45 miss% 0.03379393923684122
plot_id,batch_id 0 46 miss% 0.02760857050691474
plot_id,batch_id 0 47 miss% 0.035521386403456284
plot_id,batch_id 0 48 miss% 0.023281675709960634
plot_id,batch_id 0 49 miss% 0.03806650807174196
plot_id,batch_id 0 50 miss% 0.05024901515090427
plot_id,batch_id 0 51 miss% 0.06741319004603356
plot_id,batch_id 0 52 miss% 0.047516209155698985
plot_id,batch_id 0 53 miss% 0.04228425147313778
plot_id,batch_id 0 54 miss% 0.08242596405018972
plot_id,batch_id 0 55 miss% 0.0349831244615203
plot_id,batch_id 0 56 miss% 0.040962339216917644
plot_id,batch_id 0 57 miss% 0.03543735635399038
plot_id,batch_id 0 58 miss% 0.04389263321866927
plot_id,batch_id 0 59 miss% 0.03132622885435737
plot_id,batch_id 0 60 miss% 0.036534865412206435
plot_id,batch_id 0 61 miss% 0.0341289355629119
plot_id,batch_id 0 62 miss% 0.10271766053029294
plot_id,batch_id 0 63 miss% 0.05512715048744829
plot_id,batch_id 0 64 miss% 0.04114006997060848
plot_id,batch_id 0 65 miss% 0.016226700292927432
plot_id,batch_id 0 66 miss% 0.04239185758309986
plot_id,batch_id 0 67 miss% 0.022866507264951635
plot_id,batch_id 0 68 miss% 0.039877646989412194
plot_id,batch_id 0 69 miss% 0.03861276533794681
plot_id,batch_id 0 70 miss% 0.04021982742046919
plot_id,batch_id 0 71 miss% 0.030714723222390702
plot_id,batch_id 0 72 miss% 0.0378642469001626
plot_id,batch_id 0 73 miss% 0.03574436091584996
plot_id,batch_id 0 74 miss% 0.045103360470910514
plot_id,batch_id 0 75 miss% 0.14114555896539296
plot_id,batch_id 0 76 miss% 0.09016176281188061
plot_id,batch_id 0 77 miss% 0.07346731037883637
plot_id,batch_id 0 78 miss% 0.05049648366518615
plot_id,batch_id 0 79 miss% 0.09268726263075443
plot_id,batch_id 0 80 miss% 0.03233213091930162
plot_id,batch_id 0 81 miss% 0.04049531050181269
plot_id,batch_id 0 82 miss% 0.04200651888855714
plot_id,batch_id 0 83 miss% 0.10691012370316566
plot_id,batch_id 0 84 miss% 0.052973282218962026
plot_id,batch_id 0 85 miss% 0.02094532620352874
plot_id,batch_id 0 86 miss% 0.032038652638132395
plot_id,batch_id 0 87 miss% 0.0485041662786086
plot_id,batch_id 0 88 miss% 0.04437269681973919
plot_id,batch_id 0 89 miss% 0.033812960037535415
plot_id,batch_id 0 90 miss% 0.04656112434463514
plot_id,batch_id 0 91 miss% 0.040322980610936456
plot_id,batch_id 0 92 miss% 0.040110563446137476
plot_id,batch_id 0 93 miss% 0.05534472669858906
plot_id,batch_id 0 94 miss% 0.03424111873572873
plot_id,batch_id 0 95 miss% 0.05607490745722154
plot_id,batch_id 0 96 miss% 0.039095404105010265
plot_id,batch_id 0 97 miss% 0.04195516707619245
plot_id,batch_id 0 98 miss% 0.049051002242372646
plot_id,batch_id 0 99 miss% 0.047799513976163825
[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05
 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.15 0.15 0.15
 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15
 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.25 0.25 0.25 0.25 0.25 0.25
 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25
 0.25 0.25 0.25 0.25 0.25 0.3  0.3  0.3  0.3  0.3  0.3  0.3  0.3  0.3
 0.3  0.3  0.3  0.3  0.3  0.3  0.3  0.3  0.3  0.3  0.3  0.3  0.3  0.3
 0.3  0.3 ]
[ 1.  1.  1.  1.  1. 10. 10. 10. 10. 10. 20. 20. 20. 20. 20. 30. 30. 30.
 30. 30. 50. 50. 50. 50. 50.  1.  1.  1.  1.  1. 10. 10. 10. 10. 10. 20.
 20. 20. 20. 20. 30. 30. 30. 30. 30. 50. 50. 50. 50. 50.  1.  1.  1.  1.
  1. 10. 10. 10. 10. 10. 20. 20. 20. 20. 20. 30. 30. 30. 30. 30. 50. 50.
 50. 50. 50.  1.  1.  1.  1.  1. 10. 10. 10. 10. 10. 20. 20. 20. 20. 20.
 30. 30. 30. 30. 30. 50. 50. 50. 50. 50.]
[0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6
 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1
 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8
 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4
 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1.
 0.1 0.4 0.6 0.8 1.  0.1 0.4 0.6 0.8 1. ]
[0.08099581 0.06535714 0.08596317 0.07292893 0.02704434 0.052437
 0.03181187 0.03404617 0.03869705 0.02875041 0.01988906 0.02469074
 0.02755637 0.04063703 0.04111257 0.02727575 0.02397516 0.02571581
 0.02868578 0.03296801 0.0404234  0.03226806 0.02630986 0.02706895
 0.03128355 0.07380618 0.07626201 0.07691124 0.04133952 0.06549096
 0.02157314 0.02476514 0.04321885 0.02976736 0.03253049 0.02934082
 0.02426204 0.04447314 0.03730022 0.04116724 0.03308539 0.0229634
 0.0330584  0.04228122 0.04861754 0.03379394 0.02760857 0.03552139
 0.02328168 0.03806651 0.05024902 0.06741319 0.04751621 0.04228425
 0.08242596 0.03498312 0.04096234 0.03543736 0.04389263 0.03132623
 0.03653487 0.03412894 0.10271766 0.05512715 0.04114007 0.0162267
 0.04239186 0.02286651 0.03987765 0.03861277 0.04021983 0.03071472
 0.03786425 0.03574436 0.04510336 0.14114556 0.09016176 0.07346731
 0.05049648 0.09268726 0.03233213 0.04049531 0.04200652 0.10691012
 0.05297328 0.02094533 0.03203865 0.04850417 0.0443727  0.03381296
 0.04656112 0.04032298 0.04011056 0.05534473 0.03424112 0.05607491
 0.0390954  0.04195517 0.049051   0.04779951]
for model  0 the mean error 0.04417045456415745
