/scratch/07428/ygqin/ROM_grain/ROM_NN_PF/train
Sat Mar 19 23:44:27 CDT 2022
Launcher: Setup complete.

------------- SUMMARY ---------------
   Number of hosts:    2
   Working directory:  /scratch/07428/ygqin/ROM_grain/ROM_NN_PF/train
   Processes per host: 128
   Total processes:    256
   Total jobs:         243
   Scheduling method:  dynamic

-------------------------------------
Launcher: Starting parallel tasks...
Launcher: Task 4 running job 1 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 1)
Launcher: Task 7 running job 2 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 2)
Launcher: Task 16 running job 3 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 3)
Launcher: Task 63 running job 4 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 4)
Launcher: Task 75 running job 5 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 5)
Launcher: Task 73 running job 6 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 6)
Launcher: Task 125 running job 8 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 8)
Launcher: Task 126 running job 7 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 7)
Launcher: Task 68 running job 9 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 9)
Launcher: Task 117 running job 12 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 12)
Launcher: Task 107 running job 10 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 10)
Launcher: Task 123 running job 11 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 11)
Launcher: Task 172 running job 13 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 13)
Launcher: Task 149 running job 14 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 14)
Launcher: Task 213 running job 16 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 16)
Launcher: Task 193 running job 15 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 15)
Launcher: Task 243 running job 17 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 17)
Launcher: Task 0 running job 19 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 19)
Launcher: Task 13 running job 18 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 18)
Launcher: Task 251 running job 20 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 20)
Launcher: Task 77 running job 21 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 21)
Launcher: Task 64 running job 22 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 22)
Launcher: Task 78 running job 23 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 23)
Launcher: Task 57 running job 24 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 24)
Launcher: Task 90 running job 26 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 26)
Launcher: Task 96 running job 25 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 25)
Launcher: Task 69 running job 27 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 27)
Launcher: Task 76 running job 29 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 29)
Launcher: Task 94 running job 28 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 28)
Launcher: Task 61 running job 30 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 30)
Launcher: Task 161 running job 31 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 31)
Launcher: Task 176 running job 32 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 32)
Launcher: Task 175 running job 34 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 34)
Launcher: Task 144 running job 33 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 33)
Launcher: Task 201 running job 35 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 35)
Launcher: Task 240 running job 36 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 36)
Launcher: Task 255 running job 37 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 37)
Launcher: Task 194 running job 38 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 38)
Launcher: Task 189 running job 39 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 39)
Launcher: Task 199 running job 42 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 42)
Launcher: Task 224 running job 41 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 41)
Launcher: Task 234 running job 43 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 43)
Launcher: Task 236 running job 44 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 44)
Launcher: Task 158 running job 40 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 40)
Launcher: Task 6 running job 47 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 47)
Launcher: Task 28 running job 45 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 45)
Launcher: Task 10 running job 48 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 48)
Launcher: Task 14 running job 46 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 46)
Launcher: Task 131 running job 49 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 49)
Launcher: Task 71 running job 50 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 50)
Launcher: Task 88 running job 53 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 53)
Launcher: Task 112 running job 51 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 51)
Launcher: Task 67 running job 52 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 52)
Launcher: Task 42 running job 54 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 54)
Launcher: Task 111 running job 56 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 56)
Launcher: Task 100 running job 55 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 55)
Launcher: Task 242 running job 57 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 57)
Launcher: Task 226 running job 58 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 58)
Launcher: Task 200 running job 59 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 59)
Launcher: Task 179 running job 60 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 60)
Launcher: Task 196 running job 61 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 61)
Launcher: Task 138 running job 64 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 64)
Launcher: Task 135 running job 63 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 63)
Launcher: Task 132 running job 66 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 66)
Launcher: Task 141 running job 62 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 62)
Launcher: Task 204 running job 65 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 65)
Launcher: Task 143 running job 68 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 68)
Launcher: Task 180 running job 67 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 67)
Launcher: Task 142 running job 69 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 69)
Launcher: Task 248 running job 70 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 70)
Launcher: Task 174 running job 71 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 71)
Launcher: Task 21 running job 72 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 72)
Launcher: Task 32 running job 74 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 74)
Launcher: Task 11 running job 73 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 73)
Launcher: Task 8 running job 75 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 75)
Launcher: Task 23 running job 76 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 76)
Launcher: Task 29 running job 77 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 77)
Launcher: Task 15 running job 78 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 78)
Launcher: Task 207 running job 79 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 79)
Launcher: Task 52 running job 80 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 80)
Launcher: Task 116 running job 81 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 81)
Launcher: Task 50 running job 82 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 82)
Launcher: Task 74 running job 85 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 85)
Launcher: Task 91 running job 83 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 83)
Launcher: Task 105 running job 84 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 84)
Launcher: Task 121 running job 86 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 86)
Launcher: Task 43 running job 88 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 88)
Launcher: Task 53 running job 87 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 87)
Launcher: Task 118 running job 89 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 89)
Launcher: Task 59 running job 92 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 92)
Launcher: Task 65 running job 91 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 91)
Launcher: Task 99 running job 90 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 90)
Launcher: Task 122 running job 94 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 94)
Launcher: Task 79 running job 93 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 93)
Launcher: Task 160 running job 95 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 95)
Launcher: Task 216 running job 96 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 96)
Launcher: Task 159 running job 98 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 98)
Launcher: Task 247 running job 97 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 97)
Launcher: Task 232 running job 101 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 101)
Launcher: Task 244 running job 99 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 99)
Launcher: Task 222 running job 100 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 100)
Launcher: Task 250 running job 102 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 102)
Launcher: Task 154 running job 103 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 103)
Launcher: Task 164 running job 105 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 105)
Launcher: Task 231 running job 104 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 104)
Launcher: Task 139 running job 106 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 106)
Launcher: Task 205 running job 107 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 107)
Launcher: Task 233 running job 108 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 108)
Launcher: Task 188 running job 111 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 111)
Launcher: Task 217 running job 110 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 110)
Launcher: Task 239 running job 109 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 109)
Launcher: Task 235 running job 113 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 113)
Launcher: Task 168 running job 112 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 112)
Launcher: Task 166 running job 115 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 115)
Launcher: Task 215 running job 116 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 116)
Launcher: Task 178 running job 114 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 114)
Launcher: Task 171 running job 118 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 118)
Launcher: Task 192 running job 117 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 117)
Launcher: Task 33 running job 119 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 119)
Launcher: Task 3 running job 120 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 120)
Launcher: Task 60 running job 121 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 121)
Launcher: Task 46 running job 122 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 122)
Launcher: Task 92 running job 123 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 123)
Launcher: Task 89 running job 124 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 124)
Launcher: Task 86 running job 125 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 125)
Launcher: Task 95 running job 126 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 126)
Launcher: Task 119 running job 128 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 128)
Launcher: Task 80 running job 127 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 127)
Launcher: Task 62 running job 129 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 129)
Launcher: Task 127 running job 130 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 130)
Launcher: Task 169 running job 131 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 131)
Launcher: Task 137 running job 132 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 132)
Launcher: Task 214 running job 133 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 133)
Launcher: Task 228 running job 134 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 134)
Launcher: Task 163 running job 135 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 135)
Launcher: Task 253 running job 137 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 137)
Launcher: Task 187 running job 136 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 136)
Launcher: Task 223 running job 138 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 138)
Launcher: Task 218 running job 139 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 139)
Launcher: Task 245 running job 140 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 140)
Launcher: Task 129 running job 141 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 141)
Launcher: Task 26 running job 142 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 142)
Launcher: Task 130 running job 147 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 147)
Launcher: Task 37 running job 143 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 143)
Launcher: Task 31 running job 144 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 144)
Launcher: Task 25 running job 145 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 145)
Launcher: Task 5 running job 146 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 146)
Launcher: Task 70 running job 148 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 148)
Launcher: Task 87 running job 149 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 149)
Launcher: Task 97 running job 150 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 150)
Launcher: Task 109 running job 151 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 151)
Launcher: Task 114 running job 152 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 152)
Launcher: Task 110 running job 153 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 153)
Launcher: Task 40 running job 154 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 154)
Launcher: Task 101 running job 155 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 155)
Launcher: Task 82 running job 158 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 158)
Launcher: Task 45 running job 157 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 157)
Launcher: Task 66 running job 156 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 156)
Launcher: Task 181 running job 161 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 161)
Launcher: Task 184 running job 160 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 160)
Launcher: Task 152 running job 159 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 159)
Launcher: Task 146 running job 166 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 166)
Launcher: Task 249 running job 164 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 164)
Launcher: Task 202 running job 165 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 165)
Launcher: Task 153 running job 163 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 163)
Launcher: Task 241 running job 168 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 168)
Launcher: Task 237 running job 169 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 169)
Launcher: Task 210 running job 170 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 170)
Launcher: Task 246 running job 171 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 171)
Launcher: Task 155 running job 167 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 167)
Launcher: Task 17 running job 162 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 162)
Launcher: Task 19 running job 173 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 173)
Launcher: Task 22 running job 172 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 172)
Launcher: Task 30 running job 174 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 174)
Launcher: Task 72 running job 176 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 176)
Launcher: Task 83 running job 175 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 175)
Launcher: Task 104 running job 177 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 177)
Launcher: Task 106 running job 178 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 178)
Launcher: Task 93 running job 181 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 181)
Launcher: Task 103 running job 179 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 179)
Launcher: Task 113 running job 182 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 182)
Launcher: Task 55 running job 180 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 180)
Launcher: Task 190 running job 183 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 183)
Launcher: Task 206 running job 184 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 184)
Launcher: Task 238 running job 185 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 185)
Launcher: Task 209 running job 186 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 186)
Launcher: Task 203 running job 187 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 187)
Launcher: Task 140 running job 189 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 189)
Launcher: Task 212 running job 190 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 190)
Launcher: Task 167 running job 188 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 188)
Launcher: Task 182 running job 191 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 191)
Launcher: Task 165 running job 192 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 192)
Launcher: Task 197 running job 193 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 193)
Launcher: Task 254 running job 195 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 195)
Launcher: Task 157 running job 194 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 194)
Launcher: Task 186 running job 196 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 196)
Launcher: Task 148 running job 197 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 197)
Launcher: Task 211 running job 198 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 198)
Launcher: Task 9 running job 199 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 199)
Launcher: Task 18 running job 201 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 201)
Launcher: Task 12 running job 200 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 200)
Launcher: Task 1 running job 202 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 202)
Launcher: Task 35 running job 203 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 203)
Launcher: Task 49 running job 204 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 204)
Launcher: Task 39 running job 205 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 205)
Launcher: Task 47 running job 208 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 208)
Launcher: Task 51 running job 206 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 206)
Launcher: Task 44 running job 207 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 207)
Launcher: Task 85 running job 209 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 209)
Launcher: Task 124 running job 210 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 210)
Launcher: Task 115 running job 211 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 211)
Launcher: Task 120 running job 213 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 213)
Launcher: Task 98 running job 212 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 212)
Launcher: Task 219 running job 214 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 214)
Launcher: Task 198 running job 215 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 215)
Launcher: Task 183 running job 216 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 216)
Launcher: Task 185 running job 217 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 217)
Launcher: Task 221 running job 218 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 218)
Launcher: Task 195 running job 222 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 222)
Launcher: Task 128 running job 223 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 223)
Launcher: Task 225 running job 221 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 221)
Launcher: Task 136 running job 219 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 219)
Launcher: Task 147 running job 220 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 220)
Launcher: Task 220 running job 224 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 224)
Launcher: Task 134 running job 225 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 225)
Launcher: Task 191 running job 226 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 226)
Launcher: Task 150 running job 227 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 227)
Launcher: Task 24 running job 228 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 228)
Launcher: Task 27 running job 229 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 229)
Launcher: Task 2 running job 230 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 230)
Launcher: Task 38 running job 231 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 231)
Launcher: Task 34 running job 232 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 232)
Launcher: Task 20 running job 233 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 233)
Launcher: Task 81 running job 234 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 234)
Launcher: Task 48 running job 235 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 235)
Launcher: Task 54 running job 238 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 238)
Launcher: Task 41 running job 237 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 237)
Launcher: Task 208 running job 236 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py train 236)
Launcher: Task 56 running job 239 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 239)
Launcher: Task 58 running job 240 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 240)
Launcher: Task 84 running job 241 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 241)
Launcher: Task 102 running job 243 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 243)
Launcher: Task 36 running job 242 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py train 242)
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  41745
Epoch:0, Train loss:0.812563, valid loss:0.804348
Epoch:1, Train loss:0.061228, valid loss:0.007892
Epoch:2, Train loss:0.014253, valid loss:0.004320
Epoch:3, Train loss:0.008751, valid loss:0.003183
Epoch:4, Train loss:0.006800, valid loss:0.002931
Epoch:5, Train loss:0.006001, valid loss:0.002519
Epoch:6, Train loss:0.005400, valid loss:0.002428
Epoch:7, Train loss:0.005058, valid loss:0.002329
Epoch:8, Train loss:0.004727, valid loss:0.002365
Epoch:9, Train loss:0.004449, valid loss:0.002514
Epoch:10, Train loss:0.004293, valid loss:0.001993
Epoch:11, Train loss:0.003573, valid loss:0.001708
Epoch:12, Train loss:0.003438, valid loss:0.001646
Epoch:13, Train loss:0.003362, valid loss:0.001717
Epoch:14, Train loss:0.003239, valid loss:0.001600
Epoch:15, Train loss:0.003094, valid loss:0.001509
Epoch:16, Train loss:0.002985, valid loss:0.001526
Epoch:17, Train loss:0.002843, valid loss:0.001575
Epoch:18, Train loss:0.002755, valid loss:0.001554
Epoch:19, Train loss:0.002677, valid loss:0.001499
Epoch:20, Train loss:0.002611, valid loss:0.001523
Epoch:21, Train loss:0.002256, valid loss:0.001232
Epoch:22, Train loss:0.002193, valid loss:0.001253
Epoch:23, Train loss:0.002159, valid loss:0.001308
Epoch:24, Train loss:0.002116, valid loss:0.001257
Epoch:25, Train loss:0.002097, valid loss:0.001215
Epoch:26, Train loss:0.002020, valid loss:0.001172
Epoch:27, Train loss:0.002017, valid loss:0.001156
Epoch:28, Train loss:0.001983, valid loss:0.001175
Epoch:29, Train loss:0.001948, valid loss:0.001277
Epoch:30, Train loss:0.001923, valid loss:0.001207
Epoch:31, Train loss:0.001763, valid loss:0.001092
Epoch:32, Train loss:0.001740, valid loss:0.001097
Epoch:33, Train loss:0.001726, valid loss:0.001032
Epoch:34, Train loss:0.001720, valid loss:0.001066
Epoch:35, Train loss:0.001687, valid loss:0.000994
Epoch:36, Train loss:0.001688, valid loss:0.001073
Epoch:37, Train loss:0.001659, valid loss:0.001015
Epoch:38, Train loss:0.001656, valid loss:0.001006
Epoch:39, Train loss:0.001631, valid loss:0.000974
Epoch:40, Train loss:0.001628, valid loss:0.000957
Epoch:41, Train loss:0.001533, valid loss:0.000965
Epoch:42, Train loss:0.001521, valid loss:0.000964
Epoch:43, Train loss:0.001520, valid loss:0.000977
Epoch:44, Train loss:0.001510, valid loss:0.000958
Epoch:45, Train loss:0.001512, valid loss:0.000956
Epoch:46, Train loss:0.001500, valid loss:0.000971
Epoch:47, Train loss:0.001489, valid loss:0.000958
Epoch:48, Train loss:0.001488, valid loss:0.000966
Epoch:49, Train loss:0.001475, valid loss:0.000981
Epoch:50, Train loss:0.001468, valid loss:0.000943
Epoch:51, Train loss:0.001410, valid loss:0.000923
Epoch:52, Train loss:0.001406, valid loss:0.000936
Epoch:53, Train loss:0.001401, valid loss:0.000941
Epoch:54, Train loss:0.001400, valid loss:0.000927
Epoch:55, Train loss:0.001399, valid loss:0.000916
Epoch:56, Train loss:0.001400, valid loss:0.000921
Epoch:57, Train loss:0.001398, valid loss:0.000912
Epoch:58, Train loss:0.001399, valid loss:0.000912
Epoch:59, Train loss:0.001398, valid loss:0.000915
Epoch:60, Train loss:0.001396, valid loss:0.000919
training time 6578.851003646851
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3384052525977164
plot_id,batch_id 0 1 miss% 0.39005110140253774
plot_id,batch_id 0 2 miss% 0.397998648863183
plot_id,batch_id 0 3 miss% 0.37872876917634685
plot_id,batch_id 0 4 miss% 0.36443073072860144
plot_id,batch_id 0 5 miss% 0.27022409122505997
plot_id,batch_id 0 6 miss% 0.38405726088809333
plot_id,batch_id 0 7 miss% 0.48441242917211547
plot_id,batch_id 0 8 miss% 0.6474247139287251
plot_id,batch_id 0 9 miss% 0.4852703339560371
plot_id,batch_id 0 10 miss% 0.28555259717361553
plot_id,batch_id 0 11 miss% 0.3525042256064742
plot_id,batch_id 0 12 miss% 0.3830119914458559
plot_id,batch_id 0 13 miss% 0.34477001621242453
plot_id,batch_id 0 14 miss% 0.475560418915271
plot_id,batch_id 0 15 miss% 0.3750150177902296
plot_id,batch_id 0 16 miss% 0.47291646443542523
plot_id,batch_id 0 17 miss% 0.37806718074941237
plot_id,batch_id 0 18 miss% 0.4322599419685642
plot_id,batch_id 0 19 miss% 0.4204711228657469
plot_id,batch_id 0 20 miss% 0.33265715274592084
plot_id,batch_id 0 21 miss% 0.5209930619418125
plot_id,batch_id 0 22 miss% 0.5086430395482219
plot_id,batch_id 0 23 miss% 0.3980511169092841
plot_id,batch_id 0 24 miss% 0.36097462372207656
plot_id,batch_id 0 25 miss% 0.26563210979618945
plot_id,batch_id 0 26 miss% 0.41476440608037135
plot_id,batch_id 0 27 miss% 0.3815587285255653
plot_id,batch_id 0 28 miss% 0.34344779303041345
plot_id,batch_id 0 29 miss% 0.4466386932114808
plot_id,batch_id 0 30 miss% 0.30257306771948217
plot_id,batch_id 0 31 miss% 0.5215532291513323
plot_id,batch_id 0 32 miss% 0.46277333936562276
plot_id,batch_id 0 33 miss% 0.4391373388440214
plot_id,batch_id 0 34 miss% 0.4742972087270402
plot_id,batch_id 0 35 miss% 0.36637014645631916
plot_id,batch_id 0 36 miss% 0.5356575647097994
plot_id,batch_id 0 37 miss% 0.48796674092815195
plot_id,batch_id 0 38 miss% 0.44628715803374686
plot_id,batch_id 0 39 miss% 0.5022857769130981
plot_id,batch_id 0 40 miss% 0.3970764580313644
plot_id,batch_id 0 41 miss% 0.37619836843829224
plot_id,batch_id 0 42 miss% 0.3632915542730018
plot_id,batch_id 0 43 miss% 0.39496091474006645
plot_id,batch_id 0 44 miss% 0.26949594390162807
plot_id,batch_id 0 45 miss% 0.31986929665090497
plot_id,batch_id 0 46 miss% 0.3261309949490531
plot_id,batch_id 0 47 miss% 0.473061988004162
plot_id,batch_id 0 48 miss% 0.3554254915180481
plot_id,batch_id 0 49 miss% 0.30152114164464877
plot_id,batch_id 0 50 miss% 0.4929774565492095
plot_id,batch_id 0 51 miss% 0.4762207272907568
plot_id,batch_id 0 52 miss% 0.4665610456893615
plot_id,batch_id 0 53 miss% 0.40260990507311567
plot_id,batch_id 0 54 miss% 0.42517958552171115
plot_id,batch_id 0 55 miss% 0.4356386186673912
plot_id,batch_id 0 56 miss% 0.5072438882244571
plot_id,batch_id 0 57 miss% 0.4768255667565028
plot_id,batch_id 0 58 miss% 0.48224511760504185
plot_id,batch_id 0 59 miss% 0.4272560184279846
plot_id,batch_id 0 60 miss% 0.2501224703643657
plot_id,batch_id 0 61 miss% 0.24666635640151563
plot_id,batch_id 0 62 miss% 0.34215629316296325
plot_id,batch_id 0 63 miss% 0.34988231541240095
plot_id,batch_id 0 64 miss% 0.3345676077270535
plot_id,batch_id 0 65 miss% 0.3844509009964767
plot_id,batch_id 0 66 miss% 0.3580503550335211
plot_id,batch_id 0 67 miss% 0.3018604740552289
plot_id,batch_id 0 68 miss% 0.3874461534520263
plot_id,batch_id 0 69 miss% 0.37369921696129504
plot_id,batch_id 0 70 miss% 0.21464992883329093
plot_id,batch_id 0 71 miss% 0.3122295032158899
plot_id,batch_id 0 72 miss% 0.374704461027555
plot_id,batch_id 0 73 miss% 0.3400692008447901
plot_id,batch_id 0 74 miss% 0.43724012929331824
plot_id,batch_id 0 75 miss% 0.24497328209589472
plot_id,batch_id 0 76 miss% 0.2956159647534085
plot_id,batch_id 0 77 miss% 0.2754275334334158
plot_id,batch_id 0 78 miss% 0.2793396565909382
plot_id,batch_id 0 79 miss% 0.40156335079705485
plot_id,batch_id 0 80 miss% 0.31672213430642476
plot_id,batch_id 0 81 miss% 0.4047375619779692
plot_id,batch_id 0 82 miss% 0.22194884693731307
plot_id,batch_id 0 83 miss% 0.4260853126043078
plot_id,batch_id 0 84 miss% 0.41594124223638834
plot_id,batch_id 0 85 miss% 0.23659925759783182
plot_id,batch_id 0 86 miss% 0.2957503926628316
plot_id,batch_id 0 87 miss% 0.3341613504044573
plot_id,batch_id 0 88 miss% 0.3911131082929338
plot_id,batch_id 0 89 miss% 0.37462348567282605
plot_id,batch_id 0 90 miss% 0.2160844876016429
plot_id,batch_id 0 91 miss% 0.36945210051028793
plot_id,batch_id 0 92 miss% 0.2975155281111895
plot_id,batch_id 0 93 miss% 0.2858283717415301
plot_id,batch_id 0 94 miss% 0.32980457347643216
plot_id,batch_id 0 95 miss% 0.27232520478817784
plot_id,batch_id 0 96 miss% 0.25103582059449026
plot_id,batch_id 0 97 miss% 0.42179481042134753
plot_id,batch_id 0 98 miss% 0.3458732178790284
plot_id,batch_id 0 99 miss% 0.3437518901738576
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.33840525 0.3900511  0.39799865 0.37872877 0.36443073 0.27022409
 0.38405726 0.48441243 0.64742471 0.48527033 0.2855526  0.35250423
 0.38301199 0.34477002 0.47556042 0.37501502 0.47291646 0.37806718
 0.43225994 0.42047112 0.33265715 0.52099306 0.50864304 0.39805112
 0.36097462 0.26563211 0.41476441 0.38155873 0.34344779 0.44663869
 0.30257307 0.52155323 0.46277334 0.43913734 0.47429721 0.36637015
 0.53565756 0.48796674 0.44628716 0.50228578 0.39707646 0.37619837
 0.36329155 0.39496091 0.26949594 0.3198693  0.32613099 0.47306199
 0.35542549 0.30152114 0.49297746 0.47622073 0.46656105 0.40260991
 0.42517959 0.43563862 0.50724389 0.47682557 0.48224512 0.42725602
 0.25012247 0.24666636 0.34215629 0.34988232 0.33456761 0.3844509
 0.35805036 0.30186047 0.38744615 0.37369922 0.21464993 0.3122295
 0.37470446 0.3400692  0.43724013 0.24497328 0.29561596 0.27542753
 0.27933966 0.40156335 0.31672213 0.40473756 0.22194885 0.42608531
 0.41594124 0.23659926 0.29575039 0.33416135 0.39111311 0.37462349
 0.21608449 0.3694521  0.29751553 0.28582837 0.32980457 0.2723252
 0.25103582 0.42179481 0.34587322 0.34375189]
for model  0 the mean error 0.37799048539861757
all id 0 hidden_dim 16 learning_rate 0.0025 num_layers 3 frames 21 out win 4 err 0.37799048539861757 time 6578.851003646851
Launcher: Job 1 completed in 6858 seconds.
Launcher: Task 4 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  41745
Epoch:0, Train loss:0.781020, valid loss:0.763342
Epoch:1, Train loss:0.066171, valid loss:0.010007
Epoch:2, Train loss:0.015889, valid loss:0.005434
Epoch:3, Train loss:0.012016, valid loss:0.003596
Epoch:4, Train loss:0.009148, valid loss:0.003444
Epoch:5, Train loss:0.007422, valid loss:0.003351
Epoch:6, Train loss:0.006115, valid loss:0.002664
Epoch:7, Train loss:0.005541, valid loss:0.002464
Epoch:8, Train loss:0.005191, valid loss:0.002383
Epoch:9, Train loss:0.004824, valid loss:0.002326
Epoch:10, Train loss:0.004653, valid loss:0.002064
Epoch:11, Train loss:0.003901, valid loss:0.001833
Epoch:12, Train loss:0.003817, valid loss:0.001961
Epoch:13, Train loss:0.003698, valid loss:0.001829
Epoch:14, Train loss:0.003566, valid loss:0.001775
Epoch:15, Train loss:0.003521, valid loss:0.001674
Epoch:16, Train loss:0.003349, valid loss:0.001907
Epoch:17, Train loss:0.003231, valid loss:0.001946
Epoch:18, Train loss:0.003137, valid loss:0.001748
Epoch:19, Train loss:0.003064, valid loss:0.001500
Epoch:20, Train loss:0.002935, valid loss:0.001542
Epoch:21, Train loss:0.002612, valid loss:0.001417
Epoch:22, Train loss:0.002534, valid loss:0.001388
Epoch:23, Train loss:0.002486, valid loss:0.001339
Epoch:24, Train loss:0.002465, valid loss:0.001399
Epoch:25, Train loss:0.002406, valid loss:0.001386
Epoch:26, Train loss:0.002391, valid loss:0.001329
Epoch:27, Train loss:0.002331, valid loss:0.001323
Epoch:28, Train loss:0.002284, valid loss:0.001316
Epoch:29, Train loss:0.002300, valid loss:0.001464
Epoch:30, Train loss:0.002214, valid loss:0.001257
Epoch:31, Train loss:0.002045, valid loss:0.001247
Epoch:32, Train loss:0.002029, valid loss:0.001216
Epoch:33, Train loss:0.002012, valid loss:0.001210
Epoch:34, Train loss:0.001999, valid loss:0.001240
Epoch:35, Train loss:0.001973, valid loss:0.001261
Epoch:36, Train loss:0.001970, valid loss:0.001199
Epoch:37, Train loss:0.001939, valid loss:0.001171
Epoch:38, Train loss:0.001928, valid loss:0.001238
Epoch:39, Train loss:0.001908, valid loss:0.001283
Epoch:40, Train loss:0.001890, valid loss:0.001220
Epoch:41, Train loss:0.001813, valid loss:0.001124
Epoch:42, Train loss:0.001794, valid loss:0.001180
Epoch:43, Train loss:0.001792, valid loss:0.001127
Epoch:44, Train loss:0.001783, valid loss:0.001133
Epoch:45, Train loss:0.001774, valid loss:0.001131
Epoch:46, Train loss:0.001771, valid loss:0.001148
Epoch:47, Train loss:0.001753, valid loss:0.001135
Epoch:48, Train loss:0.001752, valid loss:0.001167
Epoch:49, Train loss:0.001747, valid loss:0.001136
Epoch:50, Train loss:0.001735, valid loss:0.001114
Epoch:51, Train loss:0.001679, valid loss:0.001105
Epoch:52, Train loss:0.001667, valid loss:0.001096
Epoch:53, Train loss:0.001666, valid loss:0.001100
Epoch:54, Train loss:0.001663, valid loss:0.001109
Epoch:55, Train loss:0.001662, valid loss:0.001106
Epoch:56, Train loss:0.001660, valid loss:0.001108
Epoch:57, Train loss:0.001662, valid loss:0.001124
Epoch:58, Train loss:0.001660, valid loss:0.001111
Epoch:59, Train loss:0.001660, valid loss:0.001111
Epoch:60, Train loss:0.001660, valid loss:0.001108
training time 6791.610110044479
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.37740463913295447
plot_id,batch_id 0 1 miss% 0.4765002171198348
plot_id,batch_id 0 2 miss% 0.5484471349619497
plot_id,batch_id 0 3 miss% 0.4370544239263542
plot_id,batch_id 0 4 miss% 0.4481284272609833
plot_id,batch_id 0 5 miss% 0.44146100516493375
plot_id,batch_id 0 6 miss% 0.5255562655972663
plot_id,batch_id 0 7 miss% 0.5566684927230111
plot_id,batch_id 0 8 miss% 0.7028637253121084
plot_id,batch_id 0 9 miss% 0.5974556852539202
plot_id,batch_id 0 10 miss% 0.3780059226034097
plot_id,batch_id 0 11 miss% 0.4245275394997324
plot_id,batch_id 0 12 miss% 0.6088944990619192
plot_id,batch_id 0 13 miss% 0.38944754023205164
plot_id,batch_id 0 14 miss% 0.4853583957151808
plot_id,batch_id 0 15 miss% 0.3692498322444982
plot_id,batch_id 0 16 miss% 0.5099339492101916
plot_id,batch_id 0 17 miss% 0.4653088981752208
plot_id,batch_id 0 18 miss% 0.5037014693769605
plot_id,batch_id 0 19 miss% 0.4240188994156895
plot_id,batch_id 0 20 miss% 0.4469955089229691
plot_id,batch_id 0 21 miss% 0.5310385689427832
plot_id,batch_id 0 22 miss% 0.5688805660904374
plot_id,batch_id 0 23 miss% 0.5602886750293721
plot_id,batch_id 0 24 miss% 0.5208624769490858
plot_id,batch_id 0 25 miss% 0.42500322881090447
plot_id,batch_id 0 26 miss% 0.5042322440540398
plot_id,batch_id 0 27 miss% 0.46358175898883913
plot_id,batch_id 0 28 miss% 0.5098916951796201
plot_id,batch_id 0 29 miss% 0.5111876182219932
plot_id,batch_id 0 30 miss% 0.48129895941926665
plot_id,batch_id 0 31 miss% 0.5901973051038708
plot_id,batch_id 0 32 miss% 0.5683587364062825
plot_id,batch_id 0 33 miss% 0.5836974788464152
plot_id,batch_id 0 34 miss% 0.5605777717375567
plot_id,batch_id 0 35 miss% 0.5076936052606181
plot_id,batch_id 0 36 miss% 0.6451405594174153
plot_id,batch_id 0 37 miss% 0.5031686501142929
plot_id,batch_id 0 38 miss% 0.5249058449272468
plot_id,batch_id 0 39 miss% 0.4793102341475917
plot_id,batch_id 0 40 miss% 0.44178313528759117
plot_id,batch_id 0 41 miss% 0.5068910379613968
plot_id,batch_id 0 42 miss% 0.4375443707319105
plot_id,batch_id 0 43 miss% 0.39717078767405994
plot_id,batch_id 0 44 miss% 0.5001758768032376
plot_id,batch_id 0 45 miss% 0.5216819981611027
plot_id,batch_id 0 46 miss% 0.4457679341695767
plot_id,batch_id 0 47 miss% 0.5860776804749032
plot_id,batch_id 0 48 miss% 0.5749811514403887
plot_id,batch_id 0 49 miss% 0.47175980935620665
plot_id,batch_id 0 50 miss% 0.6298228267053742
plot_id,batch_id 0 51 miss% 0.6163082395921493
plot_id,batch_id 0 52 miss% 0.5783336674806676
plot_id,batch_id 0 53 miss% 0.45528608323077274
plot_id,batch_id 0 54 miss% 0.43783562462483977
plot_id,batch_id 0 55 miss% 0.47503238747095183
plot_id,batch_id 0 56 miss% 0.6026279431084556
plot_id,batch_id 0 57 miss% 0.57547251832283
plot_id,batch_id 0 58 miss% 0.5576784944115817
plot_id,batch_id 0 59 miss% 0.4620958090672076
plot_id,batch_id 0 60 miss% 0.34294849701110436
plot_id,batch_id 0 61 miss% 0.37850432013617347
plot_id,batch_id 0 62 miss% 0.6225611241763249
plot_id,batch_id 0 63 miss% 0.45715506384714727
plot_id,batch_id 0 64 miss% 0.4571294208681856
plot_id,batch_id 0 65 miss% 0.387740458885569
plot_id,batch_id 0 66 miss% 0.49341975695448126
plot_id,batch_id 0 67 miss% 0.4054963402983049
plot_id,batch_id 0 68 miss% 0.5015370254698447
plot_id,batch_id 0 69 miss% 0.4972764100648488
plot_id,batch_id 0 70 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  41745
Epoch:0, Train loss:0.753763, valid loss:0.732127
Epoch:1, Train loss:0.053233, valid loss:0.007217
Epoch:2, Train loss:0.011375, valid loss:0.003730
Epoch:3, Train loss:0.007557, valid loss:0.003570
Epoch:4, Train loss:0.006167, valid loss:0.002989
Epoch:5, Train loss:0.005616, valid loss:0.002614
Epoch:6, Train loss:0.005078, valid loss:0.002464
Epoch:7, Train loss:0.004725, valid loss:0.002450
Epoch:8, Train loss:0.004653, valid loss:0.002139
Epoch:9, Train loss:0.004839, valid loss:0.002219
Epoch:10, Train loss:0.004262, valid loss:0.002267
Epoch:11, Train loss:0.003260, valid loss:0.001721
Epoch:12, Train loss:0.003173, valid loss:0.001726
Epoch:13, Train loss:0.003054, valid loss:0.001522
Epoch:14, Train loss:0.002994, valid loss:0.001526
Epoch:15, Train loss:0.002877, valid loss:0.001596
Epoch:16, Train loss:0.002790, valid loss:0.001631
Epoch:17, Train loss:0.002785, valid loss:0.001437
Epoch:18, Train loss:0.002664, valid loss:0.001502
Epoch:19, Train loss:0.002585, valid loss:0.001525
Epoch:20, Train loss:0.002517, valid loss:0.001421
Epoch:21, Train loss:0.002034, valid loss:0.001243
Epoch:22, Train loss:0.001985, valid loss:0.001288
Epoch:23, Train loss:0.001962, valid loss:0.001194
Epoch:24, Train loss:0.001924, valid loss:0.001331
Epoch:25, Train loss:0.001948, valid loss:0.001167
Epoch:26, Train loss:0.001855, valid loss:0.001203
Epoch:27, Train loss:0.001846, valid loss:0.001240
Epoch:28, Train loss:0.001808, valid loss:0.001237
Epoch:29, Train loss:0.001845, valid loss:0.001176
Epoch:30, Train loss:0.001768, valid loss:0.001158
Epoch:31, Train loss:0.001497, valid loss:0.001060
Epoch:32, Train loss:0.001458, valid loss:0.001116
Epoch:33, Train loss:0.001482, valid loss:0.001138
Epoch:34, Train loss:0.001458, valid loss:0.001094
Epoch:35, Train loss:0.001456, valid loss:0.001090
Epoch:36, Train loss:0.001408, valid loss:0.001107
Epoch:37, Train loss:0.001444, valid loss:0.001083
Epoch:38, Train loss:0.001412, valid loss:0.001085
Epoch:39, Train loss:0.001401, valid loss:0.001101
Epoch:40, Train loss:0.001366, valid loss:0.001109
Epoch:41, Train loss:0.001243, valid loss:0.001013
Epoch:42, Train loss:0.001224, valid loss:0.001047
Epoch:43, Train loss:0.001218, valid loss:0.001055
Epoch:44, Train loss:0.001216, valid loss:0.001053
Epoch:45, Train loss:0.001215, valid loss:0.001029
Epoch:46, Train loss:0.001210, valid loss:0.001038
Epoch:47, Train loss:0.001203, valid loss:0.001023
Epoch:48, Train loss:0.001202, valid loss:0.001077
Epoch:49, Train loss:0.001194, valid loss:0.001066
Epoch:50, Train loss:0.001171, valid loss:0.001029
Epoch:51, Train loss:0.001106, valid loss:0.001011
Epoch:52, Train loss:0.001093, valid loss:0.001007
Epoch:53, Train loss:0.001088, valid loss:0.001000
Epoch:54, Train loss:0.001085, valid loss:0.001002
Epoch:55, Train loss:0.001083, valid loss:0.001001
Epoch:56, Train loss:0.001082, valid loss:0.001006
Epoch:57, Train loss:0.001081, valid loss:0.001000
Epoch:58, Train loss:0.001079, valid loss:0.001006
Epoch:59, Train loss:0.001077, valid loss:0.000999
Epoch:60, Train loss:0.001078, valid loss:0.000999
training time 6794.213379383087
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.4796934048093152
plot_id,batch_id 0 1 miss% 0.45622775533416193
plot_id,batch_id 0 2 miss% 0.5093597757208427
plot_id,batch_id 0 3 miss% 0.4555538710251964
plot_id,batch_id 0 4 miss% 0.4465603457460436
plot_id,batch_id 0 5 miss% 0.46772332819563345
plot_id,batch_id 0 6 miss% 0.40427536469525255
plot_id,batch_id 0 7 miss% 0.5514895431358308
plot_id,batch_id 0 8 miss% 0.5067647867226782
plot_id,batch_id 0 9 miss% 0.48230688442222575
plot_id,batch_id 0 10 miss% 0.437146095827436
plot_id,batch_id 0 11 miss% 0.41619179322855815
plot_id,batch_id 0 12 miss% 0.4742759566571956
plot_id,batch_id 0 13 miss% 0.5039157034461736
plot_id,batch_id 0 14 miss% 0.4344450907230486
plot_id,batch_id 0 15 miss% 0.4369056027432027
plot_id,batch_id 0 16 miss% 0.6095495554710639
plot_id,batch_id 0 17 miss% 0.5977589104133585
plot_id,batch_id 0 18 miss% 0.5331598424489489
plot_id,batch_id 0 19 miss% 0.5159078393590802
plot_id,batch_id 0 20 miss% 0.6116262830288878
plot_id,batch_id 0 21 miss% 0.37035462822813897
plot_id,batch_id 0 22 miss% 0.4559511209401598
plot_id,batch_id 0 23 miss% 0.4612730785144205
plot_id,batch_id 0 24 miss% 0.409999028985004
plot_id,batch_id 0 25 miss% 0.5495405004111366
plot_id,batch_id 0 26 miss% 0.45694455862263056
plot_id,batch_id 0 27 miss% 0.46309308763660734
plot_id,batch_id 0 28 miss% 0.4624524427389042
plot_id,batch_id 0 29 miss% 0.48441175964651045
plot_id,batch_id 0 30 miss% 0.43763061506990486
plot_id,batch_id 0 31 miss% 0.5709229419865731
plot_id,batch_id 0 32 miss% 0.4485141876038117
plot_id,batch_id 0 33 miss% 0.43471562503980016
plot_id,batch_id 0 34 miss% 0.4848805847472228
plot_id,batch_id 0 35 miss% 0.5675106806974733
plot_id,batch_id 0 36 miss% 0.6647252861699311
plot_id,batch_id 0 37 miss% 0.4861344741563602
plot_id,batch_id 0 38 miss% 0.47574920477886024
plot_id,batch_id 0 39 miss% 0.7231917138015611
plot_id,batch_id 0 40 miss% 0.5068853270258589
plot_id,batch_id 0 41 miss% 0.41468087389216
plot_id,batch_id 0 42 miss% 0.5142778203904613
plot_id,batch_id 0 43 miss% 0.46174161282503784
plot_id,batch_id 0 44 miss% 0.4122526701285006
plot_id,batch_id 0 45 miss% 0.45175790843556046
plot_id,batch_id 0 46 miss% 0.5208466227005925
plot_id,batch_id 0 47 miss% 0.45651845881926667
plot_id,batch_id 0 48 miss% 0.4604603091362914
plot_id,batch_id 0 49 miss% 0.402715999537392
plot_id,batch_id 0 50 miss% 0.5569354354271275
plot_id,batch_id 0 51 miss% 0.5544357754103032
plot_id,batch_id 0 52 miss% 0.42704407451003934
plot_id,batch_id 0 53 miss% 0.35920726124230884
plot_id,batch_id 0 54 miss% 0.5033045430568948
plot_id,batch_id 0 55 miss% 0.5347555179979927
plot_id,batch_id 0 56 miss% 0.5382048177459386
plot_id,batch_id 0 57 miss% 0.5705419969349159
plot_id,batch_id 0 58 miss% 0.5767570670322442
plot_id,batch_id 0 59 miss% 0.5623821181527054
plot_id,batch_id 0 60 miss% 0.3467586225306652
plot_id,batch_id 0 61 miss% 0.4362053476238455
plot_id,batch_id 0 62 miss% 0.48245456098125694
plot_id,batch_id 0 63 miss% 0.4650291328940575
plot_id,batch_id 0 64 miss% 0.5237164987411033
plot_id,batch_id 0 65 miss% 0.44700962111792636
plot_id,batch_id 0 66 miss% 0.6321913565591103
plot_id,batch_id 0 67 miss% 0.4451898052344645
plot_id,batch_id 0 68 miss% 0.5108063449937692
plot_id,batch_id 0 69 miss% 0.5580468205363281
plot_id,batch_id 0 70 miss% 0.3832414147153589
plot_id,batch_id 0 71 miss% 0.5146207360969427
plot_id,batch_id 0 72 miss% 0.5069012949017244
plot_id,batch_id 0 73 miss% 0.4744587427309269
plot_id,batch_id 0 74 miss% 0.5039578172221273
plot_id,batch_id 0 75 miss% 0.37019073134287855
plot_id,batch_id 0 76 miss% 0.48941417998266223
plot_id,batch_id 0 77 miss% 0.44925625960101023
plot_id,batch_id 0 78 miss% 0.4903001662945231
plot_id,batch_id 0 79 miss% 0.43980965021155505
plot_id,batch_id 0 80 miss% 0.45406856718563343
plot_id,batch_id 0 81 miss% 0.5213313740976847
plot_id,batch_id 0 82 miss% 0.4494016817867067
plot_id,batch_id 0 83 miss% 0.47949230982817326
plot_id,batch_id 0 84 miss% 0.45698504726825373
plot_id,batch_id 0 85 miss% 0.38910856643162045
plot_id,batch_id 0 86 miss% 0.4797153569923803
plot_id,batch_id 0 87 miss% 0.48120260580637625
plot_id,batch_id 0 88 miss% 0.5009765119468971
plot_id,batch_id 0 89 miss% 0.511279797709436
plot_id,batch_id 0 90 miss% 0.3837417397237036
plot_id,batch_id 0 91 miss% 0.45105139634369795
plot_id,batch_id 0 92 miss% 0.5228080911331789
plot_id,batch_id 0 93 miss% 0.39579294715231383
plot_id,batch_id 0 94 miss% 0.6120758784154593
plot_id,batch_id 0 95 miss% 0.39713856314695856
plot_id,batch_id 0 96 miss% 0.4474909140719818
plot_id,batch_id 0 97 miss% 0.5685832273561763
plot_id,batch_id 0 98 miss% 0.5377389132502505
plot_id,batch_id 0 99 miss% 0.49423195928507474
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.4796934  0.45622776 0.50935978 0.45555387 0.44656035 0.46772333
 0.40427536 0.55148954 0.50676479 0.48230688 0.4371461  0.41619179
 0.47427596 0.5039157  0.43444509 0.4369056  0.60954956 0.59775891
 0.53315984 0.51590784 0.61162628 0.37035463 0.45595112 0.46127308
 0.40999903 0.5495405  0.45694456 0.46309309 0.46245244 0.48441176
 0.43763062 0.57092294 0.44851419 0.43471563 0.48488058 0.56751068
 0.66472529 0.48613447 0.4757492  0.72319171 0.50688533 0.41468087
 0.51427782 0.46174161 0.41225267 0.45175791 0.52084662 0.45651846
 0.46046031 0.402716   0.55693544 0.55443578 0.42704407 0.35920726
 0.50330454 0.53475552 0.53820482 0.570542   0.57675707 0.56238212
 0.34675862 0.43620535 0.48245456 0.46502913 0.5237165  0.44700962
 0.63219136 0.44518981 0.51080634 0.55804682 0.38324141 0.51462074
 0.50690129 0.47445874 0.50395782 0.37019073 0.48941418 0.44925626
 0.49030017 0.43980965 0.45406857 0.52133137 0.44940168 0.47949231
 0.45698505 0.38910857 0.47971536 0.48120261 0.50097651 0.5112798
 0.38374174 0.4510514  0.52280809 0.39579295 0.61207588 0.39713856
 0.44749091 0.56858323 0.53773891 0.49423196]
for model  56 the mean error 0.48528314016574936
all id 56 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 21 out win 6 err 0.48528314016574936 time 6794.213379383087
0.3654510068987008
plot_id,batch_id 0 71 miss% 0.4396334631764065
plot_id,batch_id 0 72 miss% 0.4349179288031979
plot_id,batch_id 0 73 miss% 0.3871472726328036
plot_id,batch_id 0 74 miss% 0.6034923927191275
plot_id,batch_id 0 75 miss% 0.39179499660635136
plot_id,batch_id 0 76 miss% 0.494536531763614
plot_id,batch_id 0 77 miss% 0.43958367615001553
plot_id,batch_id 0 78 miss% 0.41891109138751337
plot_id,batch_id 0 79 miss% 0.48738587194138855
plot_id,batch_id 0 80 miss% 0.41358531496919165
plot_id,batch_id 0 81 miss% 0.4424995812935942
plot_id,batch_id 0 82 miss% 0.44647155058481797
plot_id,batch_id 0 83 miss% 0.5143226854406902
plot_id,batch_id 0 84 miss% 0.48143341064135214
plot_id,batch_id 0 85 miss% 0.38993223894669465
plot_id,batch_id 0 86 miss% 0.4801511586701832
plot_id,batch_id 0 87 miss% 0.42058271297581445
plot_id,batch_id 0 88 miss% 0.5554467185300814
plot_id,batch_id 0 89 miss% 0.4509612463173163
plot_id,batch_id 0 90 miss% 0.2984015355516194
plot_id,batch_id 0 91 miss% 0.4683304142389328
plot_id,batch_id 0 92 miss% 0.37537464011407756
plot_id,batch_id 0 93 miss% 0.3808565104694102
plot_id,batch_id 0 94 miss% 0.5446606055950015
plot_id,batch_id 0 95 miss% 0.42882562011443476
plot_id,batch_id 0 96 miss% 0.45018328127267604
plot_id,batch_id 0 97 miss% 0.5435064280386306
plot_id,batch_id 0 98 miss% 0.49179364121768626
plot_id,batch_id 0 99 miss% 0.3732571409076688
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.37740464 0.47650022 0.54844713 0.43705442 0.44812843 0.44146101
 0.52555627 0.55666849 0.70286373 0.59745569 0.37800592 0.42452754
 0.6088945  0.38944754 0.4853584  0.36924983 0.50993395 0.4653089
 0.50370147 0.4240189  0.44699551 0.53103857 0.56888057 0.56028868
 0.52086248 0.42500323 0.50423224 0.46358176 0.5098917  0.51118762
 0.48129896 0.59019731 0.56835874 0.58369748 0.56057777 0.50769361
 0.64514056 0.50316865 0.52490584 0.47931023 0.44178314 0.50689104
 0.43754437 0.39717079 0.50017588 0.521682   0.44576793 0.58607768
 0.57498115 0.47175981 0.62982283 0.61630824 0.57833367 0.45528608
 0.43783562 0.47503239 0.60262794 0.57547252 0.55767849 0.46209581
 0.3429485  0.37850432 0.62256112 0.45715506 0.45712942 0.38774046
 0.49341976 0.40549634 0.50153703 0.49727641 0.36545101 0.43963346
 0.43491793 0.38714727 0.60349239 0.391795   0.49453653 0.43958368
 0.41891109 0.48738587 0.41358531 0.44249958 0.44647155 0.51432269
 0.48143341 0.38993224 0.48015116 0.42058271 0.55544672 0.45096125
 0.29840154 0.46833041 0.37537464 0.38085651 0.54466061 0.42882562
 0.45018328 0.54350643 0.49179364 0.37325714]
for model  1 the mean error 0.4838782491031495
all id 1 hidden_dim 16 learning_rate 0.0025 num_layers 3 frames 21 out win 5 err 0.4838782491031495 time 6791.610110044479
Launcher: Job 57 completed in 7026 seconds.
Launcher: Task 242 done. Exiting.
Launcher: Job 2 completed in 7028 seconds.
Launcher: Task 7 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  41745
Epoch:0, Train loss:0.753763, valid loss:0.732127
Epoch:1, Train loss:0.067887, valid loss:0.011949
Epoch:2, Train loss:0.017972, valid loss:0.004592
Epoch:3, Train loss:0.009526, valid loss:0.003758
Epoch:4, Train loss:0.007869, valid loss:0.003553
Epoch:5, Train loss:0.007086, valid loss:0.002989
Epoch:6, Train loss:0.006282, valid loss:0.002639
Epoch:7, Train loss:0.005779, valid loss:0.002652
Epoch:8, Train loss:0.005283, valid loss:0.002299
Epoch:9, Train loss:0.004910, valid loss:0.002473
Epoch:10, Train loss:0.004683, valid loss:0.002078
Epoch:11, Train loss:0.004013, valid loss:0.001788
Epoch:12, Train loss:0.003911, valid loss:0.001800
Epoch:13, Train loss:0.003810, valid loss:0.001828
Epoch:14, Train loss:0.003696, valid loss:0.001766
Epoch:15, Train loss:0.003615, valid loss:0.001783
Epoch:16, Train loss:0.003452, valid loss:0.001795
Epoch:17, Train loss:0.003388, valid loss:0.001642
Epoch:18, Train loss:0.003242, valid loss:0.001550
Epoch:19, Train loss:0.003189, valid loss:0.001616
Epoch:20, Train loss:0.003086, valid loss:0.001569
Epoch:21, Train loss:0.002768, valid loss:0.001521
Epoch:22, Train loss:0.002694, valid loss:0.001458
Epoch:23, Train loss:0.002658, valid loss:0.001429
Epoch:24, Train loss:0.002595, valid loss:0.001554
Epoch:25, Train loss:0.002571, valid loss:0.001352
Epoch:26, Train loss:0.002529, valid loss:0.001441
Epoch:27, Train loss:0.002488, valid loss:0.001388
Epoch:28, Train loss:0.002480, valid loss:0.001383
Epoch:29, Train loss:0.002417, valid loss:0.001384
Epoch:30, Train loss:0.002367, valid loss:0.001397
Epoch:31, Train loss:0.002189, valid loss:0.001256
Epoch:32, Train loss:0.002174, valid loss:0.001238
Epoch:33, Train loss:0.002157, valid loss:0.001227
Epoch:34, Train loss:0.002152, valid loss:0.001219
Epoch:35, Train loss:0.002124, valid loss:0.001226
Epoch:36, Train loss:0.002096, valid loss:0.001235
Epoch:37, Train loss:0.002086, valid loss:0.001160
Epoch:38, Train loss:0.002061, valid loss:0.001168
Epoch:39, Train loss:0.002054, valid loss:0.001203
Epoch:40, Train loss:0.002028, valid loss:0.001270
Epoch:41, Train loss:0.001934, valid loss:0.001204
Epoch:42, Train loss:0.001925, valid loss:0.001165
Epoch:43, Train loss:0.001918, valid loss:0.001177
Epoch:44, Train loss:0.001905, valid loss:0.001146
Epoch:45, Train loss:0.001903, valid loss:0.001136
Epoch:46, Train loss:0.001895, valid loss:0.001168
Epoch:47, Train loss:0.001875, valid loss:0.001202
Epoch:48, Train loss:0.001866, valid loss:0.001134
Epoch:49, Train loss:0.001860, valid loss:0.001135
Epoch:50, Train loss:0.001857, valid loss:0.001175
Epoch:51, Train loss:0.001786, valid loss:0.001103
Epoch:52, Train loss:0.001780, valid loss:0.001104
Epoch:53, Train loss:0.001779, valid loss:0.001115
Epoch:54, Train loss:0.001776, valid loss:0.001104
Epoch:55, Train loss:0.001775, valid loss:0.001108
Epoch:56, Train loss:0.001775, valid loss:0.001114
Epoch:57, Train loss:0.001774, valid loss:0.001092
Epoch:58, Train loss:0.001773, valid loss:0.001103
Epoch:59, Train loss:0.001771, valid loss:0.001093
Epoch:60, Train loss:0.001772, valid loss:0.001097
training time 6856.6272230148315
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.44994188099274246
plot_id,batch_id 0 1 miss% 0.5172160557817715
plot_id,batch_id 0 2 miss% 0.5926989796309151
plot_id,batch_id 0 3 miss% 0.4761564952229653
plot_id,batch_id 0 4 miss% 0.5187515795404188
plot_id,batch_id 0 5 miss% 0.5363505553383091
plot_id,batch_id 0 6 miss% 0.46395190039675716
plot_id,batch_id 0 7 miss% 0.6347979278115023
plot_id,batch_id 0 8 miss% 0.6584104778497382
plot_id,batch_id 0 9 miss% 0.6489530872138084
plot_id,batch_id 0 10 miss% 0.4412239818832645
plot_id,batch_id 0 11 miss% 0.44146556032862855
plot_id,batch_id 0 12 miss% 0.5728643877776649
plot_id,batch_id 0 13 miss% 0.4860684097612162
plot_id,batch_id 0 14 miss% 0.5210549870727191
plot_id,batch_id 0 15 miss% 0.45254403758204326
plot_id,batch_id 0 16 miss% 0.581681346409391
plot_id,batch_id 0 17 miss% 0.5229161441697673
plot_id,batch_id 0 18 miss% 0.5993986969978768
plot_id,batch_id 0 19 miss% 0.5267697843537156
plot_id,batch_id 0 20 miss% 0.5199046315576945
plot_id,batch_id 0 21 miss% 0.5013111863104577
plot_id,batch_id 0 22 miss% 0.5387753174554519
plot_id,batch_id 0 23 miss% 0.5907665455361704
plot_id,batch_id 0 24 miss% 0.5429853908466181
plot_id,batch_id 0 25 miss% 0.510448346611429
plot_id,batch_id 0 26 miss% 0.5189592954605944
plot_id,batch_id 0 27 miss% 0.5236010399913306
plot_id,batch_id 0 28 miss% 0.4830829375297229
plot_id,batch_id 0 29 miss% 0.5264180035247323
plot_id,batch_id 0 30 miss% 0.46414472476343505
plot_id,batch_id 0 31 miss% 0.5256351305093299
plot_id,batch_id 0 32 miss% 0.6068928177298282
plot_id,batch_id 0 33 miss% 0.47934290750964187
plot_id,batch_id 0 34 miss% 0.5180210661969872
plot_id,batch_id 0 35 miss% 0.5439406724915492
plot_id,batch_id 0 36 miss% 0.5864681079916362
plot_id,batch_id 0 37 miss% 0.4684485651151159
plot_id,batch_id 0 38 miss% 0.48384432692085355
plot_id,batch_id 0 39 miss% 0.626205881602815
plot_id,batch_id 0 40 miss% 0.5045083391374862
plot_id,batch_id 0 41 miss% 0.5188198509287918
plot_id,batch_id 0 42 miss% 0.485724538534299
plot_id,batch_id 0 43 miss% 0.43243934100378056
plot_id,batch_id 0 44 miss% 0.4471292847845966
plot_id,batch_id 0 45 miss% 0.5428940507479898
plot_id,batch_id 0 46 miss% 0.5747844464310199
plot_id,batch_id 0 47 miss% 0.5597516520852373
plot_id,batch_id 0 48 miss% 0.5127065580711084
plot_id,batch_id 0 49 miss% 0.4422141726520314
plot_id,batch_id 0 50 miss% 0.5112037676157919
plot_id,batch_id 0 51 miss% 0.6008449884134822
plot_id,batch_id 0 52 miss% 0.5817167904026649
plot_id,batch_id 0 53 miss% 0.4648588037855876
plot_id,batch_id 0 54 miss% 0.542680688514361
plot_id,batch_id 0 55 miss% 0.5096945423709623
plot_id,batch_id 0 56 miss% 0.5962054533711126
plot_id,batch_id 0 57 miss% 0.5862206939735356
plot_id,batch_id 0 58 miss% 0.6086954313878741
plot_id,batch_id 0 59 miss% 0.5433179172323422
plot_id,batch_id 0 60 miss% 0.40301389336764853
plot_id,batch_id 0 61 miss% 0.4112278202454513
plot_id,batch_id 0 62 miss% 0.418383781090097
plot_id,batch_id 0 63 miss% 0.4727630748590047
plot_id,batch_id 0 64 miss% 0.46146970712181345
plot_id,batch_id 0 65 miss% 0.39358442632826507
plot_id,batch_id 0 66 miss% 0.5058207681382574
plot_id,batch_id 0 67 miss% 0.42236847334471933
plot_id,batch_id 0 68 miss% 0.5546133716372809
plot_id,batch_id 0 69 miss% 0.5547711983990883
plot_id,batch_id 0 70 miss% 0.3822557603825259
plot_id,batch_id 0 71 miss% 0.5122762203554646
plot_id,batch_id 0 72 miss% 0.4805544417318755
plot_id,batch_id 0 73 miss% 0.4583504940189315
plot_id,batch_id 0 74 miss% 0.5164673041718998
plot_id,batch_id 0 75 miss% 0.3470892345166532
plot_id,batch_id 0 76 miss% 0.5145868142549065
plot_id,batch_id 0 77 miss% 0.4320439589392527
plot_id,batch_id 0 78 miss% 0.48014339714501825
plot_id,batch_id 0 79 miss% 0.39227942969557167
plot_id,batch_id 0 80 miss% 0.4173366559905718
plot_id,batch_id 0 81 miss% 0.6136623969691519
plot_id,batch_id 0 82 miss% 0.48325914711962414
plot_id,batch_id 0 83 miss% 0.5066774641841159
plot_id,batch_id 0 84 miss% 0.5419390027940869
plot_id,batch_id 0 85 miss% 0.42938149135822057
plot_id,batch_id 0 86 miss% 0.513697790471882
plot_id,batch_id 0 87 miss% 0.47831224731993655
plot_id,batch_id 0 88 miss% 0.5356119779451338
plot_id,batch_id 0 89 miss% 0.5029579517240507
plot_id,batch_id 0 90 miss% 0.3456535028696092
plot_id,batch_id 0 91 miss% 0.4444170514156298
plot_id,batch_id 0 92 miss% 0.4247637088134623
plot_id,batch_id 0 93 miss% 0.36919090978335867
plot_id,batch_id 0 94 miss% 0.6240012636165797
plot_id,batch_id 0 95 miss% 0.40766311298741864
plot_id,batch_id 0 96 miss% 0.4289369373867566
plot_id,batch_id 0 97 miss% 0.5319324292320209
plot_id,batch_id 0 98 miss% 0.5902868760198939
plot_id,batch_id 0 99 miss% 0.40794643452736956
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.44994188 0.51721606 0.59269898 0.4761565  0.51875158 0.53635056
 0.4639519  0.63479793 0.65841048 0.64895309 0.44122398 0.44146556
 0.57286439 0.48606841 0.52105499 0.45254404 0.58168135 0.52291614
 0.5993987  0.52676978 0.51990463 0.50131119 0.53877532 0.59076655
 0.54298539 0.51044835 0.5189593  0.52360104 0.48308294 0.526418
 0.46414472 0.52563513 0.60689282 0.47934291 0.51802107 0.54394067
 0.58646811 0.46844857 0.48384433 0.62620588 0.50450834 0.51881985
 0.48572454 0.43243934 0.44712928 0.54289405 0.57478445 0.55975165
 0.51270656 0.44221417 0.51120377 0.60084499 0.58171679 0.4648588
 0.54268069 0.50969454 0.59620545 0.58622069 0.60869543 0.54331792
 0.40301389 0.41122782 0.41838378 0.47276307 0.46146971 0.39358443
 0.50582077 0.42236847 0.55461337 0.5547712  0.38225576 0.51227622
 0.48055444 0.45835049 0.5164673  0.34708923 0.51458681 0.43204396
 0.4801434  0.39227943 0.41733666 0.6136624  0.48325915 0.50667746
 0.541939   0.42938149 0.51369779 0.47831225 0.53561198 0.50295795
 0.3456535  0.44441705 0.42476371 0.36919091 0.62400126 0.40766311
 0.42893694 0.53193243 0.59028688 0.40794643]
for model  2 the mean error 0.5048051640548726
all id 2 hidden_dim 16 learning_rate 0.0025 num_layers 3 frames 21 out win 6 err 0.5048051640548726 time 6856.6272230148315
Launcher: Job 3 completed in 7087 seconds.
Launcher: Task 16 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  41745
Epoch:0, Train loss:0.812563, valid loss:0.804348
Epoch:1, Train loss:0.045591, valid loss:0.004494
Epoch:2, Train loss:0.008537, valid loss:0.003857
Epoch:3, Train loss:0.006238, valid loss:0.002685
Epoch:4, Train loss:0.005439, valid loss:0.002384
Epoch:5, Train loss:0.004993, valid loss:0.002013
Epoch:6, Train loss:0.004577, valid loss:0.001910
Epoch:7, Train loss:0.004154, valid loss:0.002223
Epoch:8, Train loss:0.004051, valid loss:0.002140
Epoch:9, Train loss:0.003608, valid loss:0.001775
Epoch:10, Train loss:0.003633, valid loss:0.001701
Epoch:11, Train loss:0.002644, valid loss:0.001532
Epoch:12, Train loss:0.002534, valid loss:0.001189
Epoch:13, Train loss:0.002569, valid loss:0.001370
Epoch:14, Train loss:0.002439, valid loss:0.001436
Epoch:15, Train loss:0.002427, valid loss:0.001182
Epoch:16, Train loss:0.002347, valid loss:0.001262
Epoch:17, Train loss:0.002266, valid loss:0.001260
Epoch:18, Train loss:0.002249, valid loss:0.001082
Epoch:19, Train loss:0.002147, valid loss:0.001152
Epoch:20, Train loss:0.002087, valid loss:0.001320
Epoch:21, Train loss:0.001686, valid loss:0.000958
Epoch:22, Train loss:0.001664, valid loss:0.000951
Epoch:23, Train loss:0.001644, valid loss:0.000895
Epoch:24, Train loss:0.001600, valid loss:0.000954
Epoch:25, Train loss:0.001603, valid loss:0.001013
Epoch:26, Train loss:0.001558, valid loss:0.000986
Epoch:27, Train loss:0.001537, valid loss:0.000952
Epoch:28, Train loss:0.001506, valid loss:0.000927
Epoch:29, Train loss:0.001532, valid loss:0.001024
Epoch:30, Train loss:0.001452, valid loss:0.000900
Epoch:31, Train loss:0.001242, valid loss:0.000873
Epoch:32, Train loss:0.001236, valid loss:0.000827
Epoch:33, Train loss:0.001235, valid loss:0.000819
Epoch:34, Train loss:0.001233, valid loss:0.000922
Epoch:35, Train loss:0.001214, valid loss:0.000800
Epoch:36, Train loss:0.001208, valid loss:0.000882
Epoch:37, Train loss:0.001178, valid loss:0.000827
Epoch:38, Train loss:0.001178, valid loss:0.000850
Epoch:39, Train loss:0.001173, valid loss:0.000829
Epoch:40, Train loss:0.001150, valid loss:0.000835
Epoch:41, Train loss:0.001054, valid loss:0.000859
Epoch:42, Train loss:0.001037, valid loss:0.000770
Epoch:43, Train loss:0.001046, valid loss:0.000808
Epoch:44, Train loss:0.001035, valid loss:0.000749
Epoch:45, Train loss:0.001025, valid loss:0.000771
Epoch:46, Train loss:0.001024, valid loss:0.000766
Epoch:47, Train loss:0.001024, valid loss:0.000761
Epoch:48, Train loss:0.001015, valid loss:0.000745
Epoch:49, Train loss:0.001008, valid loss:0.000773
Epoch:50, Train loss:0.001007, valid loss:0.000743
Epoch:51, Train loss:0.000941, valid loss:0.000729
Epoch:52, Train loss:0.000933, valid loss:0.000729
Epoch:53, Train loss:0.000931, valid loss:0.000733
Epoch:54, Train loss:0.000927, valid loss:0.000729
Epoch:55, Train loss:0.000927, valid loss:0.000730
Epoch:56, Train loss:0.000926, valid loss:0.000728
Epoch:57, Train loss:0.000925, valid loss:0.000728
Epoch:58, Train loss:0.000924, valid loss:0.000728
Epoch:59, Train loss:0.000923, valid loss:0.000729
Epoch:60, Train loss:0.000923, valid loss:0.000729
training time 6987.995615720749
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.30395013985132113
plot_id,batch_id 0 1 miss% 0.4112193818422453
plot_id,batch_id 0 2 miss% 0.4592106130125594
plot_id,batch_id 0 3 miss% 0.29444791565882866
plot_id,batch_id 0 4 miss% 0.3318635810724587
plot_id,batch_id 0 5 miss% 0.34686560492482466
plot_id,batch_id 0 6 miss% 0.404084942018232
plot_id,batch_id 0 7 miss% 0.44271448829181054
plot_id,batch_id 0 8 miss% 0.529242686763792
plot_id,batch_id 0 9 miss% 0.37030981443789834
plot_id,batch_id 0 10 miss% 0.2724930416634021
plot_id,batch_id 0 11 miss% 0.3208860719826345
plot_id,batch_id 0 12 miss% 0.509704745440061
plot_id,batch_id 0 13 miss% 0.39013852509373936
plot_id,batch_id 0 14 miss% 0.36858718835702226
plot_id,batch_id 0 15 miss% 0.2935808264726444
plot_id,batch_id 0 16 miss% 0.4679718642364534
plot_id,batch_id 0 17 miss% 0.5314169804247135
plot_id,batch_id 0 18 miss% 0.3885302215343466
plot_id,batch_id 0 19 miss% 0.3882286109798228
plot_id,batch_id 0 20 miss% 0.38397317185995933
plot_id,batch_id 0 21 miss% 0.4950497412861289
plot_id,batch_id 0 22 miss% 0.4088817849577961
plot_id,batch_id 0 23 miss% 0.3773747673682963
plot_id,batch_id 0 24 miss% 0.37494789171553017
plot_id,batch_id 0 25 miss% 0.37842006292529073
plot_id,batch_id 0 26 miss% 0.41179587120585054
plot_id,batch_id 0 27 miss% 0.3454835177088776
plot_id,batch_id 0 28 miss% 0.5252999517186836
plot_id,batch_id 0 29 miss% 0.3610200538128938
plot_id,batch_id 0 30 miss% 0.3004388965944393
plot_id,batch_id 0 31 miss% 0.5092326183104
plot_id,batch_id 0 32 miss% 0.4881572777927044
plot_id,batch_id 0 33 miss% 0.48375677100921927
plot_id,batch_id 0 34 miss% 0.32293659817901876
plot_id,batch_id 0 35 miss% 0.3407786770496533
plot_id,batch_id 0 36 miss% 0.5437761323789996
plot_id,batch_id 0 37 miss% 0.4053156576965893
plot_id,batch_id 0 38 miss% 0.3825616746166527
plot_id,batch_id 0 39 miss% 0.3850426064123961
plot_id,batch_id 0 40 miss% 0.39651364416692897
plot_id,batch_id 0 41 miss% 0.3088367475235049
plot_id,batch_id 0 42 miss% 0.27283168324878326
plot_id,batch_id 0 43 miss% 0.32476812232671237
plot_id,batch_id 0 44 miss% 0.33874899016989585
plot_id,batch_id 0 45 miss% 0.30544925221891017
plot_id,batch_id 0 46 miss% 0.43674624946850227
plot_id,batch_id 0 47 miss% 0.3928205094173621
plot_id,batch_id 0 48 miss% 0.4198559978180153
plot_id,batch_id 0 49 miss% 0.4725889017920752
plot_id,batch_id 0 50 miss% 0.6605951867056268
plot_id,batch_id 0 51 miss% 0.4735615870969285
plot_id,batch_id 0 52 miss% 0.4270320002299698
plot_id,batch_id 0 53 miss% 0.272350812052234
plot_id,batch_id 0 54 miss% 0.29000523692453634
plot_id,batch_id 0 55 miss% 0.5471838814559069
plot_id,batch_id 0 56 miss% 0.5340815885809831
plot_id,batch_id 0 57 miss% 0.548122574463801
plot_id,batch_id 0 58 miss% 0.6193935414935892
plot_id,batch_id 0 59 miss% 0.42994753894926924
plot_id,batch_id 0 60 miss% 0.26455465659512706
plot_id,batch_id 0 61 miss% 0.28696634976362384
plot_id,batch_id 0 62 miss% 0.4760972487985364
plot_id,batch_id 0 63 miss% 0.3474348621586924
plot_id,batch_id 0 64 miss% 0.39410677295306856
plot_id,batch_id 0 65 miss% 0.4048881893200855
plot_id,batch_id 0 66 miss% 0.43519718431762505
plot_id,batch_id 0 67 miss% 0.27597452578356213
plot_id,batch_id 0 68 miss% 0.4246126994485332
plot_id,batch_id 0 69 miss% 0.41121380681998043
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  41745
Epoch:0, Train loss:0.812563, valid loss:0.804348
Epoch:1, Train loss:0.048076, valid loss:0.005536
Epoch:2, Train loss:0.011281, valid loss:0.003809
Epoch:3, Train loss:0.007080, valid loss:0.003239
Epoch:4, Train loss:0.005949, valid loss:0.002843
Epoch:5, Train loss:0.005233, valid loss:0.002314
Epoch:6, Train loss:0.004710, valid loss:0.002554
Epoch:7, Train loss:0.004364, valid loss:0.002550
Epoch:8, Train loss:0.004086, valid loss:0.002178
Epoch:9, Train loss:0.003696, valid loss:0.001839
Epoch:10, Train loss:0.003918, valid loss:0.001642
Epoch:11, Train loss:0.002794, valid loss:0.001377
Epoch:12, Train loss:0.002673, valid loss:0.001319
Epoch:13, Train loss:0.002627, valid loss:0.001531
Epoch:14, Train loss:0.002513, valid loss:0.001411
Epoch:15, Train loss:0.002519, valid loss:0.001288
Epoch:16, Train loss:0.002396, valid loss:0.001297
Epoch:17, Train loss:0.002308, valid loss:0.001227
Epoch:18, Train loss:0.002309, valid loss:0.001172
Epoch:19, Train loss:0.002186, valid loss:0.001128
Epoch:20, Train loss:0.002187, valid loss:0.001396
Epoch:21, Train loss:0.001821, valid loss:0.000975
Epoch:22, Train loss:0.001751, valid loss:0.001102
Epoch:23, Train loss:0.001744, valid loss:0.000939
Epoch:24, Train loss:0.001710, valid loss:0.000943
Epoch:25, Train loss:0.001672, valid loss:0.001040
Epoch:26, Train loss:0.001668, valid loss:0.001014
Epoch:27, Train loss:0.001639, valid loss:0.000992
Epoch:28, Train loss:0.001640, valid loss:0.000982
Epoch:29, Train loss:0.001615, valid loss:0.001180
Epoch:30, Train loss:0.001568, valid loss:0.000927
Epoch:31, Train loss:0.001385, valid loss:0.000865
Epoch:32, Train loss:0.001374, valid loss:0.000873
Epoch:33, Train loss:0.001374, valid loss:0.000861
Epoch:34, Train loss:0.001358, valid loss:0.000928
Epoch:35, Train loss:0.001341, valid loss:0.000865
Epoch:36, Train loss:0.001332, valid loss:0.000917
Epoch:37, Train loss:0.001316, valid loss:0.000855
Epoch:38, Train loss:0.001309, valid loss:0.000847
Epoch:39, Train loss:0.001294, valid loss:0.000827
Epoch:40, Train loss:0.001297, valid loss:0.000822
Epoch:41, Train loss:0.001192, valid loss:0.000825
Epoch:42, Train loss:0.001181, valid loss:0.000863
Epoch:43, Train loss:0.001179, valid loss:0.000810
Epoch:44, Train loss:0.001169, valid loss:0.000798
Epoch:45, Train loss:0.001175, valid loss:0.000827
Epoch:46, Train loss:0.001166, valid loss:0.000804
Epoch:47, Train loss:0.001160, valid loss:0.000813
Epoch:48, Train loss:0.001154, valid loss:0.000798
Epoch:49, Train loss:0.001146, valid loss:0.000855
Epoch:50, Train loss:0.001141, valid loss:0.000815
Epoch:51, Train loss:0.001087, valid loss:0.000776
Epoch:52, Train loss:0.001077, valid loss:0.000783
Epoch:53, Train loss:0.001073, valid loss:0.000780
Epoch:54, Train loss:0.001070, valid loss:0.000778
Epoch:55, Train loss:0.001070, valid loss:0.000774
Epoch:56, Train loss:0.001069, valid loss:0.000773
Epoch:57, Train loss:0.001068, valid loss:0.000773
Epoch:58, Train loss:0.001067, valid loss:0.000770
Epoch:59, Train loss:0.001066, valid loss:0.000776
Epoch:60, Train loss:0.001065, valid loss:0.000773
training time 7013.648733377457
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.28527890826193697
plot_id,batch_id 0 1 miss% 0.3707338344731134
plot_id,batch_id 0 2 miss% 0.455089318612508
plot_id,batch_id 0 3 miss% 0.36254133109743947
plot_id,batch_id 0 4 miss% 0.30268826764107265
plot_id,batch_id 0 5 miss% 0.35719409926107204
plot_id,batch_id 0 6 miss% 0.46066965875784155
plot_id,batch_id 0 7 miss% 0.4965439974298358
plot_id,batch_id 0 8 miss% 0.6346882913300164
plot_id,batch_id 0 9 miss% 0.46815909240855014
plot_id,batch_id 0 10 miss% 0.22892073579923963
plot_id,batch_id 0 11 miss% 0.359797394289466
plot_id,batch_id 0 12 miss% 0.3608297461187761
plot_id,batch_id 0 13 miss% 0.37562106698702696
plot_id,batch_id 0 14 miss% 0.49218996709114926
plot_id,batch_id 0 15 miss% 0.2529698833523757
plot_id,batch_id 0 16 miss% 0.47179502425679204
plot_id,batch_id 0 17 miss% 0.4854775124310758
plot_id,batch_id 0 18 miss% 0.48091048064892894
plot_id,batch_id 0 19 miss% 0.44704543426081994
plot_id,batch_id 0 20 miss% 0.3277520894742537
plot_id,batch_id 0 21 miss% 0.3485716582058395
plot_id,batch_id 0 22 miss% 0.3650964620611568
plot_id,batch_id 0 23 miss% 0.3956651928799619
plot_id,batch_id 0 24 miss% 0.4333097816771828
plot_id,batch_id 0 25 miss% 0.3128353090793049
plot_id,batch_id 0 26 miss% 0.41015457066813127
plot_id,batch_id 0 27 miss% 0.38170732904076804
plot_id,batch_id 0 28 miss% 0.3619129199465084
plot_id,batch_id 0 29 miss% 0.4026200328546306
plot_id,batch_id 0 30 miss% 0.3209861407059006
plot_id,batch_id 0 31 miss% 0.4360527178803498
plot_id,batch_id 0 32 miss% 0.5215040598221486
plot_id,batch_id 0 33 miss% 0.415448544675497
plot_id,batch_id 0 34 miss% 0.46862448771781323
plot_id,batch_id 0 35 miss% 0.3151360843308701
plot_id,batch_id 0 36 miss% 0.49994245458301456
plot_id,batch_id 0 37 miss% 0.3692803792398301
plot_id,batch_id 0 38 miss% 0.36239823478710625
plot_id,batch_id 0 39 miss% 0.5096431288057373
plot_id,batch_id 0 40 miss% 0.3610838169200112
plot_id,batch_id 0 41 miss% 0.3844126519756296
plot_id,batch_id 0 42 miss% 0.3239133669770159
plot_id,batch_id 0 43 miss% 0.295381030950942
plot_id,batch_id 0 44 miss% 0.25206344856545804
plot_id,batch_id 0 45 miss% 0.3755743415402482
plot_id,batch_id 0 46 miss% 0.4053683336081864
plot_id,batch_id 0 47 miss% 0.4557993019472118
plot_id,batch_id 0 48 miss% 0.37571912624454284
plot_id,batch_id 0 49 miss% 0.33835334709041526
plot_id,batch_id 0 50 miss% 0.44910002853976594
plot_id,batch_id 0 51 miss% 0.45108952592338025
plot_id,batch_id 0 52 miss% 0.46602810638152586
plot_id,batch_id 0 53 miss% 0.3382322781068784
plot_id,batch_id 0 54 miss% 0.3802162554925579
plot_id,batch_id 0 55 miss% 0.3501580423780138
plot_id,batch_id 0 56 miss% 0.4952395729850798
plot_id,batch_id 0 57 miss% 0.45920761533279586
plot_id,batch_id 0 58 miss% 0.4670528192344193
plot_id,batch_id 0 59 miss% 0.4428462081141726
plot_id,batch_id 0 60 miss% 0.22594620456390005
plot_id,batch_id 0 61 miss% 0.2602233670957983
plot_id,batch_id 0 62 miss% 0.3139975049320224
plot_id,batch_id 0 63 miss% 0.3805139242561128
plot_id,batch_id 0 64 miss% 0.3651391300334008
plot_id,batch_id 0 65 miss% 0.30304477697629995
plot_id,batch_id 0 66 miss% 0.4020673607844314
plot_id,batch_id 0 67 miss% 0.28494424291819814
plot_id,batch_id 0 68 miss% 0.408314286903697
plot_id,batch_id 0 69 miss% 0.43254324132096184
plot_id,batch_id 0 70 miss% 0.30487292810187927
plot_id,batch_id 0 71 miss% 0.36682855948251103
plot_id,batch_id 0 72 miss% 0.4507897980775443
plot_id,batch_id 0 73 miss% 0.3578792020484724
plot_id,batch_id 0 74 miss% 0.42988686174616025
plot_id,batch_id 0 75 miss% 0.331695870342348
plot_id,batch_id 0 76 miss% 0.34468088749717846
plot_id,batch_id 0 77 miss% 0.31290701465548215
plot_id,batch_id 0 78 miss% 0.3465275476945803
plot_id,batch_id 0 79 miss% 0.4147492712393435
plot_id,batch_id 0 80 miss% 0.2911744313184191
plot_id,batch_id 0 81 miss% 0.3984473063772401
plot_id,batch_id 0 82 miss% 0.3706546273729685
plot_id,batch_id 0 83 miss% 0.40758955933820623
plot_id,batch_id 0 84 miss% 0.3952518473720086
plot_id,batch_id 0 85 miss% 0.2527286371560653
plot_id,batch_id 0 86 miss% 0.34506043924831414
plot_id,batch_id 0 87 miss% 0.3944328173601893
plot_id,batch_id 0 88 miss% 0.3842689097302762
plot_id,batch_id 0 89 miss% 0.4071756650604819
plot_id,batch_id 0 90 miss% 0.28653758916024075
plot_id,batch_id 0 91 miss% 0.34344970998093954
plot_id,batch_id 0 92 miss% 0.35573899335319376
plot_id,batch_id 0 93 miss% 0.2982137205646979
plot_id,batch_id 0 94 miss% 0.41705005071837303
plot_id,batch_id 0 95 miss% 0.23367798804630496
plot_id,batch_id 0 96 miss% 0.2648802872164306
plot_id,batch_id 0 97 miss% 0.4236359880637156
plot_id,batch_id 0 98 miss% 0.37331783534654983
plot_id,batch_id 0 99 miss% 0.31747432388248326
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.30395014 0.41121938 0.45921061 0.29444792 0.33186358 0.3468656
 0.40408494 0.44271449 0.52924269 0.37030981 0.27249304 0.32088607
 0.50970475 0.39013853 0.36858719 0.29358083 0.46797186 0.53141698
 0.38853022 0.38822861 0.38397317 0.49504974 0.40888178 0.37737477
 0.37494789 0.37842006 0.41179587 0.34548352 0.52529995 0.36102005
 0.3004389  0.50923262 0.48815728 0.48375677 0.3229366  0.34077868
 0.54377613 0.40531566 0.38256167 0.38504261 0.39651364 0.30883675
 0.27283168 0.32476812 0.33874899 0.30544925 0.43674625 0.39282051
 0.419856   0.4725889  0.66059519 0.47356159 0.427032   0.27235081
 0.29000524 0.54718388 0.53408159 0.54812257 0.61939354 0.42994754
 0.26455466 0.28696635 0.47609725 0.34743486 0.39410677 0.40488819
 0.43519718 0.27597453 0.4246127  0.41121381 0.30487293 0.36682856
 0.4507898  0.3578792  0.42988686 0.33169587 0.34468089 0.31290701
 0.34652755 0.41474927 0.29117443 0.39844731 0.37065463 0.40758956
 0.39525185 0.25272864 0.34506044 0.39443282 0.38426891 0.40717567
 0.28653759 0.34344971 0.35573899 0.29821372 0.41705005 0.23367799
 0.26488029 0.42363599 0.37331784 0.31747432]
for model  54 the mean error 0.38863749978273154
all id 54 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 21 out win 4 err 0.38863749978273154 time 6987.995615720749
Launcher: Job 55 completed in 7233 seconds.
Launcher: Task 100 done. Exiting.
plot_id,batch_id 0 70 miss% 0.25011616062860936
plot_id,batch_id 0 71 miss% 0.3726551086602368
plot_id,batch_id 0 72 miss% 0.4263535320530087
plot_id,batch_id 0 73 miss% 0.39316490346745914
plot_id,batch_id 0 74 miss% 0.4633365822236499
plot_id,batch_id 0 75 miss% 0.2103311112829752
plot_id,batch_id 0 76 miss% 0.2721343136297468
plot_id,batch_id 0 77 miss% 0.2644971413140254
plot_id,batch_id 0 78 miss% 0.3463423661434908
plot_id,batch_id 0 79 miss% 0.3341052523911589
plot_id,batch_id 0 80 miss% 0.2701312174326215
plot_id,batch_id 0 81 miss% 0.4453046225166249
plot_id,batch_id 0 82 miss% 0.3725353157431795
plot_id,batch_id 0 83 miss% 0.38348702209436014
plot_id,batch_id 0 84 miss% 0.36541108701143665
plot_id,batch_id 0 85 miss% 0.22301188425942806
plot_id,batch_id 0 86 miss% 0.3681836737117554
plot_id,batch_id 0 87 miss% 0.3450092984532267
plot_id,batch_id 0 88 miss% 0.39337945106711336
plot_id,batch_id 0 89 miss% 0.3684239634258758
plot_id,batch_id 0 90 miss% 0.21848880309623214
plot_id,batch_id 0 91 miss% 0.27598114393103274
plot_id,batch_id 0 92 miss% 0.3054356544250355
plot_id,batch_id 0 93 miss% 0.30840733254953934
plot_id,batch_id 0 94 miss% 0.5578187803705255
plot_id,batch_id 0 95 miss% 0.2592133430043135
plot_id,batch_id 0 96 miss% 0.2955383677710125
plot_id,batch_id 0 97 miss% 0.41030286874514055
plot_id,batch_id 0 98 miss% 0.423189397309169
plot_id,batch_id 0 99 miss% 0.364990532673775
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.28527891 0.37073383 0.45508932 0.36254133 0.30268827 0.3571941
 0.46066966 0.496544   0.63468829 0.46815909 0.22892074 0.35979739
 0.36082975 0.37562107 0.49218997 0.25296988 0.47179502 0.48547751
 0.48091048 0.44704543 0.32775209 0.34857166 0.36509646 0.39566519
 0.43330978 0.31283531 0.41015457 0.38170733 0.36191292 0.40262003
 0.32098614 0.43605272 0.52150406 0.41544854 0.46862449 0.31513608
 0.49994245 0.36928038 0.36239823 0.50964313 0.36108382 0.38441265
 0.32391337 0.29538103 0.25206345 0.37557434 0.40536833 0.4557993
 0.37571913 0.33835335 0.44910003 0.45108953 0.46602811 0.33823228
 0.38021626 0.35015804 0.49523957 0.45920762 0.46705282 0.44284621
 0.2259462  0.26022337 0.3139975  0.38051392 0.36513913 0.30304478
 0.40206736 0.28494424 0.40831429 0.43254324 0.25011616 0.37265511
 0.42635353 0.3931649  0.46333658 0.21033111 0.27213431 0.26449714
 0.34634237 0.33410525 0.27013122 0.44530462 0.37253532 0.38348702
 0.36541109 0.22301188 0.36818367 0.3450093  0.39337945 0.36842396
 0.2184888  0.27598114 0.30543565 0.30840733 0.55781878 0.25921334
 0.29553837 0.41030287 0.4231894  0.36499053]
for model  27 the mean error 0.3763863911242387
all id 27 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 21 out win 4 err 0.3763863911242387 time 7013.648733377457
Launcher: Job 28 completed in 7263 seconds.
Launcher: Task 94 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  41745
Epoch:0, Train loss:0.781020, valid loss:0.763342
Epoch:1, Train loss:0.048802, valid loss:0.006707
Epoch:2, Train loss:0.013615, valid loss:0.004144
Epoch:3, Train loss:0.008423, valid loss:0.002981
Epoch:4, Train loss:0.006077, valid loss:0.002893
Epoch:5, Train loss:0.005517, valid loss:0.002801
Epoch:6, Train loss:0.005002, valid loss:0.002669
Epoch:7, Train loss:0.004769, valid loss:0.002725
Epoch:8, Train loss:0.004268, valid loss:0.001968
Epoch:9, Train loss:0.004229, valid loss:0.002083
Epoch:10, Train loss:0.003907, valid loss:0.001942
Epoch:11, Train loss:0.003025, valid loss:0.001463
Epoch:12, Train loss:0.002847, valid loss:0.001430
Epoch:13, Train loss:0.002763, valid loss:0.001572
Epoch:14, Train loss:0.002759, valid loss:0.001499
Epoch:15, Train loss:0.002677, valid loss:0.001436
Epoch:16, Train loss:0.002603, valid loss:0.001547
Epoch:17, Train loss:0.002472, valid loss:0.001360
Epoch:18, Train loss:0.002408, valid loss:0.001261
Epoch:19, Train loss:0.002425, valid loss:0.001244
Epoch:20, Train loss:0.002346, valid loss:0.001256
Epoch:21, Train loss:0.001879, valid loss:0.001151
Epoch:22, Train loss:0.001805, valid loss:0.001085
Epoch:23, Train loss:0.001776, valid loss:0.001147
Epoch:24, Train loss:0.001760, valid loss:0.001052
Epoch:25, Train loss:0.001715, valid loss:0.001131
Epoch:26, Train loss:0.001685, valid loss:0.001141
Epoch:27, Train loss:0.001695, valid loss:0.001053
Epoch:28, Train loss:0.001636, valid loss:0.001076
Epoch:29, Train loss:0.001608, valid loss:0.001146
Epoch:30, Train loss:0.001595, valid loss:0.001001
Epoch:31, Train loss:0.001368, valid loss:0.000989
Epoch:32, Train loss:0.001350, valid loss:0.000975
Epoch:33, Train loss:0.001344, valid loss:0.000958
Epoch:34, Train loss:0.001325, valid loss:0.001041
Epoch:35, Train loss:0.001310, valid loss:0.000950
Epoch:36, Train loss:0.001291, valid loss:0.001018
Epoch:37, Train loss:0.001286, valid loss:0.000920
Epoch:38, Train loss:0.001264, valid loss:0.000935
Epoch:39, Train loss:0.001296, valid loss:0.000985
Epoch:40, Train loss:0.001266, valid loss:0.000975
Epoch:41, Train loss:0.001140, valid loss:0.000893
Epoch:42, Train loss:0.001118, valid loss:0.000907
Epoch:43, Train loss:0.001122, valid loss:0.000908
Epoch:44, Train loss:0.001108, valid loss:0.000893
Epoch:45, Train loss:0.001110, valid loss:0.000889
Epoch:46, Train loss:0.001117, valid loss:0.000954
Epoch:47, Train loss:0.001106, valid loss:0.000887
Epoch:48, Train loss:0.001092, valid loss:0.000933
Epoch:49, Train loss:0.001076, valid loss:0.000903
Epoch:50, Train loss:0.001090, valid loss:0.000898
Epoch:51, Train loss:0.001026, valid loss:0.000865
Epoch:52, Train loss:0.001016, valid loss:0.000863
Epoch:53, Train loss:0.001012, valid loss:0.000864
Epoch:54, Train loss:0.001009, valid loss:0.000863
Epoch:55, Train loss:0.001007, valid loss:0.000863
Epoch:56, Train loss:0.001005, valid loss:0.000866
Epoch:57, Train loss:0.001004, valid loss:0.000872
Epoch:58, Train loss:0.001003, valid loss:0.000867
Epoch:59, Train loss:0.001002, valid loss:0.000863
Epoch:60, Train loss:0.001001, valid loss:0.000865
training time 7450.1221034526825
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.39857426528590345
plot_id,batch_id 0 1 miss% 0.48399986771104314
plot_id,batch_id 0 2 miss% 0.5034410155822093
plot_id,batch_id 0 3 miss% 0.4268970439278008
plot_id,batch_id 0 4 miss% 0.39975341140823284
plot_id,batch_id 0 5 miss% 0.39204019759516173
plot_id,batch_id 0 6 miss% 0.4331857481120079
plot_id,batch_id 0 7 miss% 0.6257118560491629
plot_id,batch_id 0 8 miss% 0.47458170585386744
plot_id,batch_id 0 9 miss% 0.5288776661190631
plot_id,batch_id 0 10 miss% 0.46769214384995705
plot_id,batch_id 0 11 miss% 0.4203832473691386
plot_id,batch_id 0 12 miss% 0.4531314078364029
plot_id,batch_id 0 13 miss% 0.4012145575762087
plot_id,batch_id 0 14 miss% 0.49227197067860295
plot_id,batch_id 0 15 miss% 0.5354602629778641
plot_id,batch_id 0 16 miss% 0.5175057006302288
plot_id,batch_id 0 17 miss% 0.46567245223939396
plot_id,batch_id 0 18 miss% 0.5270462240799692
plot_id,batch_id 0 19 miss% 0.4239813843629581
plot_id,batch_id 0 20 miss% 0.369806495847562
plot_id,batch_id 0 21 miss% 0.42048928090853016
plot_id,batch_id 0 22 miss% 0.4400377613689687
plot_id,batch_id 0 23 miss% 0.4983666181022438
plot_id,batch_id 0 24 miss% 0.38229556577858065
plot_id,batch_id 0 25 miss% 0.39964572406918425
plot_id,batch_id 0 26 miss% 0.45857808358259605
plot_id,batch_id 0 27 miss% 0.5191904358864068
plot_id,batch_id 0 28 miss% 0.37777332870149016
plot_id,batch_id 0 29 miss% 0.48151575974358884
plot_id,batch_id 0 30 miss% 0.4053559102927227
plot_id,batch_id 0 31 miss% 0.5131624912447904
plot_id,batch_id 0 32 miss% 0.42683617423094317
plot_id,batch_id 0 33 miss% 0.44306634203002865
plot_id,batch_id 0 34 miss% 0.4340204899096444
plot_id,batch_id 0 35 miss% 0.4158012853807436
plot_id,batch_id 0 36 miss% 0.5545389492832035
plot_id,batch_id 0 37 miss% 0.46886365656569234
plot_id,batch_id 0 38 miss% 0.45203095466866894
plot_id,batch_id 0 39 miss% 0.4408653417324478
plot_id,batch_id 0 40 miss% 0.4527428312413901
plot_id,batch_id 0 41 miss% 0.5059099612395923
plot_id,batch_id 0 42 miss% 0.3697902229957
plot_id,batch_id 0 43 miss% 0.40645692010770373
plot_id,batch_id 0 44 miss% 0.2897566321161855
plot_id,batch_id 0 45 miss% 0.49998154856556526
plot_id,batch_id 0 46 miss% 0.46759669796670345
plot_id,batch_id 0 47 miss% 0.45681453715300335
plot_id,batch_id 0 48 miss% 0.4073387787122113
plot_id,batch_id 0 49 miss% 0.29996854671222745
plot_id,batch_id 0 50 miss% 0.6353811552854786
plot_id,batch_id 0 51 miss% 0.5208564638763761
plot_id,batch_id 0 52 miss% 0.486081391929469
plot_id,batch_id 0 53 miss% 0.3979512026560167
plot_id,batch_id 0 54 miss% 0.27339935108860336
plot_id,batch_id 0 55 miss% 0.6521241552210717
plot_id,batch_id 0 56 miss% 0.5571297843040239
plot_id,batch_id 0 57 miss% 0.4715447357797928
plot_id,batch_id 0 58 miss% 0.37818357692333604
plot_id,batch_id 0 59 miss% 0.4355123968897919
plot_id,batch_id 0 60 miss% 0.32606211223211273
plot_id,batch_id 0 61 miss% 0.3951450888161975
plot_id,batch_id 0 62 miss% 0.5196158554088293
plot_id,batch_id 0 63 miss% 0.42418345741890556
plot_id,batch_id 0 64 miss% 0.48636026941675425
plot_id,batch_id 0 65 miss% 0.3341169013669608
plot_id,batch_id 0 66 miss% 0.505588593071345
plot_id,batch_id 0 67 miss% 0.38029627759068607
plot_id,batch_id 0 68 miss% 0.47262734247766164
plot_id,batch_id 0 69 miss% 0.4938196592871574
plot_id,batch_id 0 70 miss% 0.3175557884902245
plot_id,batch_id 0 71 miss% 0.5028356057403793
plot_id,batch_id 0 72 miss% 0.47342341246867364
plot_id,batch_id 0 73 miss% 0.39976949829791975
plot_id,batch_id 0 74 miss% 0.45499572082249634
plot_id,batch_id 0 75 miss% 0.46910170204335816
plot_id,batch_id 0 76 miss% 0.45253937857046667
plot_id,batch_id 0 77 miss% 0.4102847293084334
plot_id,batch_id 0 78 miss% 0.45570003858091995
plot_id,batch_id 0 79 miss% 0.4173804109456516
plot_id,batch_id 0 80 miss% 0.44160382071921983
plot_id,batch_id 0 81 miss% 0.5677967451135878
plot_id,batch_id 0 82 miss% 0.39793379968515075
plot_id,batch_id 0 83 miss% 0.40652059622932263
plot_id,batch_id 0 84 miss% 0.43787955506172593
plot_id,batch_id 0 85 miss% 0.35750421056212595
plot_id,batch_id 0 86 miss% 0.4214021940033884
plot_id,batch_id 0 87 miss% 0.3778195333763591
plot_id,batch_id 0 88 miss% 0.5263802210480867
plot_id,batch_id 0 89 miss% 0.4708167641945146
plot_id,batch_id 0 90 miss% 0.2891714946109695
plot_id,batch_id 0 91 miss% 0.4097858157592108
plot_id,batch_id 0 92 miss% 0.3714835748414308
plot_id,batch_id 0 93 miss% 0.4012729288950429
plot_id,batch_id 0 94 miss% 0.498066281508623
plot_id,batch_id 0 95 miss% 0.38376547160368363
plot_id,batch_id 0 96 miss% 0.3618382447246306
plot_id,batch_id 0 97 miss% 0.47553308043155484
plot_id,batch_id 0 98 miss% 0.4733881013551379
plot_id,batch_id 0 99 miss% 0.4672669872269615
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.39857427 0.48399987 0.50344102 0.42689704 0.39975341 0.3920402
 0.43318575 0.62571186 0.47458171 0.52887767 0.46769214 0.42038325
 0.45313141 0.40121456 0.49227197 0.53546026 0.5175057  0.46567245
 0.52704622 0.42398138 0.3698065  0.42048928 0.44003776 0.49836662
 0.38229557 0.39964572 0.45857808 0.51919044 0.37777333 0.48151576
 0.40535591 0.51316249 0.42683617 0.44306634 0.43402049 0.41580129
 0.55453895 0.46886366 0.45203095 0.44086534 0.45274283 0.50590996
 0.36979022 0.40645692 0.28975663 0.49998155 0.4675967  0.45681454
 0.40733878 0.29996855 0.63538116 0.52085646 0.48608139 0.3979512
 0.27339935 0.65212416 0.55712978 0.47154474 0.37818358 0.4355124
 0.32606211 0.39514509 0.51961586 0.42418346 0.48636027 0.3341169
 0.50558859 0.38029628 0.47262734 0.49381966 0.31755579 0.50283561
 0.47342341 0.3997695  0.45499572 0.4691017  0.45253938 0.41028473
 0.45570004 0.41738041 0.44160382 0.56779675 0.3979338  0.4065206
 0.43787956 0.35750421 0.42140219 0.37781953 0.52638022 0.47081676
 0.28917149 0.40978582 0.37148357 0.40127293 0.49806628 0.38376547
 0.36183824 0.47553308 0.4733881  0.46726699]
for model  55 the mean error 0.44466814932643317
all id 55 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 21 out win 5 err 0.44466814932643317 time 7450.1221034526825
Launcher: Job 56 completed in 7688 seconds.
Launcher: Task 111 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  41745
Epoch:0, Train loss:0.645765, valid loss:0.640951
Epoch:1, Train loss:0.029492, valid loss:0.003465
Epoch:2, Train loss:0.005671, valid loss:0.002323
Epoch:3, Train loss:0.004365, valid loss:0.001900
Epoch:4, Train loss:0.003608, valid loss:0.001633
Epoch:5, Train loss:0.003193, valid loss:0.001583
Epoch:6, Train loss:0.002894, valid loss:0.001269
Epoch:7, Train loss:0.002654, valid loss:0.001511
Epoch:8, Train loss:0.002484, valid loss:0.001262
Epoch:9, Train loss:0.002439, valid loss:0.001269
Epoch:10, Train loss:0.002251, valid loss:0.001398
Epoch:11, Train loss:0.001729, valid loss:0.000994
Epoch:12, Train loss:0.001704, valid loss:0.000955
Epoch:13, Train loss:0.001654, valid loss:0.000988
Epoch:14, Train loss:0.001627, valid loss:0.000895
Epoch:15, Train loss:0.001557, valid loss:0.000949
Epoch:16, Train loss:0.001568, valid loss:0.000888
Epoch:17, Train loss:0.001518, valid loss:0.000934
Epoch:18, Train loss:0.001468, valid loss:0.000983
Epoch:19, Train loss:0.001447, valid loss:0.000795
Epoch:20, Train loss:0.001403, valid loss:0.000777
Epoch:21, Train loss:0.001160, valid loss:0.000748
Epoch:22, Train loss:0.001155, valid loss:0.000687
Epoch:23, Train loss:0.001126, valid loss:0.000696
Epoch:24, Train loss:0.001105, valid loss:0.000744
Epoch:25, Train loss:0.001094, valid loss:0.000776
Epoch:26, Train loss:0.001075, valid loss:0.000680
Epoch:27, Train loss:0.001095, valid loss:0.000717
Epoch:28, Train loss:0.001051, valid loss:0.000689
Epoch:29, Train loss:0.001049, valid loss:0.000670
Epoch:30, Train loss:0.001040, valid loss:0.000674
Epoch:31, Train loss:0.000910, valid loss:0.000660
Epoch:32, Train loss:0.000900, valid loss:0.000640
Epoch:33, Train loss:0.000893, valid loss:0.000642
Epoch:34, Train loss:0.000883, valid loss:0.000652
Epoch:35, Train loss:0.000888, valid loss:0.000652
Epoch:36, Train loss:0.000872, valid loss:0.000643
Epoch:37, Train loss:0.000864, valid loss:0.000647
Epoch:38, Train loss:0.000869, valid loss:0.000605
Epoch:39, Train loss:0.000849, valid loss:0.000638
Epoch:40, Train loss:0.000859, valid loss:0.000603
Epoch:41, Train loss:0.000791, valid loss:0.000587
Epoch:42, Train loss:0.000782, valid loss:0.000595
Epoch:43, Train loss:0.000779, valid loss:0.000611
Epoch:44, Train loss:0.000779, valid loss:0.000596
Epoch:45, Train loss:0.000775, valid loss:0.000613
Epoch:46, Train loss:0.000769, valid loss:0.000590
Epoch:47, Train loss:0.000773, valid loss:0.000603
Epoch:48, Train loss:0.000762, valid loss:0.000585
Epoch:49, Train loss:0.000755, valid loss:0.000580
Epoch:50, Train loss:0.000761, valid loss:0.000586
Epoch:51, Train loss:0.000720, valid loss:0.000570
Epoch:52, Train loss:0.000715, valid loss:0.000567
Epoch:53, Train loss:0.000713, valid loss:0.000567
Epoch:54, Train loss:0.000711, valid loss:0.000569
Epoch:55, Train loss:0.000711, valid loss:0.000572
Epoch:56, Train loss:0.000710, valid loss:0.000569
Epoch:57, Train loss:0.000709, valid loss:0.000570
Epoch:58, Train loss:0.000708, valid loss:0.000570
Epoch:59, Train loss:0.000709, valid loss:0.000567
Epoch:60, Train loss:0.000708, valid loss:0.000568
training time 7604.581967353821
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.2630469134337954
plot_id,batch_id 0 1 miss% 0.38726179301680214
plot_id,batch_id 0 2 miss% 0.3645895051215969
plot_id,batch_id 0 3 miss% 0.31726699434474376
plot_id,batch_id 0 4 miss% 0.30681354152061985
plot_id,batch_id 0 5 miss% 0.2977449432160095
plot_id,batch_id 0 6 miss% 0.2867170424848315
plot_id,batch_id 0 7 miss% 0.48089007418864804
plot_id,batch_id 0 8 miss% 0.5085525720707726
plot_id,batch_id 0 9 miss% 0.485075791439231
plot_id,batch_id 0 10 miss% 0.25003234588912615
plot_id,batch_id 0 11 miss% 0.30296864821193215
plot_id,batch_id 0 12 miss% 0.32286404890769965
plot_id,batch_id 0 13 miss% 0.3195567905360591
plot_id,batch_id 0 14 miss% 0.41704679570173075
plot_id,batch_id 0 15 miss% 0.25472762609777094
plot_id,batch_id 0 16 miss% 0.43759834628256006
plot_id,batch_id 0 17 miss% 0.36881229643160973
plot_id,batch_id 0 18 miss% 0.41485185775847977
plot_id,batch_id 0 19 miss% 0.41199050605939924
plot_id,batch_id 0 20 miss% 0.34000125560646627
plot_id,batch_id 0 21 miss% 0.31027741651907137
plot_id,batch_id 0 22 miss% 0.41355148821599086
plot_id,batch_id 0 23 miss% 0.3959830806858264
plot_id,batch_id 0 24 miss% 0.39834799878208615
plot_id,batch_id 0 25 miss% 0.3295071444538804
plot_id,batch_id 0 26 miss% 0.3883564762691194
plot_id,batch_id 0 27 miss% 0.3992874058463244
plot_id,batch_id 0 28 miss% 0.4040222788102957
plot_id,batch_id 0 29 miss% 0.35456814714043555
plot_id,batch_id 0 30 miss% 0.288758955159897
plot_id,batch_id 0 31 miss% 0.4009660596851324
plot_id,batch_id 0 32 miss% 0.4428225743604181
plot_id,batch_id 0 33 miss% 0.4241984201729846
plot_id,batch_id 0 34 miss% 0.38665112977350496
plot_id,batch_id 0 35 miss% 0.2510284459865399
plot_id,batch_id 0 36 miss% 0.4929274778881463
plot_id,batch_id 0 37 miss% 0.38212037974459145
plot_id,batch_id 0 38 miss% 0.3159698986221892
plot_id,batch_id 0 39 miss% 0.37113241450544565
plot_id,batch_id 0 40 miss% 0.2863433682595433
plot_id,batch_id 0 41 miss% 0.42642556981134466
plot_id,batch_id 0 42 miss% 0.28591436199698234
plot_id,batch_id 0 43 miss% 0.24653394232993855
plot_id,batch_id 0 44 miss% 0.21190409815317945
plot_id,batch_id 0 45 miss% 0.3152514951148667
plot_id,batch_id 0 46 miss% 0.3409554648658107
plot_id,batch_id 0 47 miss% 0.39726473592369405
plot_id,batch_id 0 48 miss% 0.4128390158645637
plot_id,batch_id 0 49 miss% 0.2694271419313874
plot_id,batch_id 0 50 miss% 0.5032800839943441
plot_id,batch_id 0 51 miss% 0.44412565523616887
plot_id,batch_id 0 52 miss% 0.42696709435814956
plot_id,batch_id 0 53 miss% 0.341510761310907
plot_id,batch_id 0 54 miss% 0.33647992346107725
plot_id,batch_id 0 55 miss% 0.42700932812842
plot_id,batch_id 0 56 miss% 0.5306677695565128
plot_id,batch_id 0 57 miss% 0.4681340184958556
plot_id,batch_id 0 58 miss% 0.3997913516301296
plot_id,batch_id 0 59 miss% 0.45850676280968394
plot_id,batch_id 0 60 miss% 0.2066492175319422
plot_id,batch_id 0 61 miss% 0.2286715696165046
plot_id,batch_id 0 62 miss% 0.28625020256524775
plot_id,batch_id 0 63 miss% 0.28787861688685124
plot_id,batch_id 0 64 miss% 0.36060982810297354
plot_id,batch_id 0 65 miss% 0.4104680597313972
plot_id,batch_id 0 66 miss% 0.35807573809688753
plot_id,batch_id 0 67 miss% 0.2463145634199976
plot_id,batch_id 0 68 miss% 0.4036341136947135
plot_id,batch_id 0 69 miss% 0.3620934976769531
plot_id,batch_id 0 70 miss% 0.2316312987185452
plot_id,batch_id 0 71 miss% 0.28014585953073207
plot_id,batch_id 0 72 miss% 0.39986308025882766
plot_id,batch_id 0 73 miss% 0.2715467436132564
plot_id,batch_id 0 74 miss% 0.30242897229272847
plot_id,batch_id 0 75 miss% 0.21815650663975722
plot_id,batch_id 0 76 miss% 0.3097735107736034
plot_id,batch_id 0 77 miss% 0.2775744910239217
plot_id,batch_id 0 78 miss% 0.27530708128805453
plot_id,batch_id 0 79 miss% 0.28743077993364263
plot_id,batch_id 0 80 miss% 0.29008555614580406
plot_id,batch_id 0 81 miss% 0.46387433623169866
plot_id,batch_id 0 82 miss% 0.3185952122306369
plot_id,batch_id 0 83 miss% 0.3371684703555884
plot_id,batch_id 0 84 miss% 0.36233814361624295
plot_id,batch_id 0 85 miss% 0.2046029308064778
plot_id,batch_id 0 86 miss% 0.3009973057095347
plot_id,batch_id 0 87 miss% 0.31835665018884063
plot_id,batch_id 0 88 miss% 0.3298150691138374
plot_id,batch_id 0 89 miss% 0.3250084455147145
plot_id,batch_id 0 90 miss% 0.16872036050244432
plot_id,batch_id 0 91 miss% 0.2456105095682771
plot_id,batch_id 0 92 miss% 0.3266823576390163
plot_id,batch_id 0 93 miss% 0.2617504232814458
plot_id,batch_id 0 94 miss% 0.42032098765588444
plot_id,batch_id 0 95 miss% 0.19895631704162817
plot_id,batch_id 0 96 miss% 0.2871899428028492
plot_id,batch_id 0 97 miss% 0.3621676760233011
plot_id,batch_id 0 98 miss% 0.2725800975971594
plot_id,batch_id 0 99 miss% 0.3150166271554646
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.26304691 0.38726179 0.36458951 0.31726699 0.30681354 0.29774494
 0.28671704 0.48089007 0.50855257 0.48507579 0.25003235 0.30296865
 0.32286405 0.31955679 0.4170468  0.25472763 0.43759835 0.3688123
 0.41485186 0.41199051 0.34000126 0.31027742 0.41355149 0.39598308
 0.398348   0.32950714 0.38835648 0.39928741 0.40402228 0.35456815
 0.28875896 0.40096606 0.44282257 0.42419842 0.38665113 0.25102845
 0.49292748 0.38212038 0.3159699  0.37113241 0.28634337 0.42642557
 0.28591436 0.24653394 0.2119041  0.3152515  0.34095546 0.39726474
 0.41283902 0.26942714 0.50328008 0.44412566 0.42696709 0.34151076
 0.33647992 0.42700933 0.53066777 0.46813402 0.39979135 0.45850676
 0.20664922 0.22867157 0.2862502  0.28787862 0.36060983 0.41046806
 0.35807574 0.24631456 0.40363411 0.3620935  0.2316313  0.28014586
 0.39986308 0.27154674 0.30242897 0.21815651 0.30977351 0.27757449
 0.27530708 0.28743078 0.29008556 0.46387434 0.31859521 0.33716847
 0.36233814 0.20460293 0.30099731 0.31835665 0.32981507 0.32500845
 0.16872036 0.24561051 0.32668236 0.26175042 0.42032099 0.19895632
 0.28718994 0.36216768 0.2725801  0.31501663]
for model  108 the mean error 0.3433255997475171
all id 108 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 25 out win 4 err 0.3433255997475171 time 7604.581967353821
Launcher: Job 109 completed in 7853 seconds.
Launcher: Task 239 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  41745
Epoch:0, Train loss:0.753763, valid loss:0.732127
Epoch:1, Train loss:0.054855, valid loss:0.006110
Epoch:2, Train loss:0.012838, valid loss:0.003989
Epoch:3, Train loss:0.008183, valid loss:0.003189
Epoch:4, Train loss:0.006722, valid loss:0.002756
Epoch:5, Train loss:0.005941, valid loss:0.002494
Epoch:6, Train loss:0.005394, valid loss:0.002307
Epoch:7, Train loss:0.004909, valid loss:0.002445
Epoch:8, Train loss:0.004631, valid loss:0.002381
Epoch:9, Train loss:0.004440, valid loss:0.001860
Epoch:10, Train loss:0.004182, valid loss:0.001939
Epoch:11, Train loss:0.003339, valid loss:0.001607
Epoch:12, Train loss:0.003239, valid loss:0.001660
Epoch:13, Train loss:0.003157, valid loss:0.001506
Epoch:14, Train loss:0.003043, valid loss:0.001860
Epoch:15, Train loss:0.002946, valid loss:0.001488
Epoch:16, Train loss:0.002821, valid loss:0.001663
Epoch:17, Train loss:0.002710, valid loss:0.001316
Epoch:18, Train loss:0.002716, valid loss:0.001467
Epoch:19, Train loss:0.002724, valid loss:0.001460
Epoch:20, Train loss:0.002561, valid loss:0.001269
Epoch:21, Train loss:0.002134, valid loss:0.001325
Epoch:22, Train loss:0.002096, valid loss:0.001290
Epoch:23, Train loss:0.002094, valid loss:0.001220
Epoch:24, Train loss:0.002055, valid loss:0.001281
Epoch:25, Train loss:0.001987, valid loss:0.001209
Epoch:26, Train loss:0.001977, valid loss:0.001220
Epoch:27, Train loss:0.001939, valid loss:0.001235
Epoch:28, Train loss:0.001977, valid loss:0.001339
Epoch:29, Train loss:0.001864, valid loss:0.001278
Epoch:30, Train loss:0.001848, valid loss:0.001141
Epoch:31, Train loss:0.001638, valid loss:0.001142
Epoch:32, Train loss:0.001617, valid loss:0.001094
Epoch:33, Train loss:0.001607, valid loss:0.001098
Epoch:34, Train loss:0.001582, valid loss:0.001132
Epoch:35, Train loss:0.001592, valid loss:0.001138
Epoch:36, Train loss:0.001569, valid loss:0.001091
Epoch:37, Train loss:0.001551, valid loss:0.001073
Epoch:38, Train loss:0.001535, valid loss:0.001081
Epoch:39, Train loss:0.001510, valid loss:0.001057
Epoch:40, Train loss:0.001511, valid loss:0.001119
Epoch:41, Train loss:0.001404, valid loss:0.001044
Epoch:42, Train loss:0.001397, valid loss:0.001038
Epoch:43, Train loss:0.001382, valid loss:0.001042
Epoch:44, Train loss:0.001369, valid loss:0.001045
Epoch:45, Train loss:0.001364, valid loss:0.001033
Epoch:46, Train loss:0.001371, valid loss:0.001086
Epoch:47, Train loss:0.001343, valid loss:0.001035
Epoch:48, Train loss:0.001349, valid loss:0.001011
Epoch:49, Train loss:0.001355, valid loss:0.001006
Epoch:50, Train loss:0.001335, valid loss:0.001035
Epoch:51, Train loss:0.001258, valid loss:0.000998
Epoch:52, Train loss:0.001250, valid loss:0.000990
Epoch:53, Train loss:0.001248, valid loss:0.000998
Epoch:54, Train loss:0.001246, valid loss:0.000989
Epoch:55, Train loss:0.001245, valid loss:0.000992
Epoch:56, Train loss:0.001243, valid loss:0.000994
Epoch:57, Train loss:0.001243, valid loss:0.000990
Epoch:58, Train loss:0.001242, valid loss:0.000988
Epoch:59, Train loss:0.001242, valid loss:0.000985
Epoch:60, Train loss:0.001241, valid loss:0.000987
training time 7663.153849363327
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.4437981367852706
plot_id,batch_id 0 1 miss% 0.5534304621361262
plot_id,batch_id 0 2 miss% 0.6059796178545273
plot_id,batch_id 0 3 miss% 0.5359804926092017
plot_id,batch_id 0 4 miss% 0.48675200202972935
plot_id,batch_id 0 5 miss% 0.5168094666173844
plot_id,batch_id 0 6 miss% 0.4346689252907988
plot_id,batch_id 0 7 miss% 0.5836826814075144
plot_id,batch_id 0 8 miss% 0.6444025376737447
plot_id,batch_id 0 9 miss% 0.713074553431659
plot_id,batch_id 0 10 miss% 0.5389377647813202
plot_id,batch_id 0 11 miss% 0.45305568857504264
plot_id,batch_id 0 12 miss% 0.5043306502964062
plot_id,batch_id 0 13 miss% 0.4908080211366677
plot_id,batch_id 0 14 miss% 0.5521313278077815
plot_id,batch_id 0 15 miss% 0.5205745290988092
plot_id,batch_id 0 16 miss% 0.6531448947612235
plot_id,batch_id 0 17 miss% 0.5618240132888838
plot_id,batch_id 0 18 miss% 0.6079447967317745
plot_id,batch_id 0 19 miss% 0.5744442853498962
plot_id,batch_id 0 20 miss% 0.5357892544450894
plot_id,batch_id 0 21 miss% 0.6019377490952155
plot_id,batch_id 0 22 miss% 0.5794870674746826
plot_id,batch_id 0 23 miss% 0.5953383621771884
plot_id,batch_id 0 24 miss% 0.5308614726462971
plot_id,batch_id 0 25 miss% 0.5160241614175031
plot_id,batch_id 0 26 miss% 0.5818151164844619
plot_id,batch_id 0 27 miss% 0.5946658908996407
plot_id,batch_id 0 28 miss% 0.5692018598675962
plot_id,batch_id 0 29 miss% 0.5740750012925436
plot_id,batch_id 0 30 miss% 0.48395571712012914
plot_id,batch_id 0 31 miss% 0.5260000611240806
plot_id,batch_id 0 32 miss% 0.6714521612925971
plot_id,batch_id 0 33 miss% 0.6066436473960457
plot_id,batch_id 0 34 miss% 0.5427357776055982
plot_id,batch_id 0 35 miss% 0.57782236534485
plot_id,batch_id 0 36 miss% 0.62046427556764
plot_id,batch_id 0 37 miss% 0.573066700642085
plot_id,batch_id 0 38 miss% 0.5861215529062251
plot_id,batch_id 0 39 miss% 0.7829213518229565
plot_id,batch_id 0 40 miss% 0.5222384184923545
plot_id,batch_id 0 41 miss% 0.5574194235502746
plot_id,batch_id 0 42 miss% 0.4750718550390627
plot_id,batch_id 0 43 miss% 0.4596169026408463
plot_id,batch_id 0 44 miss% 0.4965107883596941
plot_id,batch_id 0 45 miss% 0.5207062907115977
plot_id,batch_id 0 46 miss% 0.5086717771602358
plot_id,batch_id 0 47 miss% 0.587221632884751
plot_id,batch_id 0 48 miss% 0.6481914629631256
plot_id,batch_id 0 49 miss% 0.42249284399499465
plot_id,batch_id 0 50 miss% 0.6771210956813831
plot_id,batch_id 0 51 miss% 0.6423315434732769
plot_id,batch_id 0 52 miss% 0.6722827990434506
plot_id,batch_id 0 53 miss% 0.46950135746727606
plot_id,batch_id 0 54 miss% 0.5357017377722895
plot_id,batch_id 0 55 miss% 0.6508958177462486
plot_id,batch_id 0 56 miss% 0.7012349376551922
plot_id,batch_id 0 57 miss% 0.6168817646817707
plot_id,batch_id 0 58 miss% 0.6586455904113521
plot_id,batch_id 0 59 miss% 0.5787794538892572
plot_id,batch_id 0 60 miss% 0.492060354172639
plot_id,batch_id 0 61 miss% 0.4026261896236586
plot_id,batch_id 0 62 miss% 0.49305522250589473
plot_id,batch_id 0 63 miss% 0.541696386606446
plot_id,batch_id 0 64 miss% 0.6248059499447274
plot_id,batch_id 0 65 miss% 0.477599612492961
plot_id,batch_id 0 66 miss% 0.537889244300473
plot_id,batch_id 0 67 miss% 0.4755873315101758
plot_id,batch_id 0 68 miss% 0.6010058701336061
plot_id,batch_id 0 69 miss% 0.6404896665454505
plot_id,batch_id 0 70 miss% 0.3581500663666219
plot_id,batch_id 0 71 miss% 0.49336527324451956
plot_id,batch_id 0 72 miss% 0.5289011443118121
plot_id,batch_id 0 73 miss% 0.5363191011640438
plot_id,batch_id 0 74 miss% 0.5289102619564235
plot_id,batch_id 0 75 miss% 0.558251341573778
plot_id,batch_id 0 76 miss% 0.5063146389020574
plot_id,batch_id 0 77 miss% 0.48514970198957047
plot_id,batch_id 0 78 miss% 0.4589059710668576
plot_id,batch_id 0 79 miss% 0.4260670741182519
plot_id,batch_id 0 80 miss% 0.44928969610160246
plot_id,batch_id 0 81 miss% 0.6586464137859079
plot_id,batch_id 0 82 miss% 0.5756738595554171
plot_id,batch_id 0 83 miss% 0.5579641815194232
plot_id,batch_id 0 84 miss% 0.5809550895992515
plot_id,batch_id 0 85 miss% 0.4332468530144458
plot_id,batch_id 0 86 miss% 0.45954158582754256
plot_id,batch_id 0 87 miss% 0.5189407776177399
plot_id,batch_id 0 88 miss% 0.5481156568200942
plot_id,batch_id 0 89 miss% 0.47588280436136604
plot_id,batch_id 0 90 miss% 0.3851061730480782
plot_id,batch_id 0 91 miss% 0.4925315903635802
plot_id,batch_id 0 92 miss% 0.46573753432723314
plot_id,batch_id 0 93 miss% 0.4976485231676534
plot_id,batch_id 0 94 miss% 0.6687656053464629
plot_id,batch_id 0 95 miss% 0.4469226227237823
plot_id,batch_id 0 96 miss% 0.4847130383284167
plot_id,batch_id 0 97 miss% 0.5764091011904946
plot_id,batch_id 0 98 miss% 0.6046448321579371
plot_id,batch_id 0 99 miss% 0.5680480448740233
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.44379814 0.55343046 0.60597962 0.53598049 0.486752   0.51680947
 0.43466893 0.58368268 0.64440254 0.71307455 0.53893776 0.45305569
 0.50433065 0.49080802 0.55213133 0.52057453 0.65314489 0.56182401
 0.6079448  0.57444429 0.53578925 0.60193775 0.57948707 0.59533836
 0.53086147 0.51602416 0.58181512 0.59466589 0.56920186 0.574075
 0.48395572 0.52600006 0.67145216 0.60664365 0.54273578 0.57782237
 0.62046428 0.5730667  0.58612155 0.78292135 0.52223842 0.55741942
 0.47507186 0.4596169  0.49651079 0.52070629 0.50867178 0.58722163
 0.64819146 0.42249284 0.6771211  0.64233154 0.6722828  0.46950136
 0.53570174 0.65089582 0.70123494 0.61688176 0.65864559 0.57877945
 0.49206035 0.40262619 0.49305522 0.54169639 0.62480595 0.47759961
 0.53788924 0.47558733 0.60100587 0.64048967 0.35815007 0.49336527
 0.52890114 0.5363191  0.52891026 0.55825134 0.50631464 0.4851497
 0.45890597 0.42606707 0.4492897  0.65864641 0.57567386 0.55796418
 0.58095509 0.43324685 0.45954159 0.51894078 0.54811566 0.4758828
 0.38510617 0.49253159 0.46573753 0.49764852 0.66876561 0.44692262
 0.48471304 0.5764091  0.60464483 0.56804804]
for model  29 the mean error 0.5464360627416505
all id 29 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 21 out win 6 err 0.5464360627416505 time 7663.153849363327
Launcher: Job 30 completed in 7893 seconds.
Launcher: Task 61 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  41745
Epoch:0, Train loss:0.781020, valid loss:0.763342
Epoch:1, Train loss:0.054225, valid loss:0.005955
Epoch:2, Train loss:0.015354, valid loss:0.004703
Epoch:3, Train loss:0.012879, valid loss:0.004243
Epoch:4, Train loss:0.008206, valid loss:0.003365
Epoch:5, Train loss:0.006234, valid loss:0.002571
Epoch:6, Train loss:0.005309, valid loss:0.002412
Epoch:7, Train loss:0.004934, valid loss:0.002601
Epoch:8, Train loss:0.004528, valid loss:0.002233
Epoch:9, Train loss:0.004305, valid loss:0.002001
Epoch:10, Train loss:0.004046, valid loss:0.002074
Epoch:11, Train loss:0.003228, valid loss:0.001467
Epoch:12, Train loss:0.003103, valid loss:0.001522
Epoch:13, Train loss:0.002981, valid loss:0.001571
Epoch:14, Train loss:0.002877, valid loss:0.001569
Epoch:15, Train loss:0.002808, valid loss:0.001301
Epoch:16, Train loss:0.002698, valid loss:0.001435
Epoch:17, Train loss:0.002651, valid loss:0.001290
Epoch:18, Train loss:0.002504, valid loss:0.001260
Epoch:19, Train loss:0.002511, valid loss:0.001258
Epoch:20, Train loss:0.002451, valid loss:0.001238
Epoch:21, Train loss:0.002034, valid loss:0.001044
Epoch:22, Train loss:0.001956, valid loss:0.001092
Epoch:23, Train loss:0.001917, valid loss:0.001070
Epoch:24, Train loss:0.001911, valid loss:0.001020
Epoch:25, Train loss:0.001875, valid loss:0.001064
Epoch:26, Train loss:0.001848, valid loss:0.001042
Epoch:27, Train loss:0.001812, valid loss:0.001020
Epoch:28, Train loss:0.001796, valid loss:0.001068
Epoch:29, Train loss:0.001778, valid loss:0.001047
Epoch:30, Train loss:0.001738, valid loss:0.000953
Epoch:31, Train loss:0.001546, valid loss:0.000913
Epoch:32, Train loss:0.001526, valid loss:0.000932
Epoch:33, Train loss:0.001497, valid loss:0.000965
Epoch:34, Train loss:0.001496, valid loss:0.000885
Epoch:35, Train loss:0.001472, valid loss:0.000904
Epoch:36, Train loss:0.001449, valid loss:0.000936
Epoch:37, Train loss:0.001466, valid loss:0.000856
Epoch:38, Train loss:0.001440, valid loss:0.000897
Epoch:39, Train loss:0.001427, valid loss:0.000976
Epoch:40, Train loss:0.001404, valid loss:0.000897
Epoch:41, Train loss:0.001313, valid loss:0.000818
Epoch:42, Train loss:0.001298, valid loss:0.000814
Epoch:43, Train loss:0.001296, valid loss:0.000852
Epoch:44, Train loss:0.001277, valid loss:0.000817
Epoch:45, Train loss:0.001285, valid loss:0.000839
Epoch:46, Train loss:0.001278, valid loss:0.000847
Epoch:47, Train loss:0.001270, valid loss:0.000835
Epoch:48, Train loss:0.001264, valid loss:0.000814
Epoch:49, Train loss:0.001252, valid loss:0.000808
Epoch:50, Train loss:0.001242, valid loss:0.000835
Epoch:51, Train loss:0.001186, valid loss:0.000796
Epoch:52, Train loss:0.001177, valid loss:0.000799
Epoch:53, Train loss:0.001172, valid loss:0.000802
Epoch:54, Train loss:0.001169, valid loss:0.000797
Epoch:55, Train loss:0.001168, valid loss:0.000797
Epoch:56, Train loss:0.001165, valid loss:0.000794
Epoch:57, Train loss:0.001165, valid loss:0.000805
Epoch:58, Train loss:0.001164, valid loss:0.000808
Epoch:59, Train loss:0.001163, valid loss:0.000787
Epoch:60, Train loss:0.001162, valid loss:0.000793
training time 7779.473310470581
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3790280721630821
plot_id,batch_id 0 1 miss% 0.41992641495281724
plot_id,batch_id 0 2 miss% 0.46507765539362717
plot_id,batch_id 0 3 miss% 0.42721099065564866
plot_id,batch_id 0 4 miss% 0.3790820339552481
plot_id,batch_id 0 5 miss% 0.4062193638583521
plot_id,batch_id 0 6 miss% 0.40856653087957306
plot_id,batch_id 0 7 miss% 0.5409749752062083
plot_id,batch_id 0 8 miss% 0.4788318817814031
plot_id,batch_id 0 9 miss% 0.5503140454535376
plot_id,batch_id 0 10 miss% 0.3458934374324377
plot_id,batch_id 0 11 miss% 0.38770897702273094
plot_id,batch_id 0 12 miss% 0.3624683756569382
plot_id,batch_id 0 13 miss% 0.36397859825271917
plot_id,batch_id 0 14 miss% 0.4402479929248419
plot_id,batch_id 0 15 miss% 0.4167086619422941
plot_id,batch_id 0 16 miss% 0.5389060160814017
plot_id,batch_id 0 17 miss% 0.5193042621777271
plot_id,batch_id 0 18 miss% 0.47148474709794297
plot_id,batch_id 0 19 miss% 0.4895092784096475
plot_id,batch_id 0 20 miss% 0.38088996199645414
plot_id,batch_id 0 21 miss% 0.4024024889908521
plot_id,batch_id 0 22 miss% 0.4211129894307386
plot_id,batch_id 0 23 miss% 0.4568217205488332
plot_id,batch_id 0 24 miss% 0.3750559355830489
plot_id,batch_id 0 25 miss% 0.4293469453868456
plot_id,batch_id 0 26 miss% 0.46980505708136966
plot_id,batch_id 0 27 miss% 0.38626757873459455
plot_id,batch_id 0 28 miss% 0.48617260137354296
plot_id,batch_id 0 29 miss% 0.42927708100838047
plot_id,batch_id 0 30 miss% 0.38292502739550227
plot_id,batch_id 0 31 miss% 0.4423629619377529
plot_id,batch_id 0 32 miss% 0.45477401272310186
plot_id,batch_id 0 33 miss% 0.47758325610275393
plot_id,batch_id 0 34 miss% 0.4017234716541973
plot_id,batch_id 0 35 miss% 0.4030207781772561
plot_id,batch_id 0 36 miss% 0.5726972544060162
plot_id,batch_id 0 37 miss% 0.4659381360519542
plot_id,batch_id 0 38 miss% 0.5229784495394605
plot_id,batch_id 0 39 miss% 0.5596916658094577
plot_id,batch_id 0 40 miss% 0.4214531195183519
plot_id,batch_id 0 41 miss% 0.42809960682451276
plot_id,batch_id 0 42 miss% 0.302679155329993
plot_id,batch_id 0 43 miss% 0.349755048102963
plot_id,batch_id 0 44 miss% 0.3983300387693177
plot_id,batch_id 0 45 miss% 0.39123980050129764
plot_id,batch_id 0 46 miss% 0.4416179011874584
plot_id,batch_id 0 47 miss% 0.4108333388596673
plot_id,batch_id 0 48 miss% 0.4233861012005184
plot_id,batch_id 0 49 miss% 0.33730935307169063
plot_id,batch_id 0 50 miss% 0.524913166038263
plot_id,batch_id 0 51 miss% 0.5298871359499837
plot_id,batch_id 0 52 miss% 0.5595074748404817
plot_id,batch_id 0 53 miss% 0.3911205547160246
plot_id,batch_id 0 54 miss% 0.4413814781824414
plot_id,batch_id 0 55 miss% 0.3882032571598975
plot_id,batch_id 0 56 miss% 0.5324115790201195
plot_id,batch_id 0 57 miss% 0.49952109741447803
plot_id,batch_id 0 58 miss% 0.4791064580589319
plot_id,batch_id 0 59 miss% 0.5473529582881019
plot_id,batch_id 0 60 miss% 0.3271557775816984
plot_id,batch_id 0 61 miss% 0.3484370519230259
plot_id,batch_id 0 62 miss% 0.4140979727083509
plot_id,batch_id 0 63 miss% 0.4125016036195713
plot_id,batch_id 0 64 miss% 0.4486317351705596
plot_id,batch_id 0 65 miss% 0.405421707372057
plot_id,batch_id 0 66 miss% 0.42717550560453493
plot_id,batch_id 0 67 miss% 0.38826797030079346
plot_id,batch_id 0 68 miss% 0.520854075276103
plot_id,batch_id 0 69 miss% 0.4572461522437656
plot_id,batch_id 0 70 miss% 0.27642322333045927
plot_id,batch_id 0 71 miss% 0.449127251814695
plot_id,batch_id 0 72 miss% 0.4706114983861706
plot_id,batch_id 0 73 miss% 0.4520761465164171
plot_id,batch_id 0 74 miss% 0.4975359528163154
plot_id,batch_id 0 75 miss% 0.30354219346718436
plot_id,batch_id 0 76 miss% 0.39444672780609474
plot_id,batch_id 0 77 miss% 0.3829006285585119
plot_id,batch_id 0 78 miss% 0.3570403051521432
plot_id,batch_id 0 79 miss% 0.4119017054470992
plot_id,batch_id 0 80 miss% 0.37476714495487506
plot_id,batch_id 0 81 miss% 0.4706186876702617
plot_id,batch_id 0 82 miss% 0.4015564892110624
plot_id,batch_id 0 83 miss% 0.4200689738793471
plot_id,batch_id 0 84 miss% 0.506495350508818
plot_id,batch_id 0 85 miss% 0.31305394885861115
plot_id,batch_id 0 86 miss% 0.4614626511085865
plot_id,batch_id 0 87 miss% 0.42150769780950664
plot_id,batch_id 0 88 miss% 0.43269648865691956
plot_id,batch_id 0 89 miss% 0.44116779195750633
plot_id,batch_id 0 90 miss% 0.2551371723450612
plot_id,batch_id 0 91 miss% 0.3864889676606618
plot_id,batch_id 0 92 miss% 0.3834115270906847
plot_id,batch_id 0 93 miss% 0.31917956134322595
plot_id,batch_id 0 94 miss% 0.5309419371605139
plot_id,batch_id 0 95 miss% 0.34714125322298384
plot_id,batch_id 0 96 miss% 0.3745717521805254
plot_id,batch_id 0 97 miss% 0.47024383974485795
plot_id,batch_id 0 98 miss% 0.44827631645098603
plot_id,batch_id 0 99 miss% 0.3981141335997024
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.37902807 0.41992641 0.46507766 0.42721099 0.37908203 0.40621936
 0.40856653 0.54097498 0.47883188 0.55031405 0.34589344 0.38770898
 0.36246838 0.3639786  0.44024799 0.41670866 0.53890602 0.51930426
 0.47148475 0.48950928 0.38088996 0.40240249 0.42111299 0.45682172
 0.37505594 0.42934695 0.46980506 0.38626758 0.4861726  0.42927708
 0.38292503 0.44236296 0.45477401 0.47758326 0.40172347 0.40302078
 0.57269725 0.46593814 0.52297845 0.55969167 0.42145312 0.42809961
 0.30267916 0.34975505 0.39833004 0.3912398  0.4416179  0.41083334
 0.4233861  0.33730935 0.52491317 0.52988714 0.55950747 0.39112055
 0.44138148 0.38820326 0.53241158 0.4995211  0.47910646 0.54735296
 0.32715578 0.34843705 0.41409797 0.4125016  0.44863174 0.40542171
 0.42717551 0.38826797 0.52085408 0.45724615 0.27642322 0.44912725
 0.4706115  0.45207615 0.49753595 0.30354219 0.39444673 0.38290063
 0.35704031 0.41190171 0.37476714 0.47061869 0.40155649 0.42006897
 0.50649535 0.31305395 0.46146265 0.4215077  0.43269649 0.44116779
 0.25513717 0.38648897 0.38341153 0.31917956 0.53094194 0.34714125
 0.37457175 0.47024384 0.44827632 0.39811413]
for model  28 the mean error 0.4281269517877504
all id 28 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 21 out win 5 err 0.4281269517877504 time 7779.473310470581
Launcher: Job 29 completed in 8023 seconds.
Launcher: Task 76 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  55697
Epoch:0, Train loss:0.777853, valid loss:0.738563
Epoch:1, Train loss:0.150839, valid loss:0.005867
Epoch:2, Train loss:0.014899, valid loss:0.004904
Epoch:3, Train loss:0.012811, valid loss:0.004363
Epoch:4, Train loss:0.011519, valid loss:0.003610
Epoch:5, Train loss:0.009163, valid loss:0.003199
Epoch:6, Train loss:0.007796, valid loss:0.003217
Epoch:7, Train loss:0.007262, valid loss:0.002904
Epoch:8, Train loss:0.006946, valid loss:0.002785
Epoch:9, Train loss:0.006721, valid loss:0.003193
Epoch:10, Train loss:0.006147, valid loss:0.002247
Epoch:11, Train loss:0.004746, valid loss:0.002033
Epoch:12, Train loss:0.004574, valid loss:0.002046
Epoch:13, Train loss:0.004588, valid loss:0.001907
Epoch:14, Train loss:0.003853, valid loss:0.001464
Epoch:15, Train loss:0.002207, valid loss:0.001090
Epoch:16, Train loss:0.002082, valid loss:0.001211
Epoch:17, Train loss:0.002048, valid loss:0.001183
Epoch:18, Train loss:0.001947, valid loss:0.001119
Epoch:19, Train loss:0.001915, valid loss:0.001155
Epoch:20, Train loss:0.001838, valid loss:0.001122
Epoch:21, Train loss:0.001485, valid loss:0.000872
Epoch:22, Train loss:0.001465, valid loss:0.000944
Epoch:23, Train loss:0.001422, valid loss:0.000905
Epoch:24, Train loss:0.001437, valid loss:0.000990
Epoch:25, Train loss:0.001410, valid loss:0.000953
Epoch:26, Train loss:0.001375, valid loss:0.000901
Epoch:27, Train loss:0.001362, valid loss:0.000880
Epoch:28, Train loss:0.001340, valid loss:0.000877
Epoch:29, Train loss:0.001311, valid loss:0.000880
Epoch:30, Train loss:0.001302, valid loss:0.000957
Epoch:31, Train loss:0.001132, valid loss:0.000790
Epoch:32, Train loss:0.001122, valid loss:0.000794
Epoch:33, Train loss:0.001100, valid loss:0.000810
Epoch:34, Train loss:0.001103, valid loss:0.000770
Epoch:35, Train loss:0.001095, valid loss:0.000792
Epoch:36, Train loss:0.001086, valid loss:0.000842
Epoch:37, Train loss:0.001066, valid loss:0.000771
Epoch:38, Train loss:0.001060, valid loss:0.000761
Epoch:39, Train loss:0.001042, valid loss:0.000799
Epoch:40, Train loss:0.001057, valid loss:0.000769
Epoch:41, Train loss:0.000964, valid loss:0.000770
Epoch:42, Train loss:0.000955, valid loss:0.000749
Epoch:43, Train loss:0.000945, valid loss:0.000759
Epoch:44, Train loss:0.000943, valid loss:0.000723
Epoch:45, Train loss:0.000941, valid loss:0.000737
Epoch:46, Train loss:0.000931, valid loss:0.000797
Epoch:47, Train loss:0.000933, valid loss:0.000794
Epoch:48, Train loss:0.000924, valid loss:0.000709
Epoch:49, Train loss:0.000914, valid loss:0.000733
Epoch:50, Train loss:0.000923, valid loss:0.000698
Epoch:51, Train loss:0.000877, valid loss:0.000690
Epoch:52, Train loss:0.000867, valid loss:0.000698
Epoch:53, Train loss:0.000864, valid loss:0.000690
Epoch:54, Train loss:0.000861, valid loss:0.000698
Epoch:55, Train loss:0.000861, valid loss:0.000689
Epoch:56, Train loss:0.000860, valid loss:0.000695
Epoch:57, Train loss:0.000858, valid loss:0.000683
Epoch:58, Train loss:0.000858, valid loss:0.000688
Epoch:59, Train loss:0.000857, valid loss:0.000693
Epoch:60, Train loss:0.000856, valid loss:0.000685
training time 7910.8780398368835
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.06976038913778575
plot_id,batch_id 0 1 miss% 0.050221398455312476
plot_id,batch_id 0 2 miss% 0.09748052295298673
plot_id,batch_id 0 3 miss% 0.046872879667480785
plot_id,batch_id 0 4 miss% 0.057322132180568176
plot_id,batch_id 0 5 miss% 0.1155344974577543
plot_id,batch_id 0 6 miss% 0.05490941663359516
plot_id,batch_id 0 7 miss% 0.09642431393419346
plot_id,batch_id 0 8 miss% 0.06729301698899626
plot_id,batch_id 0 9 miss% 0.051401775278191786
plot_id,batch_id 0 10 miss% 0.09976633633080526
plot_id,batch_id 0 11 miss% 0.09433827912159434
plot_id,batch_id 0 12 miss% 0.05856451084591814
plot_id,batch_id 0 13 miss% 0.08127327350680473
plot_id,batch_id 0 14 miss% 0.06567108658039592
plot_id,batch_id 0 15 miss% 0.061732817152095845
plot_id,batch_id 0 16 miss% 0.1247156547424429
plot_id,batch_id 0 17 miss% 0.1083331046012469
plot_id,batch_id 0 18 miss% 0.07345371936161464
plot_id,batch_id 0 19 miss% 0.08425023801260133
plot_id,batch_id 0 20 miss% 0.09357592827428264
plot_id,batch_id 0 21 miss% 0.09722422167038465
plot_id,batch_id 0 22 miss% 0.049638661292453846
plot_id,batch_id 0 23 miss% 0.033504501939268024
plot_id,batch_id 0 24 miss% 0.038251137525447766
plot_id,batch_id 0 25 miss% 0.06636957192816532
plot_id,batch_id 0 26 miss% 0.07572967193215681
plot_id,batch_id 0 27 miss% 0.053896016596509534
plot_id,batch_id 0 28 miss% 0.04166369422309058
plot_id,batch_id 0 29 miss% 0.030936576682919947
plot_id,batch_id 0 30 miss% 0.062702565441883
plot_id,batch_id 0 31 miss% 0.07815221669320913
plot_id,batch_id 0 32 miss% 0.10858766087862702
plot_id,batch_id 0 33 miss% 0.07634894909436736
plot_id,batch_id 0 34 miss% 0.04544139267592996
plot_id,batch_id 0 35 miss% 0.06635073747957601
plot_id,batch_id 0 36 miss% 0.10120262903463992
plot_id,batch_id 0 37 miss% 0.08641302290065057
plot_id,batch_id 0 38 miss% 0.05102224515216365
plot_id,batch_id 0 39 miss% 0.03718564606166133
plot_id,batch_id 0 40 miss% 0.05107340396256016
plot_id,batch_id 0 41 miss% 0.044123928717959796
plot_id,batch_id 0 42 miss% 0.04712874818650995
plot_id,batch_id 0 43 miss% 0.035691502236315814
plot_id,batch_id 0 44 miss% 0.03398954157591042
plot_id,batch_id 0 45 miss% 0.06534676874448118
plot_id,batch_id 0 46 miss% 0.038424260511796315
plot_id,batch_id 0 47 miss% 0.02963727404388556
plot_id,batch_id 0 48 miss% 0.03179491674062934
plot_id,batch_id 0 49 miss% 0.0637600945304427
plot_id,batch_id 0 50 miss% 0.08583534837557989
plot_id,batch_id 0 51 miss% 0.03173775184074062
plot_id,batch_id 0 52 miss% 0.02037551469268135
plot_id,batch_id 0 53 miss% 0.015280413208392945
plot_id,batch_id 0 54 miss% 0.03762488838524919
plot_id,batch_id 0 55 miss% 0.06971646218516532
plot_id,batch_id 0 56 miss% 0.11657031905807015
plot_id,batch_id 0 57 miss% 0.08480308395807189
plot_id,batch_id 0 58 miss% 0.04967993165250905
plot_id,batch_id 0 59 miss% 0.031224516526471076
plot_id,batch_id 0 60 miss% 0.047304919371381524
plot_id,batch_id 0 61 miss% 0.037653918335030435
plot_id,batch_id 0 62 miss% 0.07334836718802548
plot_id,batch_id 0 63 miss% 0.05237040753837696
plot_id,batch_id 0 64 miss% 0.0636219716270076
plot_id,batch_id 0 65 miss% 0.07857362751638579
plot_id,batch_id 0 66 miss% 0.06548134923082026
plot_id,batch_id 0 67 miss% 0.0291429155517378
plot_id,batch_id 0 68 miss% 0.049299646315117795
plot_id,batch_id 0 69 miss% 0.08145964874900888
plot_id,batch_id 0 70 miss% 0.09059262859866464
plot_id,batch_id 0 71 miss% 0.036499563511717535
plot_id,batch_id 0 72 miss% 0.16160242196435556
plot_id,batch_id 0 73 miss% 0.06341138534933306
plot_id,batch_id 0 74 miss% 0.07071679986996487
plot_id,batch_id 0 75 miss% 0.08860489889710682
plot_id,batch_id 0 76 miss% 0.10660773952630463
plot_id,batch_id 0 77 miss% 0.060825309895677374
plot_id,batch_id 0 78 miss% 0.03515480920920758
plot_id,batch_id 0 79 miss% 0.06711071535301115
plot_id,batch_id 0 80 miss% 0.056164993933712486
plot_id,batch_id 0 81 miss% 0.07659187257796324
plot_id,batch_id 0 82 miss% 0.08488980834984572
plot_id,batch_id 0 83 miss% 0.06836183181174134
plot_id,batch_id 0 84 miss% 0.09215943350834477
plot_id,batch_id 0 85 miss% 0.04251465187305143
plot_id,batch_id 0 86 miss% 0.04450242195842914
plot_id,batch_id 0 87 miss% 0.06803716856430185
plot_id,batch_id 0 88 miss% 0.074858513525213
plot_id,batch_id 0 89 miss% 0.05356009074522448
plot_id,batch_id 0 90 miss% 0.03415709867190066
plot_id,batch_id 0 91 miss% 0.06611557980185419
plot_id,batch_id 0 92 miss% 0.0549459101023061
plot_id,batch_id 0 93 miss% 0.05600395383352866
plot_id,batch_id 0 94 miss% 0.0708819432126666
plot_id,batch_id 0 95 miss% 0.060323665502714244
plot_id,batch_id 0 96 miss% 0.09270642178104958
plot_id,batch_id 0 97 miss% 0.07216769216956068
plot_id,batch_id 0 98 miss% 0.04649676653421723
plot_id,batch_id 0 99 miss% 0.08243406754948386
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06976039 0.0502214  0.09748052 0.04687288 0.05732213 0.1155345
 0.05490942 0.09642431 0.06729302 0.05140178 0.09976634 0.09433828
 0.05856451 0.08127327 0.06567109 0.06173282 0.12471565 0.1083331
 0.07345372 0.08425024 0.09357593 0.09722422 0.04963866 0.0335045
 0.03825114 0.06636957 0.07572967 0.05389602 0.04166369 0.03093658
 0.06270257 0.07815222 0.10858766 0.07634895 0.04544139 0.06635074
 0.10120263 0.08641302 0.05102225 0.03718565 0.0510734  0.04412393
 0.04712875 0.0356915  0.03398954 0.06534677 0.03842426 0.02963727
 0.03179492 0.06376009 0.08583535 0.03173775 0.02037551 0.01528041
 0.03762489 0.06971646 0.11657032 0.08480308 0.04967993 0.03122452
 0.04730492 0.03765392 0.07334837 0.05237041 0.06362197 0.07857363
 0.06548135 0.02914292 0.04929965 0.08145965 0.09059263 0.03649956
 0.16160242 0.06341139 0.0707168  0.0886049  0.10660774 0.06082531
 0.03515481 0.06711072 0.05616499 0.07659187 0.08488981 0.06836183
 0.09215943 0.04251465 0.04450242 0.06803717 0.07485851 0.05356009
 0.0341571  0.06611558 0.05494591 0.05600395 0.07088194 0.06032367
 0.09270642 0.07216769 0.04649677 0.08243407]
for model  36 the mean error 0.06494592009158541
all id 36 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 21 out win 4 err 0.06494592009158541 time 7910.8780398368835
Launcher: Job 37 completed in 8183 seconds.
Launcher: Task 255 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  55697
Epoch:0, Train loss:0.742758, valid loss:0.704333
Epoch:1, Train loss:0.188486, valid loss:0.005592
Epoch:2, Train loss:0.015124, valid loss:0.005232
Epoch:3, Train loss:0.013279, valid loss:0.003752
Epoch:4, Train loss:0.010053, valid loss:0.003577
Epoch:5, Train loss:0.008863, valid loss:0.003224
Epoch:6, Train loss:0.008386, valid loss:0.002980
Epoch:7, Train loss:0.007912, valid loss:0.003039
Epoch:8, Train loss:0.007754, valid loss:0.003078
Epoch:9, Train loss:0.007415, valid loss:0.002971
Epoch:10, Train loss:0.007522, valid loss:0.002939
Epoch:11, Train loss:0.006498, valid loss:0.002447
Epoch:12, Train loss:0.006314, valid loss:0.002369
Epoch:13, Train loss:0.006287, valid loss:0.002358
Epoch:14, Train loss:0.006157, valid loss:0.002498
Epoch:15, Train loss:0.006095, valid loss:0.002300
Epoch:16, Train loss:0.005456, valid loss:0.001733
Epoch:17, Train loss:0.002825, valid loss:0.001380
Epoch:18, Train loss:0.002521, valid loss:0.001356
Epoch:19, Train loss:0.002318, valid loss:0.001293
Epoch:20, Train loss:0.002322, valid loss:0.001398
Epoch:21, Train loss:0.001868, valid loss:0.001134
Epoch:22, Train loss:0.001785, valid loss:0.001054
Epoch:23, Train loss:0.001757, valid loss:0.001019
Epoch:24, Train loss:0.001764, valid loss:0.001022
Epoch:25, Train loss:0.001726, valid loss:0.001056
Epoch:26, Train loss:0.001680, valid loss:0.001030
Epoch:27, Train loss:0.001642, valid loss:0.001103
Epoch:28, Train loss:0.001646, valid loss:0.001000
Epoch:29, Train loss:0.001625, valid loss:0.001000
Epoch:30, Train loss:0.001575, valid loss:0.000972
Epoch:31, Train loss:0.001386, valid loss:0.001000
Epoch:32, Train loss:0.001348, valid loss:0.000912
Epoch:33, Train loss:0.001330, valid loss:0.000927
Epoch:34, Train loss:0.001326, valid loss:0.000988
Epoch:35, Train loss:0.001334, valid loss:0.000894
Epoch:36, Train loss:0.001306, valid loss:0.001030
Epoch:37, Train loss:0.001282, valid loss:0.000923
Epoch:38, Train loss:0.001278, valid loss:0.000892
Epoch:39, Train loss:0.001269, valid loss:0.000900
Epoch:40, Train loss:0.001276, valid loss:0.000968
Epoch:41, Train loss:0.001158, valid loss:0.000860
Epoch:42, Train loss:0.001142, valid loss:0.000873
Epoch:43, Train loss:0.001138, valid loss:0.000887
Epoch:44, Train loss:0.001131, valid loss:0.000868
Epoch:45, Train loss:0.001128, valid loss:0.000849
Epoch:46, Train loss:0.001118, valid loss:0.000878
Epoch:47, Train loss:0.001115, valid loss:0.000882
Epoch:48, Train loss:0.001113, valid loss:0.000846
Epoch:49, Train loss:0.001113, valid loss:0.000877
Epoch:50, Train loss:0.001100, valid loss:0.000905
Epoch:51, Train loss:0.001044, valid loss:0.000840
Epoch:52, Train loss:0.001033, valid loss:0.000834
Epoch:53, Train loss:0.001028, valid loss:0.000833
Epoch:54, Train loss:0.001026, valid loss:0.000834
Epoch:55, Train loss:0.001023, valid loss:0.000830
Epoch:56, Train loss:0.001022, valid loss:0.000837
Epoch:57, Train loss:0.001020, valid loss:0.000835
Epoch:58, Train loss:0.001019, valid loss:0.000830
Epoch:59, Train loss:0.001018, valid loss:0.000831
Epoch:60, Train loss:0.001018, valid loss:0.000829
training time 8390.195709705353
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.11445598065833457
plot_id,batch_id 0 1 miss% 0.03219011733466417
plot_id,batch_id 0 2 miss% 0.10533699950386509
plot_id,batch_id 0 3 miss% 0.05498009050737359
plot_id,batch_id 0 4 miss% 0.05595711028537509
plot_id,batch_id 0 5 miss% 0.13457052191453828
plot_id,batch_id 0 6 miss% 0.07648160885253107
plot_id,batch_id 0 7 miss% 0.09835321793563283
plot_id,batch_id 0 8 miss% 0.09751618488442526
plot_id,batch_id 0 9 miss% 0.04253493665324958
plot_id,batch_id 0 10 miss% 0.061197565639324575
plot_id,batch_id 0 11 miss% 0.08960497481896604
plot_id,batch_id 0 12 miss% 0.06807407986063845
plot_id,batch_id 0 13 miss% 0.06305280580369416
plot_id,batch_id 0 14 miss% 0.10984100446039938
plot_id,batch_id 0 15 miss% 0.08310211237696331
plot_id,batch_id 0 16 miss% 0.19224813121225534
plot_id,batch_id 0 17 miss% 0.07045119598388186
plot_id,batch_id 0 18 miss% 0.08977148316089407
plot_id,batch_id 0 19 miss% 0.10223160389090812
plot_id,batch_id 0 20 miss% 0.0847044712628463
plot_id,batch_id 0 21 miss% 0.01863482127104632
plot_id,batch_id 0 22 miss% 0.07438413475133913
plot_id,batch_id 0 23 miss% 0.044472362954400615
plot_id,batch_id 0 24 miss% 0.04605113705978467
plot_id,batch_id 0 25 miss% 0.0465369220324932
plot_id,batch_id 0 26 miss% 0.04916566661971375
plot_id,batch_id 0 27 miss% 0.03193825627079793
plot_id,batch_id 0 28 miss% 0.03458595437057137
plot_id,batch_id 0 29 miss% 0.0443067501834364
plot_id,batch_id 0 30 miss% 0.030242105466488322
plot_id,batch_id 0 31 miss% 0.10427498824329745
plot_id,batch_id 0 32 miss% 0.1097259094175079
plot_id,batch_id 0 33 miss% 0.06998653711890601
plot_id,batch_id 0 34 miss% 0.07090251611698947
plot_id,batch_id 0 35 miss% 0.11598202211518538
plot_id,batch_id 0 36 miss% 0.11306392018491333
plot_id,batch_id 0 37 miss% 0.08422731538141598
plot_id,batch_id 0 38 miss% 0.03442281348661331
plot_id,batch_id 0 39 miss% 0.025912898006210053
plot_id,batch_id 0 40 miss% 0.08062492032310853
plot_id,batch_id 0 41 miss% 0.0558753217975416
plot_id,batch_id 0 42 miss% 0.02257342950618335
plot_id,batch_id 0 43 miss% 0.03555742291734943
plot_id,batch_id 0 44 miss% 0.026050220310940727
plot_id,batch_id 0 45 miss% 0.0482500914778089
plot_id,batch_id 0 46 miss% 0.046483696236091575
plot_id,batch_id 0 47 miss% 0.05095260395167086
plot_id,batch_id 0 48 miss% 0.042598189426166506
plot_id,batch_id 0 49 miss% 0.04889811653103289
plot_id,batch_id 0 50 miss% 0.14438878716283815
plot_id,batch_id 0 51 miss% 0.035427463601817616
plot_id,batch_id 0 52 miss% 0.02392364316055037
plot_id,batch_id 0 53 miss% 0.027785349054330888
plot_id,batch_id 0 54 miss% 0.058123718966609506
plot_id,batch_id 0 55 miss% 0.11066410535800562
plot_id,batch_id 0 56 miss% 0.08138744642682588
plot_id,batch_id 0 57 miss% 0.040443070523485565
plot_id,batch_id 0 58 miss% 0.039336686652034304
plot_id,batch_id 0 59 miss% 0.02595782919936642
plot_id,batch_id 0 60 miss% 0.04925308447548137
plot_id,batch_id 0 61 miss% 0.06951846493269016
plot_id,batch_id 0 62 miss% 0.11437089598484947
plot_id,batch_id 0 63 miss% 0.06671648292487935
plot_id,batch_id 0 64 miss% 0.07636299856743084
plot_id,batch_id 0 65 miss% 0.04111008052107779
plot_id,batch_id 0 66 miss% 0.13458208850105846
plot_id,batch_id 0 67 miss% 0.03056001820598744
plot_id,batch_id 0 68 miss% 0.05822140041065255
plot_id,batch_id 0 69 miss% 0.07790820784961407
plot_id,batch_id 0 70 miss% 0.04274945161774616
plot_id,batch_id 0 71 miss% 0.03304531873226007
plot_id,batch_id 0 72 miss% 0.06332231519251086
plot_id,batch_id 0 73 miss% 0.0587243642766574
plot_id,batch_id 0 74 miss% 0.13007255789265074
plot_id,batch_id 0 75 miss% 0.10416997599351611
plot_id,batch_id 0 76 miss% 0.07713343442952876
plot_id,batch_id 0 77 miss% 0.06127953758498645
plot_id,batch_id 0 78 miss% 0.07450984119364763
plot_id,batch_id 0 79 miss% 0.12369222848957637
plot_id,batch_id 0 80 miss% 0.11870819688567893
plot_id,batch_id 0 81 miss% 0.09207344018463015
plot_id,batch_id 0 82 miss% 0.09476446341070117
plot_id,batch_id 0 83 miss% 0.08618408882692927
plot_id,batch_id 0 84 miss% 0.05419155894160448
plot_id,batch_id 0 85 miss% 0.04468455487413844
plot_id,batch_id 0 86 miss% 0.06359493051052087
plot_id,batch_id 0 87 miss% 0.07950036656768485
plot_id,batch_id 0 88 miss% 0.06515272645620071
plot_id,batch_id 0 89 miss% 0.08644990485980474
plot_id,batch_id 0 90 miss% 0.03256876279668294
plot_id,batch_id 0 91 miss% 0.06275933580865269
plot_id,batch_id 0 92 miss% 0.07744680559379659
plot_id,batch_id 0 93 miss% 0.06220866638375962
plot_id,batch_id 0 94 miss% 0.06796753743075538
plot_id,batch_id 0 95 miss% 0.11246623357233396
plot_id,batch_id 0 96 miss% 0.07822268824230497
plot_id,batch_id 0 97 miss% 0.060413611108606206
plot_id,batch_id 0 98 miss% 0.05378634649600521
plot_id,batch_id 0 99 miss% 0.03938192736359279
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.11445598 0.03219012 0.105337   0.05498009 0.05595711 0.13457052
 0.07648161 0.09835322 0.09751618 0.04253494 0.06119757 0.08960497
 0.06807408 0.06305281 0.109841   0.08310211 0.19224813 0.0704512
 0.08977148 0.1022316  0.08470447 0.01863482 0.07438413 0.04447236
 0.04605114 0.04653692 0.04916567 0.03193826 0.03458595 0.04430675
 0.03024211 0.10427499 0.10972591 0.06998654 0.07090252 0.11598202
 0.11306392 0.08422732 0.03442281 0.0259129  0.08062492 0.05587532
 0.02257343 0.03555742 0.02605022 0.04825009 0.0464837  0.0509526
 0.04259819 0.04889812 0.14438879 0.03542746 0.02392364 0.02778535
 0.05812372 0.11066411 0.08138745 0.04044307 0.03933669 0.02595783
 0.04925308 0.06951846 0.1143709  0.06671648 0.076363   0.04111008
 0.13458209 0.03056002 0.0582214  0.07790821 0.04274945 0.03304532
 0.06332232 0.05872436 0.13007256 0.10416998 0.07713343 0.06127954
 0.07450984 0.12369223 0.1187082  0.09207344 0.09476446 0.08618409
 0.05419156 0.04468455 0.06359493 0.07950037 0.06515273 0.0864499
 0.03256876 0.06275934 0.07744681 0.06220867 0.06796754 0.11246623
 0.07822269 0.06041361 0.05378635 0.03938193]
for model  37 the mean error 0.06940604234726816
all id 37 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 21 out win 5 err 0.06940604234726816 time 8390.195709705353
Launcher: Job 38 completed in 8671 seconds.
Launcher: Task 194 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  41745
Epoch:0, Train loss:0.645765, valid loss:0.640951
Epoch:1, Train loss:0.028800, valid loss:0.003721
Epoch:2, Train loss:0.006750, valid loss:0.002463
Epoch:3, Train loss:0.004424, valid loss:0.001837
Epoch:4, Train loss:0.003835, valid loss:0.001688
Epoch:5, Train loss:0.003472, valid loss:0.001701
Epoch:6, Train loss:0.003159, valid loss:0.001486
Epoch:7, Train loss:0.002900, valid loss:0.001577
Epoch:8, Train loss:0.002751, valid loss:0.001220
Epoch:9, Train loss:0.002608, valid loss:0.001496
Epoch:10, Train loss:0.002495, valid loss:0.001267
Epoch:11, Train loss:0.001817, valid loss:0.001011
Epoch:12, Train loss:0.001795, valid loss:0.000933
Epoch:13, Train loss:0.001748, valid loss:0.000872
Epoch:14, Train loss:0.001693, valid loss:0.000981
Epoch:15, Train loss:0.001655, valid loss:0.000823
Epoch:16, Train loss:0.001618, valid loss:0.000853
Epoch:17, Train loss:0.001602, valid loss:0.000812
Epoch:18, Train loss:0.001502, valid loss:0.000901
Epoch:19, Train loss:0.001554, valid loss:0.000736
Epoch:20, Train loss:0.001496, valid loss:0.000881
Epoch:21, Train loss:0.001171, valid loss:0.000727
Epoch:22, Train loss:0.001149, valid loss:0.000706
Epoch:23, Train loss:0.001143, valid loss:0.000666
Epoch:24, Train loss:0.001125, valid loss:0.000727
Epoch:25, Train loss:0.001103, valid loss:0.000623
Epoch:26, Train loss:0.001090, valid loss:0.000649
Epoch:27, Train loss:0.001089, valid loss:0.000684
Epoch:28, Train loss:0.001064, valid loss:0.000620
Epoch:29, Train loss:0.001049, valid loss:0.000657
Epoch:30, Train loss:0.001042, valid loss:0.000720
Epoch:31, Train loss:0.000896, valid loss:0.000632
Epoch:32, Train loss:0.000883, valid loss:0.000571
Epoch:33, Train loss:0.000869, valid loss:0.000613
Epoch:34, Train loss:0.000863, valid loss:0.000605
Epoch:35, Train loss:0.000867, valid loss:0.000635
Epoch:36, Train loss:0.000855, valid loss:0.000577
Epoch:37, Train loss:0.000856, valid loss:0.000581
Epoch:38, Train loss:0.000843, valid loss:0.000570
Epoch:39, Train loss:0.000844, valid loss:0.000620
Epoch:40, Train loss:0.000837, valid loss:0.000628
Epoch:41, Train loss:0.000761, valid loss:0.000547
Epoch:42, Train loss:0.000754, valid loss:0.000540
Epoch:43, Train loss:0.000753, valid loss:0.000553
Epoch:44, Train loss:0.000752, valid loss:0.000545
Epoch:45, Train loss:0.000744, valid loss:0.000550
Epoch:46, Train loss:0.000746, valid loss:0.000549
Epoch:47, Train loss:0.000741, valid loss:0.000541
Epoch:48, Train loss:0.000735, valid loss:0.000547
Epoch:49, Train loss:0.000732, valid loss:0.000551
Epoch:50, Train loss:0.000730, valid loss:0.000553
Epoch:51, Train loss:0.000687, valid loss:0.000533
Epoch:52, Train loss:0.000683, valid loss:0.000530
Epoch:53, Train loss:0.000681, valid loss:0.000528
Epoch:54, Train loss:0.000679, valid loss:0.000527
Epoch:55, Train loss:0.000678, valid loss:0.000526
Epoch:56, Train loss:0.000678, valid loss:0.000528
Epoch:57, Train loss:0.000677, valid loss:0.000528
Epoch:58, Train loss:0.000677, valid loss:0.000530
Epoch:59, Train loss:0.000676, valid loss:0.000527
Epoch:60, Train loss:0.000676, valid loss:0.000525
training time 8561.74935245514
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.25581721915900646
plot_id,batch_id 0 1 miss% 0.3566070681987512
plot_id,batch_id 0 2 miss% 0.40209744357880334
plot_id,batch_id 0 3 miss% 0.3367307067325441
plot_id,batch_id 0 4 miss% 0.28686015442042967
plot_id,batch_id 0 5 miss% 0.3230445693776827
plot_id,batch_id 0 6 miss% 0.39093001815731043
plot_id,batch_id 0 7 miss% 0.44326099801760427
plot_id,batch_id 0 8 miss% 0.5055094918532648
plot_id,batch_id 0 9 miss% 0.3632225394782277
plot_id,batch_id 0 10 miss% 0.29093410065151876
plot_id,batch_id 0 11 miss% 0.2957410904296341
plot_id,batch_id 0 12 miss% 0.3434510532834213
plot_id,batch_id 0 13 miss% 0.2848730199350869
plot_id,batch_id 0 14 miss% 0.3746942493632939
plot_id,batch_id 0 15 miss% 0.3367732790454156
plot_id,batch_id 0 16 miss% 0.4223461575233673
plot_id,batch_id 0 17 miss% 0.4399709111822443
plot_id,batch_id 0 18 miss% 0.36497749067590823
plot_id,batch_id 0 19 miss% 0.3367563875852673
plot_id,batch_id 0 20 miss% 0.42221675118129615
plot_id,batch_id 0 21 miss% 0.3439770277108708
plot_id,batch_id 0 22 miss% 0.43351152958861694
plot_id,batch_id 0 23 miss% 0.36265177448680885
plot_id,batch_id 0 24 miss% 0.32568570536463776
plot_id,batch_id 0 25 miss% 0.39365012308755015
plot_id,batch_id 0 26 miss% 0.404456585617241
plot_id,batch_id 0 27 miss% 0.38995147347126896
plot_id,batch_id 0 28 miss% 0.4621455561074672
plot_id,batch_id 0 29 miss% 0.33173349649474926
plot_id,batch_id 0 30 miss% 0.23178197115131738
plot_id,batch_id 0 31 miss% 0.47902046311066654
plot_id,batch_id 0 32 miss% 0.4587019734749825
plot_id,batch_id 0 33 miss% 0.4198941140853623
plot_id,batch_id 0 34 miss% 0.389108399768427
plot_id,batch_id 0 35 miss% 0.27644821958148963
plot_id,batch_id 0 36 miss% 0.4637796039015343
plot_id,batch_id 0 37 miss% 0.3954010106100252
plot_id,batch_id 0 38 miss% 0.27010131395960313
plot_id,batch_id 0 39 miss% 0.3258372230547417
plot_id,batch_id 0 40 miss% 0.3028515246279969
plot_id,batch_id 0 41 miss% 0.43120309161147147
plot_id,batch_id 0 42 miss% 0.37636806437733505
plot_id,batch_id 0 43 miss% 0.2590268999756723
plot_id,batch_id 0 44 miss% 0.2669642034174921
plot_id,batch_id 0 45 miss% 0.33947702911558253
plot_id,batch_id 0 46 miss% 0.3965213265215777
plot_id,batch_id 0 47 miss% 0.41571402307810984
plot_id,batch_id 0 48 miss% 0.4143126439295108
plot_id,batch_id 0 49 miss% 0.33187439014782927
plot_id,batch_id 0 50 miss% 0.4827495685003348
plot_id,batch_id 0 51 miss% 0.46918361685257365
plot_id,batch_id 0 52 miss% 0.4428402545343975
plot_id,batch_id 0 53 miss% 0.27281117704259505
plot_id,batch_id 0 54 miss% 0.30744399499265085
plot_id,batch_id 0 55 miss% 0.435028464381261
plot_id,batch_id 0 56 miss% 0.5668036654378543
plot_id,batch_id 0 57 miss% 0.4480915405390639
plot_id,batch_id 0 58 miss% 0.34830514587065753
plot_id,batch_id 0 59 miss% 0.41477099146574725
plot_id,batch_id 0 60 miss% 0.21344752232358105
plot_id,batch_id 0 61 miss% 0.25322582508550384
plot_id,batch_id 0 62 miss% 0.3160971633519714
plot_id,batch_id 0 63 miss% 0.37888024745127474
plot_id,batch_id 0 64 miss% 0.3255439337080569
plot_id,batch_id 0 65 miss% 0.4691358105110184
plot_id,batch_id 0 66 miss% 0.3511399290626655
plot_id,batch_id 0 67 miss% 0.22859997760236436
plot_id,batch_id 0 68 miss% 0.32061390478547996
plot_id,batch_id 0 69 miss% 0.3430192538417324
plot_id,batch_id 0 70 miss% 0.2065578655315425
plot_id,batch_id 0 71 miss% 0.32590455297834503
plot_id,batch_id 0 72 miss% 0.42157780453667537
plot_id,batch_id 0 73 miss% 0.3412567189113227
plot_id,batch_id 0 74 miss% 0.291183911985619
plot_id,batch_id 0 75 miss% 0.20455043987256027
plot_id,batch_id 0 76 miss% 0.2656948173885841
plot_id,batch_id 0 77 miss% 0.2557707383628971
plot_id,batch_id 0 78 miss% 0.23273842357732366
plot_id,batch_id 0 79 miss% 0.2844862258605976
plot_id,batch_id 0 80 miss% 0.2592832775647507
plot_id,batch_id 0 81 miss% 0.40490263026947293
plot_id,batch_id 0 82 miss% 0.2988246381849314
plot_id,batch_id 0 83 miss% 0.3648326018475353
plot_id,batch_id 0 84 miss% 0.29316253812609566
plot_id,batch_id 0 85 miss% 0.21693744974682136
plot_id,batch_id 0 86 miss% 0.2977048889264405
plot_id,batch_id 0 87 miss% 0.40036096143983235
plot_id,batch_id 0 88 miss% 0.3389901521218036
plot_id,batch_id 0 89 miss% 0.41332058335019467
plot_id,batch_id 0 90 miss% 0.19472885310917262
plot_id,batch_id 0 91 miss% 0.2679082233836255
plot_id,batch_id 0 92 miss% 0.2758876972259604
plot_id,batch_id 0 93 miss% 0.26974946581155296
plot_id,batch_id 0 94 miss% 0.4352935356277477
plot_id,batch_id 0 95 miss% 0.2366805095854623
plot_id,batch_id 0 96 miss% 0.2486859785364016
plot_id,batch_id 0 97 miss% 0.3598487485669641
plot_id,batch_id 0 98 miss% 0.3163815304787868
plot_id,batch_id 0 99 miss% 0.41464234635124464
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.25581722 0.35660707 0.40209744 0.33673071 0.28686015 0.32304457
 0.39093002 0.443261   0.50550949 0.36322254 0.2909341  0.29574109
 0.34345105 0.28487302 0.37469425 0.33677328 0.42234616 0.43997091
 0.36497749 0.33675639 0.42221675 0.34397703 0.43351153 0.36265177
 0.32568571 0.39365012 0.40445659 0.38995147 0.46214556 0.3317335
 0.23178197 0.47902046 0.45870197 0.41989411 0.3891084  0.27644822
 0.4637796  0.39540101 0.27010131 0.32583722 0.30285152 0.43120309
 0.37636806 0.2590269  0.2669642  0.33947703 0.39652133 0.41571402
 0.41431264 0.33187439 0.48274957 0.46918362 0.44284025 0.27281118
 0.30744399 0.43502846 0.56680367 0.44809154 0.34830515 0.41477099
 0.21344752 0.25322583 0.31609716 0.37888025 0.32554393 0.46913581
 0.35113993 0.22859998 0.3206139  0.34301925 0.20655787 0.32590455
 0.4215778  0.34125672 0.29118391 0.20455044 0.26569482 0.25577074
 0.23273842 0.28448623 0.25928328 0.40490263 0.29882464 0.3648326
 0.29316254 0.21693745 0.29770489 0.40036096 0.33899015 0.41332058
 0.19472885 0.26790822 0.2758877  0.26974947 0.43529354 0.23668051
 0.24868598 0.35984875 0.31638153 0.41464235]
for model  135 the mean error 0.3479454555786106
all id 135 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 25 out win 4 err 0.3479454555786106 time 8561.74935245514
Launcher: Job 136 completed in 8817 seconds.
Launcher: Task 187 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  41745
Epoch:0, Train loss:0.614092, valid loss:0.600074
Epoch:1, Train loss:0.031312, valid loss:0.003154
Epoch:2, Train loss:0.006116, valid loss:0.002823
Epoch:3, Train loss:0.004682, valid loss:0.001844
Epoch:4, Train loss:0.004027, valid loss:0.001979
Epoch:5, Train loss:0.003688, valid loss:0.001742
Epoch:6, Train loss:0.003491, valid loss:0.001493
Epoch:7, Train loss:0.003302, valid loss:0.001686
Epoch:8, Train loss:0.003094, valid loss:0.001585
Epoch:9, Train loss:0.002855, valid loss:0.001316
Epoch:10, Train loss:0.002810, valid loss:0.001425
Epoch:11, Train loss:0.002125, valid loss:0.001127
Epoch:12, Train loss:0.002047, valid loss:0.001072
Epoch:13, Train loss:0.001936, valid loss:0.001158
Epoch:14, Train loss:0.002021, valid loss:0.001055
Epoch:15, Train loss:0.001857, valid loss:0.000895
Epoch:16, Train loss:0.001794, valid loss:0.001035
Epoch:17, Train loss:0.001752, valid loss:0.000966
Epoch:18, Train loss:0.001742, valid loss:0.001043
Epoch:19, Train loss:0.001692, valid loss:0.000918
Epoch:20, Train loss:0.001617, valid loss:0.001009
Epoch:21, Train loss:0.001320, valid loss:0.000822
Epoch:22, Train loss:0.001264, valid loss:0.000793
Epoch:23, Train loss:0.001261, valid loss:0.000793
Epoch:24, Train loss:0.001263, valid loss:0.000791
Epoch:25, Train loss:0.001255, valid loss:0.000739
Epoch:26, Train loss:0.001224, valid loss:0.000787
Epoch:27, Train loss:0.001197, valid loss:0.000748
Epoch:28, Train loss:0.001180, valid loss:0.000894
Epoch:29, Train loss:0.001181, valid loss:0.000793
Epoch:30, Train loss:0.001155, valid loss:0.000786
Epoch:31, Train loss:0.000997, valid loss:0.000743
Epoch:32, Train loss:0.000964, valid loss:0.000685
Epoch:33, Train loss:0.000972, valid loss:0.000721
Epoch:34, Train loss:0.000954, valid loss:0.000735
Epoch:35, Train loss:0.000942, valid loss:0.000742
Epoch:36, Train loss:0.000944, valid loss:0.000752
Epoch:37, Train loss:0.000944, valid loss:0.000693
Epoch:38, Train loss:0.000934, valid loss:0.000691
Epoch:39, Train loss:0.000928, valid loss:0.000689
Epoch:40, Train loss:0.000914, valid loss:0.000735
Epoch:41, Train loss:0.000829, valid loss:0.000656
Epoch:42, Train loss:0.000819, valid loss:0.000657
Epoch:43, Train loss:0.000817, valid loss:0.000677
Epoch:44, Train loss:0.000809, valid loss:0.000684
Epoch:45, Train loss:0.000807, valid loss:0.000640
Epoch:46, Train loss:0.000807, valid loss:0.000666
Epoch:47, Train loss:0.000805, valid loss:0.000675
Epoch:48, Train loss:0.000804, valid loss:0.000696
Epoch:49, Train loss:0.000804, valid loss:0.000660
Epoch:50, Train loss:0.000791, valid loss:0.000692
Epoch:51, Train loss:0.000749, valid loss:0.000642
Epoch:52, Train loss:0.000744, valid loss:0.000640
Epoch:53, Train loss:0.000741, valid loss:0.000642
Epoch:54, Train loss:0.000739, valid loss:0.000640
Epoch:55, Train loss:0.000738, valid loss:0.000639
Epoch:56, Train loss:0.000737, valid loss:0.000641
Epoch:57, Train loss:0.000735, valid loss:0.000641
Epoch:58, Train loss:0.000735, valid loss:0.000638
Epoch:59, Train loss:0.000734, valid loss:0.000641
Epoch:60, Train loss:0.000733, valid loss:0.000639
training time 8640.360721349716
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.33199947894289034
plot_id,batch_id 0 1 miss% 0.41256796098856574
plot_id,batch_id 0 2 miss% 0.45334590562870264
plot_id,batch_id 0 3 miss% 0.36896943417278494
plot_id,batch_id 0 4 miss% 0.3277213743493713
plot_id,batch_id 0 5 miss% 0.3685269844371019
plot_id,batch_id 0 6 miss% 0.37083834654421216
plot_id,batch_id 0 7 miss% 0.5152040884148417
plot_id,batch_id 0 8 miss% 0.6055166188254555
plot_id,batch_id 0 9 miss% 0.48772457627415416
plot_id,batch_id 0 10 miss% 0.34460281721333585
plot_id,batch_id 0 11 miss% 0.34210094644756445
plot_id,batch_id 0 12 miss% 0.4067781097172477
plot_id,batch_id 0 13 miss% 0.3379205537438834
plot_id,batch_id 0 14 miss% 0.4658661251074524
plot_id,batch_id 0 15 miss% 0.3346458938767535
plot_id,batch_id 0 16 miss% 0.5321796527007374
plot_id,batch_id 0 17 miss% 0.5042386971886907
plot_id,batch_id 0 18 miss% 0.4143167782934016
plot_id,batch_id 0 19 miss% 0.5042101572926881
plot_id,batch_id 0 20 miss% 0.4092299129597064
plot_id,batch_id 0 21 miss% 0.5236371570814679
plot_id,batch_id 0 22 miss% 0.4607492631214181
plot_id,batch_id 0 23 miss% 0.4234687511401487
plot_id,batch_id 0 24 miss% 0.3044754189051577
plot_id,batch_id 0 25 miss% 0.4295645740009425
plot_id,batch_id 0 26 miss% 0.42825429734118853
plot_id,batch_id 0 27 miss% 0.36594357918211995
plot_id,batch_id 0 28 miss% 0.4283379775456268
plot_id,batch_id 0 29 miss% 0.41747012900374175
plot_id,batch_id 0 30 miss% 0.3022112303911828
plot_id,batch_id 0 31 miss% 0.4667310867365239
plot_id,batch_id 0 32 miss% 0.44889548272109897
plot_id,batch_id 0 33 miss% 0.42477921726226064
plot_id,batch_id 0 34 miss% 0.3711479482021478
plot_id,batch_id 0 35 miss% 0.44201580686845615
plot_id,batch_id 0 36 miss% 0.5030670332685034
plot_id,batch_id 0 37 miss% 0.4043797901856213
plot_id,batch_id 0 38 miss% 0.47173323264865646
plot_id,batch_id 0 39 miss% 0.4124638077167365
plot_id,batch_id 0 40 miss% 0.38789246310060954
plot_id,batch_id 0 41 miss% 0.47487122906256896
plot_id,batch_id 0 42 miss% 0.3296279212458852
plot_id,batch_id 0 43 miss% 0.36259117881638736
plot_id,batch_id 0 44 miss% 0.2722424670924526
plot_id,batch_id 0 45 miss% 0.40916005435659925
plot_id,batch_id 0 46 miss% 0.42016092459830057
plot_id,batch_id 0 47 miss% 0.4228394661687259
plot_id,batch_id 0 48 miss% 0.3776075103415492
plot_id,batch_id 0 49 miss% 0.3725528513663301
plot_id,batch_id 0 50 miss% 0.5265365050826418
plot_id,batch_id 0 51 miss% 0.46541863476235523
plot_id,batch_id 0 52 miss% 0.4374683924231029
plot_id,batch_id 0 53 miss% 0.3077079580540738
plot_id,batch_id 0 54 miss% 0.3798462636074738
plot_id,batch_id 0 55 miss% 0.5235472821100811
plot_id,batch_id 0 56 miss% 0.5772114372097926
plot_id,batch_id 0 57 miss% 0.4244857410404383
plot_id,batch_id 0 58 miss% 0.4199521391957795
plot_id,batch_id 0 59 miss% 0.4633945865154204
plot_id,batch_id 0 60 miss% 0.31915923642987104
plot_id,batch_id 0 61 miss% 0.28785809709026633
plot_id,batch_id 0 62 miss% 0.49099471847039217
plot_id,batch_id 0 63 miss% 0.4358995647856926
plot_id,batch_id 0 64 miss% 0.42153976301358786
plot_id,batch_id 0 65 miss% 0.44133670445477896
plot_id,batch_id 0 66 miss% 0.40422967692387546
plot_id,batch_id 0 67 miss% 0.3217217640906317
plot_id,batch_id 0 68 miss% 0.5177435112520223
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  89105
Epoch:0, Train loss:0.684004, valid loss:0.639811
Epoch:1, Train loss:0.176227, valid loss:0.006136
Epoch:2, Train loss:0.017067, valid loss:0.005462
Epoch:3, Train loss:0.015949, valid loss:0.004982
Epoch:4, Train loss:0.015149, valid loss:0.005057
Epoch:5, Train loss:0.014610, valid loss:0.004514
Epoch:6, Train loss:0.014163, valid loss:0.004517
Epoch:7, Train loss:0.012391, valid loss:0.003495
Epoch:8, Train loss:0.010094, valid loss:0.003346
Epoch:9, Train loss:0.008782, valid loss:0.003063
Epoch:10, Train loss:0.006467, valid loss:0.002169
Epoch:11, Train loss:0.004539, valid loss:0.001647
Epoch:12, Train loss:0.003381, valid loss:0.001439
Epoch:13, Train loss:0.002793, valid loss:0.001424
Epoch:14, Train loss:0.002599, valid loss:0.001411
Epoch:15, Train loss:0.002586, valid loss:0.001440
Epoch:16, Train loss:0.002396, valid loss:0.001421
Epoch:17, Train loss:0.002265, valid loss:0.001441
Epoch:18, Train loss:0.002224, valid loss:0.001277
Epoch:19, Train loss:0.002255, valid loss:0.001220
Epoch:20, Train loss:0.002048, valid loss:0.001431
Epoch:21, Train loss:0.001626, valid loss:0.001088
Epoch:22, Train loss:0.001593, valid loss:0.001069
Epoch:23, Train loss:0.001526, valid loss:0.001025
Epoch:24, Train loss:0.001499, valid loss:0.001053
Epoch:25, Train loss:0.001493, valid loss:0.001059
Epoch:26, Train loss:0.001477, valid loss:0.001026
Epoch:27, Train loss:0.001436, valid loss:0.001076
Epoch:28, Train loss:0.001438, valid loss:0.001075
Epoch:29, Train loss:0.001388, valid loss:0.001156
Epoch:30, Train loss:0.001359, valid loss:0.000989
Epoch:31, Train loss:0.001132, valid loss:0.000943
Epoch:32, Train loss:0.001103, valid loss:0.000952
Epoch:33, Train loss:0.001109, valid loss:0.000929
Epoch:34, Train loss:0.001061, valid loss:0.000927
Epoch:35, Train loss:0.001067, valid loss:0.000917
Epoch:36, Train loss:0.001056, valid loss:0.000914
Epoch:37, Train loss:0.001049, valid loss:0.000880
Epoch:38, Train loss:0.001033, valid loss:0.000959
Epoch:39, Train loss:0.001019, valid loss:0.000925
Epoch:40, Train loss:0.001005, valid loss:0.000910
Epoch:41, Train loss:0.000892, valid loss:0.000866
Epoch:42, Train loss:0.000888, valid loss:0.000871
Epoch:43, Train loss:0.000895, valid loss:0.000923
Epoch:44, Train loss:0.000880, valid loss:0.000876
Epoch:45, Train loss:0.000866, valid loss:0.000863
Epoch:46, Train loss:0.000864, valid loss:0.000884
Epoch:47, Train loss:0.000860, valid loss:0.000911
Epoch:48, Train loss:0.000850, valid loss:0.000886
Epoch:49, Train loss:0.000843, valid loss:0.000837
Epoch:50, Train loss:0.000829, valid loss:0.000866
Epoch:51, Train loss:0.000781, valid loss:0.000844
Epoch:52, Train loss:0.000771, valid loss:0.000840
Epoch:53, Train loss:0.000768, valid loss:0.000846
Epoch:54, Train loss:0.000765, valid loss:0.000841
Epoch:55, Train loss:0.000763, valid loss:0.000844
Epoch:56, Train loss:0.000762, valid loss:0.000841
Epoch:57, Train loss:0.000761, valid loss:0.000839
Epoch:58, Train loss:0.000760, valid loss:0.000845
Epoch:59, Train loss:0.000759, valid loss:0.000838
Epoch:60, Train loss:0.000759, valid loss:0.000841
training time 8670.51211309433
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.44977875720714616
plot_id,batch_id 0 1 miss% 0.46223469328056127
plot_id,batch_id 0 2 miss% 0.5323988519738602
plot_id,batch_id 0 3 miss% 0.4232480365162524
plot_id,batch_id 0 4 miss% 0.4693540724464077
plot_id,batch_id 0 5 miss% 0.5064219613365788
plot_id,batch_id 0 6 miss% 0.4096805918643383
plot_id,batch_id 0 7 miss% 0.5798699037432539
plot_id,batch_id 0 8 miss% 0.5546351402624707
plot_id,batch_id 0 9 miss% 0.4958961452776936
plot_id,batch_id 0 10 miss% 0.4124221388626127
plot_id,batch_id 0 11 miss% 0.46252781197844034
plot_id,batch_id 0 12 miss% 0.4911399939232868
plot_id,batch_id 0 13 miss% 0.45980919524909025
plot_id,batch_id 0 14 miss% 0.5255475351010651
plot_id,batch_id 0 15 miss% 0.5068597106331143
plot_id,batch_id 0 16 miss% 0.6014273322790005
plot_id,batch_id 0 17 miss% 0.5380355167203079
plot_id,batch_id 0 18 miss% 0.5477775811625067
plot_id,batch_id 0 19 miss% 0.502728243480386
plot_id,batch_id 0 20 miss% 0.4596525409385703
plot_id,batch_id 0 21 miss% 0.5125199127795598
plot_id,batch_id 0 22 miss% 0.5332486625334855
plot_id,batch_id 0 23 miss% 0.4993899803239582
plot_id,batch_id 0 24 miss% 0.45539353552289547
plot_id,batch_id 0 25 miss% 0.4576199313908342
plot_id,batch_id 0 26 miss% 0.5084610314746362
plot_id,batch_id 0 27 miss% 0.42445003679174736
plot_id,batch_id 0 28 miss% 0.446131402578858
plot_id,batch_id 0 29 miss% 0.44399546626813596
plot_id,batch_id 0 30 miss% 0.45163410730345593
plot_id,batch_id 0 31 miss% 0.5239126649671931
plot_id,batch_id 0 32 miss% 0.5102643807267286
plot_id,batch_id 0 33 miss% 0.4394541072519056
plot_id,batch_id 0 34 miss% 0.41470125010055153
plot_id,batch_id 0 35 miss% 0.5043558726898822
plot_id,batch_id 0 36 miss% 0.5833512695563184
plot_id,batch_id 0 37 miss% 0.5306700685168437
plot_id,batch_id 0 38 miss% 0.4810426414637531
plot_id,batch_id 0 39 miss% 0.5756846900549024
plot_id,batch_id 0 40 miss% 0.4341682611006633
plot_id,batch_id 0 41 miss% 0.5177871732930949
plot_id,batch_id 0 42 miss% 0.39010756951207015
plot_id,batch_id 0 43 miss% 0.3895613145159129
plot_id,batch_id 0 44 miss% 0.4141074665612924
plot_id,batch_id 0 45 miss% 0.4494216351152824
plot_id,batch_id 0 46 miss% 0.48090008057927486
plot_id,batch_id 0 47 miss% 0.48983771569324924
plot_id,batch_id 0 48 miss% 0.5057959675651221
plot_id,batch_id 0 49 miss% 0.4466813009437708
plot_id,batch_id 0 50 miss% 0.5656993756703499
plot_id,batch_id 0 51 miss% 0.5337582945101916
plot_id,batch_id 0 52 miss% 0.535581710048794
plot_id,batch_id 0 53 miss% 0.4535829992573573
plot_id,batch_id 0 54 miss% 0.4075328799385843
plot_id,batch_id 0 55 miss% 0.49942364780972537
plot_id,batch_id 0 56 miss% 0.5462371626019453
plot_id,batch_id 0 57 miss% 0.532567884432782
plot_id,batch_id 0 58 miss% 0.4775359070755914
plot_id,batch_id 0 59 miss% 0.5653286660445616
plot_id,batch_id 0 60 miss% 0.3721950219024735
plot_id,batch_id 0 61 miss% 0.4218152157600775
plot_id,batch_id 0 62 miss% 0.44623769326768065
plot_id,batch_id 0 63 miss% 0.5193833700580878
plot_id,batch_id 0 64 miss% 0.4458410330953004
plot_id,batch_id 0 65 miss% 0.4501009150625834
plot_id,batch_id 0 66 miss% 0.5124409065600188
plot_id,batch_id 0 67 miss% 0.4398352011102896
plot_id,batch_id 0 68 miss% 0.5610367781132582
plot_id,batch_id 0 69 miss% 0.5245731827227997
plot_id,batch_id 0 70 miss% plot_id,batch_id 0 69 miss% 0.4872310217771583
plot_id,batch_id 0 70 miss% 0.3502417758672869
plot_id,batch_id 0 71 miss% 0.3756916093215023
plot_id,batch_id 0 72 miss% 0.42091993173081915
plot_id,batch_id 0 73 miss% 0.31974294917560175
plot_id,batch_id 0 74 miss% 0.4520427005788333
plot_id,batch_id 0 75 miss% 0.3272113933192052
plot_id,batch_id 0 76 miss% 0.3897826148238584
plot_id,batch_id 0 77 miss% 0.36476756226867135
plot_id,batch_id 0 78 miss% 0.35186653130967027
plot_id,batch_id 0 79 miss% 0.31218908217771785
plot_id,batch_id 0 80 miss% 0.31681150137581926
plot_id,batch_id 0 81 miss% 0.45985264759845523
plot_id,batch_id 0 82 miss% 0.34644496303890465
plot_id,batch_id 0 83 miss% 0.4256450336416396
plot_id,batch_id 0 84 miss% 0.42302307385506965
plot_id,batch_id 0 85 miss% 0.27972978965879736
plot_id,batch_id 0 86 miss% 0.41107478800493885
plot_id,batch_id 0 87 miss% 0.4676519243439351
plot_id,batch_id 0 88 miss% 0.42933066183305685
plot_id,batch_id 0 89 miss% 0.4253633484380947
plot_id,batch_id 0 90 miss% 0.24831730907576308
plot_id,batch_id 0 91 miss% 0.4519019625543067
plot_id,batch_id 0 92 miss% 0.33592987495954346
plot_id,batch_id 0 93 miss% 0.3097795964767143
plot_id,batch_id 0 94 miss% 0.5168150654786468
plot_id,batch_id 0 95 miss% 0.4338850314412225
plot_id,batch_id 0 96 miss% 0.33179430407041927
plot_id,batch_id 0 97 miss% 0.5057855645182412
plot_id,batch_id 0 98 miss% 0.42758558005405106
plot_id,batch_id 0 99 miss% 0.46447761194047915
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.33199948 0.41256796 0.45334591 0.36896943 0.32772137 0.36852698
 0.37083835 0.51520409 0.60551662 0.48772458 0.34460282 0.34210095
 0.40677811 0.33792055 0.46586613 0.33464589 0.53217965 0.5042387
 0.41431678 0.50421016 0.40922991 0.52363716 0.46074926 0.42346875
 0.30447542 0.42956457 0.4282543  0.36594358 0.42833798 0.41747013
 0.30221123 0.46673109 0.44889548 0.42477922 0.37114795 0.44201581
 0.50306703 0.40437979 0.47173323 0.41246381 0.38789246 0.47487123
 0.32962792 0.36259118 0.27224247 0.40916005 0.42016092 0.42283947
 0.37760751 0.37255285 0.52653651 0.46541863 0.43746839 0.30770796
 0.37984626 0.52354728 0.57721144 0.42448574 0.41995214 0.46339459
 0.31915924 0.2878581  0.49099472 0.43589956 0.42153976 0.4413367
 0.40422968 0.32172176 0.51774351 0.48723102 0.35024178 0.37569161
 0.42091993 0.31974295 0.4520427  0.32721139 0.38978261 0.36476756
 0.35186653 0.31218908 0.3168115  0.45985265 0.34644496 0.42564503
 0.42302307 0.27972979 0.41107479 0.46765192 0.42933066 0.42536335
 0.24831731 0.45190196 0.33592987 0.3097796  0.51681507 0.43388503
 0.3317943  0.50578556 0.42758558 0.46447761]
for model  136 the mean error 0.4102631504381265
all id 136 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 25 out win 5 err 0.4102631504381265 time 8640.360721349716
Launcher: Job 137 completed in 8884 seconds.
Launcher: Task 253 done. Exiting.
0.35752708056773996
plot_id,batch_id 0 71 miss% 0.531200685096212
plot_id,batch_id 0 72 miss% 0.5483451421027372
plot_id,batch_id 0 73 miss% 0.4825099723146426
plot_id,batch_id 0 74 miss% 0.5476904833906165
plot_id,batch_id 0 75 miss% 0.36213196525132013
plot_id,batch_id 0 76 miss% 0.4764374444229791
plot_id,batch_id 0 77 miss% 0.4621071938316374
plot_id,batch_id 0 78 miss% 0.452207925221288
plot_id,batch_id 0 79 miss% 0.477609676871808
plot_id,batch_id 0 80 miss% 0.4393780435623775
plot_id,batch_id 0 81 miss% 0.5747730649734606
plot_id,batch_id 0 82 miss% 0.5222304848659467
plot_id,batch_id 0 83 miss% 0.48236792379430204
plot_id,batch_id 0 84 miss% 0.46921137883455344
plot_id,batch_id 0 85 miss% 0.4617349165464619
plot_id,batch_id 0 86 miss% 0.4380148354126074
plot_id,batch_id 0 87 miss% 0.4966798833540996
plot_id,batch_id 0 88 miss% 0.6026342967000992
plot_id,batch_id 0 89 miss% 0.5051903717315306
plot_id,batch_id 0 90 miss% 0.367370536721879
plot_id,batch_id 0 91 miss% 0.44248272495805535
plot_id,batch_id 0 92 miss% 0.47222401152197857
plot_id,batch_id 0 93 miss% 0.3621773308774201
plot_id,batch_id 0 94 miss% 0.5708130003157754
plot_id,batch_id 0 95 miss% 0.3959142530042215
plot_id,batch_id 0 96 miss% 0.4602966488582175
plot_id,batch_id 0 97 miss% 0.5419701373854474
plot_id,batch_id 0 98 miss% 0.5688649935337775
plot_id,batch_id 0 99 miss% 0.42406528400213245
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.44977876 0.46223469 0.53239885 0.42324804 0.46935407 0.50642196
 0.40968059 0.5798699  0.55463514 0.49589615 0.41242214 0.46252781
 0.49113999 0.4598092  0.52554754 0.50685971 0.60142733 0.53803552
 0.54777758 0.50272824 0.45965254 0.51251991 0.53324866 0.49938998
 0.45539354 0.45761993 0.50846103 0.42445004 0.4461314  0.44399547
 0.45163411 0.52391266 0.51026438 0.43945411 0.41470125 0.50435587
 0.58335127 0.53067007 0.48104264 0.57568469 0.43416826 0.51778717
 0.39010757 0.38956131 0.41410747 0.44942164 0.48090008 0.48983772
 0.50579597 0.4466813  0.56569938 0.53375829 0.53558171 0.453583
 0.40753288 0.49942365 0.54623716 0.53256788 0.47753591 0.56532867
 0.37219502 0.42181522 0.44623769 0.51938337 0.44584103 0.45010092
 0.51244091 0.4398352  0.56103678 0.52457318 0.35752708 0.53120069
 0.54834514 0.48250997 0.54769048 0.36213197 0.47643744 0.46210719
 0.45220793 0.47760968 0.43937804 0.57477306 0.52223048 0.48236792
 0.46921138 0.46173492 0.43801484 0.49667988 0.6026343  0.50519037
 0.36737054 0.44248272 0.47222401 0.36217733 0.570813   0.39591425
 0.46029665 0.54197014 0.56886499 0.42406528]
for model  32 the mean error 0.483129647864801
all id 32 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 21 out win 6 err 0.483129647864801 time 8670.51211309433
Launcher: Job 33 completed in 8898 seconds.
Launcher: Task 144 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  41745
Epoch:0, Train loss:0.645765, valid loss:0.640951
Epoch:1, Train loss:0.038707, valid loss:0.004310
Epoch:2, Train loss:0.006749, valid loss:0.003022
Epoch:3, Train loss:0.005208, valid loss:0.002184
Epoch:4, Train loss:0.004435, valid loss:0.001938
Epoch:5, Train loss:0.003914, valid loss:0.001861
Epoch:6, Train loss:0.003597, valid loss:0.001841
Epoch:7, Train loss:0.003244, valid loss:0.001814
Epoch:8, Train loss:0.003018, valid loss:0.001521
Epoch:9, Train loss:0.002825, valid loss:0.001377
Epoch:10, Train loss:0.002593, valid loss:0.001323
Epoch:11, Train loss:0.002127, valid loss:0.001145
Epoch:12, Train loss:0.002060, valid loss:0.001248
Epoch:13, Train loss:0.002015, valid loss:0.001025
Epoch:14, Train loss:0.001938, valid loss:0.001070
Epoch:15, Train loss:0.001869, valid loss:0.001022
Epoch:16, Train loss:0.001825, valid loss:0.001021
Epoch:17, Train loss:0.001776, valid loss:0.001033
Epoch:18, Train loss:0.001729, valid loss:0.000924
Epoch:19, Train loss:0.001687, valid loss:0.000885
Epoch:20, Train loss:0.001646, valid loss:0.000917
Epoch:21, Train loss:0.001426, valid loss:0.000835
Epoch:22, Train loss:0.001411, valid loss:0.000797
Epoch:23, Train loss:0.001386, valid loss:0.000803
Epoch:24, Train loss:0.001378, valid loss:0.000807
Epoch:25, Train loss:0.001335, valid loss:0.000776
Epoch:26, Train loss:0.001333, valid loss:0.000800
Epoch:27, Train loss:0.001307, valid loss:0.000845
Epoch:28, Train loss:0.001296, valid loss:0.000766
Epoch:29, Train loss:0.001274, valid loss:0.000861
Epoch:30, Train loss:0.001267, valid loss:0.000761
Epoch:31, Train loss:0.001146, valid loss:0.000730
Epoch:32, Train loss:0.001141, valid loss:0.000726
Epoch:33, Train loss:0.001127, valid loss:0.000749
Epoch:34, Train loss:0.001120, valid loss:0.000686
Epoch:35, Train loss:0.001117, valid loss:0.000752
Epoch:36, Train loss:0.001110, valid loss:0.000711
Epoch:37, Train loss:0.001097, valid loss:0.000714
Epoch:38, Train loss:0.001094, valid loss:0.000707
Epoch:39, Train loss:0.001084, valid loss:0.000725
Epoch:40, Train loss:0.001075, valid loss:0.000741
Epoch:41, Train loss:0.001020, valid loss:0.000686
Epoch:42, Train loss:0.001018, valid loss:0.000709
Epoch:43, Train loss:0.001008, valid loss:0.000692
Epoch:44, Train loss:0.001009, valid loss:0.000698
Epoch:45, Train loss:0.001003, valid loss:0.000679
Epoch:46, Train loss:0.000999, valid loss:0.000703
Epoch:47, Train loss:0.000998, valid loss:0.000711
Epoch:48, Train loss:0.000995, valid loss:0.000678
Epoch:49, Train loss:0.000983, valid loss:0.000694
Epoch:50, Train loss:0.000985, valid loss:0.000682
Epoch:51, Train loss:0.000946, valid loss:0.000671
Epoch:52, Train loss:0.000941, valid loss:0.000672
Epoch:53, Train loss:0.000939, valid loss:0.000661
Epoch:54, Train loss:0.000938, valid loss:0.000678
Epoch:55, Train loss:0.000937, valid loss:0.000669
Epoch:56, Train loss:0.000938, valid loss:0.000667
Epoch:57, Train loss:0.000937, valid loss:0.000671
Epoch:58, Train loss:0.000937, valid loss:0.000680
Epoch:59, Train loss:0.000937, valid loss:0.000682
Epoch:60, Train loss:0.000935, valid loss:0.000664
training time 8704.145343780518
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.26077308282619266
plot_id,batch_id 0 1 miss% 0.3381698947310613
plot_id,batch_id 0 2 miss% 0.3832106829164495
plot_id,batch_id 0 3 miss% 0.26735829626146623
plot_id,batch_id 0 4 miss% 0.28883923429409525
plot_id,batch_id 0 5 miss% 0.2615430896488242
plot_id,batch_id 0 6 miss% 0.29955786873926965
plot_id,batch_id 0 7 miss% 0.45055007732083474
plot_id,batch_id 0 8 miss% 0.5757411274891328
plot_id,batch_id 0 9 miss% 0.4439684607112477
plot_id,batch_id 0 10 miss% 0.23055811194767778
plot_id,batch_id 0 11 miss% 0.35143729152043857
plot_id,batch_id 0 12 miss% 0.3306881546598947
plot_id,batch_id 0 13 miss% 0.30148572257710915
plot_id,batch_id 0 14 miss% 0.4162477658196334
plot_id,batch_id 0 15 miss% 0.22175736466883725
plot_id,batch_id 0 16 miss% 0.36096643596375005
plot_id,batch_id 0 17 miss% 0.39719902078772895
plot_id,batch_id 0 18 miss% 0.40244549428445514
plot_id,batch_id 0 19 miss% 0.3948412005234832
plot_id,batch_id 0 20 miss% 0.34763106451559195
plot_id,batch_id 0 21 miss% 0.46941543812151576
plot_id,batch_id 0 22 miss% 0.43378452895504027
plot_id,batch_id 0 23 miss% 0.36034929440029273
plot_id,batch_id 0 24 miss% 0.26558805522201684
plot_id,batch_id 0 25 miss% 0.28200642776247326
plot_id,batch_id 0 26 miss% 0.3974433348211677
plot_id,batch_id 0 27 miss% 0.386066672790459
plot_id,batch_id 0 28 miss% 0.40094306959356946
plot_id,batch_id 0 29 miss% 0.33332916281535524
plot_id,batch_id 0 30 miss% 0.21499760077056937
plot_id,batch_id 0 31 miss% 0.3381902675162978
plot_id,batch_id 0 32 miss% 0.43541888495491077
plot_id,batch_id 0 33 miss% 0.40015348212704477
plot_id,batch_id 0 34 miss% 0.3745678938754471
plot_id,batch_id 0 35 miss% 0.19001479619845094
plot_id,batch_id 0 36 miss% 0.40600259962843643
plot_id,batch_id 0 37 miss% 0.43622130772371726
plot_id,batch_id 0 38 miss% 0.3364698409493359
plot_id,batch_id 0 39 miss% 0.40376543071038
plot_id,batch_id 0 40 miss% 0.3327439350001696
plot_id,batch_id 0 41 miss% 0.2412976512536028
plot_id,batch_id 0 42 miss% 0.29000394938618523
plot_id,batch_id 0 43 miss% 0.3792087165850381
plot_id,batch_id 0 44 miss% 0.2201591042554125
plot_id,batch_id 0 45 miss% 0.24375580473161318
plot_id,batch_id 0 46 miss% 0.27944063692690857
plot_id,batch_id 0 47 miss% 0.3108517279087154
plot_id,batch_id 0 48 miss% 0.25725669703040593
plot_id,batch_id 0 49 miss% 0.19392839525917976
plot_id,batch_id 0 50 miss% 0.4311855770679927
plot_id,batch_id 0 51 miss% 0.3892969490898091
plot_id,batch_id 0 52 miss% 0.32776836362072825
plot_id,batch_id 0 53 miss% 0.27204974660303133
plot_id,batch_id 0 54 miss% 0.3058361798346461
plot_id,batch_id 0 55 miss% 0.37129526014333714
plot_id,batch_id 0 56 miss% 0.4315224053368155
plot_id,batch_id 0 57 miss% 0.36240175761441207
plot_id,batch_id 0 58 miss% 0.349882461878832
plot_id,batch_id 0 59 miss% 0.32851994759163494
plot_id,batch_id 0 60 miss% 0.2196674363583129
plot_id,batch_id 0 61 miss% 0.21690852741086808
plot_id,batch_id 0 62 miss% 0.42153805904913183
plot_id,batch_id 0 63 miss% 0.35032328747806035
plot_id,batch_id 0 64 miss% 0.3431940226946135
plot_id,batch_id 0 65 miss% 0.2055289703105828
plot_id,batch_id 0 66 miss% 0.34871872418460265
plot_id,batch_id 0 67 miss% 0.2172434187964151
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  55697
Epoch:0, Train loss:0.777853, valid loss:0.738563
Epoch:1, Train loss:0.264246, valid loss:0.005865
Epoch:2, Train loss:0.012557, valid loss:0.004265
Epoch:3, Train loss:0.010228, valid loss:0.003750
Epoch:4, Train loss:0.009231, valid loss:0.003598
Epoch:5, Train loss:0.008686, valid loss:0.003217
Epoch:6, Train loss:0.008159, valid loss:0.003084
Epoch:7, Train loss:0.007042, valid loss:0.002880
Epoch:8, Train loss:0.006490, valid loss:0.002785
Epoch:9, Train loss:0.006091, valid loss:0.002310
Epoch:10, Train loss:0.004981, valid loss:0.001878
Epoch:11, Train loss:0.004343, valid loss:0.001758
Epoch:12, Train loss:0.004207, valid loss:0.001604
Epoch:13, Train loss:0.004102, valid loss:0.001822
Epoch:14, Train loss:0.004037, valid loss:0.001681
Epoch:15, Train loss:0.003892, valid loss:0.001503
Epoch:16, Train loss:0.003809, valid loss:0.001557
Epoch:17, Train loss:0.003211, valid loss:0.001149
Epoch:18, Train loss:0.002322, valid loss:0.001143
Epoch:19, Train loss:0.002248, valid loss:0.001282
Epoch:20, Train loss:0.002174, valid loss:0.001305
Epoch:21, Train loss:0.001835, valid loss:0.001042
Epoch:22, Train loss:0.001800, valid loss:0.001086
Epoch:23, Train loss:0.001767, valid loss:0.001016
Epoch:24, Train loss:0.001753, valid loss:0.000995
Epoch:25, Train loss:0.001725, valid loss:0.001075
Epoch:26, Train loss:0.001697, valid loss:0.001007
Epoch:27, Train loss:0.001640, valid loss:0.000963
Epoch:28, Train loss:0.001634, valid loss:0.000992
Epoch:29, Train loss:0.001593, valid loss:0.000951
Epoch:30, Train loss:0.001576, valid loss:0.001086
Epoch:31, Train loss:0.001433, valid loss:0.000857
Epoch:32, Train loss:0.001430, valid loss:0.000867
Epoch:33, Train loss:0.001402, valid loss:0.000874
Epoch:34, Train loss:0.001390, valid loss:0.000903
Epoch:35, Train loss:0.001372, valid loss:0.000883
Epoch:36, Train loss:0.001370, valid loss:0.000884
Epoch:37, Train loss:0.001361, valid loss:0.000813
Epoch:38, Train loss:0.001329, valid loss:0.000824
Epoch:39, Train loss:0.001329, valid loss:0.000836
Epoch:40, Train loss:0.001317, valid loss:0.000832
Epoch:41, Train loss:0.001241, valid loss:0.000836
Epoch:42, Train loss:0.001231, valid loss:0.000842
Epoch:43, Train loss:0.001225, valid loss:0.000828
Epoch:44, Train loss:0.001223, valid loss:0.000796
Epoch:45, Train loss:0.001216, valid loss:0.000793
Epoch:46, Train loss:0.001205, valid loss:0.000842
Epoch:47, Train loss:0.001203, valid loss:0.000798
Epoch:48, Train loss:0.001194, valid loss:0.000784
Epoch:49, Train loss:0.001191, valid loss:0.000817
Epoch:50, Train loss:0.001188, valid loss:0.000800
Epoch:51, Train loss:0.001145, valid loss:0.000768
Epoch:52, Train loss:0.001138, valid loss:0.000758
Epoch:53, Train loss:0.001135, valid loss:0.000763
Epoch:54, Train loss:0.001133, valid loss:0.000791
Epoch:55, Train loss:0.001133, valid loss:0.000764
Epoch:56, Train loss:0.001131, valid loss:0.000806
Epoch:57, Train loss:0.001132, valid loss:0.000760
Epoch:58, Train loss:0.001130, valid loss:0.000766
Epoch:59, Train loss:0.001130, valid loss:0.000770
Epoch:60, Train loss:0.001129, valid loss:0.000762
training time 8735.45194196701
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07463729770347592
plot_id,batch_id 0 1 miss% 0.05079945112880189
plot_id,batch_id 0 2 miss% 0.11637160954241683
plot_id,batch_id 0 3 miss% 0.09760709000780268
plot_id,batch_id 0 4 miss% 0.07002645973870451
plot_id,batch_id 0 5 miss% 0.07364250741815376
plot_id,batch_id 0 6 miss% 0.04284429548533892
plot_id,batch_id 0 7 miss% 0.13390072555039648
plot_id,batch_id 0 8 miss% 0.09565606297982203
plot_id,batch_id 0 9 miss% 0.08360551809417775
plot_id,batch_id 0 10 miss% 0.06681704080419713
plot_id,batch_id 0 11 miss% 0.09355820335481713
plot_id,batch_id 0 12 miss% 0.07874612740763653
plot_id,batch_id 0 13 miss% 0.06291984729880024
plot_id,batch_id 0 14 miss% 0.12535981303472044
plot_id,batch_id 0 15 miss% 0.046693028549605914
plot_id,batch_id 0 16 miss% 0.14084969929898605
plot_id,batch_id 0 17 miss% 0.04205532289094424
plot_id,batch_id 0 18 miss% 0.08613056577699567
plot_id,batch_id 0 19 miss% 0.11342314655915937
plot_id,batch_id 0 20 miss% 0.0859749787436897
plot_id,batch_id 0 21 miss% 0.06202362714770032
plot_id,batch_id 0 22 miss% 0.07680044177458822
plot_id,batch_id 0 23 miss% 0.05296863525379681
plot_id,batch_id 0 24 miss% 0.05151907083120464
plot_id,batch_id 0 25 miss% 0.046316784839101593
plot_id,batch_id 0 26 miss% 0.07674431445934471
plot_id,batch_id 0 27 miss% 0.07627193667836271
plot_id,batch_id 0 28 miss% 0.045171905705353614
plot_id,batch_id 0 29 miss% 0.02475171024262944
plot_id,batch_id 0 30 miss% 0.03363797364271704
plot_id,batch_id 0 31 miss% 0.1133152493078453
plot_id,batch_id 0 32 miss% 0.13112719882360624
plot_id,batch_id 0 33 miss% 0.07899138432797656
plot_id,batch_id 0 34 miss% 0.046432258122741085
plot_id,batch_id 0 35 miss% 0.052404577698965046
plot_id,batch_id 0 36 miss% 0.09312455269049366
plot_id,batch_id 0 37 miss% 0.13035582251722896
plot_id,batch_id 0 38 miss% 0.04408190270578222
plot_id,batch_id 0 39 miss% 0.06252667141701529
plot_id,batch_id 0 40 miss% 0.09276428872141416
plot_id,batch_id 0 41 miss% 0.056170912170689924
plot_id,batch_id 0 42 miss% 0.05890007243474529
plot_id,batch_id 0 43 miss% 0.06167538392418251
plot_id,batch_id 0 44 miss% 0.06409994428036114
plot_id,batch_id 0 45 miss% 0.06608257710412739
plot_id,batch_id 0 46 miss% 0.04462628433092304
plot_id,batch_id 0 47 miss% 0.04899935121532832
plot_id,batch_id 0 48 miss% 0.06663550148185043
plot_id,batch_id 0 49 miss% 0.09945838921890054
plot_id,batch_id 0 50 miss% 0.09382713489214546
plot_id,batch_id 0 51 miss% 0.07907382965315385
plot_id,batch_id 0 52 miss% 0.0687848869966639
plot_id,batch_id 0 53 miss% 0.05916091499511493
plot_id,batch_id 0 54 miss% 0.033749227011882266
plot_id,batch_id 0 55 miss% 0.10923170105193254
plot_id,batch_id 0 56 miss% 0.09785658249736018
plot_id,batch_id 0 57 miss% 0.09456707964652805
plot_id,batch_id 0 58 miss% 0.05504160811738452
plot_id,batch_id 0 59 miss% 0.057165474175652285
plot_id,batch_id 0 60 miss% 0.04500569593405248
plot_id,batch_id 0 61 miss% 0.03363330177847746
plot_id,batch_id 0 62 miss% 0.19936494886434256
plot_id,batch_id 0 63 miss% 0.07887660900490075
plot_id,batch_id 0 64 miss% 0.05642655090409732
plot_id,batch_id 0 65 miss% 0.14032040479530558
plot_id,batch_id 0 66 miss% 0.05908142774290071
plot_id,batch_id 0 67 miss% 0.0402574182006364
plot_id,batch_id 0 68 miss% plot_id,batch_id 0 68 miss% 0.3319598559622198
plot_id,batch_id 0 69 miss% 0.35698536219799915
plot_id,batch_id 0 70 miss% 0.21096407460774744
plot_id,batch_id 0 71 miss% 0.22722862780753156
plot_id,batch_id 0 72 miss% 0.39023515529061026
plot_id,batch_id 0 73 miss% 0.27105446217921464
plot_id,batch_id 0 74 miss% 0.3180023885683716
plot_id,batch_id 0 75 miss% 0.13037903879774218
plot_id,batch_id 0 76 miss% 0.20396194953579452
plot_id,batch_id 0 77 miss% 0.24217468247304202
plot_id,batch_id 0 78 miss% 0.2806396302612923
plot_id,batch_id 0 79 miss% 0.3567339077379266
plot_id,batch_id 0 80 miss% 0.2538559109179314
plot_id,batch_id 0 81 miss% 0.39163437081172514
plot_id,batch_id 0 82 miss% 0.309217880853088
plot_id,batch_id 0 83 miss% 0.361760024233946
plot_id,batch_id 0 84 miss% 0.3745260244121159
plot_id,batch_id 0 85 miss% 0.1798196915892647
plot_id,batch_id 0 86 miss% 0.26858932149992654
plot_id,batch_id 0 87 miss% 0.3202758949574175
plot_id,batch_id 0 88 miss% 0.3505648312340919
plot_id,batch_id 0 89 miss% 0.297538927105055
plot_id,batch_id 0 90 miss% 0.16915454350432343
plot_id,batch_id 0 91 miss% 0.2995104141810501
plot_id,batch_id 0 92 miss% 0.2817504332368255
plot_id,batch_id 0 93 miss% 0.26237485206433414
plot_id,batch_id 0 94 miss% 0.4124065719507772
plot_id,batch_id 0 95 miss% 0.1399626079180326
plot_id,batch_id 0 96 miss% 0.21491709170158163
plot_id,batch_id 0 97 miss% 0.32890352117870614
plot_id,batch_id 0 98 miss% 0.30630450746656185
plot_id,batch_id 0 99 miss% 0.31285508296066006
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.26077308 0.33816989 0.38321068 0.2673583  0.28883923 0.26154309
 0.29955787 0.45055008 0.57574113 0.44396846 0.23055811 0.35143729
 0.33068815 0.30148572 0.41624777 0.22175736 0.36096644 0.39719902
 0.40244549 0.3948412  0.34763106 0.46941544 0.43378453 0.36034929
 0.26558806 0.28200643 0.39744333 0.38606667 0.40094307 0.33332916
 0.2149976  0.33819027 0.43541888 0.40015348 0.37456789 0.1900148
 0.4060026  0.43622131 0.33646984 0.40376543 0.33274394 0.24129765
 0.29000395 0.37920872 0.2201591  0.2437558  0.27944064 0.31085173
 0.2572567  0.1939284  0.43118558 0.38929695 0.32776836 0.27204975
 0.30583618 0.37129526 0.43152241 0.36240176 0.34988246 0.32851995
 0.21966744 0.21690853 0.42153806 0.35032329 0.34319402 0.20552897
 0.34871872 0.21724342 0.33195986 0.35698536 0.21096407 0.22722863
 0.39023516 0.27105446 0.31800239 0.13037904 0.20396195 0.24217468
 0.28063963 0.35673391 0.25385591 0.39163437 0.30921788 0.36176002
 0.37452602 0.17981969 0.26858932 0.32027589 0.35056483 0.29753893
 0.16915454 0.29951041 0.28175043 0.26237485 0.41240657 0.13996261
 0.21491709 0.32890352 0.30630451 0.31285508]
for model  81 the mean error 0.31987466883741617
all id 81 hidden_dim 16 learning_rate 0.0025 num_layers 3 frames 25 out win 4 err 0.31987466883741617 time 8704.145343780518
Launcher: Job 82 completed in 8961 seconds.
Launcher: Task 50 done. Exiting.
0.059257538827011756
plot_id,batch_id 0 69 miss% 0.11345587133303414
plot_id,batch_id 0 70 miss% 0.10176123563521308
plot_id,batch_id 0 71 miss% 0.07881321466485278
plot_id,batch_id 0 72 miss% 0.0967085382578737
plot_id,batch_id 0 73 miss% 0.09245371890666589
plot_id,batch_id 0 74 miss% 0.10438681583505577
plot_id,batch_id 0 75 miss% 0.12476238312113914
plot_id,batch_id 0 76 miss% 0.0842498138698838
plot_id,batch_id 0 77 miss% 0.04535186543971889
plot_id,batch_id 0 78 miss% 0.02708038757315241
plot_id,batch_id 0 79 miss% 0.05648261187641381
plot_id,batch_id 0 80 miss% 0.06916096269918218
plot_id,batch_id 0 81 miss% 0.08896681863833522
plot_id,batch_id 0 82 miss% 0.0711764555680689
plot_id,batch_id 0 83 miss% 0.08521383937571504
plot_id,batch_id 0 84 miss% 0.09910147746560435
plot_id,batch_id 0 85 miss% 0.053482029074142755
plot_id,batch_id 0 86 miss% 0.031024993428743566
plot_id,batch_id 0 87 miss% 0.059411693638971794
plot_id,batch_id 0 88 miss% 0.10694181342861622
plot_id,batch_id 0 89 miss% 0.0842917576266244
plot_id,batch_id 0 90 miss% 0.04427444512845255
plot_id,batch_id 0 91 miss% 0.06488820817454109
plot_id,batch_id 0 92 miss% 0.05907499643475466
plot_id,batch_id 0 93 miss% 0.0559872078835585
plot_id,batch_id 0 94 miss% 0.06346063031356747
plot_id,batch_id 0 95 miss% 0.044178272818727805
plot_id,batch_id 0 96 miss% 0.04608816926029222
plot_id,batch_id 0 97 miss% 0.04389547025995507
plot_id,batch_id 0 98 miss% 0.06966070927424323
plot_id,batch_id 0 99 miss% 0.09033907919004801
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0746373  0.05079945 0.11637161 0.09760709 0.07002646 0.07364251
 0.0428443  0.13390073 0.09565606 0.08360552 0.06681704 0.0935582
 0.07874613 0.06291985 0.12535981 0.04669303 0.1408497  0.04205532
 0.08613057 0.11342315 0.08597498 0.06202363 0.07680044 0.05296864
 0.05151907 0.04631678 0.07674431 0.07627194 0.04517191 0.02475171
 0.03363797 0.11331525 0.1311272  0.07899138 0.04643226 0.05240458
 0.09312455 0.13035582 0.0440819  0.06252667 0.09276429 0.05617091
 0.05890007 0.06167538 0.06409994 0.06608258 0.04462628 0.04899935
 0.0666355  0.09945839 0.09382713 0.07907383 0.06878489 0.05916091
 0.03374923 0.1092317  0.09785658 0.09456708 0.05504161 0.05716547
 0.0450057  0.0336333  0.19936495 0.07887661 0.05642655 0.1403204
 0.05908143 0.04025742 0.05925754 0.11345587 0.10176124 0.07881321
 0.09670854 0.09245372 0.10438682 0.12476238 0.08424981 0.04535187
 0.02708039 0.05648261 0.06916096 0.08896682 0.07117646 0.08521384
 0.09910148 0.05348203 0.03102499 0.05941169 0.10694181 0.08429176
 0.04427445 0.06488821 0.059075   0.05598721 0.06346063 0.04417827
 0.04608817 0.04389547 0.06966071 0.09033908]
for model  9 the mean error 0.07456405335722312
all id 9 hidden_dim 16 learning_rate 0.0025 num_layers 4 frames 21 out win 4 err 0.07456405335722312 time 8735.45194196701
Launcher: Job 10 completed in 9013 seconds.
Launcher: Task 107 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  41745
Epoch:0, Train loss:0.586994, valid loss:0.568980
Epoch:1, Train loss:0.035669, valid loss:0.003949
Epoch:2, Train loss:0.006916, valid loss:0.002497
Epoch:3, Train loss:0.005125, valid loss:0.002011
Epoch:4, Train loss:0.004271, valid loss:0.001861
Epoch:5, Train loss:0.003828, valid loss:0.001787
Epoch:6, Train loss:0.003504, valid loss:0.001627
Epoch:7, Train loss:0.003255, valid loss:0.001603
Epoch:8, Train loss:0.003059, valid loss:0.001441
Epoch:9, Train loss:0.002891, valid loss:0.001368
Epoch:10, Train loss:0.002759, valid loss:0.001375
Epoch:11, Train loss:0.002175, valid loss:0.001115
Epoch:12, Train loss:0.002095, valid loss:0.001136
Epoch:13, Train loss:0.002048, valid loss:0.001252
Epoch:14, Train loss:0.002014, valid loss:0.001093
Epoch:15, Train loss:0.001988, valid loss:0.001076
Epoch:16, Train loss:0.001872, valid loss:0.001149
Epoch:17, Train loss:0.001882, valid loss:0.001219
Epoch:18, Train loss:0.001811, valid loss:0.001026
Epoch:19, Train loss:0.001755, valid loss:0.001177
Epoch:20, Train loss:0.001727, valid loss:0.001048
Epoch:21, Train loss:0.001439, valid loss:0.000885
Epoch:22, Train loss:0.001423, valid loss:0.000889
Epoch:23, Train loss:0.001408, valid loss:0.000866
Epoch:24, Train loss:0.001368, valid loss:0.000907
Epoch:25, Train loss:0.001345, valid loss:0.000878
Epoch:26, Train loss:0.001337, valid loss:0.000843
Epoch:27, Train loss:0.001305, valid loss:0.000843
Epoch:28, Train loss:0.001301, valid loss:0.000846
Epoch:29, Train loss:0.001277, valid loss:0.000835
Epoch:30, Train loss:0.001275, valid loss:0.000841
Epoch:31, Train loss:0.001102, valid loss:0.000807
Epoch:32, Train loss:0.001096, valid loss:0.000759
Epoch:33, Train loss:0.001083, valid loss:0.000732
Epoch:34, Train loss:0.001058, valid loss:0.000777
Epoch:35, Train loss:0.001091, valid loss:0.000711
Epoch:36, Train loss:0.001053, valid loss:0.000783
Epoch:37, Train loss:0.001038, valid loss:0.000723
Epoch:38, Train loss:0.001034, valid loss:0.000740
Epoch:39, Train loss:0.001036, valid loss:0.000758
Epoch:40, Train loss:0.001011, valid loss:0.000792
Epoch:41, Train loss:0.000940, valid loss:0.000697
Epoch:42, Train loss:0.000930, valid loss:0.000688
Epoch:43, Train loss:0.000929, valid loss:0.000689
Epoch:44, Train loss:0.000926, valid loss:0.000715
Epoch:45, Train loss:0.000924, valid loss:0.000692
Epoch:46, Train loss:0.000913, valid loss:0.000669
Epoch:47, Train loss:0.000915, valid loss:0.000689
Epoch:48, Train loss:0.000914, valid loss:0.000697
Epoch:49, Train loss:0.000902, valid loss:0.000694
Epoch:50, Train loss:0.000904, valid loss:0.000693
Epoch:51, Train loss:0.000852, valid loss:0.000670
Epoch:52, Train loss:0.000846, valid loss:0.000668
Epoch:53, Train loss:0.000843, valid loss:0.000665
Epoch:54, Train loss:0.000842, valid loss:0.000661
Epoch:55, Train loss:0.000840, valid loss:0.000662
Epoch:56, Train loss:0.000839, valid loss:0.000662
Epoch:57, Train loss:0.000839, valid loss:0.000660
Epoch:58, Train loss:0.000838, valid loss:0.000669
Epoch:59, Train loss:0.000837, valid loss:0.000663
Epoch:60, Train loss:0.000837, valid loss:0.000659
training time 8940.081202507019
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.43155900323224267
plot_id,batch_id 0 1 miss% 0.4121172323735984
plot_id,batch_id 0 2 miss% 0.47848337201995567
plot_id,batch_id 0 3 miss% 0.355719371771334
plot_id,batch_id 0 4 miss% 0.2906885526627545
plot_id,batch_id 0 5 miss% 0.46856723273194983
plot_id,batch_id 0 6 miss% 0.5105728502664734
plot_id,batch_id 0 7 miss% 0.4776136330584686
plot_id,batch_id 0 8 miss% 0.56469426141562
plot_id,batch_id 0 9 miss% 0.4242325125917418
plot_id,batch_id 0 10 miss% 0.34681568546145153
plot_id,batch_id 0 11 miss% 0.40793338834888204
plot_id,batch_id 0 12 miss% 0.40600682468039323
plot_id,batch_id 0 13 miss% 0.384898373136127
plot_id,batch_id 0 14 miss% 0.483603080963008
plot_id,batch_id 0 15 miss% 0.5573965612465297
plot_id,batch_id 0 16 miss% 0.5403342382256925
plot_id,batch_id 0 17 miss% 0.49417408882292874
plot_id,batch_id 0 18 miss% 0.5136603639433086
plot_id,batch_id 0 19 miss% 0.48676315033634415
plot_id,batch_id 0 20 miss% 0.4108798990020101
plot_id,batch_id 0 21 miss% 0.3651339508750817
plot_id,batch_id 0 22 miss% 0.39765633132826655
plot_id,batch_id 0 23 miss% 0.37242540072262204
plot_id,batch_id 0 24 miss% 0.3509402053824717
plot_id,batch_id 0 25 miss% 0.40142637154463734
plot_id,batch_id 0 26 miss% 0.40838216727064075
plot_id,batch_id 0 27 miss% 0.41712525559193286
plot_id,batch_id 0 28 miss% 0.468525099641881
plot_id,batch_id 0 29 miss% 0.37450042604815015
plot_id,batch_id 0 30 miss% 0.4330344159004258
plot_id,batch_id 0 31 miss% 0.5553621252519905
plot_id,batch_id 0 32 miss% 0.43304164938887624
plot_id,batch_id 0 33 miss% 0.4643645495956924
plot_id,batch_id 0 34 miss% 0.4783651638527684
plot_id,batch_id 0 35 miss% 0.41179010683663114
plot_id,batch_id 0 36 miss% 0.5639500299218891
plot_id,batch_id 0 37 miss% 0.40422326491675636
plot_id,batch_id 0 38 miss% 0.4877913614580305
plot_id,batch_id 0 39 miss% 0.41013637369578
plot_id,batch_id 0 40 miss% 0.3606639148578935
plot_id,batch_id 0 41 miss% 0.36093189295628886
plot_id,batch_id 0 42 miss% 0.3025416604170832
plot_id,batch_id 0 43 miss% 0.38837740098077833
plot_id,batch_id 0 44 miss% 0.33713758163276025
plot_id,batch_id 0 45 miss% 0.45149545540877
plot_id,batch_id 0 46 miss% 0.38049118335654397
plot_id,batch_id 0 47 miss% 0.4135367639269491
plot_id,batch_id 0 48 miss% 0.38671545833267934
plot_id,batch_id 0 49 miss% 0.3013085492941559
plot_id,batch_id 0 50 miss% 0.5812602877157858
plot_id,batch_id 0 51 miss% 0.5011229742180574
plot_id,batch_id 0 52 miss% 0.4472037955659397
plot_id,batch_id 0 53 miss% 0.35385742566209116
plot_id,batch_id 0 54 miss% 0.36818850531446157
plot_id,batch_id 0 55 miss% 0.47581161504371616
plot_id,batch_id 0 56 miss% 0.536086158844143
plot_id,batch_id 0 57 miss% 0.4517078478918828
plot_id,batch_id 0 58 miss% 0.478244369137088
plot_id,batch_id 0 59 miss% 0.4689606652364287
plot_id,batch_id 0 60 miss% 0.3268879241056929
plot_id,batch_id 0 61 miss% 0.34464474152132685
plot_id,batch_id 0 62 miss% 0.47430774383469415
plot_id,batch_id 0 63 miss% 0.411147404507724
plot_id,batch_id 0 64 miss% 0.42068948877866813
plot_id,batch_id 0 65 miss% 0.38155185512298695
plot_id,batch_id 0 66 miss% 0.47335411125856347
plot_id,batch_id 0 67 miss% 0.36579153590492947
plot_id,batch_id 0 68 miss% 0.5151025497039512
plot_id,batch_id 0 69 miss% 0.5296742788318329
plot_id,batch_id 0 70 miss% 0.3062521217265471
plot_id,batch_id 0 71 miss% 0.48602478427197643
plot_id,batch_id 0 72 miss% 0.45612095231222527
plot_id,batch_id 0 73 miss% 0.4149894813541958
plot_id,batch_id 0 74 miss% 0.49675777090522133
plot_id,batch_id 0 75 miss% 0.3449058806965464
plot_id,batch_id 0 76 miss% 0.3957445899872867
plot_id,batch_id 0 77 miss% 0.39303627763909604
plot_id,batch_id 0 78 miss% 0.39126417202314717
plot_id,batch_id 0 79 miss% 0.4604455168513804
plot_id,batch_id 0 80 miss% 0.36267678494544914
plot_id,batch_id 0 81 miss% 0.45376963417301114
plot_id,batch_id 0 82 miss% 0.43878893973746935
plot_id,batch_id 0 83 miss% 0.43349631357566604
plot_id,batch_id 0 84 miss% 0.3536218785175314
plot_id,batch_id 0 85 miss% 0.34031894134377527
plot_id,batch_id 0 86 miss% 0.42638749931349995
plot_id,batch_id 0 87 miss% 0.4430702342636229
plot_id,batch_id 0 88 miss% 0.48049564178771165
plot_id,batch_id 0 89 miss% 0.4265353811938361
plot_id,batch_id 0 90 miss% 0.3145662466415685
plot_id,batch_id 0 91 miss% 0.381292104433385
plot_id,batch_id 0 92 miss% 0.3903768844358581
plot_id,batch_id 0 93 miss% 0.37210398660800564
plot_id,batch_id 0 94 miss% 0.5496397690993257
plot_id,batch_id 0 95 miss% 0.34358609041304533
plot_id,batch_id 0 96 miss% 0.37123795947873506
plot_id,batch_id 0 97 miss% 0.4682194765124022
plot_id,batch_id 0 98 miss% 0.4873441855172841
plot_id,batch_id 0 99 miss% 0.42369560488995966
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.431559   0.41211723 0.47848337 0.35571937 0.29068855 0.46856723
 0.51057285 0.47761363 0.56469426 0.42423251 0.34681569 0.40793339
 0.40600682 0.38489837 0.48360308 0.55739656 0.54033424 0.49417409
 0.51366036 0.48676315 0.4108799  0.36513395 0.39765633 0.3724254
 0.35094021 0.40142637 0.40838217 0.41712526 0.4685251  0.37450043
 0.43303442 0.55536213 0.43304165 0.46436455 0.47836516 0.41179011
 0.56395003 0.40422326 0.48779136 0.41013637 0.36066391 0.36093189
 0.30254166 0.3883774  0.33713758 0.45149546 0.38049118 0.41353676
 0.38671546 0.30130855 0.58126029 0.50112297 0.4472038  0.35385743
 0.36818851 0.47581162 0.53608616 0.45170785 0.47824437 0.46896067
 0.32688792 0.34464474 0.47430774 0.4111474  0.42068949 0.38155186
 0.47335411 0.36579154 0.51510255 0.52967428 0.30625212 0.48602478
 0.45612095 0.41498948 0.49675777 0.34490588 0.39574459 0.39303628
 0.39126417 0.46044552 0.36267678 0.45376963 0.43878894 0.43349631
 0.35362188 0.34031894 0.4263875  0.44307023 0.48049564 0.42653538
 0.31456625 0.3812921  0.39037688 0.37210399 0.54963977 0.34358609
 0.37123796 0.46821948 0.48734419 0.4236956 ]
for model  110 the mean error 0.4264044816952797
all id 110 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 25 out win 6 err 0.4264044816952797 time 8940.081202507019
Launcher: Job 111 completed in 9176 seconds.
Launcher: Task 188 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  55697
Epoch:0, Train loss:0.777853, valid loss:0.738563
Epoch:1, Train loss:0.528504, valid loss:0.525638
Epoch:2, Train loss:0.516989, valid loss:0.524814
Epoch:3, Train loss:0.515830, valid loss:0.524560
Epoch:4, Train loss:0.515198, valid loss:0.524575
Epoch:5, Train loss:0.514712, valid loss:0.524337
Epoch:6, Train loss:0.514397, valid loss:0.524285
Epoch:7, Train loss:0.514033, valid loss:0.523928
Epoch:8, Train loss:0.513933, valid loss:0.523962
Epoch:9, Train loss:0.513821, valid loss:0.523888
Epoch:10, Train loss:0.513674, valid loss:0.524011
Epoch:11, Train loss:0.512883, valid loss:0.523584
Epoch:12, Train loss:0.512738, valid loss:0.523480
Epoch:13, Train loss:0.512657, valid loss:0.523564
Epoch:14, Train loss:0.512616, valid loss:0.523413
Epoch:15, Train loss:0.512596, valid loss:0.523477
Epoch:16, Train loss:0.512617, valid loss:0.523857
Epoch:17, Train loss:0.512471, valid loss:0.523658
Epoch:18, Train loss:0.512499, valid loss:0.523347
Epoch:19, Train loss:0.512383, valid loss:0.523436
Epoch:20, Train loss:0.512360, valid loss:0.523577
Epoch:21, Train loss:0.511974, valid loss:0.523210
Epoch:22, Train loss:0.511955, valid loss:0.523224
Epoch:23, Train loss:0.511932, valid loss:0.523323
Epoch:24, Train loss:0.511934, valid loss:0.523216
Epoch:25, Train loss:0.511892, valid loss:0.523244
Epoch:26, Train loss:0.511840, valid loss:0.523144
Epoch:27, Train loss:0.511855, valid loss:0.523149
Epoch:28, Train loss:0.511829, valid loss:0.523204
Epoch:29, Train loss:0.511822, valid loss:0.523164
Epoch:30, Train loss:0.511780, valid loss:0.523328
Epoch:31, Train loss:0.511632, valid loss:0.523087
Epoch:32, Train loss:0.511594, valid loss:0.523101
Epoch:33, Train loss:0.511584, valid loss:0.523122
Epoch:34, Train loss:0.511591, valid loss:0.523126
Epoch:35, Train loss:0.511551, valid loss:0.523135
Epoch:36, Train loss:0.511565, valid loss:0.523103
Epoch:37, Train loss:0.511558, valid loss:0.523095
Epoch:38, Train loss:0.511539, valid loss:0.523159
Epoch:39, Train loss:0.511582, valid loss:0.523119
Epoch:40, Train loss:0.511528, valid loss:0.523152
Epoch:41, Train loss:0.511443, valid loss:0.523067
Epoch:42, Train loss:0.511434, valid loss:0.523076
Epoch:43, Train loss:0.511424, valid loss:0.523096
Epoch:44, Train loss:0.511418, valid loss:0.523052
Epoch:45, Train loss:0.511422, valid loss:0.523067
Epoch:46, Train loss:0.511408, valid loss:0.523069
Epoch:47, Train loss:0.511414, valid loss:0.523091
Epoch:48, Train loss:0.511406, valid loss:0.523039
Epoch:49, Train loss:0.511401, valid loss:0.523111
Epoch:50, Train loss:0.511399, valid loss:0.523064
Epoch:51, Train loss:0.511358, valid loss:0.523052
Epoch:52, Train loss:0.511351, valid loss:0.523056
Epoch:53, Train loss:0.511348, valid loss:0.523047
Epoch:54, Train loss:0.511346, valid loss:0.523049
Epoch:55, Train loss:0.511345, valid loss:0.523047
Epoch:56, Train loss:0.511344, valid loss:0.523049
Epoch:57, Train loss:0.511342, valid loss:0.523049
Epoch:58, Train loss:0.511342, valid loss:0.523049
Epoch:59, Train loss:0.511341, valid loss:0.523050
Epoch:60, Train loss:0.511341, valid loss:0.523047
training time 9057.84750676155
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.8727936542681987
plot_id,batch_id 0 1 miss% 0.8402043322830282
plot_id,batch_id 0 2 miss% 0.8362954555431842
plot_id,batch_id 0 3 miss% 0.8306346577823123
plot_id,batch_id 0 4 miss% 0.8283479451628631
plot_id,batch_id 0 5 miss% 0.8775409022617494
plot_id,batch_id 0 6 miss% 0.8440936270776153
plot_id,batch_id 0 7 miss% 0.8365889057553437
plot_id,batch_id 0 8 miss% 0.8365515448171831
plot_id,batch_id 0 9 miss% 0.8380474796287061
plot_id,batch_id 0 10 miss% 0.8752200187209112
plot_id,batch_id 0 11 miss% 0.8436083985007996
plot_id,batch_id 0 12 miss% 0.8348315036298514
plot_id,batch_id 0 13 miss% 0.8343858772101747
plot_id,batch_id 0 14 miss% 0.8324413728767193
plot_id,batch_id 0 15 miss% 0.8766686204706938
plot_id,batch_id 0 16 miss% 0.846040253772906
plot_id,batch_id 0 17 miss% 0.8351869295207172
plot_id,batch_id 0 18 miss% 0.835499105181464
plot_id,batch_id 0 19 miss% 0.8365079722750507
plot_id,batch_id 0 20 miss% 0.859103794193547
plot_id,batch_id 0 21 miss% 0.8366051454260488
plot_id,batch_id 0 22 miss% 0.8399218014836242
plot_id,batch_id 0 23 miss% 0.8286751938184116
plot_id,batch_id 0 24 miss% 0.8283895858718887
plot_id,batch_id 0 25 miss% 0.8534626613231739
plot_id,batch_id 0 26 miss% 0.8409832142120998
plot_id,batch_id 0 27 miss% 0.8336947068808764
plot_id,batch_id 0 28 miss% 0.8312627357225655
plot_id,batch_id 0 29 miss% 0.837695665343552
plot_id,batch_id 0 30 miss% 0.8562408603020447
plot_id,batch_id 0 31 miss% 0.8350085131933879
plot_id,batch_id 0 32 miss% 0.8307048035227931
plot_id,batch_id 0 33 miss% 0.8360013468148251
plot_id,batch_id 0 34 miss% 0.8279420962958437
plot_id,batch_id 0 35 miss% 0.8567774777768378
plot_id,batch_id 0 36 miss% 0.8332792832468058
plot_id,batch_id 0 37 miss% 0.8584692620949413
plot_id,batch_id 0 38 miss% 0.8323232398026305
plot_id,batch_id 0 39 miss% 0.830780266621281
plot_id,batch_id 0 40 miss% 0.8460852975245282
plot_id,batch_id 0 41 miss% 0.8300884669365021
plot_id,batch_id 0 42 miss% 0.8289110793462855
plot_id,batch_id 0 43 miss% 0.8256914089681533
plot_id,batch_id 0 44 miss% 0.8288661564134311
plot_id,batch_id 0 45 miss% 0.8406973716596807
plot_id,batch_id 0 46 miss% 0.8326544785935582
plot_id,batch_id 0 47 miss% 0.829080185373826
plot_id,batch_id 0 48 miss% 0.8261679385615621
plot_id,batch_id 0 49 miss% 0.8271487926966866
plot_id,batch_id 0 50 miss% 0.844925606455167
plot_id,batch_id 0 51 miss% 0.8455485630674691
plot_id,batch_id 0 52 miss% 0.8308103029182925
plot_id,batch_id 0 53 miss% 0.8277315148760137
plot_id,batch_id 0 54 miss% 0.8303252833558423
plot_id,batch_id 0 55 miss% 0.8418547790281289
plot_id,batch_id 0 56 miss% 0.8352747955834465
plot_id,batch_id 0 57 miss% 0.8313029218777559
plot_id,batch_id 0 58 miss% 0.8286665816983403
plot_id,batch_id 0 59 miss% 0.8302011215454167
plot_id,batch_id 0 60 miss% 0.8935535791329352
plot_id,batch_id 0 61 miss% 0.8552358881108972
plot_id,batch_id 0 62 miss% 0.8465526899100466
plot_id,batch_id 0 63 miss% 0.8381106097884268
plot_id,batch_id 0 64 miss% 0.836175369868769
plot_id,batch_id 0 65 miss% 0.9009370805536283
plot_id,batch_id 0 66 miss% 0.8573631680154492
plot_id,batch_id 0 67 miss% 0.8461085343090066
plot_id,batch_id 0 68 miss% 0.8385771633883328
plot_id,batch_id 0 69 miss% 0.8348600617742069
plot_id,batch_id 0 70 miss% 0.8958383777059338
plot_id,batch_id 0 71 miss% 0.8644479555233375
plot_id,batch_id 0 72 miss% 0.844024588417605
plot_id,batch_id 0 73 miss% 0.841562163706597
plot_id,batch_id 0 74 miss% 0.838309072339245
plot_id,batch_id 0 75 miss% 0.899318163989272
plot_id,batch_id 0 76 miss% 0.8575146038423517
plot_id,batch_id 0 77 miss% 0.8454829393338832
plot_id,batch_id 0 78 miss% 0.8396290210315491
plot_id,batch_id 0 79 miss% 0.8392133142011785
plot_id,batch_id 0 80 miss% 0.8895200611594752
plot_id,batch_id 0 81 miss% 0.8492419037752151
plot_id,batch_id 0 82 miss% 0.8427747642813517
plot_id,batch_id 0 83 miss% 0.8351764263061722
plot_id,batch_id 0 84 miss% 0.8367345711176764
plot_id,batch_id 0 85 miss% 0.8852941356283999
plot_id,batch_id 0 86 miss% 0.8450249054426755
plot_id,batch_id 0 87 miss% 0.8403341926032433
plot_id,batch_id 0 88 miss% 0.8350597300470658
plot_id,batch_id 0 89 miss% 0.8331719540095842
plot_id,batch_id 0 90 miss% 0.8949628777465314
plot_id,batch_id 0 91 miss% 0.8498393874531802
plot_id,batch_id 0 92 miss% 0.8394892744810313
plot_id,batch_id 0 93 miss% 0.8362284750265384
plot_id,batch_id 0 94 miss% 0.8356135391230585
plot_id,batch_id 0 95 miss% 0.8942310411893474
plot_id,batch_id 0 96 miss% 0.8530250835804333
plot_id,batch_id 0 97 miss% 0.8409959531972143
plot_id,batch_id 0 98 miss% 0.834975472554963
plot_id,batch_id 0 99 miss% 0.8349316889104359
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.87279365 0.84020433 0.83629546 0.83063466 0.82834795 0.8775409
 0.84409363 0.83658891 0.83655154 0.83804748 0.87522002 0.8436084
 0.8348315  0.83438588 0.83244137 0.87666862 0.84604025 0.83518693
 0.83549911 0.83650797 0.85910379 0.83660515 0.8399218  0.82867519
 0.82838959 0.85346266 0.84098321 0.83369471 0.83126274 0.83769567
 0.85624086 0.83500851 0.8307048  0.83600135 0.8279421  0.85677748
 0.83327928 0.85846926 0.83232324 0.83078027 0.8460853  0.83008847
 0.82891108 0.82569141 0.82886616 0.84069737 0.83265448 0.82908019
 0.82616794 0.82714879 0.84492561 0.84554856 0.8308103  0.82773151
 0.83032528 0.84185478 0.8352748  0.83130292 0.82866658 0.83020112
 0.89355358 0.85523589 0.84655269 0.83811061 0.83617537 0.90093708
 0.85736317 0.84610853 0.83857716 0.83486006 0.89583838 0.86444796
 0.84402459 0.84156216 0.83830907 0.89931816 0.8575146  0.84548294
 0.83962902 0.83921331 0.88952006 0.8492419  0.84277476 0.83517643
 0.83673457 0.88529414 0.84502491 0.84033419 0.83505973 0.83317195
 0.89496288 0.84983939 0.83948927 0.83622848 0.83561354 0.89423104
 0.85302508 0.84099595 0.83497547 0.83493169]
for model  63 the mean error 0.8446028263974296
all id 63 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 21 out win 4 err 0.8446028263974296 time 9057.84750676155
Launcher: Job 64 completed in 9194 seconds.
Launcher: Task 138 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  41745
Epoch:0, Train loss:0.586994, valid loss:0.568980
Epoch:1, Train loss:0.032061, valid loss:0.003889
Epoch:2, Train loss:0.008676, valid loss:0.002849
Epoch:3, Train loss:0.007161, valid loss:0.002338
Epoch:4, Train loss:0.005669, valid loss:0.002190
Epoch:5, Train loss:0.004378, valid loss:0.001787
Epoch:6, Train loss:0.003573, valid loss:0.001608
Epoch:7, Train loss:0.003359, valid loss:0.001553
Epoch:8, Train loss:0.003222, valid loss:0.001518
Epoch:9, Train loss:0.003090, valid loss:0.001492
Epoch:10, Train loss:0.002956, valid loss:0.001341
Epoch:11, Train loss:0.002239, valid loss:0.001118
Epoch:12, Train loss:0.002197, valid loss:0.001161
Epoch:13, Train loss:0.002105, valid loss:0.001064
Epoch:14, Train loss:0.002060, valid loss:0.001192
Epoch:15, Train loss:0.002019, valid loss:0.001091
Epoch:16, Train loss:0.001962, valid loss:0.001088
Epoch:17, Train loss:0.001895, valid loss:0.001224
Epoch:18, Train loss:0.001874, valid loss:0.001087
Epoch:19, Train loss:0.001803, valid loss:0.001037
Epoch:20, Train loss:0.001787, valid loss:0.001275
Epoch:21, Train loss:0.001423, valid loss:0.000862
Epoch:22, Train loss:0.001385, valid loss:0.000822
Epoch:23, Train loss:0.001371, valid loss:0.000875
Epoch:24, Train loss:0.001377, valid loss:0.000895
Epoch:25, Train loss:0.001326, valid loss:0.000835
Epoch:26, Train loss:0.001315, valid loss:0.000810
Epoch:27, Train loss:0.001289, valid loss:0.000822
Epoch:28, Train loss:0.001279, valid loss:0.000788
Epoch:29, Train loss:0.001273, valid loss:0.000794
Epoch:30, Train loss:0.001229, valid loss:0.000776
Epoch:31, Train loss:0.001087, valid loss:0.000806
Epoch:32, Train loss:0.001055, valid loss:0.000743
Epoch:33, Train loss:0.001037, valid loss:0.000693
Epoch:34, Train loss:0.001029, valid loss:0.000695
Epoch:35, Train loss:0.001028, valid loss:0.000709
Epoch:36, Train loss:0.001013, valid loss:0.000735
Epoch:37, Train loss:0.001012, valid loss:0.000805
Epoch:38, Train loss:0.000999, valid loss:0.000743
Epoch:39, Train loss:0.000986, valid loss:0.000722
Epoch:40, Train loss:0.000982, valid loss:0.000780
Epoch:41, Train loss:0.000888, valid loss:0.000700
Epoch:42, Train loss:0.000882, valid loss:0.000685
Epoch:43, Train loss:0.000870, valid loss:0.000677
Epoch:44, Train loss:0.000875, valid loss:0.000676
Epoch:45, Train loss:0.000867, valid loss:0.000670
Epoch:46, Train loss:0.000860, valid loss:0.000666
Epoch:47, Train loss:0.000856, valid loss:0.000672
Epoch:48, Train loss:0.000854, valid loss:0.000679
Epoch:49, Train loss:0.000853, valid loss:0.000682
Epoch:50, Train loss:0.000843, valid loss:0.000677
Epoch:51, Train loss:0.000790, valid loss:0.000657
Epoch:52, Train loss:0.000785, valid loss:0.000662
Epoch:53, Train loss:0.000782, valid loss:0.000654
Epoch:54, Train loss:0.000781, valid loss:0.000657
Epoch:55, Train loss:0.000780, valid loss:0.000655
Epoch:56, Train loss:0.000779, valid loss:0.000656
Epoch:57, Train loss:0.000778, valid loss:0.000656
Epoch:58, Train loss:0.000777, valid loss:0.000657
Epoch:59, Train loss:0.000777, valid loss:0.000659
Epoch:60, Train loss:0.000776, valid loss:0.000653
training time 9058.120113611221
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.11074261843998175
plot_id,batch_id 0 1 miss% 0.07235327604625048
plot_id,batch_id 0 2 miss% 0.12174616809419468
plot_id,batch_id 0 3 miss% 0.0801963877240515
plot_id,batch_id 0 4 miss% 0.07917085837591449
plot_id,batch_id 0 5 miss% 0.09145582294160458
plot_id,batch_id 0 6 miss% 0.18881747745953195
plot_id,batch_id 0 7 miss% 0.1702228835080973
plot_id,batch_id 0 8 miss% 0.14081071596042027
plot_id,batch_id 0 9 miss% 0.08828753052788678
plot_id,batch_id 0 10 miss% 0.0836866295199623
plot_id,batch_id 0 11 miss% 0.05931241401516673
plot_id,batch_id 0 12 miss% 0.1577665554986693
plot_id,batch_id 0 13 miss% 0.13062483893133794
plot_id,batch_id 0 14 miss% 0.15982015625896576
plot_id,batch_id 0 15 miss% 0.07494763158340144
plot_id,batch_id 0 16 miss% 0.1872473791763173
plot_id,batch_id 0 17 miss% 0.0647240185738296
plot_id,batch_id 0 18 miss% 0.07578506102611295
plot_id,batch_id 0 19 miss% 0.12444854931915242
plot_id,batch_id 0 20 miss% 0.07504230175466647
plot_id,batch_id 0 21 miss% 0.0780219213481308
plot_id,batch_id 0 22 miss% 0.1254698766137597
plot_id,batch_id 0 23 miss% 0.04875985785725234
plot_id,batch_id 0 24 miss% 0.10433379466109684
plot_id,batch_id 0 25 miss% 0.07390949408456639
plot_id,batch_id 0 26 miss% 0.06613818677120756
plot_id,batch_id 0 27 miss% 0.049374239659989966
plot_id,batch_id 0 28 miss% 0.05350929037874001
plot_id,batch_id 0 29 miss% 0.042702106281533175
plot_id,batch_id 0 30 miss% 0.158063158465852
plot_id,batch_id 0 31 miss% 0.15867307541657688
plot_id,batch_id 0 32 miss% 0.11675052267075274
plot_id,batch_id 0 33 miss% 0.07596037589508654
plot_id,batch_id 0 34 miss% 0.061833898450548874
plot_id,batch_id 0 35 miss% 0.1663580124729333
plot_id,batch_id 0 36 miss% 0.1853374060322335
plot_id,batch_id 0 37 miss% 0.06863182401143592
plot_id,batch_id 0 38 miss% 0.07870226066334877
plot_id,batch_id 0 39 miss% 0.050392925817139715
plot_id,batch_id 0 40 miss% 0.20230151941145064
plot_id,batch_id 0 41 miss% 0.06178596523581855
plot_id,batch_id 0 42 miss% 0.06854348810870384
plot_id,batch_id 0 43 miss% 0.11604860999782218
plot_id,batch_id 0 44 miss% 0.058818333080870505
plot_id,batch_id 0 45 miss% 0.13891539580869375
plot_id,batch_id 0 46 miss% 0.04102058165480714
plot_id,batch_id 0 47 miss% 0.0503133059316711
plot_id,batch_id 0 48 miss% 0.04281750925108107
plot_id,batch_id 0 49 miss% 0.044220656843958275
plot_id,batch_id 0 50 miss% 0.1630399816961614
plot_id,batch_id 0 51 miss% 0.03155246176509453
plot_id,batch_id 0 52 miss% 0.05632212964369252
plot_id,batch_id 0 53 miss% 0.048883123789087594
plot_id,batch_id 0 54 miss% 0.05036854536468821
plot_id,batch_id 0 55 miss% 0.21677281146961513
plot_id,batch_id 0 56 miss% 0.1145644991573035
plot_id,batch_id 0 57 miss% 0.05674213105451426
plot_id,batch_id 0 58 miss% 0.02807871706884433
plot_id,batch_id 0 59 miss% 0.04147928687859828
plot_id,batch_id 0 60 miss% 0.05493196657107938
plot_id,batch_id 0 61 miss% 0.03942045019275216
plot_id,batch_id 0 62 miss% 0.08406965870971178
plot_id,batch_id 0 63 miss% 0.07098802629525407
plot_id,batch_id 0 64 miss% 0.10347143690793041
plot_id,batch_id 0 65 miss% 0.06480009044502467
plot_id,batch_id 0 66 miss% 0.19077729341973834
plot_id,batch_id 0 67 miss% 0.07464773502500516
plot_id,batch_id 0 68 miss% 0.07677744581355224
plot_id,batch_id 0 69 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  89105
Epoch:0, Train loss:0.731501, valid loss:0.682973
Epoch:1, Train loss:0.132732, valid loss:0.004873
Epoch:2, Train loss:0.011076, valid loss:0.003749
Epoch:3, Train loss:0.009741, valid loss:0.003412
Epoch:4, Train loss:0.008134, valid loss:0.003012
Epoch:5, Train loss:0.007105, valid loss:0.002619
Epoch:6, Train loss:0.005699, valid loss:0.002360
Epoch:7, Train loss:0.005378, valid loss:0.002360
Epoch:8, Train loss:0.004892, valid loss:0.002109
Epoch:9, Train loss:0.003877, valid loss:0.002320
Epoch:10, Train loss:0.003537, valid loss:0.001998
Epoch:11, Train loss:0.002554, valid loss:0.001298
Epoch:12, Train loss:0.002431, valid loss:0.001354
Epoch:13, Train loss:0.002327, valid loss:0.001343
Epoch:14, Train loss:0.002267, valid loss:0.001413
Epoch:15, Train loss:0.002239, valid loss:0.001091
Epoch:16, Train loss:0.002073, valid loss:0.001302
Epoch:17, Train loss:0.002141, valid loss:0.001370
Epoch:18, Train loss:0.002008, valid loss:0.001105
Epoch:19, Train loss:0.001991, valid loss:0.001438
Epoch:20, Train loss:0.001898, valid loss:0.001012
Epoch:21, Train loss:0.001399, valid loss:0.000999
Epoch:22, Train loss:0.001309, valid loss:0.000893
Epoch:23, Train loss:0.001339, valid loss:0.000815
Epoch:24, Train loss:0.001301, valid loss:0.000903
Epoch:25, Train loss:0.001256, valid loss:0.000871
Epoch:26, Train loss:0.001270, valid loss:0.000812
Epoch:27, Train loss:0.001234, valid loss:0.000939
Epoch:28, Train loss:0.001191, valid loss:0.000948
Epoch:29, Train loss:0.001225, valid loss:0.000815
Epoch:30, Train loss:0.001161, valid loss:0.000761
Epoch:31, Train loss:0.000946, valid loss:0.000742
Epoch:32, Train loss:0.000910, valid loss:0.000752
Epoch:33, Train loss:0.000908, valid loss:0.000731
Epoch:34, Train loss:0.000892, valid loss:0.000695
Epoch:35, Train loss:0.000896, valid loss:0.000704
Epoch:36, Train loss:0.000866, valid loss:0.000682
Epoch:37, Train loss:0.000884, valid loss:0.000656
Epoch:38, Train loss:0.000847, valid loss:0.000696
Epoch:39, Train loss:0.000842, valid loss:0.000737
Epoch:40, Train loss:0.000834, valid loss:0.000709
Epoch:41, Train loss:0.000730, valid loss:0.000653
Epoch:42, Train loss:0.000719, valid loss:0.000686
Epoch:43, Train loss:0.000715, valid loss:0.000645
Epoch:44, Train loss:0.000724, valid loss:0.000660
Epoch:45, Train loss:0.000711, valid loss:0.000748
Epoch:46, Train loss:0.000700, valid loss:0.000673
Epoch:47, Train loss:0.000696, valid loss:0.000684
Epoch:48, Train loss:0.000696, valid loss:0.000643
Epoch:49, Train loss:0.000690, valid loss:0.000647
Epoch:50, Train loss:0.000682, valid loss:0.000765
Epoch:51, Train loss:0.000653, valid loss:0.000644
Epoch:52, Train loss:0.000637, valid loss:0.000638
Epoch:53, Train loss:0.000632, valid loss:0.000630
Epoch:54, Train loss:0.000628, valid loss:0.000628
Epoch:55, Train loss:0.000626, valid loss:0.000630
Epoch:56, Train loss:0.000625, valid loss:0.000622
Epoch:57, Train loss:0.000623, valid loss:0.000621
Epoch:58, Train loss:0.000622, valid loss:0.000619
Epoch:59, Train loss:0.000621, valid loss:0.000618
Epoch:60, Train loss:0.000620, valid loss:0.000619
training time 9075.276557207108
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.33005653655937195
plot_id,batch_id 0 1 miss% 0.35387094926935014
plot_id,batch_id 0 2 miss% 0.41659134320637786
plot_id,batch_id 0 3 miss% 0.33831902504133676
plot_id,batch_id 0 4 miss% 0.2653052087951656
plot_id,batch_id 0 5 miss% 0.27999242676643527
plot_id,batch_id 0 6 miss% 0.3151607522985161
plot_id,batch_id 0 7 miss% 0.4265676485336912
plot_id,batch_id 0 8 miss% 0.5993587789344239
plot_id,batch_id 0 9 miss% 0.3664790289400674
plot_id,batch_id 0 10 miss% 0.2769479197979906
plot_id,batch_id 0 11 miss% 0.2978382759689496
plot_id,batch_id 0 12 miss% 0.40329877473193454
plot_id,batch_id 0 13 miss% 0.3226335958597012
plot_id,batch_id 0 14 miss% 0.38354383713546114
plot_id,batch_id 0 15 miss% 0.26158184876840757
plot_id,batch_id 0 16 miss% 0.4561211883511536
plot_id,batch_id 0 17 miss% 0.40398080646176615
plot_id,batch_id 0 18 miss% 0.398483676168886
plot_id,batch_id 0 19 miss% 0.39326424014307254
plot_id,batch_id 0 20 miss% 0.3429957402213141
plot_id,batch_id 0 21 miss% 0.3082930370606439
plot_id,batch_id 0 22 miss% 0.3676145409621405
plot_id,batch_id 0 23 miss% 0.34940227279623326
plot_id,batch_id 0 24 miss% 0.25697017066432304
plot_id,batch_id 0 25 miss% 0.32206702732622117
plot_id,batch_id 0 26 miss% 0.3738378089512235
plot_id,batch_id 0 27 miss% 0.3855359105153632
plot_id,batch_id 0 28 miss% 0.36314788647243795
plot_id,batch_id 0 29 miss% 0.3500841191964508
plot_id,batch_id 0 30 miss% 0.3126701072609002
plot_id,batch_id 0 31 miss% 0.49562674966275394
plot_id,batch_id 0 32 miss% 0.4714306208546578
plot_id,batch_id 0 33 miss% 0.38185437632621777
plot_id,batch_id 0 34 miss% 0.3802817032161095
plot_id,batch_id 0 35 miss% 0.3149075257532599
plot_id,batch_id 0 36 miss% 0.47138492535704185
plot_id,batch_id 0 37 miss% 0.2765905587952981
plot_id,batch_id 0 38 miss% 0.32968679461908545
plot_id,batch_id 0 39 miss% 0.2891280844624353
plot_id,batch_id 0 40 miss% 0.39052379534505394
plot_id,batch_id 0 41 miss% 0.34845195388916167
plot_id,batch_id 0 42 miss% 0.2571902741630266
plot_id,batch_id 0 43 miss% 0.34517673047194314
plot_id,batch_id 0 44 miss% 0.2105499571693293
plot_id,batch_id 0 45 miss% 0.37563755206908234
plot_id,batch_id 0 46 miss% 0.3441371098023982
plot_id,batch_id 0 47 miss% 0.39995554504203934
plot_id,batch_id 0 48 miss% 0.2391839116790937
plot_id,batch_id 0 49 miss% 0.21002557479848472
plot_id,batch_id 0 50 miss% 0.47515897931663753
plot_id,batch_id 0 51 miss% 0.4052507203881439
plot_id,batch_id 0 52 miss% 0.29587272344134075
plot_id,batch_id 0 53 miss% 0.2108607481870376
plot_id,batch_id 0 54 miss% 0.3730791190768728
plot_id,batch_id 0 55 miss% 0.3688499740805084
plot_id,batch_id 0 56 miss% 0.5052228370436951
plot_id,batch_id 0 57 miss% 0.3102884669224319
plot_id,batch_id 0 58 miss% 0.35200561955443466
plot_id,batch_id 0 59 miss% 0.26514142739084756
plot_id,batch_id 0 60 miss% 0.30055791475532234
plot_id,batch_id 0 61 miss% 0.2673653037002643
plot_id,batch_id 0 62 miss% 0.3593248228058578
plot_id,batch_id 0 63 miss% 0.45556666392794243
plot_id,batch_id 0 64 miss% 0.36968857056768156
plot_id,batch_id 0 65 miss% 0.39944921284969725
plot_id,batch_id 0 66 miss% 0.34784055045965495
plot_id,batch_id 0 67 miss% 0.2519323519944346
plot_id,batch_id 0 68 miss% 0.44269980037767037
plot_id,batch_id 0 69 miss% 0.4181283365606061
plot_id,batch_id 0 70 miss% 0.26087012968657
plot_id,batch_id 0 71 miss% 0.3450792006844407
plot_id,batch_id 0 72 miss% 0.2789959284925779
plot_id,batch_id 0 73 miss% 0.3891325973143229
plot_id,batch_id 0 74 miss% 0.3748134275381406
plot_id,batch_id 0 75 miss% 0.24443687817542606
plot_id,batch_id 0 76 miss% 0.40501826228719956
plot_id,batch_id 0 77 miss% 0.3284369561567609
plot_id,batch_id 0 78 miss% 0.3232309988987845
plot_id,batch_id 0 79 miss% 0.29700748734134097
plot_id,batch_id 0 80 miss% 0.24369045561999936
plot_id,batch_id 0 81 miss% 0.3735149665664616
plot_id,batch_id 0 82 miss% 0.3777705676692733
plot_id,batch_id 0 83 miss% 0.4782112884000865
plot_id,batch_id 0 84 miss% 0.3468959943869543
plot_id,batch_id 0 85 miss% 0.27740244822980153
plot_id,batch_id 0 86 miss% 0.30002739362657976
plot_id,batch_id 0 87 miss% 0.3756552202222779
plot_id,batch_id 0 88 miss% 0.40360143879010724
plot_id,batch_id 0 89 miss% 0.4357771833212783
plot_id,batch_id 0 90 miss% 0.21033187700866318
plot_id,batch_id 0 91 miss% 0.31239341666944687
plot_id,batch_id 0 92 miss% 0.33916106936127616
plot_id,batch_id 0 93 miss% 0.3188773090871047
plot_id,batch_id 0 94 miss% 0.39035332400595774
plot_id,batch_id 0 95 miss% 0.2577483652030234
plot_id,batch_id 0 96 miss% 0.3599175204551797
plot_id,batch_id 0 97 miss% 0.43843820337264944
plot_id,batch_id 0 98 miss% 0.4046194404972354
plot_id,batch_id 0 99 miss% 0.39588926849793976
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.33005654 0.35387095 0.41659134 0.33831903 0.26530521 0.27999243
 0.31516075 0.42656765 0.59935878 0.36647903 0.27694792 0.29783828
 0.40329877 0.3226336  0.38354384 0.26158185 0.45612119 0.40398081
 0.39848368 0.39326424 0.34299574 0.30829304 0.36761454 0.34940227
 0.25697017 0.32206703 0.37383781 0.38553591 0.36314789 0.35008412
 0.31267011 0.49562675 0.47143062 0.38185438 0.3802817  0.31490753
 0.47138493 0.27659056 0.32968679 0.28912808 0.3905238  0.34845195
 0.25719027 0.34517673 0.21054996 0.37563755 0.34413711 0.39995555
 0.23918391 0.21002557 0.47515898 0.40525072 0.29587272 0.21086075
 0.37307912 0.36884997 0.50522284 0.31028847 0.35200562 0.26514143
 0.30055791 0.2673653  0.35932482 0.45556666 0.36968857 0.39944921
 0.34784055 0.25193235 0.4426998  0.41812834 0.26087013 0.3450792
 0.27899593 0.3891326  0.37481343 0.24443688 0.40501826 0.32843696
 0.323231   0.29700749 0.24369046 0.37351497 0.37777057 0.47821129
 0.34689599 0.27740245 0.30002739 0.37565522 0.40360144 0.43577718
 0.21033188 0.31239342 0.33916107 0.31887731 0.39035332 0.25774837
 0.35991752 0.4384382  0.40461944 0.39588927]
for model  57 the mean error 0.3501532098760372
all id 57 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 21 out win 4 err 0.3501532098760372 time 9075.276557207108
Launcher: Job 58 completed in 9326 seconds.
Launcher: Task 226 done. Exiting.
0.22015871042927776
plot_id,batch_id 0 70 miss% 0.07047344283312469
plot_id,batch_id 0 71 miss% 0.06371550976390276
plot_id,batch_id 0 72 miss% 0.12593178915518888
plot_id,batch_id 0 73 miss% 0.04993620239960925
plot_id,batch_id 0 74 miss% 0.12814075951362922
plot_id,batch_id 0 75 miss% 0.19433934257920998
plot_id,batch_id 0 76 miss% 0.15950216769521922
plot_id,batch_id 0 77 miss% 0.1481955494353333
plot_id,batch_id 0 78 miss% 0.10381260246862274
plot_id,batch_id 0 79 miss% 0.06907270744809874
plot_id,batch_id 0 80 miss% 0.13095166338897485
plot_id,batch_id 0 81 miss% 0.12223268771023812
plot_id,batch_id 0 82 miss% 0.10687735250357114
plot_id,batch_id 0 83 miss% 0.09011597008926324
plot_id,batch_id 0 84 miss% 0.15802394098368533
plot_id,batch_id 0 85 miss% 0.036557223145984793
plot_id,batch_id 0 86 miss% 0.09278240926893802
plot_id,batch_id 0 87 miss% 0.10704251846096992
plot_id,batch_id 0 88 miss% 0.11726951088692983
plot_id,batch_id 0 89 miss% 0.09361405747126277
plot_id,batch_id 0 90 miss% 0.09755104213135699
plot_id,batch_id 0 91 miss% 0.12949727503703523
plot_id,batch_id 0 92 miss% 0.08283461870481743
plot_id,batch_id 0 93 miss% 0.1407440382774622
plot_id,batch_id 0 94 miss% 0.10087235543988185
plot_id,batch_id 0 95 miss% 0.050914286788435084
plot_id,batch_id 0 96 miss% 0.1297677321389008
plot_id,batch_id 0 97 miss% 0.06239288897337869
plot_id,batch_id 0 98 miss% 0.08776765298704188
plot_id,batch_id 0 99 miss% 0.07276384340634375
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.11074262 0.07235328 0.12174617 0.08019639 0.07917086 0.09145582
 0.18881748 0.17022288 0.14081072 0.08828753 0.08368663 0.05931241
 0.15776656 0.13062484 0.15982016 0.07494763 0.18724738 0.06472402
 0.07578506 0.12444855 0.0750423  0.07802192 0.12546988 0.04875986
 0.10433379 0.07390949 0.06613819 0.04937424 0.05350929 0.04270211
 0.15806316 0.15867308 0.11675052 0.07596038 0.0618339  0.16635801
 0.18533741 0.06863182 0.07870226 0.05039293 0.20230152 0.06178597
 0.06854349 0.11604861 0.05881833 0.1389154  0.04102058 0.05031331
 0.04281751 0.04422066 0.16303998 0.03155246 0.05632213 0.04888312
 0.05036855 0.21677281 0.1145645  0.05674213 0.02807872 0.04147929
 0.05493197 0.03942045 0.08406966 0.07098803 0.10347144 0.06480009
 0.19077729 0.07464774 0.07677745 0.22015871 0.07047344 0.06371551
 0.12593179 0.0499362  0.12814076 0.19433934 0.15950217 0.14819555
 0.1038126  0.06907271 0.13095166 0.12223269 0.10687735 0.09011597
 0.15802394 0.03655722 0.09278241 0.10704252 0.11726951 0.09361406
 0.09755104 0.12949728 0.08283462 0.14074404 0.10087236 0.05091429
 0.12976773 0.06239289 0.08776765 0.07276384]
for model  137 the mean error 0.09836456510395913
all id 137 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 25 out win 6 err 0.09836456510395913 time 9058.120113611221
Launcher: Job 138 completed in 9328 seconds.
Launcher: Task 223 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  55697
Epoch:0, Train loss:0.742758, valid loss:0.704333
Epoch:1, Train loss:0.197000, valid loss:0.006882
Epoch:2, Train loss:0.017599, valid loss:0.005596
Epoch:3, Train loss:0.014488, valid loss:0.004525
Epoch:4, Train loss:0.012261, valid loss:0.003438
Epoch:5, Train loss:0.009779, valid loss:0.003584
Epoch:6, Train loss:0.007095, valid loss:0.002510
Epoch:7, Train loss:0.006316, valid loss:0.002160
Epoch:8, Train loss:0.006063, valid loss:0.002593
Epoch:9, Train loss:0.005707, valid loss:0.002556
Epoch:10, Train loss:0.005631, valid loss:0.001978
Epoch:11, Train loss:0.003252, valid loss:0.001581
Epoch:12, Train loss:0.002997, valid loss:0.001495
Epoch:13, Train loss:0.002931, valid loss:0.001384
Epoch:14, Train loss:0.002856, valid loss:0.001541
Epoch:15, Train loss:0.002755, valid loss:0.001560
Epoch:16, Train loss:0.002654, valid loss:0.001551
Epoch:17, Train loss:0.002589, valid loss:0.001506
Epoch:18, Train loss:0.002528, valid loss:0.001460
Epoch:19, Train loss:0.002532, valid loss:0.001291
Epoch:20, Train loss:0.002374, valid loss:0.001615
Epoch:21, Train loss:0.001965, valid loss:0.001110
Epoch:22, Train loss:0.001833, valid loss:0.001108
Epoch:23, Train loss:0.001805, valid loss:0.001077
Epoch:24, Train loss:0.001771, valid loss:0.001074
Epoch:25, Train loss:0.001761, valid loss:0.001157
Epoch:26, Train loss:0.001699, valid loss:0.001223
Epoch:27, Train loss:0.001743, valid loss:0.001103
Epoch:28, Train loss:0.001686, valid loss:0.001108
Epoch:29, Train loss:0.001624, valid loss:0.001267
Epoch:30, Train loss:0.001643, valid loss:0.001111
Epoch:31, Train loss:0.001353, valid loss:0.000994
Epoch:32, Train loss:0.001319, valid loss:0.001087
Epoch:33, Train loss:0.001322, valid loss:0.000972
Epoch:34, Train loss:0.001306, valid loss:0.000972
Epoch:35, Train loss:0.001293, valid loss:0.000989
Epoch:36, Train loss:0.001271, valid loss:0.000968
Epoch:37, Train loss:0.001240, valid loss:0.000981
Epoch:38, Train loss:0.001242, valid loss:0.000982
Epoch:39, Train loss:0.001222, valid loss:0.000983
Epoch:40, Train loss:0.001227, valid loss:0.000961
Epoch:41, Train loss:0.001087, valid loss:0.000931
Epoch:42, Train loss:0.001084, valid loss:0.000919
Epoch:43, Train loss:0.001065, valid loss:0.000960
Epoch:44, Train loss:0.001063, valid loss:0.000935
Epoch:45, Train loss:0.001079, valid loss:0.000906
Epoch:46, Train loss:0.001048, valid loss:0.000923
Epoch:47, Train loss:0.001046, valid loss:0.000945
Epoch:48, Train loss:0.001052, valid loss:0.000916
Epoch:49, Train loss:0.001038, valid loss:0.000898
Epoch:50, Train loss:0.001034, valid loss:0.000931
Epoch:51, Train loss:0.000962, valid loss:0.000889
Epoch:52, Train loss:0.000955, valid loss:0.000880
Epoch:53, Train loss:0.000950, valid loss:0.000881
Epoch:54, Train loss:0.000948, valid loss:0.000881
Epoch:55, Train loss:0.000946, valid loss:0.000883
Epoch:56, Train loss:0.000945, valid loss:0.000881
Epoch:57, Train loss:0.000944, valid loss:0.000883
Epoch:58, Train loss:0.000942, valid loss:0.000880
Epoch:59, Train loss:0.000941, valid loss:0.000880
Epoch:60, Train loss:0.000942, valid loss:0.000882
training time 9171.717075824738
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07784006166358105
plot_id,batch_id 0 1 miss% 0.051592052936605674
plot_id,batch_id 0 2 miss% 0.08984181587401781
plot_id,batch_id 0 3 miss% 0.04266450952255749
plot_id,batch_id 0 4 miss% 0.040794740425971435
plot_id,batch_id 0 5 miss% 0.061610647779859834
plot_id,batch_id 0 6 miss% 0.07794426972727642
plot_id,batch_id 0 7 miss% 0.10307541212861043
plot_id,batch_id 0 8 miss% 0.08968239793661943
plot_id,batch_id 0 9 miss% 0.06724558022155151
plot_id,batch_id 0 10 miss% 0.044877978892599424
plot_id,batch_id 0 11 miss% 0.06599451941670768
plot_id,batch_id 0 12 miss% 0.06388842073813078
plot_id,batch_id 0 13 miss% 0.061609688585630235
plot_id,batch_id 0 14 miss% 0.09551423383993517
plot_id,batch_id 0 15 miss% 0.06161078493057207
plot_id,batch_id 0 16 miss% 0.1681935038517169
plot_id,batch_id 0 17 miss% 0.07615494210363967
plot_id,batch_id 0 18 miss% 0.08998909057646279
plot_id,batch_id 0 19 miss% 0.07744585319566762
plot_id,batch_id 0 20 miss% 0.036158933641888484
plot_id,batch_id 0 21 miss% 0.08872278735958372
plot_id,batch_id 0 22 miss% 0.05232168716991112
plot_id,batch_id 0 23 miss% 0.04482268374345968
plot_id,batch_id 0 24 miss% 0.05202825427721367
plot_id,batch_id 0 25 miss% 0.06706995373419465
plot_id,batch_id 0 26 miss% 0.04898321944972728
plot_id,batch_id 0 27 miss% 0.07498523405361103
plot_id,batch_id 0 28 miss% 0.03709179644750696
plot_id,batch_id 0 29 miss% 0.04599978543962104
plot_id,batch_id 0 30 miss% 0.04517228082796081
plot_id,batch_id 0 31 miss% 0.09726822423363658
plot_id,batch_id 0 32 miss% 0.09982053273470225
plot_id,batch_id 0 33 miss% 0.0961203397592081
plot_id,batch_id 0 34 miss% 0.10792603313859296
plot_id,batch_id 0 35 miss% 0.0465899755042649
plot_id,batch_id 0 36 miss% 0.143323560818148
plot_id,batch_id 0 37 miss% 0.07839009479915422
plot_id,batch_id 0 38 miss% 0.061286704039856074
plot_id,batch_id 0 39 miss% 0.03428496536050809
plot_id,batch_id 0 40 miss% 0.04884248039004338
plot_id,batch_id 0 41 miss% 0.07157373624291287
plot_id,batch_id 0 42 miss% 0.038454726147610994
plot_id,batch_id 0 43 miss% 0.05790522698550465
plot_id,batch_id 0 44 miss% 0.06406466729604259
plot_id,batch_id 0 45 miss% 0.056748937133882536
plot_id,batch_id 0 46 miss% 0.054290671549921204
plot_id,batch_id 0 47 miss% 0.04866272922483263
plot_id,batch_id 0 48 miss% 0.03489820183339968
plot_id,batch_id 0 49 miss% 0.04143544024570258
plot_id,batch_id 0 50 miss% 0.15934153684025065
plot_id,batch_id 0 51 miss% 0.04718578386649413
plot_id,batch_id 0 52 miss% 0.05923077525132172
plot_id,batch_id 0 53 miss% 0.03128796315417698
plot_id,batch_id 0 54 miss% 0.05731767306498221
plot_id,batch_id 0 55 miss% 0.061983941806910435
plot_id,batch_id 0 56 miss% 0.07209757807644505
plot_id,batch_id 0 57 miss% 0.07024429106715376
plot_id,batch_id 0 58 miss% 0.06203864931284402
plot_id,batch_id 0 59 miss% 0.04975575744925548
plot_id,batch_id 0 60 miss% 0.061507731326743795
plot_id,batch_id 0 61 miss% 0.07345055476044336
plot_id,batch_id 0 62 miss% 0.08724359689592334
plot_id,batch_id 0 63 miss% 0.042375106607840225
plot_id,batch_id 0 64 miss% 0.051288219810796364
plot_id,batch_id 0 65 miss% 0.06390309657829525
plot_id,batch_id 0 66 miss% 0.14363122334012302
plot_id,batch_id 0 67 miss% 0.02851740319359543
plot_id,batch_id 0 68 miss% 0.04168440271690486
plot_id,batch_id 0 69 the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  55697
Epoch:0, Train loss:0.715920, valid loss:0.678783
Epoch:1, Train loss:0.353833, valid loss:0.005130
Epoch:2, Train loss:0.013453, valid loss:0.004347
Epoch:3, Train loss:0.011796, valid loss:0.003849
Epoch:4, Train loss:0.010626, valid loss:0.003574
Epoch:5, Train loss:0.009196, valid loss:0.003237
Epoch:6, Train loss:0.008727, valid loss:0.003070
Epoch:7, Train loss:0.008321, valid loss:0.003068
Epoch:8, Train loss:0.007061, valid loss:0.002864
Epoch:9, Train loss:0.006742, valid loss:0.002831
Epoch:10, Train loss:0.006415, valid loss:0.002659
Epoch:11, Train loss:0.005639, valid loss:0.002347
Epoch:12, Train loss:0.005552, valid loss:0.002166
Epoch:13, Train loss:0.005477, valid loss:0.002250
Epoch:14, Train loss:0.005285, valid loss:0.002147
Epoch:15, Train loss:0.005227, valid loss:0.002283
Epoch:16, Train loss:0.005116, valid loss:0.002046
Epoch:17, Train loss:0.005130, valid loss:0.002174
Epoch:18, Train loss:0.004988, valid loss:0.001993
Epoch:19, Train loss:0.004950, valid loss:0.001999
Epoch:20, Train loss:0.004885, valid loss:0.002168
Epoch:21, Train loss:0.004491, valid loss:0.001886
Epoch:22, Train loss:0.004398, valid loss:0.001882
Epoch:23, Train loss:0.004407, valid loss:0.001977
Epoch:24, Train loss:0.004367, valid loss:0.001838
Epoch:25, Train loss:0.004343, valid loss:0.001863
Epoch:26, Train loss:0.004329, valid loss:0.001796
Epoch:27, Train loss:0.004245, valid loss:0.001799
Epoch:28, Train loss:0.004248, valid loss:0.001905
Epoch:29, Train loss:0.004215, valid loss:0.001851
Epoch:30, Train loss:0.004165, valid loss:0.001802
Epoch:31, Train loss:0.003988, valid loss:0.001764
Epoch:32, Train loss:0.003974, valid loss:0.001760
Epoch:33, Train loss:0.003956, valid loss:0.001776
Epoch:34, Train loss:0.003941, valid loss:0.001724
Epoch:35, Train loss:0.003127, valid loss:0.001195
Epoch:36, Train loss:0.002895, valid loss:0.001181
Epoch:37, Train loss:0.002899, valid loss:0.001229
Epoch:38, Train loss:0.002855, valid loss:0.001222
Epoch:39, Train loss:0.002830, valid loss:0.001155
Epoch:40, Train loss:0.002833, valid loss:0.001203
Epoch:41, Train loss:0.002732, valid loss:0.001156
Epoch:42, Train loss:0.002719, valid loss:0.001160
Epoch:43, Train loss:0.002716, valid loss:0.001177
Epoch:44, Train loss:0.002708, valid loss:0.001176
Epoch:45, Train loss:0.002703, valid loss:0.001146
Epoch:46, Train loss:0.002684, valid loss:0.001154
Epoch:47, Train loss:0.002680, valid loss:0.001164
Epoch:48, Train loss:0.002674, valid loss:0.001151
Epoch:49, Train loss:0.002671, valid loss:0.001166
Epoch:50, Train loss:0.002674, valid loss:0.001148
Epoch:51, Train loss:0.002602, valid loss:0.001138
Epoch:52, Train loss:0.002597, valid loss:0.001137
Epoch:53, Train loss:0.002593, valid loss:0.001140
Epoch:54, Train loss:0.002591, valid loss:0.001138
Epoch:55, Train loss:0.002588, valid loss:0.001136
Epoch:56, Train loss:0.002589, valid loss:0.001139
Epoch:57, Train loss:0.002587, valid loss:0.001137
Epoch:58, Train loss:0.002586, valid loss:0.001136
Epoch:59, Train loss:0.002586, valid loss:0.001131
Epoch:60, Train loss:0.002585, valid loss:0.001143
training time 9179.672453165054
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.0841069576621591
plot_id,batch_id 0 1 miss% 0.06162424706847539
plot_id,batch_id 0 2 miss% 0.05832498524050585
plot_id,batch_id 0 3 miss% 0.05801503052333077
plot_id,batch_id 0 4 miss% 0.05482142313924754
plot_id,batch_id 0 5 miss% 0.0700960646562334
plot_id,batch_id 0 6 miss% 0.039028508874525795
plot_id,batch_id 0 7 miss% 0.07021737481096456
plot_id,batch_id 0 8 miss% 0.04938074344132445
plot_id,batch_id 0 9 miss% 0.036051094265651014
plot_id,batch_id 0 10 miss% 0.05069683303912077
plot_id,batch_id 0 11 miss% 0.034633208831982475
plot_id,batch_id 0 12 miss% 0.08157078551466951
plot_id,batch_id 0 13 miss% 0.12339895750428832
plot_id,batch_id 0 14 miss% 0.07167431791875808
plot_id,batch_id 0 15 miss% 0.036808652466831744
plot_id,batch_id 0 16 miss% 0.04220104956974578
plot_id,batch_id 0 17 miss% 0.051927615408861504
plot_id,batch_id 0 18 miss% 0.07202531109975667
plot_id,batch_id 0 19 miss% 0.09127231288819379
plot_id,batch_id 0 20 miss% 0.06596372553482471
plot_id,batch_id 0 21 miss% 0.04009601020609972
plot_id,batch_id 0 22 miss% 0.05216430803780862
plot_id,batch_id 0 23 miss% 0.03300534804499958
plot_id,batch_id 0 24 miss% 0.06618269348917707
plot_id,batch_id 0 25 miss% 0.04433229530028564
plot_id,batch_id 0 26 miss% 0.03572076126729273
plot_id,batch_id 0 27 miss% 0.03207289714283166
plot_id,batch_id 0 28 miss% 0.03387399601571389
plot_id,batch_id 0 29 miss% 0.02454851550844198
plot_id,batch_id 0 30 miss% 0.13121693878881815
plot_id,batch_id 0 31 miss% 0.09793950717056993
plot_id,batch_id 0 32 miss% 0.08846265289553225
plot_id,batch_id 0 33 miss% 0.029874305904359424
plot_id,batch_id 0 34 miss% 0.04100457374033794
plot_id,batch_id 0 35 miss% 0.06207856275284829
plot_id,batch_id 0 36 miss% 0.13572149884804277
plot_id,batch_id 0 37 miss% 0.042347920099339055
plot_id,batch_id 0 38 miss% 0.034435566258738386
plot_id,batch_id 0 39 miss% 0.04531709307830278
plot_id,batch_id 0 40 miss% 0.09431829012480425
plot_id,batch_id 0 41 miss% 0.051770558404175215
plot_id,batch_id 0 42 miss% 0.03275846911027455
plot_id,batch_id 0 43 miss% 0.04801482405664582
plot_id,batch_id 0 44 miss% 0.03390860341565981
plot_id,batch_id 0 45 miss% 0.04209794316212208
plot_id,batch_id 0 46 miss% 0.04056396578229464
plot_id,batch_id 0 47 miss% 0.03485390462015787
plot_id,batch_id 0 48 miss% 0.025489095459100705
plot_id,batch_id 0 49 miss% 0.032454763521794106
plot_id,batch_id 0 50 miss% 0.11894608089864453
plot_id,batch_id 0 51 miss% 0.043668739957275826
plot_id,batch_id 0 52 miss% 0.023771808215214223
plot_id,batch_id 0 53 miss% 0.022850922000392855
plot_id,batch_id 0 54 miss% 0.02933101566838076
plot_id,batch_id 0 55 miss% 0.06112519483548832
plot_id,batch_id 0 56 miss% 0.05249967353775026
plot_id,batch_id 0 57 miss% 0.056241795131732254
plot_id,batch_id 0 58 miss% 0.03231369250120942
plot_id,batch_id 0 59 miss% 0.025463111769138406
plot_id,batch_id 0 60 miss% 0.04364409067795788
plot_id,batch_id 0 61 miss% 0.05740136012457915
plot_id,batch_id 0 62 miss% 0.04993920767108263
plot_id,batch_id 0 63 miss% 0.04776322015634807
plot_id,batch_id 0 64 miss% 0.04735355316275723
plot_id,batch_id 0 65 miss% 0.0502211917484217
plot_id,batch_id 0 66 miss% 0.09421466012965497
plot_id,batch_id 0 67 miss% 0.03410541416576704
plot_id,batch_id 0 68 miss% 0.04770554888969687
miss% 0.08803875022232842
plot_id,batch_id 0 70 miss% 0.07127240681660076
plot_id,batch_id 0 71 miss% 0.05714104748123049
plot_id,batch_id 0 72 miss% 0.08519506331988691
plot_id,batch_id 0 73 miss% 0.05660654336890952
plot_id,batch_id 0 74 miss% 0.05974509397082147
plot_id,batch_id 0 75 miss% 0.0917929829525608
plot_id,batch_id 0 76 miss% 0.07752797073384282
plot_id,batch_id 0 77 miss% 0.023781793126637297
plot_id,batch_id 0 78 miss% 0.03155105478703992
plot_id,batch_id 0 79 miss% 0.05622108719077943
plot_id,batch_id 0 80 miss% 0.05930636343922357
plot_id,batch_id 0 81 miss% 0.06456032293213482
plot_id,batch_id 0 82 miss% 0.08759956388669934
plot_id,batch_id 0 83 miss% 0.058558345960686514
plot_id,batch_id 0 84 miss% 0.05763350251030639
plot_id,batch_id 0 85 miss% 0.06649101169906231
plot_id,batch_id 0 86 miss% 0.06187650003251591
plot_id,batch_id 0 87 miss% 0.05472920940443292
plot_id,batch_id 0 88 miss% 0.06819068188416294
plot_id,batch_id 0 89 miss% 0.05806145955002642
plot_id,batch_id 0 90 miss% 0.05780209764073587
plot_id,batch_id 0 91 miss% 0.0642592693111672
plot_id,batch_id 0 92 miss% 0.0627749592067912
plot_id,batch_id 0 93 miss% 0.055135068753047405
plot_id,batch_id 0 94 miss% 0.12235320211353176
plot_id,batch_id 0 95 miss% 0.0882268141482195
plot_id,batch_id 0 96 miss% 0.05016641137242025
plot_id,batch_id 0 97 miss% 0.052582245121331896
plot_id,batch_id 0 98 miss% 0.05609138773941823
plot_id,batch_id 0 99 miss% 0.09397937748930202
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07784006 0.05159205 0.08984182 0.04266451 0.04079474 0.06161065
 0.07794427 0.10307541 0.0896824  0.06724558 0.04487798 0.06599452
 0.06388842 0.06160969 0.09551423 0.06161078 0.1681935  0.07615494
 0.08998909 0.07744585 0.03615893 0.08872279 0.05232169 0.04482268
 0.05202825 0.06706995 0.04898322 0.07498523 0.0370918  0.04599979
 0.04517228 0.09726822 0.09982053 0.09612034 0.10792603 0.04658998
 0.14332356 0.07839009 0.0612867  0.03428497 0.04884248 0.07157374
 0.03845473 0.05790523 0.06406467 0.05674894 0.05429067 0.04866273
 0.0348982  0.04143544 0.15934154 0.04718578 0.05923078 0.03128796
 0.05731767 0.06198394 0.07209758 0.07024429 0.06203865 0.04975576
 0.06150773 0.07345055 0.0872436  0.04237511 0.05128822 0.0639031
 0.14363122 0.0285174  0.0416844  0.08803875 0.07127241 0.05714105
 0.08519506 0.05660654 0.05974509 0.09179298 0.07752797 0.02378179
 0.03155105 0.05622109 0.05930636 0.06456032 0.08759956 0.05855835
 0.0576335  0.06649101 0.0618765  0.05472921 0.06819068 0.05806146
 0.0578021  0.06425927 0.06277496 0.05513507 0.1223532  0.08822681
 0.05016641 0.05258225 0.05609139 0.09397938]
for model  64 the mean error 0.0668615124121667
all id 64 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 21 out win 5 err 0.0668615124121667 time 9171.717075824738
Launcher: Job 65 completed in 9445 seconds.
Launcher: Task 204 done. Exiting.
plot_id,batch_id 0 69 miss% 0.09053122766002479
plot_id,batch_id 0 70 miss% 0.1084511608041641
plot_id,batch_id 0 71 miss% 0.05119649579723233
plot_id,batch_id 0 72 miss% 0.11550994076315493
plot_id,batch_id 0 73 miss% 0.051693154314170264
plot_id,batch_id 0 74 miss% 0.08434100289616268
plot_id,batch_id 0 75 miss% 0.04331501345220676
plot_id,batch_id 0 76 miss% 0.10275684642193109
plot_id,batch_id 0 77 miss% 0.04192813355708
plot_id,batch_id 0 78 miss% 0.03789428113145935
plot_id,batch_id 0 79 miss% 0.055742967790298484
plot_id,batch_id 0 80 miss% 0.0531523632622733
plot_id,batch_id 0 81 miss% 0.06686838787080913
plot_id,batch_id 0 82 miss% 0.09500445457234545
plot_id,batch_id 0 83 miss% 0.06548429022174375
plot_id,batch_id 0 84 miss% 0.07728608728642838
plot_id,batch_id 0 85 miss% 0.04875532857413013
plot_id,batch_id 0 86 miss% 0.04721684503329202
plot_id,batch_id 0 87 miss% 0.10778188952796637
plot_id,batch_id 0 88 miss% 0.06786275301228731
plot_id,batch_id 0 89 miss% 0.05888267249731916
plot_id,batch_id 0 90 miss% 0.037535296576084606
plot_id,batch_id 0 91 miss% 0.06767113741789049
plot_id,batch_id 0 92 miss% 0.03995758766466806
plot_id,batch_id 0 93 miss% 0.054140065413194646
plot_id,batch_id 0 94 miss% 0.09423753500296023
plot_id,batch_id 0 95 miss% 0.052388755948329147
plot_id,batch_id 0 96 miss% 0.04664452963850022
plot_id,batch_id 0 97 miss% 0.028508718029381812
plot_id,batch_id 0 98 miss% 0.04311126681248176
plot_id,batch_id 0 99 miss% 0.06226092330788573
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08410696 0.06162425 0.05832499 0.05801503 0.05482142 0.07009606
 0.03902851 0.07021737 0.04938074 0.03605109 0.05069683 0.03463321
 0.08157079 0.12339896 0.07167432 0.03680865 0.04220105 0.05192762
 0.07202531 0.09127231 0.06596373 0.04009601 0.05216431 0.03300535
 0.06618269 0.0443323  0.03572076 0.0320729  0.033874   0.02454852
 0.13121694 0.09793951 0.08846265 0.02987431 0.04100457 0.06207856
 0.1357215  0.04234792 0.03443557 0.04531709 0.09431829 0.05177056
 0.03275847 0.04801482 0.0339086  0.04209794 0.04056397 0.0348539
 0.0254891  0.03245476 0.11894608 0.04366874 0.02377181 0.02285092
 0.02933102 0.06112519 0.05249967 0.0562418  0.03231369 0.02546311
 0.04364409 0.05740136 0.04993921 0.04776322 0.04735355 0.05022119
 0.09421466 0.03410541 0.04770555 0.09053123 0.10845116 0.0511965
 0.11550994 0.05169315 0.084341   0.04331501 0.10275685 0.04192813
 0.03789428 0.05574297 0.05315236 0.06686839 0.09500445 0.06548429
 0.07728609 0.04875533 0.04721685 0.10778189 0.06786275 0.05888267
 0.0375353  0.06767114 0.03995759 0.05414007 0.09423754 0.05238876
 0.04664453 0.02850872 0.04311127 0.06226092]
for model  38 the mean error 0.057471364551653685
all id 38 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 21 out win 6 err 0.057471364551653685 time 9179.672453165054
Launcher: Job 39 completed in 9451 seconds.
Launcher: Task 189 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  55697
Epoch:0, Train loss:0.715920, valid loss:0.678783
Epoch:1, Train loss:0.323392, valid loss:0.006087
Epoch:2, Train loss:0.017155, valid loss:0.006269
Epoch:3, Train loss:0.015654, valid loss:0.005164
Epoch:4, Train loss:0.014739, valid loss:0.005272
Epoch:5, Train loss:0.013541, valid loss:0.004137
Epoch:6, Train loss:0.011111, valid loss:0.003753
Epoch:7, Train loss:0.009622, valid loss:0.003496
Epoch:8, Train loss:0.008769, valid loss:0.003371
Epoch:9, Train loss:0.008549, valid loss:0.002967
Epoch:10, Train loss:0.008294, valid loss:0.003127
Epoch:11, Train loss:0.007639, valid loss:0.002865
Epoch:12, Train loss:0.007490, valid loss:0.002838
Epoch:13, Train loss:0.007367, valid loss:0.002785
Epoch:14, Train loss:0.007263, valid loss:0.002723
Epoch:15, Train loss:0.007150, valid loss:0.002921
Epoch:16, Train loss:0.007031, valid loss:0.002666
Epoch:17, Train loss:0.007075, valid loss:0.002663
Epoch:18, Train loss:0.006869, valid loss:0.002643
Epoch:19, Train loss:0.006726, valid loss:0.002568
Epoch:20, Train loss:0.006630, valid loss:0.002495
Epoch:21, Train loss:0.006296, valid loss:0.002404
Epoch:22, Train loss:0.006241, valid loss:0.002378
Epoch:23, Train loss:0.006202, valid loss:0.002359
Epoch:24, Train loss:0.006163, valid loss:0.002407
Epoch:25, Train loss:0.006122, valid loss:0.002370
Epoch:26, Train loss:0.005622, valid loss:0.002192
Epoch:27, Train loss:0.004274, valid loss:0.002023
Epoch:28, Train loss:0.003150, valid loss:0.001451
Epoch:29, Train loss:0.002481, valid loss:0.001410
Epoch:30, Train loss:0.002381, valid loss:0.001367
Epoch:31, Train loss:0.002176, valid loss:0.001338
Epoch:32, Train loss:0.002134, valid loss:0.001276
Epoch:33, Train loss:0.002136, valid loss:0.001328
Epoch:34, Train loss:0.002085, valid loss:0.001304
Epoch:35, Train loss:0.002069, valid loss:0.001301
Epoch:36, Train loss:0.002044, valid loss:0.001381
Epoch:37, Train loss:0.002027, valid loss:0.001267
Epoch:38, Train loss:0.001983, valid loss:0.001250
Epoch:39, Train loss:0.001975, valid loss:0.001227
Epoch:40, Train loss:0.001957, valid loss:0.001320
Epoch:41, Train loss:0.001873, valid loss:0.001215
Epoch:42, Train loss:0.001852, valid loss:0.001227
Epoch:43, Train loss:0.001845, valid loss:0.001230
Epoch:44, Train loss:0.001836, valid loss:0.001248
Epoch:45, Train loss:0.001828, valid loss:0.001203
Epoch:46, Train loss:0.001812, valid loss:0.001178
Epoch:47, Train loss:0.001799, valid loss:0.001196
Epoch:48, Train loss:0.001787, valid loss:0.001195
Epoch:49, Train loss:0.001781, valid loss:0.001190
Epoch:50, Train loss:0.001779, valid loss:0.001198
Epoch:51, Train loss:0.001716, valid loss:0.001179
Epoch:52, Train loss:0.001705, valid loss:0.001179
Epoch:53, Train loss:0.001701, valid loss:0.001177
Epoch:54, Train loss:0.001700, valid loss:0.001178
Epoch:55, Train loss:0.001698, valid loss:0.001174
Epoch:56, Train loss:0.001697, valid loss:0.001172
Epoch:57, Train loss:0.001697, valid loss:0.001172
Epoch:58, Train loss:0.001697, valid loss:0.001176
Epoch:59, Train loss:0.001696, valid loss:0.001177
Epoch:60, Train loss:0.001696, valid loss:0.001182
training time 9247.356668233871
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.1311879413979179
plot_id,batch_id 0 1 miss% 0.04209612059986207
plot_id,batch_id 0 2 miss% 0.13176231306597122
plot_id,batch_id 0 3 miss% 0.13675476081897353
plot_id,batch_id 0 4 miss% 0.07018352413838642
plot_id,batch_id 0 5 miss% 0.06978182167755836
plot_id,batch_id 0 6 miss% 0.06179663685819267
plot_id,batch_id 0 7 miss% 0.1102475667585578
plot_id,batch_id 0 8 miss% 0.07295045478399054
plot_id,batch_id 0 9 miss% 0.048285210249973815
plot_id,batch_id 0 10 miss% 0.09152701829317761
plot_id,batch_id 0 11 miss% 0.05440358134631207
plot_id,batch_id 0 12 miss% 0.07901855133472624
plot_id,batch_id 0 13 miss% 0.11828972411736344
plot_id,batch_id 0 14 miss% 0.13595777144140042
plot_id,batch_id 0 15 miss% 0.11953559520648568
plot_id,batch_id 0 16 miss% 0.15770198207621425
plot_id,batch_id 0 17 miss% 0.06713585077806537
plot_id,batch_id 0 18 miss% 0.04929084774670246
plot_id,batch_id 0 19 miss% 0.09769276925892471
plot_id,batch_id 0 20 miss% 0.08651896716159882
plot_id,batch_id 0 21 miss% 0.10803036257985264
plot_id,batch_id 0 22 miss% 0.08891470263509561
plot_id,batch_id 0 23 miss% 0.02675131974205842
plot_id,batch_id 0 24 miss% 0.08602523726498021
plot_id,batch_id 0 25 miss% 0.04517885693748039
plot_id,batch_id 0 26 miss% 0.08684601890347844
plot_id,batch_id 0 27 miss% 0.04996214303177637
plot_id,batch_id 0 28 miss% 0.04223992251886059
plot_id,batch_id 0 29 miss% 0.0335425248978089
plot_id,batch_id 0 30 miss% 0.046873531989134465
plot_id,batch_id 0 31 miss% 0.0787419896449959
plot_id,batch_id 0 32 miss% 0.12811958875368076
plot_id,batch_id 0 33 miss% 0.0777140535556386
plot_id,batch_id 0 34 miss% 0.0366141298777218
plot_id,batch_id 0 35 miss% 0.11450312534033491
plot_id,batch_id 0 36 miss% 0.16292645080789758
plot_id,batch_id 0 37 miss% 0.08169845450262365
plot_id,batch_id 0 38 miss% 0.05823826521270107
plot_id,batch_id 0 39 miss% 0.043600194944667846
plot_id,batch_id 0 40 miss% 0.0737978628840704
plot_id,batch_id 0 41 miss% 0.042086148598824626
plot_id,batch_id 0 42 miss% 0.04961719284841
plot_id,batch_id 0 43 miss% 0.04894320586384106
plot_id,batch_id 0 44 miss% 0.04747369046903823
plot_id,batch_id 0 45 miss% 0.06377848546512652
plot_id,batch_id 0 46 miss% 0.03746948292662722
plot_id,batch_id 0 47 miss% 0.03163352293684658
plot_id,batch_id 0 48 miss% 0.029912819403276933
plot_id,batch_id 0 49 miss% 0.06543712753124052
plot_id,batch_id 0 50 miss% 0.18610623734438844
plot_id,batch_id 0 51 miss% 0.05085373901558572
plot_id,batch_id 0 52 miss% 0.04605760270594232
plot_id,batch_id 0 53 miss% 0.041898338716184626
plot_id,batch_id 0 54 miss% 0.03727216702063316
plot_id,batch_id 0 55 miss% 0.08237957216613292
plot_id,batch_id 0 56 miss% 0.07656719593742392
plot_id,batch_id 0 57 miss% 0.0654239577366585
plot_id,batch_id 0 58 miss% 0.05783531766169976
plot_id,batch_id 0 59 miss% 0.04055378513869296
plot_id,batch_id 0 60 miss% 0.039361922948394246
plot_id,batch_id 0 61 miss% 0.05336108592505285
plot_id,batch_id 0 62 miss% 0.0896753703736262
plot_id,batch_id 0 63 miss% 0.06269283451872938
plot_id,batch_id 0 64 miss% 0.06910194520958746
plot_id,batch_id 0 65 miss% 0.061048550810806894
plot_id,batch_id 0 66 miss% 0.12278623157449732
plot_id,batch_id 0 67 miss% 0.03705390499563097
plot_id,batch_id 0 68 miss% 0.027845536173826093
plot_id,batch_id 0 69 miss% 0.1083565048720566
plot_id,batch_id 0 70 miss% 0.08335423125979845
plot_id,batch_id 0 71 miss% 0.037846289760731226
plot_id,batch_id 0 72 miss% 0.052228796640889585
plot_id,batch_id 0 73 miss% 0.06961166003711784
plot_id,batch_id 0 74 miss% 0.12781330166592572
plot_id,batch_id 0 75 miss% 0.04948435966479673
plot_id,batch_id 0 76 miss% 0.12908732959574118
plot_id,batch_id 0 77 miss% 0.0732370653308804
plot_id,batch_id 0 78 miss% 0.06786971113550268
plot_id,batch_id 0 79 miss% 0.0723882932221884
plot_id,batch_id 0 80 miss% 0.0508411501390534
plot_id,batch_id 0 81 miss% 0.06844920288634429
plot_id,batch_id 0 82 miss% 0.07450165400538272
plot_id,batch_id 0 83 miss% 0.16739065769107866
plot_id,batch_id 0 84 miss% 0.08169974129665836
plot_id,batch_id 0 85 miss% 0.03100314229336882
plot_id,batch_id 0 86 miss% 0.06724769485913777
plot_id,batch_id 0 87 miss% 0.1215016540024389
plot_id,batch_id 0 88 miss% 0.09773961069904255
plot_id,batch_id 0 89 miss% 0.04748454558790937
plot_id,batch_id 0 90 miss% 0.041081186650747496
plot_id,batch_id 0 91 miss% 0.0647800995468846
plot_id,batch_id 0 92 miss% 0.03764625102708385
plot_id,batch_id 0 93 miss% 0.08943435146829587
plot_id,batch_id 0 94 miss% 0.07321263007771153
plot_id,batch_id 0 95 miss% 0.08711583168183408
plot_id,batch_id 0 96 miss% 0.042608857505314866
plot_id,batch_id 0 97 miss% 0.0436744740295821
plot_id,batch_id 0 98 miss% 0.04245058381308607
plot_id,batch_id 0 99 miss% 0.0944882269146521
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.13118794 0.04209612 0.13176231 0.13675476 0.07018352 0.06978182
 0.06179664 0.11024757 0.07295045 0.04828521 0.09152702 0.05440358
 0.07901855 0.11828972 0.13595777 0.1195356  0.15770198 0.06713585
 0.04929085 0.09769277 0.08651897 0.10803036 0.0889147  0.02675132
 0.08602524 0.04517886 0.08684602 0.04996214 0.04223992 0.03354252
 0.04687353 0.07874199 0.12811959 0.07771405 0.03661413 0.11450313
 0.16292645 0.08169845 0.05823827 0.04360019 0.07379786 0.04208615
 0.04961719 0.04894321 0.04747369 0.06377849 0.03746948 0.03163352
 0.02991282 0.06543713 0.18610624 0.05085374 0.0460576  0.04189834
 0.03727217 0.08237957 0.0765672  0.06542396 0.05783532 0.04055379
 0.03936192 0.05336109 0.08967537 0.06269283 0.06910195 0.06104855
 0.12278623 0.0370539  0.02784554 0.1083565  0.08335423 0.03784629
 0.0522288  0.06961166 0.1278133  0.04948436 0.12908733 0.07323707
 0.06786971 0.07238829 0.05084115 0.0684492  0.07450165 0.16739066
 0.08169974 0.03100314 0.06724769 0.12150165 0.09773961 0.04748455
 0.04108119 0.0647801  0.03764625 0.08943435 0.07321263 0.08711583
 0.04260886 0.04367447 0.04245058 0.09448823]
for model  11 the mean error 0.07362293806513177
all id 11 hidden_dim 16 learning_rate 0.0025 num_layers 4 frames 21 out win 6 err 0.07362293806513177 time 9247.356668233871
Launcher: Job 12 completed in 9527 seconds.
Launcher: Task 117 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  89105
Epoch:0, Train loss:0.705019, valid loss:0.658115
Epoch:1, Train loss:0.148939, valid loss:0.006160
Epoch:2, Train loss:0.016498, valid loss:0.005554
Epoch:3, Train loss:0.015092, valid loss:0.004763
Epoch:4, Train loss:0.012290, valid loss:0.003683
Epoch:5, Train loss:0.008426, valid loss:0.003077
Epoch:6, Train loss:0.007258, valid loss:0.003055
Epoch:7, Train loss:0.006799, valid loss:0.002846
Epoch:8, Train loss:0.006500, valid loss:0.002600
Epoch:9, Train loss:0.004519, valid loss:0.001969
Epoch:10, Train loss:0.003718, valid loss:0.002001
Epoch:11, Train loss:0.002768, valid loss:0.001421
Epoch:12, Train loss:0.002631, valid loss:0.001403
Epoch:13, Train loss:0.002473, valid loss:0.001279
Epoch:14, Train loss:0.002372, valid loss:0.001449
Epoch:15, Train loss:0.002369, valid loss:0.001448
Epoch:16, Train loss:0.002324, valid loss:0.001286
Epoch:17, Train loss:0.002181, valid loss:0.001306
Epoch:18, Train loss:0.002056, valid loss:0.001260
Epoch:19, Train loss:0.002027, valid loss:0.001110
Epoch:20, Train loss:0.001940, valid loss:0.001149
Epoch:21, Train loss:0.001520, valid loss:0.000992
Epoch:22, Train loss:0.001494, valid loss:0.000953
Epoch:23, Train loss:0.001455, valid loss:0.000989
Epoch:24, Train loss:0.001441, valid loss:0.000921
Epoch:25, Train loss:0.001356, valid loss:0.000964
Epoch:26, Train loss:0.001385, valid loss:0.000898
Epoch:27, Train loss:0.001362, valid loss:0.000874
Epoch:28, Train loss:0.001321, valid loss:0.000967
Epoch:29, Train loss:0.001308, valid loss:0.000977
Epoch:30, Train loss:0.001287, valid loss:0.000889
Epoch:31, Train loss:0.001063, valid loss:0.000815
Epoch:32, Train loss:0.001046, valid loss:0.000876
Epoch:33, Train loss:0.001043, valid loss:0.000951
Epoch:34, Train loss:0.001028, valid loss:0.000840
Epoch:35, Train loss:0.001011, valid loss:0.000814
Epoch:36, Train loss:0.000999, valid loss:0.000830
Epoch:37, Train loss:0.000974, valid loss:0.000817
Epoch:38, Train loss:0.000983, valid loss:0.000838
Epoch:39, Train loss:0.000960, valid loss:0.000868
Epoch:40, Train loss:0.000941, valid loss:0.000850
Epoch:41, Train loss:0.000854, valid loss:0.000797
Epoch:42, Train loss:0.000838, valid loss:0.000828
Epoch:43, Train loss:0.000830, valid loss:0.000813
Epoch:44, Train loss:0.000830, valid loss:0.000794
Epoch:45, Train loss:0.000825, valid loss:0.000802
Epoch:46, Train loss:0.000814, valid loss:0.000792
Epoch:47, Train loss:0.000815, valid loss:0.000790
Epoch:48, Train loss:0.000817, valid loss:0.000810
Epoch:49, Train loss:0.000797, valid loss:0.000808
Epoch:50, Train loss:0.000801, valid loss:0.000780
Epoch:51, Train loss:0.000740, valid loss:0.000779
Epoch:52, Train loss:0.000735, valid loss:0.000774
Epoch:53, Train loss:0.000732, valid loss:0.000767
Epoch:54, Train loss:0.000730, valid loss:0.000765
Epoch:55, Train loss:0.000728, valid loss:0.000771
Epoch:56, Train loss:0.000727, valid loss:0.000767
Epoch:57, Train loss:0.000726, valid loss:0.000767
Epoch:58, Train loss:0.000725, valid loss:0.000774
Epoch:59, Train loss:0.000724, valid loss:0.000772
Epoch:60, Train loss:0.000724, valid loss:0.000774
training time 9331.11327457428
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.36810442224600876
plot_id,batch_id 0 1 miss% 0.4350991211089701
plot_id,batch_id 0 2 miss% 0.5342229061349146
plot_id,batch_id 0 3 miss% 0.3944704282050483
plot_id,batch_id 0 4 miss% 0.34142758854496263
plot_id,batch_id 0 5 miss% 0.3896962609878236
plot_id,batch_id 0 6 miss% 0.4470669826631152
plot_id,batch_id 0 7 miss% 0.5323784543112107
plot_id,batch_id 0 8 miss% 0.6187387967499939
plot_id,batch_id 0 9 miss% 0.4673575999607643
plot_id,batch_id 0 10 miss% 0.3448267579660023
plot_id,batch_id 0 11 miss% 0.40599266188567595
plot_id,batch_id 0 12 miss% 0.44265680312156813
plot_id,batch_id 0 13 miss% 0.40366678621269364
plot_id,batch_id 0 14 miss% 0.4892993048691227
plot_id,batch_id 0 15 miss% 0.3642108847198046
plot_id,batch_id 0 16 miss% 0.5237184519852174
plot_id,batch_id 0 17 miss% 0.48459057586486093
plot_id,batch_id 0 18 miss% 0.44972943812582516
plot_id,batch_id 0 19 miss% 0.43608792401116214
plot_id,batch_id 0 20 miss% 0.3914295168746594
plot_id,batch_id 0 21 miss% 0.46728149015442927
plot_id,batch_id 0 22 miss% 0.5134038148024311
plot_id,batch_id 0 23 miss% 0.4440536272116951
plot_id,batch_id 0 24 miss% 0.40948526137522745
plot_id,batch_id 0 25 miss% 0.42785067452232206
plot_id,batch_id 0 26 miss% 0.43344106829890283
plot_id,batch_id 0 27 miss% 0.4589571231871457
plot_id,batch_id 0 28 miss% 0.55382612239087
plot_id,batch_id 0 29 miss% 0.4182236596220336
plot_id,batch_id 0 30 miss% 0.3930447409982483
plot_id,batch_id 0 31 miss% 0.5087125317699044
plot_id,batch_id 0 32 miss% 0.5144549891537312
plot_id,batch_id 0 33 miss% 0.4887618345230947
plot_id,batch_id 0 34 miss% 0.3395208556277672
plot_id,batch_id 0 35 miss% 0.459921246375334
plot_id,batch_id 0 36 miss% 0.5539130569329522
plot_id,batch_id 0 37 miss% 0.364378647158288
plot_id,batch_id 0 38 miss% 0.4137496762641964
plot_id,batch_id 0 39 miss% 0.40182792469212625
plot_id,batch_id 0 40 miss% 0.44954148894873347
plot_id,batch_id 0 41 miss% 0.4284548149492918
plot_id,batch_id 0 42 miss% 0.29863894710529526
plot_id,batch_id 0 43 miss% 0.34197267471448706
plot_id,batch_id 0 44 miss% 0.3779153475335744
plot_id,batch_id 0 45 miss% 0.31990594713491594
plot_id,batch_id 0 46 miss% 0.44634987317687
plot_id,batch_id 0 47 miss% 0.44691013814186664
plot_id,batch_id 0 48 miss% 0.3517053142110977
plot_id,batch_id 0 49 miss% 0.35950201892538924
plot_id,batch_id 0 50 miss% 0.607293982163256
plot_id,batch_id 0 51 miss% 0.5099908869513193
plot_id,batch_id 0 52 miss% 0.5265218096935796
plot_id,batch_id 0 53 miss% 0.2955695725754186
plot_id,batch_id 0 54 miss% 0.4057737480915478
plot_id,batch_id 0 55 miss% 0.5346682603184055
plot_id,batch_id 0 56 miss% 0.5507387912843809
plot_id,batch_id 0 57 miss% 0.6260164154586476
plot_id,batch_id 0 58 miss% 0.5408431219960665
plot_id,batch_id 0 59 miss% 0.37949239561309395
plot_id,batch_id 0 60 miss% 0.30758873361833544
plot_id,batch_id 0 61 miss% 0.3099463607350369
plot_id,batch_id 0 62 miss% 0.41357653196095084
plot_id,batch_id 0 63 miss% 0.46144270299010043
plot_id,batch_id 0 64 miss% 0.4610688137002753
plot_id,batch_id 0 65 miss% 0.3450394605708697
plot_id,batch_id 0 66 miss% 0.47897113221379667
plot_id,batch_id 0 67 miss% 0.3691576055404549
plot_id,batch_id 0 68 miss% 0.47466384823724045
plot_id,batch_id 0 69 miss% 0.4944338812502049
plot_id,batch_id 0 70 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  89105
Epoch:0, Train loss:0.731501, valid loss:0.682973
Epoch:1, Train loss:0.115753, valid loss:0.005534
Epoch:2, Train loss:0.015082, valid loss:0.004787
Epoch:3, Train loss:0.011967, valid loss:0.004016
Epoch:4, Train loss:0.009891, valid loss:0.003457
Epoch:5, Train loss:0.008607, valid loss:0.002907
Epoch:6, Train loss:0.007701, valid loss:0.002858
Epoch:7, Train loss:0.006508, valid loss:0.002535
Epoch:8, Train loss:0.005914, valid loss:0.002083
Epoch:9, Train loss:0.004338, valid loss:0.001653
Epoch:10, Train loss:0.003203, valid loss:0.001937
Epoch:11, Train loss:0.002371, valid loss:0.001271
Epoch:12, Train loss:0.002266, valid loss:0.001048
Epoch:13, Train loss:0.002176, valid loss:0.001192
Epoch:14, Train loss:0.002132, valid loss:0.001084
Epoch:15, Train loss:0.002044, valid loss:0.001073
Epoch:16, Train loss:0.002073, valid loss:0.001268
Epoch:17, Train loss:0.001923, valid loss:0.000984
Epoch:18, Train loss:0.001837, valid loss:0.001101
Epoch:19, Train loss:0.001787, valid loss:0.001138
Epoch:20, Train loss:0.001822, valid loss:0.001085
Epoch:21, Train loss:0.001379, valid loss:0.000845
Epoch:22, Train loss:0.001327, valid loss:0.000868
Epoch:23, Train loss:0.001317, valid loss:0.000798
Epoch:24, Train loss:0.001267, valid loss:0.000854
Epoch:25, Train loss:0.001275, valid loss:0.000886
Epoch:26, Train loss:0.001215, valid loss:0.000770
Epoch:27, Train loss:0.001229, valid loss:0.000753
Epoch:28, Train loss:0.001195, valid loss:0.000819
Epoch:29, Train loss:0.001207, valid loss:0.000931
Epoch:30, Train loss:0.001195, valid loss:0.000904
Epoch:31, Train loss:0.000964, valid loss:0.000738
Epoch:32, Train loss:0.000944, valid loss:0.000742
Epoch:33, Train loss:0.000945, valid loss:0.000697
Epoch:34, Train loss:0.000918, valid loss:0.000698
Epoch:35, Train loss:0.000917, valid loss:0.000754
Epoch:36, Train loss:0.000905, valid loss:0.000733
Epoch:37, Train loss:0.000898, valid loss:0.000704
Epoch:38, Train loss:0.000896, valid loss:0.000709
Epoch:39, Train loss:0.000879, valid loss:0.000734
Epoch:40, Train loss:0.000876, valid loss:0.000723
Epoch:41, Train loss:0.000774, valid loss:0.000717
Epoch:42, Train loss:0.000770, valid loss:0.000704
Epoch:43, Train loss:0.000765, valid loss:0.000679
Epoch:44, Train loss:0.000763, valid loss:0.000672
Epoch:45, Train loss:0.000753, valid loss:0.000708
Epoch:46, Train loss:0.000756, valid loss:0.000688
Epoch:47, Train loss:0.000757, valid loss:0.000687
Epoch:48, Train loss:0.000742, valid loss:0.000686
Epoch:49, Train loss:0.000739, valid loss:0.000680
Epoch:50, Train loss:0.000735, valid loss:0.000719
Epoch:51, Train loss:0.000683, valid loss:0.000672
Epoch:52, Train loss:0.000676, valid loss:0.000676
Epoch:53, Train loss:0.000673, valid loss:0.000673
Epoch:54, Train loss:0.000671, valid loss:0.000670
Epoch:55, Train loss:0.000670, valid loss:0.000674
Epoch:56, Train loss:0.000669, valid loss:0.000680
Epoch:57, Train loss:0.000669, valid loss:0.000672
Epoch:58, Train loss:0.000668, valid loss:0.000676
Epoch:59, Train loss:0.000668, valid loss:0.000678
Epoch:60, Train loss:0.000667, valid loss:0.000678
training time 9329.766389369965
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.33321943801114773
plot_id,batch_id 0 1 miss% 0.40385230174380427
plot_id,batch_id 0 2 miss% 0.4885537076014988
plot_id,batch_id 0 3 miss% 0.38904421012500173
plot_id,batch_id 0 4 miss% 0.3343590803127931
plot_id,batch_id 0 5 miss% 0.36240682798677104
plot_id,batch_id 0 6 miss% 0.3117051183102209
plot_id,batch_id 0 7 miss% 0.5217063067824834
plot_id,batch_id 0 8 miss% 0.5710796293301692
plot_id,batch_id 0 9 miss% 0.6069152678141357
plot_id,batch_id 0 10 miss% 0.2907386780900513
plot_id,batch_id 0 11 miss% 0.31050362645248886
plot_id,batch_id 0 12 miss% 0.33841554796805307
plot_id,batch_id 0 13 miss% 0.3289695754849388
plot_id,batch_id 0 14 miss% 0.471309421969757
plot_id,batch_id 0 15 miss% 0.30627311392380113
plot_id,batch_id 0 16 miss% 0.4828880850077879
plot_id,batch_id 0 17 miss% 0.41377079614558226
plot_id,batch_id 0 18 miss% 0.4818635542999817
plot_id,batch_id 0 19 miss% 0.4204426760575644
plot_id,batch_id 0 20 miss% 0.3745554496475929
plot_id,batch_id 0 21 miss% 0.5479904131654032
plot_id,batch_id 0 22 miss% 0.48202061874077323
plot_id,batch_id 0 23 miss% 0.49691338312199274
plot_id,batch_id 0 24 miss% 0.4510578451885409
plot_id,batch_id 0 25 miss% 0.3508927502667854
plot_id,batch_id 0 26 miss% 0.47819164614203674
plot_id,batch_id 0 27 miss% 0.4789361111625568
plot_id,batch_id 0 28 miss% 0.5055845371604747
plot_id,batch_id 0 29 miss% 0.43989239907622457
plot_id,batch_id 0 30 miss% 0.2761754114327316
plot_id,batch_id 0 31 miss% 0.4623470033237176
plot_id,batch_id 0 32 miss% 0.49768004745892525
plot_id,batch_id 0 33 miss% 0.4931663781284556
plot_id,batch_id 0 34 miss% 0.39434084806794223
plot_id,batch_id 0 35 miss% 0.3379441245042516
plot_id,batch_id 0 36 miss% 0.46382179719038946
plot_id,batch_id 0 37 miss% 0.4454354442645665
plot_id,batch_id 0 38 miss% 0.48595109011309345
plot_id,batch_id 0 39 miss% 0.44049239272414553
plot_id,batch_id 0 40 miss% 0.40381295458871397
plot_id,batch_id 0 41 miss% 0.40423551419181564
plot_id,batch_id 0 42 miss% 0.39521817177197277
plot_id,batch_id 0 43 miss% 0.349611903040921
plot_id,batch_id 0 44 miss% 0.4616433355070196
plot_id,batch_id 0 45 miss% 0.2709592278049015
plot_id,batch_id 0 46 miss% 0.3880643251174988
plot_id,batch_id 0 47 miss% 0.5067285074982887
plot_id,batch_id 0 48 miss% 0.48668207824481796
plot_id,batch_id 0 49 miss% 0.32615810470867185
plot_id,batch_id 0 50 miss% 0.5038174201451882
plot_id,batch_id 0 51 miss% 0.5184920961190996
plot_id,batch_id 0 52 miss% 0.4848545918108129
plot_id,batch_id 0 53 miss% 0.34848607903960604
plot_id,batch_id 0 54 miss% 0.40161853053602264
plot_id,batch_id 0 55 miss% 0.5100985539828725
plot_id,batch_id 0 56 miss% 0.5629119003629705
plot_id,batch_id 0 57 miss% 0.5245092019620268
plot_id,batch_id 0 58 miss% 0.44708509210600766
plot_id,batch_id 0 59 miss% 0.42079207738981406
plot_id,batch_id 0 60 miss% 0.23140189509655884
plot_id,batch_id 0 61 miss% 0.2490961895319411
plot_id,batch_id 0 62 miss% 0.34447290025957855
plot_id,batch_id 0 63 miss% 0.3782919994853934
plot_id,batch_id 0 64 miss% 0.48729902318061935
plot_id,batch_id 0 65 miss% 0.3082967674360804
plot_id,batch_id 0 66 miss% 0.4175059795068672
plot_id,batch_id 0 67 miss% 0.28818077062659553
plot_id,batch_id 0 68 miss% 0.42243605733226003
plot_id,batch_id 0 69 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  89105
Epoch:0, Train loss:0.731501, valid loss:0.682973
Epoch:1, Train loss:0.133579, valid loss:0.006299
Epoch:2, Train loss:0.016294, valid loss:0.005341
Epoch:3, Train loss:0.014969, valid loss:0.005163
Epoch:4, Train loss:0.013695, valid loss:0.004286
Epoch:5, Train loss:0.009510, valid loss:0.003262
Epoch:6, Train loss:0.005369, valid loss:0.001962
Epoch:7, Train loss:0.004127, valid loss:0.001838
Epoch:8, Train loss:0.003618, valid loss:0.001763
Epoch:9, Train loss:0.003476, valid loss:0.001637
Epoch:10, Train loss:0.003258, valid loss:0.001697
Epoch:11, Train loss:0.002486, valid loss:0.001419
Epoch:12, Train loss:0.002422, valid loss:0.001328
Epoch:13, Train loss:0.002341, valid loss:0.001124
Epoch:14, Train loss:0.002240, valid loss:0.001211
Epoch:15, Train loss:0.002150, valid loss:0.001100
Epoch:16, Train loss:0.002106, valid loss:0.001123
Epoch:17, Train loss:0.002014, valid loss:0.001051
Epoch:18, Train loss:0.001930, valid loss:0.001050
Epoch:19, Train loss:0.001896, valid loss:0.000951
Epoch:20, Train loss:0.001852, valid loss:0.000990
Epoch:21, Train loss:0.001517, valid loss:0.000914
Epoch:22, Train loss:0.001486, valid loss:0.000917
Epoch:23, Train loss:0.001480, valid loss:0.000851
Epoch:24, Train loss:0.001417, valid loss:0.000975
Epoch:25, Train loss:0.001426, valid loss:0.000868
Epoch:26, Train loss:0.001369, valid loss:0.000823
Epoch:27, Train loss:0.001356, valid loss:0.000849
Epoch:28, Train loss:0.001344, valid loss:0.000889
Epoch:29, Train loss:0.001315, valid loss:0.000952
Epoch:30, Train loss:0.001302, valid loss:0.000821
Epoch:31, Train loss:0.001135, valid loss:0.000778
Epoch:32, Train loss:0.001115, valid loss:0.000757
Epoch:33, Train loss:0.001109, valid loss:0.000804
Epoch:34, Train loss:0.001093, valid loss:0.000775
Epoch:35, Train loss:0.001079, valid loss:0.000750
Epoch:36, Train loss:0.001066, valid loss:0.000731
Epoch:37, Train loss:0.001062, valid loss:0.000714
Epoch:38, Train loss:0.001058, valid loss:0.000751
Epoch:39, Train loss:0.001039, valid loss:0.000777
Epoch:40, Train loss:0.001040, valid loss:0.000759
Epoch:41, Train loss:0.000948, valid loss:0.000702
Epoch:42, Train loss:0.000942, valid loss:0.000715
Epoch:43, Train loss:0.000940, valid loss:0.000711
Epoch:44, Train loss:0.000933, valid loss:0.000713
Epoch:45, Train loss:0.000929, valid loss:0.000727
Epoch:46, Train loss:0.000925, valid loss:0.000698
Epoch:47, Train loss:0.000920, valid loss:0.000704
Epoch:48, Train loss:0.000914, valid loss:0.000700
Epoch:49, Train loss:0.000912, valid loss:0.000681
Epoch:50, Train loss:0.000910, valid loss:0.000733
Epoch:51, Train loss:0.000859, valid loss:0.000682
Epoch:52, Train loss:0.000854, valid loss:0.000673
Epoch:53, Train loss:0.000853, valid loss:0.000675
Epoch:54, Train loss:0.000852, valid loss:0.000679
Epoch:55, Train loss:0.000850, valid loss:0.000691
Epoch:56, Train loss:0.000850, valid loss:0.000676
Epoch:57, Train loss:0.000849, valid loss:0.000671
Epoch:58, Train loss:0.000849, valid loss:0.000671
Epoch:59, Train loss:0.000849, valid loss:0.000679
Epoch:60, Train loss:0.000848, valid loss:0.000677
training time 9340.575456142426
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.28871060196487086
plot_id,batch_id 0 1 miss% 0.39552105000239324
plot_id,batch_id 0 2 miss% 0.44370287598289515
plot_id,batch_id 0 3 miss% 0.33472170255843403
plot_id,batch_id 0 4 miss% 0.3211655972163756
plot_id,batch_id 0 5 miss% 0.30898972222520654
plot_id,batch_id 0 6 miss% 0.3325812090439141
plot_id,batch_id 0 7 miss% 0.4938529197728113
plot_id,batch_id 0 8 miss% 0.6148975014014508
plot_id,batch_id 0 9 miss% 0.5525273846455847
plot_id,batch_id 0 10 miss% 0.23781074565547392
plot_id,batch_id 0 11 miss% 0.33285219655176385
plot_id,batch_id 0 12 miss% 0.33661695582703516
plot_id,batch_id 0 13 miss% 0.3277882181407418
plot_id,batch_id 0 14 miss% 0.41794624510926764
plot_id,batch_id 0 15 miss% 0.3313330300907074
plot_id,batch_id 0 16 miss% 0.4683530626755801
plot_id,batch_id 0 17 miss% 0.4317380646451209
plot_id,batch_id 0 18 miss% 0.4325271986212717
plot_id,batch_id 0 19 miss% 0.37231585385649824
plot_id,batch_id 0 20 miss% 0.3547258814430096
plot_id,batch_id 0 21 miss% 0.5025640316116016
plot_id,batch_id 0 22 miss% 0.4591443112833018
plot_id,batch_id 0 23 miss% 0.46811340456962824
plot_id,batch_id 0 24 miss% 0.38541074579718937
plot_id,batch_id 0 25 miss% 0.357314211855168
plot_id,batch_id 0 26 miss% 0.3943683332125602
plot_id,batch_id 0 27 miss% 0.4120647417408786
plot_id,batch_id 0 28 miss% 0.4717605370202891
plot_id,batch_id 0 29 miss% 0.4356187146036492
plot_id,batch_id 0 30 miss% 0.270566928046438
plot_id,batch_id 0 31 miss% 0.4633580265504201
plot_id,batch_id 0 32 miss% 0.42675080034651897
plot_id,batch_id 0 33 miss% 0.44781129799111835
plot_id,batch_id 0 34 miss% 0.4042610591914185
plot_id,batch_id 0 35 miss% 0.3417751513862804
plot_id,batch_id 0 36 miss% 0.4735412703157503
plot_id,batch_id 0 37 miss% 0.42047018100778094
plot_id,batch_id 0 38 miss% 0.5068166389695598
plot_id,batch_id 0 39 miss% 0.5096990102962206
plot_id,batch_id 0 40 miss% 0.40390230931702004
plot_id,batch_id 0 41 miss% 0.39827257200312877
plot_id,batch_id 0 42 miss% 0.3547178235540821
plot_id,batch_id 0 43 miss% 0.322367447443656
plot_id,batch_id 0 44 miss% 0.3532124681467627
plot_id,batch_id 0 45 miss% 0.38857473886587124
plot_id,batch_id 0 46 miss% 0.3986297146390825
plot_id,batch_id 0 47 miss% 0.46330331418155063
plot_id,batch_id 0 48 miss% 0.4434518611679262
plot_id,batch_id 0 49 miss% 0.2978108395971009
plot_id,batch_id 0 50 miss% 0.5288674759449776
plot_id,batch_id 0 51 miss% 0.48289266420912735
plot_id,batch_id 0 52 miss% 0.502538959551555
plot_id,batch_id 0 53 miss% 0.4256762145144236
plot_id,batch_id 0 54 miss% 0.4059662061539244
plot_id,batch_id 0 55 miss% 0.4553006302223901
plot_id,batch_id 0 56 miss% 0.5614131248254814
plot_id,batch_id 0 57 miss% 0.4817283538755835
plot_id,batch_id 0 58 miss% 0.46048496817546725
plot_id,batch_id 0 59 miss% 0.49391966729321746
plot_id,batch_id 0 60 miss% 0.2884445597226517
plot_id,batch_id 0 61 miss% 0.2947589412168004
plot_id,batch_id 0 62 miss% 0.4893909010843351
plot_id,batch_id 0 63 miss% 0.35988282723481857
plot_id,batch_id 0 64 miss% 0.39544945814022003
plot_id,batch_id 0 65 miss% 0.2665518389833058
plot_id,batch_id 0 66 miss% 0.3589059099827573
plot_id,batch_id 0 67 miss% 0.2618570545886912
plot_id,batch_id 0 68 miss% 0.385497136666403
plot_id,batch_id 0 69 miss% 0.4252606416577151
0.30767756633721716
plot_id,batch_id 0 71 miss% 0.4262318093312147
plot_id,batch_id 0 72 miss% 0.4574996132815509
plot_id,batch_id 0 73 miss% 0.3830270895394101
plot_id,batch_id 0 74 miss% 0.4721428833009212
plot_id,batch_id 0 75 miss% 0.36465272330820475
plot_id,batch_id 0 76 miss% 0.4382815543607985
plot_id,batch_id 0 77 miss% 0.3909814848681869
plot_id,batch_id 0 78 miss% 0.3896334763846739
plot_id,batch_id 0 79 miss% 0.4191207423530772
plot_id,batch_id 0 80 miss% 0.3693139155712848
plot_id,batch_id 0 81 miss% 0.48881840796711645
plot_id,batch_id 0 82 miss% 0.41845055786346247
plot_id,batch_id 0 83 miss% 0.46417070296836294
plot_id,batch_id 0 84 miss% 0.3658807826545311
plot_id,batch_id 0 85 miss% 0.35844775669082574
plot_id,batch_id 0 86 miss% 0.4335780569136903
plot_id,batch_id 0 87 miss% 0.4197612555495486
plot_id,batch_id 0 88 miss% 0.5274451473394222
plot_id,batch_id 0 89 miss% 0.4732543585807122
plot_id,batch_id 0 90 miss% 0.2780713004635105
plot_id,batch_id 0 91 miss% 0.38726890731772373
plot_id,batch_id 0 92 miss% 0.3671619039418799
plot_id,batch_id 0 93 miss% 0.34770520664564725
plot_id,batch_id 0 94 miss% 0.47006208708853997
plot_id,batch_id 0 95 miss% 0.31309232115059416
plot_id,batch_id 0 96 miss% 0.3832865696915807
plot_id,batch_id 0 97 miss% 0.499404962523266
plot_id,batch_id 0 98 miss% 0.5124220510182899
plot_id,batch_id 0 99 miss% 0.43661930868701215
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.36810442 0.43509912 0.53422291 0.39447043 0.34142759 0.38969626
 0.44706698 0.53237845 0.6187388  0.4673576  0.34482676 0.40599266
 0.4426568  0.40366679 0.4892993  0.36421088 0.52371845 0.48459058
 0.44972944 0.43608792 0.39142952 0.46728149 0.51340381 0.44405363
 0.40948526 0.42785067 0.43344107 0.45895712 0.55382612 0.41822366
 0.39304474 0.50871253 0.51445499 0.48876183 0.33952086 0.45992125
 0.55391306 0.36437865 0.41374968 0.40182792 0.44954149 0.42845481
 0.29863895 0.34197267 0.37791535 0.31990595 0.44634987 0.44691014
 0.35170531 0.35950202 0.60729398 0.50999089 0.52652181 0.29556957
 0.40577375 0.53466826 0.55073879 0.62601642 0.54084312 0.3794924
 0.30758873 0.30994636 0.41357653 0.4614427  0.46106881 0.34503946
 0.47897113 0.36915761 0.47466385 0.49443388 0.30767757 0.42623181
 0.45749961 0.38302709 0.47214288 0.36465272 0.43828155 0.39098148
 0.38963348 0.41912074 0.36931392 0.48881841 0.41845056 0.4641707
 0.36588078 0.35844776 0.43357806 0.41976126 0.52744515 0.47325436
 0.2780713  0.38726891 0.3671619  0.34770521 0.47006209 0.31309232
 0.38328657 0.49940496 0.51242205 0.43661931]
for model  31 the mean error 0.4310673913510686
all id 31 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 21 out win 5 err 0.4310673913510686 time 9331.11327457428
Launcher: Job 32 completed in 9568 seconds.
Launcher: Task 176 done. Exiting.
0.4468284273094215
plot_id,batch_id 0 70 miss% 0.25038146156407
plot_id,batch_id 0 71 miss% 0.3967901184373032
plot_id,batch_id 0 72 miss% 0.4084764059864969
plot_id,batch_id 0 73 miss% 0.2955183940733515
plot_id,batch_id 0 74 miss% 0.3204917645401259
plot_id,batch_id 0 75 miss% 0.2516572520828772
plot_id,batch_id 0 76 miss% 0.25847383257221795
plot_id,batch_id 0 77 miss% 0.27420793688110884
plot_id,batch_id 0 78 miss% 0.2913386505229228
plot_id,batch_id 0 79 miss% 0.34745976362592473
plot_id,batch_id 0 80 miss% 0.26814244915134106
plot_id,batch_id 0 81 miss% 0.4956567481732061
plot_id,batch_id 0 82 miss% 0.4032199618715929
plot_id,batch_id 0 83 miss% 0.43013060601615033
plot_id,batch_id 0 84 miss% 0.4761237322334452
plot_id,batch_id 0 85 miss% 0.24276490304972626
plot_id,batch_id 0 86 miss% 0.3037427532547013
plot_id,batch_id 0 87 miss% 0.38120495226317613
plot_id,batch_id 0 88 miss% 0.38499935031515337
plot_id,batch_id 0 89 miss% 0.33764386246136474
plot_id,batch_id 0 90 miss% 0.21723348793531333
plot_id,batch_id 0 91 miss% 0.36961510636447464
plot_id,batch_id 0 92 miss% 0.29974706297277776
plot_id,batch_id 0 93 miss% 0.3439868295855073
plot_id,batch_id 0 94 miss% 0.468716569519672
plot_id,batch_id 0 95 miss% 0.22127383939681294
plot_id,batch_id 0 96 miss% 0.30240453923693045
plot_id,batch_id 0 97 miss% 0.3418533166019825
plot_id,batch_id 0 98 miss% 0.3348064356416305
plot_id,batch_id 0 99 miss% 0.30688314458762045
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.33321944 0.4038523  0.48855371 0.38904421 0.33435908 0.36240683
 0.31170512 0.52170631 0.57107963 0.60691527 0.29073868 0.31050363
 0.33841555 0.32896958 0.47130942 0.30627311 0.48288809 0.4137708
 0.48186355 0.42044268 0.37455545 0.54799041 0.48202062 0.49691338
 0.45105785 0.35089275 0.47819165 0.47893611 0.50558454 0.4398924
 0.27617541 0.462347   0.49768005 0.49316638 0.39434085 0.33794412
 0.4638218  0.44543544 0.48595109 0.44049239 0.40381295 0.40423551
 0.39521817 0.3496119  0.46164334 0.27095923 0.38806433 0.50672851
 0.48668208 0.3261581  0.50381742 0.5184921  0.48485459 0.34848608
 0.40161853 0.51009855 0.5629119  0.5245092  0.44708509 0.42079208
 0.2314019  0.24909619 0.3444729  0.378292   0.48729902 0.30829677
 0.41750598 0.28818077 0.42243606 0.44682843 0.25038146 0.39679012
 0.40847641 0.29551839 0.32049176 0.25165725 0.25847383 0.27420794
 0.29133865 0.34745976 0.26814245 0.49565675 0.40321996 0.43013061
 0.47612373 0.2427649  0.30374275 0.38120495 0.38499935 0.33764386
 0.21723349 0.36961511 0.29974706 0.34398683 0.46871657 0.22127384
 0.30240454 0.34185332 0.33480644 0.30688314]
for model  30 the mean error 0.39385941560911975
all id 30 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 21 out win 4 err 0.39385941560911975 time 9329.766389369965
Launcher: Job 31 completed in 9576 seconds.
Launcher: Task 161 done. Exiting.
plot_id,batch_id 0 70 miss% 0.2779895204382484
plot_id,batch_id 0 71 miss% 0.36420682971721485
plot_id,batch_id 0 72 miss% 0.35938401888530846
plot_id,batch_id 0 73 miss% 0.3361022399022581
plot_id,batch_id 0 74 miss% 0.3113668837569602
plot_id,batch_id 0 75 miss% 0.2616922765846502
plot_id,batch_id 0 76 miss% 0.30135176185817486
plot_id,batch_id 0 77 miss% 0.3242432719559057
plot_id,batch_id 0 78 miss% 0.28158525706938775
plot_id,batch_id 0 79 miss% 0.3493994041308326
plot_id,batch_id 0 80 miss% 0.2805343647783053
plot_id,batch_id 0 81 miss% 0.49380337833111676
plot_id,batch_id 0 82 miss% 0.31849898437121327
plot_id,batch_id 0 83 miss% 0.4092316114399727
plot_id,batch_id 0 84 miss% 0.4191190557712753
plot_id,batch_id 0 85 miss% 0.25731668911279154
plot_id,batch_id 0 86 miss% 0.36429170185279486
plot_id,batch_id 0 87 miss% 0.4050816911819665
plot_id,batch_id 0 88 miss% 0.3673671513042144
plot_id,batch_id 0 89 miss% 0.36287536688624566
plot_id,batch_id 0 90 miss% 0.19306091053911587
plot_id,batch_id 0 91 miss% 0.24139805357913505
plot_id,batch_id 0 92 miss% 0.32014032242156865
plot_id,batch_id 0 93 miss% 0.29138314332388826
plot_id,batch_id 0 94 miss% 0.40508205163047395
plot_id,batch_id 0 95 miss% 0.2729839401472237
plot_id,batch_id 0 96 miss% 0.24533084643185504
plot_id,batch_id 0 97 miss% 0.38231523759580316
plot_id,batch_id 0 98 miss% 0.34547216924556534
plot_id,batch_id 0 99 miss% 0.40694298074721
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.2887106  0.39552105 0.44370288 0.3347217  0.3211656  0.30898972
 0.33258121 0.49385292 0.6148975  0.55252738 0.23781075 0.3328522
 0.33661696 0.32778822 0.41794625 0.33133303 0.46835306 0.43173806
 0.4325272  0.37231585 0.35472588 0.50256403 0.45914431 0.4681134
 0.38541075 0.35731421 0.39436833 0.41206474 0.47176054 0.43561871
 0.27056693 0.46335803 0.4267508  0.4478113  0.40426106 0.34177515
 0.47354127 0.42047018 0.50681664 0.50969901 0.40390231 0.39827257
 0.35471782 0.32236745 0.35321247 0.38857474 0.39862971 0.46330331
 0.44345186 0.29781084 0.52886748 0.48289266 0.50253896 0.42567621
 0.40596621 0.45530063 0.56141312 0.48172835 0.46048497 0.49391967
 0.28844456 0.29475894 0.4893909  0.35988283 0.39544946 0.26655184
 0.35890591 0.26185705 0.38549714 0.42526064 0.27798952 0.36420683
 0.35938402 0.33610224 0.31136688 0.26169228 0.30135176 0.32424327
 0.28158526 0.3493994  0.28053436 0.49380338 0.31849898 0.40923161
 0.41911906 0.25731669 0.3642917  0.40508169 0.36736715 0.36287537
 0.19306091 0.24139805 0.32014032 0.29138314 0.40508205 0.27298394
 0.24533085 0.38231524 0.34547217 0.40694298]
for model  3 the mean error 0.3838066915117689
all id 3 hidden_dim 24 learning_rate 0.0025 num_layers 3 frames 21 out win 4 err 0.3838066915117689 time 9340.575456142426
Launcher: Job 4 completed in 9591 seconds.
Launcher: Task 63 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  55697
Epoch:0, Train loss:0.715920, valid loss:0.678783
Epoch:1, Train loss:0.462256, valid loss:0.009521
Epoch:2, Train loss:0.019145, valid loss:0.007118
Epoch:3, Train loss:0.017062, valid loss:0.005247
Epoch:4, Train loss:0.016001, valid loss:0.005308
Epoch:5, Train loss:0.015466, valid loss:0.004723
Epoch:6, Train loss:0.015058, valid loss:0.004697
Epoch:7, Train loss:0.014848, valid loss:0.005464
Epoch:8, Train loss:0.014576, valid loss:0.004733
Epoch:9, Train loss:0.014263, valid loss:0.004478
Epoch:10, Train loss:0.014121, valid loss:0.004464
Epoch:11, Train loss:0.013433, valid loss:0.004016
Epoch:12, Train loss:0.013311, valid loss:0.004104
Epoch:13, Train loss:0.013331, valid loss:0.004167
Epoch:14, Train loss:0.013195, valid loss:0.004194
Epoch:15, Train loss:0.013120, valid loss:0.004141
Epoch:16, Train loss:0.011651, valid loss:0.002848
Epoch:17, Train loss:0.006764, valid loss:0.002869
Epoch:18, Train loss:0.005503, valid loss:0.002370
Epoch:19, Train loss:0.004670, valid loss:0.002309
Epoch:20, Train loss:0.004014, valid loss:0.001620
Epoch:21, Train loss:0.002835, valid loss:0.001538
Epoch:22, Train loss:0.002736, valid loss:0.001471
Epoch:23, Train loss:0.002653, valid loss:0.001411
Epoch:24, Train loss:0.002624, valid loss:0.001436
Epoch:25, Train loss:0.002515, valid loss:0.001388
Epoch:26, Train loss:0.002482, valid loss:0.001406
Epoch:27, Train loss:0.002427, valid loss:0.001355
Epoch:28, Train loss:0.002311, valid loss:0.001439
Epoch:29, Train loss:0.002317, valid loss:0.001314
Epoch:30, Train loss:0.002299, valid loss:0.001371
Epoch:31, Train loss:0.001964, valid loss:0.001260
Epoch:32, Train loss:0.001912, valid loss:0.001186
Epoch:33, Train loss:0.001876, valid loss:0.001401
Epoch:34, Train loss:0.001852, valid loss:0.001201
Epoch:35, Train loss:0.001831, valid loss:0.001202
Epoch:36, Train loss:0.001794, valid loss:0.001171
Epoch:37, Train loss:0.001789, valid loss:0.001174
Epoch:38, Train loss:0.001767, valid loss:0.001194
Epoch:39, Train loss:0.001744, valid loss:0.001179
Epoch:40, Train loss:0.001722, valid loss:0.001287
Epoch:41, Train loss:0.001573, valid loss:0.001120
Epoch:42, Train loss:0.001552, valid loss:0.001118
Epoch:43, Train loss:0.001529, valid loss:0.001206
Epoch:44, Train loss:0.001534, valid loss:0.001149
Epoch:45, Train loss:0.001504, valid loss:0.001143
Epoch:46, Train loss:0.001494, valid loss:0.001156
Epoch:47, Train loss:0.001471, valid loss:0.001121
Epoch:48, Train loss:0.001479, valid loss:0.001083
Epoch:49, Train loss:0.001467, valid loss:0.001131
Epoch:50, Train loss:0.001461, valid loss:0.001093
Epoch:51, Train loss:0.001376, valid loss:0.001074
Epoch:52, Train loss:0.001359, valid loss:0.001074
Epoch:53, Train loss:0.001353, valid loss:0.001076
Epoch:54, Train loss:0.001349, valid loss:0.001076
Epoch:55, Train loss:0.001348, valid loss:0.001083
Epoch:56, Train loss:0.001346, valid loss:0.001078
Epoch:57, Train loss:0.001343, valid loss:0.001080
Epoch:58, Train loss:0.001342, valid loss:0.001081
Epoch:59, Train loss:0.001341, valid loss:0.001085
Epoch:60, Train loss:0.001339, valid loss:0.001083
training time 9380.594336748123
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.09195565615440347
plot_id,batch_id 0 1 miss% 0.07686929752313991
plot_id,batch_id 0 2 miss% 0.11042414156694186
plot_id,batch_id 0 3 miss% 0.041762088869357825
plot_id,batch_id 0 4 miss% 0.0394163278807133
plot_id,batch_id 0 5 miss% 0.05557350450787949
plot_id,batch_id 0 6 miss% 0.07826425767128042
plot_id,batch_id 0 7 miss% 0.08840957953626404
plot_id,batch_id 0 8 miss% 0.084317019370011
plot_id,batch_id 0 9 miss% 0.04359511184178875
plot_id,batch_id 0 10 miss% 0.05427524740033024
plot_id,batch_id 0 11 miss% 0.05791103456456678
plot_id,batch_id 0 12 miss% 0.05641664437204258
plot_id,batch_id 0 13 miss% 0.08497004824276726
plot_id,batch_id 0 14 miss% 0.09505805524236399
plot_id,batch_id 0 15 miss% 0.048210452145205744
plot_id,batch_id 0 16 miss% 0.08389769114768258
plot_id,batch_id 0 17 miss% 0.06429642067156256
plot_id,batch_id 0 18 miss% 0.05115550455611114
plot_id,batch_id 0 19 miss% 0.07801305937516397
plot_id,batch_id 0 20 miss% 0.11128804587661789
plot_id,batch_id 0 21 miss% 0.04057733921243186
plot_id,batch_id 0 22 miss% 0.04919469788761844
plot_id,batch_id 0 23 miss% 0.03139779838310631
plot_id,batch_id 0 24 miss% 0.059876561038165264
plot_id,batch_id 0 25 miss% 0.047954172507006815
plot_id,batch_id 0 26 miss% 0.05228496050484971
plot_id,batch_id 0 27 miss% 0.023105391162854337
plot_id,batch_id 0 28 miss% 0.03389270191605508
plot_id,batch_id 0 29 miss% 0.02516889555936842
plot_id,batch_id 0 30 miss% 0.04970318422324376
plot_id,batch_id 0 31 miss% 0.10131833738098611
plot_id,batch_id 0 32 miss% 0.12586273962265795
plot_id,batch_id 0 33 miss% 0.05933695519264996
plot_id,batch_id 0 34 miss% 0.05256326065383214
plot_id,batch_id 0 35 miss% 0.03561433349900629
plot_id,batch_id 0 36 miss% 0.09192556122448332
plot_id,batch_id 0 37 miss% 0.06176533999165589
plot_id,batch_id 0 38 miss% 0.030284785723546332
plot_id,batch_id 0 39 miss% 0.0315997177634174
plot_id,batch_id 0 40 miss% 0.09282474239771385
plot_id,batch_id 0 41 miss% 0.029991237849315466
plot_id,batch_id 0 42 miss% 0.04105273479655021
plot_id,batch_id 0 43 miss% 0.08423688761177452
plot_id,batch_id 0 44 miss% 0.0560850314720317
plot_id,batch_id 0 45 miss% 0.044122932881816644
plot_id,batch_id 0 46 miss% 0.04790173515273532
plot_id,batch_id 0 47 miss% 0.03791412229693015
plot_id,batch_id 0 48 miss% 0.04053811661799864
plot_id,batch_id 0 49 miss% 0.034473723994589194
plot_id,batch_id 0 50 miss% 0.12523269554790992
plot_id,batch_id 0 51 miss% 0.036445445140281904
plot_id,batch_id 0 52 miss% 0.024379199546931133
plot_id,batch_id 0 53 miss% 0.04240679151470114
plot_id,batch_id 0 54 miss% 0.05787005251751302
plot_id,batch_id 0 55 miss% 0.0808161708613415
plot_id,batch_id 0 56 miss% 0.050268983715790035
plot_id,batch_id 0 57 miss% 0.034524552159816174
plot_id,batch_id 0 58 miss% 0.030925479585070293
plot_id,batch_id 0 59 miss% 0.01729470732577743
plot_id,batch_id 0 60 miss% 0.05011709927289267
plot_id,batch_id 0 61 miss% 0.03672708079061763
plot_id,batch_id 0 62 miss% 0.07085433944537566
plot_id,batch_id 0 63 miss% 0.07456406182519831
plot_id,batch_id 0 64 miss% 0.0689843421834829
plot_id,batch_id 0 65 miss% 0.067230597011902
plot_id,batch_id 0 66 miss% 0.04699580069691281
plot_id,batch_id 0 67 miss% 0.023954363893904578
plot_id,batch_id 0 68 miss% 0.06780051377828268
plot_id,batch_id 0 69 miss% 0.07111807993930544
plot_id,batch_id 0 70 miss% 0.052851322623223264
plot_id,batch_id 0 71 miss% 0.05433822611901658
plot_id,batch_id 0 72 miss% 0.1079263327156748
plot_id,batch_id 0 73 miss% 0.06229711255462597
plot_id,batch_id 0 74 miss% 0.11582459021488345
plot_id,batch_id 0 75 miss% 0.044620281942301346
plot_id,batch_id 0 76 miss% 0.10534723407982714
plot_id,batch_id 0 77 miss% 0.07847230456807627
plot_id,batch_id 0 78 miss% 0.05661746933728855
plot_id,batch_id 0 79 miss% 0.05728105175882494
plot_id,batch_id 0 80 miss% 0.04128165711598251
plot_id,batch_id 0 81 miss% 0.09247438289557647
plot_id,batch_id 0 82 miss% 0.06865786426703564
plot_id,batch_id 0 83 miss% 0.061232424526650835
plot_id,batch_id 0 84 miss% 0.1335327680868973
plot_id,batch_id 0 85 miss% 0.08076955380642821
plot_id,batch_id 0 86 miss% 0.05338373415241342
plot_id,batch_id 0 87 miss% 0.06641044949100174
plot_id,batch_id 0 88 miss% 0.0708416788577929
plot_id,batch_id 0 89 miss% 0.0842379456173175
plot_id,batch_id 0 90 miss% 0.05284789522815286
plot_id,batch_id 0 91 miss% 0.055194171459944
plot_id,batch_id 0 92 miss% 0.05849954409016691
plot_id,batch_id 0 93 miss% 0.10713226486194553
plot_id,batch_id 0 94 miss% 0.14337468129629397
plot_id,batch_id 0 95 miss% 0.07254576745871388
plot_id,batch_id 0 96 miss% 0.07069202133071062
plot_id,batch_id 0 97 miss% 0.06300627296383719
plot_id,batch_id 0 98 miss% 0.05655718831283077
plot_id,batch_id 0 99 miss% 0.10418200304322749
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09195566 0.0768693  0.11042414 0.04176209 0.03941633 0.0555735
 0.07826426 0.08840958 0.08431702 0.04359511 0.05427525 0.05791103
 0.05641664 0.08497005 0.09505806 0.04821045 0.08389769 0.06429642
 0.0511555  0.07801306 0.11128805 0.04057734 0.0491947  0.0313978
 0.05987656 0.04795417 0.05228496 0.02310539 0.0338927  0.0251689
 0.04970318 0.10131834 0.12586274 0.05933696 0.05256326 0.03561433
 0.09192556 0.06176534 0.03028479 0.03159972 0.09282474 0.02999124
 0.04105273 0.08423689 0.05608503 0.04412293 0.04790174 0.03791412
 0.04053812 0.03447372 0.1252327  0.03644545 0.0243792  0.04240679
 0.05787005 0.08081617 0.05026898 0.03452455 0.03092548 0.01729471
 0.0501171  0.03672708 0.07085434 0.07456406 0.06898434 0.0672306
 0.0469958  0.02395436 0.06780051 0.07111808 0.05285132 0.05433823
 0.10792633 0.06229711 0.11582459 0.04462028 0.10534723 0.0784723
 0.05661747 0.05728105 0.04128166 0.09247438 0.06865786 0.06123242
 0.13353277 0.08076955 0.05338373 0.06641045 0.07084168 0.08423795
 0.0528479  0.05519417 0.05849954 0.10713226 0.14337468 0.07254577
 0.07069202 0.06300627 0.05655719 0.104182  ]
for model  65 the mean error 0.06359587738564254
all id 65 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 21 out win 6 err 0.06359587738564254 time 9380.594336748123
Launcher: Job 66 completed in 9652 seconds.
Launcher: Task 132 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  89105
Epoch:0, Train loss:0.684004, valid loss:0.639811
Epoch:1, Train loss:0.381019, valid loss:0.004820
Epoch:2, Train loss:0.013803, valid loss:0.004187
Epoch:3, Train loss:0.008981, valid loss:0.003338
Epoch:4, Train loss:0.006569, valid loss:0.002729
Epoch:5, Train loss:0.005851, valid loss:0.002700
Epoch:6, Train loss:0.005227, valid loss:0.002302
Epoch:7, Train loss:0.004798, valid loss:0.001995
Epoch:8, Train loss:0.004439, valid loss:0.002408
Epoch:9, Train loss:0.004294, valid loss:0.001843
Epoch:10, Train loss:0.004086, valid loss:0.002097
Epoch:11, Train loss:0.003128, valid loss:0.001513
Epoch:12, Train loss:0.002921, valid loss:0.001537
Epoch:13, Train loss:0.002780, valid loss:0.001733
Epoch:14, Train loss:0.002716, valid loss:0.002056
Epoch:15, Train loss:0.002696, valid loss:0.001499
Epoch:16, Train loss:0.002653, valid loss:0.001455
Epoch:17, Train loss:0.002464, valid loss:0.001363
Epoch:18, Train loss:0.002426, valid loss:0.001515
Epoch:19, Train loss:0.002519, valid loss:0.001429
Epoch:20, Train loss:0.002297, valid loss:0.001427
Epoch:21, Train loss:0.001811, valid loss:0.001083
Epoch:22, Train loss:0.001749, valid loss:0.001100
Epoch:23, Train loss:0.001730, valid loss:0.001194
Epoch:24, Train loss:0.001677, valid loss:0.001160
Epoch:25, Train loss:0.001651, valid loss:0.001218
Epoch:26, Train loss:0.001621, valid loss:0.001083
Epoch:27, Train loss:0.001633, valid loss:0.001156
Epoch:28, Train loss:0.001531, valid loss:0.001216
Epoch:29, Train loss:0.001534, valid loss:0.001158
Epoch:30, Train loss:0.001574, valid loss:0.001099
Epoch:31, Train loss:0.001262, valid loss:0.001008
Epoch:32, Train loss:0.001209, valid loss:0.000961
Epoch:33, Train loss:0.001184, valid loss:0.000987
Epoch:34, Train loss:0.001197, valid loss:0.000975
Epoch:35, Train loss:0.001200, valid loss:0.001030
Epoch:36, Train loss:0.001151, valid loss:0.000967
Epoch:37, Train loss:0.001139, valid loss:0.000983
Epoch:38, Train loss:0.001120, valid loss:0.000988
Epoch:39, Train loss:0.001139, valid loss:0.000986
Epoch:40, Train loss:0.001115, valid loss:0.000915
Epoch:41, Train loss:0.000954, valid loss:0.000901
Epoch:42, Train loss:0.000940, valid loss:0.000897
Epoch:43, Train loss:0.000946, valid loss:0.000934
Epoch:44, Train loss:0.000933, valid loss:0.000921
Epoch:45, Train loss:0.000930, valid loss:0.000921
Epoch:46, Train loss:0.000913, valid loss:0.000926
Epoch:47, Train loss:0.000902, valid loss:0.000952
Epoch:48, Train loss:0.000906, valid loss:0.000923
Epoch:49, Train loss:0.000898, valid loss:0.000986
Epoch:50, Train loss:0.000876, valid loss:0.000960
Epoch:51, Train loss:0.000843, valid loss:0.000891
Epoch:52, Train loss:0.000825, valid loss:0.000873
Epoch:53, Train loss:0.000817, valid loss:0.000882
Epoch:54, Train loss:0.000812, valid loss:0.000869
Epoch:55, Train loss:0.000808, valid loss:0.000865
Epoch:56, Train loss:0.000806, valid loss:0.000867
Epoch:57, Train loss:0.000804, valid loss:0.000867
Epoch:58, Train loss:0.000802, valid loss:0.000872
Epoch:59, Train loss:0.000801, valid loss:0.000871
Epoch:60, Train loss:0.000799, valid loss:0.000871
training time 9494.598680257797
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.4280510360947092
plot_id,batch_id 0 1 miss% 0.5251519996251144
plot_id,batch_id 0 2 miss% 0.5113644706861491
plot_id,batch_id 0 3 miss% 0.4504379947017536
plot_id,batch_id 0 4 miss% 0.38329056796498606
plot_id,batch_id 0 5 miss% 0.38970443610303607
plot_id,batch_id 0 6 miss% 0.38047913772885184
plot_id,batch_id 0 7 miss% 0.5011135484547053
plot_id,batch_id 0 8 miss% 0.5324184439411466
plot_id,batch_id 0 9 miss% 0.5175571149463054
plot_id,batch_id 0 10 miss% 0.40679140628077753
plot_id,batch_id 0 11 miss% 0.47932944530485105
plot_id,batch_id 0 12 miss% 0.5121249702240057
plot_id,batch_id 0 13 miss% 0.4142828570856654
plot_id,batch_id 0 14 miss% 0.5136002193218266
plot_id,batch_id 0 15 miss% 0.456717166213016
plot_id,batch_id 0 16 miss% 0.544571422809543
plot_id,batch_id 0 17 miss% 0.5244053130428288
plot_id,batch_id 0 18 miss% 0.5097488728298966
plot_id,batch_id 0 19 miss% 0.4976009565105946
plot_id,batch_id 0 20 miss% 0.6479367611700495
plot_id,batch_id 0 21 miss% 0.40921649832981777
plot_id,batch_id 0 22 miss% 0.5099837979710616
plot_id,batch_id 0 23 miss% 0.3933969066389365
plot_id,batch_id 0 24 miss% 0.5804742096334091
plot_id,batch_id 0 25 miss% 0.49031464078394127
plot_id,batch_id 0 26 miss% 0.4704253130368583
plot_id,batch_id 0 27 miss% 0.4551769128382014
plot_id,batch_id 0 28 miss% 0.4441898697174519
plot_id,batch_id 0 29 miss% 0.4591120162830933
plot_id,batch_id 0 30 miss% 0.4711588442384611
plot_id,batch_id 0 31 miss% 0.6289693426567518
plot_id,batch_id 0 32 miss% 0.42533864648527503
plot_id,batch_id 0 33 miss% 0.40321764265366683
plot_id,batch_id 0 34 miss% 0.37826385162279763
plot_id,batch_id 0 35 miss% 0.4494754616009159
plot_id,batch_id 0 36 miss% 0.6715473833070675
plot_id,batch_id 0 37 miss% 0.41827914152416285
plot_id,batch_id 0 38 miss% 0.48883853144875256
plot_id,batch_id 0 39 miss% 0.42333623308705864
plot_id,batch_id 0 40 miss% 0.581780365402522
plot_id,batch_id 0 41 miss% 0.5183232421133468
plot_id,batch_id 0 42 miss% 0.5413066029473376
plot_id,batch_id 0 43 miss% 0.45155728888655383
plot_id,batch_id 0 44 miss% 0.5594026385653084
plot_id,batch_id 0 45 miss% 0.4988213930196966
plot_id,batch_id 0 46 miss% 0.3992695860223507
plot_id,batch_id 0 47 miss% 0.40982788353649785
plot_id,batch_id 0 48 miss% 0.46844810382536706
plot_id,batch_id 0 49 miss% 0.36858300549085876
plot_id,batch_id 0 50 miss% 0.5671192721711846
plot_id,batch_id 0 51 miss% 0.5072042504778564
plot_id,batch_id 0 52 miss% 0.5436930231067603
plot_id,batch_id 0 53 miss% 0.6080762622386169
plot_id,batch_id 0 54 miss% 0.5051209975601256
plot_id,batch_id 0 55 miss% 0.5352746216765446
plot_id,batch_id 0 56 miss% 0.44062337423391
plot_id,batch_id 0 57 miss% 0.3612277070546786
plot_id,batch_id 0 58 miss% 0.4800101159271462
plot_id,batch_id 0 59 miss% 0.4021869012114025
plot_id,batch_id 0 60 miss% 0.4462888698056031
plot_id,batch_id 0 61 miss% 0.38469305058726994
plot_id,batch_id 0 62 miss% 0.5953907236341865
plot_id,batch_id 0 63 miss% 0.4777813277647672
plot_id,batch_id 0 64 miss% 0.440430365406639
plot_id,batch_id 0 65 miss% 0.43552756025732337
plot_id,batch_id 0 66 miss% 0.4769014419735423
plot_id,batch_id 0 67 miss% 0.4176928664263618
plot_id,batch_id 0 68 miss% 0.4172086212587351
plot_id,batch_id 0 69 miss% 0.5296974712536635
plot_id,batch_id 0 70 miss% 0.3462244468705095
plot_id,batch_id 0 71 miss% 0.4979610721732871
plot_id,batch_id 0 72 miss% 0.5191418776493641
plot_id,batch_id 0 73 miss% 0.5678349513795463
plot_id,batch_id 0 74 miss% 0.5350543983798771
plot_id,batch_id 0 75 miss% 0.36290284865532846
plot_id,batch_id 0 76 miss% 0.4580581220097258
plot_id,batch_id 0 77 miss% 0.44165500553724424
plot_id,batch_id 0 78 miss% 0.42727466191563884
plot_id,batch_id 0 79 miss% 0.46848064326816663
plot_id,batch_id 0 80 miss% 0.4060987962931342
plot_id,batch_id 0 81 miss% 0.48894807734178813
plot_id,batch_id 0 82 miss% 0.43919390959238097
plot_id,batch_id 0 83 miss% 0.48204394181859467
plot_id,batch_id 0 84 miss% 0.4174019655330265
plot_id,batch_id 0 85 miss% 0.3692257894099841
plot_id,batch_id 0 86 miss% 0.4093255493112389
plot_id,batch_id 0 87 miss% 0.5181803000773968
plot_id,batch_id 0 88 miss% 0.49959100265409384
plot_id,batch_id 0 89 miss% 0.5039360979483524
plot_id,batch_id 0 90 miss% 0.4064602327095816
plot_id,batch_id 0 91 miss% 0.3980518757132815
plot_id,batch_id 0 92 miss% 0.38156952743640893
plot_id,batch_id 0 93 miss% 0.4419152372515541
plot_id,batch_id 0 94 miss% 0.553507568883373
plot_id,batch_id 0 95 miss% 0.33705010665405655
plot_id,batch_id 0 96 miss% 0.4255896870995145
plot_id,batch_id 0 97 miss% 0.4713068053568977
plot_id,batch_id 0 98 miss% 0.5866117777463129
plot_id,batch_id 0 99 miss% 0.44395317096479203
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.42805104 0.525152   0.51136447 0.45043799 0.38329057 0.38970444
 0.38047914 0.50111355 0.53241844 0.51755711 0.40679141 0.47932945
 0.51212497 0.41428286 0.51360022 0.45671717 0.54457142 0.52440531
 0.50974887 0.49760096 0.64793676 0.4092165  0.5099838  0.39339691
 0.58047421 0.49031464 0.47042531 0.45517691 0.44418987 0.45911202
 0.47115884 0.62896934 0.42533865 0.40321764 0.37826385 0.44947546
 0.67154738 0.41827914 0.48883853 0.42333623 0.58178037 0.51832324
 0.5413066  0.45155729 0.55940264 0.49882139 0.39926959 0.40982788
 0.4684481  0.36858301 0.56711927 0.50720425 0.54369302 0.60807626
 0.505121   0.53527462 0.44062337 0.36122771 0.48001012 0.4021869
 0.44628887 0.38469305 0.59539072 0.47778133 0.44043037 0.43552756
 0.47690144 0.41769287 0.41720862 0.52969747 0.34622445 0.49796107
 0.51914188 0.56783495 0.5350544  0.36290285 0.45805812 0.44165501
 0.42727466 0.46848064 0.4060988  0.48894808 0.43919391 0.48204394
 0.41740197 0.36922579 0.40932555 0.5181803  0.499591   0.5039361
 0.40646023 0.39805188 0.38156953 0.44191524 0.55350757 0.33705011
 0.42558969 0.47130681 0.58661178 0.44395317]
for model  59 the mean error 0.470714117663681
all id 59 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 21 out win 6 err 0.470714117663681 time 9494.598680257797
Launcher: Job 60 completed in 9724 seconds.
Launcher: Task 179 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  41745
Epoch:0, Train loss:0.508655, valid loss:0.506774
Epoch:1, Train loss:0.022032, valid loss:0.002451
Epoch:2, Train loss:0.004139, valid loss:0.001640
Epoch:3, Train loss:0.003235, valid loss:0.001338
Epoch:4, Train loss:0.002760, valid loss:0.001300
Epoch:5, Train loss:0.002466, valid loss:0.001191
Epoch:6, Train loss:0.002280, valid loss:0.001225
Epoch:7, Train loss:0.002104, valid loss:0.001038
Epoch:8, Train loss:0.001892, valid loss:0.000883
Epoch:9, Train loss:0.001786, valid loss:0.000989
Epoch:10, Train loss:0.001673, valid loss:0.000882
Epoch:11, Train loss:0.001380, valid loss:0.000684
Epoch:12, Train loss:0.001333, valid loss:0.000614
Epoch:13, Train loss:0.001290, valid loss:0.000626
Epoch:14, Train loss:0.001256, valid loss:0.000635
Epoch:15, Train loss:0.001229, valid loss:0.000646
Epoch:16, Train loss:0.001180, valid loss:0.000671
Epoch:17, Train loss:0.001161, valid loss:0.000599
Epoch:18, Train loss:0.001137, valid loss:0.000618
Epoch:19, Train loss:0.001098, valid loss:0.000658
Epoch:20, Train loss:0.001086, valid loss:0.000555
Epoch:21, Train loss:0.000937, valid loss:0.000542
Epoch:22, Train loss:0.000931, valid loss:0.000627
Epoch:23, Train loss:0.000916, valid loss:0.000545
Epoch:24, Train loss:0.000895, valid loss:0.000551
Epoch:25, Train loss:0.000896, valid loss:0.000551
Epoch:26, Train loss:0.000871, valid loss:0.000546
Epoch:27, Train loss:0.000877, valid loss:0.000596
Epoch:28, Train loss:0.000858, valid loss:0.000525
Epoch:29, Train loss:0.000845, valid loss:0.000517
Epoch:30, Train loss:0.000852, valid loss:0.000548
Epoch:31, Train loss:0.000771, valid loss:0.000487
Epoch:32, Train loss:0.000761, valid loss:0.000502
Epoch:33, Train loss:0.000753, valid loss:0.000488
Epoch:34, Train loss:0.000753, valid loss:0.000481
Epoch:35, Train loss:0.000747, valid loss:0.000497
Epoch:36, Train loss:0.000739, valid loss:0.000512
Epoch:37, Train loss:0.000730, valid loss:0.000495
Epoch:38, Train loss:0.000732, valid loss:0.000492
Epoch:39, Train loss:0.000733, valid loss:0.000516
Epoch:40, Train loss:0.000736, valid loss:0.000496
Epoch:41, Train loss:0.000686, valid loss:0.000495
Epoch:42, Train loss:0.000683, valid loss:0.000483
Epoch:43, Train loss:0.000678, valid loss:0.000489
Epoch:44, Train loss:0.000678, valid loss:0.000487
Epoch:45, Train loss:0.000673, valid loss:0.000476
Epoch:46, Train loss:0.000670, valid loss:0.000474
Epoch:47, Train loss:0.000671, valid loss:0.000458
Epoch:48, Train loss:0.000668, valid loss:0.000486
Epoch:49, Train loss:0.000665, valid loss:0.000503
Epoch:50, Train loss:0.000671, valid loss:0.000484
Epoch:51, Train loss:0.000641, valid loss:0.000467
Epoch:52, Train loss:0.000637, valid loss:0.000473
Epoch:53, Train loss:0.000636, valid loss:0.000461
Epoch:54, Train loss:0.000635, valid loss:0.000457
Epoch:55, Train loss:0.000634, valid loss:0.000456
Epoch:56, Train loss:0.000634, valid loss:0.000465
Epoch:57, Train loss:0.000633, valid loss:0.000467
Epoch:58, Train loss:0.000633, valid loss:0.000458
Epoch:59, Train loss:0.000633, valid loss:0.000455
Epoch:60, Train loss:0.000633, valid loss:0.000454
training time 9553.855821371078
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.2118431052866474
plot_id,batch_id 0 1 miss% 0.2745915828831635
plot_id,batch_id 0 2 miss% 0.3604009379962313
plot_id,batch_id 0 3 miss% 0.3526671107836202
plot_id,batch_id 0 4 miss% 0.2739077908347159
plot_id,batch_id 0 5 miss% 0.22274131930026272
plot_id,batch_id 0 6 miss% 0.2730538846593975
plot_id,batch_id 0 7 miss% 0.422050845823847
plot_id,batch_id 0 8 miss% 0.6106342877043034
plot_id,batch_id 0 9 miss% 0.28534601963098455
plot_id,batch_id 0 10 miss% 0.19422120578110147
plot_id,batch_id 0 11 miss% 0.2630872576070053
plot_id,batch_id 0 12 miss% 0.3742732107955852
plot_id,batch_id 0 13 miss% 0.295676861882099
plot_id,batch_id 0 14 miss% 0.40828716450941266
plot_id,batch_id 0 15 miss% 0.17738098805042835
plot_id,batch_id 0 16 miss% 0.34579671532838907
plot_id,batch_id 0 17 miss% 0.32795516822874454
plot_id,batch_id 0 18 miss% 0.36679936396516793
plot_id,batch_id 0 19 miss% 0.296478801552439
plot_id,batch_id 0 20 miss% 0.4308364479728627
plot_id,batch_id 0 21 miss% 0.3609117260492383
plot_id,batch_id 0 22 miss% 0.27220321617121723
plot_id,batch_id 0 23 miss% 0.25590147115642736
plot_id,batch_id 0 24 miss% 0.21861285466312605
plot_id,batch_id 0 25 miss% 0.23990431800740267
plot_id,batch_id 0 26 miss% 0.3214970326655277
plot_id,batch_id 0 27 miss% 0.37513611472311115
plot_id,batch_id 0 28 miss% 0.3390039800880996
plot_id,batch_id 0 29 miss% 0.3705188633456284
plot_id,batch_id 0 30 miss% 0.25945104022738796
plot_id,batch_id 0 31 miss% 0.4425328780951592
plot_id,batch_id 0 32 miss% 0.4699996170704678
plot_id,batch_id 0 33 miss% 0.3401164038208016
plot_id,batch_id 0 34 miss% 0.37742898346838993
plot_id,batch_id 0 35 miss% 0.266257227653136
plot_id,batch_id 0 36 miss% 0.4709674612077793
plot_id,batch_id 0 37 miss% 0.3524441797959636
plot_id,batch_id 0 38 miss% 0.21468678395088178
plot_id,batch_id 0 39 miss% 0.3253999301104692
plot_id,batch_id 0 40 miss% 0.23813108336553268
plot_id,batch_id 0 41 miss% 0.24522391213435815
plot_id,batch_id 0 42 miss% 0.2221660538892032
plot_id,batch_id 0 43 miss% 0.22244743571747566
plot_id,batch_id 0 44 miss% 0.16280338505548267
plot_id,batch_id 0 45 miss% 0.30750735442682603
plot_id,batch_id 0 46 miss% 0.26413721994169714
plot_id,batch_id 0 47 miss% 0.27670302322445306
plot_id,batch_id 0 48 miss% 0.2444305518672812
plot_id,batch_id 0 49 miss% 0.1853056529150202
plot_id,batch_id 0 50 miss% 0.3125477995682164
plot_id,batch_id 0 51 miss% 0.3770950148907981
plot_id,batch_id 0 52 miss% 0.305661630062872
plot_id,batch_id 0 53 miss% 0.2204543321708335
plot_id,batch_id 0 54 miss% 0.23255849719106958
plot_id,batch_id 0 55 miss% 0.22907497755138911
plot_id,batch_id 0 56 miss% 0.3371177613098321
plot_id,batch_id 0 57 miss% 0.2273729154244452
plot_id,batch_id 0 58 miss% 0.278207297274959
plot_id,batch_id 0 59 miss% 0.25567092748842696
plot_id,batch_id 0 60 miss% 0.15744960203147215
plot_id,batch_id 0 61 miss% 0.20810107202833084
plot_id,batch_id 0 62 miss% 0.2706235289994194
plot_id,batch_id 0 63 miss% 0.28734710107070605
plot_id,batch_id 0 64 miss% 0.2553353781435742
plot_id,batch_id 0 65 miss% 0.41560035985036375
plot_id,batch_id 0 66 miss% 0.2692328033990991
plot_id,batch_id 0 67 miss% 0.20984811429284736
plot_id,batch_id 0 68 miss% 0.3712940438769111
plot_id,batch_id 0 69 miss% 0.37001289517843483
plot_id,batch_id 0 70 miss% 0.28180204877315757
plot_id,batch_id 0 71 miss% 0.26925398579272214
plot_id,batch_id 0 72 miss% 0.2535466340662504
plot_id,batch_id 0 73 miss% 0.2989544393715018
plot_id,batch_id 0 74 miss% 0.4244273593336236
plot_id,batch_id 0 75 miss% 0.19244867333931034
plot_id,batch_id 0 76 miss% 0.24390797164350014
plot_id,batch_id 0 77 miss% 0.25442939547233784
plot_id,batch_id 0 78 miss% 0.42099811223759426
plot_id,batch_id 0 79 miss% 0.3456535097968521
plot_id,batch_id 0 80 miss% 0.1476550820719484
plot_id,batch_id 0 81 miss% 0.3309023466430229
plot_id,batch_id 0 82 miss% 0.19526741231372105
plot_id,batch_id 0 83 miss% 0.3758024208314267
plot_id,batch_id 0 84 miss% 0.2806554832960869
plot_id,batch_id 0 85 miss% 0.13333479115775615
plot_id,batch_id 0 86 miss% 0.2580951100772466
plot_id,batch_id 0 87 miss% 0.3243234221604477
plot_id,batch_id 0 88 miss% 0.3595799244806556
plot_id,batch_id 0 89 miss% 0.33733047088227075
plot_id,batch_id 0 90 miss% 0.11980270787623072
plot_id,batch_id 0 91 miss% 0.2810594798816414
plot_id,batch_id 0 92 miss% 0.25210352148709053
plot_id,batch_id 0 93 miss% 0.2950365062216006
plot_id,batch_id 0 94 miss% 0.3306151203787436
plot_id,batch_id 0 95 miss% 0.30153107276286956
plot_id,batch_id 0 96 miss% 0.21975489598089054
plot_id,batch_id 0 97 miss% 0.3801449454953241
plot_id,batch_id 0 98 miss% 0.25635622947159215
plot_id,batch_id 0 99 miss% 0.32154094456483334
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.21184311 0.27459158 0.36040094 0.35266711 0.27390779 0.22274132
 0.27305388 0.42205085 0.61063429 0.28534602 0.19422121 0.26308726
 0.37427321 0.29567686 0.40828716 0.17738099 0.34579672 0.32795517
 0.36679936 0.2964788  0.43083645 0.36091173 0.27220322 0.25590147
 0.21861285 0.23990432 0.32149703 0.37513611 0.33900398 0.37051886
 0.25945104 0.44253288 0.46999962 0.3401164  0.37742898 0.26625723
 0.47096746 0.35244418 0.21468678 0.32539993 0.23813108 0.24522391
 0.22216605 0.22244744 0.16280339 0.30750735 0.26413722 0.27670302
 0.24443055 0.18530565 0.3125478  0.37709501 0.30566163 0.22045433
 0.2325585  0.22907498 0.33711776 0.22737292 0.2782073  0.25567093
 0.1574496  0.20810107 0.27062353 0.2873471  0.25533538 0.41560036
 0.2692328  0.20984811 0.37129404 0.3700129  0.28180205 0.26925399
 0.25354663 0.29895444 0.42442736 0.19244867 0.24390797 0.2544294
 0.42099811 0.34565351 0.14765508 0.33090235 0.19526741 0.37580242
 0.28065548 0.13333479 0.25809511 0.32432342 0.35957992 0.33733047
 0.11980271 0.28105948 0.25210352 0.29503651 0.33061512 0.30153107
 0.2197549  0.38014495 0.25635623 0.32154094]
for model  162 the mean error 0.294867798950502
all id 162 hidden_dim 16 learning_rate 0.0025 num_layers 3 frames 31 out win 4 err 0.294867798950502 time 9553.855821371078
Launcher: Job 163 completed in 9804 seconds.
Launcher: Task 153 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  55697
Epoch:0, Train loss:0.742758, valid loss:0.704333
Epoch:1, Train loss:0.306298, valid loss:0.006748
Epoch:2, Train loss:0.017722, valid loss:0.005725
Epoch:3, Train loss:0.016327, valid loss:0.005028
Epoch:4, Train loss:0.015600, valid loss:0.005517
Epoch:5, Train loss:0.014256, valid loss:0.004205
Epoch:6, Train loss:0.010209, valid loss:0.003591
Epoch:7, Train loss:0.008573, valid loss:0.003013
Epoch:8, Train loss:0.006943, valid loss:0.002412
Epoch:9, Train loss:0.006367, valid loss:0.002345
Epoch:10, Train loss:0.005989, valid loss:0.002716
Epoch:11, Train loss:0.005377, valid loss:0.002089
Epoch:12, Train loss:0.005213, valid loss:0.001996
Epoch:13, Train loss:0.005088, valid loss:0.001990
Epoch:14, Train loss:0.004968, valid loss:0.001929
Epoch:15, Train loss:0.004865, valid loss:0.002052
Epoch:16, Train loss:0.004737, valid loss:0.001923
Epoch:17, Train loss:0.004637, valid loss:0.002199
Epoch:18, Train loss:0.004479, valid loss:0.001799
Epoch:19, Train loss:0.004399, valid loss:0.001697
Epoch:20, Train loss:0.004264, valid loss:0.001646
Epoch:21, Train loss:0.003955, valid loss:0.001563
Epoch:22, Train loss:0.003885, valid loss:0.001495
Epoch:23, Train loss:0.003841, valid loss:0.001520
Epoch:24, Train loss:0.003773, valid loss:0.001445
Epoch:25, Train loss:0.003754, valid loss:0.001465
Epoch:26, Train loss:0.003689, valid loss:0.001390
Epoch:27, Train loss:0.003660, valid loss:0.001462
Epoch:28, Train loss:0.003616, valid loss:0.001439
Epoch:29, Train loss:0.003553, valid loss:0.001387
Epoch:30, Train loss:0.003536, valid loss:0.001362
Epoch:31, Train loss:0.003360, valid loss:0.001369
Epoch:32, Train loss:0.003318, valid loss:0.001298
Epoch:33, Train loss:0.003309, valid loss:0.001347
Epoch:34, Train loss:0.003283, valid loss:0.001283
Epoch:35, Train loss:0.003264, valid loss:0.001333
Epoch:36, Train loss:0.003252, valid loss:0.001389
Epoch:37, Train loss:0.003223, valid loss:0.001278
Epoch:38, Train loss:0.003200, valid loss:0.001249
Epoch:39, Train loss:0.003199, valid loss:0.001285
Epoch:40, Train loss:0.003199, valid loss:0.001270
Epoch:41, Train loss:0.003074, valid loss:0.001193
Epoch:42, Train loss:0.002130, valid loss:0.001129
Epoch:43, Train loss:0.001811, valid loss:0.001115
Epoch:44, Train loss:0.001728, valid loss:0.001042
Epoch:45, Train loss:0.001685, valid loss:0.001017
Epoch:46, Train loss:0.001653, valid loss:0.001059
Epoch:47, Train loss:0.001640, valid loss:0.001020
Epoch:48, Train loss:0.001622, valid loss:0.001006
Epoch:49, Train loss:0.001603, valid loss:0.001018
Epoch:50, Train loss:0.001595, valid loss:0.001015
Epoch:51, Train loss:0.001536, valid loss:0.000989
Epoch:52, Train loss:0.001526, valid loss:0.000992
Epoch:53, Train loss:0.001524, valid loss:0.000992
Epoch:54, Train loss:0.001522, valid loss:0.000992
Epoch:55, Train loss:0.001520, valid loss:0.000987
Epoch:56, Train loss:0.001520, valid loss:0.001009
Epoch:57, Train loss:0.001519, valid loss:0.000990
Epoch:58, Train loss:0.001518, valid loss:0.000988
Epoch:59, Train loss:0.001517, valid loss:0.000985
Epoch:60, Train loss:0.001518, valid loss:0.000997
training time 9667.59119296074
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.10504224547660891
plot_id,batch_id 0 1 miss% 0.04755512028113582
plot_id,batch_id 0 2 miss% 0.09397664087479739
plot_id,batch_id 0 3 miss% 0.07541230791351587
plot_id,batch_id 0 4 miss% 0.093042064942431
plot_id,batch_id 0 5 miss% 0.08415260076649936
plot_id,batch_id 0 6 miss% 0.0948698701363475
plot_id,batch_id 0 7 miss% 0.11714188797963374
plot_id,batch_id 0 8 miss% 0.0734786558101959
plot_id,batch_id 0 9 miss% 0.05255925328318817
plot_id,batch_id 0 10 miss% 0.0556843472431126
plot_id,batch_id 0 11 miss% 0.0800594705944001
plot_id,batch_id 0 12 miss% 0.09492410095163425
plot_id,batch_id 0 13 miss% 0.07950707486109831
plot_id,batch_id 0 14 miss% 0.0929702116607975
plot_id,batch_id 0 15 miss% 0.10866332797510449
plot_id,batch_id 0 16 miss% 0.11018245815215175
plot_id,batch_id 0 17 miss% 0.051565521570448025
plot_id,batch_id 0 18 miss% 0.0696905297133957
plot_id,batch_id 0 19 miss% 0.10207890646720978
plot_id,batch_id 0 20 miss% 0.0590046182358842
plot_id,batch_id 0 21 miss% 0.04903309723934622
plot_id,batch_id 0 22 miss% 0.08122573276415788
plot_id,batch_id 0 23 miss% 0.02849308460977725
plot_id,batch_id 0 24 miss% 0.03963333362351423
plot_id,batch_id 0 25 miss% 0.1177730772800753
plot_id,batch_id 0 26 miss% 0.05625701160328212
plot_id,batch_id 0 27 miss% 0.0583278546439633
plot_id,batch_id 0 28 miss% 0.03007392398980421
plot_id,batch_id 0 29 miss% 0.046861760132462155
plot_id,batch_id 0 30 miss% 0.027049987087553216
plot_id,batch_id 0 31 miss% 0.13170310218516376
plot_id,batch_id 0 32 miss% 0.1285190928482224
plot_id,batch_id 0 33 miss% 0.055336767133797406
plot_id,batch_id 0 34 miss% 0.07503540643105426
plot_id,batch_id 0 35 miss% 0.0791517047609881
plot_id,batch_id 0 36 miss% 0.13524162558695657
plot_id,batch_id 0 37 miss% 0.08169507803879178
plot_id,batch_id 0 38 miss% 0.060261307250708114
plot_id,batch_id 0 39 miss% 0.04536851128674852
plot_id,batch_id 0 40 miss% 0.11008108930306908
plot_id,batch_id 0 41 miss% 0.04729296403522264
plot_id,batch_id 0 42 miss% 0.019085137471224197
plot_id,batch_id 0 43 miss% 0.07862201357828229
plot_id,batch_id 0 44 miss% 0.062242187841570366
plot_id,batch_id 0 45 miss% 0.04117553821147918
plot_id,batch_id 0 46 miss% 0.08209500028692555
plot_id,batch_id 0 47 miss% 0.051074734824668955
plot_id,batch_id 0 48 miss% 0.034756035322074
plot_id,batch_id 0 49 miss% 0.027095843639876646
plot_id,batch_id 0 50 miss% 0.1559868120333568
plot_id,batch_id 0 51 miss% 0.0718070011349578
plot_id,batch_id 0 52 miss% 0.04838096568056608
plot_id,batch_id 0 53 miss% 0.03291995809650373
plot_id,batch_id 0 54 miss% 0.04348556846619821
plot_id,batch_id 0 55 miss% 0.09670809469407667
plot_id,batch_id 0 56 miss% 0.07160125907176261
plot_id,batch_id 0 57 miss% 0.055643207303110266
plot_id,batch_id 0 58 miss% 0.04054569988554223
plot_id,batch_id 0 59 miss% 0.03808970588250139
plot_id,batch_id 0 60 miss% 0.06513960898867804
plot_id,batch_id 0 61 miss% 0.12415024383155887
plot_id,batch_id 0 62 miss% 0.07342472667402763
plot_id,batch_id 0 63 miss% 0.08607602607238868
plot_id,batch_id 0 64 miss% 0.06614145554416664
plot_id,batch_id 0 65 miss% 0.13191883566214896
plot_id,batch_id 0 66 miss% 0.10739164188938384
plot_id,batch_id 0 67 miss% 0.024932457668501603
plot_id,batch_id 0 68 miss% 0.06963951295845348
plot_id,batch_id 0 69 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  89105
Epoch:0, Train loss:0.705019, valid loss:0.658115
Epoch:1, Train loss:0.158230, valid loss:0.006970
Epoch:2, Train loss:0.017028, valid loss:0.006185
Epoch:3, Train loss:0.015100, valid loss:0.005310
Epoch:4, Train loss:0.013603, valid loss:0.005152
Epoch:5, Train loss:0.013051, valid loss:0.004465
Epoch:6, Train loss:0.012538, valid loss:0.004237
Epoch:7, Train loss:0.011514, valid loss:0.004256
Epoch:8, Train loss:0.009678, valid loss:0.002705
Epoch:9, Train loss:0.004485, valid loss:0.001745
Epoch:10, Train loss:0.003766, valid loss:0.001848
Epoch:11, Train loss:0.002915, valid loss:0.001390
Epoch:12, Train loss:0.002768, valid loss:0.001463
Epoch:13, Train loss:0.002639, valid loss:0.001330
Epoch:14, Train loss:0.002565, valid loss:0.001437
Epoch:15, Train loss:0.002434, valid loss:0.001296
Epoch:16, Train loss:0.002374, valid loss:0.001227
Epoch:17, Train loss:0.002289, valid loss:0.001328
Epoch:18, Train loss:0.002142, valid loss:0.001288
Epoch:19, Train loss:0.002188, valid loss:0.001266
Epoch:20, Train loss:0.002069, valid loss:0.001161
Epoch:21, Train loss:0.001713, valid loss:0.001068
Epoch:22, Train loss:0.001688, valid loss:0.001027
Epoch:23, Train loss:0.001660, valid loss:0.001084
Epoch:24, Train loss:0.001630, valid loss:0.001027
Epoch:25, Train loss:0.001570, valid loss:0.001022
Epoch:26, Train loss:0.001566, valid loss:0.001094
Epoch:27, Train loss:0.001547, valid loss:0.000954
Epoch:28, Train loss:0.001524, valid loss:0.000950
Epoch:29, Train loss:0.001476, valid loss:0.001027
Epoch:30, Train loss:0.001498, valid loss:0.001089
Epoch:31, Train loss:0.001302, valid loss:0.000928
Epoch:32, Train loss:0.001274, valid loss:0.000919
Epoch:33, Train loss:0.001263, valid loss:0.000912
Epoch:34, Train loss:0.001251, valid loss:0.000940
Epoch:35, Train loss:0.001237, valid loss:0.000884
Epoch:36, Train loss:0.001222, valid loss:0.000864
Epoch:37, Train loss:0.001223, valid loss:0.000868
Epoch:38, Train loss:0.001217, valid loss:0.000870
Epoch:39, Train loss:0.001193, valid loss:0.000905
Epoch:40, Train loss:0.001188, valid loss:0.000883
Epoch:41, Train loss:0.001099, valid loss:0.000872
Epoch:42, Train loss:0.001089, valid loss:0.000849
Epoch:43, Train loss:0.001076, valid loss:0.000870
Epoch:44, Train loss:0.001082, valid loss:0.000850
Epoch:45, Train loss:0.001068, valid loss:0.000857
Epoch:46, Train loss:0.001078, valid loss:0.000844
Epoch:47, Train loss:0.001059, valid loss:0.000845
Epoch:48, Train loss:0.001049, valid loss:0.000844
Epoch:49, Train loss:0.001053, valid loss:0.000882
Epoch:50, Train loss:0.001042, valid loss:0.000828
Epoch:51, Train loss:0.000982, valid loss:0.000833
Epoch:52, Train loss:0.000977, valid loss:0.000817
Epoch:53, Train loss:0.000976, valid loss:0.000829
Epoch:54, Train loss:0.000974, valid loss:0.000816
Epoch:55, Train loss:0.000972, valid loss:0.000825
Epoch:56, Train loss:0.000973, valid loss:0.000819
Epoch:57, Train loss:0.000972, valid loss:0.000824
Epoch:58, Train loss:0.000972, valid loss:0.000823
Epoch:59, Train loss:0.000972, valid loss:0.000814
Epoch:60, Train loss:0.000972, valid loss:0.000822
training time 9732.142283201218
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.387227579300045
plot_id,batch_id 0 1 miss% 0.43809877409152315
plot_id,batch_id 0 2 miss% 0.5172138794738544
plot_id,batch_id 0 3 miss% 0.4698131841735766
plot_id,batch_id 0 4 miss% 0.39604790365415016
plot_id,batch_id 0 5 miss% 0.4879450299069055
plot_id,batch_id 0 6 miss% 0.44505179210070306
plot_id,batch_id 0 7 miss% 0.5976433537067388
plot_id,batch_id 0 8 miss% 0.6215916938037388
plot_id,batch_id 0 9 miss% 0.6733081598692685
plot_id,batch_id 0 10 miss% 0.34652404220531247
plot_id,batch_id 0 11 miss% 0.3921236863832978
plot_id,batch_id 0 12 miss% 0.418115573053098
plot_id,batch_id 0 13 miss% 0.39640972849445194
plot_id,batch_id 0 14 miss% 0.5352129031382764
plot_id,batch_id 0 15 miss% 0.43790933580049346
plot_id,batch_id 0 16 miss% 0.5558007310028696
plot_id,batch_id 0 17 miss% 0.5059115544296128
plot_id,batch_id 0 18 miss% 0.5385666280667958
plot_id,batch_id 0 19 miss% 0.48989196892171305
plot_id,batch_id 0 20 miss% 0.4253826072447717
plot_id,batch_id 0 21 miss% 0.5340833680713302
plot_id,batch_id 0 22 miss% 0.5171789211418105
plot_id,batch_id 0 23 miss% 0.5205744116676394
plot_id,batch_id 0 24 miss% 0.4518178586833108
plot_id,batch_id 0 25 miss% 0.42975929759287723
plot_id,batch_id 0 26 miss% 0.5132041314050091
plot_id,batch_id 0 27 miss% 0.3931150298327568
plot_id,batch_id 0 28 miss% 0.46252369359964135
plot_id,batch_id 0 29 miss% 0.48618511957043714
plot_id,batch_id 0 30 miss% 0.37151137892051467
plot_id,batch_id 0 31 miss% 0.5718657899060694
plot_id,batch_id 0 32 miss% 0.5457648521040288
plot_id,batch_id 0 33 miss% 0.48396813084441076
plot_id,batch_id 0 34 miss% 0.4499626249934858
plot_id,batch_id 0 35 miss% 0.42826620088931666
plot_id,batch_id 0 36 miss% 0.6296590067349774
plot_id,batch_id 0 37 miss% 0.4476191426236277
plot_id,batch_id 0 38 miss% 0.5550686729014644
plot_id,batch_id 0 39 miss% 0.5237804838742897
plot_id,batch_id 0 40 miss% 0.3889259841602476
plot_id,batch_id 0 41 miss% 0.4383966560593279
plot_id,batch_id 0 42 miss% 0.3130722197332531
plot_id,batch_id 0 43 miss% 0.33907353003436774
plot_id,batch_id 0 44 miss% 0.3830842251284409
plot_id,batch_id 0 45 miss% 0.48811759093471185
plot_id,batch_id 0 46 miss% 0.47243572642652315
plot_id,batch_id 0 47 miss% 0.45542737942313
plot_id,batch_id 0 48 miss% 0.41412725722424937
plot_id,batch_id 0 49 miss% 0.43629294627428483
plot_id,batch_id 0 50 miss% 0.5081071749011077
plot_id,batch_id 0 51 miss% 0.5607442734052134
plot_id,batch_id 0 52 miss% 0.5485636336359251
plot_id,batch_id 0 53 miss% 0.405116927317038
plot_id,batch_id 0 54 miss% 0.39265003161169026
plot_id,batch_id 0 55 miss% 0.41266331081564
plot_id,batch_id 0 56 miss% 0.5565026805193799
plot_id,batch_id 0 57 miss% 0.5598091393904359
plot_id,batch_id 0 58 miss% 0.5175956832569232
plot_id,batch_id 0 59 miss% 0.49632659557239583
plot_id,batch_id 0 60 miss% 0.41345564850523736
plot_id,batch_id 0 61 miss% 0.34799538479346287
plot_id,batch_id 0 62 miss% 0.49537508960469745
plot_id,batch_id 0 63 miss% 0.42210996409171536
plot_id,batch_id 0 64 miss% 0.48792250353776867
plot_id,batch_id 0 65 miss% 0.3647427454926336
plot_id,batch_id 0 66 miss% 0.4626779888491823
plot_id,batch_id 0 67 miss% 0.3850221168265596
plot_id,batch_id 0 68 miss% 0.5056476151713486
plot_id,batch_id 0 69 miss% 0.5366597011903587
plot_id,batch_id 0 70 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  41745
Epoch:0, Train loss:0.614092, valid loss:0.600074
Epoch:1, Train loss:0.035841, valid loss:0.003877
Epoch:2, Train loss:0.007048, valid loss:0.002619
Epoch:3, Train loss:0.004629, valid loss:0.001860
Epoch:4, Train loss:0.003912, valid loss:0.001858
Epoch:5, Train loss:0.003441, valid loss:0.001703
Epoch:6, Train loss:0.003254, valid loss:0.001456
Epoch:7, Train loss:0.003018, valid loss:0.001600
Epoch:8, Train loss:0.002805, valid loss:0.001401
Epoch:9, Train loss:0.002600, valid loss:0.001426
Epoch:10, Train loss:0.002451, valid loss:0.001283
Epoch:11, Train loss:0.001934, valid loss:0.001022
Epoch:12, Train loss:0.001870, valid loss:0.001033
Epoch:13, Train loss:0.001891, valid loss:0.001024
Epoch:14, Train loss:0.001808, valid loss:0.000957
Epoch:15, Train loss:0.001698, valid loss:0.001179
Epoch:16, Train loss:0.001698, valid loss:0.001079
Epoch:17, Train loss:0.001638, valid loss:0.001058
Epoch:18, Train loss:0.001628, valid loss:0.000943
Epoch:19, Train loss:0.001577, valid loss:0.000935
Epoch:20, Train loss:0.001527, valid loss:0.000966
Epoch:21, Train loss:0.001269, valid loss:0.000819
Epoch:22, Train loss:0.001266, valid loss:0.000727
Epoch:23, Train loss:0.001219, valid loss:0.000803
Epoch:24, Train loss:0.001216, valid loss:0.000752
Epoch:25, Train loss:0.001205, valid loss:0.000721
Epoch:26, Train loss:0.001172, valid loss:0.000736
Epoch:27, Train loss:0.001176, valid loss:0.000717
Epoch:28, Train loss:0.001156, valid loss:0.000766
Epoch:29, Train loss:0.001124, valid loss:0.000759
Epoch:30, Train loss:0.001147, valid loss:0.000785
Epoch:31, Train loss:0.000993, valid loss:0.000686
Epoch:32, Train loss:0.000978, valid loss:0.000649
Epoch:33, Train loss:0.000971, valid loss:0.000695
Epoch:34, Train loss:0.000960, valid loss:0.000692
Epoch:35, Train loss:0.000957, valid loss:0.000629
Epoch:36, Train loss:0.000951, valid loss:0.000706
Epoch:37, Train loss:0.000941, valid loss:0.000714
Epoch:38, Train loss:0.000935, valid loss:0.000664
Epoch:39, Train loss:0.000928, valid loss:0.000678
Epoch:40, Train loss:0.000911, valid loss:0.000679
Epoch:41, Train loss:0.000851, valid loss:0.000625
Epoch:42, Train loss:0.000841, valid loss:0.000625
Epoch:43, Train loss:0.000839, valid loss:0.000621
Epoch:44, Train loss:0.000832, valid loss:0.000637
Epoch:45, Train loss:0.000830, valid loss:0.000646
Epoch:46, Train loss:0.000828, valid loss:0.000633
Epoch:47, Train loss:0.000822, valid loss:0.000614
Epoch:48, Train loss:0.000837, valid loss:0.000653
Epoch:49, Train loss:0.000822, valid loss:0.000620
Epoch:50, Train loss:0.000810, valid loss:0.000639
Epoch:51, Train loss:0.000769, valid loss:0.000612
Epoch:52, Train loss:0.000764, valid loss:0.000611
Epoch:53, Train loss:0.000762, valid loss:0.000609
Epoch:54, Train loss:0.000762, valid loss:0.000605
Epoch:55, Train loss:0.000760, valid loss:0.000608
Epoch:56, Train loss:0.000759, valid loss:0.000609
Epoch:57, Train loss:0.000759, valid loss:0.000618
Epoch:58, Train loss:0.000758, valid loss:0.000621
Epoch:59, Train loss:0.000757, valid loss:0.000614
Epoch:60, Train loss:0.000757, valid loss:0.000609
training time 9728.319925785065
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.3144453321763496
plot_id,batch_id 0 1 miss% 0.3794157321287938
plot_id,batch_id 0 2 miss% 0.36967573635404666
plot_id,batch_id 0 3 miss% 0.3099239364509985
plot_id,batch_id 0 4 miss% 0.2928719063399599
plot_id,batch_id 0 5 miss% 0.3142819235904888
plot_id,batch_id 0 6 miss% 0.31074512713579683
plot_id,batch_id 0 7 miss% 0.49551491155155847
plot_id,batch_id 0 8 miss% 0.5042829712143617
plot_id,batch_id 0 9 miss% 0.48090138805665583
plot_id,batch_id 0 10 miss% 0.28895843818370887
plot_id,batch_id 0 11 miss% 0.3371334070302789
plot_id,batch_id 0 12 miss% 0.3681313223191279
plot_id,batch_id 0 13 miss% 0.35199614377518407
plot_id,batch_id 0 14 miss% 0.439790138471763
plot_id,batch_id 0 15 miss% 0.339349349953161
plot_id,batch_id 0 16 miss% 0.5085178631180008
plot_id,batch_id 0 17 miss% 0.4447285114373124
plot_id,batch_id 0 18 miss% 0.4432703508149932
plot_id,batch_id 0 19 miss% 0.42328805474518605
plot_id,batch_id 0 20 miss% 0.3615258480725402
plot_id,batch_id 0 21 miss% 0.35654301850280107
plot_id,batch_id 0 22 miss% 0.36840027520140234
plot_id,batch_id 0 23 miss% 0.3474613408858422
plot_id,batch_id 0 24 miss% 0.3771356114869415
plot_id,batch_id 0 25 miss% 0.31642556272270583
plot_id,batch_id 0 26 miss% 0.3898987989890911
plot_id,batch_id 0 27 miss% 0.412090818033485
plot_id,batch_id 0 28 miss% 0.3058205884302755
plot_id,batch_id 0 29 miss% 0.3874461636998925
plot_id,batch_id 0 30 miss% 0.34442420235646715
plot_id,batch_id 0 31 miss% 0.4896123665901748
plot_id,batch_id 0 32 miss% 0.40755613583130856
plot_id,batch_id 0 33 miss% 0.401230203650488
plot_id,batch_id 0 34 miss% 0.368753905628797
plot_id,batch_id 0 35 miss% 0.308605152512542
plot_id,batch_id 0 36 miss% 0.5525433616903395
plot_id,batch_id 0 37 miss% 0.4159299148312316
plot_id,batch_id 0 38 miss% 0.48280076561932533
plot_id,batch_id 0 39 miss% 0.4870222859211489
plot_id,batch_id 0 40 miss% 0.2955822086162007
plot_id,batch_id 0 41 miss% 0.3718076452102039
plot_id,batch_id 0 42 miss% 0.27392300177249124
plot_id,batch_id 0 43 miss% 0.2729848073740465
plot_id,batch_id 0 44 miss% 0.2913821418153165
plot_id,batch_id 0 45 miss% 0.34534187501296676
plot_id,batch_id 0 46 miss% 0.4120299985254281
plot_id,batch_id 0 47 miss% 0.4231884348220549
plot_id,batch_id 0 48 miss% 0.3894352306448946
plot_id,batch_id 0 49 miss% 0.30888403634632955
plot_id,batch_id 0 50 miss% 0.49121598353648693
plot_id,batch_id 0 51 miss% 0.48326492275575755
plot_id,batch_id 0 52 miss% 0.4601429523012216
plot_id,batch_id 0 53 miss% 0.39302916664265725
plot_id,batch_id 0 54 miss% 0.30194280099057763
plot_id,batch_id 0 55 miss% 0.3738639072988819
plot_id,batch_id 0 56 miss% 0.5260617753015483
plot_id,batch_id 0 57 miss% 0.4178898939474396
plot_id,batch_id 0 58 miss% 0.43823857735454386
plot_id,batch_id 0 59 miss% 0.5041619793486299
plot_id,batch_id 0 60 miss% 0.2529062954340726
plot_id,batch_id 0 61 miss% 0.2919675089887107
plot_id,batch_id 0 62 miss% 0.37896265407258234
plot_id,batch_id 0 63 miss% 0.3952688881473609
plot_id,batch_id 0 64 miss% 0.4020946978293209
plot_id,batch_id 0 65 miss% 0.28177333166551194
plot_id,batch_id 0 66 miss% 0.39018351827821723
plot_id,batch_id 0 67 miss% 0.30985806620555156
plot_id,batch_id 0 68 miss% 0.43237006881596046
plot_id,batch_id 0 690.11175807981146828
plot_id,batch_id 0 70 miss% 0.09301840401718384
plot_id,batch_id 0 71 miss% 0.0692891266396435
plot_id,batch_id 0 72 miss% 0.15632398349448778
plot_id,batch_id 0 73 miss% 0.06875996368241276
plot_id,batch_id 0 74 miss% 0.13637325964453903
plot_id,batch_id 0 75 miss% 0.16351116784710767
plot_id,batch_id 0 76 miss% 0.1428282612470727
plot_id,batch_id 0 77 miss% 0.05607953520813012
plot_id,batch_id 0 78 miss% 0.05607296082699802
plot_id,batch_id 0 79 miss% 0.06275443813943268
plot_id,batch_id 0 80 miss% 0.0678043177955251
plot_id,batch_id 0 81 miss% 0.08717719861441074
plot_id,batch_id 0 82 miss% 0.07292950672395114
plot_id,batch_id 0 83 miss% 0.08878605587610193
plot_id,batch_id 0 84 miss% 0.0965907185835423
plot_id,batch_id 0 85 miss% 0.0799746837154905
plot_id,batch_id 0 86 miss% 0.05101206290159417
plot_id,batch_id 0 87 miss% 0.08505874342195889
plot_id,batch_id 0 88 miss% 0.08940358458354532
plot_id,batch_id 0 89 miss% 0.06038350368833403
plot_id,batch_id 0 90 miss% 0.03694781427162417
plot_id,batch_id 0 91 miss% 0.05117188075055966
plot_id,batch_id 0 92 miss% 0.09635210095085378
plot_id,batch_id 0 93 miss% 0.06914951511742429
plot_id,batch_id 0 94 miss% 0.09419769822528945
plot_id,batch_id 0 95 miss% 0.09455829213412065
plot_id,batch_id 0 96 miss% 0.040608227096098556
plot_id,batch_id 0 97 miss% 0.027944666173433098
plot_id,batch_id 0 98 miss% 0.05604142945870307
plot_id,batch_id 0 99 miss% 0.058365249472912474
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10504225 0.04755512 0.09397664 0.07541231 0.09304206 0.0841526
 0.09486987 0.11714189 0.07347866 0.05255925 0.05568435 0.08005947
 0.0949241  0.07950707 0.09297021 0.10866333 0.11018246 0.05156552
 0.06969053 0.10207891 0.05900462 0.0490331  0.08122573 0.02849308
 0.03963333 0.11777308 0.05625701 0.05832785 0.03007392 0.04686176
 0.02704999 0.1317031  0.12851909 0.05533677 0.07503541 0.0791517
 0.13524163 0.08169508 0.06026131 0.04536851 0.11008109 0.04729296
 0.01908514 0.07862201 0.06224219 0.04117554 0.082095   0.05107473
 0.03475604 0.02709584 0.15598681 0.071807   0.04838097 0.03291996
 0.04348557 0.09670809 0.07160126 0.05564321 0.0405457  0.03808971
 0.06513961 0.12415024 0.07342473 0.08607603 0.06614146 0.13191884
 0.10739164 0.02493246 0.06963951 0.11175808 0.0930184  0.06928913
 0.15632398 0.06875996 0.13637326 0.16351117 0.14282826 0.05607954
 0.05607296 0.06275444 0.06780432 0.0871772  0.07292951 0.08878606
 0.09659072 0.07997468 0.05101206 0.08505874 0.08940358 0.0603835
 0.03694781 0.05117188 0.0963521  0.06914952 0.0941977  0.09455829
 0.04060823 0.02794467 0.05604143 0.05836525]
for model  10 the mean error 0.07547332429552184
all id 10 hidden_dim 16 learning_rate 0.0025 num_layers 4 frames 21 out win 5 err 0.07547332429552184 time 9667.59119296074
Launcher: Job 11 completed in 9947 seconds.
Launcher: Task 123 done. Exiting.
0.31017781336147543
plot_id,batch_id 0 71 miss% 0.44443822090371804
plot_id,batch_id 0 72 miss% 0.4724375893377946
plot_id,batch_id 0 73 miss% 0.4680010746744312
plot_id,batch_id 0 74 miss% 0.47045845430368366
plot_id,batch_id 0 75 miss% 0.4092940596427252
plot_id,batch_id 0 76 miss% 0.4379928046345248
plot_id,batch_id 0 77 miss% 0.408490342428019
plot_id,batch_id 0 78 miss% 0.374263708614549
plot_id,batch_id 0 79 miss% 0.4696373549612176
plot_id,batch_id 0 80 miss% 0.44682975727142143
plot_id,batch_id 0 81 miss% 0.5568678907973115
plot_id,batch_id 0 82 miss% 0.4569001533688045
plot_id,batch_id 0 83 miss% 0.42289769605003624
plot_id,batch_id 0 84 miss% 0.5149672824137289
plot_id,batch_id 0 85 miss% 0.33625099262673996
plot_id,batch_id 0 86 miss% 0.4645976489811317
plot_id,batch_id 0 87 miss% 0.45959422755720303
plot_id,batch_id 0 88 miss% 0.49522783487843985
plot_id,batch_id 0 89 miss% 0.4423347234531432
plot_id,batch_id 0 90 miss% 0.34024797133084966
plot_id,batch_id 0 91 miss% 0.4194080925449848
plot_id,batch_id 0 92 miss% 0.3983479225591766
plot_id,batch_id 0 93 miss% 0.3247689969761247
plot_id,batch_id 0 94 miss% 0.5131469630124853
plot_id,batch_id 0 95 miss% 0.3851387736405589
plot_id,batch_id 0 96 miss% 0.41985191012670614
plot_id,batch_id 0 97 miss% 0.516724496122833
plot_id,batch_id 0 98 miss% 0.4823714128792791
plot_id,batch_id 0 99 miss% 0.43229549641104453
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.38722758 0.43809877 0.51721388 0.46981318 0.3960479  0.48794503
 0.44505179 0.59764335 0.62159169 0.67330816 0.34652404 0.39212369
 0.41811557 0.39640973 0.5352129  0.43790934 0.55580073 0.50591155
 0.53856663 0.48989197 0.42538261 0.53408337 0.51717892 0.52057441
 0.45181786 0.4297593  0.51320413 0.39311503 0.46252369 0.48618512
 0.37151138 0.57186579 0.54576485 0.48396813 0.44996262 0.4282662
 0.62965901 0.44761914 0.55506867 0.52378048 0.38892598 0.43839666
 0.31307222 0.33907353 0.38308423 0.48811759 0.47243573 0.45542738
 0.41412726 0.43629295 0.50810717 0.56074427 0.54856363 0.40511693
 0.39265003 0.41266331 0.55650268 0.55980914 0.51759568 0.4963266
 0.41345565 0.34799538 0.49537509 0.42210996 0.4879225  0.36474275
 0.46267799 0.38502212 0.50564762 0.5366597  0.31017781 0.44443822
 0.47243759 0.46800107 0.47045845 0.40929406 0.4379928  0.40849034
 0.37426371 0.46963735 0.44682976 0.55686789 0.45690015 0.4228977
 0.51496728 0.33625099 0.46459765 0.45959423 0.49522783 0.44233472
 0.34024797 0.41940809 0.39834792 0.324769   0.51314696 0.38513877
 0.41985191 0.5167245  0.48237141 0.4322955 ]
for model  4 the mean error 0.4599430161392959
all id 4 hidden_dim 24 learning_rate 0.0025 num_layers 3 frames 21 out win 5 err 0.4599430161392959 time 9732.142283201218
Launcher: Job 5 completed in 9966 seconds.
Launcher: Task 75 done. Exiting.
 miss% 0.39000086819482965
plot_id,batch_id 0 70 miss% 0.2826441912586345
plot_id,batch_id 0 71 miss% 0.32728257450280884
plot_id,batch_id 0 72 miss% 0.41278694821729367
plot_id,batch_id 0 73 miss% 0.32160593095956475
plot_id,batch_id 0 74 miss% 0.4196577206741238
plot_id,batch_id 0 75 miss% 0.2703882979489416
plot_id,batch_id 0 76 miss% 0.3061186656087976
plot_id,batch_id 0 77 miss% 0.3366648853545747
plot_id,batch_id 0 78 miss% 0.3156285238224947
plot_id,batch_id 0 79 miss% 0.42909293293982365
plot_id,batch_id 0 80 miss% 0.27342090087481086
plot_id,batch_id 0 81 miss% 0.42562030653961463
plot_id,batch_id 0 82 miss% 0.2790763017172054
plot_id,batch_id 0 83 miss% 0.37985076987750765
plot_id,batch_id 0 84 miss% 0.33417346131133596
plot_id,batch_id 0 85 miss% 0.2528461395074762
plot_id,batch_id 0 86 miss% 0.29551601001005046
plot_id,batch_id 0 87 miss% 0.3576927478391597
plot_id,batch_id 0 88 miss% 0.40027277810701817
plot_id,batch_id 0 89 miss% 0.40260443021522446
plot_id,batch_id 0 90 miss% 0.23276865535889812
plot_id,batch_id 0 91 miss% 0.3598947633395251
plot_id,batch_id 0 92 miss% 0.34956674124079956
plot_id,batch_id 0 93 miss% 0.3504428445888389
plot_id,batch_id 0 94 miss% 0.39369799150820817
plot_id,batch_id 0 95 miss% 0.24896853184594622
plot_id,batch_id 0 96 miss% 0.3218848828006753
plot_id,batch_id 0 97 miss% 0.4269709690072595
plot_id,batch_id 0 98 miss% 0.4648845040739986
plot_id,batch_id 0 99 miss% 0.4157200959179215
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.31444533 0.37941573 0.36967574 0.30992394 0.29287191 0.31428192
 0.31074513 0.49551491 0.50428297 0.48090139 0.28895844 0.33713341
 0.36813132 0.35199614 0.43979014 0.33934935 0.50851786 0.44472851
 0.44327035 0.42328805 0.36152585 0.35654302 0.36840028 0.34746134
 0.37713561 0.31642556 0.3898988  0.41209082 0.30582059 0.38744616
 0.3444242  0.48961237 0.40755614 0.4012302  0.36875391 0.30860515
 0.55254336 0.41592991 0.48280077 0.48702229 0.29558221 0.37180765
 0.273923   0.27298481 0.29138214 0.34534188 0.41203    0.42318843
 0.38943523 0.30888404 0.49121598 0.48326492 0.46014295 0.39302917
 0.3019428  0.37386391 0.52606178 0.41788989 0.43823858 0.50416198
 0.2529063  0.29196751 0.37896265 0.39526889 0.4020947  0.28177333
 0.39018352 0.30985807 0.43237007 0.39000087 0.28264419 0.32728257
 0.41278695 0.32160593 0.41965772 0.2703883  0.30611867 0.33666489
 0.31562852 0.42909293 0.2734209  0.42562031 0.2790763  0.37985077
 0.33417346 0.25284614 0.29551601 0.35769275 0.40027278 0.40260443
 0.23276866 0.35989476 0.34956674 0.35044284 0.39369799 0.24896853
 0.32188488 0.42697097 0.4648845  0.4157201 ]
for model  109 the mean error 0.37285950599722845
all id 109 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 25 out win 5 err 0.37285950599722845 time 9728.319925785065
Launcher: Job 110 completed in 9971 seconds.
Launcher: Task 217 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  89105
Epoch:0, Train loss:0.705019, valid loss:0.658115
Epoch:1, Train loss:0.186669, valid loss:0.006208
Epoch:2, Train loss:0.016875, valid loss:0.005343
Epoch:3, Train loss:0.014978, valid loss:0.004963
Epoch:4, Train loss:0.012319, valid loss:0.004147
Epoch:5, Train loss:0.007511, valid loss:0.002789
Epoch:6, Train loss:0.005758, valid loss:0.002871
Epoch:7, Train loss:0.005110, valid loss:0.002370
Epoch:8, Train loss:0.004843, valid loss:0.002427
Epoch:9, Train loss:0.004762, valid loss:0.002619
Epoch:10, Train loss:0.004327, valid loss:0.002081
Epoch:11, Train loss:0.003367, valid loss:0.001523
Epoch:12, Train loss:0.003002, valid loss:0.001519
Epoch:13, Train loss:0.003007, valid loss:0.001422
Epoch:14, Train loss:0.002830, valid loss:0.001457
Epoch:15, Train loss:0.002684, valid loss:0.001620
Epoch:16, Train loss:0.002877, valid loss:0.001256
Epoch:17, Train loss:0.002470, valid loss:0.001271
Epoch:18, Train loss:0.002370, valid loss:0.001447
Epoch:19, Train loss:0.002347, valid loss:0.001184
Epoch:20, Train loss:0.002323, valid loss:0.001303
Epoch:21, Train loss:0.001728, valid loss:0.001097
Epoch:22, Train loss:0.001637, valid loss:0.000992
Epoch:23, Train loss:0.001602, valid loss:0.001008
Epoch:24, Train loss:0.001637, valid loss:0.000935
Epoch:25, Train loss:0.001572, valid loss:0.001108
Epoch:26, Train loss:0.001561, valid loss:0.000974
Epoch:27, Train loss:0.001530, valid loss:0.000923
Epoch:28, Train loss:0.001512, valid loss:0.000981
Epoch:29, Train loss:0.001450, valid loss:0.000932
Epoch:30, Train loss:0.001426, valid loss:0.000985
Epoch:31, Train loss:0.001164, valid loss:0.000890
Epoch:32, Train loss:0.001124, valid loss:0.000829
Epoch:33, Train loss:0.001133, valid loss:0.000894
Epoch:34, Train loss:0.001080, valid loss:0.000830
Epoch:35, Train loss:0.001074, valid loss:0.000823
Epoch:36, Train loss:0.001079, valid loss:0.000864
Epoch:37, Train loss:0.001067, valid loss:0.000804
Epoch:38, Train loss:0.001054, valid loss:0.000809
Epoch:39, Train loss:0.001025, valid loss:0.000836
Epoch:40, Train loss:0.001026, valid loss:0.000856
Epoch:41, Train loss:0.000893, valid loss:0.000766
Epoch:42, Train loss:0.000867, valid loss:0.000790
Epoch:43, Train loss:0.000854, valid loss:0.000788
Epoch:44, Train loss:0.000853, valid loss:0.000781
Epoch:45, Train loss:0.000852, valid loss:0.000792
Epoch:46, Train loss:0.000841, valid loss:0.000776
Epoch:47, Train loss:0.000844, valid loss:0.000788
Epoch:48, Train loss:0.000837, valid loss:0.000809
Epoch:49, Train loss:0.000816, valid loss:0.000785
Epoch:50, Train loss:0.000820, valid loss:0.000766
Epoch:51, Train loss:0.000762, valid loss:0.000744
Epoch:52, Train loss:0.000754, valid loss:0.000740
Epoch:53, Train loss:0.000750, valid loss:0.000741
Epoch:54, Train loss:0.000748, valid loss:0.000737
Epoch:55, Train loss:0.000745, valid loss:0.000739
Epoch:56, Train loss:0.000744, valid loss:0.000736
Epoch:57, Train loss:0.000742, valid loss:0.000733
Epoch:58, Train loss:0.000741, valid loss:0.000733
Epoch:59, Train loss:0.000739, valid loss:0.000740
Epoch:60, Train loss:0.000738, valid loss:0.000735
training time 9886.22462940216
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.4086763544578385
plot_id,batch_id 0 1 miss% 0.4596172731493028
plot_id,batch_id 0 2 miss% 0.48183651559320567
plot_id,batch_id 0 3 miss% 0.39652271086376806
plot_id,batch_id 0 4 miss% 0.361878500346958
plot_id,batch_id 0 5 miss% 0.43831555578671355
plot_id,batch_id 0 6 miss% 0.4225907362054896
plot_id,batch_id 0 7 miss% 0.5382555484209617
plot_id,batch_id 0 8 miss% 0.5008089025674336
plot_id,batch_id 0 9 miss% 0.5327744808447981
plot_id,batch_id 0 10 miss% 0.40894105807127945
plot_id,batch_id 0 11 miss% 0.4058635766104971
plot_id,batch_id 0 12 miss% 0.46647089429461774
plot_id,batch_id 0 13 miss% 0.3755169570254397
plot_id,batch_id 0 14 miss% 0.5153941492848634
plot_id,batch_id 0 15 miss% 0.5781641941238309
plot_id,batch_id 0 16 miss% 0.5403400712756593
plot_id,batch_id 0 17 miss% 0.4779515357310961
plot_id,batch_id 0 18 miss% 0.47016761852234074
plot_id,batch_id 0 19 miss% 0.414207395852317
plot_id,batch_id 0 20 miss% 0.4765021184224935
plot_id,batch_id 0 21 miss% 0.369031550987206
plot_id,batch_id 0 22 miss% 0.40759386408348197
plot_id,batch_id 0 23 miss% 0.4477227964802341
plot_id,batch_id 0 24 miss% 0.3357017067655883
plot_id,batch_id 0 25 miss% 0.41833645797789715
plot_id,batch_id 0 26 miss% 0.46135851618154106
plot_id,batch_id 0 27 miss% 0.36290550106340297
plot_id,batch_id 0 28 miss% 0.5631905682576778
plot_id,batch_id 0 29 miss% 0.3795164644284078
plot_id,batch_id 0 30 miss% 0.3459000752978189
plot_id,batch_id 0 31 miss% 0.5145150223356627
plot_id,batch_id 0 32 miss% 0.4590746014945016
plot_id,batch_id 0 33 miss% 0.45197431264199855
plot_id,batch_id 0 34 miss% 0.38953179280618655
plot_id,batch_id 0 35 miss% 0.4570816803800934
plot_id,batch_id 0 36 miss% 0.5305234423731734
plot_id,batch_id 0 37 miss% 0.3926041320041246
plot_id,batch_id 0 38 miss% 0.4084998261578014
plot_id,batch_id 0 39 miss% 0.5614154235161241
plot_id,batch_id 0 40 miss% 0.4345607560135642
plot_id,batch_id 0 41 miss% 0.5009774249750965
plot_id,batch_id 0 42 miss% 0.36295520648144597
plot_id,batch_id 0 43 miss% 0.3543909135020292
plot_id,batch_id 0 44 miss% 0.34272336724014185
plot_id,batch_id 0 45 miss% 0.37401897266846207
plot_id,batch_id 0 46 miss% 0.42365259999883487
plot_id,batch_id 0 47 miss% 0.4615927957299091
plot_id,batch_id 0 48 miss% 0.4726233861326851
plot_id,batch_id 0 49 miss% 0.4191121490389366
plot_id,batch_id 0 50 miss% 0.4735663859599179
plot_id,batch_id 0 51 miss% 0.4058429099688115
plot_id,batch_id 0 52 miss% 0.3564658186341359
plot_id,batch_id 0 53 miss% 0.29626348941660297
plot_id,batch_id 0 54 miss% 0.43329369768877296
plot_id,batch_id 0 55 miss% 0.4473320208798884
plot_id,batch_id 0 56 miss% 0.41550281485475393
plot_id,batch_id 0 57 miss% 0.3706023595034386
plot_id,batch_id 0 58 miss% 0.43152149388325195
plot_id,batch_id 0 59 miss% 0.4281664355655498
plot_id,batch_id 0 60 miss% 0.3566793648480727
plot_id,batch_id 0 61 miss% 0.30674505555550025
plot_id,batch_id 0 62 miss% 0.41448304320772617
plot_id,batch_id 0 63 miss% 0.49309327194276686
plot_id,batch_id 0 64 miss% 0.4644926799622752
plot_id,batch_id 0 65 miss% 0.38700188345582126
plot_id,batch_id 0 66 miss% 0.46491122314888805
plot_id,batch_id 0 67 miss% 0.3661845126584516
plot_id,batch_id 0 68 miss% 0.5045129446099929
plot_id,batch_id 0 69 miss% 0.4858518071820252
plot_id,batch_id 0 70 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  41745
Epoch:0, Train loss:0.508655, valid loss:0.506774
Epoch:1, Train loss:0.018853, valid loss:0.002238
Epoch:2, Train loss:0.003439, valid loss:0.001403
Epoch:3, Train loss:0.002600, valid loss:0.001199
Epoch:4, Train loss:0.002268, valid loss:0.001507
Epoch:5, Train loss:0.002024, valid loss:0.001057
Epoch:6, Train loss:0.001832, valid loss:0.000972
Epoch:7, Train loss:0.001754, valid loss:0.000971
Epoch:8, Train loss:0.001607, valid loss:0.000816
Epoch:9, Train loss:0.001557, valid loss:0.000869
Epoch:10, Train loss:0.001494, valid loss:0.000875
Epoch:11, Train loss:0.001144, valid loss:0.000646
Epoch:12, Train loss:0.001118, valid loss:0.000661
Epoch:13, Train loss:0.001118, valid loss:0.000592
Epoch:14, Train loss:0.001077, valid loss:0.000556
Epoch:15, Train loss:0.001053, valid loss:0.000558
Epoch:16, Train loss:0.001023, valid loss:0.000673
Epoch:17, Train loss:0.001016, valid loss:0.000639
Epoch:18, Train loss:0.001025, valid loss:0.000549
Epoch:19, Train loss:0.000986, valid loss:0.000565
Epoch:20, Train loss:0.000968, valid loss:0.000530
Epoch:21, Train loss:0.000809, valid loss:0.000492
Epoch:22, Train loss:0.000797, valid loss:0.000534
Epoch:23, Train loss:0.000777, valid loss:0.000501
Epoch:24, Train loss:0.000782, valid loss:0.000482
Epoch:25, Train loss:0.000769, valid loss:0.000507
Epoch:26, Train loss:0.000764, valid loss:0.000519
Epoch:27, Train loss:0.000747, valid loss:0.000520
Epoch:28, Train loss:0.000736, valid loss:0.000575
Epoch:29, Train loss:0.000749, valid loss:0.000494
Epoch:30, Train loss:0.000736, valid loss:0.000490
Epoch:31, Train loss:0.000641, valid loss:0.000446
Epoch:32, Train loss:0.000640, valid loss:0.000488
Epoch:33, Train loss:0.000630, valid loss:0.000436
Epoch:34, Train loss:0.000640, valid loss:0.000421
Epoch:35, Train loss:0.000622, valid loss:0.000433
Epoch:36, Train loss:0.000623, valid loss:0.000431
Epoch:37, Train loss:0.000624, valid loss:0.000441
Epoch:38, Train loss:0.000617, valid loss:0.000449
Epoch:39, Train loss:0.000616, valid loss:0.000439
Epoch:40, Train loss:0.000611, valid loss:0.000422
Epoch:41, Train loss:0.000566, valid loss:0.000415
Epoch:42, Train loss:0.000559, valid loss:0.000415
Epoch:43, Train loss:0.000556, valid loss:0.000428
Epoch:44, Train loss:0.000559, valid loss:0.000417
Epoch:45, Train loss:0.000554, valid loss:0.000434
Epoch:46, Train loss:0.000553, valid loss:0.000402
Epoch:47, Train loss:0.000550, valid loss:0.000405
Epoch:48, Train loss:0.000547, valid loss:0.000434
Epoch:49, Train loss:0.000545, valid loss:0.000421
Epoch:50, Train loss:0.000548, valid loss:0.000425
Epoch:51, Train loss:0.000521, valid loss:0.000400
Epoch:52, Train loss:0.000515, valid loss:0.000394
Epoch:53, Train loss:0.000513, valid loss:0.000391
Epoch:54, Train loss:0.000513, valid loss:0.000394
Epoch:55, Train loss:0.000512, valid loss:0.000396
Epoch:56, Train loss:0.000511, valid loss:0.000393
Epoch:57, Train loss:0.000511, valid loss:0.000398
Epoch:58, Train loss:0.000511, valid loss:0.000398
Epoch:59, Train loss:0.000510, valid loss:0.000399
Epoch:60, Train loss:0.000510, valid loss:0.000395
training time 9894.018086194992
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.20638127360588834
plot_id,batch_id 0 1 miss% 0.3248814566897712
plot_id,batch_id 0 2 miss% 0.3980195531147465
plot_id,batch_id 0 3 miss% 0.29620928915971456
plot_id,batch_id 0 4 miss% 0.27765046667357207
plot_id,batch_id 0 5 miss% 0.17604606061219488
plot_id,batch_id 0 6 miss% 0.346867510570753
plot_id,batch_id 0 7 miss% 0.4208876062994789
plot_id,batch_id 0 8 miss% 0.5241955537842475
plot_id,batch_id 0 9 miss% 0.377646772749974
plot_id,batch_id 0 10 miss% 0.2079486313357165
plot_id,batch_id 0 11 miss% 0.21343705462527615
plot_id,batch_id 0 12 miss% 0.30568974029662177
plot_id,batch_id 0 13 miss% 0.26939791982913436
plot_id,batch_id 0 14 miss% 0.421582746905667
plot_id,batch_id 0 15 miss% 0.19479272755303925
plot_id,batch_id 0 16 miss% 0.34534188642164176
plot_id,batch_id 0 17 miss% 0.3240648195448847
plot_id,batch_id 0 18 miss% 0.3668136104042653
plot_id,batch_id 0 19 miss% 0.35352273125651484
plot_id,batch_id 0 20 miss% 0.2892633666291388
plot_id,batch_id 0 21 miss% 0.39442491644088745
plot_id,batch_id 0 22 miss% 0.37365674485428174
plot_id,batch_id 0 23 miss% 0.2829519077976863
plot_id,batch_id 0 24 miss% 0.2568275182393056
plot_id,batch_id 0 25 miss% 0.27643649165481415
plot_id,batch_id 0 26 miss% 0.3437979699906722
plot_id,batch_id 0 27 miss% 0.3483080025758883
plot_id,batch_id 0 28 miss% 0.4114121749802544
plot_id,batch_id 0 29 miss% 0.34547256191059195
plot_id,batch_id 0 30 miss% 0.24711756264306042
plot_id,batch_id 0 31 miss% 0.4330680298947497
plot_id,batch_id 0 32 miss% 0.4184344922271644
plot_id,batch_id 0 33 miss% 0.4516443841587788
plot_id,batch_id 0 34 miss% 0.3916282151771473
plot_id,batch_id 0 35 miss% 0.19122999427574586
plot_id,batch_id 0 36 miss% 0.47309724440244977
plot_id,batch_id 0 37 miss% 0.33905087031552905
plot_id,batch_id 0 38 miss% 0.32373422045866634
plot_id,batch_id 0 39 miss% 0.3806613246553498
plot_id,batch_id 0 40 miss% 0.3742316398750241
plot_id,batch_id 0 41 miss% 0.3150789052909417
plot_id,batch_id 0 42 miss% 0.30045824527525583
plot_id,batch_id 0 43 miss% 0.20816196152117733
plot_id,batch_id 0 44 miss% 0.20732048604148834
plot_id,batch_id 0 45 miss% 0.27837625030628715
plot_id,batch_id 0 46 miss% 0.323921803616437
plot_id,batch_id 0 47 miss% 0.447998021260496
plot_id,batch_id 0 48 miss% 0.42345864262084887
plot_id,batch_id 0 49 miss% 0.23815513343496933
plot_id,batch_id 0 50 miss% 0.5782836968387634
plot_id,batch_id 0 51 miss% 0.43188513268172285
plot_id,batch_id 0 52 miss% 0.42258274020812314
plot_id,batch_id 0 53 miss% 0.31236693176349156
plot_id,batch_id 0 54 miss% 0.3734894880403241
plot_id,batch_id 0 55 miss% 0.3386180753080427
plot_id,batch_id 0 56 miss% 0.48901369090744506
plot_id,batch_id 0 57 miss% 0.4682031202377928
plot_id,batch_id 0 58 miss% 0.5184589032771284
plot_id,batch_id 0 59 miss% 0.4189800293316272
plot_id,batch_id 0 60 miss% 0.16625826947414396
plot_id,batch_id 0 61 miss% 0.20052077147576364
plot_id,batch_id 0 62 miss% 0.3133832269398046
plot_id,batch_id 0 63 miss% 0.2848841163231534
plot_id,batch_id 0 64 miss% 0.2982257952609754
plot_id,batch_id 0 65 miss% 0.289593994393715
plot_id,batch_id 0 660.302710651017632
plot_id,batch_id 0 71 miss% 0.44686061237395386
plot_id,batch_id 0 72 miss% 0.4807389383025765
plot_id,batch_id 0 73 miss% 0.41156980318442116
plot_id,batch_id 0 74 miss% 0.4823754313493665
plot_id,batch_id 0 75 miss% 0.2706553784535636
plot_id,batch_id 0 76 miss% 0.41789150606812325
plot_id,batch_id 0 77 miss% 0.4095812360626884
plot_id,batch_id 0 78 miss% 0.429160536278102
plot_id,batch_id 0 79 miss% 0.3658333346568949
plot_id,batch_id 0 80 miss% 0.4236284221803931
plot_id,batch_id 0 81 miss% 0.5014547338589296
plot_id,batch_id 0 82 miss% 0.4368761743978492
plot_id,batch_id 0 83 miss% 0.42225254572758586
plot_id,batch_id 0 84 miss% 0.43434446875371946
plot_id,batch_id 0 85 miss% 0.35379205825136684
plot_id,batch_id 0 86 miss% 0.4630570041903659
plot_id,batch_id 0 87 miss% 0.4911114028406225
plot_id,batch_id 0 88 miss% 0.4901866051667142
plot_id,batch_id 0 89 miss% 0.5298394280044081
plot_id,batch_id 0 90 miss% 0.29682312128860694
plot_id,batch_id 0 91 miss% 0.3930360105720639
plot_id,batch_id 0 92 miss% 0.41368759172990166
plot_id,batch_id 0 93 miss% 0.3216614458139577
plot_id,batch_id 0 94 miss% 0.4854202523428302
plot_id,batch_id 0 95 miss% 0.2867438731224462
plot_id,batch_id 0 96 miss% 0.4240940248691135
plot_id,batch_id 0 97 miss% 0.525059149808801
plot_id,batch_id 0 98 miss% 0.49513813656893924
plot_id,batch_id 0 99 miss% 0.4245636610659317
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.40867635 0.45961727 0.48183652 0.39652271 0.3618785  0.43831556
 0.42259074 0.53825555 0.5008089  0.53277448 0.40894106 0.40586358
 0.46647089 0.37551696 0.51539415 0.57816419 0.54034007 0.47795154
 0.47016762 0.4142074  0.47650212 0.36903155 0.40759386 0.4477228
 0.33570171 0.41833646 0.46135852 0.3629055  0.56319057 0.37951646
 0.34590008 0.51451502 0.4590746  0.45197431 0.38953179 0.45708168
 0.53052344 0.39260413 0.40849983 0.56141542 0.43456076 0.50097742
 0.36295521 0.35439091 0.34272337 0.37401897 0.4236526  0.4615928
 0.47262339 0.41911215 0.47356639 0.40584291 0.35646582 0.29626349
 0.4332937  0.44733202 0.41550281 0.37060236 0.43152149 0.42816644
 0.35667936 0.30674506 0.41448304 0.49309327 0.46449268 0.38700188
 0.46491122 0.36618451 0.50451294 0.48585181 0.30271065 0.44686061
 0.48073894 0.4115698  0.48237543 0.27065538 0.41789151 0.40958124
 0.42916054 0.36583333 0.42362842 0.50145473 0.43687617 0.42225255
 0.43434447 0.35379206 0.463057   0.4911114  0.49018661 0.52983943
 0.29682312 0.39303601 0.41368759 0.32166145 0.48542025 0.28674387
 0.42409402 0.52505915 0.49513814 0.42456366]
for model  58 the mean error 0.4300654220569345
all id 58 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 21 out win 5 err 0.4300654220569345 time 9886.22462940216
Launcher: Job 59 completed in 10123 seconds.
Launcher: Task 200 done. Exiting.
 miss% 0.23671250618467268
plot_id,batch_id 0 67 miss% 0.2189270369571682
plot_id,batch_id 0 68 miss% 0.3047042038751193
plot_id,batch_id 0 69 miss% 0.2927550794998044
plot_id,batch_id 0 70 miss% 0.1964863788451033
plot_id,batch_id 0 71 miss% 0.23866334921273408
plot_id,batch_id 0 72 miss% 0.2509693406552718
plot_id,batch_id 0 73 miss% 0.2873009090978506
plot_id,batch_id 0 74 miss% 0.2878550773484729
plot_id,batch_id 0 75 miss% 0.18906566544111914
plot_id,batch_id 0 76 miss% 0.32528883296686945
plot_id,batch_id 0 77 miss% 0.22699271865247728
plot_id,batch_id 0 78 miss% 0.2805034056173239
plot_id,batch_id 0 79 miss% 0.29313260826733
plot_id,batch_id 0 80 miss% 0.1964393797045763
plot_id,batch_id 0 81 miss% 0.3881390183038581
plot_id,batch_id 0 82 miss% 0.23579288470313833
plot_id,batch_id 0 83 miss% 0.40343983673068384
plot_id,batch_id 0 84 miss% 0.25338688135064735
plot_id,batch_id 0 85 miss% 0.16728451373748718
plot_id,batch_id 0 86 miss% 0.2121228765846081
plot_id,batch_id 0 87 miss% 0.30247908681404423
plot_id,batch_id 0 88 miss% 0.28261719229301374
plot_id,batch_id 0 89 miss% 0.3333933004384067
plot_id,batch_id 0 90 miss% 0.14364914282143432
plot_id,batch_id 0 91 miss% 0.2270281024136054
plot_id,batch_id 0 92 miss% 0.28740629311295035
plot_id,batch_id 0 93 miss% 0.20767988541487567
plot_id,batch_id 0 94 miss% 0.36013631993192957
plot_id,batch_id 0 95 miss% 0.16693651728493566
plot_id,batch_id 0 96 miss% 0.2671565565232946
plot_id,batch_id 0 97 miss% 0.3264233318371322
plot_id,batch_id 0 98 miss% 0.2532830431521847
plot_id,batch_id 0 99 miss% 0.28826732659161197
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.20638127 0.32488146 0.39801955 0.29620929 0.27765047 0.17604606
 0.34686751 0.42088761 0.52419555 0.37764677 0.20794863 0.21343705
 0.30568974 0.26939792 0.42158275 0.19479273 0.34534189 0.32406482
 0.36681361 0.35352273 0.28926337 0.39442492 0.37365674 0.28295191
 0.25682752 0.27643649 0.34379797 0.348308   0.41141217 0.34547256
 0.24711756 0.43306803 0.41843449 0.45164438 0.39162822 0.19122999
 0.47309724 0.33905087 0.32373422 0.38066132 0.37423164 0.31507891
 0.30045825 0.20816196 0.20732049 0.27837625 0.3239218  0.44799802
 0.42345864 0.23815513 0.5782837  0.43188513 0.42258274 0.31236693
 0.37348949 0.33861808 0.48901369 0.46820312 0.5184589  0.41898003
 0.16625827 0.20052077 0.31338323 0.28488412 0.2982258  0.28959399
 0.23671251 0.21892704 0.3047042  0.29275508 0.19648638 0.23866335
 0.25096934 0.28730091 0.28785508 0.18906567 0.32528883 0.22699272
 0.28050341 0.29313261 0.19643938 0.38813902 0.23579288 0.40343984
 0.25338688 0.16728451 0.21212288 0.30247909 0.28261719 0.3333933
 0.14364914 0.2270281  0.28740629 0.20767989 0.36013632 0.16693652
 0.26715656 0.32642333 0.25328304 0.28826733]
for model  189 the mean error 0.3127792307878594
all id 189 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 31 out win 4 err 0.3127792307878594 time 9894.018086194992
Launcher: Job 190 completed in 10141 seconds.
Launcher: Task 212 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  89105
Epoch:0, Train loss:0.562425, valid loss:0.517456
Epoch:1, Train loss:0.069394, valid loss:0.003788
Epoch:2, Train loss:0.005755, valid loss:0.002154
Epoch:3, Train loss:0.003940, valid loss:0.002066
Epoch:4, Train loss:0.003355, valid loss:0.001456
Epoch:5, Train loss:0.002900, valid loss:0.001517
Epoch:6, Train loss:0.002734, valid loss:0.001346
Epoch:7, Train loss:0.002423, valid loss:0.001312
Epoch:8, Train loss:0.002280, valid loss:0.000982
Epoch:9, Train loss:0.002152, valid loss:0.001362
Epoch:10, Train loss:0.002091, valid loss:0.001074
Epoch:11, Train loss:0.001477, valid loss:0.000782
Epoch:12, Train loss:0.001471, valid loss:0.000783
Epoch:13, Train loss:0.001398, valid loss:0.000817
Epoch:14, Train loss:0.001359, valid loss:0.000714
Epoch:15, Train loss:0.001337, valid loss:0.000798
Epoch:16, Train loss:0.001305, valid loss:0.000815
Epoch:17, Train loss:0.001234, valid loss:0.000760
Epoch:18, Train loss:0.001230, valid loss:0.000713
Epoch:19, Train loss:0.001207, valid loss:0.000734
Epoch:20, Train loss:0.001157, valid loss:0.000644
Epoch:21, Train loss:0.000900, valid loss:0.000735
Epoch:22, Train loss:0.000890, valid loss:0.000590
Epoch:23, Train loss:0.000865, valid loss:0.000610
Epoch:24, Train loss:0.000857, valid loss:0.000675
Epoch:25, Train loss:0.000852, valid loss:0.000584
Epoch:26, Train loss:0.000842, valid loss:0.000589
Epoch:27, Train loss:0.000826, valid loss:0.000619
Epoch:28, Train loss:0.000793, valid loss:0.000613
Epoch:29, Train loss:0.000780, valid loss:0.000564
Epoch:30, Train loss:0.000806, valid loss:0.000557
Epoch:31, Train loss:0.000667, valid loss:0.000529
Epoch:32, Train loss:0.000642, valid loss:0.000506
Epoch:33, Train loss:0.000648, valid loss:0.000551
Epoch:34, Train loss:0.000637, valid loss:0.000509
Epoch:35, Train loss:0.000641, valid loss:0.000519
Epoch:36, Train loss:0.000637, valid loss:0.000549
Epoch:37, Train loss:0.000623, valid loss:0.000545
Epoch:38, Train loss:0.000620, valid loss:0.000530
Epoch:39, Train loss:0.000628, valid loss:0.000580
Epoch:40, Train loss:0.000613, valid loss:0.000512
Epoch:41, Train loss:0.000550, valid loss:0.000506
Epoch:42, Train loss:0.000545, valid loss:0.000496
Epoch:43, Train loss:0.000548, valid loss:0.000498
Epoch:44, Train loss:0.000539, valid loss:0.000498
Epoch:45, Train loss:0.000539, valid loss:0.000493
Epoch:46, Train loss:0.000537, valid loss:0.000510
Epoch:47, Train loss:0.000544, valid loss:0.000512
Epoch:48, Train loss:0.000530, valid loss:0.000516
Epoch:49, Train loss:0.000523, valid loss:0.000497
Epoch:50, Train loss:0.000523, valid loss:0.000489
Epoch:51, Train loss:0.000495, valid loss:0.000478
Epoch:52, Train loss:0.000493, valid loss:0.000479
Epoch:53, Train loss:0.000491, valid loss:0.000478
Epoch:54, Train loss:0.000490, valid loss:0.000478
Epoch:55, Train loss:0.000489, valid loss:0.000479
Epoch:56, Train loss:0.000488, valid loss:0.000483
Epoch:57, Train loss:0.000488, valid loss:0.000477
Epoch:58, Train loss:0.000487, valid loss:0.000477
Epoch:59, Train loss:0.000487, valid loss:0.000479
Epoch:60, Train loss:0.000487, valid loss:0.000478
training time 9947.078343153
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.2699516799681652
plot_id,batch_id 0 1 miss% 0.34993668150483404
plot_id,batch_id 0 2 miss% 0.4017051339173732
plot_id,batch_id 0 3 miss% 0.37973805105391106
plot_id,batch_id 0 4 miss% 0.3729594598878142
plot_id,batch_id 0 5 miss% 0.2858213876100329
plot_id,batch_id 0 6 miss% 0.3205196562573299
plot_id,batch_id 0 7 miss% 0.4605856318731741
plot_id,batch_id 0 8 miss% 0.5174733399061148
plot_id,batch_id 0 9 miss% 0.46683578194536546
plot_id,batch_id 0 10 miss% 0.2666014969728626
plot_id,batch_id 0 11 miss% 0.29355375061492883
plot_id,batch_id 0 12 miss% 0.3628400778709541
plot_id,batch_id 0 13 miss% 0.35075756208420983
plot_id,batch_id 0 14 miss% 0.4630607883262742
plot_id,batch_id 0 15 miss% 0.2445636430557225
plot_id,batch_id 0 16 miss% 0.39557500202363505
plot_id,batch_id 0 17 miss% 0.3485271239729645
plot_id,batch_id 0 18 miss% 0.42983031951928485
plot_id,batch_id 0 19 miss% 0.4159628634650427
plot_id,batch_id 0 20 miss% 0.29597286258193634
plot_id,batch_id 0 21 miss% 0.43961679072244486
plot_id,batch_id 0 22 miss% 0.43447680270366784
plot_id,batch_id 0 23 miss% 0.36671066069200803
plot_id,batch_id 0 24 miss% 0.37016384359203003
plot_id,batch_id 0 25 miss% 0.2358714120418339
plot_id,batch_id 0 26 miss% 0.3761754883976472
plot_id,batch_id 0 27 miss% 0.31825065354771215
plot_id,batch_id 0 28 miss% 0.3566280885007714
plot_id,batch_id 0 29 miss% 0.3676526449970857
plot_id,batch_id 0 30 miss% 0.25320467339613845
plot_id,batch_id 0 31 miss% 0.463126505975541
plot_id,batch_id 0 32 miss% 0.48610460582510545
plot_id,batch_id 0 33 miss% 0.4509062207281023
plot_id,batch_id 0 34 miss% 0.4041700490828417
plot_id,batch_id 0 35 miss% 0.32256754785076763
plot_id,batch_id 0 36 miss% 0.4959173034776842
plot_id,batch_id 0 37 miss% 0.442986855748607
plot_id,batch_id 0 38 miss% 0.3919395920901133
plot_id,batch_id 0 39 miss% 0.4226889438545758
plot_id,batch_id 0 40 miss% 0.32016635203672605
plot_id,batch_id 0 41 miss% 0.40650403102714144
plot_id,batch_id 0 42 miss% 0.32012262828979543
plot_id,batch_id 0 43 miss% 0.34711798287757767
plot_id,batch_id 0 44 miss% 0.29353911445133185
plot_id,batch_id 0 45 miss% 0.23339111218877887
plot_id,batch_id 0 46 miss% 0.38127411206678785
plot_id,batch_id 0 47 miss% 0.4493892837978858
plot_id,batch_id 0 48 miss% 0.4181061245918497
plot_id,batch_id 0 49 miss% 0.33585403853122187
plot_id,batch_id 0 50 miss% 0.5040025663120582
plot_id,batch_id 0 51 miss% 0.4316931441925832
plot_id,batch_id 0 52 miss% 0.42200848986223777
plot_id,batch_id 0 53 miss% 0.36184369900025004
plot_id,batch_id 0 54 miss% 0.3729323919662088
plot_id,batch_id 0 55 miss% 0.4213297053200718
plot_id,batch_id 0 56 miss% 0.4566100722072811
plot_id,batch_id 0 57 miss% 0.4415103477426187
plot_id,batch_id 0 58 miss% 0.3823832505332711
plot_id,batch_id 0 59 miss% 0.5175498376626678
plot_id,batch_id 0 60 miss% 0.28330068975745043
plot_id,batch_id 0 61 miss% 0.23083160957739943
plot_id,batch_id 0 62 miss% 0.31988117321578435
plot_id,batch_id 0 63 miss% 0.32265897807503735
plot_id,batch_id 0 64 miss% 0.35271337494792043
plot_id,batch_id 0 65 miss% 0.2282367903058207
plot_id,batch_id 0 66 miss% 0.38436890025633597
plot_id,batch_id 0 67 miss% 0.2810998670312772
plot_id,batch_id 0 68 miss% 0.34804425084522234
plot_id,batch_id 0 69 miss% 0.38651496345494546
plot_id,batch_id 0 70 miss% 0.22260515313644313
plot_id,batch_id 0 71 miss% 0.2885092021134808
plot_id,batch_id 0 72 miss% 0.3707116217768736
plot_id,batch_id 0 73 miss% 0.34028118563565635
plot_id,batch_id 0 74 miss% 0.38105431513865196
plot_id,batch_id 0 75 miss% 0.2219578305427181
plot_id,batch_id 0 76 miss% 0.35855255664063135
plot_id,batch_id 0 77 miss% 0.2514583054603437
plot_id,batch_id 0 78 miss% 0.30481445602276896
plot_id,batch_id 0 79 miss% 0.3384985592441745
plot_id,batch_id 0 80 miss% 0.27016299442671243
plot_id,batch_id 0 81 miss% 0.41741621485463115
plot_id,batch_id 0 82 miss% 0.36279020109742594
plot_id,batch_id 0 83 miss% 0.36510388255528364
plot_id,batch_id 0 84 miss% 0.36496055659852555
plot_id,batch_id 0 85 miss% 0.22188611432032046
plot_id,batch_id 0 86 miss% 0.2819889614216781
plot_id,batch_id 0 87 miss% 0.36279944432546496
plot_id,batch_id 0 88 miss% 0.3579911981294615
plot_id,batch_id 0 89 miss% 0.34242572105410496
plot_id,batch_id 0 90 miss% 0.2065956056634765
plot_id,batch_id 0 91 miss% 0.2841526013651457
plot_id,batch_id 0 92 miss% 0.35702207009785464
plot_id,batch_id 0 93 miss% 0.3542506024222666
plot_id,batch_id 0 94 miss% 0.3764829365301041
plot_id,batch_id 0 95 miss% 0.2057915634332044
plot_id,batch_id 0 96 miss% 0.28617321963173886
plot_id,batch_id 0 97 miss% 0.3027813429116095
plot_id,batch_id 0 98 miss% 0.3020455437624897
plot_id,batch_id 0 99 miss% 0.3014412633926936
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.26995168 0.34993668 0.40170513 0.37973805 0.37295946 0.28582139
 0.32051966 0.46058563 0.51747334 0.46683578 0.2666015  0.29355375
 0.36284008 0.35075756 0.46306079 0.24456364 0.395575   0.34852712
 0.42983032 0.41596286 0.29597286 0.43961679 0.4344768  0.36671066
 0.37016384 0.23587141 0.37617549 0.31825065 0.35662809 0.36765264
 0.25320467 0.46312651 0.48610461 0.45090622 0.40417005 0.32256755
 0.4959173  0.44298686 0.39193959 0.42268894 0.32016635 0.40650403
 0.32012263 0.34711798 0.29353911 0.23339111 0.38127411 0.44938928
 0.41810612 0.33585404 0.50400257 0.43169314 0.42200849 0.3618437
 0.37293239 0.42132971 0.45661007 0.44151035 0.38238325 0.51754984
 0.28330069 0.23083161 0.31988117 0.32265898 0.35271337 0.22823679
 0.3843689  0.28109987 0.34804425 0.38651496 0.22260515 0.2885092
 0.37071162 0.34028119 0.38105432 0.22195783 0.35855256 0.25145831
 0.30481446 0.33849856 0.27016299 0.41741621 0.3627902  0.36510388
 0.36496056 0.22188611 0.28198896 0.36279944 0.3579912  0.34242572
 0.20659561 0.2841526  0.35702207 0.3542506  0.37648294 0.20579156
 0.28617322 0.30278134 0.30204554 0.30144126]
for model  111 the mean error 0.35449615083468083
all id 111 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 25 out win 4 err 0.35449615083468083 time 9947.078343153
Launcher: Job 112 completed in 10189 seconds.
Launcher: Task 168 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  41745
Epoch:0, Train loss:0.508655, valid loss:0.506774
Epoch:1, Train loss:0.019150, valid loss:0.002378
Epoch:2, Train loss:0.003271, valid loss:0.001624
Epoch:3, Train loss:0.002680, valid loss:0.001188
Epoch:4, Train loss:0.002375, valid loss:0.001214
Epoch:5, Train loss:0.002155, valid loss:0.001246
Epoch:6, Train loss:0.002014, valid loss:0.001065
Epoch:7, Train loss:0.001882, valid loss:0.000841
Epoch:8, Train loss:0.001724, valid loss:0.000965
Epoch:9, Train loss:0.001683, valid loss:0.001035
Epoch:10, Train loss:0.001660, valid loss:0.000778
Epoch:11, Train loss:0.001224, valid loss:0.000609
Epoch:12, Train loss:0.001187, valid loss:0.000658
Epoch:13, Train loss:0.001177, valid loss:0.000623
Epoch:14, Train loss:0.001192, valid loss:0.000699
Epoch:15, Train loss:0.001119, valid loss:0.000626
Epoch:16, Train loss:0.001109, valid loss:0.000612
Epoch:17, Train loss:0.001106, valid loss:0.000638
Epoch:18, Train loss:0.001072, valid loss:0.000533
Epoch:19, Train loss:0.001067, valid loss:0.000634
Epoch:20, Train loss:0.001023, valid loss:0.000549
Epoch:21, Train loss:0.000818, valid loss:0.000506
Epoch:22, Train loss:0.000832, valid loss:0.000501
Epoch:23, Train loss:0.000818, valid loss:0.000461
Epoch:24, Train loss:0.000805, valid loss:0.000477
Epoch:25, Train loss:0.000802, valid loss:0.000481
Epoch:26, Train loss:0.000787, valid loss:0.000470
Epoch:27, Train loss:0.000777, valid loss:0.000549
Epoch:28, Train loss:0.000782, valid loss:0.000460
Epoch:29, Train loss:0.000768, valid loss:0.000494
Epoch:30, Train loss:0.000756, valid loss:0.000484
Epoch:31, Train loss:0.000653, valid loss:0.000444
Epoch:32, Train loss:0.000650, valid loss:0.000449
Epoch:33, Train loss:0.000636, valid loss:0.000421
Epoch:34, Train loss:0.000640, valid loss:0.000440
Epoch:35, Train loss:0.000625, valid loss:0.000437
Epoch:36, Train loss:0.000635, valid loss:0.000527
Epoch:37, Train loss:0.000619, valid loss:0.000427
Epoch:38, Train loss:0.000622, valid loss:0.000453
Epoch:39, Train loss:0.000611, valid loss:0.000437
Epoch:40, Train loss:0.000612, valid loss:0.000440
Epoch:41, Train loss:0.000563, valid loss:0.000415
Epoch:42, Train loss:0.000553, valid loss:0.000429
Epoch:43, Train loss:0.000553, valid loss:0.000427
Epoch:44, Train loss:0.000558, valid loss:0.000427
Epoch:45, Train loss:0.000549, valid loss:0.000416
Epoch:46, Train loss:0.000548, valid loss:0.000427
Epoch:47, Train loss:0.000544, valid loss:0.000416
Epoch:48, Train loss:0.000543, valid loss:0.000415
Epoch:49, Train loss:0.000539, valid loss:0.000424
Epoch:50, Train loss:0.000537, valid loss:0.000425
Epoch:51, Train loss:0.000514, valid loss:0.000408
Epoch:52, Train loss:0.000507, valid loss:0.000407
Epoch:53, Train loss:0.000506, valid loss:0.000405
Epoch:54, Train loss:0.000504, valid loss:0.000405
Epoch:55, Train loss:0.000504, valid loss:0.000407
Epoch:56, Train loss:0.000503, valid loss:0.000405
Epoch:57, Train loss:0.000502, valid loss:0.000404
Epoch:58, Train loss:0.000502, valid loss:0.000406
Epoch:59, Train loss:0.000502, valid loss:0.000405
Epoch:60, Train loss:0.000501, valid loss:0.000403
training time 10062.323021173477
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.18305437274227349
plot_id,batch_id 0 1 miss% 0.25616095793245613
plot_id,batch_id 0 2 miss% 0.3364972895881114
plot_id,batch_id 0 3 miss% 0.2353165448963031
plot_id,batch_id 0 4 miss% 0.236357321758693
plot_id,batch_id 0 5 miss% 0.20649142888864636
plot_id,batch_id 0 6 miss% 0.24806627398521422
plot_id,batch_id 0 7 miss% 0.37101512124690783
plot_id,batch_id 0 8 miss% 0.2817539856628325
plot_id,batch_id 0 9 miss% 0.264748301314584
plot_id,batch_id 0 10 miss% 0.15826856457501567
plot_id,batch_id 0 11 miss% 0.23687910248188254
plot_id,batch_id 0 12 miss% 0.325849737563625
plot_id,batch_id 0 13 miss% 0.2424832093638061
plot_id,batch_id 0 14 miss% 0.39744217655419145
plot_id,batch_id 0 15 miss% 0.21463337391085324
plot_id,batch_id 0 16 miss% 0.39483943291271606
plot_id,batch_id 0 17 miss% 0.31511297089356904
plot_id,batch_id 0 18 miss% 0.33525871496288134
plot_id,batch_id 0 19 miss% 0.27756377305091173
plot_id,batch_id 0 20 miss% 0.3656659621651246
plot_id,batch_id 0 21 miss% 0.29756483703271847
plot_id,batch_id 0 22 miss% 0.2502070480977308
plot_id,batch_id 0 23 miss% 0.18106462040519325
plot_id,batch_id 0 24 miss% 0.20948343699072478
plot_id,batch_id 0 25 miss% 0.22570809174647805
plot_id,batch_id 0 26 miss% 0.28183260023603485
plot_id,batch_id 0 27 miss% 0.23805638649232708
plot_id,batch_id 0 28 miss% 0.2768538220991438
plot_id,batch_id 0 29 miss% 0.2505987405157771
plot_id,batch_id 0 30 miss% 0.1798772246631585
plot_id,batch_id 0 31 miss% 0.3158677033914277
plot_id,batch_id 0 32 miss% 0.37277582296294703
plot_id,batch_id 0 33 miss% 0.3356297035926582
plot_id,batch_id 0 34 miss% 0.18963816745284137
plot_id,batch_id 0 35 miss% 0.19220805358855975
plot_id,batch_id 0 36 miss% 0.31820705814409395
plot_id,batch_id 0 37 miss% 0.2791495991344016
plot_id,batch_id 0 38 miss% 0.2370224716428794
plot_id,batch_id 0 39 miss% 0.30074258840405005
plot_id,batch_id 0 40 miss% 0.39626061081865505
plot_id,batch_id 0 41 miss% 0.22903464457485945
plot_id,batch_id 0 42 miss% 0.20239867546629478
plot_id,batch_id 0 43 miss% 0.20009272149946952
plot_id,batch_id 0 44 miss% 0.41402832563574665
plot_id,batch_id 0 45 miss% 0.29623718730562565
plot_id,batch_id 0 46 miss% 0.30895362282704025
plot_id,batch_id 0 47 miss% 0.19930581672166459
plot_id,batch_id 0 48 miss% 0.22979249814514038
plot_id,batch_id 0 49 miss% 0.17987962080162867
plot_id,batch_id 0 50 miss% 0.2769664404928524
plot_id,batch_id 0 51 miss% 0.31361220121082534
plot_id,batch_id 0 52 miss% 0.19803517228104042
plot_id,batch_id 0 53 miss% 0.3565533273904182
plot_id,batch_id 0 54 miss% 0.22132600360589688
plot_id,batch_id 0 55 miss% 0.2385073997424239
plot_id,batch_id 0 56 miss% 0.3338493867235183
plot_id,batch_id 0 57 miss% 0.30435577316566514
plot_id,batch_id 0 58 miss% 0.3653279988248215
plot_id,batch_id 0 59 miss% 0.26417828907572527
plot_id,batch_id 0 60 miss% 0.14774391107502355
plot_id,batch_id 0 61 miss% 0.18030542436389252
plot_id,batch_id 0 62 miss% 0.27972429907480545
plot_id,batch_id 0 63 miss% 0.2793292600225234
plot_id,batch_id 0 64 miss% 0.28767205204260465
plot_id,batch_id 0 65 miss% 0.17496248274242623
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  89105
Epoch:0, Train loss:0.684004, valid loss:0.639811
Epoch:1, Train loss:0.198843, valid loss:0.006492
Epoch:2, Train loss:0.017283, valid loss:0.005298
Epoch:3, Train loss:0.016082, valid loss:0.004907
Epoch:4, Train loss:0.015408, valid loss:0.004791
Epoch:5, Train loss:0.014932, valid loss:0.004672
Epoch:6, Train loss:0.014413, valid loss:0.004545
Epoch:7, Train loss:0.012041, valid loss:0.003651
Epoch:8, Train loss:0.007176, valid loss:0.002775
Epoch:9, Train loss:0.004564, valid loss:0.001977
Epoch:10, Train loss:0.004275, valid loss:0.001915
Epoch:11, Train loss:0.003342, valid loss:0.001563
Epoch:12, Train loss:0.003178, valid loss:0.001670
Epoch:13, Train loss:0.003032, valid loss:0.001537
Epoch:14, Train loss:0.002883, valid loss:0.001499
Epoch:15, Train loss:0.002809, valid loss:0.001587
Epoch:16, Train loss:0.002681, valid loss:0.001377
Epoch:17, Train loss:0.002576, valid loss:0.001399
Epoch:18, Train loss:0.002458, valid loss:0.001296
Epoch:19, Train loss:0.002404, valid loss:0.001434
Epoch:20, Train loss:0.002283, valid loss:0.001215
Epoch:21, Train loss:0.001928, valid loss:0.001116
Epoch:22, Train loss:0.001854, valid loss:0.001196
Epoch:23, Train loss:0.001804, valid loss:0.001115
Epoch:24, Train loss:0.001809, valid loss:0.001130
Epoch:25, Train loss:0.001770, valid loss:0.001079
Epoch:26, Train loss:0.001733, valid loss:0.001155
Epoch:27, Train loss:0.001692, valid loss:0.001039
Epoch:28, Train loss:0.001662, valid loss:0.001090
Epoch:29, Train loss:0.001587, valid loss:0.001058
Epoch:30, Train loss:0.001606, valid loss:0.001019
Epoch:31, Train loss:0.001410, valid loss:0.000990
Epoch:32, Train loss:0.001379, valid loss:0.000979
Epoch:33, Train loss:0.001345, valid loss:0.001009
Epoch:34, Train loss:0.001348, valid loss:0.000955
Epoch:35, Train loss:0.001349, valid loss:0.000997
Epoch:36, Train loss:0.001309, valid loss:0.000935
Epoch:37, Train loss:0.001311, valid loss:0.001000
Epoch:38, Train loss:0.001288, valid loss:0.000958
Epoch:39, Train loss:0.001279, valid loss:0.001025
Epoch:40, Train loss:0.001245, valid loss:0.001069
Epoch:41, Train loss:0.001161, valid loss:0.000911
Epoch:42, Train loss:0.001150, valid loss:0.000903
Epoch:43, Train loss:0.001148, valid loss:0.000944
Epoch:44, Train loss:0.001131, valid loss:0.000916
Epoch:45, Train loss:0.001125, valid loss:0.000880
Epoch:46, Train loss:0.001124, valid loss:0.000907
Epoch:47, Train loss:0.001112, valid loss:0.000931
Epoch:48, Train loss:0.001112, valid loss:0.000895
Epoch:49, Train loss:0.001098, valid loss:0.000922
Epoch:50, Train loss:0.001090, valid loss:0.000901
Epoch:51, Train loss:0.001039, valid loss:0.000879
Epoch:52, Train loss:0.001028, valid loss:0.000883
Epoch:53, Train loss:0.001028, valid loss:0.000883
Epoch:54, Train loss:0.001025, valid loss:0.000876
Epoch:55, Train loss:0.001023, valid loss:0.000879
Epoch:56, Train loss:0.001022, valid loss:0.000872
Epoch:57, Train loss:0.001023, valid loss:0.000891
Epoch:58, Train loss:0.001023, valid loss:0.000900
Epoch:59, Train loss:0.001023, valid loss:0.000907
Epoch:60, Train loss:0.001021, valid loss:0.000900
training time 10077.747910499573
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.07678063227029666
plot_id,batch_id 0 1 miss% 0.061557919002364014
plot_id,batch_id 0 2 miss% 0.08422534104724415
plot_id,batch_id 0 3 miss% 0.0757914560273621
plot_id,batch_id 0 4 miss% 0.03453855700655426
plot_id,batch_id 0 5 miss% 0.07825653519331197
plot_id,batch_id 0 6 miss% 0.05414975209263464
plot_id,batch_id 0 7 miss% 0.10817620286408205
plot_id,batch_id 0 8 miss% 0.06672929369928036
plot_id,batch_id 0 9 miss% 0.04182097571015102
plot_id,batch_id 0 10 miss% 0.07588147077790612
plot_id,batch_id 0 11 miss% 0.07161028693690401
plot_id,batch_id 0 12 miss% 0.0625433226699926
plot_id,batch_id 0 13 miss% 0.06731941824627302
plot_id,batch_id 0 14 miss% 0.12496246767611284
plot_id,batch_id 0 15 miss% 0.11524964422065505
plot_id,batch_id 0 16 miss% 0.057054808788833046
plot_id,batch_id 0 17 miss% 0.08298253601658977
plot_id,batch_id 0 18 miss% 0.04935394082742544
plot_id,batch_id 0 19 miss% 0.10146445591470679
plot_id,batch_id 0 20 miss% 0.04128477242651461
plot_id,batch_id 0 21 miss% 0.024353487405540573
plot_id,batch_id 0 22 miss% 0.11129978798935466
plot_id,batch_id 0 23 miss% 0.02337984796510211
plot_id,batch_id 0 24 miss% 0.03219555339782995
plot_id,batch_id 0 25 miss% 0.05234011688823206
plot_id,batch_id 0 26 miss% 0.03850842502113838
plot_id,batch_id 0 27 miss% 0.029091971514399916
plot_id,batch_id 0 28 miss% 0.05956832725889322
plot_id,batch_id 0 29 miss% 0.023906212866629622
plot_id,batch_id 0 30 miss% 0.06457666151019593
plot_id,batch_id 0 31 miss% 0.08277138716426582
plot_id,batch_id 0 32 miss% 0.13524984573388038
plot_id,batch_id 0 33 miss% 0.05865955839278624
plot_id,batch_id 0 34 miss% 0.06268132336413754
plot_id,batch_id 0 35 miss% 0.06033127949574423
plot_id,batch_id 0 36 miss% 0.081133385505609
plot_id,batch_id 0 37 miss% 0.0730947311952514
plot_id,batch_id 0 38 miss% 0.06058974353862271
plot_id,batch_id 0 39 miss% 0.04970219506054852
plot_id,batch_id 0 40 miss% 0.13332260548407046
plot_id,batch_id 0 41 miss% 0.04069555447367609
plot_id,batch_id 0 42 miss% 0.03892835422141215
plot_id,batch_id 0 43 miss% 0.049541009728948655
plot_id,batch_id 0 44 miss% 0.03225930180423834
plot_id,batch_id 0 45 miss% 0.052596703974069695
plot_id,batch_id 0 46 miss% 0.03220418466241353
plot_id,batch_id 0 47 miss% 0.01949945521482559
plot_id,batch_id 0 48 miss% 0.03689042862319052
plot_id,batch_id 0 49 miss% 0.0386610461410947
plot_id,batch_id 0 50 miss% 0.09571813520903874
plot_id,batch_id 0 51 miss% 0.030509788454511015
plot_id,batch_id 0 52 miss% 0.017750354516649015
plot_id,batch_id 0 53 miss% 0.022115796679260828
plot_id,batch_id 0 54 miss% 0.023217025770264802
plot_id,batch_id 0 55 miss% 0.10222441320201872
plot_id,batch_id 0 56 miss% 0.06570456742333387
plot_id,batch_id 0 57 miss% 0.02287386822649691
plot_id,batch_id 0 58 miss% 0.05754558426331444
plot_id,batch_id 0 59 miss% 0.03593204603889816
plot_id,batch_id 0 60 miss% 0.09273919105444332
plot_id,batch_id 0 61 miss% 0.02253642481248932
plot_id,batch_id 0 62 miss% 0.07053149103254776
plot_id,batch_id 0 63 miss% 0.046333669416825395
plot_id,batch_id 0 64 miss% 0.10423502046983765
plot_id,batch_id 0 65 miss% 0.02855963498710876
plot_id,batch_id 0 66 miss% 0.059997454663628866
plot_id,batch_id 0 67 miss% 0.06779571517569112
plot_id,batch_id 0 68 miss% 0.04361028717644425
plot_id,batch_id 0 66 miss% 0.23749129745292885
plot_id,batch_id 0 67 miss% 0.18988295918549347
plot_id,batch_id 0 68 miss% 0.3047064045561434
plot_id,batch_id 0 69 miss% 0.3572469970437739
plot_id,batch_id 0 70 miss% 0.1902935096462473
plot_id,batch_id 0 71 miss% 0.21334709056580203
plot_id,batch_id 0 72 miss% 0.2758633619080759
plot_id,batch_id 0 73 miss% 0.260892652792755
plot_id,batch_id 0 74 miss% 0.22868850300412386
plot_id,batch_id 0 75 miss% 0.131484648743987
plot_id,batch_id 0 76 miss% 0.2862031218505181
plot_id,batch_id 0 77 miss% 0.22029823546773483
plot_id,batch_id 0 78 miss% 0.29166266343019726
plot_id,batch_id 0 79 miss% 0.30479414920191095
plot_id,batch_id 0 80 miss% 0.23647120313427808
plot_id,batch_id 0 81 miss% 0.27306596471312833
plot_id,batch_id 0 82 miss% 0.24507229216985743
plot_id,batch_id 0 83 miss% 0.29916726468243426
plot_id,batch_id 0 84 miss% 0.26734841787327907
plot_id,batch_id 0 85 miss% 0.16144264566997335
plot_id,batch_id 0 86 miss% 0.21421924757480315
plot_id,batch_id 0 87 miss% 0.27714242232572617
plot_id,batch_id 0 88 miss% 0.28827654663064856
plot_id,batch_id 0 89 miss% 0.30836252329958996
plot_id,batch_id 0 90 miss% 0.1822867206900615
plot_id,batch_id 0 91 miss% 0.2104367501002371
plot_id,batch_id 0 92 miss% 0.21043056227127974
plot_id,batch_id 0 93 miss% 0.2083217871224202
plot_id,batch_id 0 94 miss% 0.385464309056569
plot_id,batch_id 0 95 miss% 0.2205939104360884
plot_id,batch_id 0 96 miss% 0.24252300563835105
plot_id,batch_id 0 97 miss% 0.26779809475284055
plot_id,batch_id 0 98 miss% 0.22977661517222553
plot_id,batch_id 0 99 miss% 0.2993617830077129
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.18305437 0.25616096 0.33649729 0.23531654 0.23635732 0.20649143
 0.24806627 0.37101512 0.28175399 0.2647483  0.15826856 0.2368791
 0.32584974 0.24248321 0.39744218 0.21463337 0.39483943 0.31511297
 0.33525871 0.27756377 0.36566596 0.29756484 0.25020705 0.18106462
 0.20948344 0.22570809 0.2818326  0.23805639 0.27685382 0.25059874
 0.17987722 0.3158677  0.37277582 0.3356297  0.18963817 0.19220805
 0.31820706 0.2791496  0.23702247 0.30074259 0.39626061 0.22903464
 0.20239868 0.20009272 0.41402833 0.29623719 0.30895362 0.19930582
 0.2297925  0.17987962 0.27696644 0.3136122  0.19803517 0.35655333
 0.221326   0.2385074  0.33384939 0.30435577 0.365328   0.26417829
 0.14774391 0.18030542 0.2797243  0.27932926 0.28767205 0.17496248
 0.2374913  0.18988296 0.3047064  0.357247   0.19029351 0.21334709
 0.27586336 0.26089265 0.2286885  0.13148465 0.28620312 0.22029824
 0.29166266 0.30479415 0.2364712  0.27306596 0.24507229 0.29916726
 0.26734842 0.16144265 0.21421925 0.27714242 0.28827655 0.30836252
 0.18228672 0.21043675 0.21043056 0.20832179 0.38546431 0.22059391
 0.24252301 0.26779809 0.22977662 0.29936178]
for model  216 the mean error 0.26214797399845524
all id 216 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 31 out win 4 err 0.26214797399845524 time 10062.323021173477
Launcher: Job 217 completed in 10311 seconds.
Launcher: Task 185 done. Exiting.
plot_id,batch_id 0 69 miss% 0.11090732498310739
plot_id,batch_id 0 70 miss% 0.08940670533272504
plot_id,batch_id 0 71 miss% 0.051255286002931714
plot_id,batch_id 0 72 miss% 0.03503671920906057
plot_id,batch_id 0 73 miss% 0.06417689886946093
plot_id,batch_id 0 74 miss% 0.09184887872981792
plot_id,batch_id 0 75 miss% 0.0842101009273772
plot_id,batch_id 0 76 miss% 0.09525076139573206
plot_id,batch_id 0 77 miss% 0.08871730542942159
plot_id,batch_id 0 78 miss% 0.050332315689759856
plot_id,batch_id 0 79 miss% 0.07732242380755502
plot_id,batch_id 0 80 miss% 0.053822783328377324
plot_id,batch_id 0 81 miss% 0.08793486943334117
plot_id,batch_id 0 82 miss% 0.06378013618208338
plot_id,batch_id 0 83 miss% 0.0468084602292209
plot_id,batch_id 0 84 miss% 0.060202296397550134
plot_id,batch_id 0 85 miss% 0.024995139647812203
plot_id,batch_id 0 86 miss% 0.05205192538290709
plot_id,batch_id 0 87 miss% 0.06947843099901109
plot_id,batch_id 0 88 miss% 0.08183208552157681
plot_id,batch_id 0 89 miss% 0.0952697571931384
plot_id,batch_id 0 90 miss% 0.04181228842920106
plot_id,batch_id 0 91 miss% 0.1071236218698626
plot_id,batch_id 0 92 miss% 0.0644663072829613
plot_id,batch_id 0 93 miss% 0.06559381729826219
plot_id,batch_id 0 94 miss% 0.06128096885410859
plot_id,batch_id 0 95 miss% 0.08995272236644909
plot_id,batch_id 0 96 miss% 0.06161351752523656
plot_id,batch_id 0 97 miss% 0.03887714377474578
plot_id,batch_id 0 98 miss% 0.04582720746644779
plot_id,batch_id 0 99 miss% 0.050720286537070135
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07678063 0.06155792 0.08422534 0.07579146 0.03453856 0.07825654
 0.05414975 0.1081762  0.06672929 0.04182098 0.07588147 0.07161029
 0.06254332 0.06731942 0.12496247 0.11524964 0.05705481 0.08298254
 0.04935394 0.10146446 0.04128477 0.02435349 0.11129979 0.02337985
 0.03219555 0.05234012 0.03850843 0.02909197 0.05956833 0.02390621
 0.06457666 0.08277139 0.13524985 0.05865956 0.06268132 0.06033128
 0.08113339 0.07309473 0.06058974 0.0497022  0.13332261 0.04069555
 0.03892835 0.04954101 0.0322593  0.0525967  0.03220418 0.01949946
 0.03689043 0.03866105 0.09571814 0.03050979 0.01775035 0.0221158
 0.02321703 0.10222441 0.06570457 0.02287387 0.05754558 0.03593205
 0.09273919 0.02253642 0.07053149 0.04633367 0.10423502 0.02855963
 0.05999745 0.06779572 0.04361029 0.11090732 0.08940671 0.05125529
 0.03503672 0.0641769  0.09184888 0.0842101  0.09525076 0.08871731
 0.05033232 0.07732242 0.05382278 0.08793487 0.06378014 0.04680846
 0.0602023  0.02499514 0.05205193 0.06947843 0.08183209 0.09526976
 0.04181229 0.10712362 0.06446631 0.06559382 0.06128097 0.08995272
 0.06161352 0.03887714 0.04582721 0.05072029]
for model  5 the mean error 0.06241605229680412
all id 5 hidden_dim 24 learning_rate 0.0025 num_layers 3 frames 21 out win 6 err 0.06241605229680412 time 10077.747910499573
Launcher: Job 6 completed in 10361 seconds.
Launcher: Task 73 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  41745
Epoch:0, Train loss:0.614092, valid loss:0.600074
Epoch:1, Train loss:0.041304, valid loss:0.004791
Epoch:2, Train loss:0.009056, valid loss:0.002826
Epoch:3, Train loss:0.005704, valid loss:0.002162
Epoch:4, Train loss:0.004830, valid loss:0.002135
Epoch:5, Train loss:0.004222, valid loss:0.001890
Epoch:6, Train loss:0.003793, valid loss:0.001780
Epoch:7, Train loss:0.003471, valid loss:0.001856
Epoch:8, Train loss:0.003231, valid loss:0.001492
Epoch:9, Train loss:0.002980, valid loss:0.001623
Epoch:10, Train loss:0.002780, valid loss:0.001266
Epoch:11, Train loss:0.002313, valid loss:0.001200
Epoch:12, Train loss:0.002218, valid loss:0.001080
Epoch:13, Train loss:0.002116, valid loss:0.001137
Epoch:14, Train loss:0.002047, valid loss:0.001122
Epoch:15, Train loss:0.001992, valid loss:0.001017
Epoch:16, Train loss:0.001943, valid loss:0.001037
Epoch:17, Train loss:0.001865, valid loss:0.001033
Epoch:18, Train loss:0.001844, valid loss:0.000936
Epoch:19, Train loss:0.001760, valid loss:0.000973
Epoch:20, Train loss:0.001745, valid loss:0.001106
Epoch:21, Train loss:0.001508, valid loss:0.000872
Epoch:22, Train loss:0.001466, valid loss:0.000879
Epoch:23, Train loss:0.001440, valid loss:0.000861
Epoch:24, Train loss:0.001419, valid loss:0.000908
Epoch:25, Train loss:0.001431, valid loss:0.000854
Epoch:26, Train loss:0.001382, valid loss:0.000859
Epoch:27, Train loss:0.001372, valid loss:0.000881
Epoch:28, Train loss:0.001368, valid loss:0.000810
Epoch:29, Train loss:0.001346, valid loss:0.000924
Epoch:30, Train loss:0.001329, valid loss:0.000865
Epoch:31, Train loss:0.001202, valid loss:0.000781
Epoch:32, Train loss:0.001179, valid loss:0.000791
Epoch:33, Train loss:0.001177, valid loss:0.000792
Epoch:34, Train loss:0.001161, valid loss:0.000761
Epoch:35, Train loss:0.001164, valid loss:0.000727
Epoch:36, Train loss:0.001165, valid loss:0.000779
Epoch:37, Train loss:0.001153, valid loss:0.000828
Epoch:38, Train loss:0.001137, valid loss:0.000782
Epoch:39, Train loss:0.001136, valid loss:0.000757
Epoch:40, Train loss:0.001127, valid loss:0.000799
Epoch:41, Train loss:0.001059, valid loss:0.000717
Epoch:42, Train loss:0.001051, valid loss:0.000733
Epoch:43, Train loss:0.001048, valid loss:0.000762
Epoch:44, Train loss:0.001042, valid loss:0.000721
Epoch:45, Train loss:0.001044, valid loss:0.000714
Epoch:46, Train loss:0.001037, valid loss:0.000715
Epoch:47, Train loss:0.001035, valid loss:0.000731
Epoch:48, Train loss:0.001031, valid loss:0.000730
Epoch:49, Train loss:0.001022, valid loss:0.000717
Epoch:50, Train loss:0.001020, valid loss:0.000716
Epoch:51, Train loss:0.000978, valid loss:0.000701
Epoch:52, Train loss:0.000972, valid loss:0.000707
Epoch:53, Train loss:0.000969, valid loss:0.000703
Epoch:54, Train loss:0.000969, valid loss:0.000699
Epoch:55, Train loss:0.000968, valid loss:0.000695
Epoch:56, Train loss:0.000968, valid loss:0.000700
Epoch:57, Train loss:0.000967, valid loss:0.000705
Epoch:58, Train loss:0.000967, valid loss:0.000701
Epoch:59, Train loss:0.000967, valid loss:0.000701
Epoch:60, Train loss:0.000967, valid loss:0.000700
training time 10179.123655080795
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.3158338311515026
plot_id,batch_id 0 1 miss% 0.3593953795283736
plot_id,batch_id 0 2 miss% 0.4654975882098961
plot_id,batch_id 0 3 miss% 0.3423855705257194
plot_id,batch_id 0 4 miss% 0.47229544879793134
plot_id,batch_id 0 5 miss% 0.3422916918645406
plot_id,batch_id 0 6 miss% 0.37540150514563286
plot_id,batch_id 0 7 miss% 0.557646306316851
plot_id,batch_id 0 8 miss% 0.6461918252103448
plot_id,batch_id 0 9 miss% 0.5703167045305615
plot_id,batch_id 0 10 miss% 0.2901108935447516
plot_id,batch_id 0 11 miss% 0.3427677291276411
plot_id,batch_id 0 12 miss% 0.3907436363750753
plot_id,batch_id 0 13 miss% 0.3432974353337079
plot_id,batch_id 0 14 miss% 0.44527887318093634
plot_id,batch_id 0 15 miss% 0.31061643864094113
plot_id,batch_id 0 16 miss% 0.4541159604995121
plot_id,batch_id 0 17 miss% 0.4659986021815164
plot_id,batch_id 0 18 miss% 0.46683189597234104
plot_id,batch_id 0 19 miss% 0.4281843628382022
plot_id,batch_id 0 20 miss% 0.35379842554463736
plot_id,batch_id 0 21 miss% 0.4337631175467872
plot_id,batch_id 0 22 miss% 0.4351257383620509
plot_id,batch_id 0 23 miss% 0.4634922485486069
plot_id,batch_id 0 24 miss% 0.39652968961468094
plot_id,batch_id 0 25 miss% 0.3289167376159264
plot_id,batch_id 0 26 miss% 0.40773381299078226
plot_id,batch_id 0 27 miss% 0.34568569285499745
plot_id,batch_id 0 28 miss% 0.4624367802878683
plot_id,batch_id 0 29 miss% 0.4765334318045288
plot_id,batch_id 0 30 miss% 0.3146903524903335
plot_id,batch_id 0 31 miss% 0.4546141293471053
plot_id,batch_id 0 32 miss% 0.5700803238390099
plot_id,batch_id 0 33 miss% 0.5301991477919964
plot_id,batch_id 0 34 miss% 0.45945630720283664
plot_id,batch_id 0 35 miss% 0.35130789602641754
plot_id,batch_id 0 36 miss% 0.5708077471988647
plot_id,batch_id 0 37 miss% 0.4798251080343297
plot_id,batch_id 0 38 miss% 0.4697731620910115
plot_id,batch_id 0 39 miss% 0.4939590685713064
plot_id,batch_id 0 40 miss% 0.27843406286277805
plot_id,batch_id 0 41 miss% 0.3929676751638492
plot_id,batch_id 0 42 miss% 0.35967829852729755
plot_id,batch_id 0 43 miss% 0.3408861778466721
plot_id,batch_id 0 44 miss% 0.3060899832093501
plot_id,batch_id 0 45 miss% 0.354000918417345
plot_id,batch_id 0 46 miss% 0.3606146968748569
plot_id,batch_id 0 47 miss% 0.49586270859362946
plot_id,batch_id 0 48 miss% 0.467888549430333
plot_id,batch_id 0 49 miss% 0.3280542534462477
plot_id,batch_id 0 50 miss% 0.4827072745251636
plot_id,batch_id 0 51 miss% 0.46342517407793193
plot_id,batch_id 0 52 miss% 0.4886199402674872
plot_id,batch_id 0 53 miss% 0.41530349863038934
plot_id,batch_id 0 54 miss% 0.47632295565102806
plot_id,batch_id 0 55 miss% 0.47814885888213987
plot_id,batch_id 0 56 miss% 0.538558160578666
plot_id,batch_id 0 57 miss% 0.46839291698592245
plot_id,batch_id 0 58 miss% 0.4795572500499585
plot_id,batch_id 0 59 miss% 0.4940634475505755
plot_id,batch_id 0 60 miss% 0.28564587194075697
plot_id,batch_id 0 61 miss% 0.3202307848214719
plot_id,batch_id 0 62 miss% 0.3748818299865708
plot_id,batch_id 0 63 miss% 0.43449823544829397
plot_id,batch_id 0 64 miss% 0.37917135019775483
plot_id,batch_id 0 65 miss% 0.281406359939407
plot_id,batch_id 0 66 miss% 0.4003709575379388
plot_id,batch_id 0 67 miss% 0.31078293571075755
plot_id,batch_id 0 68 miss% 0.4439682015135405
plot_id,batch_id 0 69 miss% 0.4787812615375139
plot_id,batch_id 0 70 miss% 0.309242587418943
plot_id,batch_id 0 71 miss% 0.3717662992898515
plot_id,batch_id 0 72 miss% 0.4169517289119861
plot_id,batch_id 0 73 miss% 0.3212061168232214
plot_id,batch_id 0 74 miss% 0.3941748542528674
plot_id,batch_id 0 75 miss% 0.2878010707725
plot_id,batch_id 0 76 miss% 0.326998765601313
plot_id,batch_id 0 77 miss% 0.3265956576007098
plot_id,batch_id 0 78 miss% 0.2978935516108364
plot_id,batch_id 0 79 miss% 0.2980805999543373
plot_id,batch_id 0 80 miss% 0.3362923748425948
plot_id,batch_id 0 81 miss% 0.45203947559335794
plot_id,batch_id 0 82 miss% 0.32383258631683215
plot_id,batch_id 0 83 miss% 0.42232193591949063
plot_id,batch_id 0 84 miss% 0.40408118140591015
plot_id,batch_id 0 85 miss% 0.2920193736579275
plot_id,batch_id 0 86 miss% 0.3392188897254363
plot_id,batch_id 0 87 miss% 0.36761305564805
plot_id,batch_id 0 88 miss% 0.3772409157828017
plot_id,batch_id 0 89 miss% 0.39103262193025917
plot_id,batch_id 0 90 miss% 0.20350015383238212
plot_id,batch_id 0 91 miss% 0.3422169001781672
plot_id,batch_id 0 92 miss% 0.3388439416757264
plot_id,batch_id 0 93 miss% 0.28204384453761006
plot_id,batch_id 0 94 miss% 0.4053102933418743
plot_id,batch_id 0 95 miss% 0.27772876994872814
plot_id,batch_id 0 96 miss% 0.2996071269603294
plot_id,batch_id 0 97 miss% 0.4924437670768015
plot_id,batch_id 0 98 miss% 0.3531477086369778
plot_id,batch_id 0 99 miss% 0.41172966446522513
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.31583383 0.35939538 0.46549759 0.34238557 0.47229545 0.34229169
 0.37540151 0.55764631 0.64619183 0.5703167  0.29011089 0.34276773
 0.39074364 0.34329744 0.44527887 0.31061644 0.45411596 0.4659986
 0.4668319  0.42818436 0.35379843 0.43376312 0.43512574 0.46349225
 0.39652969 0.32891674 0.40773381 0.34568569 0.46243678 0.47653343
 0.31469035 0.45461413 0.57008032 0.53019915 0.45945631 0.3513079
 0.57080775 0.47982511 0.46977316 0.49395907 0.27843406 0.39296768
 0.3596783  0.34088618 0.30608998 0.35400092 0.3606147  0.49586271
 0.46788855 0.32805425 0.48270727 0.46342517 0.48861994 0.4153035
 0.47632296 0.47814886 0.53855816 0.46839292 0.47955725 0.49406345
 0.28564587 0.32023078 0.37488183 0.43449824 0.37917135 0.28140636
 0.40037096 0.31078294 0.4439682  0.47878126 0.30924259 0.3717663
 0.41695173 0.32120612 0.39417485 0.28780107 0.32699877 0.32659566
 0.29789355 0.2980806  0.33629237 0.45203948 0.32383259 0.42232194
 0.40408118 0.29201937 0.33921889 0.36761306 0.37724092 0.39103262
 0.20350015 0.3422169  0.33884394 0.28204384 0.40531029 0.27772877
 0.29960713 0.49244377 0.35314771 0.41172966]
for model  82 the mean error 0.3982622100065874
all id 82 hidden_dim 16 learning_rate 0.0025 num_layers 3 frames 25 out win 5 err 0.3982622100065874 time 10179.123655080795
Launcher: Job 83 completed in 10420 seconds.
Launcher: Task 91 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  41745
Epoch:0, Train loss:0.586994, valid loss:0.568980
Epoch:1, Train loss:0.042429, valid loss:0.005787
Epoch:2, Train loss:0.010500, valid loss:0.002838
Epoch:3, Train loss:0.006023, valid loss:0.002459
Epoch:4, Train loss:0.005223, valid loss:0.002454
Epoch:5, Train loss:0.004509, valid loss:0.002025
Epoch:6, Train loss:0.004110, valid loss:0.001921
Epoch:7, Train loss:0.003802, valid loss:0.001874
Epoch:8, Train loss:0.003615, valid loss:0.001702
Epoch:9, Train loss:0.003396, valid loss:0.001540
Epoch:10, Train loss:0.003243, valid loss:0.001554
Epoch:11, Train loss:0.002748, valid loss:0.001352
Epoch:12, Train loss:0.002706, valid loss:0.001262
Epoch:13, Train loss:0.002614, valid loss:0.001349
Epoch:14, Train loss:0.002536, valid loss:0.001289
Epoch:15, Train loss:0.002493, valid loss:0.001250
Epoch:16, Train loss:0.002359, valid loss:0.001203
Epoch:17, Train loss:0.002300, valid loss:0.001182
Epoch:18, Train loss:0.002267, valid loss:0.001154
Epoch:19, Train loss:0.002181, valid loss:0.001248
Epoch:20, Train loss:0.002146, valid loss:0.001248
Epoch:21, Train loss:0.001893, valid loss:0.001043
Epoch:22, Train loss:0.001848, valid loss:0.001036
Epoch:23, Train loss:0.001818, valid loss:0.001075
Epoch:24, Train loss:0.001805, valid loss:0.001005
Epoch:25, Train loss:0.001767, valid loss:0.000983
Epoch:26, Train loss:0.001744, valid loss:0.000967
Epoch:27, Train loss:0.001708, valid loss:0.000937
Epoch:28, Train loss:0.001697, valid loss:0.000988
Epoch:29, Train loss:0.001679, valid loss:0.000945
Epoch:30, Train loss:0.001654, valid loss:0.000955
Epoch:31, Train loss:0.001517, valid loss:0.000933
Epoch:32, Train loss:0.001499, valid loss:0.000878
Epoch:33, Train loss:0.001496, valid loss:0.000901
Epoch:34, Train loss:0.001475, valid loss:0.000905
Epoch:35, Train loss:0.001468, valid loss:0.000886
Epoch:36, Train loss:0.001456, valid loss:0.000870
Epoch:37, Train loss:0.001441, valid loss:0.000885
Epoch:38, Train loss:0.001434, valid loss:0.000869
Epoch:39, Train loss:0.001421, valid loss:0.000842
Epoch:40, Train loss:0.001406, valid loss:0.000894
Epoch:41, Train loss:0.001348, valid loss:0.000839
Epoch:42, Train loss:0.001332, valid loss:0.000843
Epoch:43, Train loss:0.001328, valid loss:0.000835
Epoch:44, Train loss:0.001328, valid loss:0.000830
Epoch:45, Train loss:0.001320, valid loss:0.000826
Epoch:46, Train loss:0.001310, valid loss:0.000811
Epoch:47, Train loss:0.001309, valid loss:0.000827
Epoch:48, Train loss:0.001299, valid loss:0.000848
Epoch:49, Train loss:0.001295, valid loss:0.000809
Epoch:50, Train loss:0.001290, valid loss:0.000811
Epoch:51, Train loss:0.001245, valid loss:0.000802
Epoch:52, Train loss:0.001238, valid loss:0.000801
Epoch:53, Train loss:0.001236, valid loss:0.000799
Epoch:54, Train loss:0.001235, valid loss:0.000793
Epoch:55, Train loss:0.001235, valid loss:0.000801
Epoch:56, Train loss:0.001234, valid loss:0.000794
Epoch:57, Train loss:0.001233, valid loss:0.000796
Epoch:58, Train loss:0.001232, valid loss:0.000797
Epoch:59, Train loss:0.001233, valid loss:0.000804
Epoch:60, Train loss:0.001233, valid loss:0.000798
training time 10365.157176733017
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.40885174227093213
plot_id,batch_id 0 1 miss% 0.47980302983974343
plot_id,batch_id 0 2 miss% 0.49685252530456614
plot_id,batch_id 0 3 miss% 0.4341849749787096
plot_id,batch_id 0 4 miss% 0.3832526844225894
plot_id,batch_id 0 5 miss% 0.4701467244885161
plot_id,batch_id 0 6 miss% 0.5067804155640799
plot_id,batch_id 0 7 miss% 0.5649927597921479
plot_id,batch_id 0 8 miss% 0.6427367368516365
plot_id,batch_id 0 9 miss% 0.5407732759379055
plot_id,batch_id 0 10 miss% 0.3797594002858984
plot_id,batch_id 0 11 miss% 0.470927117531979
plot_id,batch_id 0 12 miss% 0.456231837772767
plot_id,batch_id 0 13 miss% 0.4484408864342175
plot_id,batch_id 0 14 miss% 0.5511180429553405
plot_id,batch_id 0 15 miss% 0.3781598033148416
plot_id,batch_id 0 16 miss% 0.5621352651875532
plot_id,batch_id 0 17 miss% 0.5562482457956167
plot_id,batch_id 0 18 miss% 0.4911523979103145
plot_id,batch_id 0 19 miss% 0.4760326532573197
plot_id,batch_id 0 20 miss% 0.488097962825209
plot_id,batch_id 0 21 miss% 0.5384214400729738
plot_id,batch_id 0 22 miss% 0.511334791126096
plot_id,batch_id 0 23 miss% 0.4399368971487313
plot_id,batch_id 0 24 miss% 0.4260811600462598
plot_id,batch_id 0 25 miss% 0.4069174024666395
plot_id,batch_id 0 26 miss% 0.5120538509809065
plot_id,batch_id 0 27 miss% 0.4795588265331102
plot_id,batch_id 0 28 miss% 0.48543004439795273
plot_id,batch_id 0 29 miss% 0.4277423291521836
plot_id,batch_id 0 30 miss% 0.37602254179752465
plot_id,batch_id 0 31 miss% 0.5405778933393709
plot_id,batch_id 0 32 miss% 0.5869110515969668
plot_id,batch_id 0 33 miss% 0.4960311363064326
plot_id,batch_id 0 34 miss% 0.4607952736064957
plot_id,batch_id 0 35 miss% 0.44193894483696144
plot_id,batch_id 0 36 miss% 0.5361515400260974
plot_id,batch_id 0 37 miss% 0.5163435548398212
plot_id,batch_id 0 38 miss% 0.4829467280775486
plot_id,batch_id 0 39 miss% 0.5349693515827851
plot_id,batch_id 0 40 miss% 0.42984514416749076
plot_id,batch_id 0 41 miss% 0.47492474377572996
plot_id,batch_id 0 42 miss% 0.3439557451300459
plot_id,batch_id 0 43 miss% 0.3961439102585573
plot_id,batch_id 0 44 miss% 0.37815705868009963
plot_id,batch_id 0 45 miss% 0.4950641143064584
plot_id,batch_id 0 46 miss% 0.49268554696034583
plot_id,batch_id 0 47 miss% 0.4111365369381142
plot_id,batch_id 0 48 miss% 0.4498554783780038
plot_id,batch_id 0 49 miss% 0.4283112879576893
plot_id,batch_id 0 50 miss% 0.45754920225261897
plot_id,batch_id 0 51 miss% 0.5288814019747914
plot_id,batch_id 0 52 miss% 0.49070730240962035
plot_id,batch_id 0 53 miss% 0.41714126651273
plot_id,batch_id 0 54 miss% 0.43730998497606516
plot_id,batch_id 0 55 miss% 0.5474961950449228
plot_id,batch_id 0 56 miss% 0.5632097741990391
plot_id,batch_id 0 57 miss% 0.5858597671987754
plot_id,batch_id 0 58 miss% 0.5545000990578632
plot_id,batch_id 0 59 miss% 0.4816466344861879
plot_id,batch_id 0 60 miss% 0.40635072861127525
plot_id,batch_id 0 61 miss% 0.34736561649138115
plot_id,batch_id 0 62 miss% 0.5242132150239502
plot_id,batch_id 0 63 miss% 0.45802035195641877
plot_id,batch_id 0 64 miss% 0.4108181300159259
plot_id,batch_id 0 65 miss% 0.33199233625729524
plot_id,batch_id 0 66 miss% 0.5287119529297576
plot_id,batch_id 0 67 miss% 0.40227717678369124
plot_id,batch_id 0 68 miss% 0.4561044944004614
plot_id,batch_id 0 69 miss% 0.5065686410725251
plot_id,batch_id 0 70 miss% 0.339154688825168
plot_id,batch_id 0 71 miss% 0.44355178639407433
plot_id,batch_id 0 72 miss% 0.48161083148475975
plot_id,batch_id 0 73 miss% 0.4078754794964162
plot_id,batch_id 0 74 miss% 0.4249830441971703
plot_id,batch_id 0 75 miss% 0.3911624528901986
plot_id,batch_id 0 76 miss% 0.46466274554818593
plot_id,batch_id 0 77 miss% 0.41516023247752054
plot_id,batch_id 0 78 miss% 0.42701615448278285
plot_id,batch_id 0 79 miss% 0.40069274323994725
plot_id,batch_id 0 80 miss% 0.4173526878371431
plot_id,batch_id 0 81 miss% 0.5633466670059775
plot_id,batch_id 0 82 miss% 0.43883549530695093
plot_id,batch_id 0 83 miss% 0.4441412268610578
plot_id,batch_id 0 84 miss% 0.4464105281010726
plot_id,batch_id 0 85 miss% 0.3693708353504188
plot_id,batch_id 0 86 miss% 0.4050114624969088
plot_id,batch_id 0 87 miss% 0.469767883913888
plot_id,batch_id 0 88 miss% 0.4999151086672994
plot_id,batch_id 0 89 miss% 0.5115146000393951
plot_id,batch_id 0 90 miss% 0.29802360623952506
plot_id,batch_id 0 91 miss% 0.41003940054807736
plot_id,batch_id 0 92 miss% 0.40453937858943445
plot_id,batch_id 0 93 miss% 0.3427388695770065
plot_id,batch_id 0 94 miss% 0.557095711169114
plot_id,batch_id 0 95 miss% 0.3910941475872717
plot_id,batch_id 0 96 miss% 0.4152296965859323
plot_id,batch_id 0 97 miss% 0.5238266027576624
plot_id,batch_id 0 98 miss% 0.5032856582225519
plot_id,batch_id 0 99 miss% 0.44904090127182406
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.40885174 0.47980303 0.49685253 0.43418497 0.38325268 0.47014672
 0.50678042 0.56499276 0.64273674 0.54077328 0.3797594  0.47092712
 0.45623184 0.44844089 0.55111804 0.3781598  0.56213527 0.55624825
 0.4911524  0.47603265 0.48809796 0.53842144 0.51133479 0.4399369
 0.42608116 0.4069174  0.51205385 0.47955883 0.48543004 0.42774233
 0.37602254 0.54057789 0.58691105 0.49603114 0.46079527 0.44193894
 0.53615154 0.51634355 0.48294673 0.53496935 0.42984514 0.47492474
 0.34395575 0.39614391 0.37815706 0.49506411 0.49268555 0.41113654
 0.44985548 0.42831129 0.4575492  0.5288814  0.4907073  0.41714127
 0.43730998 0.5474962  0.56320977 0.58585977 0.5545001  0.48164663
 0.40635073 0.34736562 0.52421322 0.45802035 0.41081813 0.33199234
 0.52871195 0.40227718 0.45610449 0.50656864 0.33915469 0.44355179
 0.48161083 0.40787548 0.42498304 0.39116245 0.46466275 0.41516023
 0.42701615 0.40069274 0.41735269 0.56334667 0.4388355  0.44414123
 0.44641053 0.36937084 0.40501146 0.46976788 0.49991511 0.5115146
 0.29802361 0.4100394  0.40453938 0.34273887 0.55709571 0.39109415
 0.4152297  0.5238266  0.50328566 0.4490409 ]
for model  83 the mean error 0.46180097706029316
all id 83 hidden_dim 16 learning_rate 0.0025 num_layers 3 frames 25 out win 6 err 0.46180097706029316 time 10365.157176733017
Launcher: Job 84 completed in 10594 seconds.
Launcher: Task 105 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  55697
Epoch:0, Train loss:0.608634, valid loss:0.572475
Epoch:1, Train loss:0.098345, valid loss:0.004402
Epoch:2, Train loss:0.012807, valid loss:0.004199
Epoch:3, Train loss:0.011653, valid loss:0.003443
Epoch:4, Train loss:0.008165, valid loss:0.002384
Epoch:5, Train loss:0.004939, valid loss:0.002110
Epoch:6, Train loss:0.003255, valid loss:0.001652
Epoch:7, Train loss:0.002720, valid loss:0.001251
Epoch:8, Train loss:0.002501, valid loss:0.001271
Epoch:9, Train loss:0.002339, valid loss:0.001237
Epoch:10, Train loss:0.002229, valid loss:0.001104
Epoch:11, Train loss:0.001655, valid loss:0.000887
Epoch:12, Train loss:0.001601, valid loss:0.000886
Epoch:13, Train loss:0.001603, valid loss:0.000808
Epoch:14, Train loss:0.001553, valid loss:0.000881
Epoch:15, Train loss:0.001485, valid loss:0.000800
Epoch:16, Train loss:0.001466, valid loss:0.000883
Epoch:17, Train loss:0.001419, valid loss:0.000831
Epoch:18, Train loss:0.001378, valid loss:0.000812
Epoch:19, Train loss:0.001391, valid loss:0.000719
Epoch:20, Train loss:0.001336, valid loss:0.000769
Epoch:21, Train loss:0.001068, valid loss:0.000659
Epoch:22, Train loss:0.001044, valid loss:0.000697
Epoch:23, Train loss:0.001051, valid loss:0.000642
Epoch:24, Train loss:0.001035, valid loss:0.000643
Epoch:25, Train loss:0.001009, valid loss:0.000632
Epoch:26, Train loss:0.000986, valid loss:0.000725
Epoch:27, Train loss:0.001014, valid loss:0.000642
Epoch:28, Train loss:0.000971, valid loss:0.000683
Epoch:29, Train loss:0.000966, valid loss:0.000604
Epoch:30, Train loss:0.000943, valid loss:0.000611
Epoch:31, Train loss:0.000824, valid loss:0.000572
Epoch:32, Train loss:0.000822, valid loss:0.000562
Epoch:33, Train loss:0.000802, valid loss:0.000586
Epoch:34, Train loss:0.000806, valid loss:0.000560
Epoch:35, Train loss:0.000805, valid loss:0.000570
Epoch:36, Train loss:0.000790, valid loss:0.000582
Epoch:37, Train loss:0.000786, valid loss:0.000622
Epoch:38, Train loss:0.000781, valid loss:0.000544
Epoch:39, Train loss:0.000782, valid loss:0.000567
Epoch:40, Train loss:0.000777, valid loss:0.000558
Epoch:41, Train loss:0.000706, valid loss:0.000528
Epoch:42, Train loss:0.000699, valid loss:0.000533
Epoch:43, Train loss:0.000701, valid loss:0.000516
Epoch:44, Train loss:0.000698, valid loss:0.000521
Epoch:45, Train loss:0.000693, valid loss:0.000514
Epoch:46, Train loss:0.000694, valid loss:0.000523
Epoch:47, Train loss:0.000683, valid loss:0.000536
Epoch:48, Train loss:0.000690, valid loss:0.000561
Epoch:49, Train loss:0.000684, valid loss:0.000514
Epoch:50, Train loss:0.000675, valid loss:0.000522
Epoch:51, Train loss:0.000645, valid loss:0.000510
Epoch:52, Train loss:0.000639, valid loss:0.000510
Epoch:53, Train loss:0.000637, valid loss:0.000507
Epoch:54, Train loss:0.000635, valid loss:0.000505
Epoch:55, Train loss:0.000634, valid loss:0.000504
Epoch:56, Train loss:0.000633, valid loss:0.000510
Epoch:57, Train loss:0.000633, valid loss:0.000506
Epoch:58, Train loss:0.000633, valid loss:0.000506
Epoch:59, Train loss:0.000632, valid loss:0.000510
Epoch:60, Train loss:0.000632, valid loss:0.000510
training time 10387.769671440125
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.0675405654461094
plot_id,batch_id 0 1 miss% 0.04586338103378108
plot_id,batch_id 0 2 miss% 0.11327476538804757
plot_id,batch_id 0 3 miss% 0.060997298193428126
plot_id,batch_id 0 4 miss% 0.10171084827746932
plot_id,batch_id 0 5 miss% 0.04841593065688535
plot_id,batch_id 0 6 miss% 0.03726448984076089
plot_id,batch_id 0 7 miss% 0.08150572598409467
plot_id,batch_id 0 8 miss% 0.07291563455080548
plot_id,batch_id 0 9 miss% 0.05241437420866788
plot_id,batch_id 0 10 miss% 0.02994325096466175
plot_id,batch_id 0 11 miss% 0.09621642222003786
plot_id,batch_id 0 12 miss% 0.09459560738321109
plot_id,batch_id 0 13 miss% 0.05275642854590414
plot_id,batch_id 0 14 miss% 0.10521795838195401
plot_id,batch_id 0 15 miss% 0.028131721205704757
plot_id,batch_id 0 16 miss% 0.06765294609302797
plot_id,batch_id 0 17 miss% 0.05533018789314966
plot_id,batch_id 0 18 miss% 0.06902565708327545
plot_id,batch_id 0 19 miss% 0.10417443764749937
plot_id,batch_id 0 20 miss% 0.0672214559734772
plot_id,batch_id 0 21 miss% 0.04192451028787996
plot_id,batch_id 0 22 miss% 0.07964813968819145
plot_id,batch_id 0 23 miss% 0.027630705692966356
plot_id,batch_id 0 24 miss% 0.07237404015926868
plot_id,batch_id 0 25 miss% 0.06962111506904566
plot_id,batch_id 0 26 miss% 0.06713887475924073
plot_id,batch_id 0 27 miss% 0.059900749341517984
plot_id,batch_id 0 28 miss% 0.02531709904059071
plot_id,batch_id 0 29 miss% 0.028219060882153462
plot_id,batch_id 0 30 miss% 0.04219405547951174
plot_id,batch_id 0 31 miss% 0.09284310357980859
plot_id,batch_id 0 32 miss% 0.0843953721579141
plot_id,batch_id 0 33 miss% 0.04644552026216623
plot_id,batch_id 0 34 miss% 0.03555802894360351
plot_id,batch_id 0 35 miss% 0.03139671701224034
plot_id,batch_id 0 36 miss% 0.04223005324518972
plot_id,batch_id 0 37 miss% 0.12565012918631227
plot_id,batch_id 0 38 miss% 0.066153899987705
plot_id,batch_id 0 39 miss% 0.07247923838580773
plot_id,batch_id 0 40 miss% 0.09071028185349211
plot_id,batch_id 0 41 miss% 0.06719990822821158
plot_id,batch_id 0 42 miss% 0.025787089028687853
plot_id,batch_id 0 43 miss% 0.06885904155389287
plot_id,batch_id 0 44 miss% 0.03026470899923581
plot_id,batch_id 0 45 miss% 0.05079203428761228
plot_id,batch_id 0 46 miss% 0.046411084478217894
plot_id,batch_id 0 47 miss% 0.026808632188297866
plot_id,batch_id 0 48 miss% 0.03185410284071146
plot_id,batch_id 0 49 miss% 0.03330407884639194
plot_id,batch_id 0 50 miss% 0.15207654860805497
plot_id,batch_id 0 51 miss% 0.04832517346047743
plot_id,batch_id 0 52 miss% 0.062066576319895546
plot_id,batch_id 0 53 miss% 0.023855144012853064
plot_id,batch_id 0 54 miss% 0.033083996329338665
plot_id,batch_id 0 55 miss% 0.07016868742994553
plot_id,batch_id 0 56 miss% 0.05088162468105489
plot_id,batch_id 0 57 miss% 0.056403952745089025
plot_id,batch_id 0 58 miss% 0.048651028686729804
plot_id,batch_id 0 59 miss% 0.025269371585190898
plot_id,batch_id 0 60 miss% 0.03256573620150642
plot_id,batch_id 0 61 miss% 0.05011094859080339
plot_id,batch_id 0 62 miss% 0.07813785506308434
plot_id,batch_id 0 63 miss% 0.04148366757635823
plot_id,batch_id 0 64 miss% 0.045665480730276424
plot_id,batch_id 0 65 miss% 0.058064131527548066
plot_id,batch_id 0 66 miss% 0.07295006121782581
plot_id,batch_id 0 67 miss% 0.05249670904466762
plot_id,batch_id 0 68 miss% 0.0450001534608995
plot_id,batch_id 0 69 miss% 0.0940486112883732
plot_id,batch_id 0 70 miss% 0.04098828388029533
plot_id,batch_id 0 71 miss% 0.04628775350587855
plot_id,batch_id 0 72 miss% 0.12875626475449337
plot_id,batch_id 0 73 miss% 0.05682702756914281
plot_id,batch_id 0 74 miss% 0.14815650238367625
plot_id,batch_id 0 75 miss% 0.03280825064180867
plot_id,batch_id 0 76 miss% 0.05820831345845069
plot_id,batch_id 0 77 miss% 0.08938522226587642
plot_id,batch_id 0 78 miss% 0.04718478984352382
plot_id,batch_id 0 79 miss% 0.11479813267302849
plot_id,batch_id 0 80 miss% 0.0570493950492222
plot_id,batch_id 0 81 miss% 0.07952408564053785
plot_id,batch_id 0 82 miss% 0.05509843379244469
plot_id,batch_id 0 83 miss% 0.08579252001694256
plot_id,batch_id 0 84 miss% 0.05645862927064724
plot_id,batch_id 0 85 miss% 0.03992798691749922
plot_id,batch_id 0 86 miss% 0.028973990580155956
plot_id,batch_id 0 87 miss% 0.07482586450355426
plot_id,batch_id 0 88 miss% 0.10830888619155918
plot_id,batch_id 0 89 miss% 0.06654682668835356
plot_id,batch_id 0 90 miss% 0.048415490382655114
plot_id,batch_id 0 91 miss% 0.0397224083564744
plot_id,batch_id 0 92 miss% 0.05127041660516035
plot_id,batch_id 0 93 miss% 0.06662171056343867
plot_id,batch_id 0 94 miss% 0.03445820326322454
plot_id,batch_id 0 95 miss% 0.04102497776898763
plot_id,batch_id 0 96 miss% 0.04867814157338704
plot_id,batch_id 0 97 miss% 0.03192519027527059
plot_id,batch_id 0 98 miss% 0.04074888864033137
plot_id,batch_id 0 99 miss% 0.06672407212079583
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06754057 0.04586338 0.11327477 0.0609973  0.10171085 0.04841593
 0.03726449 0.08150573 0.07291563 0.05241437 0.02994325 0.09621642
 0.09459561 0.05275643 0.10521796 0.02813172 0.06765295 0.05533019
 0.06902566 0.10417444 0.06722146 0.04192451 0.07964814 0.02763071
 0.07237404 0.06962112 0.06713887 0.05990075 0.0253171  0.02821906
 0.04219406 0.0928431  0.08439537 0.04644552 0.03555803 0.03139672
 0.04223005 0.12565013 0.0661539  0.07247924 0.09071028 0.06719991
 0.02578709 0.06885904 0.03026471 0.05079203 0.04641108 0.02680863
 0.0318541  0.03330408 0.15207655 0.04832517 0.06206658 0.02385514
 0.033084   0.07016869 0.05088162 0.05640395 0.04865103 0.02526937
 0.03256574 0.05011095 0.07813786 0.04148367 0.04566548 0.05806413
 0.07295006 0.05249671 0.04500015 0.09404861 0.04098828 0.04628775
 0.12875626 0.05682703 0.1481565  0.03280825 0.05820831 0.08938522
 0.04718479 0.11479813 0.0570494  0.07952409 0.05509843 0.08579252
 0.05645863 0.03992799 0.02897399 0.07482586 0.10830889 0.06654683
 0.04841549 0.03972241 0.05127042 0.06662171 0.0344582  0.04102498
 0.04867814 0.03192519 0.04074889 0.06672407]
for model  117 the mean error 0.06062082580174609
all id 117 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 25 out win 4 err 0.06062082580174609 time 10387.769671440125
Launcher: Job 118 completed in 10657 seconds.
Launcher: Task 171 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  69649
Epoch:0, Train loss:0.795382, valid loss:0.773365
Epoch:1, Train loss:0.069090, valid loss:0.008361
Epoch:2, Train loss:0.014252, valid loss:0.004496
Epoch:3, Train loss:0.010161, valid loss:0.003481
Epoch:4, Train loss:0.006904, valid loss:0.002553
Epoch:5, Train loss:0.005475, valid loss:0.002698
Epoch:6, Train loss:0.004808, valid loss:0.002117
Epoch:7, Train loss:0.004419, valid loss:0.001964
Epoch:8, Train loss:0.003949, valid loss:0.002203
Epoch:9, Train loss:0.003642, valid loss:0.001899
Epoch:10, Train loss:0.003481, valid loss:0.001774
Epoch:11, Train loss:0.002675, valid loss:0.001459
Epoch:12, Train loss:0.002572, valid loss:0.001293
Epoch:13, Train loss:0.002531, valid loss:0.001303
Epoch:14, Train loss:0.002418, valid loss:0.001206
Epoch:15, Train loss:0.002358, valid loss:0.001409
Epoch:16, Train loss:0.002337, valid loss:0.001336
Epoch:17, Train loss:0.002168, valid loss:0.001130
Epoch:18, Train loss:0.002138, valid loss:0.001277
Epoch:19, Train loss:0.002051, valid loss:0.001056
Epoch:20, Train loss:0.002010, valid loss:0.001149
Epoch:21, Train loss:0.001681, valid loss:0.000953
Epoch:22, Train loss:0.001649, valid loss:0.000971
Epoch:23, Train loss:0.001624, valid loss:0.000926
Epoch:24, Train loss:0.001617, valid loss:0.000870
Epoch:25, Train loss:0.001577, valid loss:0.000913
Epoch:26, Train loss:0.001543, valid loss:0.000863
Epoch:27, Train loss:0.001543, valid loss:0.001042
Epoch:28, Train loss:0.001517, valid loss:0.000969
Epoch:29, Train loss:0.001477, valid loss:0.000904
Epoch:30, Train loss:0.001467, valid loss:0.000878
Epoch:31, Train loss:0.001299, valid loss:0.000876
Epoch:32, Train loss:0.001295, valid loss:0.000889
Epoch:33, Train loss:0.001280, valid loss:0.000814
Epoch:34, Train loss:0.001269, valid loss:0.000818
Epoch:35, Train loss:0.001263, valid loss:0.000793
Epoch:36, Train loss:0.001253, valid loss:0.000792
Epoch:37, Train loss:0.001233, valid loss:0.000771
Epoch:38, Train loss:0.001222, valid loss:0.000791
Epoch:39, Train loss:0.001206, valid loss:0.000798
Epoch:40, Train loss:0.001221, valid loss:0.000826
Epoch:41, Train loss:0.001132, valid loss:0.000774
Epoch:42, Train loss:0.001117, valid loss:0.000788
Epoch:43, Train loss:0.001118, valid loss:0.000748
Epoch:44, Train loss:0.001109, valid loss:0.000757
Epoch:45, Train loss:0.001106, valid loss:0.000789
Epoch:46, Train loss:0.001100, valid loss:0.000756
Epoch:47, Train loss:0.001096, valid loss:0.000849
Epoch:48, Train loss:0.001088, valid loss:0.000758
Epoch:49, Train loss:0.001084, valid loss:0.000753
Epoch:50, Train loss:0.001084, valid loss:0.000731
Epoch:51, Train loss:0.001030, valid loss:0.000727
Epoch:52, Train loss:0.001022, valid loss:0.000732
Epoch:53, Train loss:0.001021, valid loss:0.000731
Epoch:54, Train loss:0.001020, valid loss:0.000731
Epoch:55, Train loss:0.001019, valid loss:0.000730
Epoch:56, Train loss:0.001018, valid loss:0.000726
Epoch:57, Train loss:0.001018, valid loss:0.000725
Epoch:58, Train loss:0.001017, valid loss:0.000736
Epoch:59, Train loss:0.001018, valid loss:0.000741
Epoch:60, Train loss:0.001016, valid loss:0.000727
training time 10476.967697381973
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08150251516079252
plot_id,batch_id 0 1 miss% 0.05794282104647632
plot_id,batch_id 0 2 miss% 0.12564850716078813
plot_id,batch_id 0 3 miss% 0.06522503616754911
plot_id,batch_id 0 4 miss% 0.0820183230135276
plot_id,batch_id 0 5 miss% 0.0627545512553451
plot_id,batch_id 0 6 miss% 0.037472130186706844
plot_id,batch_id 0 7 miss% 0.10233268996084931
plot_id,batch_id 0 8 miss% 0.08345516196179757
plot_id,batch_id 0 9 miss% 0.07181605227575877
plot_id,batch_id 0 10 miss% 0.03339792792605978
plot_id,batch_id 0 11 miss% 0.07152439352708335
plot_id,batch_id 0 12 miss% 0.06317518158332426
plot_id,batch_id 0 13 miss% 0.05429439337154063
plot_id,batch_id 0 14 miss% 0.12306817971644698
plot_id,batch_id 0 15 miss% 0.04632693868309199
plot_id,batch_id 0 16 miss% 0.1481543174274398
plot_id,batch_id 0 17 miss% 0.0670239548305093
plot_id,batch_id 0 18 miss% 0.07623763714505716
plot_id,batch_id 0 19 miss% 0.11378644015410656
plot_id,batch_id 0 20 miss% 0.052913810857618385
plot_id,batch_id 0 21 miss% 0.07311813912686771
plot_id,batch_id 0 22 miss% 0.04469475583511335
plot_id,batch_id 0 23 miss% 0.040379155296375716
plot_id,batch_id 0 24 miss% 0.0390366319804983
plot_id,batch_id 0 25 miss% 0.10455295137476517
plot_id,batch_id 0 26 miss% 0.05144901661210928
plot_id,batch_id 0 27 miss% 0.04191682782081467
plot_id,batch_id 0 28 miss% 0.017493211961048596
plot_id,batch_id 0 29 miss% 0.022423718401317086
plot_id,batch_id 0 30 miss% 0.02650048610181045
plot_id,batch_id 0 31 miss% 0.09845573654735185
plot_id,batch_id 0 32 miss% 0.15492687629817808
plot_id,batch_id 0 33 miss% 0.054353979555958415
plot_id,batch_id 0 34 miss% 0.06971803605951872
plot_id,batch_id 0 35 miss% 0.050301244624965186
plot_id,batch_id 0 36 miss% 0.09544753129995819
plot_id,batch_id 0 37 miss% 0.0760040398216168
plot_id,batch_id 0 38 miss% 0.0853291720432946
plot_id,batch_id 0 39 miss% 0.029184660628873146
plot_id,batch_id 0 40 miss% 0.08577695962251433
plot_id,batch_id 0 41 miss% 0.057420106267659095
plot_id,batch_id 0 42 miss% 0.023407924314680716
plot_id,batch_id 0 43 miss% 0.047249396734715565
plot_id,batch_id 0 44 miss% 0.027457544854959656
plot_id,batch_id 0 45 miss% 0.05495565170952303
plot_id,batch_id 0 46 miss% 0.05837967381470049
plot_id,batch_id 0 47 miss% 0.02495261070273316
plot_id,batch_id 0 48 miss% 0.034462296102880874
plot_id,batch_id 0 49 miss% 0.06098712132107765
plot_id,batch_id 0 50 miss% 0.21208068233680888
plot_id,batch_id 0 51 miss% 0.04766288376145142
plot_id,batch_id 0 52 miss% 0.03311440634918927
plot_id,batch_id 0 53 miss% 0.018962419718784942
plot_id,batch_id 0 54 miss% 0.031511945695521584
plot_id,batch_id 0 55 miss% 0.0984505439405806
plot_id,batch_id 0 56 miss% 0.13418655498029639
plot_id,batch_id 0 57 miss% 0.07006002856205554
plot_id,batch_id 0 58 miss% 0.047590610615981424
plot_id,batch_id 0 59 miss% 0.03338386582094422
plot_id,batch_id 0 60 miss% 0.026426933677019784
plot_id,batch_id 0 61 miss% 0.05273662176374383
plot_id,batch_id 0 62 miss% 0.07446417201375746
plot_id,batch_id 0 63 miss% 0.08531576843751695
plot_id,batch_id 0 64 miss% 0.07153623572193732
plot_id,batch_id 0 65 miss% 0.08340036994703194
plot_id,batch_id 0 66 miss% 0.028091684946216264
plot_id,batch_id 0 67 miss% 0.026903884568889428
plot_id,batch_id 0 68 miss% 0.0331848349185116
plot_id,batch_id 0 69 miss% 0.0687182876478632
plot_id,batch_id 0 70 miss% 0.05787065413468954
plot_id,batch_id 0 71 miss% 0.037868735592629196
plot_id,batch_id 0 72 miss% 0.07278047588398236
plot_id,batch_id 0 73 miss% 0.080124289301759
plot_id,batch_id 0 74 miss% 0.09519175963949619
plot_id,batch_id 0 75 miss% 0.10771265000509878
plot_id,batch_id 0 76 miss% 0.12441404571666277
plot_id,batch_id 0 77 miss% 0.019054923717266737
plot_id,batch_id 0 78 miss% 0.048940633413563736
plot_id,batch_id 0 79 miss% 0.0697554122880642
plot_id,batch_id 0 80 miss% 0.06281891800033151
plot_id,batch_id 0 81 miss% 0.12992826968298088
plot_id,batch_id 0 82 miss% 0.06446210933977989
plot_id,batch_id 0 83 miss% 0.06563806642491442
plot_id,batch_id 0 84 miss% 0.10484115820805891
plot_id,batch_id 0 85 miss% 0.03844483703234326
plot_id,batch_id 0 86 miss% 0.02898214520716951
plot_id,batch_id 0 87 miss% 0.09984176239959565
plot_id,batch_id 0 88 miss% 0.08974958775450585
plot_id,batch_id 0 89 miss% 0.06539761859144647
plot_id,batch_id 0 90 miss% 0.04641180911165731
plot_id,batch_id 0 91 miss% 0.027358182998313392
plot_id,batch_id 0 92 miss% 0.05772616423350841
plot_id,batch_id 0 93 miss% 0.04636511128722525
plot_id,batch_id 0 94 miss% 0.07720629346056446
plot_id,batch_id 0 95 miss% 0.05369193228157085
plot_id,batch_id 0 96 miss% 0.055338398341909026
plot_id,batch_id 0 97 miss% 0.04020661628100395
plot_id,batch_id 0 98 miss% 0.035040356177012566
plot_id,batch_id 0 99 miss% 0.08332678223640566
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08150252 0.05794282 0.12564851 0.06522504 0.08201832 0.06275455
 0.03747213 0.10233269 0.08345516 0.07181605 0.03339793 0.07152439
 0.06317518 0.05429439 0.12306818 0.04632694 0.14815432 0.06702395
 0.07623764 0.11378644 0.05291381 0.07311814 0.04469476 0.04037916
 0.03903663 0.10455295 0.05144902 0.04191683 0.01749321 0.02242372
 0.02650049 0.09845574 0.15492688 0.05435398 0.06971804 0.05030124
 0.09544753 0.07600404 0.08532917 0.02918466 0.08577696 0.05742011
 0.02340792 0.0472494  0.02745754 0.05495565 0.05837967 0.02495261
 0.0344623  0.06098712 0.21208068 0.04766288 0.03311441 0.01896242
 0.03151195 0.09845054 0.13418655 0.07006003 0.04759061 0.03338387
 0.02642693 0.05273662 0.07446417 0.08531577 0.07153624 0.08340037
 0.02809168 0.02690388 0.03318483 0.06871829 0.05787065 0.03786874
 0.07278048 0.08012429 0.09519176 0.10771265 0.12441405 0.01905492
 0.04894063 0.06975541 0.06281892 0.12992827 0.06446211 0.06563807
 0.10484116 0.03844484 0.02898215 0.09984176 0.08974959 0.06539762
 0.04641181 0.02735818 0.05772616 0.04636511 0.07720629 0.05369193
 0.0553384  0.04020662 0.03504036 0.08332678]
for model  18 the mean error 0.0653467885441536
all id 18 hidden_dim 16 learning_rate 0.0025 num_layers 5 frames 21 out win 4 err 0.0653467885441536 time 10476.967697381973
Launcher: Job 19 completed in 10751 seconds.
Launcher: Task 0 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  120401
Epoch:0, Train loss:0.784553, valid loss:0.780369
Epoch:1, Train loss:0.157267, valid loss:0.005356
Epoch:2, Train loss:0.015072, valid loss:0.004445
Epoch:3, Train loss:0.012979, valid loss:0.004306
Epoch:4, Train loss:0.011100, valid loss:0.003698
Epoch:5, Train loss:0.010537, valid loss:0.003647
Epoch:6, Train loss:0.009994, valid loss:0.003352
Epoch:7, Train loss:0.009624, valid loss:0.003250
Epoch:8, Train loss:0.007886, valid loss:0.002662
Epoch:9, Train loss:0.007622, valid loss:0.002672
Epoch:10, Train loss:0.006640, valid loss:0.002390
Epoch:11, Train loss:0.004086, valid loss:0.001415
Epoch:12, Train loss:0.002574, valid loss:0.001075
Epoch:13, Train loss:0.002156, valid loss:0.001226
Epoch:14, Train loss:0.002112, valid loss:0.001180
Epoch:15, Train loss:0.001971, valid loss:0.001005
Epoch:16, Train loss:0.001914, valid loss:0.001088
Epoch:17, Train loss:0.001857, valid loss:0.000981
Epoch:18, Train loss:0.001758, valid loss:0.001327
Epoch:19, Train loss:0.001707, valid loss:0.001108
Epoch:20, Train loss:0.001683, valid loss:0.000989
Epoch:21, Train loss:0.001274, valid loss:0.000845
Epoch:22, Train loss:0.001205, valid loss:0.000789
Epoch:23, Train loss:0.001228, valid loss:0.000917
Epoch:24, Train loss:0.001188, valid loss:0.000818
Epoch:25, Train loss:0.001187, valid loss:0.000883
Epoch:26, Train loss:0.001141, valid loss:0.000787
Epoch:27, Train loss:0.001116, valid loss:0.000777
Epoch:28, Train loss:0.001084, valid loss:0.000789
Epoch:29, Train loss:0.001102, valid loss:0.000888
Epoch:30, Train loss:0.001038, valid loss:0.000789
Epoch:31, Train loss:0.000887, valid loss:0.000768
Epoch:32, Train loss:0.000865, valid loss:0.000719
Epoch:33, Train loss:0.000839, valid loss:0.000716
Epoch:34, Train loss:0.000835, valid loss:0.000777
Epoch:35, Train loss:0.000845, valid loss:0.000749
Epoch:36, Train loss:0.000822, valid loss:0.000697
Epoch:37, Train loss:0.000802, valid loss:0.000686
Epoch:38, Train loss:0.000811, valid loss:0.000673
Epoch:39, Train loss:0.000788, valid loss:0.000639
Epoch:40, Train loss:0.000805, valid loss:0.000696
Epoch:41, Train loss:0.000699, valid loss:0.000645
Epoch:42, Train loss:0.000685, valid loss:0.000634
Epoch:43, Train loss:0.000683, valid loss:0.000644
Epoch:44, Train loss:0.000680, valid loss:0.000622
Epoch:45, Train loss:0.000672, valid loss:0.000632
Epoch:46, Train loss:0.000676, valid loss:0.000650
Epoch:47, Train loss:0.000672, valid loss:0.000641
Epoch:48, Train loss:0.000658, valid loss:0.000653
Epoch:49, Train loss:0.000656, valid loss:0.000629
Epoch:50, Train loss:0.000651, valid loss:0.000661
Epoch:51, Train loss:0.000618, valid loss:0.000626
Epoch:52, Train loss:0.000607, valid loss:0.000618
Epoch:53, Train loss:0.000603, valid loss:0.000619
Epoch:54, Train loss:0.000600, valid loss:0.000617
Epoch:55, Train loss:0.000599, valid loss:0.000621
Epoch:56, Train loss:0.000598, valid loss:0.000617
Epoch:57, Train loss:0.000597, valid loss:0.000620
Epoch:58, Train loss:0.000596, valid loss:0.000618
Epoch:59, Train loss:0.000596, valid loss:0.000618
Epoch:60, Train loss:0.000595, valid loss:0.000620
training time 10593.247476577759
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07189058119358233
plot_id,batch_id 0 1 miss% 0.04609481432847615
plot_id,batch_id 0 2 miss% 0.10905706736110639
plot_id,batch_id 0 3 miss% 0.05922842523436946
plot_id,batch_id 0 4 miss% 0.051855516543528365
plot_id,batch_id 0 5 miss% 0.11736376747692355
plot_id,batch_id 0 6 miss% 0.03220918960746356
plot_id,batch_id 0 7 miss% 0.10322000032008823
plot_id,batch_id 0 8 miss% 0.06499390360921592
plot_id,batch_id 0 9 miss% 0.10596301900278052
plot_id,batch_id 0 10 miss% 0.058838096073153864
plot_id,batch_id 0 11 miss% 0.10925077599232758
plot_id,batch_id 0 12 miss% 0.10802753149245298
plot_id,batch_id 0 13 miss% 0.02947939656827268
plot_id,batch_id 0 14 miss% 0.09236706583980053
plot_id,batch_id 0 15 miss% 0.02956496749237668
plot_id,batch_id 0 16 miss% 0.08594855977551795
plot_id,batch_id 0 17 miss% 0.077460325434445
plot_id,batch_id 0 18 miss% 0.05437950381037082
plot_id,batch_id 0 19 miss% 0.07852894200279874
plot_id,batch_id 0 20 miss% 0.0600737396725423
plot_id,batch_id 0 21 miss% 0.051590049043528825
plot_id,batch_id 0 22 miss% 0.03875178333776621
plot_id,batch_id 0 23 miss% 0.054729761788360407
plot_id,batch_id 0 24 miss% 0.03636505294379372
plot_id,batch_id 0 25 miss% 0.038398756015243445
plot_id,batch_id 0 26 miss% 0.06753507044057928
plot_id,batch_id 0 27 miss% 0.04972229511761072
plot_id,batch_id 0 28 miss% 0.021209631501261653
plot_id,batch_id 0 29 miss% 0.02715908506704388
plot_id,batch_id 0 30 miss% 0.03764126242695766
plot_id,batch_id 0 31 miss% 0.06586457962356003
plot_id,batch_id 0 32 miss% 0.09586971117679477
plot_id,batch_id 0 33 miss% 0.0388324034136254
plot_id,batch_id 0 34 miss% 0.03231663939188882
plot_id,batch_id 0 35 miss% 0.07003236612604459
plot_id,batch_id 0 36 miss% 0.06234768114862455
plot_id,batch_id 0 37 miss% 0.059228576079402344
plot_id,batch_id 0 38 miss% 0.05422198003453479
plot_id,batch_id 0 39 miss% 0.04494168220681569
plot_id,batch_id 0 40 miss% 0.06007937331615554
plot_id,batch_id 0 41 miss% 0.029073608852000093
plot_id,batch_id 0 42 miss% 0.013504555238794246
plot_id,batch_id 0 43 miss% 0.04393124043238764
plot_id,batch_id 0 44 miss% 0.01987689694729583
plot_id,batch_id 0 45 miss% 0.04820994190890463
plot_id,batch_id 0 46 miss% 0.04386186789918568
plot_id,batch_id 0 47 miss% 0.02956907025520893
plot_id,batch_id 0 48 miss% 0.023864187826341265
plot_id,batch_id 0 49 miss% 0.04668122565425742
plot_id,batch_id 0 50 miss% 0.1318367920134185
plot_id,batch_id 0 51 miss% 0.04923975476208484
plot_id,batch_id 0 52 miss% 0.023232921898819595
plot_id,batch_id 0 53 miss% 0.028442274680374472
plot_id,batch_id 0 54 miss% 0.019307804601860257
plot_id,batch_id 0 55 miss% 0.06148000288492566
plot_id,batch_id 0 56 miss% 0.05079484714801396
plot_id,batch_id 0 57 miss% 0.041971546743412774
plot_id,batch_id 0 58 miss% 0.03538002491446378
plot_id,batch_id 0 59 miss% 0.030411541532006356
plot_id,batch_id 0 60 miss% 0.03568386096827176
plot_id,batch_id 0 61 miss% 0.03381904706793133
plot_id,batch_id 0 62 miss% 0.07977834111523022
plot_id,batch_id 0 63 miss% 0.0746251001880853
plot_id,batch_id 0 64 miss% 0.06533997645884113
plot_id,batch_id 0 65 miss% 0.04863955062171287
plot_id,batch_id 0 66 miss% 0.026441621270151837
plot_id,batch_id 0 67 miss% 0.023395892290877594
plot_id,batch_id 0 68 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  69649
Epoch:0, Train loss:0.757601, valid loss:0.730829
Epoch:1, Train loss:0.073704, valid loss:0.012120
Epoch:2, Train loss:0.019595, valid loss:0.005317
Epoch:3, Train loss:0.013497, valid loss:0.004836
Epoch:4, Train loss:0.011712, valid loss:0.003504
Epoch:5, Train loss:0.009716, valid loss:0.003620
Epoch:6, Train loss:0.007999, valid loss:0.003150
Epoch:7, Train loss:0.006460, valid loss:0.002717
Epoch:8, Train loss:0.004946, valid loss:0.001975
Epoch:9, Train loss:0.004126, valid loss:0.001941
Epoch:10, Train loss:0.003737, valid loss:0.001857
Epoch:11, Train loss:0.002957, valid loss:0.001536
Epoch:12, Train loss:0.002812, valid loss:0.001436
Epoch:13, Train loss:0.002729, valid loss:0.001371
Epoch:14, Train loss:0.002579, valid loss:0.001369
Epoch:15, Train loss:0.002554, valid loss:0.001282
Epoch:16, Train loss:0.002446, valid loss:0.001505
Epoch:17, Train loss:0.002351, valid loss:0.001312
Epoch:18, Train loss:0.002257, valid loss:0.001283
Epoch:19, Train loss:0.002206, valid loss:0.001280
Epoch:20, Train loss:0.002163, valid loss:0.001228
Epoch:21, Train loss:0.001797, valid loss:0.001118
Epoch:22, Train loss:0.001759, valid loss:0.001040
Epoch:23, Train loss:0.001766, valid loss:0.001077
Epoch:24, Train loss:0.001716, valid loss:0.001068
Epoch:25, Train loss:0.001696, valid loss:0.000997
Epoch:26, Train loss:0.001671, valid loss:0.001075
Epoch:27, Train loss:0.001620, valid loss:0.001055
Epoch:28, Train loss:0.001645, valid loss:0.001019
Epoch:29, Train loss:0.001581, valid loss:0.000965
Epoch:30, Train loss:0.001529, valid loss:0.001034
Epoch:31, Train loss:0.001388, valid loss:0.000949
Epoch:32, Train loss:0.001357, valid loss:0.000903
Epoch:33, Train loss:0.001370, valid loss:0.000918
Epoch:34, Train loss:0.001337, valid loss:0.000932
Epoch:35, Train loss:0.001344, valid loss:0.000958
Epoch:36, Train loss:0.001314, valid loss:0.000904
Epoch:37, Train loss:0.001327, valid loss:0.000904
Epoch:38, Train loss:0.001301, valid loss:0.000967
Epoch:39, Train loss:0.001286, valid loss:0.000920
Epoch:40, Train loss:0.001286, valid loss:0.000899
Epoch:41, Train loss:0.001179, valid loss:0.000925
Epoch:42, Train loss:0.001178, valid loss:0.000873
Epoch:43, Train loss:0.001176, valid loss:0.000887
Epoch:44, Train loss:0.001180, valid loss:0.000882
Epoch:45, Train loss:0.001160, valid loss:0.000918
Epoch:46, Train loss:0.001156, valid loss:0.000888
Epoch:47, Train loss:0.001146, valid loss:0.000879
Epoch:48, Train loss:0.001149, valid loss:0.000868
Epoch:49, Train loss:0.001146, valid loss:0.000897
Epoch:50, Train loss:0.001140, valid loss:0.000941
Epoch:51, Train loss:0.001080, valid loss:0.000854
Epoch:52, Train loss:0.001072, valid loss:0.000865
Epoch:53, Train loss:0.001070, valid loss:0.000856
Epoch:54, Train loss:0.001069, valid loss:0.000848
Epoch:55, Train loss:0.001067, valid loss:0.000850
Epoch:56, Train loss:0.001066, valid loss:0.000863
Epoch:57, Train loss:0.001065, valid loss:0.000852
Epoch:58, Train loss:0.001066, valid loss:0.000851
Epoch:59, Train loss:0.001065, valid loss:0.000853
Epoch:60, Train loss:0.001065, valid loss:0.000856
training time 10632.731805086136
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.11321659559279565
plot_id,batch_id 0 1 miss% 0.0757073360564537
plot_id,batch_id 0 2 miss% 0.10887434678304195
plot_id,batch_id 0 3 miss% 0.04878782888080102
plot_id,batch_id 0 4 miss% 0.05816378613402678
plot_id,batch_id 0 5 miss% 0.07245183306102017
plot_id,batch_id 0 6 miss% 0.044823458592096715
plot_id,batch_id 0 7 miss% 0.09165037724391643
plot_id,batch_id 0 8 miss% 0.08679253192155997
plot_id,batch_id 0 9 miss% 0.04511803635073631
plot_id,batch_id 0 10 miss% 0.03787317269010055
plot_id,batch_id 0 11 miss% 0.05520215308629667
plot_id,batch_id 0 12 miss% 0.06910109943452654
plot_id,batch_id 0 13 miss% 0.06563377570972255
plot_id,batch_id 0 14 miss% 0.11367390846335111
plot_id,batch_id 0 15 miss% 0.05234785315145376
plot_id,batch_id 0 16 miss% 0.17393183420654404
plot_id,batch_id 0 17 miss% 0.035250601653201485
plot_id,batch_id 0 18 miss% 0.05277372662423647
plot_id,batch_id 0 19 miss% 0.15864725543482575
plot_id,batch_id 0 20 miss% 0.06600623973412814
plot_id,batch_id 0 21 miss% 0.0394674364978555
plot_id,batch_id 0 22 miss% 0.07268958824819104
plot_id,batch_id 0 23 miss% 0.04126084194749178
plot_id,batch_id 0 24 miss% 0.050526757287088296
plot_id,batch_id 0 25 miss% 0.04639691476195469
plot_id,batch_id 0 26 miss% 0.06478045629788956
plot_id,batch_id 0 27 miss% 0.050804957798720206
plot_id,batch_id 0 28 miss% 0.04075159581022062
plot_id,batch_id 0 29 miss% 0.026400457247858694
plot_id,batch_id 0 30 miss% 0.041942402960974054
plot_id,batch_id 0 31 miss% 0.09357700870640105
plot_id,batch_id 0 32 miss% 0.12420886793467263
plot_id,batch_id 0 33 miss% 0.07837033590691596
plot_id,batch_id 0 34 miss% 0.04599088904318133
plot_id,batch_id 0 35 miss% 0.07520676511627578
plot_id,batch_id 0 36 miss% 0.1913420264543919
plot_id,batch_id 0 37 miss% 0.08069960833795325
plot_id,batch_id 0 38 miss% 0.04745619339574808
plot_id,batch_id 0 39 miss% 0.035616323953121656
plot_id,batch_id 0 40 miss% 0.06675123779088353
plot_id,batch_id 0 41 miss% 0.030899819022182192
plot_id,batch_id 0 42 miss% 0.04826973139454151
plot_id,batch_id 0 43 miss% 0.05907363865187048
plot_id,batch_id 0 44 miss% 0.024928319793327654
plot_id,batch_id 0 45 miss% 0.06219639984854134
plot_id,batch_id 0 46 miss% 0.03658517745786419
plot_id,batch_id 0 47 miss% 0.04769479499372233
plot_id,batch_id 0 48 miss% 0.031512969314357916
plot_id,batch_id 0 49 miss% 0.061376063242167475
plot_id,batch_id 0 50 miss% 0.13866019037724436
plot_id,batch_id 0 51 miss% 0.041785897677860305
plot_id,batch_id 0 52 miss% 0.06149283502710276
plot_id,batch_id 0 53 miss% 0.061317436459466294
plot_id,batch_id 0 54 miss% 0.03203681607627284
plot_id,batch_id 0 55 miss% 0.10708377823159365
plot_id,batch_id 0 56 miss% 0.07970170366439419
plot_id,batch_id 0 57 miss% 0.0313307796037064
plot_id,batch_id 0 58 miss% 0.0478570347099855
plot_id,batch_id 0 59 miss% 0.04512776188333328
plot_id,batch_id 0 60 miss% 0.04354231976344621
plot_id,batch_id 0 61 miss% 0.05831786039758021
plot_id,batch_id 0 62 miss% 0.07494031544466176
plot_id,batch_id 0 63 miss% 0.03608122306531015
plot_id,batch_id 0 64 miss% 0.09716951538786574
plot_id,batch_id 0 65 miss% 0.10152041936260744
plot_id,batch_id 0 66 miss% 0.029547484296152716
plot_id,batch_id 0 67 miss% 0.04793750859475491
plot_id,batch_id 0 68 miss% 0.04309246774783103
plot_id,batch_id 0 0.06122716007845318
plot_id,batch_id 0 69 miss% 0.06729531848532086
plot_id,batch_id 0 70 miss% 0.04955506640112446
plot_id,batch_id 0 71 miss% 0.035237941953901265
plot_id,batch_id 0 72 miss% 0.08678758070916343
plot_id,batch_id 0 73 miss% 0.043259784257067696
plot_id,batch_id 0 74 miss% 0.06187822726499259
plot_id,batch_id 0 75 miss% 0.12472488115286687
plot_id,batch_id 0 76 miss% 0.14234875910722405
plot_id,batch_id 0 77 miss% 0.04916238854629214
plot_id,batch_id 0 78 miss% 0.060850468821336025
plot_id,batch_id 0 79 miss% 0.12563267025056252
plot_id,batch_id 0 80 miss% 0.05061271400786087
plot_id,batch_id 0 81 miss% 0.08831370834844977
plot_id,batch_id 0 82 miss% 0.04014534846363105
plot_id,batch_id 0 83 miss% 0.09385756115994952
plot_id,batch_id 0 84 miss% 0.04629236261723503
plot_id,batch_id 0 85 miss% 0.037235139397487245
plot_id,batch_id 0 86 miss% 0.05107086220481123
plot_id,batch_id 0 87 miss% 0.0675777884842209
plot_id,batch_id 0 88 miss% 0.07777401874666443
plot_id,batch_id 0 89 miss% 0.08389027449248979
plot_id,batch_id 0 90 miss% 0.04271694341123157
plot_id,batch_id 0 91 miss% 0.05118189144773583
plot_id,batch_id 0 92 miss% 0.03533597737303775
plot_id,batch_id 0 93 miss% 0.042983712094940785
plot_id,batch_id 0 94 miss% 0.07557094295000609
plot_id,batch_id 0 95 miss% 0.05006629509692183
plot_id,batch_id 0 96 miss% 0.04994125435521463
plot_id,batch_id 0 97 miss% 0.0518769637723251
plot_id,batch_id 0 98 miss% 0.022982663372180478
plot_id,batch_id 0 99 miss% 0.0701641566688916
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07189058 0.04609481 0.10905707 0.05922843 0.05185552 0.11736377
 0.03220919 0.10322    0.0649939  0.10596302 0.0588381  0.10925078
 0.10802753 0.0294794  0.09236707 0.02956497 0.08594856 0.07746033
 0.0543795  0.07852894 0.06007374 0.05159005 0.03875178 0.05472976
 0.03636505 0.03839876 0.06753507 0.0497223  0.02120963 0.02715909
 0.03764126 0.06586458 0.09586971 0.0388324  0.03231664 0.07003237
 0.06234768 0.05922858 0.05422198 0.04494168 0.06007937 0.02907361
 0.01350456 0.04393124 0.0198769  0.04820994 0.04386187 0.02956907
 0.02386419 0.04668123 0.13183679 0.04923975 0.02323292 0.02844227
 0.0193078  0.06148    0.05079485 0.04197155 0.03538002 0.03041154
 0.03568386 0.03381905 0.07977834 0.0746251  0.06533998 0.04863955
 0.02644162 0.02339589 0.06122716 0.06729532 0.04955507 0.03523794
 0.08678758 0.04325978 0.06187823 0.12472488 0.14234876 0.04916239
 0.06085047 0.12563267 0.05061271 0.08831371 0.04014535 0.09385756
 0.04629236 0.03723514 0.05107086 0.06757779 0.07777402 0.08389027
 0.04271694 0.05118189 0.03533598 0.04298371 0.07557094 0.0500663
 0.04994125 0.05187696 0.02298266 0.07016416]
for model  39 the mean error 0.057485772506996345
all id 39 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 21 out win 4 err 0.057485772506996345 time 10593.247476577759
Launcher: Job 40 completed in 10866 seconds.
Launcher: Task 158 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  154129
Epoch:0, Train loss:0.725999, valid loss:0.745131
Epoch:1, Train loss:0.098264, valid loss:0.005632
Epoch:2, Train loss:0.014396, valid loss:0.004946
Epoch:3, Train loss:0.012934, valid loss:0.004511
Epoch:4, Train loss:0.011375, valid loss:0.003346
Epoch:5, Train loss:0.008836, valid loss:0.002146
Epoch:6, Train loss:0.004080, valid loss:0.001937
Epoch:7, Train loss:0.003331, valid loss:0.001667
Epoch:8, Train loss:0.003177, valid loss:0.001776
Epoch:9, Train loss:0.002886, valid loss:0.001441
Epoch:10, Train loss:0.002660, valid loss:0.002018
Epoch:11, Train loss:0.001881, valid loss:0.001114
Epoch:12, Train loss:0.001762, valid loss:0.000918
Epoch:13, Train loss:0.001818, valid loss:0.001044
Epoch:14, Train loss:0.001770, valid loss:0.001080
Epoch:15, Train loss:0.001756, valid loss:0.001006
Epoch:16, Train loss:0.001628, valid loss:0.000980
Epoch:17, Train loss:0.001563, valid loss:0.001179
Epoch:18, Train loss:0.001549, valid loss:0.000989
Epoch:19, Train loss:0.001523, valid loss:0.000922
Epoch:20, Train loss:0.001428, valid loss:0.001201
Epoch:21, Train loss:0.001081, valid loss:0.000841
Epoch:22, Train loss:0.001028, valid loss:0.000894
Epoch:23, Train loss:0.001018, valid loss:0.000717
Epoch:24, Train loss:0.001012, valid loss:0.000776
Epoch:25, Train loss:0.000992, valid loss:0.000838
Epoch:26, Train loss:0.001014, valid loss:0.000739
Epoch:27, Train loss:0.000946, valid loss:0.000680
Epoch:28, Train loss:0.000918, valid loss:0.000734
Epoch:29, Train loss:0.000977, valid loss:0.000819
Epoch:30, Train loss:0.000913, valid loss:0.000703
Epoch:31, Train loss:0.000746, valid loss:0.000613
Epoch:32, Train loss:0.000721, valid loss:0.000684
Epoch:33, Train loss:0.000711, valid loss:0.000653
Epoch:34, Train loss:0.000703, valid loss:0.000643
Epoch:35, Train loss:0.000711, valid loss:0.000688
Epoch:36, Train loss:0.000706, valid loss:0.000724
Epoch:37, Train loss:0.000697, valid loss:0.000645
Epoch:38, Train loss:0.000677, valid loss:0.000686
Epoch:39, Train loss:0.000665, valid loss:0.000667
Epoch:40, Train loss:0.000686, valid loss:0.000661
Epoch:41, Train loss:0.000586, valid loss:0.000606
Epoch:42, Train loss:0.000580, valid loss:0.000612
Epoch:43, Train loss:0.000573, valid loss:0.000621
Epoch:44, Train loss:0.000570, valid loss:0.000631
Epoch:45, Train loss:0.000571, valid loss:0.000596
Epoch:46, Train loss:0.000565, valid loss:0.000598
Epoch:47, Train loss:0.000557, valid loss:0.000597
Epoch:48, Train loss:0.000560, valid loss:0.000614
Epoch:49, Train loss:0.000567, valid loss:0.000617
Epoch:50, Train loss:0.000555, valid loss:0.000626
Epoch:51, Train loss:0.000516, valid loss:0.000600
Epoch:52, Train loss:0.000510, valid loss:0.000592
Epoch:53, Train loss:0.000508, valid loss:0.000590
Epoch:54, Train loss:0.000506, valid loss:0.000588
Epoch:55, Train loss:0.000505, valid loss:0.000588
Epoch:56, Train loss:0.000504, valid loss:0.000586
Epoch:57, Train loss:0.000504, valid loss:0.000583
Epoch:58, Train loss:0.000503, valid loss:0.000580
Epoch:59, Train loss:0.000502, valid loss:0.000580
Epoch:60, Train loss:0.000502, valid loss:0.000581
training time 10670.992431402206
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07176855929697049
plot_id,batch_id 0 1 miss% 0.04327847551560981
plot_id,batch_id 0 2 miss% 0.08036189306739971
plot_id,batch_id 0 3 miss% 0.08288746581030365
plot_id,batch_id 0 4 miss% 0.027943582072170303
plot_id,batch_id 0 5 miss% 0.057418210574182026
plot_id,batch_id 0 6 miss% 0.04525878104910974
plot_id,batch_id 0 7 miss% 0.11351870663278116
plot_id,batch_id 0 8 miss% 0.06694444942147257
plot_id,batch_id 0 9 miss% 0.024539165425997746
plot_id,batch_id 0 10 miss% 0.033625079648465284
plot_id,batch_id 0 11 miss% 0.05396921619083199
plot_id,batch_id 0 12 miss% 0.05985994784219342
plot_id,batch_id 0 13 miss% 0.041211869291202255
plot_id,batch_id 0 14 miss% 0.09055859357025381
plot_id,batch_id 0 15 miss% 0.056299124295475275
plot_id,batch_id 0 16 miss% 0.14472032789466707
plot_id,batch_id 0 17 miss% 0.031244435607941157
plot_id,batch_id 0 18 miss% 0.0660406997183548
plot_id,batch_id 0 19 miss% 0.07546698829829708
plot_id,batch_id 0 20 miss% 0.04122230528435173
plot_id,batch_id 0 21 miss% 0.09284029429181455
plot_id,batch_id 0 22 miss% 0.035965026185674195
plot_id,batch_id 0 23 miss% 0.04337689655926689
plot_id,batch_id 0 24 miss% 0.051196972727063944
plot_id,batch_id 0 25 miss% 0.06321039226149615
plot_id,batch_id 0 26 miss% 0.03887805921864691
plot_id,batch_id 0 27 miss% 0.030817732903077093
plot_id,batch_id 0 28 miss% 0.05452805833607074
plot_id,batch_id 0 29 miss% 0.03197036453859817
plot_id,batch_id 0 30 miss% 0.03006512736835872
plot_id,batch_id 0 31 miss% 0.06999243951720686
plot_id,batch_id 0 32 miss% 0.11840737841395108
plot_id,batch_id 0 33 miss% 0.04459584411602276
plot_id,batch_id 0 34 miss% 0.03291946762733358
plot_id,batch_id 0 35 miss% 0.07756425568874481
plot_id,batch_id 0 36 miss% 0.055798228602053516
plot_id,batch_id 0 37 miss% 0.06979684241345886
plot_id,batch_id 0 38 miss% 0.027723394017389213
plot_id,batch_id 0 39 miss% 0.05093734159152269
plot_id,batch_id 0 40 miss% 0.09111073205092826
plot_id,batch_id 0 41 miss% 0.04278843626020738
plot_id,batch_id 0 42 miss% 0.0315628007382424
plot_id,batch_id 0 43 miss% 0.07639511457623274
plot_id,batch_id 0 44 miss% 0.03249372720068956
plot_id,batch_id 0 45 miss% 0.0427003828366906
plot_id,batch_id 0 46 miss% 0.030803702193144343
plot_id,batch_id 0 47 miss% 0.02630952306908393
plot_id,batch_id 0 48 miss% 0.027824676347010392
plot_id,batch_id 0 49 miss% 0.044618479547886856
plot_id,batch_id 0 50 miss% 0.12013199178872312
plot_id,batch_id 0 51 miss% 0.037872534357698454
plot_id,batch_id 0 52 miss% 0.04241729980210353
plot_id,batch_id 0 53 miss% 0.02776905549722682
plot_id,batch_id 0 54 miss% 0.031048366943293167
plot_id,batch_id 0 55 miss% 0.0723834912751601
plot_id,batch_id 0 56 miss% 0.09868894747592453
plot_id,batch_id 0 57 miss% 0.04347413004373058
plot_id,batch_id 0 58 miss% 0.04218420366054434
plot_id,batch_id 0 59 miss% 0.035586645494738514
plot_id,batch_id 0 60 miss% 0.02777684264116428
plot_id,batch_id 0 61 miss% 0.05565552664753612
plot_id,batch_id 0 62 miss% 0.05924229250834118
plot_id,batch_id 0 63 miss% 0.06598130014109475
plot_id,batch_id 0 64 miss% 0.05069467931760461
plot_id,batch_id 0 65 miss% 0.049144281782105344
plot_id,batch_id 0 66 miss% 0.04963579937235911
plot_id,batch_id 0 67 miss% 0.028344050863178562
plot_id,batch_id 0 68 miss% 69 miss% 0.09824441502626319
plot_id,batch_id 0 70 miss% 0.08880240746116114
plot_id,batch_id 0 71 miss% 0.03355107157607969
plot_id,batch_id 0 72 miss% 0.07223186577325857
plot_id,batch_id 0 73 miss% 0.047814375807996454
plot_id,batch_id 0 74 miss% 0.08288321080061196
plot_id,batch_id 0 75 miss% 0.07621341830133094
plot_id,batch_id 0 76 miss% 0.09541590384047481
plot_id,batch_id 0 77 miss% 0.07036325458316321
plot_id,batch_id 0 78 miss% 0.03672345099475719
plot_id,batch_id 0 79 miss% 0.08791525426793438
plot_id,batch_id 0 80 miss% 0.034740972370195876
plot_id,batch_id 0 81 miss% 0.07304480516096075
plot_id,batch_id 0 82 miss% 0.10812031662717306
plot_id,batch_id 0 83 miss% 0.08859549262208817
plot_id,batch_id 0 84 miss% 0.09551224545087496
plot_id,batch_id 0 85 miss% 0.020262436685244096
plot_id,batch_id 0 86 miss% 0.03864942104827929
plot_id,batch_id 0 87 miss% 0.14363451178445458
plot_id,batch_id 0 88 miss% 0.12089312162877026
plot_id,batch_id 0 89 miss% 0.07530240519028228
plot_id,batch_id 0 90 miss% 0.022540717782411204
plot_id,batch_id 0 91 miss% 0.06491966890314288
plot_id,batch_id 0 92 miss% 0.07449967541343194
plot_id,batch_id 0 93 miss% 0.04375862708084176
plot_id,batch_id 0 94 miss% 0.09012779303610458
plot_id,batch_id 0 95 miss% 0.0522763395272581
plot_id,batch_id 0 96 miss% 0.03481361384200604
plot_id,batch_id 0 97 miss% 0.06072634857322571
plot_id,batch_id 0 98 miss% 0.046829633113002735
plot_id,batch_id 0 99 miss% 0.062081018651222016
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.1132166  0.07570734 0.10887435 0.04878783 0.05816379 0.07245183
 0.04482346 0.09165038 0.08679253 0.04511804 0.03787317 0.05520215
 0.0691011  0.06563378 0.11367391 0.05234785 0.17393183 0.0352506
 0.05277373 0.15864726 0.06600624 0.03946744 0.07268959 0.04126084
 0.05052676 0.04639691 0.06478046 0.05080496 0.0407516  0.02640046
 0.0419424  0.09357701 0.12420887 0.07837034 0.04599089 0.07520677
 0.19134203 0.08069961 0.04745619 0.03561632 0.06675124 0.03089982
 0.04826973 0.05907364 0.02492832 0.0621964  0.03658518 0.04769479
 0.03151297 0.06137606 0.13866019 0.0417859  0.06149284 0.06131744
 0.03203682 0.10708378 0.0797017  0.03133078 0.04785703 0.04512776
 0.04354232 0.05831786 0.07494032 0.03608122 0.09716952 0.10152042
 0.02954748 0.04793751 0.04309247 0.09824442 0.08880241 0.03355107
 0.07223187 0.04781438 0.08288321 0.07621342 0.0954159  0.07036325
 0.03672345 0.08791525 0.03474097 0.07304481 0.10812032 0.08859549
 0.09551225 0.02026244 0.03864942 0.14363451 0.12089312 0.07530241
 0.02254072 0.06491967 0.07449968 0.04375863 0.09012779 0.05227634
 0.03481361 0.06072635 0.04682963 0.06208102]
for model  19 the mean error 0.06662836470716373
all id 19 hidden_dim 16 learning_rate 0.0025 num_layers 5 frames 21 out win 5 err 0.06662836470716373 time 10632.731805086136
Launcher: Job 20 completed in 10903 seconds.
Launcher: Task 251 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  89105
Epoch:0, Train loss:0.562425, valid loss:0.517456
Epoch:1, Train loss:0.089577, valid loss:0.003166
Epoch:2, Train loss:0.006110, valid loss:0.002422
Epoch:3, Train loss:0.004551, valid loss:0.002726
Epoch:4, Train loss:0.003904, valid loss:0.001900
Epoch:5, Train loss:0.003343, valid loss:0.001648
Epoch:6, Train loss:0.003028, valid loss:0.001223
Epoch:7, Train loss:0.002829, valid loss:0.001249
Epoch:8, Train loss:0.002603, valid loss:0.001076
Epoch:9, Train loss:0.002429, valid loss:0.001339
Epoch:10, Train loss:0.002327, valid loss:0.001339
Epoch:11, Train loss:0.001678, valid loss:0.000982
Epoch:12, Train loss:0.001617, valid loss:0.001120
Epoch:13, Train loss:0.001612, valid loss:0.000881
Epoch:14, Train loss:0.001557, valid loss:0.001214
Epoch:15, Train loss:0.001547, valid loss:0.000935
Epoch:16, Train loss:0.001497, valid loss:0.000810
Epoch:17, Train loss:0.001470, valid loss:0.000761
Epoch:18, Train loss:0.001401, valid loss:0.000871
Epoch:19, Train loss:0.001358, valid loss:0.000771
Epoch:20, Train loss:0.001326, valid loss:0.000769
Epoch:21, Train loss:0.001012, valid loss:0.000679
Epoch:22, Train loss:0.001002, valid loss:0.000638
Epoch:23, Train loss:0.000998, valid loss:0.000693
Epoch:24, Train loss:0.000996, valid loss:0.000681
Epoch:25, Train loss:0.000974, valid loss:0.000598
Epoch:26, Train loss:0.000944, valid loss:0.000609
Epoch:27, Train loss:0.000943, valid loss:0.000601
Epoch:28, Train loss:0.000930, valid loss:0.000612
Epoch:29, Train loss:0.000914, valid loss:0.000638
Epoch:30, Train loss:0.000912, valid loss:0.000632
Epoch:31, Train loss:0.000744, valid loss:0.000548
Epoch:32, Train loss:0.000728, valid loss:0.000556
Epoch:33, Train loss:0.000708, valid loss:0.000545
Epoch:34, Train loss:0.000714, valid loss:0.000587
Epoch:35, Train loss:0.000698, valid loss:0.000561
Epoch:36, Train loss:0.000708, valid loss:0.000555
Epoch:37, Train loss:0.000692, valid loss:0.000543
Epoch:38, Train loss:0.000692, valid loss:0.000545
Epoch:39, Train loss:0.000681, valid loss:0.000563
Epoch:40, Train loss:0.000681, valid loss:0.000537
Epoch:41, Train loss:0.000603, valid loss:0.000525
Epoch:42, Train loss:0.000590, valid loss:0.000525
Epoch:43, Train loss:0.000584, valid loss:0.000513
Epoch:44, Train loss:0.000584, valid loss:0.000520
Epoch:45, Train loss:0.000577, valid loss:0.000520
Epoch:46, Train loss:0.000580, valid loss:0.000518
Epoch:47, Train loss:0.000579, valid loss:0.000515
Epoch:48, Train loss:0.000585, valid loss:0.000506
Epoch:49, Train loss:0.000566, valid loss:0.000514
Epoch:50, Train loss:0.000572, valid loss:0.000505
Epoch:51, Train loss:0.000532, valid loss:0.000497
Epoch:52, Train loss:0.000529, valid loss:0.000498
Epoch:53, Train loss:0.000528, valid loss:0.000495
Epoch:54, Train loss:0.000527, valid loss:0.000495
Epoch:55, Train loss:0.000526, valid loss:0.000496
Epoch:56, Train loss:0.000525, valid loss:0.000494
Epoch:57, Train loss:0.000525, valid loss:0.000495
Epoch:58, Train loss:0.000524, valid loss:0.000494
Epoch:59, Train loss:0.000524, valid loss:0.000495
Epoch:60, Train loss:0.000523, valid loss:0.000493
training time 10729.625663518906
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.26325557100853264
plot_id,batch_id 0 1 miss% 0.33319732761043475
plot_id,batch_id 0 2 miss% 0.4116193974586442
plot_id,batch_id 0 3 miss% 0.31662172195350524
plot_id,batch_id 0 4 miss% 0.3088655913156431
plot_id,batch_id 0 5 miss% 0.3161882825921545
plot_id,batch_id 0 6 miss% 0.27443949018371294
plot_id,batch_id 0 7 miss% 0.3876754795815879
plot_id,batch_id 0 8 miss% 0.36507059284880494
plot_id,batch_id 0 9 miss% 0.31712422533592705
plot_id,batch_id 0 10 miss% 0.23484535844516824
plot_id,batch_id 0 11 miss% 0.3635753921456454
plot_id,batch_id 0 12 miss% 0.3436719508642994
plot_id,batch_id 0 13 miss% 0.31266379900361535
plot_id,batch_id 0 14 miss% 0.43995364125421904
plot_id,batch_id 0 15 miss% 0.2689035885197228
plot_id,batch_id 0 16 miss% 0.4426936358975186
plot_id,batch_id 0 17 miss% 0.42340628643625977
plot_id,batch_id 0 18 miss% 0.3994057190160034
plot_id,batch_id 0 19 miss% 0.36672102430380915
plot_id,batch_id 0 20 miss% 0.37775759085877864
plot_id,batch_id 0 21 miss% 0.41608994607482197
plot_id,batch_id 0 22 miss% 0.3588456919534125
plot_id,batch_id 0 23 miss% 0.2229507617072847
plot_id,batch_id 0 24 miss% 0.4572678654446961
plot_id,batch_id 0 25 miss% 0.3434249298371749
plot_id,batch_id 0 26 miss% 0.2645878333332179
plot_id,batch_id 0 27 miss% 0.3607264033619223
plot_id,batch_id 0 28 miss% 0.3915421440727763
plot_id,batch_id 0 29 miss% 0.3156457344552569
plot_id,batch_id 0 30 miss% 0.258093682237143
plot_id,batch_id 0 31 miss% 0.4790263060212293
plot_id,batch_id 0 32 miss% 0.376031368958488
plot_id,batch_id 0 33 miss% 0.35886631637055155
plot_id,batch_id 0 34 miss% 0.3942698395476431
plot_id,batch_id 0 35 miss% 0.31999623361437207
plot_id,batch_id 0 36 miss% 0.4550538077832054
plot_id,batch_id 0 37 miss% 0.33602877773742823
plot_id,batch_id 0 38 miss% 0.28219031286685325
plot_id,batch_id 0 39 miss% 0.3368497725890774
plot_id,batch_id 0 40 miss% 0.457069809233875
plot_id,batch_id 0 41 miss% 0.3206778746444942
plot_id,batch_id 0 42 miss% 0.39290562988819927
plot_id,batch_id 0 43 miss% 0.47654013731781864
plot_id,batch_id 0 44 miss% 0.387228985869152
plot_id,batch_id 0 45 miss% 0.30231488371458176
plot_id,batch_id 0 46 miss% 0.4786683674220761
plot_id,batch_id 0 47 miss% 0.2869032028344901
plot_id,batch_id 0 48 miss% 0.3234605240240225
plot_id,batch_id 0 49 miss% 0.5103156871101833
plot_id,batch_id 0 50 miss% 0.4745175628273679
plot_id,batch_id 0 51 miss% 0.37485723257334563
plot_id,batch_id 0 52 miss% 0.4690301585532897
plot_id,batch_id 0 53 miss% 0.43879734305418505
plot_id,batch_id 0 54 miss% 0.3859530460671717
plot_id,batch_id 0 55 miss% 0.4803909817614548
plot_id,batch_id 0 56 miss% 0.4798370764569881
plot_id,batch_id 0 57 miss% 0.3416432773762471
plot_id,batch_id 0 58 miss% 0.4152333099381892
plot_id,batch_id 0 59 miss% 0.35903467240684755
plot_id,batch_id 0 60 miss% 0.16639528702755574
plot_id,batch_id 0 61 miss% 0.21878729285268514
plot_id,batch_id 0 62 miss% 0.24946701218647818
plot_id,batch_id 0 63 miss% 0.32114379316577324
plot_id,batch_id 0 64 miss% 0.27297539712818947
plot_id,batch_id 0 65 miss% 0.2996596366004103
plot_id,batch_id 0 66 miss% 0.30966672149794067
plot_id,batch_id 0 67 miss% 0.23868130324524015
plot_id,batch_id 00.045874287329446824
plot_id,batch_id 0 69 miss% 0.07402090527381482
plot_id,batch_id 0 70 miss% 0.07228829454244554
plot_id,batch_id 0 71 miss% 0.029524212743443443
plot_id,batch_id 0 72 miss% 0.07603739276534247
plot_id,batch_id 0 73 miss% 0.04751408317378971
plot_id,batch_id 0 74 miss% 0.08468077060423211
plot_id,batch_id 0 75 miss% 0.069683157074491
plot_id,batch_id 0 76 miss% 0.07831204867739495
plot_id,batch_id 0 77 miss% 0.06624763377938195
plot_id,batch_id 0 78 miss% 0.06550223684963286
plot_id,batch_id 0 79 miss% 0.09302884103877114
plot_id,batch_id 0 80 miss% 0.038515940505029954
plot_id,batch_id 0 81 miss% 0.08204403878588067
plot_id,batch_id 0 82 miss% 0.04666609850311069
plot_id,batch_id 0 83 miss% 0.0676331375398535
plot_id,batch_id 0 84 miss% 0.07289016194353874
plot_id,batch_id 0 85 miss% 0.022860194516143472
plot_id,batch_id 0 86 miss% 0.047688004339911785
plot_id,batch_id 0 87 miss% 0.07900301098582588
plot_id,batch_id 0 88 miss% 0.10990760744644584
plot_id,batch_id 0 89 miss% 0.08505076355227123
plot_id,batch_id 0 90 miss% 0.02115408669769595
plot_id,batch_id 0 91 miss% 0.03460199217262298
plot_id,batch_id 0 92 miss% 0.058942370905309155
plot_id,batch_id 0 93 miss% 0.055815298339894404
plot_id,batch_id 0 94 miss% 0.0680430672515852
plot_id,batch_id 0 95 miss% 0.058861183837886605
plot_id,batch_id 0 96 miss% 0.06488579036225212
plot_id,batch_id 0 97 miss% 0.053437140879439536
plot_id,batch_id 0 98 miss% 0.032721933724906775
plot_id,batch_id 0 99 miss% 0.043930607132768
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07176856 0.04327848 0.08036189 0.08288747 0.02794358 0.05741821
 0.04525878 0.11351871 0.06694445 0.02453917 0.03362508 0.05396922
 0.05985995 0.04121187 0.09055859 0.05629912 0.14472033 0.03124444
 0.0660407  0.07546699 0.04122231 0.09284029 0.03596503 0.0433769
 0.05119697 0.06321039 0.03887806 0.03081773 0.05452806 0.03197036
 0.03006513 0.06999244 0.11840738 0.04459584 0.03291947 0.07756426
 0.05579823 0.06979684 0.02772339 0.05093734 0.09111073 0.04278844
 0.0315628  0.07639511 0.03249373 0.04270038 0.0308037  0.02630952
 0.02782468 0.04461848 0.12013199 0.03787253 0.0424173  0.02776906
 0.03104837 0.07238349 0.09868895 0.04347413 0.0421842  0.03558665
 0.02777684 0.05565553 0.05924229 0.0659813  0.05069468 0.04914428
 0.0496358  0.02834405 0.04587429 0.07402091 0.07228829 0.02952421
 0.07603739 0.04751408 0.08468077 0.06968316 0.07831205 0.06624763
 0.06550224 0.09302884 0.03851594 0.08204404 0.0466661  0.06763314
 0.07289016 0.02286019 0.047688   0.07900301 0.10990761 0.08505076
 0.02115409 0.03460199 0.05894237 0.0558153  0.06804307 0.05886118
 0.06488579 0.05343714 0.03272193 0.04393061]
for model  33 the mean error 0.05660727300592984
all id 33 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 21 out win 4 err 0.05660727300592984 time 10670.992431402206
Launcher: Job 34 completed in 10941 seconds.
Launcher: Task 175 done. Exiting.
 68 miss% 0.36805077605783376
plot_id,batch_id 0 69 miss% 0.32968818810259065
plot_id,batch_id 0 70 miss% 0.18174559802034515
plot_id,batch_id 0 71 miss% 0.262367477241829
plot_id,batch_id 0 72 miss% 0.3649682681886974
plot_id,batch_id 0 73 miss% 0.32489772233872666
plot_id,batch_id 0 74 miss% 0.27303440383831246
plot_id,batch_id 0 75 miss% 0.20981478990411323
plot_id,batch_id 0 76 miss% 0.3262531514053765
plot_id,batch_id 0 77 miss% 0.24899954076050898
plot_id,batch_id 0 78 miss% 0.26607871195340066
plot_id,batch_id 0 79 miss% 0.27518807305434717
plot_id,batch_id 0 80 miss% 0.18771327798183451
plot_id,batch_id 0 81 miss% 0.3218612668101989
plot_id,batch_id 0 82 miss% 0.31015863273336486
plot_id,batch_id 0 83 miss% 0.32791804696705806
plot_id,batch_id 0 84 miss% 0.382094845765264
plot_id,batch_id 0 85 miss% 0.20057473499773382
plot_id,batch_id 0 86 miss% 0.31909380543373367
plot_id,batch_id 0 87 miss% 0.366612657915359
plot_id,batch_id 0 88 miss% 0.3274680398135045
plot_id,batch_id 0 89 miss% 0.3667743064750594
plot_id,batch_id 0 90 miss% 0.16596783180094032
plot_id,batch_id 0 91 miss% 0.2627914770295865
plot_id,batch_id 0 92 miss% 0.3304700832221502
plot_id,batch_id 0 93 miss% 0.2225973569259908
plot_id,batch_id 0 94 miss% 0.37459485301599166
plot_id,batch_id 0 95 miss% 0.19149027099415353
plot_id,batch_id 0 96 miss% 0.24412480075140217
plot_id,batch_id 0 97 miss% 0.3740785440371364
plot_id,batch_id 0 98 miss% 0.3770882024630534
plot_id,batch_id 0 99 miss% 0.3557770304610227
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.26325557 0.33319733 0.4116194  0.31662172 0.30886559 0.31618828
 0.27443949 0.38767548 0.36507059 0.31712423 0.23484536 0.36357539
 0.34367195 0.3126638  0.43995364 0.26890359 0.44269364 0.42340629
 0.39940572 0.36672102 0.37775759 0.41608995 0.35884569 0.22295076
 0.45726787 0.34342493 0.26458783 0.3607264  0.39154214 0.31564573
 0.25809368 0.47902631 0.37603137 0.35886632 0.39426984 0.31999623
 0.45505381 0.33602878 0.28219031 0.33684977 0.45706981 0.32067787
 0.39290563 0.47654014 0.38722899 0.30231488 0.47866837 0.2869032
 0.32346052 0.51031569 0.47451756 0.37485723 0.46903016 0.43879734
 0.38595305 0.48039098 0.47983708 0.34164328 0.41523331 0.35903467
 0.16639529 0.21878729 0.24946701 0.32114379 0.2729754  0.29965964
 0.30966672 0.2386813  0.36805078 0.32968819 0.1817456  0.26236748
 0.36496827 0.32489772 0.2730344  0.20981479 0.32625315 0.24899954
 0.26607871 0.27518807 0.18771328 0.32186127 0.31015863 0.32791805
 0.38209485 0.20057473 0.31909381 0.36661266 0.32746804 0.36677431
 0.16596783 0.26279148 0.33047008 0.22259736 0.37459485 0.19149027
 0.2441248  0.37407854 0.3770882  0.35577703]
for model  138 the mean error 0.33667636367839415
all id 138 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 25 out win 4 err 0.33667636367839415 time 10729.625663518906
Launcher: Job 139 completed in 10977 seconds.
Launcher: Task 218 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  120401
Epoch:0, Train loss:0.784553, valid loss:0.780369
Epoch:1, Train loss:0.140621, valid loss:0.006992
Epoch:2, Train loss:0.016707, valid loss:0.005712
Epoch:3, Train loss:0.014257, valid loss:0.004802
Epoch:4, Train loss:0.012773, valid loss:0.004323
Epoch:5, Train loss:0.012119, valid loss:0.004474
Epoch:6, Train loss:0.011669, valid loss:0.003723
Epoch:7, Train loss:0.007722, valid loss:0.002934
Epoch:8, Train loss:0.004429, valid loss:0.001711
Epoch:9, Train loss:0.003368, valid loss:0.002260
Epoch:10, Train loss:0.003121, valid loss:0.001397
Epoch:11, Train loss:0.002369, valid loss:0.001233
Epoch:12, Train loss:0.002230, valid loss:0.001151
Epoch:13, Train loss:0.002144, valid loss:0.001211
Epoch:14, Train loss:0.002065, valid loss:0.001205
Epoch:15, Train loss:0.002036, valid loss:0.001131
Epoch:16, Train loss:0.001953, valid loss:0.001309
Epoch:17, Train loss:0.001834, valid loss:0.001143
Epoch:18, Train loss:0.001783, valid loss:0.001863
Epoch:19, Train loss:0.001748, valid loss:0.001256
Epoch:20, Train loss:0.001713, valid loss:0.001037
Epoch:21, Train loss:0.001328, valid loss:0.000900
Epoch:22, Train loss:0.001282, valid loss:0.000868
Epoch:23, Train loss:0.001284, valid loss:0.000943
Epoch:24, Train loss:0.001265, valid loss:0.000845
Epoch:25, Train loss:0.001263, valid loss:0.000884
Epoch:26, Train loss:0.001201, valid loss:0.000858
Epoch:27, Train loss:0.001212, valid loss:0.000803
Epoch:28, Train loss:0.001165, valid loss:0.000761
Epoch:29, Train loss:0.001174, valid loss:0.000920
Epoch:30, Train loss:0.001142, valid loss:0.000785
Epoch:31, Train loss:0.000983, valid loss:0.000733
Epoch:32, Train loss:0.000957, valid loss:0.000696
Epoch:33, Train loss:0.000954, valid loss:0.000726
Epoch:34, Train loss:0.000936, valid loss:0.000755
Epoch:35, Train loss:0.000939, valid loss:0.000782
Epoch:36, Train loss:0.000917, valid loss:0.000694
Epoch:37, Train loss:0.000912, valid loss:0.000721
Epoch:38, Train loss:0.000911, valid loss:0.000708
Epoch:39, Train loss:0.000896, valid loss:0.000673
Epoch:40, Train loss:0.000884, valid loss:0.000707
Epoch:41, Train loss:0.000807, valid loss:0.000654
Epoch:42, Train loss:0.000797, valid loss:0.000665
Epoch:43, Train loss:0.000786, valid loss:0.000671
Epoch:44, Train loss:0.000785, valid loss:0.000652
Epoch:45, Train loss:0.000778, valid loss:0.000636
Epoch:46, Train loss:0.000775, valid loss:0.000650
Epoch:47, Train loss:0.000774, valid loss:0.000671
Epoch:48, Train loss:0.000777, valid loss:0.000685
Epoch:49, Train loss:0.000763, valid loss:0.000634
Epoch:50, Train loss:0.000764, valid loss:0.000659
Epoch:51, Train loss:0.000718, valid loss:0.000634
Epoch:52, Train loss:0.000709, valid loss:0.000632
Epoch:53, Train loss:0.000705, valid loss:0.000629
Epoch:54, Train loss:0.000704, valid loss:0.000627
Epoch:55, Train loss:0.000703, valid loss:0.000630
Epoch:56, Train loss:0.000702, valid loss:0.000629
Epoch:57, Train loss:0.000701, valid loss:0.000631
Epoch:58, Train loss:0.000701, valid loss:0.000631
Epoch:59, Train loss:0.000701, valid loss:0.000630
Epoch:60, Train loss:0.000700, valid loss:0.000629
training time 10821.928189277649
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07468779720212743
plot_id,batch_id 0 1 miss% 0.05712396517256474
plot_id,batch_id 0 2 miss% 0.10032171326425901
plot_id,batch_id 0 3 miss% 0.07473762010491486
plot_id,batch_id 0 4 miss% 0.04799418758276049
plot_id,batch_id 0 5 miss% 0.072008710931259
plot_id,batch_id 0 6 miss% 0.04627687623057172
plot_id,batch_id 0 7 miss% 0.0975078999032943
plot_id,batch_id 0 8 miss% 0.08727962611347705
plot_id,batch_id 0 9 miss% 0.052015624163341705
plot_id,batch_id 0 10 miss% 0.024602492023567374
plot_id,batch_id 0 11 miss% 0.06666926488397239
plot_id,batch_id 0 12 miss% 0.07382645450710548
plot_id,batch_id 0 13 miss% 0.04433952896301811
plot_id,batch_id 0 14 miss% 0.06589968215317098
plot_id,batch_id 0 15 miss% 0.025290469336538397
plot_id,batch_id 0 16 miss% 0.19062372936448824
plot_id,batch_id 0 17 miss% 0.054705048530898186
plot_id,batch_id 0 18 miss% 0.07566857489276839
plot_id,batch_id 0 19 miss% 0.10379350626393065
plot_id,batch_id 0 20 miss% 0.058543095696276276
plot_id,batch_id 0 21 miss% 0.05580918227722839
plot_id,batch_id 0 22 miss% 0.042850442974163745
plot_id,batch_id 0 23 miss% 0.041994448199400826
plot_id,batch_id 0 24 miss% 0.04393041758193869
plot_id,batch_id 0 25 miss% 0.044371512907484506
plot_id,batch_id 0 26 miss% 0.04421876747465017
plot_id,batch_id 0 27 miss% 0.06529044841100771
plot_id,batch_id 0 28 miss% 0.029636938143063098
plot_id,batch_id 0 29 miss% 0.04313160791936811
plot_id,batch_id 0 30 miss% 0.045330032793526466
plot_id,batch_id 0 31 miss% 0.05389870173340925
plot_id,batch_id 0 32 miss% 0.0926377776429195
plot_id,batch_id 0 33 miss% 0.05623901928732988
plot_id,batch_id 0 34 miss% 0.040095876275334404
plot_id,batch_id 0 35 miss% 0.08494649339159978
plot_id,batch_id 0 36 miss% 0.10848847330726044
plot_id,batch_id 0 37 miss% 0.060893025594233215
plot_id,batch_id 0 38 miss% 0.040258622121721065
plot_id,batch_id 0 39 miss% 0.049869028897988776
plot_id,batch_id 0 40 miss% 0.19908744302681114
plot_id,batch_id 0 41 miss% 0.06842062437999832
plot_id,batch_id 0 42 miss% 0.026774801354059664
plot_id,batch_id 0 43 miss% 0.10301773040869026
plot_id,batch_id 0 44 miss% 0.030478664926032857
plot_id,batch_id 0 45 miss% 0.04653634895149167
plot_id,batch_id 0 46 miss% 0.030325341107602405
plot_id,batch_id 0 47 miss% 0.03414841132857998
plot_id,batch_id 0 48 miss% 0.027510685866128103
plot_id,batch_id 0 49 miss% 0.03133337907460726
plot_id,batch_id 0 50 miss% 0.09861783335926001
plot_id,batch_id 0 51 miss% 0.03241088286280375
plot_id,batch_id 0 52 miss% 0.027732048914044686
plot_id,batch_id 0 53 miss% 0.017199514130104322
plot_id,batch_id 0 54 miss% 0.04288164325611793
plot_id,batch_id 0 55 miss% 0.05120642449994289
plot_id,batch_id 0 56 miss% 0.08200594291223944
plot_id,batch_id 0 57 miss% 0.031235374262123732
plot_id,batch_id 0 58 miss% 0.037649470276737436
plot_id,batch_id 0 59 miss% 0.02949079548562282
plot_id,batch_id 0 60 miss% 0.046834517213997236
plot_id,batch_id 0 61 miss% 0.038441947122176905
plot_id,batch_id 0 62 miss% 0.07329875790883597
plot_id,batch_id 0 63 miss% 0.04572928106824483
plot_id,batch_id 0 64 miss% 0.06973515032437876
plot_id,batch_id 0 65 miss% 0.034749688918604645
plot_id,batch_id 0 66 miss% 0.02903646790544183
plot_id,batch_id 0 67 miss% 0.04921011709902141
plot_id,batch_id 0 68 miss% 0.047209763086030004
plot_id,batch_id 0 69 miss% 0.09646129194456668
plot_id,batch_id 0 70 miss% 0.02879564162060588
plot_id,batch_id 0 71 miss% 0.02861046075297213
plot_id,batch_id 0 72 miss% 0.07828072947291309
plot_id,batch_id 0 73 miss% 0.06072083800711523
plot_id,batch_id 0 74 miss% 0.05150126022551059
plot_id,batch_id 0 75 miss% 0.028553146347480192
plot_id,batch_id 0 76 miss% 0.09244379262511238
plot_id,batch_id 0 77 miss% 0.024934277964499672
plot_id,batch_id 0 78 miss% 0.059166517828612646
plot_id,batch_id 0 79 miss% 0.16067460751171486
plot_id,batch_id 0 80 miss% 0.0813711779277289
plot_id,batch_id 0 81 miss% 0.09184315622817667
plot_id,batch_id 0 82 miss% 0.04470029040359808
plot_id,batch_id 0 83 miss% 0.08800284733028152
plot_id,batch_id 0 84 miss% 0.06742753257542752
plot_id,batch_id 0 85 miss% 0.05840647387539629
plot_id,batch_id 0 86 miss% 0.0546472762113903
plot_id,batch_id 0 87 miss% 0.036703212011468145
plot_id,batch_id 0 88 miss% 0.09024085031209488
plot_id,batch_id 0 89 miss% 0.07385215238918848
plot_id,batch_id 0 90 miss% 0.02319316155038162
plot_id,batch_id 0 91 miss% 0.05553485918312092
plot_id,batch_id 0 92 miss% 0.09062117864284838
plot_id,batch_id 0 93 miss% 0.07506717042265328
plot_id,batch_id 0 94 miss% 0.05677905794177795
plot_id,batch_id 0 95 miss% 0.04352456124009915
plot_id,batch_id 0 96 miss% 0.060480178278120564
plot_id,batch_id 0 97 miss% 0.04146182732029701
plot_id,batch_id 0 98 miss% 0.04780835222570802
plot_id,batch_id 0 99 miss% 0.03771436492937056
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0746878  0.05712397 0.10032171 0.07473762 0.04799419 0.07200871
 0.04627688 0.0975079  0.08727963 0.05201562 0.02460249 0.06666926
 0.07382645 0.04433953 0.06589968 0.02529047 0.19062373 0.05470505
 0.07566857 0.10379351 0.0585431  0.05580918 0.04285044 0.04199445
 0.04393042 0.04437151 0.04421877 0.06529045 0.02963694 0.04313161
 0.04533003 0.0538987  0.09263778 0.05623902 0.04009588 0.08494649
 0.10848847 0.06089303 0.04025862 0.04986903 0.19908744 0.06842062
 0.0267748  0.10301773 0.03047866 0.04653635 0.03032534 0.03414841
 0.02751069 0.03133338 0.09861783 0.03241088 0.02773205 0.01719951
 0.04288164 0.05120642 0.08200594 0.03123537 0.03764947 0.0294908
 0.04683452 0.03844195 0.07329876 0.04572928 0.06973515 0.03474969
 0.02903647 0.04921012 0.04720976 0.09646129 0.02879564 0.02861046
 0.07828073 0.06072084 0.05150126 0.02855315 0.09244379 0.02493428
 0.05916652 0.16067461 0.08137118 0.09184316 0.04470029 0.08800285
 0.06742753 0.05840647 0.05464728 0.03670321 0.09024085 0.07385215
 0.02319316 0.05553486 0.09062118 0.07506717 0.05677906 0.04352456
 0.06048018 0.04146183 0.04780835 0.03771436]
for model  12 the mean error 0.05949637980547894
all id 12 hidden_dim 24 learning_rate 0.0025 num_layers 4 frames 21 out win 4 err 0.05949637980547894 time 10821.928189277649
Launcher: Job 13 completed in 11094 seconds.
Launcher: Task 172 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  55697
Epoch:0, Train loss:0.608634, valid loss:0.572475
Epoch:1, Train loss:0.203230, valid loss:0.013904
Epoch:2, Train loss:0.021019, valid loss:0.004312
Epoch:3, Train loss:0.009498, valid loss:0.003272
Epoch:4, Train loss:0.007768, valid loss:0.002427
Epoch:5, Train loss:0.005742, valid loss:0.002259
Epoch:6, Train loss:0.005059, valid loss:0.002137
Epoch:7, Train loss:0.004776, valid loss:0.002119
Epoch:8, Train loss:0.004559, valid loss:0.002037
Epoch:9, Train loss:0.004305, valid loss:0.002126
Epoch:10, Train loss:0.003962, valid loss:0.001664
Epoch:11, Train loss:0.003237, valid loss:0.001542
Epoch:12, Train loss:0.003071, valid loss:0.001826
Epoch:13, Train loss:0.002937, valid loss:0.001437
Epoch:14, Train loss:0.002866, valid loss:0.001314
Epoch:15, Train loss:0.002729, valid loss:0.001235
Epoch:16, Train loss:0.002668, valid loss:0.001300
Epoch:17, Train loss:0.002520, valid loss:0.001271
Epoch:18, Train loss:0.002479, valid loss:0.001168
Epoch:19, Train loss:0.002376, valid loss:0.001163
Epoch:20, Train loss:0.002325, valid loss:0.001231
Epoch:21, Train loss:0.001979, valid loss:0.000957
Epoch:22, Train loss:0.001943, valid loss:0.000962
Epoch:23, Train loss:0.001876, valid loss:0.001018
Epoch:24, Train loss:0.001878, valid loss:0.000981
Epoch:25, Train loss:0.001796, valid loss:0.001018
Epoch:26, Train loss:0.001807, valid loss:0.000996
Epoch:27, Train loss:0.001736, valid loss:0.000891
Epoch:28, Train loss:0.001717, valid loss:0.000976
Epoch:29, Train loss:0.001681, valid loss:0.001002
Epoch:30, Train loss:0.001645, valid loss:0.000920
Epoch:31, Train loss:0.001447, valid loss:0.000885
Epoch:32, Train loss:0.001427, valid loss:0.000873
Epoch:33, Train loss:0.001404, valid loss:0.000929
Epoch:34, Train loss:0.001413, valid loss:0.000867
Epoch:35, Train loss:0.001380, valid loss:0.000836
Epoch:36, Train loss:0.001364, valid loss:0.000907
Epoch:37, Train loss:0.001352, valid loss:0.000865
Epoch:38, Train loss:0.001333, valid loss:0.000801
Epoch:39, Train loss:0.001317, valid loss:0.000806
Epoch:40, Train loss:0.001316, valid loss:0.000832
Epoch:41, Train loss:0.001202, valid loss:0.000786
Epoch:42, Train loss:0.001192, valid loss:0.000780
Epoch:43, Train loss:0.001189, valid loss:0.000782
Epoch:44, Train loss:0.001183, valid loss:0.000802
Epoch:45, Train loss:0.001169, valid loss:0.000769
Epoch:46, Train loss:0.001166, valid loss:0.000800
Epoch:47, Train loss:0.001154, valid loss:0.000789
Epoch:48, Train loss:0.001151, valid loss:0.000765
Epoch:49, Train loss:0.001143, valid loss:0.000784
Epoch:50, Train loss:0.001141, valid loss:0.000804
Epoch:51, Train loss:0.001085, valid loss:0.000756
Epoch:52, Train loss:0.001070, valid loss:0.000752
Epoch:53, Train loss:0.001065, valid loss:0.000754
Epoch:54, Train loss:0.001062, valid loss:0.000748
Epoch:55, Train loss:0.001060, valid loss:0.000750
Epoch:56, Train loss:0.001059, valid loss:0.000749
Epoch:57, Train loss:0.001058, valid loss:0.000752
Epoch:58, Train loss:0.001057, valid loss:0.000749
Epoch:59, Train loss:0.001056, valid loss:0.000750
Epoch:60, Train loss:0.001055, valid loss:0.000748
training time 11023.278412103653
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.1368496287099983
plot_id,batch_id 0 1 miss% 0.04324548882564574
plot_id,batch_id 0 2 miss% 0.08159087616049125
plot_id,batch_id 0 3 miss% 0.0585078657299896
plot_id,batch_id 0 4 miss% 0.08783117560485437
plot_id,batch_id 0 5 miss% 0.06743076756997528
plot_id,batch_id 0 6 miss% 0.07041782613006095
plot_id,batch_id 0 7 miss% 0.07014827239435222
plot_id,batch_id 0 8 miss% 0.08810274754880626
plot_id,batch_id 0 9 miss% 0.04650553997262602
plot_id,batch_id 0 10 miss% 0.07395580216319601
plot_id,batch_id 0 11 miss% 0.10986909942151317
plot_id,batch_id 0 12 miss% 0.09477721916716313
plot_id,batch_id 0 13 miss% 0.08861138491149531
plot_id,batch_id 0 14 miss% 0.09876589460744563
plot_id,batch_id 0 15 miss% 0.10186683662946616
plot_id,batch_id 0 16 miss% 0.18650068880243217
plot_id,batch_id 0 17 miss% 0.14551709149656245
plot_id,batch_id 0 18 miss% 0.07130463272264745
plot_id,batch_id 0 19 miss% 0.10479584815884388
plot_id,batch_id 0 20 miss% 0.111581660112039
plot_id,batch_id 0 21 miss% 0.05477309977371969
plot_id,batch_id 0 22 miss% 0.0567281329939305
plot_id,batch_id 0 23 miss% 0.05367700925498055
plot_id,batch_id 0 24 miss% 0.05569788944245058
plot_id,batch_id 0 25 miss% 0.06812168281254397
plot_id,batch_id 0 26 miss% 0.09438339293834369
plot_id,batch_id 0 27 miss% 0.05715396340830135
plot_id,batch_id 0 28 miss% 0.05807548909559664
plot_id,batch_id 0 29 miss% 0.03292615046181324
plot_id,batch_id 0 30 miss% 0.13941076977610015
plot_id,batch_id 0 31 miss% 0.16352907846362333
plot_id,batch_id 0 32 miss% 0.10932783857494319
plot_id,batch_id 0 33 miss% 0.06871305287671871
plot_id,batch_id 0 34 miss% 0.06568651176754219
plot_id,batch_id 0 35 miss% 0.12398293078203283
plot_id,batch_id 0 36 miss% 0.087124494187569
plot_id,batch_id 0 37 miss% 0.1457902745100772
plot_id,batch_id 0 38 miss% 0.06459845862906517
plot_id,batch_id 0 39 miss% 0.07369453948863278
plot_id,batch_id 0 40 miss% 0.13439372570927813
plot_id,batch_id 0 41 miss% 0.03445958648826589
plot_id,batch_id 0 42 miss% 0.05744829140970361
plot_id,batch_id 0 43 miss% 0.15989039543077754
plot_id,batch_id 0 44 miss% 0.09532167197399595
plot_id,batch_id 0 45 miss% 0.07639734720172722
plot_id,batch_id 0 46 miss% 0.054872362069342595
plot_id,batch_id 0 47 miss% 0.040808669066276636
plot_id,batch_id 0 48 miss% 0.039516256370350895
plot_id,batch_id 0 49 miss% 0.035699619858956595
plot_id,batch_id 0 50 miss% 0.1969873105054965
plot_id,batch_id 0 51 miss% 0.11615607610554582
plot_id,batch_id 0 52 miss% 0.0370763256328931
plot_id,batch_id 0 53 miss% 0.05988258778116332
plot_id,batch_id 0 54 miss% 0.08071493651644183
plot_id,batch_id 0 55 miss% 0.13158783541165933
plot_id,batch_id 0 56 miss% 0.09976109678329473
plot_id,batch_id 0 57 miss% 0.05954707963592701
plot_id,batch_id 0 58 miss% 0.05178804287947168
plot_id,batch_id 0 59 miss% 0.053108499977307426
plot_id,batch_id 0 60 miss% 0.08627452633556867
plot_id,batch_id 0 61 miss% 0.07696536021384191
plot_id,batch_id 0 62 miss% 0.10530986840783503
plot_id,batch_id 0 63 miss% 0.058941690168849134
plot_id,batch_id 0 64 miss% 0.07622457936262111
plot_id,batch_id 0 65 miss% 0.10415019187875692
plot_id,batch_id 0 66 miss% 0.10008984516471432
plot_id,batch_id 0 67 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  41745
Epoch:0, Train loss:0.476787, valid loss:0.466052
Epoch:1, Train loss:0.017823, valid loss:0.002107
Epoch:2, Train loss:0.003850, valid loss:0.001391
Epoch:3, Train loss:0.003009, valid loss:0.001219
Epoch:4, Train loss:0.002734, valid loss:0.001152
Epoch:5, Train loss:0.002484, valid loss:0.001190
Epoch:6, Train loss:0.002274, valid loss:0.001097
Epoch:7, Train loss:0.002088, valid loss:0.000900
Epoch:8, Train loss:0.001959, valid loss:0.000984
Epoch:9, Train loss:0.001867, valid loss:0.000910
Epoch:10, Train loss:0.001754, valid loss:0.000909
Epoch:11, Train loss:0.001366, valid loss:0.000709
Epoch:12, Train loss:0.001315, valid loss:0.000658
Epoch:13, Train loss:0.001267, valid loss:0.000771
Epoch:14, Train loss:0.001261, valid loss:0.000666
Epoch:15, Train loss:0.001222, valid loss:0.000712
Epoch:16, Train loss:0.001184, valid loss:0.000679
Epoch:17, Train loss:0.001160, valid loss:0.000696
Epoch:18, Train loss:0.001126, valid loss:0.000601
Epoch:19, Train loss:0.001100, valid loss:0.000718
Epoch:20, Train loss:0.001096, valid loss:0.000634
Epoch:21, Train loss:0.000892, valid loss:0.000574
Epoch:22, Train loss:0.000890, valid loss:0.000545
Epoch:23, Train loss:0.000845, valid loss:0.000566
Epoch:24, Train loss:0.000867, valid loss:0.000548
Epoch:25, Train loss:0.000843, valid loss:0.000549
Epoch:26, Train loss:0.000850, valid loss:0.000534
Epoch:27, Train loss:0.000826, valid loss:0.000646
Epoch:28, Train loss:0.000831, valid loss:0.000543
Epoch:29, Train loss:0.000816, valid loss:0.000544
Epoch:30, Train loss:0.000790, valid loss:0.000518
Epoch:31, Train loss:0.000696, valid loss:0.000562
Epoch:32, Train loss:0.000681, valid loss:0.000515
Epoch:33, Train loss:0.000679, valid loss:0.000497
Epoch:34, Train loss:0.000669, valid loss:0.000503
Epoch:35, Train loss:0.000662, valid loss:0.000491
Epoch:36, Train loss:0.000669, valid loss:0.000496
Epoch:37, Train loss:0.000659, valid loss:0.000466
Epoch:38, Train loss:0.000656, valid loss:0.000514
Epoch:39, Train loss:0.000660, valid loss:0.000513
Epoch:40, Train loss:0.000650, valid loss:0.000535
Epoch:41, Train loss:0.000597, valid loss:0.000488
Epoch:42, Train loss:0.000589, valid loss:0.000475
Epoch:43, Train loss:0.000579, valid loss:0.000484
Epoch:44, Train loss:0.000582, valid loss:0.000501
Epoch:45, Train loss:0.000583, valid loss:0.000488
Epoch:46, Train loss:0.000576, valid loss:0.000470
Epoch:47, Train loss:0.000572, valid loss:0.000471
Epoch:48, Train loss:0.000572, valid loss:0.000473
Epoch:49, Train loss:0.000573, valid loss:0.000477
Epoch:50, Train loss:0.000569, valid loss:0.000466
Epoch:51, Train loss:0.000537, valid loss:0.000465
Epoch:52, Train loss:0.000534, valid loss:0.000464
Epoch:53, Train loss:0.000533, valid loss:0.000466
Epoch:54, Train loss:0.000532, valid loss:0.000462
Epoch:55, Train loss:0.000532, valid loss:0.000463
Epoch:56, Train loss:0.000531, valid loss:0.000461
Epoch:57, Train loss:0.000531, valid loss:0.000462
Epoch:58, Train loss:0.000530, valid loss:0.000462
Epoch:59, Train loss:0.000530, valid loss:0.000464
Epoch:60, Train loss:0.000529, valid loss:0.000461
training time 11071.097497701645
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.27026014788533176
plot_id,batch_id 0 1 miss% 0.33540024569063037
plot_id,batch_id 0 2 miss% 0.4340274042021663
plot_id,batch_id 0 3 miss% 0.3574549970419509
plot_id,batch_id 0 4 miss% 0.2586679631631634
plot_id,batch_id 0 5 miss% 0.32311201923866123
plot_id,batch_id 0 6 miss% 0.33172596871580645
plot_id,batch_id 0 7 miss% 0.44694805184171527
plot_id,batch_id 0 8 miss% 0.5454476361747583
plot_id,batch_id 0 9 miss% 0.48422552586281714
plot_id,batch_id 0 10 miss% 0.27510737856714307
plot_id,batch_id 0 11 miss% 0.2967059871222973
plot_id,batch_id 0 12 miss% 0.40907290996614387
plot_id,batch_id 0 13 miss% 0.3409119444485679
plot_id,batch_id 0 14 miss% 0.396471246378786
plot_id,batch_id 0 15 miss% 0.2446705005988827
plot_id,batch_id 0 16 miss% 0.49943176887452173
plot_id,batch_id 0 17 miss% 0.5067164789334355
plot_id,batch_id 0 18 miss% 0.4287640253567227
plot_id,batch_id 0 19 miss% 0.4067351676256794
plot_id,batch_id 0 20 miss% 0.5220790090163153
plot_id,batch_id 0 21 miss% 0.5235067182870775
plot_id,batch_id 0 22 miss% 0.37682539065134507
plot_id,batch_id 0 23 miss% 0.5317043727329174
plot_id,batch_id 0 24 miss% 0.3801286497572801
plot_id,batch_id 0 25 miss% 0.3437247056461906
plot_id,batch_id 0 26 miss% 0.2875048805976252
plot_id,batch_id 0 27 miss% 0.33962345740795064
plot_id,batch_id 0 28 miss% 0.448195622548147
plot_id,batch_id 0 29 miss% 0.3153825840024743
plot_id,batch_id 0 30 miss% 0.23957061275701297
plot_id,batch_id 0 31 miss% 0.48775222746518193
plot_id,batch_id 0 32 miss% 0.5172990033637077
plot_id,batch_id 0 33 miss% 0.44844694302444643
plot_id,batch_id 0 34 miss% 0.2942994364798107
plot_id,batch_id 0 35 miss% 0.39092449114225614
plot_id,batch_id 0 36 miss% 0.49213198711124156
plot_id,batch_id 0 37 miss% 0.38308473280647015
plot_id,batch_id 0 38 miss% 0.49054494406475985
plot_id,batch_id 0 39 miss% 0.5636879681144735
plot_id,batch_id 0 40 miss% 0.32807547558762623
plot_id,batch_id 0 41 miss% 0.29609578333859615
plot_id,batch_id 0 42 miss% 0.3272909114663542
plot_id,batch_id 0 43 miss% 0.25806827818040473
plot_id,batch_id 0 44 miss% 0.3836018196955588
plot_id,batch_id 0 45 miss% 0.47086629241841405
plot_id,batch_id 0 46 miss% 0.3373743602065268
plot_id,batch_id 0 47 miss% 0.29284518776452934
plot_id,batch_id 0 48 miss% 0.5840391313948029
plot_id,batch_id 0 49 miss% 0.261691911870613
plot_id,batch_id 0 50 miss% 0.4145412883320233
plot_id,batch_id 0 51 miss% 0.4008840708187985
plot_id,batch_id 0 52 miss% 0.35638853178699514
plot_id,batch_id 0 53 miss% 0.4066601830454919
plot_id,batch_id 0 54 miss% 0.42004240977713425
plot_id,batch_id 0 55 miss% 0.4514143622037728
plot_id,batch_id 0 56 miss% 0.3659687467715142
plot_id,batch_id 0 57 miss% 0.673919562717637
plot_id,batch_id 0 58 miss% 0.39955891111686714
plot_id,batch_id 0 59 miss% 0.391018092249035
plot_id,batch_id 0 60 miss% 0.20832975115856178
plot_id,batch_id 0 61 miss% 0.28225710845613133
plot_id,batch_id 0 62 miss% 0.2860897220512475
plot_id,batch_id 0 63 miss% 0.44239828072272064
plot_id,batch_id 0 64 miss% 0.3510008488430824
plot_id,batch_id 0 65 miss% 0.29748805495874436
plot_id,batch_id 0 66 miss% 0.3615752515587622
0.05079547906806644
plot_id,batch_id 0 68 miss% 0.040832221352630996
plot_id,batch_id 0 69 miss% 0.0963444476536836
plot_id,batch_id 0 70 miss% 0.18217929021228657
plot_id,batch_id 0 71 miss% 0.08516532153438743
plot_id,batch_id 0 72 miss% 0.24306547534006057
plot_id,batch_id 0 73 miss% 0.04820784957129282
plot_id,batch_id 0 74 miss% 0.12661305394881164
plot_id,batch_id 0 75 miss% 0.04182099061066729
plot_id,batch_id 0 76 miss% 0.13591299269883467
plot_id,batch_id 0 77 miss% 0.0857500914218245
plot_id,batch_id 0 78 miss% 0.07240543515043008
plot_id,batch_id 0 79 miss% 0.10335385664865149
plot_id,batch_id 0 80 miss% 0.04644578299398144
plot_id,batch_id 0 81 miss% 0.1428135031095234
plot_id,batch_id 0 82 miss% 0.12113172685176239
plot_id,batch_id 0 83 miss% 0.08831851623822577
plot_id,batch_id 0 84 miss% 0.09315357939326714
plot_id,batch_id 0 85 miss% 0.06860586533304372
plot_id,batch_id 0 86 miss% 0.06795503013648252
plot_id,batch_id 0 87 miss% 0.10726450034819981
plot_id,batch_id 0 88 miss% 0.1207851980869883
plot_id,batch_id 0 89 miss% 0.051480672392469305
plot_id,batch_id 0 90 miss% 0.06200087210415416
plot_id,batch_id 0 91 miss% 0.10084515344736077
plot_id,batch_id 0 92 miss% 0.13423290390053833
plot_id,batch_id 0 93 miss% 0.06299317941284824
plot_id,batch_id 0 94 miss% 0.04172505621207572
plot_id,batch_id 0 95 miss% 0.13385940421843792
plot_id,batch_id 0 96 miss% 0.06277228089416526
plot_id,batch_id 0 97 miss% 0.04400806425519667
plot_id,batch_id 0 98 miss% 0.06039611953330284
plot_id,batch_id 0 99 miss% 0.1417344032300077
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.13684963 0.04324549 0.08159088 0.05850787 0.08783118 0.06743077
 0.07041783 0.07014827 0.08810275 0.04650554 0.0739558  0.1098691
 0.09477722 0.08861138 0.09876589 0.10186684 0.18650069 0.14551709
 0.07130463 0.10479585 0.11158166 0.0547731  0.05672813 0.05367701
 0.05569789 0.06812168 0.09438339 0.05715396 0.05807549 0.03292615
 0.13941077 0.16352908 0.10932784 0.06871305 0.06568651 0.12398293
 0.08712449 0.14579027 0.06459846 0.07369454 0.13439373 0.03445959
 0.05744829 0.1598904  0.09532167 0.07639735 0.05487236 0.04080867
 0.03951626 0.03569962 0.19698731 0.11615608 0.03707633 0.05988259
 0.08071494 0.13158784 0.0997611  0.05954708 0.05178804 0.0531085
 0.08627453 0.07696536 0.10530987 0.05894169 0.07622458 0.10415019
 0.10008985 0.05079548 0.04083222 0.09634445 0.18217929 0.08516532
 0.24306548 0.04820785 0.12661305 0.04182099 0.13591299 0.08575009
 0.07240544 0.10335386 0.04644578 0.1428135  0.12113173 0.08831852
 0.09315358 0.06860587 0.06795503 0.1072645  0.1207852  0.05148067
 0.06200087 0.10084515 0.1342329  0.06299318 0.04172506 0.1338594
 0.06277228 0.04400806 0.06039612 0.1417344 ]
for model  144 the mean error 0.0879991320172131
all id 144 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 25 out win 4 err 0.0879991320172131 time 11023.278412103653
Launcher: Job 145 completed in 11289 seconds.
Launcher: Task 25 done. Exiting.
plot_id,batch_id 0 67 miss% 0.2343661392837798
plot_id,batch_id 0 68 miss% 0.40059457688476924
plot_id,batch_id 0 69 miss% 0.3843655541668347
plot_id,batch_id 0 70 miss% 0.23002938834805878
plot_id,batch_id 0 71 miss% 0.35390141558097554
plot_id,batch_id 0 72 miss% 0.43341592217904196
plot_id,batch_id 0 73 miss% 0.4489973880403113
plot_id,batch_id 0 74 miss% 0.3306528605178441
plot_id,batch_id 0 75 miss% 0.3048817468646466
plot_id,batch_id 0 76 miss% 0.3377616014257521
plot_id,batch_id 0 77 miss% 0.264739945576837
plot_id,batch_id 0 78 miss% 0.3210103549277103
plot_id,batch_id 0 79 miss% 0.3373235888678395
plot_id,batch_id 0 80 miss% 0.25734121770501983
plot_id,batch_id 0 81 miss% 0.38198818939595586
plot_id,batch_id 0 82 miss% 0.35056353332169116
plot_id,batch_id 0 83 miss% 0.3752538904544743
plot_id,batch_id 0 84 miss% 0.49803317797701013
plot_id,batch_id 0 85 miss% 0.21266992829764275
plot_id,batch_id 0 86 miss% 0.3257042852335252
plot_id,batch_id 0 87 miss% 0.3665196694759317
plot_id,batch_id 0 88 miss% 0.3779545503116621
plot_id,batch_id 0 89 miss% 0.40148531274502847
plot_id,batch_id 0 90 miss% 0.22040558283650602
plot_id,batch_id 0 91 miss% 0.30117790523522214
plot_id,batch_id 0 92 miss% 0.31310437703818556
plot_id,batch_id 0 93 miss% 0.31600213479023775
plot_id,batch_id 0 94 miss% 0.3947440189657443
plot_id,batch_id 0 95 miss% 0.3431713892134599
plot_id,batch_id 0 96 miss% 0.31545190556147173
plot_id,batch_id 0 97 miss% 0.441693514992092
plot_id,batch_id 0 98 miss% 0.3670526002880316
plot_id,batch_id 0 99 miss% 0.3554569066352077
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.27026015 0.33540025 0.4340274  0.357455   0.25866796 0.32311202
 0.33172597 0.44694805 0.54544764 0.48422553 0.27510738 0.29670599
 0.40907291 0.34091194 0.39647125 0.2446705  0.49943177 0.50671648
 0.42876403 0.40673517 0.52207901 0.52350672 0.37682539 0.53170437
 0.38012865 0.34372471 0.28750488 0.33962346 0.44819562 0.31538258
 0.23957061 0.48775223 0.517299   0.44844694 0.29429944 0.39092449
 0.49213199 0.38308473 0.49054494 0.56368797 0.32807548 0.29609578
 0.32729091 0.25806828 0.38360182 0.47086629 0.33737436 0.29284519
 0.58403913 0.26169191 0.41454129 0.40088407 0.35638853 0.40666018
 0.42004241 0.45141436 0.36596875 0.67391956 0.39955891 0.39101809
 0.20832975 0.28225711 0.28608972 0.44239828 0.35100085 0.29748805
 0.36157525 0.23436614 0.40059458 0.38436555 0.23002939 0.35390142
 0.43341592 0.44899739 0.33065286 0.30488175 0.3377616  0.26473995
 0.32101035 0.33732359 0.25734122 0.38198819 0.35056353 0.37525389
 0.49803318 0.21266993 0.32570429 0.36651967 0.37795455 0.40148531
 0.22040558 0.30117791 0.31310438 0.31600213 0.39474402 0.34317139
 0.31545191 0.44169351 0.3670526  0.35545691]
for model  217 the mean error 0.3731557400629631
all id 217 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 31 out win 5 err 0.3731557400629631 time 11071.097497701645
Launcher: Job 218 completed in 11314 seconds.
Launcher: Task 221 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  120401
Epoch:0, Train loss:0.757522, valid loss:0.743895
Epoch:1, Train loss:0.212076, valid loss:0.006745
Epoch:2, Train loss:0.012467, valid loss:0.003991
Epoch:3, Train loss:0.008967, valid loss:0.003216
Epoch:4, Train loss:0.007613, valid loss:0.002971
Epoch:5, Train loss:0.006175, valid loss:0.002929
Epoch:6, Train loss:0.005384, valid loss:0.002386
Epoch:7, Train loss:0.004926, valid loss:0.002344
Epoch:8, Train loss:0.004484, valid loss:0.001945
Epoch:9, Train loss:0.004173, valid loss:0.001848
Epoch:10, Train loss:0.003946, valid loss:0.001789
Epoch:11, Train loss:0.003023, valid loss:0.001660
Epoch:12, Train loss:0.002900, valid loss:0.001524
Epoch:13, Train loss:0.002743, valid loss:0.001567
Epoch:14, Train loss:0.002652, valid loss:0.001676
Epoch:15, Train loss:0.002493, valid loss:0.001440
Epoch:16, Train loss:0.002370, valid loss:0.001284
Epoch:17, Train loss:0.002334, valid loss:0.001297
Epoch:18, Train loss:0.002157, valid loss:0.001207
Epoch:19, Train loss:0.002111, valid loss:0.001245
Epoch:20, Train loss:0.001988, valid loss:0.001486
Epoch:21, Train loss:0.001702, valid loss:0.001047
Epoch:22, Train loss:0.001627, valid loss:0.001144
Epoch:23, Train loss:0.001575, valid loss:0.001035
Epoch:24, Train loss:0.001558, valid loss:0.001080
Epoch:25, Train loss:0.001492, valid loss:0.001089
Epoch:26, Train loss:0.001491, valid loss:0.000965
Epoch:27, Train loss:0.001441, valid loss:0.001016
Epoch:28, Train loss:0.001396, valid loss:0.001019
Epoch:29, Train loss:0.001411, valid loss:0.001036
Epoch:30, Train loss:0.001352, valid loss:0.000955
Epoch:31, Train loss:0.001195, valid loss:0.000928
Epoch:32, Train loss:0.001166, valid loss:0.000909
Epoch:33, Train loss:0.001190, valid loss:0.000953
Epoch:34, Train loss:0.001134, valid loss:0.000895
Epoch:35, Train loss:0.001110, valid loss:0.000855
Epoch:36, Train loss:0.001103, valid loss:0.000930
Epoch:37, Train loss:0.001083, valid loss:0.000877
Epoch:38, Train loss:0.001090, valid loss:0.000862
Epoch:39, Train loss:0.001061, valid loss:0.000908
Epoch:40, Train loss:0.001060, valid loss:0.000842
Epoch:41, Train loss:0.000965, valid loss:0.000828
Epoch:42, Train loss:0.000958, valid loss:0.000814
Epoch:43, Train loss:0.000947, valid loss:0.000848
Epoch:44, Train loss:0.000950, valid loss:0.000803
Epoch:45, Train loss:0.000939, valid loss:0.000825
Epoch:46, Train loss:0.000931, valid loss:0.000821
Epoch:47, Train loss:0.000923, valid loss:0.000830
Epoch:48, Train loss:0.000917, valid loss:0.000814
Epoch:49, Train loss:0.000915, valid loss:0.000816
Epoch:50, Train loss:0.000909, valid loss:0.000852
Epoch:51, Train loss:0.000861, valid loss:0.000795
Epoch:52, Train loss:0.000847, valid loss:0.000787
Epoch:53, Train loss:0.000844, valid loss:0.000784
Epoch:54, Train loss:0.000841, valid loss:0.000789
Epoch:55, Train loss:0.000841, valid loss:0.000786
Epoch:56, Train loss:0.000840, valid loss:0.000792
Epoch:57, Train loss:0.000839, valid loss:0.000782
Epoch:58, Train loss:0.000839, valid loss:0.000802
Epoch:59, Train loss:0.000838, valid loss:0.000782
Epoch:60, Train loss:0.000838, valid loss:0.000783
training time 11139.098724842072
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.09433683588296363
plot_id,batch_id 0 1 miss% 0.051997662015587406
plot_id,batch_id 0 2 miss% 0.07025225752764198
plot_id,batch_id 0 3 miss% 0.06744982435453648
plot_id,batch_id 0 4 miss% 0.04916062269498404
plot_id,batch_id 0 5 miss% 0.09450271273828473
plot_id,batch_id 0 6 miss% 0.06359260719099842
plot_id,batch_id 0 7 miss% 0.09707508986901713
plot_id,batch_id 0 8 miss% 0.05937103560315729
plot_id,batch_id 0 9 miss% 0.05871601426107399
plot_id,batch_id 0 10 miss% 0.04314726110445625
plot_id,batch_id 0 11 miss% 0.06225990234950886
plot_id,batch_id 0 12 miss% 0.05978724983621047
plot_id,batch_id 0 13 miss% 0.05971421713945164
plot_id,batch_id 0 14 miss% 0.07374482522610079
plot_id,batch_id 0 15 miss% 0.051273505652065315
plot_id,batch_id 0 16 miss% 0.15535336171206798
plot_id,batch_id 0 17 miss% 0.06984193870336398
plot_id,batch_id 0 18 miss% 0.09370379460714398
plot_id,batch_id 0 19 miss% 0.05298791442990741
plot_id,batch_id 0 20 miss% 0.08416125960387476
plot_id,batch_id 0 21 miss% 0.08725225478414143
plot_id,batch_id 0 22 miss% 0.045819293603481914
plot_id,batch_id 0 23 miss% 0.0361678317237653
plot_id,batch_id 0 24 miss% 0.04752113425276004
plot_id,batch_id 0 25 miss% 0.07906492356242345
plot_id,batch_id 0 26 miss% 0.04191470928332901
plot_id,batch_id 0 27 miss% 0.04605837067337451
plot_id,batch_id 0 28 miss% 0.04432538955105077
plot_id,batch_id 0 29 miss% 0.048361069480115364
plot_id,batch_id 0 30 miss% 0.014298396160615338
plot_id,batch_id 0 31 miss% 0.08751302683075629
plot_id,batch_id 0 32 miss% 0.14737621979571047
plot_id,batch_id 0 33 miss% 0.05431788219632245
plot_id,batch_id 0 34 miss% 0.06742503384517386
plot_id,batch_id 0 35 miss% 0.09425076389742536
plot_id,batch_id 0 36 miss% 0.08991031227776002
plot_id,batch_id 0 37 miss% 0.09007014210536
plot_id,batch_id 0 38 miss% 0.02462607706536435
plot_id,batch_id 0 39 miss% 0.029259082594922138
plot_id,batch_id 0 40 miss% 0.06047264077919771
plot_id,batch_id 0 41 miss% 0.02591636188019456
plot_id,batch_id 0 42 miss% 0.02971676210561405
plot_id,batch_id 0 43 miss% 0.05707302479494045
plot_id,batch_id 0 44 miss% 0.04020665824345071
plot_id,batch_id 0 45 miss% 0.05055511790429305
plot_id,batch_id 0 46 miss% 0.03519347332761794
plot_id,batch_id 0 47 miss% 0.04101310348733274
plot_id,batch_id 0 48 miss% 0.03752579690859454
plot_id,batch_id 0 49 miss% 0.02910388021405629
plot_id,batch_id 0 50 miss% 0.17799296398083683
plot_id,batch_id 0 51 miss% 0.06960586624724499
plot_id,batch_id 0 52 miss% 0.03429471689683326
plot_id,batch_id 0 53 miss% 0.04461773190138678
plot_id,batch_id 0 54 miss% 0.060167124783217674
plot_id,batch_id 0 55 miss% 0.10985432786673209
plot_id,batch_id 0 56 miss% 0.048477407045168217
plot_id,batch_id 0 57 miss% 0.026443467563070273
plot_id,batch_id 0 58 miss% 0.02565064709491878
plot_id,batch_id 0 59 miss% 0.023712622419314263
plot_id,batch_id 0 60 miss% 0.05991774041074439
plot_id,batch_id 0 61 miss% 0.0632188772777601
plot_id,batch_id 0 62 miss% 0.11330830679711035
plot_id,batch_id 0 63 miss% 0.06779622283386966
plot_id,batch_id 0 64 miss% 0.0686577540115199
plot_id,batch_id 0 65 miss% 0.06024733641230654
plot_id,batch_id 0 66 miss% 0.0426387303215037
plot_id,batch_id 0 67 miss% 0.047442003360194517
plot_id,batch_id 0 68 miss% 0.03473921071975067
plot_id,batch_id 0 69 miss% 0.061810946329022995
plot_id,batch_id 0 70 miss% 0.05690036757515254
plot_id,batch_id 0 71 miss% 0.06928323467465389
plot_id,batch_id 0 72 miss% 0.09035227108701091
plot_id,batch_id 0 73 miss% 0.12544421545130544
plot_id,batch_id 0 74 miss% 0.1695437348129465
plot_id,batch_id 0 75 miss% 0.0662036097022172
plot_id,batch_id 0 76 miss% 0.10974477295092847
plot_id,batch_id 0 77 miss% 0.07593223822314656
plot_id,batch_id 0 78 miss% 0.041948072783404136
plot_id,batch_id 0 79 miss% 0.02557931441598003
plot_id,batch_id 0 80 miss% 0.027570847105086203
plot_id,batch_id 0 81 miss% 0.10641564768323795
plot_id,batch_id 0 82 miss% 0.07293231841991005
plot_id,batch_id 0 83 miss% 0.07506683902482306
plot_id,batch_id 0 84 miss% 0.08450740097316795
plot_id,batch_id 0 85 miss% 0.034935116044718544
plot_id,batch_id 0 86 miss% 0.05676278351129512
plot_id,batch_id 0 87 miss% 0.08554225066093671
plot_id,batch_id 0 88 miss% 0.09199453743252688
plot_id,batch_id 0 89 miss% 0.07006754235785952
plot_id,batch_id 0 90 miss% 0.041378681509978046
plot_id,batch_id 0 91 miss% 0.09870777405297172
plot_id,batch_id 0 92 miss% 0.05713750285467771
plot_id,batch_id 0 93 miss% 0.03822829951806253
plot_id,batch_id 0 94 miss% 0.12208072856566161
plot_id,batch_id 0 95 miss% 0.055724689628222766
plot_id,batch_id 0 96 miss% 0.09518479966764921
plot_id,batch_id 0 97 miss% 0.08342967899190315
plot_id,batch_id 0 98 miss% 0.04953992078710455
plot_id,batch_id 0 99 miss% 0.09155543944007664
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09433684 0.05199766 0.07025226 0.06744982 0.04916062 0.09450271
 0.06359261 0.09707509 0.05937104 0.05871601 0.04314726 0.0622599
 0.05978725 0.05971422 0.07374483 0.05127351 0.15535336 0.06984194
 0.09370379 0.05298791 0.08416126 0.08725225 0.04581929 0.03616783
 0.04752113 0.07906492 0.04191471 0.04605837 0.04432539 0.04836107
 0.0142984  0.08751303 0.14737622 0.05431788 0.06742503 0.09425076
 0.08991031 0.09007014 0.02462608 0.02925908 0.06047264 0.02591636
 0.02971676 0.05707302 0.04020666 0.05055512 0.03519347 0.0410131
 0.0375258  0.02910388 0.17799296 0.06960587 0.03429472 0.04461773
 0.06016712 0.10985433 0.04847741 0.02644347 0.02565065 0.02371262
 0.05991774 0.06321888 0.11330831 0.06779622 0.06865775 0.06024734
 0.04263873 0.047442   0.03473921 0.06181095 0.05690037 0.06928323
 0.09035227 0.12544422 0.16954373 0.06620361 0.10974477 0.07593224
 0.04194807 0.02557931 0.02757085 0.10641565 0.07293232 0.07506684
 0.0845074  0.03493512 0.05676278 0.08554225 0.09199454 0.07006754
 0.04137868 0.09870777 0.0571375  0.0382283  0.12208073 0.05572469
 0.0951848  0.08342968 0.04953992 0.09155544]
for model  13 the mean error 0.06605027260010662
all id 13 hidden_dim 24 learning_rate 0.0025 num_layers 4 frames 21 out win 5 err 0.06605027260010662 time 11139.098724842072
Launcher: Job 14 completed in 11415 seconds.
Launcher: Task 149 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  55697
Epoch:0, Train loss:0.608634, valid loss:0.572475
Epoch:1, Train loss:0.137855, valid loss:0.004908
Epoch:2, Train loss:0.012884, valid loss:0.004268
Epoch:3, Train loss:0.010434, valid loss:0.003308
Epoch:4, Train loss:0.007607, valid loss:0.002490
Epoch:5, Train loss:0.005932, valid loss:0.002441
Epoch:6, Train loss:0.004957, valid loss:0.001821
Epoch:7, Train loss:0.004279, valid loss:0.001568
Epoch:8, Train loss:0.002838, valid loss:0.001584
Epoch:9, Train loss:0.002554, valid loss:0.001267
Epoch:10, Train loss:0.002370, valid loss:0.001384
Epoch:11, Train loss:0.001916, valid loss:0.000923
Epoch:12, Train loss:0.001823, valid loss:0.000984
Epoch:13, Train loss:0.001792, valid loss:0.000921
Epoch:14, Train loss:0.001736, valid loss:0.000882
Epoch:15, Train loss:0.001689, valid loss:0.000926
Epoch:16, Train loss:0.001656, valid loss:0.000870
Epoch:17, Train loss:0.001570, valid loss:0.000838
Epoch:18, Train loss:0.001540, valid loss:0.000804
Epoch:19, Train loss:0.001495, valid loss:0.000827
Epoch:20, Train loss:0.001489, valid loss:0.000895
Epoch:21, Train loss:0.001240, valid loss:0.000767
Epoch:22, Train loss:0.001235, valid loss:0.000765
Epoch:23, Train loss:0.001210, valid loss:0.000787
Epoch:24, Train loss:0.001190, valid loss:0.000722
Epoch:25, Train loss:0.001169, valid loss:0.000693
Epoch:26, Train loss:0.001149, valid loss:0.000759
Epoch:27, Train loss:0.001150, valid loss:0.000688
Epoch:28, Train loss:0.001124, valid loss:0.000724
Epoch:29, Train loss:0.001099, valid loss:0.000684
Epoch:30, Train loss:0.001114, valid loss:0.000687
Epoch:31, Train loss:0.000982, valid loss:0.000643
Epoch:32, Train loss:0.000972, valid loss:0.000628
Epoch:33, Train loss:0.000965, valid loss:0.000672
Epoch:34, Train loss:0.000966, valid loss:0.000624
Epoch:35, Train loss:0.000956, valid loss:0.000636
Epoch:36, Train loss:0.000943, valid loss:0.000670
Epoch:37, Train loss:0.000940, valid loss:0.000649
Epoch:38, Train loss:0.000936, valid loss:0.000614
Epoch:39, Train loss:0.000924, valid loss:0.000622
Epoch:40, Train loss:0.000924, valid loss:0.000685
Epoch:41, Train loss:0.000864, valid loss:0.000601
Epoch:42, Train loss:0.000857, valid loss:0.000608
Epoch:43, Train loss:0.000853, valid loss:0.000615
Epoch:44, Train loss:0.000851, valid loss:0.000620
Epoch:45, Train loss:0.000848, valid loss:0.000620
Epoch:46, Train loss:0.000845, valid loss:0.000604
Epoch:47, Train loss:0.000839, valid loss:0.000603
Epoch:48, Train loss:0.000835, valid loss:0.000623
Epoch:49, Train loss:0.000837, valid loss:0.000598
Epoch:50, Train loss:0.000833, valid loss:0.000612
Epoch:51, Train loss:0.000800, valid loss:0.000589
Epoch:52, Train loss:0.000795, valid loss:0.000595
Epoch:53, Train loss:0.000794, valid loss:0.000594
Epoch:54, Train loss:0.000791, valid loss:0.000596
Epoch:55, Train loss:0.000792, valid loss:0.000595
Epoch:56, Train loss:0.000789, valid loss:0.000602
Epoch:57, Train loss:0.000790, valid loss:0.000597
Epoch:58, Train loss:0.000788, valid loss:0.000640
Epoch:59, Train loss:0.000791, valid loss:0.000605
Epoch:60, Train loss:0.000789, valid loss:0.000595
training time 11342.869210481644
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.14096180930824379
plot_id,batch_id 0 1 miss% 0.055024726272160894
plot_id,batch_id 0 2 miss% 0.08823623895200153
plot_id,batch_id 0 3 miss% 0.08757584824111077
plot_id,batch_id 0 4 miss% 0.1177231402034357
plot_id,batch_id 0 5 miss% 0.061837824677601044
plot_id,batch_id 0 6 miss% 0.06523204599524755
plot_id,batch_id 0 7 miss% 0.11447124492109141
plot_id,batch_id 0 8 miss% 0.08842783068395058
plot_id,batch_id 0 9 miss% 0.022522177139004625
plot_id,batch_id 0 10 miss% 0.05812158297638049
plot_id,batch_id 0 11 miss% 0.05917461300757868
plot_id,batch_id 0 12 miss% 0.11251914299264959
plot_id,batch_id 0 13 miss% 0.05055005452039972
plot_id,batch_id 0 14 miss% 0.11889379302367116
plot_id,batch_id 0 15 miss% 0.04369529675964308
plot_id,batch_id 0 16 miss% 0.08047295260904173
plot_id,batch_id 0 17 miss% 0.06510118914246134
plot_id,batch_id 0 18 miss% 0.05579903208700629
plot_id,batch_id 0 19 miss% 0.0739401364494007
plot_id,batch_id 0 20 miss% 0.03927668367562735
plot_id,batch_id 0 21 miss% 0.06372695943791801
plot_id,batch_id 0 22 miss% 0.059820251298612195
plot_id,batch_id 0 23 miss% 0.039949112661762404
plot_id,batch_id 0 24 miss% 0.08273429302088628
plot_id,batch_id 0 25 miss% 0.12115498148364376
plot_id,batch_id 0 26 miss% 0.06573303729062532
plot_id,batch_id 0 27 miss% 0.0698049051970411
plot_id,batch_id 0 28 miss% 0.03611105214802422
plot_id,batch_id 0 29 miss% 0.0540346993278596
plot_id,batch_id 0 30 miss% 0.036209669510871
plot_id,batch_id 0 31 miss% 0.10883773177023742
plot_id,batch_id 0 32 miss% 0.13940155363161844
plot_id,batch_id 0 33 miss% 0.04957504943035277
plot_id,batch_id 0 34 miss% 0.08250131456856062
plot_id,batch_id 0 35 miss% 0.09157175681044458
plot_id,batch_id 0 36 miss% 0.13748640285823704
plot_id,batch_id 0 37 miss% 0.07829468681345328
plot_id,batch_id 0 38 miss% 0.04740610599619653
plot_id,batch_id 0 39 miss% 0.028120123104275294
plot_id,batch_id 0 40 miss% 0.1501699377162245
plot_id,batch_id 0 41 miss% 0.0383630048089994
plot_id,batch_id 0 42 miss% 0.042251774560221486
plot_id,batch_id 0 43 miss% 0.08287391282793072
plot_id,batch_id 0 44 miss% 0.07121589717859442
plot_id,batch_id 0 45 miss% 0.08567539730633823
plot_id,batch_id 0 46 miss% 0.044444931773607914
plot_id,batch_id 0 47 miss% 0.03537632539203266
plot_id,batch_id 0 48 miss% 0.056611952696081234
plot_id,batch_id 0 49 miss% 0.08377469020004195
plot_id,batch_id 0 50 miss% 0.12363171077411433
plot_id,batch_id 0 51 miss% 0.04574465779322234
plot_id,batch_id 0 52 miss% 0.025053445316892953
plot_id,batch_id 0 53 miss% 0.08806268875781746
plot_id,batch_id 0 54 miss% 0.03979085303845508
plot_id,batch_id 0 55 miss% 0.10606509145294551
plot_id,batch_id 0 56 miss% 0.0721449096614321
plot_id,batch_id 0 57 miss% 0.07501482597320845
plot_id,batch_id 0 58 miss% 0.07737835702050606
plot_id,batch_id 0 59 miss% 0.044495120745992586
plot_id,batch_id 0 60 miss% 0.05208150595712022
plot_id,batch_id 0 61 miss% 0.06073809937732695
plot_id,batch_id 0 62 miss% 0.03740748965368198
plot_id,batch_id 0 63 miss% 0.060640306879836146
plot_id,batch_id 0 64 miss% 0.04648201688902666
plot_id,batch_id 0 65 miss% 0.16619292072891362
plot_id,batch_id 0 66 miss% 0.12731677984021336
plot_id,batch_id 0 67 miss% 0.04723395560315428
plot_id,batch_id 0 68 miss% 0.037729334820233836
plot_id,batch_id 0 69 miss% 0.04497100895820717
plot_id,batch_id 0 70 miss% 0.1560464128237798
plot_id,batch_id 0 71 miss% 0.04405878139739938
plot_id,batch_id 0 72 miss% 0.12856696196064962
plot_id,batch_id 0 73 miss% 0.05541503179247815
plot_id,batch_id 0 74 miss% 0.10721005484934369
plot_id,batch_id 0 75 miss% 0.08432545291117807
plot_id,batch_id 0 76 miss% 0.07453327423174111
plot_id,batch_id 0 77 miss% 0.06612240554492313
plot_id,batch_id 0 78 miss% 0.0551035374361366
plot_id,batch_id 0 79 miss% 0.07149571468617848
plot_id,batch_id 0 80 miss% 0.04578900278158717
plot_id,batch_id 0 81 miss% 0.07971554455278938
plot_id,batch_id 0 82 miss% 0.10382385868686826
plot_id,batch_id 0 83 miss% 0.08411842409786396
plot_id,batch_id 0 84 miss% 0.07408281323513989
plot_id,batch_id 0 85 miss% 0.06287417590237535
plot_id,batch_id 0 86 miss% 0.09230049775512003
plot_id,batch_id 0 87 miss% 0.06354421959052099
plot_id,batch_id 0 88 miss% 0.10262717313429323
plot_id,batch_id 0 89 miss% 0.06100997264265648
plot_id,batch_id 0 90 miss% 0.04807683240637772
plot_id,batch_id 0 91 miss% 0.03215854636045222
plot_id,batch_id 0 92 miss% 0.07047849814726558
plot_id,batch_id 0 93 miss% 0.05091813095935164
plot_id,batch_id 0 94 miss% 0.07131215636411553
plot_id,batch_id 0 95 miss% 0.0405468217696234
plot_id,batch_id 0 96 miss% 0.07633905139012341
plot_id,batch_id 0 97 miss% 0.04536194871126206
plot_id,batch_id 0 98 miss% 0.021278222257029238
plot_id,batch_id 0 99 miss% 0.05912012287908945
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.14096181 0.05502473 0.08823624 0.08757585 0.11772314 0.06183782
 0.06523205 0.11447124 0.08842783 0.02252218 0.05812158 0.05917461
 0.11251914 0.05055005 0.11889379 0.0436953  0.08047295 0.06510119
 0.05579903 0.07394014 0.03927668 0.06372696 0.05982025 0.03994911
 0.08273429 0.12115498 0.06573304 0.06980491 0.03611105 0.0540347
 0.03620967 0.10883773 0.13940155 0.04957505 0.08250131 0.09157176
 0.1374864  0.07829469 0.04740611 0.02812012 0.15016994 0.038363
 0.04225177 0.08287391 0.0712159  0.0856754  0.04444493 0.03537633
 0.05661195 0.08377469 0.12363171 0.04574466 0.02505345 0.08806269
 0.03979085 0.10606509 0.07214491 0.07501483 0.07737836 0.04449512
 0.05208151 0.0607381  0.03740749 0.06064031 0.04648202 0.16619292
 0.12731678 0.04723396 0.03772933 0.04497101 0.15604641 0.04405878
 0.12856696 0.05541503 0.10721005 0.08432545 0.07453327 0.06612241
 0.05510354 0.07149571 0.045789   0.07971554 0.10382386 0.08411842
 0.07408281 0.06287418 0.0923005  0.06354422 0.10262717 0.06100997
 0.04807683 0.03215855 0.0704785  0.05091813 0.07131216 0.04054682
 0.07633905 0.04536195 0.02127822 0.05912012]
for model  90 the mean error 0.07189317594958415
all id 90 hidden_dim 16 learning_rate 0.0025 num_layers 4 frames 25 out win 4 err 0.07189317594958415 time 11342.869210481644
Launcher: Job 91 completed in 11613 seconds.
Launcher: Task 65 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  120401
Epoch:0, Train loss:0.784553, valid loss:0.780369
Epoch:1, Train loss:0.125910, valid loss:0.009631
Epoch:2, Train loss:0.018314, valid loss:0.005624
Epoch:3, Train loss:0.015687, valid loss:0.005053
Epoch:4, Train loss:0.012023, valid loss:0.005156
Epoch:5, Train loss:0.009202, valid loss:0.004445
Epoch:6, Train loss:0.008438, valid loss:0.003593
Epoch:7, Train loss:0.008163, valid loss:0.003814
Epoch:8, Train loss:0.006945, valid loss:0.002570
Epoch:9, Train loss:0.006554, valid loss:0.003122
Epoch:10, Train loss:0.005515, valid loss:0.001885
Epoch:11, Train loss:0.003098, valid loss:0.001658
Epoch:12, Train loss:0.002868, valid loss:0.001363
Epoch:13, Train loss:0.002718, valid loss:0.001462
Epoch:14, Train loss:0.002600, valid loss:0.001405
Epoch:15, Train loss:0.002539, valid loss:0.001652
Epoch:16, Train loss:0.002412, valid loss:0.001420
Epoch:17, Train loss:0.002281, valid loss:0.001242
Epoch:18, Train loss:0.002234, valid loss:0.001717
Epoch:19, Train loss:0.002153, valid loss:0.001427
Epoch:20, Train loss:0.002093, valid loss:0.001273
Epoch:21, Train loss:0.001564, valid loss:0.001048
Epoch:22, Train loss:0.001483, valid loss:0.000978
Epoch:23, Train loss:0.001460, valid loss:0.000960
Epoch:24, Train loss:0.001398, valid loss:0.001030
Epoch:25, Train loss:0.001409, valid loss:0.000974
Epoch:26, Train loss:0.001349, valid loss:0.000996
Epoch:27, Train loss:0.001329, valid loss:0.001027
Epoch:28, Train loss:0.001291, valid loss:0.000982
Epoch:29, Train loss:0.001274, valid loss:0.000972
Epoch:30, Train loss:0.001249, valid loss:0.000966
Epoch:31, Train loss:0.001008, valid loss:0.000815
Epoch:32, Train loss:0.000980, valid loss:0.000847
Epoch:33, Train loss:0.000977, valid loss:0.000862
Epoch:34, Train loss:0.000954, valid loss:0.000984
Epoch:35, Train loss:0.000953, valid loss:0.000847
Epoch:36, Train loss:0.000947, valid loss:0.000834
Epoch:37, Train loss:0.000910, valid loss:0.000813
Epoch:38, Train loss:0.000941, valid loss:0.000787
Epoch:39, Train loss:0.000899, valid loss:0.000810
Epoch:40, Train loss:0.000876, valid loss:0.000823
Epoch:41, Train loss:0.000784, valid loss:0.000777
Epoch:42, Train loss:0.000769, valid loss:0.000777
Epoch:43, Train loss:0.000762, valid loss:0.000772
Epoch:44, Train loss:0.000749, valid loss:0.000735
Epoch:45, Train loss:0.000742, valid loss:0.000752
Epoch:46, Train loss:0.000735, valid loss:0.000754
Epoch:47, Train loss:0.000730, valid loss:0.000767
Epoch:48, Train loss:0.000731, valid loss:0.000741
Epoch:49, Train loss:0.000719, valid loss:0.000734
Epoch:50, Train loss:0.000717, valid loss:0.000775
Epoch:51, Train loss:0.000683, valid loss:0.000740
Epoch:52, Train loss:0.000672, valid loss:0.000741
Epoch:53, Train loss:0.000667, valid loss:0.000747
Epoch:54, Train loss:0.000664, valid loss:0.000746
Epoch:55, Train loss:0.000662, valid loss:0.000744
Epoch:56, Train loss:0.000660, valid loss:0.000739
Epoch:57, Train loss:0.000658, valid loss:0.000743
Epoch:58, Train loss:0.000657, valid loss:0.000744
Epoch:59, Train loss:0.000656, valid loss:0.000742
Epoch:60, Train loss:0.000655, valid loss:0.000738
training time 11433.53115105629
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.30735884213218884
plot_id,batch_id 0 1 miss% 0.36439102305688353
plot_id,batch_id 0 2 miss% 0.33748543834469213
plot_id,batch_id 0 3 miss% 0.29475089345715005
plot_id,batch_id 0 4 miss% 0.29565736229694534
plot_id,batch_id 0 5 miss% 0.2992122319419745
plot_id,batch_id 0 6 miss% 0.3764563761708922
plot_id,batch_id 0 7 miss% 0.432274861378637
plot_id,batch_id 0 8 miss% 0.37822432261596656
plot_id,batch_id 0 9 miss% 0.38514964047397626
plot_id,batch_id 0 10 miss% 0.27540078875936547
plot_id,batch_id 0 11 miss% 0.3325261424457955
plot_id,batch_id 0 12 miss% 0.457492769770168
plot_id,batch_id 0 13 miss% 0.29777664751917676
plot_id,batch_id 0 14 miss% 0.35958191828188185
plot_id,batch_id 0 15 miss% 0.2927417006473583
plot_id,batch_id 0 16 miss% 0.3810395465662859
plot_id,batch_id 0 17 miss% 0.41318442045510223
plot_id,batch_id 0 18 miss% 0.4209446344714497
plot_id,batch_id 0 19 miss% 0.3302845627831328
plot_id,batch_id 0 20 miss% 0.3774430641432276
plot_id,batch_id 0 21 miss% 0.2665664977825167
plot_id,batch_id 0 22 miss% 0.3560273828118694
plot_id,batch_id 0 23 miss% 0.356142058546532
plot_id,batch_id 0 24 miss% 0.24853752385379316
plot_id,batch_id 0 25 miss% 0.30578773317149405
plot_id,batch_id 0 26 miss% 0.3545746045297666
plot_id,batch_id 0 27 miss% 0.2623838394780801
plot_id,batch_id 0 28 miss% 0.3316522521424384
plot_id,batch_id 0 29 miss% 0.3363134872047224
plot_id,batch_id 0 30 miss% 0.4195983914927453
plot_id,batch_id 0 31 miss% 0.5386940084093277
plot_id,batch_id 0 32 miss% 0.2819807650009606
plot_id,batch_id 0 33 miss% 0.36638050504494396
plot_id,batch_id 0 34 miss% 0.3008201503874423
plot_id,batch_id 0 35 miss% 0.2990702892054956
plot_id,batch_id 0 36 miss% 0.44342920634263555
plot_id,batch_id 0 37 miss% 0.36831120815075846
plot_id,batch_id 0 38 miss% 0.31691868013952657
plot_id,batch_id 0 39 miss% 0.36623479367865663
plot_id,batch_id 0 40 miss% 0.3758607395882977
plot_id,batch_id 0 41 miss% 0.3421388843979951
plot_id,batch_id 0 42 miss% 0.28783489699476744
plot_id,batch_id 0 43 miss% 0.37342506926363306
plot_id,batch_id 0 44 miss% 0.21972447129874106
plot_id,batch_id 0 45 miss% 0.2592609460291574
plot_id,batch_id 0 46 miss% 0.34823264369301504
plot_id,batch_id 0 47 miss% 0.3142538951363142
plot_id,batch_id 0 48 miss% 0.23644687872958114
plot_id,batch_id 0 49 miss% 0.25203247013403207
plot_id,batch_id 0 50 miss% 0.4418595228040993
plot_id,batch_id 0 51 miss% 0.44025891049481825
plot_id,batch_id 0 52 miss% 0.40138568195953417
plot_id,batch_id 0 53 miss% 0.21051234152169618
plot_id,batch_id 0 54 miss% 0.26156019744045456
plot_id,batch_id 0 55 miss% 0.4111234606164834
plot_id,batch_id 0 56 miss% 0.37453637584830013
plot_id,batch_id 0 57 miss% 0.3648139907644529
plot_id,batch_id 0 58 miss% 0.3237444698539767
plot_id,batch_id 0 59 miss% 0.2698310215676599
plot_id,batch_id 0 60 miss% 0.2553716778236439
plot_id,batch_id 0 61 miss% 0.24974610731376712
plot_id,batch_id 0 62 miss% 0.3516237407939634
plot_id,batch_id 0 63 miss% 0.3101187661530501
plot_id,batch_id 0 64 miss% 0.3793513901242382
plot_id,batch_id 0 65 miss% 0.38125439985046833
plot_id,batch_id 0 66 miss% 0.41172675439769013
plot_id,batch_id 0 67 miss% 0.2546680089759617
plot_id,batch_id 0 68 miss% 0.3090281363796241
plot_id,batch_id 0 69 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  69649
Epoch:0, Train loss:0.795382, valid loss:0.773365
Epoch:1, Train loss:0.057452, valid loss:0.007123
Epoch:2, Train loss:0.015460, valid loss:0.004256
Epoch:3, Train loss:0.012370, valid loss:0.003959
Epoch:4, Train loss:0.010135, valid loss:0.003798
Epoch:5, Train loss:0.008063, valid loss:0.002816
Epoch:6, Train loss:0.005736, valid loss:0.002656
Epoch:7, Train loss:0.004727, valid loss:0.001883
Epoch:8, Train loss:0.003996, valid loss:0.002228
Epoch:9, Train loss:0.003708, valid loss:0.001799
Epoch:10, Train loss:0.003427, valid loss:0.001838
Epoch:11, Train loss:0.002624, valid loss:0.001421
Epoch:12, Train loss:0.002471, valid loss:0.001343
Epoch:13, Train loss:0.002549, valid loss:0.001390
Epoch:14, Train loss:0.002361, valid loss:0.001217
Epoch:15, Train loss:0.002299, valid loss:0.001315
Epoch:16, Train loss:0.002259, valid loss:0.001276
Epoch:17, Train loss:0.002146, valid loss:0.001178
Epoch:18, Train loss:0.002142, valid loss:0.001064
Epoch:19, Train loss:0.002036, valid loss:0.001096
Epoch:20, Train loss:0.001978, valid loss:0.001187
Epoch:21, Train loss:0.001642, valid loss:0.001023
Epoch:22, Train loss:0.001573, valid loss:0.001007
Epoch:23, Train loss:0.001539, valid loss:0.001096
Epoch:24, Train loss:0.001522, valid loss:0.000942
Epoch:25, Train loss:0.001518, valid loss:0.001004
Epoch:26, Train loss:0.001489, valid loss:0.000963
Epoch:27, Train loss:0.001463, valid loss:0.000987
Epoch:28, Train loss:0.001444, valid loss:0.001001
Epoch:29, Train loss:0.001419, valid loss:0.000984
Epoch:30, Train loss:0.001400, valid loss:0.001027
Epoch:31, Train loss:0.001219, valid loss:0.000893
Epoch:32, Train loss:0.001206, valid loss:0.000957
Epoch:33, Train loss:0.001194, valid loss:0.000882
Epoch:34, Train loss:0.001190, valid loss:0.000857
Epoch:35, Train loss:0.001172, valid loss:0.000892
Epoch:36, Train loss:0.001168, valid loss:0.000881
Epoch:37, Train loss:0.001167, valid loss:0.000859
Epoch:38, Train loss:0.001135, valid loss:0.000886
Epoch:39, Train loss:0.001131, valid loss:0.000878
Epoch:40, Train loss:0.001121, valid loss:0.000888
Epoch:41, Train loss:0.001036, valid loss:0.000838
Epoch:42, Train loss:0.001024, valid loss:0.000858
Epoch:43, Train loss:0.001024, valid loss:0.000822
Epoch:44, Train loss:0.001016, valid loss:0.000863
Epoch:45, Train loss:0.001015, valid loss:0.000831
Epoch:46, Train loss:0.001001, valid loss:0.000824
Epoch:47, Train loss:0.000998, valid loss:0.000848
Epoch:48, Train loss:0.000996, valid loss:0.000846
Epoch:49, Train loss:0.000990, valid loss:0.000833
Epoch:50, Train loss:0.000982, valid loss:0.000846
Epoch:51, Train loss:0.000928, valid loss:0.000812
Epoch:52, Train loss:0.000922, valid loss:0.000812
Epoch:53, Train loss:0.000920, valid loss:0.000812
Epoch:54, Train loss:0.000918, valid loss:0.000810
Epoch:55, Train loss:0.000917, valid loss:0.000810
Epoch:56, Train loss:0.000916, valid loss:0.000809
Epoch:57, Train loss:0.000915, valid loss:0.000813
Epoch:58, Train loss:0.000915, valid loss:0.000811
Epoch:59, Train loss:0.000914, valid loss:0.000814
Epoch:60, Train loss:0.000914, valid loss:0.000811
training time 11424.625655412674
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07501048275147368
plot_id,batch_id 0 1 miss% 0.06450571631068959
plot_id,batch_id 0 2 miss% 0.10884791903805628
plot_id,batch_id 0 3 miss% 0.09068929784572158
plot_id,batch_id 0 4 miss% 0.054533788227768595
plot_id,batch_id 0 5 miss% 0.05279441658450296
plot_id,batch_id 0 6 miss% 0.06699742936395436
plot_id,batch_id 0 7 miss% 0.10774732005895972
plot_id,batch_id 0 8 miss% 0.09026847691823832
plot_id,batch_id 0 9 miss% 0.05393994340523439
plot_id,batch_id 0 10 miss% 0.04073599073306453
plot_id,batch_id 0 11 miss% 0.09113268144260618
plot_id,batch_id 0 12 miss% 0.07286031123993945
plot_id,batch_id 0 13 miss% 0.07470613120579586
plot_id,batch_id 0 14 miss% 0.11806979913999131
plot_id,batch_id 0 15 miss% 0.06697887643182479
plot_id,batch_id 0 16 miss% 0.18640316626908823
plot_id,batch_id 0 17 miss% 0.052509677408791096
plot_id,batch_id 0 18 miss% 0.054334813918219906
plot_id,batch_id 0 19 miss% 0.09210065137014252
plot_id,batch_id 0 20 miss% 0.0776456364536291
plot_id,batch_id 0 21 miss% 0.07053677266276495
plot_id,batch_id 0 22 miss% 0.051453798335222026
plot_id,batch_id 0 23 miss% 0.05181445811957934
plot_id,batch_id 0 24 miss% 0.07756071790885512
plot_id,batch_id 0 25 miss% 0.06825186724570668
plot_id,batch_id 0 26 miss% 0.06453310202091081
plot_id,batch_id 0 27 miss% 0.051636947875412766
plot_id,batch_id 0 28 miss% 0.08278804476942264
plot_id,batch_id 0 29 miss% 0.03203287019884717
plot_id,batch_id 0 30 miss% 0.05865385726600895
plot_id,batch_id 0 31 miss% 0.1091261174972581
plot_id,batch_id 0 32 miss% 0.08619366282903337
plot_id,batch_id 0 33 miss% 0.0627530660153074
plot_id,batch_id 0 34 miss% 0.0420381403663783
plot_id,batch_id 0 35 miss% 0.03311741383553693
plot_id,batch_id 0 36 miss% 0.07001815598669357
plot_id,batch_id 0 37 miss% 0.06435298149057207
plot_id,batch_id 0 38 miss% 0.023946818363041647
plot_id,batch_id 0 39 miss% 0.035329874945673474
plot_id,batch_id 0 40 miss% 0.1749047047409647
plot_id,batch_id 0 41 miss% 0.04594258843064296
plot_id,batch_id 0 42 miss% 0.021615708794075754
plot_id,batch_id 0 43 miss% 0.1119527665088847
plot_id,batch_id 0 44 miss% 0.030599788601278724
plot_id,batch_id 0 45 miss% 0.07660986061316574
plot_id,batch_id 0 46 miss% 0.03942791267644023
plot_id,batch_id 0 47 miss% 0.023877606241302148
plot_id,batch_id 0 48 miss% 0.04581096097537935
plot_id,batch_id 0 49 miss% 0.07636272199747518
plot_id,batch_id 0 50 miss% 0.07863969097184803
plot_id,batch_id 0 51 miss% 0.049401881311998914
plot_id,batch_id 0 52 miss% 0.03361159963778279
plot_id,batch_id 0 53 miss% 0.025008970095850153
plot_id,batch_id 0 54 miss% 0.125845069358403
plot_id,batch_id 0 55 miss% 0.1330851094678421
plot_id,batch_id 0 56 miss% 0.06684441471414587
plot_id,batch_id 0 57 miss% 0.04135845893826978
plot_id,batch_id 0 58 miss% 0.0549305412623931
plot_id,batch_id 0 59 miss% 0.06783591709619481
plot_id,batch_id 0 60 miss% 0.08409737804210159
plot_id,batch_id 0 61 miss% 0.05880638813513239
plot_id,batch_id 0 62 miss% 0.05643339966645914
plot_id,batch_id 0 63 miss% 0.09336791318763323
plot_id,batch_id 0 64 miss% 0.08829368281806056
plot_id,batch_id 0 65 miss% 0.06896367222734681
plot_id,batch_id 0 66 miss% 0.09639240544401542
plot_id,batch_id 0 67 miss% 0.05339275185113193
plot_id,batch_id 0 68 miss% 0.065643066654117220.39796351722097667
plot_id,batch_id 0 70 miss% 0.34518923696276554
plot_id,batch_id 0 71 miss% 0.38441522095986774
plot_id,batch_id 0 72 miss% 0.3756066603288264
plot_id,batch_id 0 73 miss% 0.36592873476982074
plot_id,batch_id 0 74 miss% 0.3224333482578625
plot_id,batch_id 0 75 miss% 0.20411885320177264
plot_id,batch_id 0 76 miss% 0.3601148899201601
plot_id,batch_id 0 77 miss% 0.2967839012275768
plot_id,batch_id 0 78 miss% 0.3168605282184879
plot_id,batch_id 0 79 miss% 0.3003952421805523
plot_id,batch_id 0 80 miss% 0.3005329069263268
plot_id,batch_id 0 81 miss% 0.41230737942013623
plot_id,batch_id 0 82 miss% 0.34310163093264606
plot_id,batch_id 0 83 miss% 0.3090561264692609
plot_id,batch_id 0 84 miss% 0.34432084460880663
plot_id,batch_id 0 85 miss% 0.22523251836330022
plot_id,batch_id 0 86 miss% 0.28264944623165683
plot_id,batch_id 0 87 miss% 0.3392164596448771
plot_id,batch_id 0 88 miss% 0.2964308255782352
plot_id,batch_id 0 89 miss% 0.35013178629533664
plot_id,batch_id 0 90 miss% 0.22910633940926559
plot_id,batch_id 0 91 miss% 0.37918820620835153
plot_id,batch_id 0 92 miss% 0.38458388406627747
plot_id,batch_id 0 93 miss% 0.24424113540145456
plot_id,batch_id 0 94 miss% 0.39669601585451525
plot_id,batch_id 0 95 miss% 0.22829834884974756
plot_id,batch_id 0 96 miss% 0.26206480015559164
plot_id,batch_id 0 97 miss% 0.41731763132730537
plot_id,batch_id 0 98 miss% 0.32491981454078966
plot_id,batch_id 0 99 miss% 0.31763675457709645
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.30735884 0.36439102 0.33748544 0.29475089 0.29565736 0.29921223
 0.37645638 0.43227486 0.37822432 0.38514964 0.27540079 0.33252614
 0.45749277 0.29777665 0.35958192 0.2927417  0.38103955 0.41318442
 0.42094463 0.33028456 0.37744306 0.2665665  0.35602738 0.35614206
 0.24853752 0.30578773 0.3545746  0.26238384 0.33165225 0.33631349
 0.41959839 0.53869401 0.28198077 0.36638051 0.30082015 0.29907029
 0.44342921 0.36831121 0.31691868 0.36623479 0.37586074 0.34213888
 0.2878349  0.37342507 0.21972447 0.25926095 0.34823264 0.3142539
 0.23644688 0.25203247 0.44185952 0.44025891 0.40138568 0.21051234
 0.2615602  0.41112346 0.37453638 0.36481399 0.32374447 0.26983102
 0.25537168 0.24974611 0.35162374 0.31011877 0.37935139 0.3812544
 0.41172675 0.25466801 0.30902814 0.39796352 0.34518924 0.38441522
 0.37560666 0.36592873 0.32243335 0.20411885 0.36011489 0.2967839
 0.31686053 0.30039524 0.30053291 0.41230738 0.34310163 0.30905613
 0.34432084 0.22523252 0.28264945 0.33921646 0.29643083 0.35013179
 0.22910634 0.37918821 0.38458388 0.24424114 0.39669602 0.22829835
 0.2620648  0.41731763 0.32491981 0.31763675]
for model  66 the mean error 0.3339736940314503
all id 66 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 21 out win 4 err 0.3339736940314503 time 11433.53115105629
Launcher: Job 67 completed in 11677 seconds.
Launcher: Task 180 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  120401
Epoch:0, Train loss:0.734365, valid loss:0.715748
Epoch:1, Train loss:0.263995, valid loss:0.005650
Epoch:2, Train loss:0.015157, valid loss:0.005034
Epoch:3, Train loss:0.013205, valid loss:0.004473
Epoch:4, Train loss:0.010369, valid loss:0.003012
Epoch:5, Train loss:0.006234, valid loss:0.002751
Epoch:6, Train loss:0.005478, valid loss:0.002425
Epoch:7, Train loss:0.004775, valid loss:0.002294
Epoch:8, Train loss:0.004250, valid loss:0.002262
Epoch:9, Train loss:0.003864, valid loss:0.001889
Epoch:10, Train loss:0.003610, valid loss:0.001828
Epoch:11, Train loss:0.002916, valid loss:0.001530
Epoch:12, Train loss:0.002727, valid loss:0.001439
Epoch:13, Train loss:0.002589, valid loss:0.001381
Epoch:14, Train loss:0.002531, valid loss:0.001389
Epoch:15, Train loss:0.002421, valid loss:0.001311
Epoch:16, Train loss:0.002276, valid loss:0.001297
Epoch:17, Train loss:0.002295, valid loss:0.001311
Epoch:18, Train loss:0.002194, valid loss:0.001306
Epoch:19, Train loss:0.002060, valid loss:0.001412
Epoch:20, Train loss:0.002047, valid loss:0.001286
Epoch:21, Train loss:0.001663, valid loss:0.001067
Epoch:22, Train loss:0.001580, valid loss:0.001154
Epoch:23, Train loss:0.001586, valid loss:0.001184
Epoch:24, Train loss:0.001551, valid loss:0.001155
Epoch:25, Train loss:0.001527, valid loss:0.001170
Epoch:26, Train loss:0.001479, valid loss:0.001156
Epoch:27, Train loss:0.001466, valid loss:0.001022
Epoch:28, Train loss:0.001433, valid loss:0.001053
Epoch:29, Train loss:0.001458, valid loss:0.001220
Epoch:30, Train loss:0.001387, valid loss:0.001138
Epoch:31, Train loss:0.001215, valid loss:0.000988
Epoch:32, Train loss:0.001184, valid loss:0.001040
Epoch:33, Train loss:0.001170, valid loss:0.001067
Epoch:34, Train loss:0.001158, valid loss:0.001043
Epoch:35, Train loss:0.001153, valid loss:0.001095
Epoch:36, Train loss:0.001125, valid loss:0.000988
Epoch:37, Train loss:0.001109, valid loss:0.001061
Epoch:38, Train loss:0.001129, valid loss:0.001022
Epoch:39, Train loss:0.001115, valid loss:0.000999
Epoch:40, Train loss:0.001081, valid loss:0.000974
Epoch:41, Train loss:0.000998, valid loss:0.000998
Epoch:42, Train loss:0.000986, valid loss:0.001024
Epoch:43, Train loss:0.000979, valid loss:0.001049
Epoch:44, Train loss:0.000972, valid loss:0.000969
Epoch:45, Train loss:0.000960, valid loss:0.001102
Epoch:46, Train loss:0.000971, valid loss:0.000997
Epoch:47, Train loss:0.000952, valid loss:0.000926
Epoch:48, Train loss:0.000942, valid loss:0.000934
Epoch:49, Train loss:0.000948, valid loss:0.000936
Epoch:50, Train loss:0.000934, valid loss:0.000982
Epoch:51, Train loss:0.000882, valid loss:0.000957
Epoch:52, Train loss:0.000872, valid loss:0.000966
Epoch:53, Train loss:0.000872, valid loss:0.000959
Epoch:54, Train loss:0.000869, valid loss:0.000947
Epoch:55, Train loss:0.000868, valid loss:0.000963
Epoch:56, Train loss:0.000870, valid loss:0.000970
Epoch:57, Train loss:0.000869, valid loss:0.000951
Epoch:58, Train loss:0.000869, valid loss:0.000954
Epoch:59, Train loss:0.000868, valid loss:0.000954
Epoch:60, Train loss:0.000863, valid loss:0.000942
training time 11461.474925041199
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.07246488324772125
plot_id,batch_id 0 1 miss% 0.04184028912602609
plot_id,batch_id 0 2 miss% 0.12544241532886483
plot_id,batch_id 0 3 miss% 0.061327954766539516
plot_id,batch_id 0 4 miss% 0.05915440837189037
plot_id,batch_id 0 5 miss% 0.07586681626679245
plot_id,batch_id 0 6 miss% 0.06608077654161829
plot_id,batch_id 0 7 miss% 0.10721257304026317
plot_id,batch_id 0 8 miss% 0.08983889397136294
plot_id,batch_id 0 9 miss% 0.06808580518837075
plot_id,batch_id 0 10 miss% 0.05364872147511293
plot_id,batch_id 0 11 miss% 0.05589008635746806
plot_id,batch_id 0 12 miss% 0.07749602359999781
plot_id,batch_id 0 13 miss% 0.05583691689401754
plot_id,batch_id 0 14 miss% 0.07413532326087369
plot_id,batch_id 0 15 miss% 0.08018320367968707
plot_id,batch_id 0 16 miss% 0.11051156317505666
plot_id,batch_id 0 17 miss% 0.04362060578220893
plot_id,batch_id 0 18 miss% 0.07631314601157221
plot_id,batch_id 0 19 miss% 0.06699027649843019
plot_id,batch_id 0 20 miss% 0.062408711272147956
plot_id,batch_id 0 21 miss% 0.048462075510339854
plot_id,batch_id 0 22 miss% 0.04569747921653375
plot_id,batch_id 0 23 miss% 0.04509907415430959
plot_id,batch_id 0 24 miss% 0.03985805125274634
plot_id,batch_id 0 25 miss% 0.06033688892958695
plot_id,batch_id 0 26 miss% 0.047958550379284294
plot_id,batch_id 0 27 miss% 0.04491186670590975
plot_id,batch_id 0 28 miss% 0.02499405085115771
plot_id,batch_id 0 29 miss% 0.03736629549257338
plot_id,batch_id 0 30 miss% 0.03625192362165553
plot_id,batch_id 0 31 miss% 0.0639082903452147
plot_id,batch_id 0 32 miss% 0.13095634960899083
plot_id,batch_id 0 33 miss% 0.06371457735744117
plot_id,batch_id 0 34 miss% 0.09644423553107362
plot_id,batch_id 0 35 miss% 0.06456588212398387
plot_id,batch_id 0 36 miss% 0.07081650751417305
plot_id,batch_id 0 37 miss% 0.05591508976065662
plot_id,batch_id 0 38 miss% 0.06481809036950442
plot_id,batch_id 0 39 miss% 0.0318437968235179
plot_id,batch_id 0 40 miss% 0.047051814521447174
plot_id,batch_id 0 41 miss% 0.035246036616372795
plot_id,batch_id 0 42 miss% 0.03271309095419296
plot_id,batch_id 0 43 miss% 0.048994738923641444
plot_id,batch_id 0 44 miss% 0.027145030008711855
plot_id,batch_id 0 45 miss% 0.05424344211122497
plot_id,batch_id 0 46 miss% 0.05641417490871346
plot_id,batch_id 0 47 miss% 0.03298839252219823
plot_id,batch_id 0 48 miss% 0.028942328109060896
plot_id,batch_id 0 49 miss% 0.04188586877134515
plot_id,batch_id 0 50 miss% 0.198374545576577
plot_id,batch_id 0 51 miss% 0.04756807499599121
plot_id,batch_id 0 52 miss% 0.03965745759943569
plot_id,batch_id 0 53 miss% 0.04263833814355977
plot_id,batch_id 0 54 miss% 0.02661218769078374
plot_id,batch_id 0 55 miss% 0.055571853740893414
plot_id,batch_id 0 56 miss% 0.06909611817612049
plot_id,batch_id 0 57 miss% 0.04723228520796244
plot_id,batch_id 0 58 miss% 0.04674424199599657
plot_id,batch_id 0 59 miss% 0.031804677496881754
plot_id,batch_id 0 60 miss% 0.04142889465959895
plot_id,batch_id 0 61 miss% 0.04198128976348741
plot_id,batch_id 0 62 miss% 0.08720912838016204
plot_id,batch_id 0 63 miss% 0.0854790043768121
plot_id,batch_id 0 64 miss% 0.06983821575370133
plot_id,batch_id 0 65 miss% 0.03449696966312929
plot_id,batch_id 0 66 miss% 0.12646276664030115
plot_id,batch_id 0 67 miss% 0.047215285004451185
plot_id,batch_id 0 68 miss% 0.03703187123851082
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  154129
Epoch:0, Train loss:0.708836, valid loss:0.710832
Epoch:1, Train loss:0.123265, valid loss:0.006282
Epoch:2, Train loss:0.016019, valid loss:0.005078
Epoch:3, Train loss:0.012750, valid loss:0.004424
Epoch:4, Train loss:0.011224, valid loss:0.004213
Epoch:5, Train loss:0.009451, valid loss:0.003206
Epoch:6, Train loss:0.006447, valid loss:0.003075
Epoch:7, Train loss:0.004889, valid loss:0.001969
Epoch:8, Train loss:0.003887, valid loss:0.002134
Epoch:9, Train loss:0.003545, valid loss:0.001900
Epoch:10, Train loss:0.003229, valid loss:0.001558
Epoch:11, Train loss:0.002426, valid loss:0.001283
Epoch:12, Train loss:0.002274, valid loss:0.001342
Epoch:13, Train loss:0.002197, valid loss:0.001281
Epoch:14, Train loss:0.002114, valid loss:0.001182
Epoch:15, Train loss:0.001998, valid loss:0.001198
Epoch:16, Train loss:0.001972, valid loss:0.001306
Epoch:17, Train loss:0.001941, valid loss:0.001150
Epoch:18, Train loss:0.001841, valid loss:0.001201
Epoch:19, Train loss:0.001827, valid loss:0.001403
Epoch:20, Train loss:0.001775, valid loss:0.001098
Epoch:21, Train loss:0.001296, valid loss:0.001019
Epoch:22, Train loss:0.001253, valid loss:0.000973
Epoch:23, Train loss:0.001227, valid loss:0.001006
Epoch:24, Train loss:0.001213, valid loss:0.001103
Epoch:25, Train loss:0.001279, valid loss:0.001026
Epoch:26, Train loss:0.001146, valid loss:0.000995
Epoch:27, Train loss:0.001134, valid loss:0.001060
Epoch:28, Train loss:0.001164, valid loss:0.001001
Epoch:29, Train loss:0.001106, valid loss:0.001246
Epoch:30, Train loss:0.001072, valid loss:0.001072
Epoch:31, Train loss:0.000871, valid loss:0.000898
Epoch:32, Train loss:0.000821, valid loss:0.000934
Epoch:33, Train loss:0.000831, valid loss:0.000896
Epoch:34, Train loss:0.000849, valid loss:0.000928
Epoch:35, Train loss:0.000823, valid loss:0.000933
Epoch:36, Train loss:0.000805, valid loss:0.000920
Epoch:37, Train loss:0.000813, valid loss:0.000889
Epoch:38, Train loss:0.000788, valid loss:0.000909
Epoch:39, Train loss:0.000786, valid loss:0.001008
Epoch:40, Train loss:0.000764, valid loss:0.000929
Epoch:41, Train loss:0.000662, valid loss:0.000886
Epoch:42, Train loss:0.000651, valid loss:0.000882
Epoch:43, Train loss:0.000650, valid loss:0.000894
Epoch:44, Train loss:0.000639, valid loss:0.000879
Epoch:45, Train loss:0.000632, valid loss:0.000912
Epoch:46, Train loss:0.000639, valid loss:0.000911
Epoch:47, Train loss:0.000626, valid loss:0.000918
Epoch:48, Train loss:0.000620, valid loss:0.000919
Epoch:49, Train loss:0.000629, valid loss:0.000922
Epoch:50, Train loss:0.000607, valid loss:0.000886
Epoch:51, Train loss:0.000564, valid loss:0.000885
Epoch:52, Train loss:0.000557, valid loss:0.000887
Epoch:53, Train loss:0.000554, valid loss:0.000883
Epoch:54, Train loss:0.000553, valid loss:0.000886
Epoch:55, Train loss:0.000551, valid loss:0.000887
Epoch:56, Train loss:0.000551, valid loss:0.000885
Epoch:57, Train loss:0.000550, valid loss:0.000885
Epoch:58, Train loss:0.000549, valid loss:0.000882
Epoch:59, Train loss:0.000548, valid loss:0.000881
Epoch:60, Train loss:0.000548, valid loss:0.000880
training time 11466.656998157501
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.11278758812833813
plot_id,batch_id 0 1 miss% 0.05627959017063472
plot_id,batch_id 0 2 miss% 0.10849658662576123
plot_id,batch_id 0 3 miss% 0.039831266043425746
plot_id,batch_id 0 4 miss% 0.02440989636881545
plot_id,batch_id 0 5 miss% 0.04412851078242598
plot_id,batch_id 0 6 miss% 0.03328065723544239
plot_id,batch_id 0 7 miss% 0.09180249527635848
plot_id,batch_id 0 8 miss% 0.08401993802831922
plot_id,batch_id 0 9 miss% 0.025102383372760117
plot_id,batch_id 0 10 miss% 0.03718299338873901
plot_id,batch_id 0 11 miss% 0.05074838088498481
plot_id,batch_id 0 12 miss% 0.04177884544346188
plot_id,batch_id 0 13 miss% 0.04556368622859142
plot_id,batch_id 0 14 miss% 0.07615507544955084
plot_id,batch_id 0 15 miss% 0.03863103477827793
plot_id,batch_id 0 16 miss% 0.04299469099628969
plot_id,batch_id 0 17 miss% 0.06510712859462249
plot_id,batch_id 0 18 miss% 0.05595058778276864
plot_id,batch_id 0 19 miss% 0.14136764629131635
plot_id,batch_id 0 20 miss% 0.1698362476776445
plot_id,batch_id 0 21 miss% 0.04481145827896268
plot_id,batch_id 0 22 miss% 0.06205937055520353
plot_id,batch_id 0 23 miss% 0.04698498826381018
plot_id,batch_id 0 24 miss% 0.06480533280496521
plot_id,batch_id 0 25 miss% 0.0511147593258872
plot_id,batch_id 0 26 miss% 0.03700652212905885
plot_id,batch_id 0 27 miss% 0.06291412716000601
plot_id,batch_id 0 28 miss% 0.03203332837374997
plot_id,batch_id 0 29 miss% 0.0439249508935531
plot_id,batch_id 0 30 miss% 0.060437136601782765
plot_id,batch_id 0 31 miss% 0.12025582567342862
plot_id,batch_id 0 32 miss% 0.07292229753087624
plot_id,batch_id 0 33 miss% 0.06553623044646247
plot_id,batch_id 0 34 miss% 0.039582114940458636
plot_id,batch_id 0 35 miss% 0.028769979158114065
plot_id,batch_id 0 36 miss% 0.10980361490001458
plot_id,batch_id 0 37 miss% 0.05727278237925621
plot_id,batch_id 0 38 miss% 0.0359007290913562
plot_id,batch_id 0 39 miss% 0.04181011708487112
plot_id,batch_id 0 40 miss% 0.05976344992628928
plot_id,batch_id 0 41 miss% 0.027350281550369983
plot_id,batch_id 0 42 miss% 0.04653107106822385
plot_id,batch_id 0 43 miss% 0.033293340244244435
plot_id,batch_id 0 44 miss% 0.05671755664073966
plot_id,batch_id 0 45 miss% 0.055890740294497976
plot_id,batch_id 0 46 miss% 0.03653648519359184
plot_id,batch_id 0 47 miss% 0.03126685427541991
plot_id,batch_id 0 48 miss% 0.042772409779395115
plot_id,batch_id 0 49 miss% 0.048334676051520545
plot_id,batch_id 0 50 miss% 0.132717930417286
plot_id,batch_id 0 51 miss% 0.03592353007534929
plot_id,batch_id 0 52 miss% 0.047997127798797015
plot_id,batch_id 0 53 miss% 0.04646971535199604
plot_id,batch_id 0 54 miss% 0.04005680626778519
plot_id,batch_id 0 55 miss% 0.10164326379790257
plot_id,batch_id 0 56 miss% 0.05938794948580369
plot_id,batch_id 0 57 miss% 0.04672826244805266
plot_id,batch_id 0 58 miss% 0.05825694731976936
plot_id,batch_id 0 59 miss% 0.043860851697488455
plot_id,batch_id 0 60 miss% 0.041162884178073816
plot_id,batch_id 0 61 miss% 0.031561660775385456
plot_id,batch_id 0 62 miss% 0.06423623360629503
plot_id,batch_id 0 63 miss% 0.038128849363090946
plot_id,batch_id 0 64 miss% 0.08633825049159917
plot_id,batch_id 0 65 miss% 0.08464474058795304
plot_id,batch_id 0 66 miss% 0.13780786951421034
plot_id,batch_id 0 67 miss% 0.019029725902667705
plot_id,batch_id 0 68 miss% 0.05124860034192375

plot_id,batch_id 0 69 miss% 0.11949907652256775
plot_id,batch_id 0 70 miss% 0.10116359480990166
plot_id,batch_id 0 71 miss% 0.062259610451225814
plot_id,batch_id 0 72 miss% 0.07445966468576208
plot_id,batch_id 0 73 miss% 0.05713459278699659
plot_id,batch_id 0 74 miss% 0.12278558656241848
plot_id,batch_id 0 75 miss% 0.03192539419960393
plot_id,batch_id 0 76 miss% 0.06483464617743286
plot_id,batch_id 0 77 miss% 0.03218985813517036
plot_id,batch_id 0 78 miss% 0.021210046875305057
plot_id,batch_id 0 79 miss% 0.054985568760181826
plot_id,batch_id 0 80 miss% 0.057645203249249065
plot_id,batch_id 0 81 miss% 0.10817332502916475
plot_id,batch_id 0 82 miss% 0.07495391573370411
plot_id,batch_id 0 83 miss% 0.1037337668399147
plot_id,batch_id 0 84 miss% 0.08056186488269414
plot_id,batch_id 0 85 miss% 0.08421498598778551
plot_id,batch_id 0 86 miss% 0.054032959542265856
plot_id,batch_id 0 87 miss% 0.07331672746761522
plot_id,batch_id 0 88 miss% 0.0862995448291096
plot_id,batch_id 0 89 miss% 0.07332778651397738
plot_id,batch_id 0 90 miss% 0.055980063584851365
plot_id,batch_id 0 91 miss% 0.1049213741468963
plot_id,batch_id 0 92 miss% 0.0732040626659085
plot_id,batch_id 0 93 miss% 0.04520899088665839
plot_id,batch_id 0 94 miss% 0.06857521392315181
plot_id,batch_id 0 95 miss% 0.11142211559806131
plot_id,batch_id 0 96 miss% 0.06689232731385918
plot_id,batch_id 0 97 miss% 0.04375794341864873
plot_id,batch_id 0 98 miss% 0.04128612201810665
plot_id,batch_id 0 99 miss% 0.0707821394391117
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07501048 0.06450572 0.10884792 0.0906893  0.05453379 0.05279442
 0.06699743 0.10774732 0.09026848 0.05393994 0.04073599 0.09113268
 0.07286031 0.07470613 0.1180698  0.06697888 0.18640317 0.05250968
 0.05433481 0.09210065 0.07764564 0.07053677 0.0514538  0.05181446
 0.07756072 0.06825187 0.0645331  0.05163695 0.08278804 0.03203287
 0.05865386 0.10912612 0.08619366 0.06275307 0.04203814 0.03311741
 0.07001816 0.06435298 0.02394682 0.03532987 0.1749047  0.04594259
 0.02161571 0.11195277 0.03059979 0.07660986 0.03942791 0.02387761
 0.04581096 0.07636272 0.07863969 0.04940188 0.0336116  0.02500897
 0.12584507 0.13308511 0.06684441 0.04135846 0.05493054 0.06783592
 0.08409738 0.05880639 0.0564334  0.09336791 0.08829368 0.06896367
 0.09639241 0.05339275 0.06564307 0.11949908 0.10116359 0.06225961
 0.07445966 0.05713459 0.12278559 0.03192539 0.06483465 0.03218986
 0.02121005 0.05498557 0.0576452  0.10817333 0.07495392 0.10373377
 0.08056186 0.08421499 0.05403296 0.07331673 0.08629954 0.07332779
 0.05598006 0.10492137 0.07320406 0.04520899 0.06857521 0.11142212
 0.06689233 0.04375794 0.04128612 0.07078214]
for model  45 the mean error 0.07038746197347553
all id 45 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 21 out win 4 err 0.07038746197347553 time 11424.625655412674
Launcher: Job 46 completed in 11700 seconds.
Launcher: Task 14 done. Exiting.
plot_id,batch_id 0 69 miss% 0.0848794028016121
plot_id,batch_id 0 70 miss% 0.04143527490418984
plot_id,batch_id 0 71 miss% 0.028994631345516136
plot_id,batch_id 0 72 miss% 0.0751253200953864
plot_id,batch_id 0 73 miss% 0.10106110870402163
plot_id,batch_id 0 74 miss% 0.14186022230450934
plot_id,batch_id 0 75 miss% 0.024391600850360757
plot_id,batch_id 0 76 miss% 0.08016897377781376
plot_id,batch_id 0 77 miss% 0.050306112075335696
plot_id,batch_id 0 78 miss% 0.044973226720557764
plot_id,batch_id 0 79 miss% 0.10593358383431721
plot_id,batch_id 0 80 miss% 0.06997372544922634
plot_id,batch_id 0 81 miss% 0.05788948484773358
plot_id,batch_id 0 82 miss% 0.03360857027074511
plot_id,batch_id 0 83 miss% 0.047903018910826344
plot_id,batch_id 0 84 miss% 0.059198476082227725
plot_id,batch_id 0 85 miss% 0.050412318916895675
plot_id,batch_id 0 86 miss% 0.05849658492045409
plot_id,batch_id 0 87 miss% 0.049598168015453054
plot_id,batch_id 0 88 miss% 0.0715185750467376
plot_id,batch_id 0 89 miss% 0.07956665289150296
plot_id,batch_id 0 90 miss% 0.056195546347811924
plot_id,batch_id 0 91 miss% 0.0738181751808854
plot_id,batch_id 0 92 miss% 0.061619678181784786
plot_id,batch_id 0 93 miss% 0.0660255901485716
plot_id,batch_id 0 94 miss% 0.08094464179172964
plot_id,batch_id 0 95 miss% 0.09746919954649652
plot_id,batch_id 0 96 miss% 0.05354317468937354
plot_id,batch_id 0 97 miss% 0.07974968355178386
plot_id,batch_id 0 98 miss% 0.060231521877520657
plot_id,batch_id 0 99 miss% 0.10854380551736507
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07246488 0.04184029 0.12544242 0.06132795 0.05915441 0.07586682
 0.06608078 0.10721257 0.08983889 0.06808581 0.05364872 0.05589009
 0.07749602 0.05583692 0.07413532 0.0801832  0.11051156 0.04362061
 0.07631315 0.06699028 0.06240871 0.04846208 0.04569748 0.04509907
 0.03985805 0.06033689 0.04795855 0.04491187 0.02499405 0.0373663
 0.03625192 0.06390829 0.13095635 0.06371458 0.09644424 0.06456588
 0.07081651 0.05591509 0.06481809 0.0318438  0.04705181 0.03524604
 0.03271309 0.04899474 0.02714503 0.05424344 0.05641417 0.03298839
 0.02894233 0.04188587 0.19837455 0.04756807 0.03965746 0.04263834
 0.02661219 0.05557185 0.06909612 0.04723229 0.04674424 0.03180468
 0.04142889 0.04198129 0.08720913 0.085479   0.06983822 0.03449697
 0.12646277 0.04721529 0.03703187 0.0848794  0.04143527 0.02899463
 0.07512532 0.10106111 0.14186022 0.0243916  0.08016897 0.05030611
 0.04497323 0.10593358 0.06997373 0.05788948 0.03360857 0.04790302
 0.05919848 0.05041232 0.05849658 0.04959817 0.07151858 0.07956665
 0.05619555 0.07381818 0.06161968 0.06602559 0.08094464 0.0974692
 0.05354317 0.07974968 0.06023152 0.10854381]
for model  14 the mean error 0.06275772642554688
all id 14 hidden_dim 24 learning_rate 0.0025 num_layers 4 frames 21 out win 6 err 0.06275772642554688 time 11461.474925041199
Launcher: Job 15 completed in 11731 seconds.
Launcher: Task 193 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  69649
Epoch:0, Train loss:0.727009, valid loss:0.698631
Epoch:1, Train loss:0.056710, valid loss:0.019010
Epoch:2, Train loss:0.030848, valid loss:0.009721
Epoch:3, Train loss:0.015219, valid loss:0.004682
Epoch:4, Train loss:0.010190, valid loss:0.003633
Epoch:5, Train loss:0.007910, valid loss:0.003037
Epoch:6, Train loss:0.006570, valid loss:0.002726
Epoch:7, Train loss:0.006078, valid loss:0.002812
Epoch:8, Train loss:0.005418, valid loss:0.002198
Epoch:9, Train loss:0.005079, valid loss:0.002412
Epoch:10, Train loss:0.004964, valid loss:0.001985
Epoch:11, Train loss:0.003773, valid loss:0.001776
Epoch:12, Train loss:0.003588, valid loss:0.001745
Epoch:13, Train loss:0.003404, valid loss:0.001773
Epoch:14, Train loss:0.003279, valid loss:0.001578
Epoch:15, Train loss:0.003204, valid loss:0.001673
Epoch:16, Train loss:0.003049, valid loss:0.001570
Epoch:17, Train loss:0.002993, valid loss:0.001605
Epoch:18, Train loss:0.002925, valid loss:0.001599
Epoch:19, Train loss:0.002798, valid loss:0.001371
Epoch:20, Train loss:0.002857, valid loss:0.001760
Epoch:21, Train loss:0.002249, valid loss:0.001322
Epoch:22, Train loss:0.002124, valid loss:0.001289
Epoch:23, Train loss:0.002069, valid loss:0.001303
Epoch:24, Train loss:0.001998, valid loss:0.001383
Epoch:25, Train loss:0.001967, valid loss:0.001364
Epoch:26, Train loss:0.001931, valid loss:0.001254
Epoch:27, Train loss:0.001872, valid loss:0.001259
Epoch:28, Train loss:0.001885, valid loss:0.001220
Epoch:29, Train loss:0.001822, valid loss:0.001227
Epoch:30, Train loss:0.001764, valid loss:0.001145
Epoch:31, Train loss:0.001533, valid loss:0.001114
Epoch:32, Train loss:0.001478, valid loss:0.001179
Epoch:33, Train loss:0.001476, valid loss:0.001120
Epoch:34, Train loss:0.001448, valid loss:0.001153
Epoch:35, Train loss:0.001429, valid loss:0.001187
Epoch:36, Train loss:0.001428, valid loss:0.001087
Epoch:37, Train loss:0.001411, valid loss:0.001081
Epoch:38, Train loss:0.001384, valid loss:0.001106
Epoch:39, Train loss:0.001363, valid loss:0.001115
Epoch:40, Train loss:0.001371, valid loss:0.001239
Epoch:41, Train loss:0.001236, valid loss:0.001036
Epoch:42, Train loss:0.001210, valid loss:0.001027
Epoch:43, Train loss:0.001199, valid loss:0.001099
Epoch:44, Train loss:0.001190, valid loss:0.001106
Epoch:45, Train loss:0.001187, valid loss:0.001120
Epoch:46, Train loss:0.001174, valid loss:0.001049
Epoch:47, Train loss:0.001163, valid loss:0.001036
Epoch:48, Train loss:0.001152, valid loss:0.001034
Epoch:49, Train loss:0.001152, valid loss:0.001034
Epoch:50, Train loss:0.001137, valid loss:0.001061
Epoch:51, Train loss:0.001076, valid loss:0.001045
Epoch:52, Train loss:0.001069, valid loss:0.001054
Epoch:53, Train loss:0.001065, valid loss:0.001042
Epoch:54, Train loss:0.001063, valid loss:0.001044
Epoch:55, Train loss:0.001060, valid loss:0.001042
Epoch:56, Train loss:0.001058, valid loss:0.001044
Epoch:57, Train loss:0.001057, valid loss:0.001046
Epoch:58, Train loss:0.001055, valid loss:0.001044
Epoch:59, Train loss:0.001055, valid loss:0.001037
Epoch:60, Train loss:0.001054, valid loss:0.001037
training time 11509.65824842453
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.08326995403086943
plot_id,batch_id 0 1 miss% 0.0628935328310686
plot_id,batch_id 0 2 miss% 0.06822049761957118
plot_id,batch_id 0 3 miss% 0.03353460396570287
plot_id,batch_id 0 4 miss% 0.05168144670954702
plot_id,batch_id 0 5 miss% 0.05041910231122107
plot_id,batch_id 0 6 miss% 0.049081129435942
plot_id,batch_id 0 7 miss% 0.07951431629553028
plot_id,batch_id 0 8 miss% 0.08651901915296949
plot_id,batch_id 0 9 miss% 0.033753717816105554
plot_id,batch_id 0 10 miss% 0.038107436240897685
plot_id,batch_id 0 11 miss% 0.03820801363807047
plot_id,batch_id 0 12 miss% 0.05900285152027765
plot_id,batch_id 0 13 miss% 0.0857738080636375
plot_id,batch_id 0 14 miss% 0.09276090198411437
plot_id,batch_id 0 15 miss% 0.06322967771402083
plot_id,batch_id 0 16 miss% 0.061521935128529506
plot_id,batch_id 0 17 miss% 0.06875674537450388
plot_id,batch_id 0 18 miss% 0.0568164021768968
plot_id,batch_id 0 19 miss% 0.09284098002167114
plot_id,batch_id 0 20 miss% 0.06085213569342395
plot_id,batch_id 0 21 miss% 0.04018353279680052
plot_id,batch_id 0 22 miss% 0.10528421025397532
plot_id,batch_id 0 23 miss% 0.03561776150366539
plot_id,batch_id 0 24 miss% 0.08032290923491747
plot_id,batch_id 0 25 miss% 0.05461607029952295
plot_id,batch_id 0 26 miss% 0.07446284418341732
plot_id,batch_id 0 27 miss% 0.0554401177977928
plot_id,batch_id 0 28 miss% 0.038479496245021144
plot_id,batch_id 0 29 miss% 0.030684308672052966
plot_id,batch_id 0 30 miss% 0.05946701186608754
plot_id,batch_id 0 31 miss% 0.08120582221603367
plot_id,batch_id 0 32 miss% 0.13166582294394932
plot_id,batch_id 0 33 miss% 0.033214356822534805
plot_id,batch_id 0 34 miss% 0.0375963384910499
plot_id,batch_id 0 35 miss% 0.15013458373671695
plot_id,batch_id 0 36 miss% 0.0995426157522099
plot_id,batch_id 0 37 miss% 0.08543393189292503
plot_id,batch_id 0 38 miss% 0.08659764580825648
plot_id,batch_id 0 39 miss% 0.03779652831145513
plot_id,batch_id 0 40 miss% 0.05208921494457235
plot_id,batch_id 0 41 miss% 0.05930397122832209
plot_id,batch_id 0 42 miss% 0.035017084483623354
plot_id,batch_id 0 43 miss% 0.06626796184942559
plot_id,batch_id 0 44 miss% 0.0319019504555352
plot_id,batch_id 0 45 miss% 0.051876871741692344
plot_id,batch_id 0 46 miss% 0.022859535906544767
plot_id,batch_id 0 47 miss% 0.03893310428288277
plot_id,batch_id 0 48 miss% 0.044680576031735704
plot_id,batch_id 0 49 miss% 0.04004535968177001
plot_id,batch_id 0 50 miss% 0.129867547794401
plot_id,batch_id 0 51 miss% 0.03991091839773208
plot_id,batch_id 0 52 miss% 0.023655618880087433
plot_id,batch_id 0 53 miss% 0.0268576333042153
plot_id,batch_id 0 54 miss% 0.06677192761357573
plot_id,batch_id 0 55 miss% 0.06488039158453797
plot_id,batch_id 0 56 miss% 0.06140486458460441
plot_id,batch_id 0 57 miss% 0.04902151407874776
plot_id,batch_id 0 58 miss% 0.03452049246849032
plot_id,batch_id 0 59 miss% 0.029007625618761603
plot_id,batch_id 0 60 miss% 0.05159221323308632
plot_id,batch_id 0 61 miss% 0.04917156841824829
plot_id,batch_id 0 62 miss% 0.035058903728005186
plot_id,batch_id 0 63 miss% 0.0862267445501248
plot_id,batch_id 0 64 miss% 0.08487229840267689
plot_id,batch_id 0 65 miss% 0.15046059984213836
plot_id,batch_id 0 66 miss% 0.1272974370808517
plot_id,batch_id 0 67 miss% 0.024232733723258498
plot_id,batch_id 0 68 miss% 0.05133338575109483
plot_id,batch_id 0 plot_id,batch_id 0 69 miss% 0.07029636382636552
plot_id,batch_id 0 70 miss% 0.06082013771136922
plot_id,batch_id 0 71 miss% 0.039543303577815256
plot_id,batch_id 0 72 miss% 0.09889490231065895
plot_id,batch_id 0 73 miss% 0.0655810121700345
plot_id,batch_id 0 74 miss% 0.08128198303712247
plot_id,batch_id 0 75 miss% 0.05803660882321839
plot_id,batch_id 0 76 miss% 0.07188820231185417
plot_id,batch_id 0 77 miss% 0.04417174880091616
plot_id,batch_id 0 78 miss% 0.03651763596595162
plot_id,batch_id 0 79 miss% 0.09622340295622063
plot_id,batch_id 0 80 miss% 0.04718085150195286
plot_id,batch_id 0 81 miss% 0.08997629808386481
plot_id,batch_id 0 82 miss% 0.04748409502884554
plot_id,batch_id 0 83 miss% 0.05456562769308051
plot_id,batch_id 0 84 miss% 0.07076607614063982
plot_id,batch_id 0 85 miss% 0.03804453634837856
plot_id,batch_id 0 86 miss% 0.043212120246926465
plot_id,batch_id 0 87 miss% 0.08804999299118589
plot_id,batch_id 0 88 miss% 0.06989183919727963
plot_id,batch_id 0 89 miss% 0.07726895384770754
plot_id,batch_id 0 90 miss% 0.024915648520072827
plot_id,batch_id 0 91 miss% 0.07429757049472446
plot_id,batch_id 0 92 miss% 0.053190167567308246
plot_id,batch_id 0 93 miss% 0.06151125218384457
plot_id,batch_id 0 94 miss% 0.06254556576034927
plot_id,batch_id 0 95 miss% 0.06377747135457344
plot_id,batch_id 0 96 miss% 0.047421019209143975
plot_id,batch_id 0 97 miss% 0.05280475296513041
plot_id,batch_id 0 98 miss% 0.046994414420229204
plot_id,batch_id 0 99 miss% 0.08146158602765052
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.11278759 0.05627959 0.10849659 0.03983127 0.0244099  0.04412851
 0.03328066 0.0918025  0.08401994 0.02510238 0.03718299 0.05074838
 0.04177885 0.04556369 0.07615508 0.03863103 0.04299469 0.06510713
 0.05595059 0.14136765 0.16983625 0.04481146 0.06205937 0.04698499
 0.06480533 0.05111476 0.03700652 0.06291413 0.03203333 0.04392495
 0.06043714 0.12025583 0.0729223  0.06553623 0.03958211 0.02876998
 0.10980361 0.05727278 0.03590073 0.04181012 0.05976345 0.02735028
 0.04653107 0.03329334 0.05671756 0.05589074 0.03653649 0.03126685
 0.04277241 0.04833468 0.13271793 0.03592353 0.04799713 0.04646972
 0.04005681 0.10164326 0.05938795 0.04672826 0.05825695 0.04386085
 0.04116288 0.03156166 0.06423623 0.03812885 0.08633825 0.08464474
 0.13780787 0.01902973 0.0512486  0.07029636 0.06082014 0.0395433
 0.0988949  0.06558101 0.08128198 0.05803661 0.0718882  0.04417175
 0.03651764 0.0962234  0.04718085 0.0899763  0.0474841  0.05456563
 0.07076608 0.03804454 0.04321212 0.08804999 0.06989184 0.07726895
 0.02491565 0.07429757 0.05319017 0.06151125 0.06254557 0.06377747
 0.04742102 0.05280475 0.04699441 0.08146159]
for model  35 the mean error 0.05987674100660484
all id 35 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 21 out win 6 err 0.05987674100660484 time 11466.656998157501
Launcher: Job 36 completed in 11737 seconds.
Launcher: Task 240 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  120401
Epoch:0, Train loss:0.757522, valid loss:0.743895
Epoch:1, Train loss:0.177355, valid loss:0.007185
Epoch:2, Train loss:0.013935, valid loss:0.004511
Epoch:3, Train loss:0.010213, valid loss:0.004048
Epoch:4, Train loss:0.008570, valid loss:0.003175
Epoch:5, Train loss:0.007601, valid loss:0.002766
Epoch:6, Train loss:0.007052, valid loss:0.002452
Epoch:7, Train loss:0.006711, valid loss:0.002564
Epoch:8, Train loss:0.006136, valid loss:0.002236
Epoch:9, Train loss:0.004525, valid loss:0.001582
Epoch:10, Train loss:0.004004, valid loss:0.002175
Epoch:11, Train loss:0.003080, valid loss:0.001913
Epoch:12, Train loss:0.002707, valid loss:0.001417
Epoch:13, Train loss:0.002616, valid loss:0.001396
Epoch:14, Train loss:0.002492, valid loss:0.001252
Epoch:15, Train loss:0.002455, valid loss:0.001293
Epoch:16, Train loss:0.002249, valid loss:0.001618
Epoch:17, Train loss:0.002266, valid loss:0.001150
Epoch:18, Train loss:0.002184, valid loss:0.001137
Epoch:19, Train loss:0.002087, valid loss:0.001125
Epoch:20, Train loss:0.001965, valid loss:0.001311
Epoch:21, Train loss:0.001534, valid loss:0.000997
Epoch:22, Train loss:0.001443, valid loss:0.001120
Epoch:23, Train loss:0.001434, valid loss:0.001119
Epoch:24, Train loss:0.001424, valid loss:0.001170
Epoch:25, Train loss:0.001425, valid loss:0.001103
Epoch:26, Train loss:0.001400, valid loss:0.000931
Epoch:27, Train loss:0.001298, valid loss:0.001137
Epoch:28, Train loss:0.001288, valid loss:0.001073
Epoch:29, Train loss:0.001276, valid loss:0.000969
Epoch:30, Train loss:0.001218, valid loss:0.001015
Epoch:31, Train loss:0.001005, valid loss:0.000870
Epoch:32, Train loss:0.000973, valid loss:0.000847
Epoch:33, Train loss:0.000958, valid loss:0.000907
Epoch:34, Train loss:0.000955, valid loss:0.000966
Epoch:35, Train loss:0.000943, valid loss:0.000850
Epoch:36, Train loss:0.000914, valid loss:0.000842
Epoch:37, Train loss:0.000897, valid loss:0.000823
Epoch:38, Train loss:0.000907, valid loss:0.000815
Epoch:39, Train loss:0.000886, valid loss:0.000905
Epoch:40, Train loss:0.000875, valid loss:0.000825
Epoch:41, Train loss:0.000753, valid loss:0.000794
Epoch:42, Train loss:0.000738, valid loss:0.000786
Epoch:43, Train loss:0.000735, valid loss:0.000824
Epoch:44, Train loss:0.000726, valid loss:0.000826
Epoch:45, Train loss:0.000744, valid loss:0.000778
Epoch:46, Train loss:0.000716, valid loss:0.000787
Epoch:47, Train loss:0.000722, valid loss:0.000835
Epoch:48, Train loss:0.000691, valid loss:0.000814
Epoch:49, Train loss:0.000693, valid loss:0.000799
Epoch:50, Train loss:0.000708, valid loss:0.000816
Epoch:51, Train loss:0.000655, valid loss:0.000802
Epoch:52, Train loss:0.000645, valid loss:0.000802
Epoch:53, Train loss:0.000641, valid loss:0.000802
Epoch:54, Train loss:0.000639, valid loss:0.000797
Epoch:55, Train loss:0.000637, valid loss:0.000798
Epoch:56, Train loss:0.000635, valid loss:0.000802
Epoch:57, Train loss:0.000634, valid loss:0.000797
Epoch:58, Train loss:0.000633, valid loss:0.000801
Epoch:59, Train loss:0.000633, valid loss:0.000796
Epoch:60, Train loss:0.000631, valid loss:0.000800
training time 11534.069331884384
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.10313267538969567
plot_id,batch_id 0 1 miss% 0.07715208354420112
plot_id,batch_id 0 2 miss% 0.1345058902703106
plot_id,batch_id 0 3 miss% 0.06379236137232777
plot_id,batch_id 0 4 miss% 0.046810595433685195
plot_id,batch_id 0 5 miss% 0.08342828071403069
plot_id,batch_id 0 6 miss% 0.04582067057057896
plot_id,batch_id 0 7 miss% 0.09225741225963108
plot_id,batch_id 0 8 miss% 0.05134938535802643
plot_id,batch_id 0 9 miss% 0.04871262446923356
plot_id,batch_id 0 10 miss% 0.07840813551272979
plot_id,batch_id 0 11 miss% 0.06380074334086996
plot_id,batch_id 0 12 miss% 0.07717358702711115
plot_id,batch_id 0 13 miss% 0.08257214241186185
plot_id,batch_id 0 14 miss% 0.09793046446501254
plot_id,batch_id 0 15 miss% 0.0792426697705113
plot_id,batch_id 0 16 miss% 0.11553136698465243
plot_id,batch_id 0 17 miss% 0.06048750301083498
plot_id,batch_id 0 18 miss% 0.05054775640442485
plot_id,batch_id 0 19 miss% 0.06565056932674229
plot_id,batch_id 0 20 miss% 0.12325615907552066
plot_id,batch_id 0 21 miss% 0.06126734400228544
plot_id,batch_id 0 22 miss% 0.08740684420753514
plot_id,batch_id 0 23 miss% 0.048669557197267586
plot_id,batch_id 0 24 miss% 0.048646139096298155
plot_id,batch_id 0 25 miss% 0.06866541319672845
plot_id,batch_id 0 26 miss% 0.05222270504969755
plot_id,batch_id 0 27 miss% 0.04275113540379816
plot_id,batch_id 0 28 miss% 0.030780525110354073
plot_id,batch_id 0 29 miss% 0.050192729376667224
plot_id,batch_id 0 30 miss% 0.0552428783832797
plot_id,batch_id 0 31 miss% 0.07778776120473288
plot_id,batch_id 0 32 miss% 0.12375529195989088
plot_id,batch_id 0 33 miss% 0.05540765755236345
plot_id,batch_id 0 34 miss% 0.03892462870963903
plot_id,batch_id 0 35 miss% 0.030538976857628117
plot_id,batch_id 0 36 miss% 0.10236480549106028
plot_id,batch_id 0 37 miss% 0.06621468474869395
plot_id,batch_id 0 38 miss% 0.055774532905284196
plot_id,batch_id 0 39 miss% 0.060616816006781045
plot_id,batch_id 0 40 miss% 0.05783197610334727
plot_id,batch_id 0 41 miss% 0.035586432594800994
plot_id,batch_id 0 42 miss% 0.018802102451662663
plot_id,batch_id 0 43 miss% 0.0632885933730171
plot_id,batch_id 0 44 miss% 0.04711076843281794
plot_id,batch_id 0 45 miss% 0.026240263563010503
plot_id,batch_id 0 46 miss% 0.04236308825657773
plot_id,batch_id 0 47 miss% 0.03693382782659342
plot_id,batch_id 0 48 miss% 0.05360244729116223
plot_id,batch_id 0 49 miss% 0.0295254285679349
plot_id,batch_id 0 50 miss% 0.11702550889739646
plot_id,batch_id 0 51 miss% 0.03652001592318458
plot_id,batch_id 0 52 miss% 0.03425335586782416
plot_id,batch_id 0 53 miss% 0.029461626978212647
plot_id,batch_id 0 54 miss% 0.030558510116993638
plot_id,batch_id 0 55 miss% 0.06667183735976023
plot_id,batch_id 0 56 miss% 0.06743733773978533
plot_id,batch_id 0 57 miss% 0.04827214640073002
plot_id,batch_id 0 58 miss% 0.030561289771092646
plot_id,batch_id 0 59 miss% 0.03754458755539682
plot_id,batch_id 0 60 miss% 0.06087675054529551
plot_id,batch_id 0 61 miss% 0.06427833545847389
plot_id,batch_id 0 62 miss% 0.06688873999237166
plot_id,batch_id 0 63 miss% 0.07398058807665472
plot_id,batch_id 0 64 miss% 0.04383536995213281
plot_id,batch_id 0 65 miss% 0.09864612894226102
plot_id,batch_id 0 66 miss% 0.06068087555392806
plot_id,batch_id 0 67 miss% 0.052737240946877664
plot_id,batch_id 0 68 miss% 0.05864598525629075
plot_id,batch_id 0 69 miss% 0.06886224744600426
plot_id,batch_id 0 70 miss% 0.09015938089725949
plot_id,batch_id 0 71 miss% 0.054085545689804414
plot_id,batch_id 0 72 miss% 0.08644208377643009
plot_id,batch_id 0 73 miss% 0.10852670340725713
plot_id,batch_id 0 74 miss% 0.15878668820373174
plot_id,batch_id 0 75 miss% 0.04768735156729386
plot_id,batch_id 0 76 miss% 0.15071193477272948
plot_id,batch_id 0 77 miss% 0.08461897067572081
plot_id,batch_id 0 78 miss% 0.038544366437259815
plot_id,batch_id 0 79 miss% 0.05626622978885038
plot_id,batch_id 0 80 miss% 0.08963591174543979
plot_id,batch_id 0 81 miss% 0.09039229217739894
plot_id,batch_id 0 82 miss% 0.0829662703409525
plot_id,batch_id 0 83 miss% 0.0595171263084047
plot_id,batch_id 0 84 miss% 0.049723906857009406
plot_id,batch_id 0 85 miss% 0.05777497111452417
plot_id,batch_id 0 86 miss% 0.05129676528406914
plot_id,batch_id 0 87 miss% 0.07529284686976254
plot_id,batch_id 0 88 miss% 0.05196973122350379
plot_id,batch_id 0 89 miss% 0.0546465618567801
plot_id,batch_id 0 90 miss% 0.03611063060708055
plot_id,batch_id 0 91 miss% 0.12706875132982992
plot_id,batch_id 0 92 miss% 0.03508966190104393
plot_id,batch_id 0 93 miss% 0.04153160669491494
plot_id,batch_id 0 94 miss% 0.0967001281745799
plot_id,batch_id 0 95 miss% 0.06333424131798114
plot_id,batch_id 0 96 miss% 0.06324273719604342
plot_id,batch_id 0 97 miss% 0.062469128123752785
plot_id,batch_id 0 98 miss% 0.04255607501283224
plot_id,batch_id 0 99 miss% 0.03226794384499452
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08326995 0.06289353 0.0682205  0.0335346  0.05168145 0.0504191
 0.04908113 0.07951432 0.08651902 0.03375372 0.03810744 0.03820801
 0.05900285 0.08577381 0.0927609  0.06322968 0.06152194 0.06875675
 0.0568164  0.09284098 0.06085214 0.04018353 0.10528421 0.03561776
 0.08032291 0.05461607 0.07446284 0.05544012 0.0384795  0.03068431
 0.05946701 0.08120582 0.13166582 0.03321436 0.03759634 0.15013458
 0.09954262 0.08543393 0.08659765 0.03779653 0.05208921 0.05930397
 0.03501708 0.06626796 0.03190195 0.05187687 0.02285954 0.0389331
 0.04468058 0.04004536 0.12986755 0.03991092 0.02365562 0.02685763
 0.06677193 0.06488039 0.06140486 0.04902151 0.03452049 0.02900763
 0.05159221 0.04917157 0.0350589  0.08622674 0.0848723  0.1504606
 0.12729744 0.02423273 0.05133339 0.06886225 0.09015938 0.05408555
 0.08644208 0.1085267  0.15878669 0.04768735 0.15071193 0.08461897
 0.03854437 0.05626623 0.08963591 0.09039229 0.08296627 0.05951713
 0.04972391 0.05777497 0.05129677 0.07529285 0.05196973 0.05464656
 0.03611063 0.12706875 0.03508966 0.04153161 0.09670013 0.06333424
 0.06324274 0.06246913 0.04255608 0.03226794]
for model  74 the mean error 0.06471902956856938
all id 74 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 21 out win 6 err 0.06471902956856938 time 11509.65824842453
Launcher: Job 75 completed in 11784 seconds.
Launcher: Task 8 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  154129
Epoch:0, Train loss:0.717944, valid loss:0.727560
Epoch:1, Train loss:0.054462, valid loss:0.004973
Epoch:2, Train loss:0.012023, valid loss:0.003563
Epoch:3, Train loss:0.006957, valid loss:0.002576
Epoch:4, Train loss:0.005375, valid loss:0.002318
Epoch:5, Train loss:0.004799, valid loss:0.002179
Epoch:6, Train loss:0.004218, valid loss:0.001963
Epoch:7, Train loss:0.004136, valid loss:0.002049
Epoch:8, Train loss:0.003668, valid loss:0.001681
Epoch:9, Train loss:0.003573, valid loss:0.001761
Epoch:10, Train loss:0.003241, valid loss:0.001655
Epoch:11, Train loss:0.002327, valid loss:0.001270
Epoch:12, Train loss:0.002135, valid loss:0.001255
Epoch:13, Train loss:0.002130, valid loss:0.001187
Epoch:14, Train loss:0.002128, valid loss:0.001285
Epoch:15, Train loss:0.002171, valid loss:0.001494
Epoch:16, Train loss:0.001957, valid loss:0.001082
Epoch:17, Train loss:0.001894, valid loss:0.000999
Epoch:18, Train loss:0.001808, valid loss:0.001003
Epoch:19, Train loss:0.001867, valid loss:0.001143
Epoch:20, Train loss:0.001737, valid loss:0.001025
Epoch:21, Train loss:0.001249, valid loss:0.000861
Epoch:22, Train loss:0.001213, valid loss:0.000827
Epoch:23, Train loss:0.001204, valid loss:0.000931
Epoch:24, Train loss:0.001206, valid loss:0.000824
Epoch:25, Train loss:0.001153, valid loss:0.000809
Epoch:26, Train loss:0.001130, valid loss:0.000764
Epoch:27, Train loss:0.001095, valid loss:0.000834
Epoch:28, Train loss:0.001110, valid loss:0.000964
Epoch:29, Train loss:0.001158, valid loss:0.000817
Epoch:30, Train loss:0.001099, valid loss:0.000905
Epoch:31, Train loss:0.000845, valid loss:0.000754
Epoch:32, Train loss:0.000801, valid loss:0.000740
Epoch:33, Train loss:0.000771, valid loss:0.000811
Epoch:34, Train loss:0.000780, valid loss:0.000782
Epoch:35, Train loss:0.000757, valid loss:0.000804
Epoch:36, Train loss:0.000750, valid loss:0.000747
Epoch:37, Train loss:0.000753, valid loss:0.000754
Epoch:38, Train loss:0.000743, valid loss:0.000755
Epoch:39, Train loss:0.000719, valid loss:0.000756
Epoch:40, Train loss:0.000735, valid loss:0.000775
Epoch:41, Train loss:0.000612, valid loss:0.000737
Epoch:42, Train loss:0.000589, valid loss:0.000741
Epoch:43, Train loss:0.000589, valid loss:0.000767
Epoch:44, Train loss:0.000586, valid loss:0.000770
Epoch:45, Train loss:0.000586, valid loss:0.000731
Epoch:46, Train loss:0.000588, valid loss:0.000714
Epoch:47, Train loss:0.000577, valid loss:0.000746
Epoch:48, Train loss:0.000580, valid loss:0.000749
Epoch:49, Train loss:0.000564, valid loss:0.000728
Epoch:50, Train loss:0.000555, valid loss:0.000763
Epoch:51, Train loss:0.000517, valid loss:0.000731
Epoch:52, Train loss:0.000512, valid loss:0.000728
Epoch:53, Train loss:0.000510, valid loss:0.000741
Epoch:54, Train loss:0.000508, valid loss:0.000727
Epoch:55, Train loss:0.000507, valid loss:0.000727
Epoch:56, Train loss:0.000505, valid loss:0.000732
Epoch:57, Train loss:0.000505, valid loss:0.000727
Epoch:58, Train loss:0.000504, valid loss:0.000731
Epoch:59, Train loss:0.000503, valid loss:0.000728
Epoch:60, Train loss:0.000503, valid loss:0.000725
training time 11564.651583909988
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.06570233999836994
plot_id,batch_id 0 1 miss% 0.051368873579428646
plot_id,batch_id 0 2 miss% 0.06390025527054069
plot_id,batch_id 0 3 miss% 0.03458034232140171
plot_id,batch_id 0 4 miss% 0.03249774111655698
plot_id,batch_id 0 5 miss% 0.04919921677775104
plot_id,batch_id 0 6 miss% 0.04432394868974709
plot_id,batch_id 0 7 miss% 0.0641513649919162
plot_id,batch_id 0 8 miss% 0.07237979669631686
plot_id,batch_id 0 9 miss% 0.042365208151208096
plot_id,batch_id 0 10 miss% 0.021728035235792746
plot_id,batch_id 0 11 miss% 0.04989500325011675
plot_id,batch_id 0 12 miss% 0.07281396909024596
plot_id,batch_id 0 13 miss% 0.04555304296456127
plot_id,batch_id 0 14 miss% 0.050976827892773476
plot_id,batch_id 0 15 miss% 0.02701226539089872
plot_id,batch_id 0 16 miss% 0.15158637347840137
plot_id,batch_id 0 17 miss% 0.03139745928107224
plot_id,batch_id 0 18 miss% 0.07924600189862936
plot_id,batch_id 0 19 miss% 0.10306020078410251
plot_id,batch_id 0 20 miss% 0.03297266246510681
plot_id,batch_id 0 21 miss% 0.009831640259696111
plot_id,batch_id 0 22 miss% 0.06705494357640354
plot_id,batch_id 0 23 miss% 0.024755962915552557
plot_id,batch_id 0 24 miss% 0.032763550983988136
plot_id,batch_id 0 25 miss% 0.0465305033929755
plot_id,batch_id 0 26 miss% 0.0260944116425904
plot_id,batch_id 0 27 miss% 0.05428459040685034
plot_id,batch_id 0 28 miss% 0.029854859557199406
plot_id,batch_id 0 29 miss% 0.024776859969511913
plot_id,batch_id 0 30 miss% 0.06241475914567343
plot_id,batch_id 0 31 miss% 0.07952930242770301
plot_id,batch_id 0 32 miss% 0.08506553699279311
plot_id,batch_id 0 33 miss% 0.04306452905279915
plot_id,batch_id 0 34 miss% 0.020486883924488036
plot_id,batch_id 0 35 miss% 0.03667504636994366
plot_id,batch_id 0 36 miss% 0.07644105227998425
plot_id,batch_id 0 37 miss% 0.053332298307203596
plot_id,batch_id 0 38 miss% 0.0705561680096313
plot_id,batch_id 0 39 miss% 0.04799810566961856
plot_id,batch_id 0 40 miss% 0.05396122539344833
plot_id,batch_id 0 41 miss% 0.035698733950070484
plot_id,batch_id 0 42 miss% 0.04520289850783406
plot_id,batch_id 0 43 miss% 0.03294185320545108
plot_id,batch_id 0 44 miss% 0.026443033750274732
plot_id,batch_id 0 45 miss% 0.02096765961829641
plot_id,batch_id 0 46 miss% 0.04867318924757775
plot_id,batch_id 0 47 miss% 0.01936990013322157
plot_id,batch_id 0 48 miss% 0.020210428778850476
plot_id,batch_id 0 49 miss% 0.03480782809685229
plot_id,batch_id 0 50 miss% 0.15943194238039196
plot_id,batch_id 0 51 miss% 0.037387016492317376
plot_id,batch_id 0 52 miss% 0.02789402447265232
plot_id,batch_id 0 53 miss% 0.03992639096622968
plot_id,batch_id 0 54 miss% 0.03680515595333264
plot_id,batch_id 0 55 miss% 0.06912960277471704
plot_id,batch_id 0 56 miss% 0.04014518036510672
plot_id,batch_id 0 57 miss% 0.03232404143789144
plot_id,batch_id 0 58 miss% 0.04364913423126241
plot_id,batch_id 0 59 miss% 0.024329337510793022
plot_id,batch_id 0 60 miss% 0.031788965788458134
plot_id,batch_id 0 61 miss% 0.024893725160305427
plot_id,batch_id 0 62 miss% 0.07143081671130881
plot_id,batch_id 0 63 miss% 0.05223056593239169
plot_id,batch_id 0 64 miss% 0.061817475277768576
plot_id,batch_id 0 65 miss% 0.05349937324391338
plot_id,batch_id 0 66 miss% 0.031323489566415084
plot_id,batch_id 0 67 miss% 0.023923318271592206
plot_id,batch_id 0 68 miss% 0.05876270840768541
69 miss% 0.08730514607127937
plot_id,batch_id 0 70 miss% 0.021715782746996027
plot_id,batch_id 0 71 miss% 0.08403805369736597
plot_id,batch_id 0 72 miss% 0.11855733566577431
plot_id,batch_id 0 73 miss% 0.08969938311150523
plot_id,batch_id 0 74 miss% 0.08618455321234436
plot_id,batch_id 0 75 miss% 0.17363494863806272
plot_id,batch_id 0 76 miss% 0.17137531411528356
plot_id,batch_id 0 77 miss% 0.06015435115227153
plot_id,batch_id 0 78 miss% 0.03925618392859833
plot_id,batch_id 0 79 miss% 0.060741091856138116
plot_id,batch_id 0 80 miss% 0.0496428855603978
plot_id,batch_id 0 81 miss% 0.05620393084221212
plot_id,batch_id 0 82 miss% 0.06277885890924127
plot_id,batch_id 0 83 miss% 0.11367790368695548
plot_id,batch_id 0 84 miss% 0.05174825718548973
plot_id,batch_id 0 85 miss% 0.03672694208152634
plot_id,batch_id 0 86 miss% 0.03540857358249941
plot_id,batch_id 0 87 miss% 0.12628699477042643
plot_id,batch_id 0 88 miss% 0.08061336363466026
plot_id,batch_id 0 89 miss% 0.07796168421280956
plot_id,batch_id 0 90 miss% 0.05085823837111903
plot_id,batch_id 0 91 miss% 0.06022480379664725
plot_id,batch_id 0 92 miss% 0.08757115061086294
plot_id,batch_id 0 93 miss% 0.0514911601155932
plot_id,batch_id 0 94 miss% 0.08056220150290673
plot_id,batch_id 0 95 miss% 0.1062984387356549
plot_id,batch_id 0 96 miss% 0.074030134706213
plot_id,batch_id 0 97 miss% 0.04858928447846011
plot_id,batch_id 0 98 miss% 0.03427617843988258
plot_id,batch_id 0 99 miss% 0.041919611466247994
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10313268 0.07715208 0.13450589 0.06379236 0.0468106  0.08342828
 0.04582067 0.09225741 0.05134939 0.04871262 0.07840814 0.06380074
 0.07717359 0.08257214 0.09793046 0.07924267 0.11553137 0.0604875
 0.05054776 0.06565057 0.12325616 0.06126734 0.08740684 0.04866956
 0.04864614 0.06866541 0.05222271 0.04275114 0.03078053 0.05019273
 0.05524288 0.07778776 0.12375529 0.05540766 0.03892463 0.03053898
 0.10236481 0.06621468 0.05577453 0.06061682 0.05783198 0.03558643
 0.0188021  0.06328859 0.04711077 0.02624026 0.04236309 0.03693383
 0.05360245 0.02952543 0.11702551 0.03652002 0.03425336 0.02946163
 0.03055851 0.06667184 0.06743734 0.04827215 0.03056129 0.03754459
 0.06087675 0.06427834 0.06688874 0.07398059 0.04383537 0.09864613
 0.06068088 0.05273724 0.05864599 0.08730515 0.02171578 0.08403805
 0.11855734 0.08969938 0.08618455 0.17363495 0.17137531 0.06015435
 0.03925618 0.06074109 0.04964289 0.05620393 0.06277886 0.1136779
 0.05174826 0.03672694 0.03540857 0.12628699 0.08061336 0.07796168
 0.05085824 0.0602248  0.08757115 0.05149116 0.0805622  0.10629844
 0.07403013 0.04858928 0.03427618 0.04191961]
for model  67 the mean error 0.06608487403852992
all id 67 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 21 out win 5 err 0.06608487403852992 time 11534.069331884384
Launcher: Job 68 completed in 11801 seconds.
Launcher: Task 143 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  69649
Epoch:0, Train loss:0.757601, valid loss:0.730829
Epoch:1, Train loss:0.055357, valid loss:0.007949
Epoch:2, Train loss:0.016603, valid loss:0.004564
Epoch:3, Train loss:0.009341, valid loss:0.003087
Epoch:4, Train loss:0.007616, valid loss:0.002792
Epoch:5, Train loss:0.006879, valid loss:0.002294
Epoch:6, Train loss:0.006327, valid loss:0.002268
Epoch:7, Train loss:0.005824, valid loss:0.002152
Epoch:8, Train loss:0.005498, valid loss:0.002015
Epoch:9, Train loss:0.005413, valid loss:0.001997
Epoch:10, Train loss:0.004290, valid loss:0.001857
Epoch:11, Train loss:0.002729, valid loss:0.001448
Epoch:12, Train loss:0.002563, valid loss:0.001454
Epoch:13, Train loss:0.002506, valid loss:0.001335
Epoch:14, Train loss:0.002440, valid loss:0.001345
Epoch:15, Train loss:0.002358, valid loss:0.001275
Epoch:16, Train loss:0.002509, valid loss:0.001465
Epoch:17, Train loss:0.002271, valid loss:0.001436
Epoch:18, Train loss:0.002210, valid loss:0.001288
Epoch:19, Train loss:0.002042, valid loss:0.001224
Epoch:20, Train loss:0.002129, valid loss:0.001557
Epoch:21, Train loss:0.001608, valid loss:0.001025
Epoch:22, Train loss:0.001544, valid loss:0.000963
Epoch:23, Train loss:0.001593, valid loss:0.001035
Epoch:24, Train loss:0.001498, valid loss:0.001005
Epoch:25, Train loss:0.001495, valid loss:0.001080
Epoch:26, Train loss:0.001470, valid loss:0.001097
Epoch:27, Train loss:0.001418, valid loss:0.000969
Epoch:28, Train loss:0.001414, valid loss:0.001005
Epoch:29, Train loss:0.001408, valid loss:0.001147
Epoch:30, Train loss:0.001390, valid loss:0.000956
Epoch:31, Train loss:0.001142, valid loss:0.000993
Epoch:32, Train loss:0.001118, valid loss:0.000982
Epoch:33, Train loss:0.001096, valid loss:0.000907
Epoch:34, Train loss:0.001088, valid loss:0.000904
Epoch:35, Train loss:0.001084, valid loss:0.000930
Epoch:36, Train loss:0.001048, valid loss:0.000919
Epoch:37, Train loss:0.001058, valid loss:0.000925
Epoch:38, Train loss:0.001052, valid loss:0.000937
Epoch:39, Train loss:0.001049, valid loss:0.000961
Epoch:40, Train loss:0.001061, valid loss:0.000935
Epoch:41, Train loss:0.000906, valid loss:0.000890
Epoch:42, Train loss:0.000893, valid loss:0.000895
Epoch:43, Train loss:0.000888, valid loss:0.000858
Epoch:44, Train loss:0.000894, valid loss:0.000889
Epoch:45, Train loss:0.000878, valid loss:0.000900
Epoch:46, Train loss:0.000878, valid loss:0.000863
Epoch:47, Train loss:0.000869, valid loss:0.000856
Epoch:48, Train loss:0.000858, valid loss:0.000877
Epoch:49, Train loss:0.000870, valid loss:0.000881
Epoch:50, Train loss:0.000857, valid loss:0.000924
Epoch:51, Train loss:0.000814, valid loss:0.000867
Epoch:52, Train loss:0.000798, valid loss:0.000867
Epoch:53, Train loss:0.000791, valid loss:0.000856
Epoch:54, Train loss:0.000788, valid loss:0.000852
Epoch:55, Train loss:0.000785, valid loss:0.000851
Epoch:56, Train loss:0.000783, valid loss:0.000845
Epoch:57, Train loss:0.000782, valid loss:0.000846
Epoch:58, Train loss:0.000781, valid loss:0.000849
Epoch:59, Train loss:0.000779, valid loss:0.000847
Epoch:60, Train loss:0.000778, valid loss:0.000844
training time 11596.43787407875
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08204988307176592
plot_id,batch_id 0 1 miss% 0.028142858345803552
plot_id,batch_id 0 2 miss% 0.11523565552443857
plot_id,batch_id 0 3 miss% 0.04100001665457664
plot_id,batch_id 0 4 miss% 0.05491125471495064
plot_id,batch_id 0 5 miss% 0.09106792584978178
plot_id,batch_id 0 6 miss% 0.08495513344692607
plot_id,batch_id 0 7 miss% 0.07922567863732742
plot_id,batch_id 0 8 miss% 0.09004123410082675
plot_id,batch_id 0 9 miss% 0.040911524697640034
plot_id,batch_id 0 10 miss% 0.02202313480021105
plot_id,batch_id 0 11 miss% 0.09132448400186845
plot_id,batch_id 0 12 miss% 0.04595554721249292
plot_id,batch_id 0 13 miss% 0.09699226707712287
plot_id,batch_id 0 14 miss% 0.05093845280226484
plot_id,batch_id 0 15 miss% 0.03403333901445406
plot_id,batch_id 0 16 miss% 0.06202482746940537
plot_id,batch_id 0 17 miss% 0.02808286430257431
plot_id,batch_id 0 18 miss% 0.09284523956221831
plot_id,batch_id 0 19 miss% 0.06615054797159796
plot_id,batch_id 0 20 miss% 0.05965745218529247
plot_id,batch_id 0 21 miss% 0.035909626551988344
plot_id,batch_id 0 22 miss% 0.07978985331145529
plot_id,batch_id 0 23 miss% 0.023316302819429948
plot_id,batch_id 0 24 miss% 0.040200642196559194
plot_id,batch_id 0 25 miss% 0.052352748860063164
plot_id,batch_id 0 26 miss% 0.05789142169021905
plot_id,batch_id 0 27 miss% 0.0340821084215557
plot_id,batch_id 0 28 miss% 0.022545686982517583
plot_id,batch_id 0 29 miss% 0.027066288038018336
plot_id,batch_id 0 30 miss% 0.041562873732639796
plot_id,batch_id 0 31 miss% 0.08583060138739898
plot_id,batch_id 0 32 miss% 0.05427589352518919
plot_id,batch_id 0 33 miss% 0.034265532227711386
plot_id,batch_id 0 34 miss% 0.036364840409107174
plot_id,batch_id 0 35 miss% 0.06676160617032377
plot_id,batch_id 0 36 miss% 0.0691063358394406
plot_id,batch_id 0 37 miss% 0.0951362570094792
plot_id,batch_id 0 38 miss% 0.04575466488544162
plot_id,batch_id 0 39 miss% 0.023446671577565924
plot_id,batch_id 0 40 miss% 0.10038246573244175
plot_id,batch_id 0 41 miss% 0.03600055709676251
plot_id,batch_id 0 42 miss% 0.026126018318471434
plot_id,batch_id 0 43 miss% 0.09383248506807443
plot_id,batch_id 0 44 miss% 0.0221522134443062
plot_id,batch_id 0 45 miss% 0.04125676379045571
plot_id,batch_id 0 46 miss% 0.022258395512772398
plot_id,batch_id 0 47 miss% 0.023988953056749432
plot_id,batch_id 0 48 miss% 0.02279346262947231
plot_id,batch_id 0 49 miss% 0.032422175949941665
plot_id,batch_id 0 50 miss% 0.12500454961054966
plot_id,batch_id 0 51 miss% 0.02691390374756414
plot_id,batch_id 0 52 miss% 0.026145417480991043
plot_id,batch_id 0 53 miss% 0.029212220939087635
plot_id,batch_id 0 54 miss% 0.03359450474932408
plot_id,batch_id 0 55 miss% 0.12735490195365665
plot_id,batch_id 0 56 miss% 0.04153367312755384
plot_id,batch_id 0 57 miss% 0.03378486543297782
plot_id,batch_id 0 58 miss% 0.031159178090033934
plot_id,batch_id 0 59 miss% 0.02127469563833192
plot_id,batch_id 0 60 miss% 0.0681170744922796
plot_id,batch_id 0 61 miss% 0.05356134542380129
plot_id,batch_id 0 62 miss% 0.07023571455871727
plot_id,batch_id 0 63 miss% 0.035660768492909244
plot_id,batch_id 0 64 miss% 0.0641671064007979
plot_id,batch_id 0 65 miss% 0.07037098583112592
plot_id,batch_id 0 66 miss% 0.15076547553586364
plot_id,batch_id 0 67 miss% 0.04585326441594975
plot_id,batch_id 0 68 miss% 0.0470865198024792
plot_id,batch_id 0 plot_id,batch_id 0 69 miss% 0.034014775489181345
plot_id,batch_id 0 70 miss% 0.06239755043224282
plot_id,batch_id 0 71 miss% 0.028403873962297658
plot_id,batch_id 0 72 miss% 0.0890685855030016
plot_id,batch_id 0 73 miss% 0.03697568485219116
plot_id,batch_id 0 74 miss% 0.08068607558940293
plot_id,batch_id 0 75 miss% 0.0941282215814725
plot_id,batch_id 0 76 miss% 0.07629324357661908
plot_id,batch_id 0 77 miss% 0.0465319392866781
plot_id,batch_id 0 78 miss% 0.04207934222350704
plot_id,batch_id 0 79 miss% 0.09299973753331772
plot_id,batch_id 0 80 miss% 0.0733682108973585
plot_id,batch_id 0 81 miss% 0.09737578319691721
plot_id,batch_id 0 82 miss% 0.050333426838984396
plot_id,batch_id 0 83 miss% 0.08071180372831184
plot_id,batch_id 0 84 miss% 0.10391285871306649
plot_id,batch_id 0 85 miss% 0.04662735895376724
plot_id,batch_id 0 86 miss% 0.051455890585109104
plot_id,batch_id 0 87 miss% 0.09211017834258328
plot_id,batch_id 0 88 miss% 0.07632410213795592
plot_id,batch_id 0 89 miss% 0.057862093927049725
plot_id,batch_id 0 90 miss% 0.03773876605953867
plot_id,batch_id 0 91 miss% 0.06393410040268291
plot_id,batch_id 0 92 miss% 0.08858915103651527
plot_id,batch_id 0 93 miss% 0.026991498166075927
plot_id,batch_id 0 94 miss% 0.08240067517990081
plot_id,batch_id 0 95 miss% 0.07689977833147954
plot_id,batch_id 0 96 miss% 0.05171297708671459
plot_id,batch_id 0 97 miss% 0.07001072548117679
plot_id,batch_id 0 98 miss% 0.017448069018074764
plot_id,batch_id 0 99 miss% 0.06949748949267742
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06570234 0.05136887 0.06390026 0.03458034 0.03249774 0.04919922
 0.04432395 0.06415136 0.0723798  0.04236521 0.02172804 0.049895
 0.07281397 0.04555304 0.05097683 0.02701227 0.15158637 0.03139746
 0.079246   0.1030602  0.03297266 0.00983164 0.06705494 0.02475596
 0.03276355 0.0465305  0.02609441 0.05428459 0.02985486 0.02477686
 0.06241476 0.0795293  0.08506554 0.04306453 0.02048688 0.03667505
 0.07644105 0.0533323  0.07055617 0.04799811 0.05396123 0.03569873
 0.0452029  0.03294185 0.02644303 0.02096766 0.04867319 0.0193699
 0.02021043 0.03480783 0.15943194 0.03738702 0.02789402 0.03992639
 0.03680516 0.0691296  0.04014518 0.03232404 0.04364913 0.02432934
 0.03178897 0.02489373 0.07143082 0.05223057 0.06181748 0.05349937
 0.03132349 0.02392332 0.05876271 0.03401478 0.06239755 0.02840387
 0.08906859 0.03697568 0.08068608 0.09412822 0.07629324 0.04653194
 0.04207934 0.09299974 0.07336821 0.09737578 0.05033343 0.0807118
 0.10391286 0.04662736 0.05145589 0.09211018 0.0763241  0.05786209
 0.03773877 0.0639341  0.08858915 0.0269915  0.08240068 0.07689978
 0.05171298 0.07001073 0.01744807 0.06949749]
for model  61 the mean error 0.05336074917441807
all id 61 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 21 out win 5 err 0.05336074917441807 time 11564.651583909988
Launcher: Job 62 completed in 11835 seconds.
Launcher: Task 141 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  41745
Epoch:0, Train loss:0.476787, valid loss:0.466052
Epoch:1, Train loss:0.022216, valid loss:0.002291
Epoch:2, Train loss:0.004389, valid loss:0.001703
Epoch:3, Train loss:0.003420, valid loss:0.001502
Epoch:4, Train loss:0.002883, valid loss:0.001263
Epoch:5, Train loss:0.002599, valid loss:0.001275
Epoch:6, Train loss:0.002350, valid loss:0.001159
Epoch:7, Train loss:0.002160, valid loss:0.000972
Epoch:8, Train loss:0.002011, valid loss:0.000868
Epoch:9, Train loss:0.001892, valid loss:0.001048
Epoch:10, Train loss:0.001786, valid loss:0.000883
Epoch:11, Train loss:0.001465, valid loss:0.000748
Epoch:12, Train loss:0.001411, valid loss:0.000728
Epoch:13, Train loss:0.001374, valid loss:0.000751
Epoch:14, Train loss:0.001349, valid loss:0.000747
Epoch:15, Train loss:0.001302, valid loss:0.000761
Epoch:16, Train loss:0.001268, valid loss:0.000666
Epoch:17, Train loss:0.001207, valid loss:0.000732
Epoch:18, Train loss:0.001217, valid loss:0.000651
Epoch:19, Train loss:0.001178, valid loss:0.000647
Epoch:20, Train loss:0.001163, valid loss:0.000639
Epoch:21, Train loss:0.000998, valid loss:0.000563
Epoch:22, Train loss:0.000984, valid loss:0.000551
Epoch:23, Train loss:0.000979, valid loss:0.000572
Epoch:24, Train loss:0.000957, valid loss:0.000576
Epoch:25, Train loss:0.000938, valid loss:0.000585
Epoch:26, Train loss:0.000945, valid loss:0.000573
Epoch:27, Train loss:0.000925, valid loss:0.000578
Epoch:28, Train loss:0.000914, valid loss:0.000567
Epoch:29, Train loss:0.000893, valid loss:0.000596
Epoch:30, Train loss:0.000900, valid loss:0.000529
Epoch:31, Train loss:0.000808, valid loss:0.000522
Epoch:32, Train loss:0.000791, valid loss:0.000530
Epoch:33, Train loss:0.000797, valid loss:0.000509
Epoch:34, Train loss:0.000782, valid loss:0.000509
Epoch:35, Train loss:0.000786, valid loss:0.000526
Epoch:36, Train loss:0.000774, valid loss:0.000503
Epoch:37, Train loss:0.000769, valid loss:0.000493
Epoch:38, Train loss:0.000777, valid loss:0.000545
Epoch:39, Train loss:0.000759, valid loss:0.000514
Epoch:40, Train loss:0.000766, valid loss:0.000495
Epoch:41, Train loss:0.000714, valid loss:0.000491
Epoch:42, Train loss:0.000713, valid loss:0.000508
Epoch:43, Train loss:0.000705, valid loss:0.000483
Epoch:44, Train loss:0.000703, valid loss:0.000506
Epoch:45, Train loss:0.000700, valid loss:0.000484
Epoch:46, Train loss:0.000699, valid loss:0.000494
Epoch:47, Train loss:0.000693, valid loss:0.000500
Epoch:48, Train loss:0.000698, valid loss:0.000513
Epoch:49, Train loss:0.000693, valid loss:0.000484
Epoch:50, Train loss:0.000688, valid loss:0.000496
Epoch:51, Train loss:0.000659, valid loss:0.000492
Epoch:52, Train loss:0.000656, valid loss:0.000500
Epoch:53, Train loss:0.000655, valid loss:0.000491
Epoch:54, Train loss:0.000654, valid loss:0.000495
Epoch:55, Train loss:0.000654, valid loss:0.000486
Epoch:56, Train loss:0.000654, valid loss:0.000491
Epoch:57, Train loss:0.000654, valid loss:0.000486
Epoch:58, Train loss:0.000653, valid loss:0.000489
Epoch:59, Train loss:0.000653, valid loss:0.000491
Epoch:60, Train loss:0.000652, valid loss:0.000487
training time 11622.305815935135
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.1097634858260041
plot_id,batch_id 0 1 miss% 0.05460837195465788
plot_id,batch_id 0 2 miss% 0.12967691794803382
plot_id,batch_id 0 3 miss% 0.06293459075592638
plot_id,batch_id 0 4 miss% 0.11106355358206729
plot_id,batch_id 0 5 miss% 0.04949552873025259
plot_id,batch_id 0 6 miss% 0.07483731518516498
plot_id,batch_id 0 7 miss% 0.11558263219362347
plot_id,batch_id 0 8 miss% 0.1076825653311947
plot_id,batch_id 0 9 miss% 0.07054209923184507
plot_id,batch_id 0 10 miss% 0.04683288452298771
plot_id,batch_id 0 11 miss% 0.06162751506738709
plot_id,batch_id 0 12 miss% 0.09052279214881595
plot_id,batch_id 0 13 miss% 0.04514374867909778
plot_id,batch_id 0 14 miss% 0.12705402842292207
plot_id,batch_id 0 15 miss% 0.07424192830632671
plot_id,batch_id 0 16 miss% 0.19483770019269447
plot_id,batch_id 0 17 miss% 0.04014838417527927
plot_id,batch_id 0 18 miss% 0.07427300382815322
plot_id,batch_id 0 19 miss% 0.0905277880608451
plot_id,batch_id 0 20 miss% 0.09519981166398157
plot_id,batch_id 0 21 miss% 0.07914220739307636
plot_id,batch_id 0 22 miss% 0.05582223150368873
plot_id,batch_id 0 23 miss% 0.06257662131745093
plot_id,batch_id 0 24 miss% 0.10387418468733732
plot_id,batch_id 0 25 miss% 0.13193937291219562
plot_id,batch_id 0 26 miss% 0.08654183789941554
plot_id,batch_id 0 27 miss% 0.07836599555851359
plot_id,batch_id 0 28 miss% 0.06532381327884959
plot_id,batch_id 0 29 miss% 0.04047891821139076
plot_id,batch_id 0 30 miss% 0.043082550237459945
plot_id,batch_id 0 31 miss% 0.10884828680716847
plot_id,batch_id 0 32 miss% 0.14016364056835778
plot_id,batch_id 0 33 miss% 0.10260977951214652
plot_id,batch_id 0 34 miss% 0.057641329655974294
plot_id,batch_id 0 35 miss% 0.07451624186280992
plot_id,batch_id 0 36 miss% 0.13170274028468912
plot_id,batch_id 0 37 miss% 0.08888769944506855
plot_id,batch_id 0 38 miss% 0.050806065340315835
plot_id,batch_id 0 39 miss% 0.05422112501555128
plot_id,batch_id 0 40 miss% 0.12042443222755383
plot_id,batch_id 0 41 miss% 0.08452404690915474
plot_id,batch_id 0 42 miss% 0.04934323020909234
plot_id,batch_id 0 43 miss% 0.042567588942622255
plot_id,batch_id 0 44 miss% 0.056317681388542366
plot_id,batch_id 0 45 miss% 0.046899760265251375
plot_id,batch_id 0 46 miss% 0.07314303142754765
plot_id,batch_id 0 47 miss% 0.03180056872022193
plot_id,batch_id 0 48 miss% 0.036518689420579904
plot_id,batch_id 0 49 miss% 0.03489937266981597
plot_id,batch_id 0 50 miss% 0.15224180406126148
plot_id,batch_id 0 51 miss% 0.01859046340914258
plot_id,batch_id 0 52 miss% 0.03842789787732502
plot_id,batch_id 0 53 miss% 0.03712070561277093
plot_id,batch_id 0 54 miss% 0.05213327496872429
plot_id,batch_id 0 55 miss% 0.04324388713398183
plot_id,batch_id 0 56 miss% 0.1790548750609844
plot_id,batch_id 0 57 miss% 0.075034618705367
plot_id,batch_id 0 58 miss% 0.04404232331742596
plot_id,batch_id 0 59 miss% 0.06039305643786246
plot_id,batch_id 0 60 miss% 0.05618367134329507
plot_id,batch_id 0 61 miss% 0.04282222566100637
plot_id,batch_id 0 62 miss% 0.077420200566509
plot_id,batch_id 0 63 miss% 0.04294878111855842
plot_id,batch_id 0 64 miss% 0.07707525673692457
plot_id,batch_id 0 65 miss% 0.13991960142154922
69 miss% 0.08176927530991371
plot_id,batch_id 0 70 miss% 0.06444741063114656
plot_id,batch_id 0 71 miss% 0.04882064677879048
plot_id,batch_id 0 72 miss% 0.1431276459857923
plot_id,batch_id 0 73 miss% 0.0641834466934404
plot_id,batch_id 0 74 miss% 0.18862970340895746
plot_id,batch_id 0 75 miss% 0.17072156513540088
plot_id,batch_id 0 76 miss% 0.10347612356374025
plot_id,batch_id 0 77 miss% 0.04759880611821664
plot_id,batch_id 0 78 miss% 0.05860601793219916
plot_id,batch_id 0 79 miss% 0.09322612000177102
plot_id,batch_id 0 80 miss% 0.038157004952949145
plot_id,batch_id 0 81 miss% 0.08530157115560388
plot_id,batch_id 0 82 miss% 0.04816452595801535
plot_id,batch_id 0 83 miss% 0.08479114977237437
plot_id,batch_id 0 84 miss% 0.0790443613787407
plot_id,batch_id 0 85 miss% 0.036535303626651056
plot_id,batch_id 0 86 miss% 0.03185648510882317
plot_id,batch_id 0 87 miss% 0.06434962615576527
plot_id,batch_id 0 88 miss% 0.08510243208540415
plot_id,batch_id 0 89 miss% 0.05476749046227122
plot_id,batch_id 0 90 miss% 0.06467569833702905
plot_id,batch_id 0 91 miss% 0.04665189963820629
plot_id,batch_id 0 92 miss% 0.07220438700456185
plot_id,batch_id 0 93 miss% 0.03573878952287733
plot_id,batch_id 0 94 miss% 0.09483554332753588
plot_id,batch_id 0 95 miss% 0.15917140478800337
plot_id,batch_id 0 96 miss% 0.06469411770226999
plot_id,batch_id 0 97 miss% 0.04871390145013644
plot_id,batch_id 0 98 miss% 0.04865098157997007
plot_id,batch_id 0 99 miss% 0.06941963401263444
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08204988 0.02814286 0.11523566 0.04100002 0.05491125 0.09106793
 0.08495513 0.07922568 0.09004123 0.04091152 0.02202313 0.09132448
 0.04595555 0.09699227 0.05093845 0.03403334 0.06202483 0.02808286
 0.09284524 0.06615055 0.05965745 0.03590963 0.07978985 0.0233163
 0.04020064 0.05235275 0.05789142 0.03408211 0.02254569 0.02706629
 0.04156287 0.0858306  0.05427589 0.03426553 0.03636484 0.06676161
 0.06910634 0.09513626 0.04575466 0.02344667 0.10038247 0.03600056
 0.02612602 0.09383249 0.02215221 0.04125676 0.0222584  0.02398895
 0.02279346 0.03242218 0.12500455 0.0269139  0.02614542 0.02921222
 0.0335945  0.1273549  0.04153367 0.03378487 0.03115918 0.0212747
 0.06811707 0.05356135 0.07023571 0.03566077 0.06416711 0.07037099
 0.15076548 0.04585326 0.04708652 0.08176928 0.06444741 0.04882065
 0.14312765 0.06418345 0.1886297  0.17072157 0.10347612 0.04759881
 0.05860602 0.09322612 0.038157   0.08530157 0.04816453 0.08479115
 0.07904436 0.0365353  0.03185649 0.06434963 0.08510243 0.05476749
 0.0646757  0.0466519  0.07220439 0.03573879 0.09483554 0.1591714
 0.06469412 0.0487139  0.04865098 0.06941963]
for model  73 the mean error 0.06183672002982279
all id 73 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 21 out win 5 err 0.06183672002982279 time 11596.43787407875
Launcher: Job 74 completed in 11865 seconds.
Launcher: Task 32 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  69649
Epoch:0, Train loss:0.727009, valid loss:0.698631
Epoch:1, Train loss:0.077240, valid loss:0.013757
Epoch:2, Train loss:0.023812, valid loss:0.006212
Epoch:3, Train loss:0.015613, valid loss:0.004554
Epoch:4, Train loss:0.013840, valid loss:0.004293
Epoch:5, Train loss:0.013102, valid loss:0.004142
Epoch:6, Train loss:0.012554, valid loss:0.003918
Epoch:7, Train loss:0.012142, valid loss:0.003614
Epoch:8, Train loss:0.009554, valid loss:0.002644
Epoch:9, Train loss:0.007255, valid loss:0.002122
Epoch:10, Train loss:0.004331, valid loss:0.002206
Epoch:11, Train loss:0.003520, valid loss:0.001794
Epoch:12, Train loss:0.003359, valid loss:0.001654
Epoch:13, Train loss:0.003173, valid loss:0.001660
Epoch:14, Train loss:0.003033, valid loss:0.001581
Epoch:15, Train loss:0.002935, valid loss:0.001665
Epoch:16, Train loss:0.002793, valid loss:0.001447
Epoch:17, Train loss:0.002778, valid loss:0.001472
Epoch:18, Train loss:0.002642, valid loss:0.001416
Epoch:19, Train loss:0.002526, valid loss:0.001304
Epoch:20, Train loss:0.002415, valid loss:0.001330
Epoch:21, Train loss:0.002110, valid loss:0.001215
Epoch:22, Train loss:0.002068, valid loss:0.001245
Epoch:23, Train loss:0.002016, valid loss:0.001138
Epoch:24, Train loss:0.001994, valid loss:0.001220
Epoch:25, Train loss:0.001967, valid loss:0.001096
Epoch:26, Train loss:0.001918, valid loss:0.001161
Epoch:27, Train loss:0.001886, valid loss:0.001129
Epoch:28, Train loss:0.001870, valid loss:0.001128
Epoch:29, Train loss:0.001826, valid loss:0.001068
Epoch:30, Train loss:0.001793, valid loss:0.001154
Epoch:31, Train loss:0.001624, valid loss:0.001088
Epoch:32, Train loss:0.001598, valid loss:0.001032
Epoch:33, Train loss:0.001580, valid loss:0.001043
Epoch:34, Train loss:0.001568, valid loss:0.001078
Epoch:35, Train loss:0.001551, valid loss:0.001054
Epoch:36, Train loss:0.001538, valid loss:0.001043
Epoch:37, Train loss:0.001523, valid loss:0.001013
Epoch:38, Train loss:0.001500, valid loss:0.001044
Epoch:39, Train loss:0.001491, valid loss:0.001039
Epoch:40, Train loss:0.001473, valid loss:0.001036
Epoch:41, Train loss:0.001391, valid loss:0.001033
Epoch:42, Train loss:0.001379, valid loss:0.000978
Epoch:43, Train loss:0.001365, valid loss:0.000963
Epoch:44, Train loss:0.001365, valid loss:0.000999
Epoch:45, Train loss:0.001353, valid loss:0.001001
Epoch:46, Train loss:0.001348, valid loss:0.001047
Epoch:47, Train loss:0.001344, valid loss:0.000986
Epoch:48, Train loss:0.001331, valid loss:0.000985
Epoch:49, Train loss:0.001325, valid loss:0.000952
Epoch:50, Train loss:0.001319, valid loss:0.000956
Epoch:51, Train loss:0.001263, valid loss:0.000964
Epoch:52, Train loss:0.001253, valid loss:0.000969
Epoch:53, Train loss:0.001249, valid loss:0.000951
Epoch:54, Train loss:0.001247, valid loss:0.000953
Epoch:55, Train loss:0.001246, valid loss:0.000952
Epoch:56, Train loss:0.001245, valid loss:0.000945
Epoch:57, Train loss:0.001245, valid loss:0.000957
Epoch:58, Train loss:0.001242, valid loss:0.000960
Epoch:59, Train loss:0.001244, valid loss:0.000959
Epoch:60, Train loss:0.001243, valid loss:0.000947
training time 11661.413145780563
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.11757578603933228
plot_id,batch_id 0 1 miss% 0.04343577060845207
plot_id,batch_id 0 2 miss% 0.08037957478861313
plot_id,batch_id 0 3 miss% 0.0604284430460655
plot_id,batch_id 0 4 miss% 0.0992369511723743
plot_id,batch_id 0 5 miss% 0.09991597699151437
plot_id,batch_id 0 6 miss% 0.06792206712558853
plot_id,batch_id 0 7 miss% 0.07389622424782401
plot_id,batch_id 0 8 miss% 0.08250463269528827
plot_id,batch_id 0 9 miss% 0.05585765242690045
plot_id,batch_id 0 10 miss% 0.03284697486702973
plot_id,batch_id 0 11 miss% 0.0827077948324075
plot_id,batch_id 0 12 miss% 0.03610082194507648
plot_id,batch_id 0 13 miss% 0.07070624401151006
plot_id,batch_id 0 14 miss% 0.10632416700186237
plot_id,batch_id 0 15 miss% 0.04882405360753714
plot_id,batch_id 0 16 miss% 0.0882860896770108
plot_id,batch_id 0 17 miss% 0.04231300879648092
plot_id,batch_id 0 18 miss% 0.07424018013530499
plot_id,batch_id 0 19 miss% 0.09381347101518443
plot_id,batch_id 0 20 miss% 0.06658536467814817
plot_id,batch_id 0 21 miss% 0.04791273997790183
plot_id,batch_id 0 22 miss% 0.03795960721361874
plot_id,batch_id 0 23 miss% 0.023215799366151552
plot_id,batch_id 0 24 miss% 0.028670379559473998
plot_id,batch_id 0 25 miss% 0.07499084632399733
plot_id,batch_id 0 26 miss% 0.06132138799560861
plot_id,batch_id 0 27 miss% 0.04796960160761234
plot_id,batch_id 0 28 miss% 0.03178401526773875
plot_id,batch_id 0 29 miss% 0.029503801599468304
plot_id,batch_id 0 30 miss% 0.025691107727195246
plot_id,batch_id 0 31 miss% 0.09627870970189895
plot_id,batch_id 0 32 miss% 0.08782878700145572
plot_id,batch_id 0 33 miss% 0.05594414999818682
plot_id,batch_id 0 34 miss% 0.06202775932511679
plot_id,batch_id 0 35 miss% 0.08910690202592994
plot_id,batch_id 0 36 miss% 0.11176260564838994
plot_id,batch_id 0 37 miss% 0.06602513026044336
plot_id,batch_id 0 38 miss% 0.04306612055340778
plot_id,batch_id 0 39 miss% 0.02800266383363147
plot_id,batch_id 0 40 miss% 0.04999105966525246
plot_id,batch_id 0 41 miss% 0.023968682658076144
plot_id,batch_id 0 42 miss% 0.02034845254866633
plot_id,batch_id 0 43 miss% 0.051784295246964665
plot_id,batch_id 0 44 miss% 0.0562652314899214
plot_id,batch_id 0 45 miss% 0.03717917572433423
plot_id,batch_id 0 46 miss% 0.032765378186942834
plot_id,batch_id 0 47 miss% 0.031069661513172874
plot_id,batch_id 0 48 miss% 0.02510810357566228
plot_id,batch_id 0 49 miss% 0.03273348111356647
plot_id,batch_id 0 50 miss% 0.14316711802867949
plot_id,batch_id 0 51 miss% 0.059101728451427055
plot_id,batch_id 0 52 miss% 0.029304840011861653
plot_id,batch_id 0 53 miss% 0.05603799539123222
plot_id,batch_id 0 54 miss% 0.03345100038631997
plot_id,batch_id 0 55 miss% 0.09080353870597799
plot_id,batch_id 0 56 miss% 0.06963996517828952
plot_id,batch_id 0 57 miss% 0.03077958820930559
plot_id,batch_id 0 58 miss% 0.053819009237351216
plot_id,batch_id 0 59 miss% 0.023278742411215112
plot_id,batch_id 0 60 miss% 0.028642230493777267
plot_id,batch_id 0 61 miss% 0.032368589955407
plot_id,batch_id 0 62 miss% 0.03781321802115519
plot_id,batch_id 0 63 miss% 0.05653350446300751
plot_id,batch_id 0 64 miss% 0.070533731447685
plot_id,batch_id 0 65 miss% 0.08772318839695328
plot_id,batch_id 0 66 miss% 0.11432495324412584
plot_id,batch_id 0 67 miss% 0.04381506981581835
plot_id,batch_id 0 68 miss% 0.05285664060399881
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  89105
Epoch:0, Train loss:0.536096, valid loss:0.493196
Epoch:1, Train loss:0.094761, valid loss:0.004307
Epoch:2, Train loss:0.007983, valid loss:0.003103
Epoch:3, Train loss:0.005372, valid loss:0.002202
Epoch:4, Train loss:0.004588, valid loss:0.002117
Epoch:5, Train loss:0.004100, valid loss:0.001882
Epoch:6, Train loss:0.003750, valid loss:0.001585
Epoch:7, Train loss:0.003322, valid loss:0.001450
Epoch:8, Train loss:0.003249, valid loss:0.001596
Epoch:9, Train loss:0.002904, valid loss:0.001514
Epoch:10, Train loss:0.002672, valid loss:0.001351
Epoch:11, Train loss:0.001967, valid loss:0.000975
Epoch:12, Train loss:0.001900, valid loss:0.000986
Epoch:13, Train loss:0.001807, valid loss:0.001130
Epoch:14, Train loss:0.001812, valid loss:0.000951
Epoch:15, Train loss:0.001702, valid loss:0.001093
Epoch:16, Train loss:0.001711, valid loss:0.001002
Epoch:17, Train loss:0.001595, valid loss:0.000973
Epoch:18, Train loss:0.001621, valid loss:0.000798
Epoch:19, Train loss:0.001524, valid loss:0.000986
Epoch:20, Train loss:0.001491, valid loss:0.001007
Epoch:21, Train loss:0.001152, valid loss:0.000724
Epoch:22, Train loss:0.001117, valid loss:0.000729
Epoch:23, Train loss:0.001078, valid loss:0.000765
Epoch:24, Train loss:0.001089, valid loss:0.000746
Epoch:25, Train loss:0.001064, valid loss:0.000753
Epoch:26, Train loss:0.001052, valid loss:0.000695
Epoch:27, Train loss:0.001024, valid loss:0.000727
Epoch:28, Train loss:0.000990, valid loss:0.000733
Epoch:29, Train loss:0.001006, valid loss:0.000734
Epoch:30, Train loss:0.000959, valid loss:0.000641
Epoch:31, Train loss:0.000781, valid loss:0.000618
Epoch:32, Train loss:0.000778, valid loss:0.000566
Epoch:33, Train loss:0.000759, valid loss:0.000613
Epoch:34, Train loss:0.000761, valid loss:0.000620
Epoch:35, Train loss:0.000760, valid loss:0.000609
Epoch:36, Train loss:0.000734, valid loss:0.000600
Epoch:37, Train loss:0.000743, valid loss:0.000563
Epoch:38, Train loss:0.000719, valid loss:0.000645
Epoch:39, Train loss:0.000735, valid loss:0.000607
Epoch:40, Train loss:0.000697, valid loss:0.000616
Epoch:41, Train loss:0.000624, valid loss:0.000566
Epoch:42, Train loss:0.000610, valid loss:0.000544
Epoch:43, Train loss:0.000605, valid loss:0.000576
Epoch:44, Train loss:0.000598, valid loss:0.000551
Epoch:45, Train loss:0.000607, valid loss:0.000647
Epoch:46, Train loss:0.000603, valid loss:0.000555
Epoch:47, Train loss:0.000589, valid loss:0.000557
Epoch:48, Train loss:0.000578, valid loss:0.000564
Epoch:49, Train loss:0.000584, valid loss:0.000539
Epoch:50, Train loss:0.000587, valid loss:0.000581
Epoch:51, Train loss:0.000550, valid loss:0.000546
Epoch:52, Train loss:0.000542, valid loss:0.000541
Epoch:53, Train loss:0.000539, valid loss:0.000544
Epoch:54, Train loss:0.000537, valid loss:0.000541
Epoch:55, Train loss:0.000535, valid loss:0.000543
Epoch:56, Train loss:0.000535, valid loss:0.000541
Epoch:57, Train loss:0.000534, valid loss:0.000543
Epoch:58, Train loss:0.000533, valid loss:0.000543
Epoch:59, Train loss:0.000532, valid loss:0.000538
Epoch:60, Train loss:0.000532, valid loss:0.000541
training time 11688.882124900818
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.38627795440756163
plot_id,batch_id 0 1 miss% 0.3847431841545931
plot_id,batch_id 0 2 miss% 0.3812921464604615
plot_id,batch_id 0 3 miss% 0.3365899962460921
plot_id,batch_id 0 4 miss% 0.33304781336242545
plot_id,batch_id 0 5 miss% 0.39648977033841293
plot_id,batch_id 0 6 miss% 0.36730823849303884
plot_id,batch_id 0 7 miss% 0.48761770359820944
plot_id,batch_id 0 8 miss% 0.5952341888764222
plot_id,batch_id 0 9 miss% 0.3798141875117609
plot_id,batch_id 0 10 miss% 0.34104500389498155
plot_id,batch_id 0 11 miss% 0.3727556617692133
plot_id,batch_id 0 12 miss% 0.5068095280591478
plot_id,batch_id 0 13 miss% 0.3344234128635851
plot_id,batch_id 0 14 miss% 0.4195809498766384
plot_id,batch_id 0 15 miss% 0.3576586618133794
plot_id,batch_id 0 16 miss% 0.4692759003160413
plot_id,batch_id 0 17 miss% 0.4348694749888433
plot_id,batch_id 0 18 miss% 0.46275395423201393
plot_id,batch_id 0 19 miss% 0.3648392157716303
plot_id,batch_id 0 20 miss% 0.4262207954998628
plot_id,batch_id 0 21 miss% 0.42527174975078574
plot_id,batch_id 0 22 miss% 0.34341363591743407
plot_id,batch_id 0 23 miss% 0.4317459936736748
plot_id,batch_id 0 24 miss% 0.4070638695610788
plot_id,batch_id 0 25 miss% 0.3661558901279214
plot_id,batch_id 0 26 miss% 0.3529793863418173
plot_id,batch_id 0 27 miss% 0.3808560907716079
plot_id,batch_id 0 28 miss% 0.3395927249830256
plot_id,batch_id 0 29 miss% 0.30496323410291726
plot_id,batch_id 0 30 miss% 0.3215837362145884
plot_id,batch_id 0 31 miss% 0.5384052617735314
plot_id,batch_id 0 32 miss% 0.464731664566149
plot_id,batch_id 0 33 miss% 0.36779423440228176
plot_id,batch_id 0 34 miss% 0.35216619946096267
plot_id,batch_id 0 35 miss% 0.3177662733782007
plot_id,batch_id 0 36 miss% 0.5280853914957122
plot_id,batch_id 0 37 miss% 0.3464289592042704
plot_id,batch_id 0 38 miss% 0.4524808829375988
plot_id,batch_id 0 39 miss% 0.25710645380164093
plot_id,batch_id 0 40 miss% 0.45314283787261517
plot_id,batch_id 0 41 miss% 0.46799751096284936
plot_id,batch_id 0 42 miss% 0.41147596770973077
plot_id,batch_id 0 43 miss% 0.28682434668393364
plot_id,batch_id 0 44 miss% 0.3185654883839906
plot_id,batch_id 0 45 miss% 0.3460506627225811
plot_id,batch_id 0 46 miss% 0.4602149491427295
plot_id,batch_id 0 47 miss% 0.3762845772786175
plot_id,batch_id 0 48 miss% 0.37377749117599013
plot_id,batch_id 0 49 miss% 0.4573345234005888
plot_id,batch_id 0 50 miss% 0.46002367683468326
plot_id,batch_id 0 51 miss% 0.5411266830250205
plot_id,batch_id 0 52 miss% 0.4354947412110531
plot_id,batch_id 0 53 miss% 0.32082288057017794
plot_id,batch_id 0 54 miss% 0.4809326143421495
plot_id,batch_id 0 55 miss% 0.37263774508463526
plot_id,batch_id 0 56 miss% 0.4017134281863635
plot_id,batch_id 0 57 miss% 0.3803676557208433
plot_id,batch_id 0 58 miss% 0.44863137173197976
plot_id,batch_id 0 59 miss% 0.4481659601791348
plot_id,batch_id 0 60 miss% 0.3809312362554162
plot_id,batch_id 0 61 miss% 0.31628990452621275
plot_id,batch_id 0 62 miss% 0.33395573119318667
plot_id,batch_id 0 63 miss% 0.41659943078250555
plot_id,batch_id 0 64 miss% 0.4400567905879938
plot_id,batch_id 0 65 miss% 0.4586908800456625
plot_id,batch_id 0 66 miss% 0.41798051251515644
plot_id,batch_id 0 67 miss% 0.27749531557351886
plot_id,batch_id 0 68 miss% 0.45820653664203403
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  69649
Epoch:0, Train loss:0.630832, valid loss:0.612268
Epoch:1, Train loss:0.043990, valid loss:0.004931
Epoch:2, Train loss:0.011081, valid loss:0.003114
Epoch:3, Train loss:0.009155, valid loss:0.002334
Epoch:4, Train loss:0.006038, valid loss:0.001944
Epoch:5, Train loss:0.004737, valid loss:0.001755
Epoch:6, Train loss:0.004381, valid loss:0.001588
Epoch:7, Train loss:0.002930, valid loss:0.001268
Epoch:8, Train loss:0.002528, valid loss:0.001115
Epoch:9, Train loss:0.002391, valid loss:0.001268
Epoch:10, Train loss:0.002196, valid loss:0.001051
Epoch:11, Train loss:0.001735, valid loss:0.001026
Epoch:12, Train loss:0.001650, valid loss:0.000904
Epoch:13, Train loss:0.001623, valid loss:0.000939
Epoch:14, Train loss:0.001593, valid loss:0.000879
Epoch:15, Train loss:0.001514, valid loss:0.000905
Epoch:16, Train loss:0.001494, valid loss:0.000804
Epoch:17, Train loss:0.001455, valid loss:0.000877
Epoch:18, Train loss:0.001384, valid loss:0.000798
Epoch:19, Train loss:0.001370, valid loss:0.000789
Epoch:20, Train loss:0.001302, valid loss:0.000999
Epoch:21, Train loss:0.001108, valid loss:0.000671
Epoch:22, Train loss:0.001076, valid loss:0.000674
Epoch:23, Train loss:0.001072, valid loss:0.000678
Epoch:24, Train loss:0.001058, valid loss:0.000620
Epoch:25, Train loss:0.001056, valid loss:0.000658
Epoch:26, Train loss:0.001020, valid loss:0.000670
Epoch:27, Train loss:0.001017, valid loss:0.000666
Epoch:28, Train loss:0.000984, valid loss:0.000625
Epoch:29, Train loss:0.000988, valid loss:0.000665
Epoch:30, Train loss:0.000982, valid loss:0.000622
Epoch:31, Train loss:0.000864, valid loss:0.000610
Epoch:32, Train loss:0.000849, valid loss:0.000578
Epoch:33, Train loss:0.000846, valid loss:0.000568
Epoch:34, Train loss:0.000844, valid loss:0.000578
Epoch:35, Train loss:0.000833, valid loss:0.000593
Epoch:36, Train loss:0.000831, valid loss:0.000569
Epoch:37, Train loss:0.000833, valid loss:0.000578
Epoch:38, Train loss:0.000815, valid loss:0.000560
Epoch:39, Train loss:0.000806, valid loss:0.000540
Epoch:40, Train loss:0.000815, valid loss:0.000586
Epoch:41, Train loss:0.000749, valid loss:0.000574
Epoch:42, Train loss:0.000751, valid loss:0.000538
Epoch:43, Train loss:0.000745, valid loss:0.000543
Epoch:44, Train loss:0.000741, valid loss:0.000559
Epoch:45, Train loss:0.000736, valid loss:0.000573
Epoch:46, Train loss:0.000734, valid loss:0.000574
Epoch:47, Train loss:0.000729, valid loss:0.000542
Epoch:48, Train loss:0.000726, valid loss:0.000546
Epoch:49, Train loss:0.000730, valid loss:0.000548
Epoch:50, Train loss:0.000719, valid loss:0.000567
Epoch:51, Train loss:0.000690, valid loss:0.000557
Epoch:52, Train loss:0.000686, valid loss:0.000528
Epoch:53, Train loss:0.000684, valid loss:0.000542
Epoch:54, Train loss:0.000683, valid loss:0.000544
Epoch:55, Train loss:0.000683, valid loss:0.000524
Epoch:56, Train loss:0.000681, valid loss:0.000527
Epoch:57, Train loss:0.000682, valid loss:0.000525
Epoch:58, Train loss:0.000681, valid loss:0.000537
Epoch:59, Train loss:0.000682, valid loss:0.000528
Epoch:60, Train loss:0.000680, valid loss:0.000529
training time 11671.956402540207
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.0549268012996057
plot_id,batch_id 0 1 miss% 0.059967611944898754
plot_id,batch_id 0 2 miss% 0.09199264555430568
plot_id,batch_id 0 3 miss% 0.07036252312596977
plot_id,batch_id 0 4 miss% 0.11357656661598148
plot_id,batch_id 0 5 miss% 0.056085431499479275
plot_id,batch_id 0 6 miss% 0.04366933537701828
plot_id,batch_id 0 7 miss% 0.11132251134382706
plot_id,batch_id 0 8 miss% 0.10347913507646944
plot_id,batch_id 0 9 miss% 0.05501439728320818
plot_id,batch_id 0 10 miss% 0.027388512169574122
plot_id,batch_id 0 11 miss% 0.06586029848677014
plot_id,batch_id 0 12 miss% 0.07954255740744745
plot_id,batch_id 0 13 miss% 0.05727098615564748
plot_id,batch_id 0 14 miss% 0.10333790809141465
plot_id,batch_id 0 15 miss% 0.03015228457181827
plot_id,batch_id 0 16 miss% 0.1660861748047268
plot_id,batch_id 0 17 miss% 0.06395151274571363
plot_id,batch_id 0 18 miss% 0.06272358315099615
plot_id,batch_id 0 19 miss% 0.09293963669856178
plot_id,batch_id 0 20 miss% 0.045567362644643825
plot_id,batch_id 0 21 miss% 0.03364062499765315
plot_id,batch_id 0 22 miss% 0.11814868255372904
plot_id,batch_id 0 23 miss% 0.06216539948975565
plot_id,batch_id 0 24 miss% 0.0752811972671584
plot_id,batch_id 0 25 miss% 0.07614771842356222
plot_id,batch_id 0 26 miss% 0.0403492827166925
plot_id,batch_id 0 27 miss% 0.06409827039047504
plot_id,batch_id 0 28 miss% 0.035337656536560166
plot_id,batch_id 0 29 miss% 0.04684025017111688
plot_id,batch_id 0 30 miss% 0.03230443648743558
plot_id,batch_id 0 31 miss% 0.08600074193310445
plot_id,batch_id 0 32 miss% 0.15480456642272591
plot_id,batch_id 0 33 miss% 0.08552620422279052
plot_id,batch_id 0 34 miss% 0.04258654994842297
plot_id,batch_id 0 35 miss% 0.05785403754031931
plot_id,batch_id 0 36 miss% 0.07929148616244805
plot_id,batch_id 0 37 miss% 0.11736489355131273
plot_id,batch_id 0 38 miss% 0.07819313735558105
plot_id,batch_id 0 39 miss% 0.04850645602886641
plot_id,batch_id 0 40 miss% 0.09480087064370141
plot_id,batch_id 0 41 miss% 0.06960605794003873
plot_id,batch_id 0 42 miss% 0.03849883272340933
plot_id,batch_id 0 43 miss% 0.07943107252044633
plot_id,batch_id 0 44 miss% 0.040187701981545774
plot_id,batch_id 0 45 miss% 0.0442007002255887
plot_id,batch_id 0 46 miss% 0.05760265319893178
plot_id,batch_id 0 47 miss% 0.04131189756577068
plot_id,batch_id 0 48 miss% 0.02863417890746067
plot_id,batch_id 0 49 miss% 0.05909814892742492
plot_id,batch_id 0 50 miss% 0.11842000667705595
plot_id,batch_id 0 51 miss% 0.03414921512823081
plot_id,batch_id 0 52 miss% 0.03566400490806303
plot_id,batch_id 0 53 miss% 0.03659884031217908
plot_id,batch_id 0 54 miss% 0.09372063586072925
plot_id,batch_id 0 55 miss% 0.04422440107391022
plot_id,batch_id 0 56 miss% 0.1138184017788417
plot_id,batch_id 0 57 miss% 0.054699477489584626
plot_id,batch_id 0 58 miss% 0.032345118660732
plot_id,batch_id 0 59 miss% 0.05870868552038691
plot_id,batch_id 0 60 miss% 0.04315375430357773
plot_id,batch_id 0 61 miss% 0.01697580316133935
plot_id,batch_id 0 62 miss% 0.02958243412450471
plot_id,batch_id 0 63 miss% 0.08329462782089035
plot_id,batch_id 0 64 miss% 0.061977301696320236
plot_id,batch_id 0 65 miss% 0.12142626498243173
plot_id,batch_id 0 66 miss% 0.05821492820176239
plot_id,batch_id 0 67 plot_id,batch_id 0 66 miss% 0.12619568367814238
plot_id,batch_id 0 67 miss% 0.03582474020337111
plot_id,batch_id 0 68 miss% 0.06724432525788517
plot_id,batch_id 0 69 miss% 0.06538467074014874
plot_id,batch_id 0 70 miss% 0.12089332549707571
plot_id,batch_id 0 71 miss% 0.07000899614762522
plot_id,batch_id 0 72 miss% 0.11160980155955874
plot_id,batch_id 0 73 miss% 0.07614159031692327
plot_id,batch_id 0 74 miss% 0.14065433062623334
plot_id,batch_id 0 75 miss% 0.055118122162079654
plot_id,batch_id 0 76 miss% 0.06324347952138012
plot_id,batch_id 0 77 miss% 0.03893626104929238
plot_id,batch_id 0 78 miss% 0.05643042765565043
plot_id,batch_id 0 79 miss% 0.10555198350730297
plot_id,batch_id 0 80 miss% 0.038121605623270584
plot_id,batch_id 0 81 miss% 0.12738945051654102
plot_id,batch_id 0 82 miss% 0.06990968233231203
plot_id,batch_id 0 83 miss% 0.14636931862120423
plot_id,batch_id 0 84 miss% 0.14187009961297586
plot_id,batch_id 0 85 miss% 0.04081437291954427
plot_id,batch_id 0 86 miss% 0.11970524844911701
plot_id,batch_id 0 87 miss% 0.10808613718812655
plot_id,batch_id 0 88 miss% 0.10534955584569786
plot_id,batch_id 0 89 miss% 0.061158221757472375
plot_id,batch_id 0 90 miss% 0.04253177387126557
plot_id,batch_id 0 91 miss% 0.09921177062749095
plot_id,batch_id 0 92 miss% 0.0824909713796346
plot_id,batch_id 0 93 miss% 0.04098611195356717
plot_id,batch_id 0 94 miss% 0.05834794953537665
plot_id,batch_id 0 95 miss% 0.08194003274496692
plot_id,batch_id 0 96 miss% 0.05978100231687149
plot_id,batch_id 0 97 miss% 0.12955097388541353
plot_id,batch_id 0 98 miss% 0.041772069655374236
plot_id,batch_id 0 99 miss% 0.10848608859226339
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10976349 0.05460837 0.12967692 0.06293459 0.11106355 0.04949553
 0.07483732 0.11558263 0.10768257 0.0705421  0.04683288 0.06162752
 0.09052279 0.04514375 0.12705403 0.07424193 0.1948377  0.04014838
 0.074273   0.09052779 0.09519981 0.07914221 0.05582223 0.06257662
 0.10387418 0.13193937 0.08654184 0.078366   0.06532381 0.04047892
 0.04308255 0.10884829 0.14016364 0.10260978 0.05764133 0.07451624
 0.13170274 0.0888877  0.05080607 0.05422113 0.12042443 0.08452405
 0.04934323 0.04256759 0.05631768 0.04689976 0.07314303 0.03180057
 0.03651869 0.03489937 0.1522418  0.01859046 0.0384279  0.03712071
 0.05213327 0.04324389 0.17905488 0.07503462 0.04404232 0.06039306
 0.05618367 0.04282223 0.0774202  0.04294878 0.07707526 0.1399196
 0.12619568 0.03582474 0.06724433 0.06538467 0.12089333 0.070009
 0.1116098  0.07614159 0.14065433 0.05511812 0.06324348 0.03893626
 0.05643043 0.10555198 0.03812161 0.12738945 0.06990968 0.14636932
 0.1418701  0.04081437 0.11970525 0.10808614 0.10534956 0.06115822
 0.04253177 0.09921177 0.08249097 0.04098611 0.05834795 0.08194003
 0.059781   0.12955097 0.04177207 0.10848609]
for model  163 the mean error 0.07933342508262947
all id 163 hidden_dim 16 learning_rate 0.0025 num_layers 3 frames 31 out win 5 err 0.07933342508262947 time 11622.305815935135
Launcher: Job 164 completed in 11886 seconds.
Launcher: Task 249 done. Exiting.
plot_id,batch_id 0 69 miss% 0.38894814041369846
plot_id,batch_id 0 70 miss% 0.36308010046505584
plot_id,batch_id 0 71 miss% 0.44497583782571626
plot_id,batch_id 0 72 miss% 0.48693278457592526
plot_id,batch_id 0 73 miss% 0.385041281519063
plot_id,batch_id 0 74 miss% 0.40086721210328424
plot_id,batch_id 0 75 miss% 0.22554484634792835
plot_id,batch_id 0 76 miss% 0.4133616282977168
plot_id,batch_id 0 77 miss% 0.3167798224688765
plot_id,batch_id 0 78 miss% 0.31857651055466074
plot_id,batch_id 0 79 miss% 0.3966139711867976
plot_id,batch_id 0 80 miss% 0.41889179052253445
plot_id,batch_id 0 81 miss% 0.46715298407630607
plot_id,batch_id 0 82 miss% 0.4202871497584269
plot_id,batch_id 0 83 miss% 0.42317839233899934
plot_id,batch_id 0 84 miss% 0.3603084384725719
plot_id,batch_id 0 85 miss% 0.3134190280358534
plot_id,batch_id 0 86 miss% 0.3376164555078726
plot_id,batch_id 0 87 miss% 0.4096377920490318
plot_id,batch_id 0 88 miss% 0.46131522315761275
plot_id,batch_id 0 89 miss% 0.4106904629896263
plot_id,batch_id 0 90 miss% 0.2720222704887934
plot_id,batch_id 0 91 miss% 0.35306867809242126
plot_id,batch_id 0 92 miss% 0.3195367934343057
plot_id,batch_id 0 93 miss% 0.29350926494389323
plot_id,batch_id 0 94 miss% 0.5043966880585303
plot_id,batch_id 0 95 miss% 0.32235690370585524
plot_id,batch_id 0 96 miss% 0.4322056802315736
plot_id,batch_id 0 97 miss% 0.4847063588669465
plot_id,batch_id 0 98 miss% 0.5276174974594308
plot_id,batch_id 0 99 miss% 0.3659183777246715
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.38627795 0.38474318 0.38129215 0.33659    0.33304781 0.39648977
 0.36730824 0.4876177  0.59523419 0.37981419 0.341045   0.37275566
 0.50680953 0.33442341 0.41958095 0.35765866 0.4692759  0.43486947
 0.46275395 0.36483922 0.4262208  0.42527175 0.34341364 0.43174599
 0.40706387 0.36615589 0.35297939 0.38085609 0.33959272 0.30496323
 0.32158374 0.53840526 0.46473166 0.36779423 0.3521662  0.31776627
 0.52808539 0.34642896 0.45248088 0.25710645 0.45314284 0.46799751
 0.41147597 0.28682435 0.31856549 0.34605066 0.46021495 0.37628458
 0.37377749 0.45733452 0.46002368 0.54112668 0.43549474 0.32082288
 0.48093261 0.37263775 0.40171343 0.38036766 0.44863137 0.44816596
 0.38093124 0.3162899  0.33395573 0.41659943 0.44005679 0.45869088
 0.41798051 0.27749532 0.45820654 0.38894814 0.3630801  0.44497584
 0.48693278 0.38504128 0.40086721 0.22554485 0.41336163 0.31677982
 0.31857651 0.39661397 0.41889179 0.46715298 0.42028715 0.42317839
 0.36030844 0.31341903 0.33761646 0.40963779 0.46131522 0.41069046
 0.27202227 0.35306868 0.31953679 0.29350926 0.50439669 0.3223569
 0.43220568 0.48470636 0.5276175  0.36591838]
for model  139 the mean error 0.39617589191014846
all id 139 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 25 out win 5 err 0.39617589191014846 time 11688.882124900818
Launcher: Job 140 completed in 11923 seconds.
Launcher: Task 245 done. Exiting.
plot_id,batch_id 0 69 miss% 0.08095084231744139
plot_id,batch_id 0 70 miss% 0.14603012040145066
plot_id,batch_id 0 71 miss% 0.05538402165248801
plot_id,batch_id 0 72 miss% 0.0835315368732797
plot_id,batch_id 0 73 miss% 0.06481754169749572
plot_id,batch_id 0 74 miss% 0.16965563789739493
plot_id,batch_id 0 75 miss% 0.035465706789914876
plot_id,batch_id 0 76 miss% 0.10989825456011355
plot_id,batch_id 0 77 miss% 0.0679704037392245
plot_id,batch_id 0 78 miss% 0.0407503075216394
plot_id,batch_id 0 79 miss% 0.08883468379570346
plot_id,batch_id 0 80 miss% 0.06533585203145613
plot_id,batch_id 0 81 miss% 0.09388113448309617
plot_id,batch_id 0 82 miss% 0.08733085794124255
plot_id,batch_id 0 83 miss% 0.061772638546877666
plot_id,batch_id 0 84 miss% 0.12511802248624213
plot_id,batch_id 0 85 miss% 0.03390296862079273
plot_id,batch_id 0 86 miss% 0.045845713176851044
plot_id,batch_id 0 87 miss% 0.08232977673882527
plot_id,batch_id 0 88 miss% 0.11500515072877886
plot_id,batch_id 0 89 miss% 0.05508001441800155
plot_id,batch_id 0 90 miss% 0.0646705839774493
plot_id,batch_id 0 91 miss% 0.06456347004411611
plot_id,batch_id 0 92 miss% 0.05526031078920966
plot_id,batch_id 0 93 miss% 0.05607906979154551
plot_id,batch_id 0 94 miss% 0.06371187689829629
plot_id,batch_id 0 95 miss% 0.07026869737004648
plot_id,batch_id 0 96 miss% 0.05988691464108127
plot_id,batch_id 0 97 miss% 0.03540849681194204
plot_id,batch_id 0 98 miss% 0.05055023582604878
plot_id,batch_id 0 99 miss% 0.08447737782158424
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.11757579 0.04343577 0.08037957 0.06042844 0.09923695 0.09991598
 0.06792207 0.07389622 0.08250463 0.05585765 0.03284697 0.08270779
 0.03610082 0.07070624 0.10632417 0.04882405 0.08828609 0.04231301
 0.07424018 0.09381347 0.06658536 0.04791274 0.03795961 0.0232158
 0.02867038 0.07499085 0.06132139 0.0479696  0.03178402 0.0295038
 0.02569111 0.09627871 0.08782879 0.05594415 0.06202776 0.0891069
 0.11176261 0.06602513 0.04306612 0.02800266 0.04999106 0.02396868
 0.02034845 0.0517843  0.05626523 0.03717918 0.03276538 0.03106966
 0.0251081  0.03273348 0.14316712 0.05910173 0.02930484 0.056038
 0.033451   0.09080354 0.06963997 0.03077959 0.05381901 0.02327874
 0.02864223 0.03236859 0.03781322 0.0565335  0.07053373 0.08772319
 0.11432495 0.04381507 0.05285664 0.08095084 0.14603012 0.05538402
 0.08353154 0.06481754 0.16965564 0.03546571 0.10989825 0.0679704
 0.04075031 0.08883468 0.06533585 0.09388113 0.08733086 0.06177264
 0.12511802 0.03390297 0.04584571 0.08232978 0.11500515 0.05508001
 0.06467058 0.06456347 0.05526031 0.05607907 0.06371188 0.0702687
 0.05988691 0.0354085  0.05055024 0.08447738]
for model  20 the mean error 0.06361939759262511
all id 20 hidden_dim 16 learning_rate 0.0025 num_layers 5 frames 21 out win 6 err 0.06361939759262511 time 11661.413145780563
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  69649
Epoch:0, Train loss:0.795382, valid loss:0.773365
Epoch:1, Train loss:0.050310, valid loss:0.006135
Epoch:2, Train loss:0.011774, valid loss:0.003844
Epoch:3, Train loss:0.008509, valid loss:0.002936
Epoch:4, Train loss:0.006926, valid loss:0.002639
Epoch:5, Train loss:0.005142, valid loss:0.002252
Epoch:6, Train loss:0.004595, valid loss:0.002205
Epoch:7, Train loss:0.004187, valid loss:0.001833
Epoch:8, Train loss:0.003893, valid loss:0.002001
Epoch:9, Train loss:0.003865, valid loss:0.002032
Epoch:10, Train loss:0.003581, valid loss:0.001912
Epoch:11, Train loss:0.002582, valid loss:0.001266
Epoch:12, Train loss:0.002445, valid loss:0.001347
Epoch:13, Train loss:0.002401, valid loss:0.001430
Epoch:14, Train loss:0.002404, valid loss:0.001319
Epoch:15, Train loss:0.002400, valid loss:0.001294
Epoch:16, Train loss:0.002231, valid loss:0.001474
Epoch:17, Train loss:0.002194, valid loss:0.001251
Epoch:18, Train loss:0.002170, valid loss:0.001222
Epoch:19, Train loss:0.002038, valid loss:0.001293
Epoch:20, Train loss:0.002022, valid loss:0.001253
Epoch:21, Train loss:0.001530, valid loss:0.001011
Epoch:22, Train loss:0.001469, valid loss:0.001014
Epoch:23, Train loss:0.001468, valid loss:0.000946
Epoch:24, Train loss:0.001428, valid loss:0.000957
Epoch:25, Train loss:0.001417, valid loss:0.001006
Epoch:26, Train loss:0.001390, valid loss:0.000971
Epoch:27, Train loss:0.001384, valid loss:0.001210
Epoch:28, Train loss:0.001371, valid loss:0.000960
Epoch:29, Train loss:0.001328, valid loss:0.000877
Epoch:30, Train loss:0.001290, valid loss:0.001097
Epoch:31, Train loss:0.001092, valid loss:0.000825
Epoch:32, Train loss:0.001078, valid loss:0.000964
Epoch:33, Train loss:0.001042, valid loss:0.000834
Epoch:34, Train loss:0.001055, valid loss:0.000877
Epoch:35, Train loss:0.001032, valid loss:0.000793
Epoch:36, Train loss:0.001043, valid loss:0.000867
Epoch:37, Train loss:0.001039, valid loss:0.000807
Epoch:38, Train loss:0.000999, valid loss:0.000828
Epoch:39, Train loss:0.000998, valid loss:0.000859
Epoch:40, Train loss:0.000984, valid loss:0.000819
Epoch:41, Train loss:0.000883, valid loss:0.000779
Epoch:42, Train loss:0.000868, valid loss:0.000801
Epoch:43, Train loss:0.000860, valid loss:0.000757
Epoch:44, Train loss:0.000862, valid loss:0.000756
Epoch:45, Train loss:0.000852, valid loss:0.000753
Epoch:46, Train loss:0.000852, valid loss:0.000751
Epoch:47, Train loss:0.000851, valid loss:0.000799
Epoch:48, Train loss:0.000838, valid loss:0.000759
Epoch:49, Train loss:0.000830, valid loss:0.000809
Epoch:50, Train loss:0.000835, valid loss:0.000738
Epoch:51, Train loss:0.000781, valid loss:0.000720
Epoch:52, Train loss:0.000773, valid loss:0.000719
Epoch:53, Train loss:0.000768, valid loss:0.000717
Epoch:54, Train loss:0.000766, valid loss:0.000718
Epoch:55, Train loss:0.000763, valid loss:0.000717
Epoch:56, Train loss:0.000763, valid loss:0.000716
Epoch:57, Train loss:0.000762, valid loss:0.000718
Epoch:58, Train loss:0.000760, valid loss:0.000721
Epoch:59, Train loss:0.000760, valid loss:0.000716
Epoch:60, Train loss:0.000758, valid loss:0.000718
training time 11708.010699272156
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.10907164953948206
plot_id,batch_id 0 1 miss% 0.03926388819558597
plot_id,batch_id 0 2 miss% 0.09146025526869175
plot_id,batch_id 0 3 miss% 0.04892855326111324
plot_id,batch_id 0 4 miss% 0.08421131629008671
plot_id,batch_id 0 5 miss% 0.05523105465927434
plot_id,batch_id 0 6 miss% 0.05848393057641349
plot_id,batch_id 0 7 miss% 0.08032062228489345
plot_id,batch_id 0 8 miss% 0.12262707008363985
plot_id,batch_id 0 9 miss% 0.11440777952833837
plot_id,batch_id 0 10 miss% 0.037275844014888655
plot_id,batch_id 0 11 miss% 0.06404685122630052
plot_id,batch_id 0 12 miss% 0.09964378916079099
plot_id,batch_id 0 13 miss% 0.06819379387066209
plot_id,batch_id 0 14 miss% 0.06438886611087401
plot_id,batch_id 0 15 miss% 0.03413584813797321
plot_id,batch_id 0 16 miss% 0.16785599810162827
plot_id,batch_id 0 17 miss% 0.06218578139762922
plot_id,batch_id 0 18 miss% 0.0978207619320159
plot_id,batch_id 0 19 miss% 0.03807454918736508
plot_id,batch_id 0 20 miss% 0.08890045708835154
plot_id,batch_id 0 21 miss% 0.08243448233847811
plot_id,batch_id 0 22 miss% 0.08346014818598194
plot_id,batch_id 0 23 miss% 0.06439223537475199
plot_id,batch_id 0 24 miss% 0.04396096445266974
plot_id,batch_id 0 25 miss% 0.06586450035691704
plot_id,batch_id 0 26 miss% 0.16252292195086612
plot_id,batch_id 0 27 miss% 0.05166141784979959
plot_id,batch_id 0 28 miss% 0.13142843840047047
plot_id,batch_id 0 29 miss% 0.07414105853391029
plot_id,batch_id 0 30 miss% 0.05328863342133836
plot_id,batch_id 0 31 miss% 0.07007757917245545
plot_id,batch_id 0 32 miss% 0.10426669777715193
plot_id,batch_id 0 33 miss% 0.05854862502877755
plot_id,batch_id 0 34 miss% 0.05104356832459566
plot_id,batch_id 0 35 miss% 0.09193661192010325
plot_id,batch_id 0 36 miss% 0.06738741932786041
plot_id,batch_id 0 37 miss% 0.08489673855693385
plot_id,batch_id 0 38 miss% 0.03877939139536472
plot_id,batch_id 0 39 miss% 0.048930866077092926
plot_id,batch_id 0 40 miss% 0.03992236836267143
plot_id,batch_id 0 41 miss% 0.05523396422541308
plot_id,batch_id 0 42 miss% 0.02415899625961708
plot_id,batch_id 0 43 miss% 0.036445142030556446
plot_id,batch_id 0 44 miss% 0.060285435621029455
plot_id,batch_id 0 45 miss% 0.039987050900114195
plot_id,batch_id 0 46 miss% 0.026779398242615972
plot_id,batch_id 0 47 miss% 0.028688973005864683
plot_id,batch_id 0 48 miss% 0.04395782141872886
plot_id,batch_id 0 49 miss% 0.08127395351097788
plot_id,batch_id 0 50 miss% 0.11449349081818502
plot_id,batch_id 0 51 miss% 0.03720443175680681
plot_id,batch_id 0 52 miss% 0.029181302684531393
plot_id,batch_id 0 53 miss% 0.053383197603172235
plot_id,batch_id 0 54 miss% 0.11154743698773067
plot_id,batch_id 0 55 miss% 0.05230108898792531
plot_id,batch_id 0 56 miss% 0.06832098100214613
plot_id,batch_id 0 57 miss% 0.0592358334998421
plot_id,batch_id 0 58 miss% 0.06544166625143055
plot_id,batch_id 0 59 miss% 0.045155828037589066
plot_id,batch_id 0 60 miss% 0.07157423206207239
plot_id,batch_id 0 61 miss% 0.024400865402397037
plot_id,batch_id 0 62 miss% 0.04163002047877145
plot_id,batch_id 0 63 miss% 0.03854944270767032
plot_id,batch_id 0 64 miss% 0.060654370359455104
plot_id,batch_id 0 65 miss% 0.1684460462095155
plot_id,batch_id 0 66 miss% 0.06264369330438056
plot_id,batch_id 0 67 miss% 0.022389166070654838
plot_id,batch_id 0 68 miss% Launcher: Job 21 completed in 11931 seconds.
Launcher: Task 77 done. Exiting.
miss% 0.051508186382812175
plot_id,batch_id 0 68 miss% 0.04129613943119002
plot_id,batch_id 0 69 miss% 0.0852202990733185
plot_id,batch_id 0 70 miss% 0.061813203530833874
plot_id,batch_id 0 71 miss% 0.06405111725399952
plot_id,batch_id 0 72 miss% 0.10309680607687749
plot_id,batch_id 0 73 miss% 0.06656748501864644
plot_id,batch_id 0 74 miss% 0.1068794476522858
plot_id,batch_id 0 75 miss% 0.07099527068201103
plot_id,batch_id 0 76 miss% 0.12268895065480447
plot_id,batch_id 0 77 miss% 0.06807359275014653
plot_id,batch_id 0 78 miss% 0.0331867745716863
plot_id,batch_id 0 79 miss% 0.08748419043283183
plot_id,batch_id 0 80 miss% 0.02976718569147197
plot_id,batch_id 0 81 miss% 0.09042612918491867
plot_id,batch_id 0 82 miss% 0.05373081850197717
plot_id,batch_id 0 83 miss% 0.06650285076900858
plot_id,batch_id 0 84 miss% 0.041539036663776306
plot_id,batch_id 0 85 miss% 0.03977694856738796
plot_id,batch_id 0 86 miss% 0.062058186648061495
plot_id,batch_id 0 87 miss% 0.07049332017602855
plot_id,batch_id 0 88 miss% 0.11569510589702947
plot_id,batch_id 0 89 miss% 0.08540773900741393
plot_id,batch_id 0 90 miss% 0.038143070223693926
plot_id,batch_id 0 91 miss% 0.12375167464097414
plot_id,batch_id 0 92 miss% 0.02943862059550028
plot_id,batch_id 0 93 miss% 0.050926362820603394
plot_id,batch_id 0 94 miss% 0.08510234238473784
plot_id,batch_id 0 95 miss% 0.046405461182362426
plot_id,batch_id 0 96 miss% 0.05916510686318654
plot_id,batch_id 0 97 miss% 0.03559026002159242
plot_id,batch_id 0 98 miss% 0.040311898510178426
plot_id,batch_id 0 99 miss% 0.07962004882081143
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0549268  0.05996761 0.09199265 0.07036252 0.11357657 0.05608543
 0.04366934 0.11132251 0.10347914 0.0550144  0.02738851 0.0658603
 0.07954256 0.05727099 0.10333791 0.03015228 0.16608617 0.06395151
 0.06272358 0.09293964 0.04556736 0.03364062 0.11814868 0.0621654
 0.0752812  0.07614772 0.04034928 0.06409827 0.03533766 0.04684025
 0.03230444 0.08600074 0.15480457 0.0855262  0.04258655 0.05785404
 0.07929149 0.11736489 0.07819314 0.04850646 0.09480087 0.06960606
 0.03849883 0.07943107 0.0401877  0.0442007  0.05760265 0.0413119
 0.02863418 0.05909815 0.11842001 0.03414922 0.035664   0.03659884
 0.09372064 0.0442244  0.1138184  0.05469948 0.03234512 0.05870869
 0.04315375 0.0169758  0.02958243 0.08329463 0.0619773  0.12142626
 0.05821493 0.05150819 0.04129614 0.0852203  0.0618132  0.06405112
 0.10309681 0.06656749 0.10687945 0.07099527 0.12268895 0.06807359
 0.03318677 0.08748419 0.02976719 0.09042613 0.05373082 0.06650285
 0.04153904 0.03977695 0.06205819 0.07049332 0.11569511 0.08540774
 0.03814307 0.12375167 0.02943862 0.05092636 0.08510234 0.04640546
 0.05916511 0.03559026 0.0403119  0.07962005]
for model  99 the mean error 0.06686719015264805
all id 99 hidden_dim 16 learning_rate 0.0025 num_layers 5 frames 25 out win 4 err 0.06686719015264805 time 11671.956402540207
Launcher: Job 100 completed in 11936 seconds.
Launcher: Task 222 done. Exiting.
0.03931475422958766
plot_id,batch_id 0 69 miss% 0.06966689734757625
plot_id,batch_id 0 70 miss% 0.13361765784865048
plot_id,batch_id 0 71 miss% 0.06620790367347147
plot_id,batch_id 0 72 miss% 0.06677350477726615
plot_id,batch_id 0 73 miss% 0.03583158604605702
plot_id,batch_id 0 74 miss% 0.10943584985184719
plot_id,batch_id 0 75 miss% 0.09137724665056988
plot_id,batch_id 0 76 miss% 0.10687446708182094
plot_id,batch_id 0 77 miss% 0.07322891494028472
plot_id,batch_id 0 78 miss% 0.03742119479194398
plot_id,batch_id 0 79 miss% 0.052480222893316435
plot_id,batch_id 0 80 miss% 0.1547268324375229
plot_id,batch_id 0 81 miss% 0.05631567919284494
plot_id,batch_id 0 82 miss% 0.0683105980179336
plot_id,batch_id 0 83 miss% 0.10792463168495782
plot_id,batch_id 0 84 miss% 0.07856768742248556
plot_id,batch_id 0 85 miss% 0.03036227867292504
plot_id,batch_id 0 86 miss% 0.037670119789292286
plot_id,batch_id 0 87 miss% 0.07930449169802273
plot_id,batch_id 0 88 miss% 0.06995163327673715
plot_id,batch_id 0 89 miss% 0.06382317970367768
plot_id,batch_id 0 90 miss% 0.04145860437929943
plot_id,batch_id 0 91 miss% 0.0620512564175439
plot_id,batch_id 0 92 miss% 0.10082343152289447
plot_id,batch_id 0 93 miss% 0.062324507216956246
plot_id,batch_id 0 94 miss% 0.05121041803278668
plot_id,batch_id 0 95 miss% 0.042416080735219935
plot_id,batch_id 0 96 miss% 0.0676116973371872
plot_id,batch_id 0 97 miss% 0.10113341708446325
plot_id,batch_id 0 98 miss% 0.053791833383145124
plot_id,batch_id 0 99 miss% 0.08630114218051559
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10907165 0.03926389 0.09146026 0.04892855 0.08421132 0.05523105
 0.05848393 0.08032062 0.12262707 0.11440778 0.03727584 0.06404685
 0.09964379 0.06819379 0.06438887 0.03413585 0.167856   0.06218578
 0.09782076 0.03807455 0.08890046 0.08243448 0.08346015 0.06439224
 0.04396096 0.0658645  0.16252292 0.05166142 0.13142844 0.07414106
 0.05328863 0.07007758 0.1042667  0.05854863 0.05104357 0.09193661
 0.06738742 0.08489674 0.03877939 0.04893087 0.03992237 0.05523396
 0.024159   0.03644514 0.06028544 0.03998705 0.0267794  0.02868897
 0.04395782 0.08127395 0.11449349 0.03720443 0.0291813  0.0533832
 0.11154744 0.05230109 0.06832098 0.05923583 0.06544167 0.04515583
 0.07157423 0.02440087 0.04163002 0.03854944 0.06065437 0.16844605
 0.06264369 0.02238917 0.03931475 0.0696669  0.13361766 0.0662079
 0.0667735  0.03583159 0.10943585 0.09137725 0.10687447 0.07322891
 0.03742119 0.05248022 0.15472683 0.05631568 0.0683106  0.10792463
 0.07856769 0.03036228 0.03767012 0.07930449 0.06995163 0.06382318
 0.0414586  0.06205126 0.10082343 0.06232451 0.05121042 0.04241608
 0.0676117  0.10113342 0.05379183 0.08630114]
for model  72 the mean error 0.06917146876482187
all id 72 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 21 out win 4 err 0.06917146876482187 time 11708.010699272156
Launcher: Job 73 completed in 11979 seconds.
Launcher: Task 11 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  55697
Epoch:0, Train loss:0.469126, valid loss:0.435657
Epoch:1, Train loss:0.071120, valid loss:0.002525
Epoch:2, Train loss:0.005055, valid loss:0.001998
Epoch:3, Train loss:0.004520, valid loss:0.001957
Epoch:4, Train loss:0.004265, valid loss:0.001744
Epoch:5, Train loss:0.004000, valid loss:0.001504
Epoch:6, Train loss:0.003805, valid loss:0.001525
Epoch:7, Train loss:0.002277, valid loss:0.000887
Epoch:8, Train loss:0.001775, valid loss:0.000859
Epoch:9, Train loss:0.001698, valid loss:0.000799
Epoch:10, Train loss:0.001604, valid loss:0.000750
Epoch:11, Train loss:0.001201, valid loss:0.000652
Epoch:12, Train loss:0.001183, valid loss:0.000575
Epoch:13, Train loss:0.001161, valid loss:0.000616
Epoch:14, Train loss:0.001110, valid loss:0.000689
Epoch:15, Train loss:0.001103, valid loss:0.000665
Epoch:16, Train loss:0.001072, valid loss:0.000555
Epoch:17, Train loss:0.001053, valid loss:0.000605
Epoch:18, Train loss:0.001021, valid loss:0.000612
Epoch:19, Train loss:0.001020, valid loss:0.000548
Epoch:20, Train loss:0.000982, valid loss:0.000596
Epoch:21, Train loss:0.000790, valid loss:0.000497
Epoch:22, Train loss:0.000761, valid loss:0.000509
Epoch:23, Train loss:0.000763, valid loss:0.000488
Epoch:24, Train loss:0.000759, valid loss:0.000486
Epoch:25, Train loss:0.000747, valid loss:0.000468
Epoch:26, Train loss:0.000744, valid loss:0.000496
Epoch:27, Train loss:0.000715, valid loss:0.000490
Epoch:28, Train loss:0.000732, valid loss:0.000522
Epoch:29, Train loss:0.000702, valid loss:0.000470
Epoch:30, Train loss:0.000708, valid loss:0.000541
Epoch:31, Train loss:0.000603, valid loss:0.000513
Epoch:32, Train loss:0.000599, valid loss:0.000464
Epoch:33, Train loss:0.000596, valid loss:0.000441
Epoch:34, Train loss:0.000586, valid loss:0.000443
Epoch:35, Train loss:0.000581, valid loss:0.000458
Epoch:36, Train loss:0.000582, valid loss:0.000455
Epoch:37, Train loss:0.000576, valid loss:0.000452
Epoch:38, Train loss:0.000578, valid loss:0.000445
Epoch:39, Train loss:0.000569, valid loss:0.000472
Epoch:40, Train loss:0.000563, valid loss:0.000433
Epoch:41, Train loss:0.000513, valid loss:0.000441
Epoch:42, Train loss:0.000513, valid loss:0.000439
Epoch:43, Train loss:0.000507, valid loss:0.000436
Epoch:44, Train loss:0.000506, valid loss:0.000425
Epoch:45, Train loss:0.000505, valid loss:0.000464
Epoch:46, Train loss:0.000504, valid loss:0.000413
Epoch:47, Train loss:0.000501, valid loss:0.000431
Epoch:48, Train loss:0.000499, valid loss:0.000433
Epoch:49, Train loss:0.000496, valid loss:0.000401
Epoch:50, Train loss:0.000492, valid loss:0.000411
Epoch:51, Train loss:0.000471, valid loss:0.000409
Epoch:52, Train loss:0.000464, valid loss:0.000408
Epoch:53, Train loss:0.000462, valid loss:0.000409
Epoch:54, Train loss:0.000461, valid loss:0.000408
Epoch:55, Train loss:0.000461, valid loss:0.000405
Epoch:56, Train loss:0.000460, valid loss:0.000406
Epoch:57, Train loss:0.000460, valid loss:0.000405
Epoch:58, Train loss:0.000459, valid loss:0.000405
Epoch:59, Train loss:0.000459, valid loss:0.000406
Epoch:60, Train loss:0.000459, valid loss:0.000406
training time 11777.983351945877
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.10150396656039626
plot_id,batch_id 0 1 miss% 0.04802517178473893
plot_id,batch_id 0 2 miss% 0.08027398408278404
plot_id,batch_id 0 3 miss% 0.06273122702658354
plot_id,batch_id 0 4 miss% 0.070275035527589
plot_id,batch_id 0 5 miss% 0.03656292794538958
plot_id,batch_id 0 6 miss% 0.05665788735666442
plot_id,batch_id 0 7 miss% 0.06163057149384895
plot_id,batch_id 0 8 miss% 0.07747073102187742
plot_id,batch_id 0 9 miss% 0.026269513821244605
plot_id,batch_id 0 10 miss% 0.02978086957201432
plot_id,batch_id 0 11 miss% 0.050930926740732324
plot_id,batch_id 0 12 miss% 0.09129609250534242
plot_id,batch_id 0 13 miss% 0.09759858349689325
plot_id,batch_id 0 14 miss% 0.10737267957252637
plot_id,batch_id 0 15 miss% 0.06507401170258227
plot_id,batch_id 0 16 miss% 0.1234725343511435
plot_id,batch_id 0 17 miss% 0.04253200065984972
plot_id,batch_id 0 18 miss% 0.08196106974940093
plot_id,batch_id 0 19 miss% 0.09633240045706301
plot_id,batch_id 0 20 miss% 0.03007353305226444
plot_id,batch_id 0 21 miss% 0.04219169876454207
plot_id,batch_id 0 22 miss% 0.07363225913121453
plot_id,batch_id 0 23 miss% 0.051156384265892293
plot_id,batch_id 0 24 miss% 0.06057903170950282
plot_id,batch_id 0 25 miss% 0.05409706830312759
plot_id,batch_id 0 26 miss% 0.06750668127382725
plot_id,batch_id 0 27 miss% 0.052700070580592284
plot_id,batch_id 0 28 miss% 0.041672788246036874
plot_id,batch_id 0 29 miss% 0.04164928177829704
plot_id,batch_id 0 30 miss% 0.025913618690798654
plot_id,batch_id 0 31 miss% 0.11669496151867743
plot_id,batch_id 0 32 miss% 0.11250482740087996
plot_id,batch_id 0 33 miss% 0.049581817516268854
plot_id,batch_id 0 34 miss% 0.06550943254707894
plot_id,batch_id 0 35 miss% 0.10113694170923049
plot_id,batch_id 0 36 miss% 0.14723625210851443
plot_id,batch_id 0 37 miss% 0.08043774634138583
plot_id,batch_id 0 38 miss% 0.07470423524876878
plot_id,batch_id 0 39 miss% 0.03962705372914325
plot_id,batch_id 0 40 miss% 0.09866335837420213
plot_id,batch_id 0 41 miss% 0.05558819303785232
plot_id,batch_id 0 42 miss% 0.03503474878650455
plot_id,batch_id 0 43 miss% 0.06857389772007957
plot_id,batch_id 0 44 miss% 0.031000803088472587
plot_id,batch_id 0 45 miss% 0.05470190687975658
plot_id,batch_id 0 46 miss% 0.08420200071148913
plot_id,batch_id 0 47 miss% 0.03547098422525246
plot_id,batch_id 0 48 miss% 0.034139225634966124
plot_id,batch_id 0 49 miss% 0.03526262561247014
plot_id,batch_id 0 50 miss% 0.11594683692172739
plot_id,batch_id 0 51 miss% 0.05612291430379169
plot_id,batch_id 0 52 miss% 0.025228435348986993
plot_id,batch_id 0 53 miss% 0.022526393131701253
plot_id,batch_id 0 54 miss% 0.029968369099575
plot_id,batch_id 0 55 miss% 0.061702860788983266
plot_id,batch_id 0 56 miss% 0.12182376498151992
plot_id,batch_id 0 57 miss% 0.077192743354084
plot_id,batch_id 0 58 miss% 0.04075109825069697
plot_id,batch_id 0 59 miss% 0.045056155986167135
plot_id,batch_id 0 60 miss% 0.03156920245680407
plot_id,batch_id 0 61 miss% 0.028380755962813912
plot_id,batch_id 0 62 miss% 0.22930527323266095
plot_id,batch_id 0 63 miss% 0.06718103708672021
plot_id,batch_id 0 64 miss% 0.06858371279686128
plot_id,batch_id 0 65the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  120401
Epoch:0, Train loss:0.734365, valid loss:0.715748
Epoch:1, Train loss:0.213010, valid loss:0.005679
Epoch:2, Train loss:0.015894, valid loss:0.004992
Epoch:3, Train loss:0.014522, valid loss:0.004828
Epoch:4, Train loss:0.012782, valid loss:0.004088
Epoch:5, Train loss:0.011663, valid loss:0.004091
Epoch:6, Train loss:0.009617, valid loss:0.003048
Epoch:7, Train loss:0.008143, valid loss:0.002906
Epoch:8, Train loss:0.007631, valid loss:0.003273
Epoch:9, Train loss:0.007501, valid loss:0.002902
Epoch:10, Train loss:0.007327, valid loss:0.002687
Epoch:11, Train loss:0.006590, valid loss:0.002457
Epoch:12, Train loss:0.006267, valid loss:0.002247
Epoch:13, Train loss:0.006164, valid loss:0.002317
Epoch:14, Train loss:0.006067, valid loss:0.002485
Epoch:15, Train loss:0.006024, valid loss:0.002363
Epoch:16, Train loss:0.005942, valid loss:0.002217
Epoch:17, Train loss:0.005806, valid loss:0.002223
Epoch:18, Train loss:0.005736, valid loss:0.002263
Epoch:19, Train loss:0.003902, valid loss:0.001294
Epoch:20, Train loss:0.002055, valid loss:0.001301
Epoch:21, Train loss:0.001609, valid loss:0.001057
Epoch:22, Train loss:0.001514, valid loss:0.001162
Epoch:23, Train loss:0.001481, valid loss:0.001107
Epoch:24, Train loss:0.001483, valid loss:0.001026
Epoch:25, Train loss:0.001401, valid loss:0.001127
Epoch:26, Train loss:0.001422, valid loss:0.000969
Epoch:27, Train loss:0.001382, valid loss:0.000933
Epoch:28, Train loss:0.001309, valid loss:0.000950
Epoch:29, Train loss:0.001299, valid loss:0.001047
Epoch:30, Train loss:0.001260, valid loss:0.001186
Epoch:31, Train loss:0.001062, valid loss:0.000905
Epoch:32, Train loss:0.001028, valid loss:0.000927
Epoch:33, Train loss:0.001020, valid loss:0.000890
Epoch:34, Train loss:0.001005, valid loss:0.001020
Epoch:35, Train loss:0.001006, valid loss:0.000952
Epoch:36, Train loss:0.000987, valid loss:0.000969
Epoch:37, Train loss:0.000966, valid loss:0.000943
Epoch:38, Train loss:0.000971, valid loss:0.000936
Epoch:39, Train loss:0.000957, valid loss:0.000974
Epoch:40, Train loss:0.000935, valid loss:0.000925
Epoch:41, Train loss:0.000834, valid loss:0.000882
Epoch:42, Train loss:0.000805, valid loss:0.000857
Epoch:43, Train loss:0.000800, valid loss:0.000959
Epoch:44, Train loss:0.000806, valid loss:0.000863
Epoch:45, Train loss:0.000798, valid loss:0.000918
Epoch:46, Train loss:0.000805, valid loss:0.000862
Epoch:47, Train loss:0.000776, valid loss:0.000868
Epoch:48, Train loss:0.000778, valid loss:0.000862
Epoch:49, Train loss:0.000766, valid loss:0.000852
Epoch:50, Train loss:0.000777, valid loss:0.000845
Epoch:51, Train loss:0.000711, valid loss:0.000823
Epoch:52, Train loss:0.000704, valid loss:0.000823
Epoch:53, Train loss:0.000701, valid loss:0.000816
Epoch:54, Train loss:0.000699, valid loss:0.000816
Epoch:55, Train loss:0.000696, valid loss:0.000814
Epoch:56, Train loss:0.000696, valid loss:0.000815
Epoch:57, Train loss:0.000695, valid loss:0.000814
Epoch:58, Train loss:0.000693, valid loss:0.000813
Epoch:59, Train loss:0.000692, valid loss:0.000815
Epoch:60, Train loss:0.000691, valid loss:0.000813
training time 11806.34562754631
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.3985244018585742
plot_id,batch_id 0 1 miss% 0.5552215598316219
plot_id,batch_id 0 2 miss% 0.5691535641795784
plot_id,batch_id 0 3 miss% 0.40800335479649413
plot_id,batch_id 0 4 miss% 0.47928643982474467
plot_id,batch_id 0 5 miss% 0.5489774519573947
plot_id,batch_id 0 6 miss% 0.4368986665477785
plot_id,batch_id 0 7 miss% 0.5970155404988672
plot_id,batch_id 0 8 miss% 0.6463688843796096
plot_id,batch_id 0 9 miss% 0.6762111246062431
plot_id,batch_id 0 10 miss% 0.4095721779920923
plot_id,batch_id 0 11 miss% 0.5068117121793029
plot_id,batch_id 0 12 miss% 0.5257511287364651
plot_id,batch_id 0 13 miss% 0.4270294235605491
plot_id,batch_id 0 14 miss% 0.5749380743861848
plot_id,batch_id 0 15 miss% 0.4989509296611524
plot_id,batch_id 0 16 miss% 0.5779420574095391
plot_id,batch_id 0 17 miss% 0.5516256005804181
plot_id,batch_id 0 18 miss% 0.513162428701984
plot_id,batch_id 0 19 miss% 0.5034094510657261
plot_id,batch_id 0 20 miss% 0.44770562100023586
plot_id,batch_id 0 21 miss% 0.5441008920794798
plot_id,batch_id 0 22 miss% 0.5687943984330552
plot_id,batch_id 0 23 miss% 0.5599495426769423
plot_id,batch_id 0 24 miss% 0.5808329019205829
plot_id,batch_id 0 25 miss% 0.5015431262451229
plot_id,batch_id 0 26 miss% 0.5388750252126464
plot_id,batch_id 0 27 miss% 0.47696174207117686
plot_id,batch_id 0 28 miss% 0.5334190133980252
plot_id,batch_id 0 29 miss% 0.4978026676280314
plot_id,batch_id 0 30 miss% 0.4656047941241804
plot_id,batch_id 0 31 miss% 0.6523118327586733
plot_id,batch_id 0 32 miss% 0.5379162197120838
plot_id,batch_id 0 33 miss% 0.525195238451556
plot_id,batch_id 0 34 miss% 0.47789074658910435
plot_id,batch_id 0 35 miss% 0.5137392438532496
plot_id,batch_id 0 36 miss% 0.6760627579180917
plot_id,batch_id 0 37 miss% 0.559155160925164
plot_id,batch_id 0 38 miss% 0.5432601680498002
plot_id,batch_id 0 39 miss% 0.5135940440888594
plot_id,batch_id 0 40 miss% 0.4883933359589135
plot_id,batch_id 0 41 miss% 0.5083116998515688
plot_id,batch_id 0 42 miss% 0.49649278259330154
plot_id,batch_id 0 43 miss% 0.4124874371417817
plot_id,batch_id 0 44 miss% 0.42106884118997523
plot_id,batch_id 0 45 miss% 0.5657760686914464
plot_id,batch_id 0 46 miss% 0.5239922311057951
plot_id,batch_id 0 47 miss% 0.523009545932429
plot_id,batch_id 0 48 miss% 0.5653256090780535
plot_id,batch_id 0 49 miss% 0.46811874358110717
plot_id,batch_id 0 50 miss% 0.5558973070794383
plot_id,batch_id 0 51 miss% 0.5332930440875965
plot_id,batch_id 0 52 miss% 0.6127791167246885
plot_id,batch_id 0 53 miss% 0.44896396595249655
plot_id,batch_id 0 54 miss% 0.4095882233313381
plot_id,batch_id 0 55 miss% 0.4423401385161375
plot_id,batch_id 0 56 miss% 0.6167064740054122
plot_id,batch_id 0 57 miss% 0.5469230679515157
plot_id,batch_id 0 58 miss% 0.5252480720834656
plot_id,batch_id 0 59 miss% 0.5761913360378412
plot_id,batch_id 0 60 miss% 0.399546996576027
plot_id,batch_id 0 61 miss% 0.48020643118597595
plot_id,batch_id 0 62 miss% 0.5056251874145633
plot_id,batch_id 0 63 miss% 0.4727649015648789
plot_id,batch_id 0 64 miss% 0.4642846581663413
plot_id,batch_id 0 65 miss% 0.3877934029789834
plot_id,batch_id 0 66 miss% 0.559408697567383
plot_id,batch_id 0 67 miss% 0.4404270650135698
plot_id,batch_id 0 68 miss% 0.5593963502248155
plot_id,batch_id 0 69 miss% 0.5579619837918621
plot_id,batch_id 0 70 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  69649
Epoch:0, Train loss:0.727009, valid loss:0.698631
Epoch:1, Train loss:0.062817, valid loss:0.017202
Epoch:2, Train loss:0.021902, valid loss:0.005721
Epoch:3, Train loss:0.015291, valid loss:0.005533
Epoch:4, Train loss:0.014043, valid loss:0.004766
Epoch:5, Train loss:0.013254, valid loss:0.004751
Epoch:6, Train loss:0.012941, valid loss:0.004175
Epoch:7, Train loss:0.012610, valid loss:0.004032
Epoch:8, Train loss:0.012353, valid loss:0.004099
Epoch:9, Train loss:0.012173, valid loss:0.004214
Epoch:10, Train loss:0.012015, valid loss:0.004089
Epoch:11, Train loss:0.011297, valid loss:0.003935
Epoch:12, Train loss:0.011133, valid loss:0.003669
Epoch:13, Train loss:0.011056, valid loss:0.003702
Epoch:14, Train loss:0.010842, valid loss:0.003198
Epoch:15, Train loss:0.005302, valid loss:0.001583
Epoch:16, Train loss:0.003100, valid loss:0.001673
Epoch:17, Train loss:0.002878, valid loss:0.001555
Epoch:18, Train loss:0.002698, valid loss:0.001403
Epoch:19, Train loss:0.002645, valid loss:0.001393
Epoch:20, Train loss:0.002499, valid loss:0.001322
Epoch:21, Train loss:0.002060, valid loss:0.001166
Epoch:22, Train loss:0.001967, valid loss:0.001262
Epoch:23, Train loss:0.001952, valid loss:0.001233
Epoch:24, Train loss:0.001907, valid loss:0.001206
Epoch:25, Train loss:0.001833, valid loss:0.001166
Epoch:26, Train loss:0.001828, valid loss:0.001210
Epoch:27, Train loss:0.001788, valid loss:0.001144
Epoch:28, Train loss:0.001736, valid loss:0.001211
Epoch:29, Train loss:0.001743, valid loss:0.001041
Epoch:30, Train loss:0.001687, valid loss:0.001170
Epoch:31, Train loss:0.001475, valid loss:0.001076
Epoch:32, Train loss:0.001425, valid loss:0.001072
Epoch:33, Train loss:0.001444, valid loss:0.001008
Epoch:34, Train loss:0.001411, valid loss:0.001031
Epoch:35, Train loss:0.001409, valid loss:0.001033
Epoch:36, Train loss:0.001373, valid loss:0.001060
Epoch:37, Train loss:0.001386, valid loss:0.001031
Epoch:38, Train loss:0.001341, valid loss:0.001079
Epoch:39, Train loss:0.001347, valid loss:0.001050
Epoch:40, Train loss:0.001311, valid loss:0.001036
Epoch:41, Train loss:0.001210, valid loss:0.000991
Epoch:42, Train loss:0.001204, valid loss:0.000987
Epoch:43, Train loss:0.001176, valid loss:0.000977
Epoch:44, Train loss:0.001179, valid loss:0.001029
Epoch:45, Train loss:0.001177, valid loss:0.000948
Epoch:46, Train loss:0.001155, valid loss:0.001016
Epoch:47, Train loss:0.001149, valid loss:0.001046
Epoch:48, Train loss:0.001149, valid loss:0.000947
Epoch:49, Train loss:0.001143, valid loss:0.001006
Epoch:50, Train loss:0.001136, valid loss:0.000999
Epoch:51, Train loss:0.001064, valid loss:0.000947
Epoch:52, Train loss:0.001054, valid loss:0.000951
Epoch:53, Train loss:0.001050, valid loss:0.000939
Epoch:54, Train loss:0.001049, valid loss:0.000937
Epoch:55, Train loss:0.001046, valid loss:0.000936
Epoch:56, Train loss:0.001045, valid loss:0.000945
Epoch:57, Train loss:0.001043, valid loss:0.000942
Epoch:58, Train loss:0.001043, valid loss:0.000949
Epoch:59, Train loss:0.001044, valid loss:0.000939
Epoch:60, Train loss:0.001042, valid loss:0.000942
training time 11796.707819700241
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.09028692671191878
plot_id,batch_id 0 1 miss% 0.0470716131912662
plot_id,batch_id 0 2 miss% 0.10505359409260537
plot_id,batch_id 0 3 miss% 0.04178973624991509
plot_id,batch_id 0 4 miss% 0.05391918042922409
plot_id,batch_id 0 5 miss% 0.0539495512108296
plot_id,batch_id 0 6 miss% 0.05066141136205421
plot_id,batch_id 0 7 miss% 0.08495924192658674
plot_id,batch_id 0 8 miss% 0.09126878632952537
plot_id,batch_id 0 9 miss% 0.03614966700240053
plot_id,batch_id 0 10 miss% 0.07548794625876552
plot_id,batch_id 0 11 miss% 0.06882331288019049
plot_id,batch_id 0 12 miss% 0.05913476913252374
plot_id,batch_id 0 13 miss% 0.07958728924297243
plot_id,batch_id 0 14 miss% 0.1023780095171458
plot_id,batch_id 0 15 miss% 0.027589159456931175
plot_id,batch_id 0 16 miss% 0.0550157576345601
plot_id,batch_id 0 17 miss% 0.048249228328152496
plot_id,batch_id 0 18 miss% 0.043359577363541915
plot_id,batch_id 0 19 miss% 0.11797350930098723
plot_id,batch_id 0 20 miss% 0.0620291177756647
plot_id,batch_id 0 21 miss% 0.029142318407288006
plot_id,batch_id 0 22 miss% 0.08902029370972968
plot_id,batch_id 0 23 miss% 0.03337874300561685
plot_id,batch_id 0 24 miss% 0.043624785458914135
plot_id,batch_id 0 25 miss% 0.055964503868890285
plot_id,batch_id 0 26 miss% 0.04669460566846632
plot_id,batch_id 0 27 miss% 0.04642769732304361
plot_id,batch_id 0 28 miss% 0.03466292730336978
plot_id,batch_id 0 29 miss% 0.022073901044557247
plot_id,batch_id 0 30 miss% 0.044065766601681475
plot_id,batch_id 0 31 miss% 0.11783265766783525
plot_id,batch_id 0 32 miss% 0.08603335420550018
plot_id,batch_id 0 33 miss% 0.06651981961277016
plot_id,batch_id 0 34 miss% 0.04344319778962305
plot_id,batch_id 0 35 miss% 0.09256724465561851
plot_id,batch_id 0 36 miss% 0.08024705070161667
plot_id,batch_id 0 37 miss% 0.0720503208864658
plot_id,batch_id 0 38 miss% 0.03802525862876911
plot_id,batch_id 0 39 miss% 0.056090441214868586
plot_id,batch_id 0 40 miss% 0.03866884506887194
plot_id,batch_id 0 41 miss% 0.037441929406816385
plot_id,batch_id 0 42 miss% 0.038304092847146885
plot_id,batch_id 0 43 miss% 0.04900939505849544
plot_id,batch_id 0 44 miss% 0.028876073482984127
plot_id,batch_id 0 45 miss% 0.041348487161716026
plot_id,batch_id 0 46 miss% 0.05539352287257922
plot_id,batch_id 0 47 miss% 0.018396116641967077
plot_id,batch_id 0 48 miss% 0.022715579464337846
plot_id,batch_id 0 49 miss% 0.03597208183401808
plot_id,batch_id 0 50 miss% 0.1421886825706204
plot_id,batch_id 0 51 miss% 0.03731111509300951
plot_id,batch_id 0 52 miss% 0.03407308512398429
plot_id,batch_id 0 53 miss% 0.025761290192042312
plot_id,batch_id 0 54 miss% 0.026107822379907403
plot_id,batch_id 0 55 miss% 0.07572383824296335
plot_id,batch_id 0 56 miss% 0.07287126557356871
plot_id,batch_id 0 57 miss% 0.040550661497976086
plot_id,batch_id 0 58 miss% 0.02878035269438515
plot_id,batch_id 0 59 miss% 0.02614132828739688
plot_id,batch_id 0 60 miss% 0.08904295063633379
plot_id,batch_id 0 61 miss% 0.038204957362940356
plot_id,batch_id 0 62 miss% 0.09697521094154195
plot_id,batch_id 0 63 miss% 0.060039031679354836
plot_id,batch_id 0 64 miss% 0.09161229096027584
plot_id,batch_id 0 65 miss% 0.04663819627839875
plot_id,batch_id 0 66 miss% 0.06334806325759892
plot_id,batch_id 0 67 miss% 0.04204170875368917
plot_id,batch_id 0 68 miss% 0.058351447564783096
0.3713955469513597
plot_id,batch_id 0 71 miss% 0.5040884037378127
plot_id,batch_id 0 72 miss% 0.49834046329508425
plot_id,batch_id 0 73 miss% 0.536007265835245
plot_id,batch_id 0 74 miss% 0.5336880948713993
plot_id,batch_id 0 75 miss% 0.39707190936257847
plot_id,batch_id 0 76 miss% 0.4574331308703105
plot_id,batch_id 0 77 miss% 0.47116733250250037
plot_id,batch_id 0 78 miss% 0.42653966560803036
plot_id,batch_id 0 79 miss% 0.5117447082527973
plot_id,batch_id 0 80 miss% 0.45614774819618736
plot_id,batch_id 0 81 miss% 0.6047514989933719
plot_id,batch_id 0 82 miss% 0.5041216614092354
plot_id,batch_id 0 83 miss% 0.505805936937393
plot_id,batch_id 0 84 miss% 0.5025847664675129
plot_id,batch_id 0 85 miss% 0.41487982850158683
plot_id,batch_id 0 86 miss% 0.435702682456928
plot_id,batch_id 0 87 miss% 0.528064199197493
plot_id,batch_id 0 88 miss% 0.523473883282161
plot_id,batch_id 0 89 miss% 0.5160090674876798
plot_id,batch_id 0 90 miss% 0.36626130928822054
plot_id,batch_id 0 91 miss% 0.418318112857759
plot_id,batch_id 0 92 miss% 0.42490948179539106
plot_id,batch_id 0 93 miss% 0.39556128583746386
plot_id,batch_id 0 94 miss% 0.5910569966963514
plot_id,batch_id 0 95 miss% 0.39067587184927544
plot_id,batch_id 0 96 miss% 0.4133539852684265
plot_id,batch_id 0 97 miss% 0.5677276224914078
plot_id,batch_id 0 98 miss% 0.5681840091879848
plot_id,batch_id 0 99 miss% 0.483133087072372
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.3985244  0.55522156 0.56915356 0.40800335 0.47928644 0.54897745
 0.43689867 0.59701554 0.64636888 0.67621112 0.40957218 0.50681171
 0.52575113 0.42702942 0.57493807 0.49895093 0.57794206 0.5516256
 0.51316243 0.50340945 0.44770562 0.54410089 0.5687944  0.55994954
 0.5808329  0.50154313 0.53887503 0.47696174 0.53341901 0.49780267
 0.46560479 0.65231183 0.53791622 0.52519524 0.47789075 0.51373924
 0.67606276 0.55915516 0.54326017 0.51359404 0.48839334 0.5083117
 0.49649278 0.41248744 0.42106884 0.56577607 0.52399223 0.52300955
 0.56532561 0.46811874 0.55589731 0.53329304 0.61277912 0.44896397
 0.40958822 0.44234014 0.61670647 0.54692307 0.52524807 0.57619134
 0.399547   0.48020643 0.50562519 0.4727649  0.46428466 0.3877934
 0.5594087  0.44042707 0.55939635 0.55796198 0.37139555 0.5040884
 0.49834046 0.53600727 0.53368809 0.39707191 0.45743313 0.47116733
 0.42653967 0.51174471 0.45614775 0.6047515  0.50412166 0.50580594
 0.50258477 0.41487983 0.43570268 0.5280642  0.52347388 0.51600907
 0.36626131 0.41831811 0.42490948 0.39556129 0.591057   0.39067587
 0.41335399 0.56772762 0.56818401 0.48313309]
for model  41 the mean error 0.5050609138183042
all id 41 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 21 out win 6 err 0.5050609138183042 time 11806.34562754631
Launcher: Job 42 completed in 12028 seconds.
Launcher: Task 199 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  69649
Epoch:0, Train loss:0.757601, valid loss:0.730829
Epoch:1, Train loss:0.059267, valid loss:0.007140
Epoch:2, Train loss:0.015036, valid loss:0.005053
Epoch:3, Train loss:0.011542, valid loss:0.003914
Epoch:4, Train loss:0.010544, valid loss:0.003540
Epoch:5, Train loss:0.009795, valid loss:0.003750
Epoch:6, Train loss:0.008979, valid loss:0.002900
Epoch:7, Train loss:0.007497, valid loss:0.002830
Epoch:8, Train loss:0.005685, valid loss:0.002616
Epoch:9, Train loss:0.004834, valid loss:0.002517
Epoch:10, Train loss:0.004537, valid loss:0.002226
Epoch:11, Train loss:0.003639, valid loss:0.001842
Epoch:12, Train loss:0.003503, valid loss:0.002020
Epoch:13, Train loss:0.003300, valid loss:0.001328
Epoch:14, Train loss:0.002465, valid loss:0.001319
Epoch:15, Train loss:0.002421, valid loss:0.001290
Epoch:16, Train loss:0.002277, valid loss:0.001534
Epoch:17, Train loss:0.002215, valid loss:0.001215
Epoch:18, Train loss:0.002188, valid loss:0.001448
Epoch:19, Train loss:0.002148, valid loss:0.001305
Epoch:20, Train loss:0.002026, valid loss:0.001175
Epoch:21, Train loss:0.001611, valid loss:0.001036
Epoch:22, Train loss:0.001559, valid loss:0.000978
Epoch:23, Train loss:0.001533, valid loss:0.000957
Epoch:24, Train loss:0.001508, valid loss:0.000980
Epoch:25, Train loss:0.001501, valid loss:0.001151
Epoch:26, Train loss:0.001485, valid loss:0.001005
Epoch:27, Train loss:0.001450, valid loss:0.000948
Epoch:28, Train loss:0.001408, valid loss:0.001042
Epoch:29, Train loss:0.001384, valid loss:0.000999
Epoch:30, Train loss:0.001354, valid loss:0.000947
Epoch:31, Train loss:0.001190, valid loss:0.000903
Epoch:32, Train loss:0.001145, valid loss:0.000858
Epoch:33, Train loss:0.001140, valid loss:0.000881
Epoch:34, Train loss:0.001142, valid loss:0.000846
Epoch:35, Train loss:0.001131, valid loss:0.001010
Epoch:36, Train loss:0.001102, valid loss:0.000877
Epoch:37, Train loss:0.001096, valid loss:0.000887
Epoch:38, Train loss:0.001091, valid loss:0.000909
Epoch:39, Train loss:0.001071, valid loss:0.000998
Epoch:40, Train loss:0.001073, valid loss:0.000868
Epoch:41, Train loss:0.000956, valid loss:0.000848
Epoch:42, Train loss:0.000954, valid loss:0.000870
Epoch:43, Train loss:0.000952, valid loss:0.000842
Epoch:44, Train loss:0.000949, valid loss:0.000814
Epoch:45, Train loss:0.000936, valid loss:0.000851
Epoch:46, Train loss:0.000936, valid loss:0.000886
Epoch:47, Train loss:0.000925, valid loss:0.000801
Epoch:48, Train loss:0.000919, valid loss:0.000800
Epoch:49, Train loss:0.000917, valid loss:0.000836
Epoch:50, Train loss:0.000925, valid loss:0.001007
Epoch:51, Train loss:0.000903, valid loss:0.000796
Epoch:52, Train loss:0.000868, valid loss:0.000780
Epoch:53, Train loss:0.000857, valid loss:0.000788
Epoch:54, Train loss:0.000851, valid loss:0.000780
Epoch:55, Train loss:0.000847, valid loss:0.000775
Epoch:56, Train loss:0.000845, valid loss:0.000786
Epoch:57, Train loss:0.000843, valid loss:0.000776
Epoch:58, Train loss:0.000841, valid loss:0.000776
Epoch:59, Train loss:0.000839, valid loss:0.000776
Epoch:60, Train loss:0.000839, valid loss:0.000782
training time 11807.991078853607
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.05843185649315816
plot_id,batch_id 0 1 miss% 0.055792071144101577
plot_id,batch_id 0 2 miss% 0.11299955083640292
plot_id,batch_id 0 3 miss% 0.046199484554305253
plot_id,batch_id 0 4 miss% 0.03999967342143923
plot_id,batch_id 0 5 miss% 0.10617518669893866
plot_id,batch_id 0 6 miss% 0.051516791489071574
plot_id,batch_id 0 7 miss% 0.09130761444007464
plot_id,batch_id 0 8 miss% 0.06895283857862283
plot_id,batch_id 0 9 miss% 0.04314109810746398
plot_id,batch_id 0 10 miss% 0.021097590290794534
plot_id,batch_id 0 11 miss% 0.07371101575216814
plot_id,batch_id 0 12 miss% 0.0403793895609651
plot_id,batch_id 0 13 miss% 0.06574953252902636
plot_id,batch_id 0 14 miss% 0.0924814533704808
plot_id,batch_id 0 15 miss% 0.06700737607559878
plot_id,batch_id 0 16 miss% 0.07392069434807627
plot_id,batch_id 0 17 miss% 0.04582306728090801
plot_id,batch_id 0 18 miss% 0.07390548394502744
plot_id,batch_id 0 19 miss% 0.07800947485502449
plot_id,batch_id 0 20 miss% 0.05552156180511595
plot_id,batch_id 0 21 miss% 0.06476765578378173
plot_id,batch_id 0 22 miss% 0.05901510334566009
plot_id,batch_id 0 23 miss% 0.029370872963272284
plot_id,batch_id 0 24 miss% 0.04966524917715582
plot_id,batch_id 0 25 miss% 0.05970881411989056
plot_id,batch_id 0 26 miss% 0.05277330305058592
plot_id,batch_id 0 27 miss% 0.05415271514752981
plot_id,batch_id 0 28 miss% 0.022386404991778745
plot_id,batch_id 0 29 miss% 0.024627230129446596
plot_id,batch_id 0 30 miss% 0.05764946578391646
plot_id,batch_id 0 31 miss% 0.0618955696386521
plot_id,batch_id 0 32 miss% 0.10148305175719168
plot_id,batch_id 0 33 miss% 0.07383250698959493
plot_id,batch_id 0 34 miss% 0.06434571891450637
plot_id,batch_id 0 35 miss% 0.1239596222312473
plot_id,batch_id 0 36 miss% 0.07204833418731138
plot_id,batch_id 0 37 miss% 0.07135496387083913
plot_id,batch_id 0 38 miss% 0.05112438604143086
plot_id,batch_id 0 39 miss% 0.04332766942085878
plot_id,batch_id 0 40 miss% 0.0817288560090176
plot_id,batch_id 0 41 miss% 0.07779120133320158
plot_id,batch_id 0 42 miss% 0.029744784250310487
plot_id,batch_id 0 43 miss% 0.056515061563932954
plot_id,batch_id 0 44 miss% 0.08229754230970444
plot_id,batch_id 0 45 miss% 0.06585062341878446
plot_id,batch_id 0 46 miss% 0.05183677537933266
plot_id,batch_id 0 47 miss% 0.03224022233763602
plot_id,batch_id 0 48 miss% 0.017040852580879163
plot_id,batch_id 0 49 miss% 0.06847409782069543
plot_id,batch_id 0 50 miss% 0.13981081095849843
plot_id,batch_id 0 51 miss% 0.048833156792337594
plot_id,batch_id 0 52 miss% 0.0289527617833605
plot_id,batch_id 0 53 miss% 0.032248296574404374
plot_id,batch_id 0 54 miss% 0.0329277053906043
plot_id,batch_id 0 55 miss% 0.0750687490644704
plot_id,batch_id 0 56 miss% 0.08060516775141337
plot_id,batch_id 0 57 miss% 0.039849633553403895
plot_id,batch_id 0 58 miss% 0.023482142966751933
plot_id,batch_id 0 59 miss% 0.028218571139405707
plot_id,batch_id 0 60 miss% 0.029457169238517187
plot_id,batch_id 0 61 miss% 0.04028212486404308
plot_id,batch_id 0 62 miss% 0.08005620408944701
plot_id,batch_id 0 63 miss% 0.033152771729568396
plot_id,batch_id 0 64 miss% 0.06573674695382038
plot_id,batch_id 0 65 miss% 0.09855495859075687
plot_id,batch_id 0 66 miss% 0.025914203058196177
plot_id,batch_id 0 67 miss% 0.059248202289932626
plot_id,batch_id 0 68 miss% 0.04714797516517591
plot_id,batch_id 0 the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  154129
Epoch:0, Train loss:0.717944, valid loss:0.727560
Epoch:1, Train loss:0.111173, valid loss:0.004902
Epoch:2, Train loss:0.010609, valid loss:0.003963
Epoch:3, Train loss:0.006064, valid loss:0.002685
Epoch:4, Train loss:0.005114, valid loss:0.002202
Epoch:5, Train loss:0.004509, valid loss:0.001929
Epoch:6, Train loss:0.004048, valid loss:0.001843
Epoch:7, Train loss:0.003618, valid loss:0.001603
Epoch:8, Train loss:0.003465, valid loss:0.001540
Epoch:9, Train loss:0.003159, valid loss:0.001455
Epoch:10, Train loss:0.002946, valid loss:0.001525
Epoch:11, Train loss:0.002066, valid loss:0.001359
Epoch:12, Train loss:0.001979, valid loss:0.001067
Epoch:13, Train loss:0.002028, valid loss:0.001089
Epoch:14, Train loss:0.001851, valid loss:0.001067
Epoch:15, Train loss:0.001833, valid loss:0.001201
Epoch:16, Train loss:0.001720, valid loss:0.001242
Epoch:17, Train loss:0.001788, valid loss:0.001084
Epoch:18, Train loss:0.001670, valid loss:0.001052
Epoch:19, Train loss:0.001624, valid loss:0.001092
Epoch:20, Train loss:0.001544, valid loss:0.001087
Epoch:21, Train loss:0.001153, valid loss:0.000858
Epoch:22, Train loss:0.001147, valid loss:0.000867
Epoch:23, Train loss:0.001099, valid loss:0.000854
Epoch:24, Train loss:0.001086, valid loss:0.000870
Epoch:25, Train loss:0.001106, valid loss:0.000908
Epoch:26, Train loss:0.001066, valid loss:0.000898
Epoch:27, Train loss:0.001008, valid loss:0.000896
Epoch:28, Train loss:0.001029, valid loss:0.000837
Epoch:29, Train loss:0.001024, valid loss:0.000843
Epoch:30, Train loss:0.000986, valid loss:0.000920
Epoch:31, Train loss:0.000801, valid loss:0.000806
Epoch:32, Train loss:0.000756, valid loss:0.000794
Epoch:33, Train loss:0.000747, valid loss:0.000841
Epoch:34, Train loss:0.000765, valid loss:0.000808
Epoch:35, Train loss:0.000740, valid loss:0.000802
Epoch:36, Train loss:0.000758, valid loss:0.001001
Epoch:37, Train loss:0.000719, valid loss:0.000879
Epoch:38, Train loss:0.000722, valid loss:0.000783
Epoch:39, Train loss:0.000719, valid loss:0.000826
Epoch:40, Train loss:0.000722, valid loss:0.000797
Epoch:41, Train loss:0.000611, valid loss:0.000782
Epoch:42, Train loss:0.000607, valid loss:0.000771
Epoch:43, Train loss:0.000606, valid loss:0.000792
Epoch:44, Train loss:0.000586, valid loss:0.000826
Epoch:45, Train loss:0.000581, valid loss:0.000783
Epoch:46, Train loss:0.000592, valid loss:0.000761
Epoch:47, Train loss:0.000595, valid loss:0.000794
Epoch:48, Train loss:0.000582, valid loss:0.000849
Epoch:49, Train loss:0.000580, valid loss:0.000776
Epoch:50, Train loss:0.000563, valid loss:0.000763
Epoch:51, Train loss:0.000525, valid loss:0.000730
Epoch:52, Train loss:0.000519, valid loss:0.000738
Epoch:53, Train loss:0.000518, valid loss:0.000768
Epoch:54, Train loss:0.000516, valid loss:0.000756
Epoch:55, Train loss:0.000516, valid loss:0.000746
Epoch:56, Train loss:0.000515, valid loss:0.000752
Epoch:57, Train loss:0.000514, valid loss:0.000758
Epoch:58, Train loss:0.000514, valid loss:0.000746
Epoch:59, Train loss:0.000513, valid loss:0.000757
Epoch:60, Train loss:0.000513, valid loss:0.000756
training time 11822.211483001709
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.09096824308974989
plot_id,batch_id 0 1 miss% 0.05642106373124511
plot_id,batch_id 0 2 miss% 0.10905318938626711
plot_id,batch_id 0 3 miss% 0.0462651895215014
plot_id,batch_id 0 4 miss% 0.03621616715149559
plot_id,batch_id 0 5 miss% 0.021193636771204137
plot_id,batch_id 0 6 miss% 0.04264332411367443
plot_id,batch_id 0 7 miss% 0.09224610992299714
plot_id,batch_id 0 8 miss% 0.07030973620292097
plot_id,batch_id 0 9 miss% 0.027204658825430317
plot_id,batch_id 0 10 miss% 0.033593802321709165
plot_id,batch_id 0 11 miss% 0.09645757771691472
plot_id,batch_id 0 12 miss% 0.083182840257068
plot_id,batch_id 0 13 miss% 0.04503393477461241
plot_id,batch_id 0 14 miss% 0.04021891593505091
plot_id,batch_id 0 15 miss% 0.056919951513967205
plot_id,batch_id 0 16 miss% 0.1949348250307857
plot_id,batch_id 0 17 miss% 0.027663069094467967
plot_id,batch_id 0 18 miss% 0.07119147974384493
plot_id,batch_id 0 19 miss% 0.056123440795889434
plot_id,batch_id 0 20 miss% 0.06108997391426341
plot_id,batch_id 0 21 miss% 0.04239567512973065
plot_id,batch_id 0 22 miss% 0.0485310703653127
plot_id,batch_id 0 23 miss% 0.03994657017707064
plot_id,batch_id 0 24 miss% 0.03580157534812604
plot_id,batch_id 0 25 miss% 0.06961636947315653
plot_id,batch_id 0 26 miss% 0.05300069822943566
plot_id,batch_id 0 27 miss% 0.03790851853889006
plot_id,batch_id 0 28 miss% 0.020795287937264654
plot_id,batch_id 0 29 miss% 0.027010553662185268
plot_id,batch_id 0 30 miss% 0.018794723758170633
plot_id,batch_id 0 31 miss% 0.07466514825962965
plot_id,batch_id 0 32 miss% 0.10444622640184764
plot_id,batch_id 0 33 miss% 0.05901893037307197
plot_id,batch_id 0 34 miss% 0.03105422878972932
plot_id,batch_id 0 35 miss% 0.07830684909103743
plot_id,batch_id 0 36 miss% 0.06291908011560263
plot_id,batch_id 0 37 miss% 0.07736223765207004
plot_id,batch_id 0 38 miss% 0.032004730813063036
plot_id,batch_id 0 39 miss% 0.03080733592226359
plot_id,batch_id 0 40 miss% 0.07709934345272698
plot_id,batch_id 0 41 miss% 0.05095715476811573
plot_id,batch_id 0 42 miss% 0.05299295975552943
plot_id,batch_id 0 43 miss% 0.04162904944505284
plot_id,batch_id 0 44 miss% 0.04594961634402623
plot_id,batch_id 0 45 miss% 0.033447350472815024
plot_id,batch_id 0 46 miss% 0.02586221292583696
plot_id,batch_id 0 47 miss% 0.03127777190430275
plot_id,batch_id 0 48 miss% 0.021607028714828346
plot_id,batch_id 0 49 miss% 0.04229469764505448
plot_id,batch_id 0 50 miss% 0.1412557540775678
plot_id,batch_id 0 51 miss% 0.03366344435824875
plot_id,batch_id 0 52 miss% 0.013909651130155116
plot_id,batch_id 0 53 miss% 0.04017408239066894
plot_id,batch_id 0 54 miss% 0.03651270429094332
plot_id,batch_id 0 55 miss% 0.11209556332109799
plot_id,batch_id 0 56 miss% 0.08720071424212643
plot_id,batch_id 0 57 miss% 0.04764856567861465
plot_id,batch_id 0 58 miss% 0.03476870327768579
plot_id,batch_id 0 59 miss% 0.026344808530962945
plot_id,batch_id 0 60 miss% 0.03899318644626627
plot_id,batch_id 0 61 miss% 0.02256870758179437
plot_id,batch_id 0 62 miss% 0.08240121043090848
plot_id,batch_id 0 63 miss% 0.04336541036575916
plot_id,batch_id 0 64 miss% 0.0655272566679937
plot_id,batch_id 0 65 miss% 0.10448157303558345
plot_id,batch_id 0 66 miss% 0.09218518421683652
plot_id,batch_id 0 67 miss% 0.024902150299612832
plot_id,batch_id 0 68 miss% 0.041365961358835274
plot_id,batch_id 0  miss% 0.21101371997456886
plot_id,batch_id 0 66 miss% 0.03476391196978915
plot_id,batch_id 0 67 miss% 0.05913216171048791
plot_id,batch_id 0 68 miss% 0.04575278871178703
plot_id,batch_id 0 69 miss% 0.09504671524946325
plot_id,batch_id 0 70 miss% 0.061743290932484575
plot_id,batch_id 0 71 miss% 0.07848528121803225
plot_id,batch_id 0 72 miss% 0.047182679804616254
plot_id,batch_id 0 73 miss% 0.07019984087525495
plot_id,batch_id 0 74 miss% 0.1250273367967545
plot_id,batch_id 0 75 miss% 0.037763650397754665
plot_id,batch_id 0 76 miss% 0.07317086430896663
plot_id,batch_id 0 77 miss% 0.03229280860642
plot_id,batch_id 0 78 miss% 0.04200003342867677
plot_id,batch_id 0 79 miss% 0.11678531878451195
plot_id,batch_id 0 80 miss% 0.08825097382582396
plot_id,batch_id 0 81 miss% 0.12290463447945273
plot_id,batch_id 0 82 miss% 0.06786499397078445
plot_id,batch_id 0 83 miss% 0.11279262467905887
plot_id,batch_id 0 84 miss% 0.05629933455873333
plot_id,batch_id 0 85 miss% 0.05699548409464504
plot_id,batch_id 0 86 miss% 0.058672973842597446
plot_id,batch_id 0 87 miss% 0.060814503545743887
plot_id,batch_id 0 88 miss% 0.09666453896789923
plot_id,batch_id 0 89 miss% 0.06652940525745643
plot_id,batch_id 0 90 miss% 0.047393063621565354
plot_id,batch_id 0 91 miss% 0.054316357379380495
plot_id,batch_id 0 92 miss% 0.043166262050111634
plot_id,batch_id 0 93 miss% 0.08706057472742061
plot_id,batch_id 0 94 miss% 0.0883621127530171
plot_id,batch_id 0 95 miss% 0.048001010440667055
plot_id,batch_id 0 96 miss% 0.03599616179300183
plot_id,batch_id 0 97 miss% 0.05190496173283839
plot_id,batch_id 0 98 miss% 0.04385265956949596
plot_id,batch_id 0 99 miss% 0.06511446655701074
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10150397 0.04802517 0.08027398 0.06273123 0.07027504 0.03656293
 0.05665789 0.06163057 0.07747073 0.02626951 0.02978087 0.05093093
 0.09129609 0.09759858 0.10737268 0.06507401 0.12347253 0.042532
 0.08196107 0.0963324  0.03007353 0.0421917  0.07363226 0.05115638
 0.06057903 0.05409707 0.06750668 0.05270007 0.04167279 0.04164928
 0.02591362 0.11669496 0.11250483 0.04958182 0.06550943 0.10113694
 0.14723625 0.08043775 0.07470424 0.03962705 0.09866336 0.05558819
 0.03503475 0.0685739  0.0310008  0.05470191 0.084202   0.03547098
 0.03413923 0.03526263 0.11594684 0.05612291 0.02522844 0.02252639
 0.02996837 0.06170286 0.12182376 0.07719274 0.0407511  0.04505616
 0.0315692  0.02838076 0.22930527 0.06718104 0.06858371 0.21101372
 0.03476391 0.05913216 0.04575279 0.09504672 0.06174329 0.07848528
 0.04718268 0.07019984 0.12502734 0.03776365 0.07317086 0.03229281
 0.04200003 0.11678532 0.08825097 0.12290463 0.06786499 0.11279262
 0.05629933 0.05699548 0.05867297 0.0608145  0.09666454 0.06652941
 0.04739306 0.05431636 0.04316626 0.08706057 0.08836211 0.04800101
 0.03599616 0.05190496 0.04385266 0.06511447]
for model  225 the mean error 0.06773652667735124
all id 225 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 31 out win 4 err 0.06773652667735124 time 11777.983351945877
Launcher: Job 226 completed in 12042 seconds.
Launcher: Task 191 done. Exiting.
plot_id,batch_id 0 69 miss% 0.11924634450178348
plot_id,batch_id 0 70 miss% 0.12693226619636763
plot_id,batch_id 0 71 miss% 0.05393675914259524
plot_id,batch_id 0 72 miss% 0.10982111990000905
plot_id,batch_id 0 73 miss% 0.07800971514314109
plot_id,batch_id 0 74 miss% 0.09484183080399188
plot_id,batch_id 0 75 miss% 0.08486895978599812
plot_id,batch_id 0 76 miss% 0.1257890152522661
plot_id,batch_id 0 77 miss% 0.06638624313746193
plot_id,batch_id 0 78 miss% 0.048092820010460134
plot_id,batch_id 0 79 miss% 0.07947575408242905
plot_id,batch_id 0 80 miss% 0.031087957126364148
plot_id,batch_id 0 81 miss% 0.09197926644522299
plot_id,batch_id 0 82 miss% 0.11518719964002334
plot_id,batch_id 0 83 miss% 0.09054354689254018
plot_id,batch_id 0 84 miss% 0.07331081784989933
plot_id,batch_id 0 85 miss% 0.0340367272314835
plot_id,batch_id 0 86 miss% 0.05707087327708619
plot_id,batch_id 0 87 miss% 0.10853020682427822
plot_id,batch_id 0 88 miss% 0.11809602986225677
plot_id,batch_id 0 89 miss% 0.08054681188802582
plot_id,batch_id 0 90 miss% 0.06130126310517596
plot_id,batch_id 0 91 miss% 0.09927773440873548
plot_id,batch_id 0 92 miss% 0.02672633006218208
plot_id,batch_id 0 93 miss% 0.08310166457367217
plot_id,batch_id 0 94 miss% 0.08564570009729694
plot_id,batch_id 0 95 miss% 0.059762760107472446
plot_id,batch_id 0 96 miss% 0.06328974697616162
plot_id,batch_id 0 97 miss% 0.045143355693117934
plot_id,batch_id 0 98 miss% 0.10207094257847106
plot_id,batch_id 0 99 miss% 0.049016098257119875
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09028693 0.04707161 0.10505359 0.04178974 0.05391918 0.05394955
 0.05066141 0.08495924 0.09126879 0.03614967 0.07548795 0.06882331
 0.05913477 0.07958729 0.10237801 0.02758916 0.05501576 0.04824923
 0.04335958 0.11797351 0.06202912 0.02914232 0.08902029 0.03337874
 0.04362479 0.0559645  0.04669461 0.0464277  0.03466293 0.0220739
 0.04406577 0.11783266 0.08603335 0.06651982 0.0434432  0.09256724
 0.08024705 0.07205032 0.03802526 0.05609044 0.03866885 0.03744193
 0.03830409 0.0490094  0.02887607 0.04134849 0.05539352 0.01839612
 0.02271558 0.03597208 0.14218868 0.03731112 0.03407309 0.02576129
 0.02610782 0.07572384 0.07287127 0.04055066 0.02878035 0.02614133
 0.08904295 0.03820496 0.09697521 0.06003903 0.09161229 0.0466382
 0.06334806 0.04204171 0.05835145 0.11924634 0.12693227 0.05393676
 0.10982112 0.07800972 0.09484183 0.08486896 0.12578902 0.06638624
 0.04809282 0.07947575 0.03108796 0.09197927 0.1151872  0.09054355
 0.07331082 0.03403673 0.05707087 0.10853021 0.11809603 0.08054681
 0.06130126 0.09927773 0.02672633 0.08310166 0.0856457  0.05976276
 0.06328975 0.04514336 0.10207094 0.0490161 ]
for model  47 the mean error 0.06417617556935179
all id 47 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 21 out win 6 err 0.06417617556935179 time 11796.707819700241
Launcher: Job 48 completed in 12068 seconds.
Launcher: Task 10 done. Exiting.
69 miss% 0.11573640624224558
plot_id,batch_id 0 70 miss% 0.05318537976711106
plot_id,batch_id 0 71 miss% 0.04282946061541478
plot_id,batch_id 0 72 miss% 0.08636083171725756
plot_id,batch_id 0 73 miss% 0.05198646070335271
plot_id,batch_id 0 74 miss% 0.09330726880640666
plot_id,batch_id 0 75 miss% 0.11280189398880999
plot_id,batch_id 0 76 miss% 0.1251359025203703
plot_id,batch_id 0 77 miss% 0.04645386272418091
plot_id,batch_id 0 78 miss% 0.024527972676680415
plot_id,batch_id 0 79 miss% 0.05795031057982477
plot_id,batch_id 0 80 miss% 0.03760634973580899
plot_id,batch_id 0 81 miss% 0.05971236922409966
plot_id,batch_id 0 82 miss% 0.04225342965736874
plot_id,batch_id 0 83 miss% 0.06779930285192884
plot_id,batch_id 0 84 miss% 0.11590397773015686
plot_id,batch_id 0 85 miss% 0.03126506289110612
plot_id,batch_id 0 86 miss% 0.06299972238451727
plot_id,batch_id 0 87 miss% 0.0925201373285046
plot_id,batch_id 0 88 miss% 0.07658805751793378
plot_id,batch_id 0 89 miss% 0.05524275435086107
plot_id,batch_id 0 90 miss% 0.05681124372015226
plot_id,batch_id 0 91 miss% 0.05532973932921196
plot_id,batch_id 0 92 miss% 0.03319784547010288
plot_id,batch_id 0 93 miss% 0.0734657216184801
plot_id,batch_id 0 94 miss% 0.07310408509981738
plot_id,batch_id 0 95 miss% 0.07114234016828945
plot_id,batch_id 0 96 miss% 0.05279068548267743
plot_id,batch_id 0 97 miss% 0.06727889393807127
plot_id,batch_id 0 98 miss% 0.05346717930139537
plot_id,batch_id 0 99 miss% 0.10261214450579781
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05843186 0.05579207 0.11299955 0.04619948 0.03999967 0.10617519
 0.05151679 0.09130761 0.06895284 0.0431411  0.02109759 0.07371102
 0.04037939 0.06574953 0.09248145 0.06700738 0.07392069 0.04582307
 0.07390548 0.07800947 0.05552156 0.06476766 0.0590151  0.02937087
 0.04966525 0.05970881 0.0527733  0.05415272 0.0223864  0.02462723
 0.05764947 0.06189557 0.10148305 0.07383251 0.06434572 0.12395962
 0.07204833 0.07135496 0.05112439 0.04332767 0.08172886 0.0777912
 0.02974478 0.05651506 0.08229754 0.06585062 0.05183678 0.03224022
 0.01704085 0.0684741  0.13981081 0.04883316 0.02895276 0.0322483
 0.03292771 0.07506875 0.08060517 0.03984963 0.02348214 0.02821857
 0.02945717 0.04028212 0.0800562  0.03315277 0.06573675 0.09855496
 0.0259142  0.0592482  0.04714798 0.11573641 0.05318538 0.04282946
 0.08636083 0.05198646 0.09330727 0.11280189 0.1251359  0.04645386
 0.02452797 0.05795031 0.03760635 0.05971237 0.04225343 0.0677993
 0.11590398 0.03126506 0.06299972 0.09252014 0.07658806 0.05524275
 0.05681124 0.05532974 0.03319785 0.07346572 0.07310409 0.07114234
 0.05279069 0.06727889 0.05346718 0.10261214]
for model  46 the mean error 0.06164045604726955
all id 46 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 21 out win 5 err 0.06164045604726955 time 11807.991078853607
Launcher: Job 47 completed in 12079 seconds.
Launcher: Task 6 done. Exiting.
69 miss% 0.0995589908464907
plot_id,batch_id 0 70 miss% 0.07383223759134991
plot_id,batch_id 0 71 miss% 0.04943892747857623
plot_id,batch_id 0 72 miss% 0.08447372777590245
plot_id,batch_id 0 73 miss% 0.060048657341138664
plot_id,batch_id 0 74 miss% 0.06742992070516732
plot_id,batch_id 0 75 miss% 0.09268788119306454
plot_id,batch_id 0 76 miss% 0.1164148350870345
plot_id,batch_id 0 77 miss% 0.0896976475576621
plot_id,batch_id 0 78 miss% 0.02476573168689906
plot_id,batch_id 0 79 miss% 0.07949846938086376
plot_id,batch_id 0 80 miss% 0.0595601616458091
plot_id,batch_id 0 81 miss% 0.0816180447323571
plot_id,batch_id 0 82 miss% 0.06482333722298188
plot_id,batch_id 0 83 miss% 0.07500046809954589
plot_id,batch_id 0 84 miss% 0.0663876411200862
plot_id,batch_id 0 85 miss% 0.029499679164504623
plot_id,batch_id 0 86 miss% 0.04084853216842291
plot_id,batch_id 0 87 miss% 0.09139625980685781
plot_id,batch_id 0 88 miss% 0.07922565260856651
plot_id,batch_id 0 89 miss% 0.06829438769129145
plot_id,batch_id 0 90 miss% 0.04421767853234769
plot_id,batch_id 0 91 miss% 0.05513291808520566
plot_id,batch_id 0 92 miss% 0.032799289319563524
plot_id,batch_id 0 93 miss% 0.05815587904528851
plot_id,batch_id 0 94 miss% 0.09988076597987566
plot_id,batch_id 0 95 miss% 0.08540608694394777
plot_id,batch_id 0 96 miss% 0.042292080206553145
plot_id,batch_id 0 97 miss% 0.056014167097230096
plot_id,batch_id 0 98 miss% 0.030817470542718245
plot_id,batch_id 0 99 miss% 0.07910198119328274
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09096824 0.05642106 0.10905319 0.04626519 0.03621617 0.02119364
 0.04264332 0.09224611 0.07030974 0.02720466 0.0335938  0.09645758
 0.08318284 0.04503393 0.04021892 0.05691995 0.19493483 0.02766307
 0.07119148 0.05612344 0.06108997 0.04239568 0.04853107 0.03994657
 0.03580158 0.06961637 0.0530007  0.03790852 0.02079529 0.02701055
 0.01879472 0.07466515 0.10444623 0.05901893 0.03105423 0.07830685
 0.06291908 0.07736224 0.03200473 0.03080734 0.07709934 0.05095715
 0.05299296 0.04162905 0.04594962 0.03344735 0.02586221 0.03127777
 0.02160703 0.0422947  0.14125575 0.03366344 0.01390965 0.04017408
 0.0365127  0.11209556 0.08720071 0.04764857 0.0347687  0.02634481
 0.03899319 0.02256871 0.08240121 0.04336541 0.06552726 0.10448157
 0.09218518 0.02490215 0.04136596 0.09955899 0.07383224 0.04943893
 0.08447373 0.06004866 0.06742992 0.09268788 0.11641484 0.08969765
 0.02476573 0.07949847 0.05956016 0.08161804 0.06482334 0.07500047
 0.06638764 0.02949968 0.04084853 0.09139626 0.07922565 0.06829439
 0.04421768 0.05513292 0.03279929 0.05815588 0.09988077 0.08540609
 0.04229208 0.05601417 0.03081747 0.07910198]
for model  34 the mean error 0.05894118264833255
all id 34 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 21 out win 5 err 0.05894118264833255 time 11822.211483001709
Launcher: Job 35 completed in 12087 seconds.
Launcher: Task 201 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  89105
Epoch:0, Train loss:0.515416, valid loss:0.475198
Epoch:1, Train loss:0.095383, valid loss:0.003569
Epoch:2, Train loss:0.008257, valid loss:0.003135
Epoch:3, Train loss:0.007263, valid loss:0.002735
Epoch:4, Train loss:0.005655, valid loss:0.002108
Epoch:5, Train loss:0.005162, valid loss:0.002057
Epoch:6, Train loss:0.004783, valid loss:0.001786
Epoch:7, Train loss:0.004719, valid loss:0.001707
Epoch:8, Train loss:0.004307, valid loss:0.001751
Epoch:9, Train loss:0.004182, valid loss:0.001792
Epoch:10, Train loss:0.003987, valid loss:0.001506
Epoch:11, Train loss:0.002667, valid loss:0.001250
Epoch:12, Train loss:0.001866, valid loss:0.001083
Epoch:13, Train loss:0.001787, valid loss:0.001068
Epoch:14, Train loss:0.001688, valid loss:0.001040
Epoch:15, Train loss:0.001673, valid loss:0.000980
Epoch:16, Train loss:0.001552, valid loss:0.001226
Epoch:17, Train loss:0.001587, valid loss:0.000982
Epoch:18, Train loss:0.001579, valid loss:0.000982
Epoch:19, Train loss:0.001419, valid loss:0.000901
Epoch:20, Train loss:0.001394, valid loss:0.000922
Epoch:21, Train loss:0.001097, valid loss:0.000758
Epoch:22, Train loss:0.001044, valid loss:0.000737
Epoch:23, Train loss:0.001048, valid loss:0.000736
Epoch:24, Train loss:0.001052, valid loss:0.000751
Epoch:25, Train loss:0.001009, valid loss:0.000815
Epoch:26, Train loss:0.000999, valid loss:0.000801
Epoch:27, Train loss:0.000970, valid loss:0.000731
Epoch:28, Train loss:0.000967, valid loss:0.000725
Epoch:29, Train loss:0.000956, valid loss:0.000804
Epoch:30, Train loss:0.000923, valid loss:0.000681
Epoch:31, Train loss:0.000781, valid loss:0.000698
Epoch:32, Train loss:0.000761, valid loss:0.000673
Epoch:33, Train loss:0.000757, valid loss:0.000765
Epoch:34, Train loss:0.000747, valid loss:0.000638
Epoch:35, Train loss:0.000732, valid loss:0.000637
Epoch:36, Train loss:0.000737, valid loss:0.000696
Epoch:37, Train loss:0.000717, valid loss:0.000632
Epoch:38, Train loss:0.000707, valid loss:0.000665
Epoch:39, Train loss:0.000727, valid loss:0.000650
Epoch:40, Train loss:0.000711, valid loss:0.000631
Epoch:41, Train loss:0.000620, valid loss:0.000615
Epoch:42, Train loss:0.000624, valid loss:0.000614
Epoch:43, Train loss:0.000610, valid loss:0.000581
Epoch:44, Train loss:0.000603, valid loss:0.000611
Epoch:45, Train loss:0.000607, valid loss:0.000619
Epoch:46, Train loss:0.000601, valid loss:0.000617
Epoch:47, Train loss:0.000597, valid loss:0.000588
Epoch:48, Train loss:0.000593, valid loss:0.000611
Epoch:49, Train loss:0.000593, valid loss:0.000599
Epoch:50, Train loss:0.000582, valid loss:0.000617
Epoch:51, Train loss:0.000544, valid loss:0.000580
Epoch:52, Train loss:0.000538, valid loss:0.000576
Epoch:53, Train loss:0.000536, valid loss:0.000578
Epoch:54, Train loss:0.000535, valid loss:0.000582
Epoch:55, Train loss:0.000534, valid loss:0.000578
Epoch:56, Train loss:0.000533, valid loss:0.000583
Epoch:57, Train loss:0.000532, valid loss:0.000580
Epoch:58, Train loss:0.000532, valid loss:0.000578
Epoch:59, Train loss:0.000531, valid loss:0.000584
Epoch:60, Train loss:0.000531, valid loss:0.000573
training time 11915.54627776146
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3768705705925016
plot_id,batch_id 0 1 miss% 0.4425598359530653
plot_id,batch_id 0 2 miss% 0.4881155607172306
plot_id,batch_id 0 3 miss% 0.3771875366799828
plot_id,batch_id 0 4 miss% 0.36790472791317014
plot_id,batch_id 0 5 miss% 0.436776849541717
plot_id,batch_id 0 6 miss% 0.42441538065109946
plot_id,batch_id 0 7 miss% 0.5324402549372929
plot_id,batch_id 0 8 miss% 0.5157460397622715
plot_id,batch_id 0 9 miss% 0.4454580869129329
plot_id,batch_id 0 10 miss% 0.4149627569743518
plot_id,batch_id 0 11 miss% 0.3879768678965503
plot_id,batch_id 0 12 miss% 0.43903635368083166
plot_id,batch_id 0 13 miss% 0.3633862773342155
plot_id,batch_id 0 14 miss% 0.45666236935120325
plot_id,batch_id 0 15 miss% 0.3891812648065417
plot_id,batch_id 0 16 miss% 0.5343690520040202
plot_id,batch_id 0 17 miss% 0.5589876546532805
plot_id,batch_id 0 18 miss% 0.46194800823867166
plot_id,batch_id 0 19 miss% 0.537010527728012
plot_id,batch_id 0 20 miss% 0.41521509823809377
plot_id,batch_id 0 21 miss% 0.456011575253845
plot_id,batch_id 0 22 miss% 0.4324998700333824
plot_id,batch_id 0 23 miss% 0.4221573594050244
plot_id,batch_id 0 24 miss% 0.41415637537736616
plot_id,batch_id 0 25 miss% 0.474090259035598
plot_id,batch_id 0 26 miss% 0.4326562877732926
plot_id,batch_id 0 27 miss% 0.4390504174190917
plot_id,batch_id 0 28 miss% 0.4204906948569706
plot_id,batch_id 0 29 miss% 0.3784384067233462
plot_id,batch_id 0 30 miss% 0.4472239711980663
plot_id,batch_id 0 31 miss% 0.5373422435647087
plot_id,batch_id 0 32 miss% 0.45845211627212895
plot_id,batch_id 0 33 miss% 0.4309610062898793
plot_id,batch_id 0 34 miss% 0.36227153455767896
plot_id,batch_id 0 35 miss% 0.4736173399872716
plot_id,batch_id 0 36 miss% 0.549197923921429
plot_id,batch_id 0 37 miss% 0.37687271869365785
plot_id,batch_id 0 38 miss% 0.5085543136228969
plot_id,batch_id 0 39 miss% 0.44160937418483187
plot_id,batch_id 0 40 miss% 0.44614384084707603
plot_id,batch_id 0 41 miss% 0.4255977738946756
plot_id,batch_id 0 42 miss% 0.34498522624234607
plot_id,batch_id 0 43 miss% 0.33280573926665635
plot_id,batch_id 0 44 miss% 0.4227664824830066
plot_id,batch_id 0 45 miss% 0.38005989575455174
plot_id,batch_id 0 46 miss% 0.45395017181636604
plot_id,batch_id 0 47 miss% 0.4221566109615357
plot_id,batch_id 0 48 miss% 0.42512531080023924
plot_id,batch_id 0 49 miss% 0.31802213428539
plot_id,batch_id 0 50 miss% 0.5397439639909241
plot_id,batch_id 0 51 miss% 0.5402371539932552
plot_id,batch_id 0 52 miss% 0.45446589777749863
plot_id,batch_id 0 53 miss% 0.37701109407098127
plot_id,batch_id 0 54 miss% 0.32772178759357495
plot_id,batch_id 0 55 miss% 0.5616586851931732
plot_id,batch_id 0 56 miss% 0.5655539374506724
plot_id,batch_id 0 57 miss% 0.5033599230720456
plot_id,batch_id 0 58 miss% 0.4478765623318593
plot_id,batch_id 0 59 miss% 0.540670615200072
plot_id,batch_id 0 60 miss% 0.332789874296533
plot_id,batch_id 0 61 miss% 0.35244575720853927
plot_id,batch_id 0 62 miss% 0.5301379480257437
plot_id,batch_id 0 63 miss% 0.45633637314138575
plot_id,batch_id 0 64 miss% 0.46748725485353476
plot_id,batch_id 0 65 miss% 0.4022067910274957
plot_id,batch_id 0 66 miss% 0.5516119033632231
plot_id,batch_id 0 67 miss% 0.3994436590452566
plot_id,batch_id 0 68 miss% 0.4992751682484173
plot_id,batch_id 0 69 miss% 0.547403885156584
plot_id,batch_id 0 70 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  89105
Epoch:0, Train loss:0.562425, valid loss:0.517456
Epoch:1, Train loss:0.072199, valid loss:0.004657
Epoch:2, Train loss:0.012685, valid loss:0.003982
Epoch:3, Train loss:0.011751, valid loss:0.003854
Epoch:4, Train loss:0.011132, valid loss:0.003552
Epoch:5, Train loss:0.006706, valid loss:0.001927
Epoch:6, Train loss:0.003312, valid loss:0.001409
Epoch:7, Train loss:0.002753, valid loss:0.001307
Epoch:8, Train loss:0.002539, valid loss:0.001194
Epoch:9, Train loss:0.002315, valid loss:0.001254
Epoch:10, Train loss:0.002187, valid loss:0.001072
Epoch:11, Train loss:0.001689, valid loss:0.000976
Epoch:12, Train loss:0.001634, valid loss:0.000918
Epoch:13, Train loss:0.001584, valid loss:0.000859
Epoch:14, Train loss:0.001530, valid loss:0.000815
Epoch:15, Train loss:0.001514, valid loss:0.001009
Epoch:16, Train loss:0.001457, valid loss:0.000781
Epoch:17, Train loss:0.001401, valid loss:0.000885
Epoch:18, Train loss:0.001354, valid loss:0.000786
Epoch:19, Train loss:0.001323, valid loss:0.000732
Epoch:20, Train loss:0.001321, valid loss:0.000709
Epoch:21, Train loss:0.001055, valid loss:0.000672
Epoch:22, Train loss:0.001030, valid loss:0.000688
Epoch:23, Train loss:0.001023, valid loss:0.000665
Epoch:24, Train loss:0.001009, valid loss:0.000706
Epoch:25, Train loss:0.001000, valid loss:0.000637
Epoch:26, Train loss:0.000967, valid loss:0.000602
Epoch:27, Train loss:0.000960, valid loss:0.000632
Epoch:28, Train loss:0.000937, valid loss:0.000637
Epoch:29, Train loss:0.000933, valid loss:0.000615
Epoch:30, Train loss:0.000925, valid loss:0.000601
Epoch:31, Train loss:0.000807, valid loss:0.000569
Epoch:32, Train loss:0.000793, valid loss:0.000560
Epoch:33, Train loss:0.000789, valid loss:0.000559
Epoch:34, Train loss:0.000778, valid loss:0.000560
Epoch:35, Train loss:0.000780, valid loss:0.000554
Epoch:36, Train loss:0.000776, valid loss:0.000584
Epoch:37, Train loss:0.000764, valid loss:0.000583
Epoch:38, Train loss:0.000754, valid loss:0.000572
Epoch:39, Train loss:0.000749, valid loss:0.000573
Epoch:40, Train loss:0.000743, valid loss:0.000547
Epoch:41, Train loss:0.000687, valid loss:0.000532
Epoch:42, Train loss:0.000684, valid loss:0.000541
Epoch:43, Train loss:0.000677, valid loss:0.000544
Epoch:44, Train loss:0.000673, valid loss:0.000540
Epoch:45, Train loss:0.000675, valid loss:0.000527
Epoch:46, Train loss:0.000673, valid loss:0.000536
Epoch:47, Train loss:0.000667, valid loss:0.000546
Epoch:48, Train loss:0.000667, valid loss:0.000532
Epoch:49, Train loss:0.000660, valid loss:0.000566
Epoch:50, Train loss:0.000653, valid loss:0.000530
Epoch:51, Train loss:0.000628, valid loss:0.000513
Epoch:52, Train loss:0.000620, valid loss:0.000516
Epoch:53, Train loss:0.000618, valid loss:0.000512
Epoch:54, Train loss:0.000617, valid loss:0.000515
Epoch:55, Train loss:0.000616, valid loss:0.000522
Epoch:56, Train loss:0.000616, valid loss:0.000520
Epoch:57, Train loss:0.000615, valid loss:0.000517
Epoch:58, Train loss:0.000615, valid loss:0.000517
Epoch:59, Train loss:0.000614, valid loss:0.000515
Epoch:60, Train loss:0.000614, valid loss:0.000517
training time 11902.699295282364
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.04098653721956542
plot_id,batch_id 0 1 miss% 0.045134837036978494
plot_id,batch_id 0 2 miss% 0.1322808715844092
plot_id,batch_id 0 3 miss% 0.06253787085011489
plot_id,batch_id 0 4 miss% 0.04481786851202215
plot_id,batch_id 0 5 miss% 0.06107503949106763
plot_id,batch_id 0 6 miss% 0.04724085631841731
plot_id,batch_id 0 7 miss% 0.10338893214631766
plot_id,batch_id 0 8 miss% 0.08763518538801084
plot_id,batch_id 0 9 miss% 0.07230839105333114
plot_id,batch_id 0 10 miss% 0.1094533402164511
plot_id,batch_id 0 11 miss% 0.10068751447347052
plot_id,batch_id 0 12 miss% 0.08741860160465482
plot_id,batch_id 0 13 miss% 0.07284073141189945
plot_id,batch_id 0 14 miss% 0.09434923187240554
plot_id,batch_id 0 15 miss% 0.06769254579703342
plot_id,batch_id 0 16 miss% 0.14558104762619267
plot_id,batch_id 0 17 miss% 0.0261521382039832
plot_id,batch_id 0 18 miss% 0.047602060508687344
plot_id,batch_id 0 19 miss% 0.12510437384453507
plot_id,batch_id 0 20 miss% 0.10800813061737755
plot_id,batch_id 0 21 miss% 0.0373562122740699
plot_id,batch_id 0 22 miss% 0.039158553424140805
plot_id,batch_id 0 23 miss% 0.029046046364181163
plot_id,batch_id 0 24 miss% 0.05866304467928374
plot_id,batch_id 0 25 miss% 0.07038697797303445
plot_id,batch_id 0 26 miss% 0.0645274753889515
plot_id,batch_id 0 27 miss% 0.04265792178484207
plot_id,batch_id 0 28 miss% 0.027290480266285656
plot_id,batch_id 0 29 miss% 0.025700150043641744
plot_id,batch_id 0 30 miss% 0.045880066157208806
plot_id,batch_id 0 31 miss% 0.10530363380064074
plot_id,batch_id 0 32 miss% 0.09826407058784443
plot_id,batch_id 0 33 miss% 0.043851711460337055
plot_id,batch_id 0 34 miss% 0.06664004989583794
plot_id,batch_id 0 35 miss% 0.047765504005715186
plot_id,batch_id 0 36 miss% 0.11887074610242156
plot_id,batch_id 0 37 miss% 0.11566218074879205
plot_id,batch_id 0 38 miss% 0.04679897605523728
plot_id,batch_id 0 39 miss% 0.055223355577574615
plot_id,batch_id 0 40 miss% 0.19121208938117987
plot_id,batch_id 0 41 miss% 0.09285559894746527
plot_id,batch_id 0 42 miss% 0.024518005796188704
plot_id,batch_id 0 43 miss% 0.0594302132913941
plot_id,batch_id 0 44 miss% 0.028011956571602062
plot_id,batch_id 0 45 miss% 0.057660887959881266
plot_id,batch_id 0 46 miss% 0.02168892324250078
plot_id,batch_id 0 47 miss% 0.04138378329018183
plot_id,batch_id 0 48 miss% 0.034306589091791274
plot_id,batch_id 0 49 miss% 0.01847481514141401
plot_id,batch_id 0 50 miss% 0.1530499718232836
plot_id,batch_id 0 51 miss% 0.036348940352793534
plot_id,batch_id 0 52 miss% 0.046827664081283374
plot_id,batch_id 0 53 miss% 0.02666701750837226
plot_id,batch_id 0 54 miss% 0.03191794451765683
plot_id,batch_id 0 55 miss% 0.10240624511235633
plot_id,batch_id 0 56 miss% 0.0745103664796664
plot_id,batch_id 0 57 miss% 0.05232803007155817
plot_id,batch_id 0 58 miss% 0.04084230344809582
plot_id,batch_id 0 59 miss% 0.027159760546638507
plot_id,batch_id 0 60 miss% 0.04156837261684128
plot_id,batch_id 0 61 miss% 0.019072192058022627
plot_id,batch_id 0 62 miss% 0.046109407304594674
plot_id,batch_id 0 63 miss% 0.06431822161320154
plot_id,batch_id 0 64 miss% 0.0652660975998906
plot_id,batch_id 0 65 miss% 0.04730010365107327
plot_id,batch_id 0 66 miss% 0.054469405993040766
0.3488114328761885
plot_id,batch_id 0 71 miss% 0.4364782173603853
plot_id,batch_id 0 72 miss% 0.4726467116068416
plot_id,batch_id 0 73 miss% 0.6110663093388227
plot_id,batch_id 0 74 miss% 0.395011235417407
plot_id,batch_id 0 75 miss% 0.47722823942090364
plot_id,batch_id 0 76 miss% 0.41824892528718316
plot_id,batch_id 0 77 miss% 0.44000961923606047
plot_id,batch_id 0 78 miss% 0.4003008606592381
plot_id,batch_id 0 79 miss% 0.409052444999632
plot_id,batch_id 0 80 miss% 0.5497192721521191
plot_id,batch_id 0 81 miss% 0.47688327380915807
plot_id,batch_id 0 82 miss% 0.46323222868523123
plot_id,batch_id 0 83 miss% 0.4625597391207641
plot_id,batch_id 0 84 miss% 0.3692839206792206
plot_id,batch_id 0 85 miss% 0.3486010101596192
plot_id,batch_id 0 86 miss% 0.4250399148572057
plot_id,batch_id 0 87 miss% 0.4574759662773397
plot_id,batch_id 0 88 miss% 0.4894179650832504
plot_id,batch_id 0 89 miss% 0.5322577233370114
plot_id,batch_id 0 90 miss% 0.30927190768092155
plot_id,batch_id 0 91 miss% 0.38681631437476244
plot_id,batch_id 0 92 miss% 0.41564135597331087
plot_id,batch_id 0 93 miss% 0.40691622405568606
plot_id,batch_id 0 94 miss% 0.5717385481386528
plot_id,batch_id 0 95 miss% 0.3134047179686469
plot_id,batch_id 0 96 miss% 0.4474660408043143
plot_id,batch_id 0 97 miss% 0.5351331392203107
plot_id,batch_id 0 98 miss% 0.5296302730878896
plot_id,batch_id 0 99 miss% 0.365201019338736
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.37687057 0.44255984 0.48811556 0.37718754 0.36790473 0.43677685
 0.42441538 0.53244025 0.51574604 0.44545809 0.41496276 0.38797687
 0.43903635 0.36338628 0.45666237 0.38918126 0.53436905 0.55898765
 0.46194801 0.53701053 0.4152151  0.45601158 0.43249987 0.42215736
 0.41415638 0.47409026 0.43265629 0.43905042 0.42049069 0.37843841
 0.44722397 0.53734224 0.45845212 0.43096101 0.36227153 0.47361734
 0.54919792 0.37687272 0.50855431 0.44160937 0.44614384 0.42559777
 0.34498523 0.33280574 0.42276648 0.3800599  0.45395017 0.42215661
 0.42512531 0.31802213 0.53974396 0.54023715 0.4544659  0.37701109
 0.32772179 0.56165869 0.56555394 0.50335992 0.44787656 0.54067062
 0.33278987 0.35244576 0.53013795 0.45633637 0.46748725 0.40220679
 0.5516119  0.39944366 0.49927517 0.54740389 0.34881143 0.43647822
 0.47264671 0.61106631 0.39501124 0.47722824 0.41824893 0.44000962
 0.40030086 0.40905244 0.54971927 0.47688327 0.46323223 0.46255974
 0.36928392 0.34860101 0.42503991 0.45747597 0.48941797 0.53225772
 0.30927191 0.38681631 0.41564136 0.40691622 0.57173855 0.31340472
 0.44746604 0.53513314 0.52963027 0.36520102]
for model  113 the mean error 0.44455460835136934
all id 113 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 25 out win 6 err 0.44455460835136934 time 11915.54627776146
Launcher: Job 114 completed in 12144 seconds.
Launcher: Task 178 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  41745
Epoch:0, Train loss:0.476787, valid loss:0.466052
Epoch:1, Train loss:0.017686, valid loss:0.002177
Epoch:2, Train loss:0.003916, valid loss:0.001409
Epoch:3, Train loss:0.003028, valid loss:0.001316
Epoch:4, Train loss:0.002651, valid loss:0.001188
Epoch:5, Train loss:0.002392, valid loss:0.001379
Epoch:6, Train loss:0.002228, valid loss:0.001084
Epoch:7, Train loss:0.002032, valid loss:0.001025
Epoch:8, Train loss:0.001994, valid loss:0.000880
Epoch:9, Train loss:0.001822, valid loss:0.000828
Epoch:10, Train loss:0.001751, valid loss:0.000782
Epoch:11, Train loss:0.001361, valid loss:0.000743
Epoch:12, Train loss:0.001338, valid loss:0.000720
Epoch:13, Train loss:0.001288, valid loss:0.000678
Epoch:14, Train loss:0.001259, valid loss:0.000667
Epoch:15, Train loss:0.001256, valid loss:0.000725
Epoch:16, Train loss:0.001182, valid loss:0.000692
Epoch:17, Train loss:0.001174, valid loss:0.000650
Epoch:18, Train loss:0.001147, valid loss:0.000644
Epoch:19, Train loss:0.001128, valid loss:0.000708
Epoch:20, Train loss:0.001093, valid loss:0.000620
Epoch:21, Train loss:0.000914, valid loss:0.000547
Epoch:22, Train loss:0.000902, valid loss:0.000550
Epoch:23, Train loss:0.000889, valid loss:0.000535
Epoch:24, Train loss:0.000868, valid loss:0.000505
Epoch:25, Train loss:0.000860, valid loss:0.000550
Epoch:26, Train loss:0.000863, valid loss:0.000531
Epoch:27, Train loss:0.000852, valid loss:0.000572
Epoch:28, Train loss:0.000837, valid loss:0.000561
Epoch:29, Train loss:0.000826, valid loss:0.000543
Epoch:30, Train loss:0.000819, valid loss:0.000520
Epoch:31, Train loss:0.000718, valid loss:0.000514
Epoch:32, Train loss:0.000714, valid loss:0.000508
Epoch:33, Train loss:0.000701, valid loss:0.000492
Epoch:34, Train loss:0.000699, valid loss:0.000484
Epoch:35, Train loss:0.000693, valid loss:0.000512
Epoch:36, Train loss:0.000694, valid loss:0.000488
Epoch:37, Train loss:0.000694, valid loss:0.000473
Epoch:38, Train loss:0.000683, valid loss:0.000494
Epoch:39, Train loss:0.000678, valid loss:0.000506
Epoch:40, Train loss:0.000675, valid loss:0.000532
Epoch:41, Train loss:0.000628, valid loss:0.000479
Epoch:42, Train loss:0.000620, valid loss:0.000480
Epoch:43, Train loss:0.000616, valid loss:0.000473
Epoch:44, Train loss:0.000617, valid loss:0.000482
Epoch:45, Train loss:0.000610, valid loss:0.000470
Epoch:46, Train loss:0.000607, valid loss:0.000482
Epoch:47, Train loss:0.000607, valid loss:0.000467
Epoch:48, Train loss:0.000605, valid loss:0.000469
Epoch:49, Train loss:0.000611, valid loss:0.000479
Epoch:50, Train loss:0.000601, valid loss:0.000481
Epoch:51, Train loss:0.000571, valid loss:0.000464
Epoch:52, Train loss:0.000566, valid loss:0.000467
Epoch:53, Train loss:0.000565, valid loss:0.000462
Epoch:54, Train loss:0.000564, valid loss:0.000459
Epoch:55, Train loss:0.000563, valid loss:0.000457
Epoch:56, Train loss:0.000563, valid loss:0.000458
Epoch:57, Train loss:0.000562, valid loss:0.000462
Epoch:58, Train loss:0.000562, valid loss:0.000455
Epoch:59, Train loss:0.000561, valid loss:0.000460
Epoch:60, Train loss:0.000561, valid loss:0.000458
training time 11960.005131721497
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.2955362027187719
plot_id,batch_id 0 1 miss% 0.34398745297983435
plot_id,batch_id 0 2 miss% 0.4326407817778198
plot_id,batch_id 0 3 miss% 0.2815674820176377
plot_id,batch_id 0 4 miss% 0.34777535438241
plot_id,batch_id 0 5 miss% 0.2693148665636618
plot_id,batch_id 0 6 miss% 0.3657708549923411
plot_id,batch_id 0 7 miss% 0.4801997138237327
plot_id,batch_id 0 8 miss% 0.6167328354403769
plot_id,batch_id 0 9 miss% 0.4661173289217559
plot_id,batch_id 0 10 miss% 0.2866036044960912
plot_id,batch_id 0 11 miss% 0.3030518455963516
plot_id,batch_id 0 12 miss% 0.29208513566369
plot_id,batch_id 0 13 miss% 0.33613890743009944
plot_id,batch_id 0 14 miss% 0.4608576126685623
plot_id,batch_id 0 15 miss% 0.3540625024175915
plot_id,batch_id 0 16 miss% 0.4559696549203106
plot_id,batch_id 0 17 miss% 0.4263739358460345
plot_id,batch_id 0 18 miss% 0.4111462622218918
plot_id,batch_id 0 19 miss% 0.39946204789325385
plot_id,batch_id 0 20 miss% 0.4205562809036991
plot_id,batch_id 0 21 miss% 0.4141220336330299
plot_id,batch_id 0 22 miss% 0.40183708049663697
plot_id,batch_id 0 23 miss% 0.4088907985975053
plot_id,batch_id 0 24 miss% 0.3360092114027195
plot_id,batch_id 0 25 miss% 0.28266011604490343
plot_id,batch_id 0 26 miss% 0.3660361192603208
plot_id,batch_id 0 27 miss% 0.3739210027097894
plot_id,batch_id 0 28 miss% 0.41681841848835516
plot_id,batch_id 0 29 miss% 0.4008492883832662
plot_id,batch_id 0 30 miss% 0.338239835493324
plot_id,batch_id 0 31 miss% 0.3834806581151711
plot_id,batch_id 0 32 miss% 0.42481018330849896
plot_id,batch_id 0 33 miss% 0.4575114917614248
plot_id,batch_id 0 34 miss% 0.4209791454711792
plot_id,batch_id 0 35 miss% 0.2952030440776894
plot_id,batch_id 0 36 miss% 0.48869586170250384
plot_id,batch_id 0 37 miss% 0.44216620646291777
plot_id,batch_id 0 38 miss% 0.39655898830827874
plot_id,batch_id 0 39 miss% 0.44835178396040337
plot_id,batch_id 0 40 miss% 0.3289464862718414
plot_id,batch_id 0 41 miss% 0.4039879122150389
plot_id,batch_id 0 42 miss% 0.3089758396849469
plot_id,batch_id 0 43 miss% 0.28794261450308273
plot_id,batch_id 0 44 miss% 0.3410806093738827
plot_id,batch_id 0 45 miss% 0.2592033891774302
plot_id,batch_id 0 46 miss% 0.33693994684848505
plot_id,batch_id 0 47 miss% 0.4368407782236352
plot_id,batch_id 0 48 miss% 0.3864398235045763
plot_id,batch_id 0 49 miss% 0.2855434337485931
plot_id,batch_id 0 50 miss% 0.5272868149132387
plot_id,batch_id 0 51 miss% 0.4482676862902633
plot_id,batch_id 0 52 miss% 0.45188525824075204
plot_id,batch_id 0 53 miss% 0.2948913479475609
plot_id,batch_id 0 54 miss% 0.43714326311417845
plot_id,batch_id 0 55 miss% 0.44126108798211167
plot_id,batch_id 0 56 miss% 0.5337963872567454
plot_id,batch_id 0 57 miss% 0.4383168883221074
plot_id,batch_id 0 58 miss% 0.4392512375780706
plot_id,batch_id 0 59 miss% 0.438378775667694
plot_id,batch_id 0 60 miss% 0.214590579061583
plot_id,batch_id 0 61 miss% 0.23690242959319402
plot_id,batch_id 0 62 miss% 0.33780238359214887
plot_id,batch_id 0 63 miss% 0.330224953680966
plot_id,batch_id 0 64 miss% 0.3738000742693621
plot_id,batch_id 0 65 miss% 0.2984926111108188
plot_id,batch_id 0 66 miss% 0.3895683855069047
plot_id,batch_id 0 the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  89105
Epoch:0, Train loss:0.536096, valid loss:0.493196
Epoch:1, Train loss:0.083370, valid loss:0.003908
Epoch:2, Train loss:0.010396, valid loss:0.002952
Epoch:3, Train loss:0.008216, valid loss:0.002540
Epoch:4, Train loss:0.006682, valid loss:0.002518
Epoch:5, Train loss:0.006142, valid loss:0.001972
Epoch:6, Train loss:0.005791, valid loss:0.001773
Epoch:7, Train loss:0.005471, valid loss:0.001607
Epoch:8, Train loss:0.005311, valid loss:0.001644
Epoch:9, Train loss:0.004313, valid loss:0.001593
Epoch:10, Train loss:0.003119, valid loss:0.001440
Epoch:11, Train loss:0.001737, valid loss:0.000979
Epoch:12, Train loss:0.001628, valid loss:0.000913
Epoch:13, Train loss:0.001627, valid loss:0.001249
Epoch:14, Train loss:0.001540, valid loss:0.000972
Epoch:15, Train loss:0.001496, valid loss:0.000913
Epoch:16, Train loss:0.001530, valid loss:0.000805
Epoch:17, Train loss:0.001409, valid loss:0.000866
Epoch:18, Train loss:0.001393, valid loss:0.000839
Epoch:19, Train loss:0.001394, valid loss:0.000829
Epoch:20, Train loss:0.001301, valid loss:0.000941
Epoch:21, Train loss:0.001037, valid loss:0.000783
Epoch:22, Train loss:0.001008, valid loss:0.000699
Epoch:23, Train loss:0.000956, valid loss:0.000762
Epoch:24, Train loss:0.000967, valid loss:0.000784
Epoch:25, Train loss:0.000941, valid loss:0.000676
Epoch:26, Train loss:0.000937, valid loss:0.000660
Epoch:27, Train loss:0.000957, valid loss:0.000674
Epoch:28, Train loss:0.000906, valid loss:0.000657
Epoch:29, Train loss:0.000905, valid loss:0.000773
Epoch:30, Train loss:0.000879, valid loss:0.000641
Epoch:31, Train loss:0.000730, valid loss:0.000652
Epoch:32, Train loss:0.000728, valid loss:0.000607
Epoch:33, Train loss:0.000717, valid loss:0.000654
Epoch:34, Train loss:0.000719, valid loss:0.000624
Epoch:35, Train loss:0.000708, valid loss:0.000628
Epoch:36, Train loss:0.000696, valid loss:0.000592
Epoch:37, Train loss:0.000690, valid loss:0.000640
Epoch:38, Train loss:0.000691, valid loss:0.000624
Epoch:39, Train loss:0.000668, valid loss:0.000656
Epoch:40, Train loss:0.000681, valid loss:0.000649
Epoch:41, Train loss:0.000602, valid loss:0.000600
Epoch:42, Train loss:0.000599, valid loss:0.000577
Epoch:43, Train loss:0.000598, valid loss:0.000573
Epoch:44, Train loss:0.000597, valid loss:0.000581
Epoch:45, Train loss:0.000585, valid loss:0.000614
Epoch:46, Train loss:0.000590, valid loss:0.000587
Epoch:47, Train loss:0.000582, valid loss:0.000592
Epoch:48, Train loss:0.000578, valid loss:0.000584
Epoch:49, Train loss:0.000570, valid loss:0.000589
Epoch:50, Train loss:0.000576, valid loss:0.000575
Epoch:51, Train loss:0.000541, valid loss:0.000558
Epoch:52, Train loss:0.000535, valid loss:0.000562
Epoch:53, Train loss:0.000532, valid loss:0.000559
Epoch:54, Train loss:0.000531, valid loss:0.000561
Epoch:55, Train loss:0.000529, valid loss:0.000563
Epoch:56, Train loss:0.000528, valid loss:0.000562
Epoch:57, Train loss:0.000528, valid loss:0.000562
Epoch:58, Train loss:0.000527, valid loss:0.000564
Epoch:59, Train loss:0.000527, valid loss:0.000564
Epoch:60, Train loss:0.000526, valid loss:0.000566
training time 11966.500348806381
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.3211048907178863
plot_id,batch_id 0 1 miss% 0.38947284559601275
plot_id,batch_id 0 2 miss% 0.47831142419128675
plot_id,batch_id 0 3 miss% 0.42410665444859863
plot_id,batch_id 0 4 miss% 0.32326420499808595
plot_id,batch_id 0 5 miss% 0.3807209899286109
plot_id,batch_id 0 6 miss% 0.33913661815387663
plot_id,batch_id 0 7 miss% 0.47646613620813694
plot_id,batch_id 0 8 miss% 0.6272202206925936
plot_id,batch_id 0 9 miss% 0.5145114588818948
plot_id,batch_id 0 10 miss% 0.29651493296925097
plot_id,batch_id 0 11 miss% 0.41952134081354114
plot_id,batch_id 0 12 miss% 0.36157638349015914
plot_id,batch_id 0 13 miss% 0.3635092204159643
plot_id,batch_id 0 14 miss% 0.48444033967753924
plot_id,batch_id 0 15 miss% 0.34652413028319784
plot_id,batch_id 0 16 miss% 0.48544496513295265
plot_id,batch_id 0 17 miss% 0.4184196892549968
plot_id,batch_id 0 18 miss% 0.4726797159357234
plot_id,batch_id 0 19 miss% 0.44600463306284366
plot_id,batch_id 0 20 miss% 0.5951819484669085
plot_id,batch_id 0 21 miss% 0.4300356083059714
plot_id,batch_id 0 22 miss% 0.4753500726798153
plot_id,batch_id 0 23 miss% 0.4178583814806993
plot_id,batch_id 0 24 miss% 0.34863206986511036
plot_id,batch_id 0 25 miss% 0.35022735331603133
plot_id,batch_id 0 26 miss% 0.4065487789540108
plot_id,batch_id 0 27 miss% 0.3685107791590041
plot_id,batch_id 0 28 miss% 0.4171903374941053
plot_id,batch_id 0 29 miss% 0.3758855561911514
plot_id,batch_id 0 30 miss% 0.34983818400513367
plot_id,batch_id 0 31 miss% 0.4239311089776506
plot_id,batch_id 0 32 miss% 0.46054009091134873
plot_id,batch_id 0 33 miss% 0.45732117929763494
plot_id,batch_id 0 34 miss% 0.35354270902515755
plot_id,batch_id 0 35 miss% 0.36200821208778905
plot_id,batch_id 0 36 miss% 0.5024076858882319
plot_id,batch_id 0 37 miss% 0.4850795183368956
plot_id,batch_id 0 38 miss% 0.40895401952397853
plot_id,batch_id 0 39 miss% 0.4567901328579146
plot_id,batch_id 0 40 miss% 0.2956202772355277
plot_id,batch_id 0 41 miss% 0.4171383901379438
plot_id,batch_id 0 42 miss% 0.30358225928071947
plot_id,batch_id 0 43 miss% 0.3159599256829567
plot_id,batch_id 0 44 miss% 0.307520191262196
plot_id,batch_id 0 45 miss% 0.3057587724567024
plot_id,batch_id 0 46 miss% 0.4132851550538467
plot_id,batch_id 0 47 miss% 0.3813911659128444
plot_id,batch_id 0 48 miss% 0.3867071484110998
plot_id,batch_id 0 49 miss% 0.3369794424558465
plot_id,batch_id 0 50 miss% 0.4599839825113564
plot_id,batch_id 0 51 miss% 0.4382356261200329
plot_id,batch_id 0 52 miss% 0.46602612177382524
plot_id,batch_id 0 53 miss% 0.36726473186026043
plot_id,batch_id 0 54 miss% 0.3305680276267911
plot_id,batch_id 0 55 miss% 0.4361000418101433
plot_id,batch_id 0 56 miss% 0.5220338988671372
plot_id,batch_id 0 57 miss% 0.44968977288855927
plot_id,batch_id 0 58 miss% 0.4202080054065941
plot_id,batch_id 0 59 miss% 0.4222915917839476
plot_id,batch_id 0 60 miss% 0.33221270926145324
plot_id,batch_id 0 61 miss% 0.29633891274592544
plot_id,batch_id 0 62 miss% 0.40841085853108516
plot_id,batch_id 0 63 miss% 0.3618939160318297
plot_id,batch_id 0 64 miss% 0.3553957225672558
plot_id,batch_id 0 65 miss% 0.3999891112964209
plot_id,batch_id 0 66 miss% 0.4183435346464808
plot_id,batch_id 0 67 miss% 0.3304002825507608
plot_id,batch_id 0 68 miss% 0.4354140716866944
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  120401
Epoch:0, Train loss:0.734365, valid loss:0.715748
Epoch:1, Train loss:0.236235, valid loss:0.005763
Epoch:2, Train loss:0.014456, valid loss:0.004811
Epoch:3, Train loss:0.012474, valid loss:0.004445
Epoch:4, Train loss:0.011423, valid loss:0.003934
Epoch:5, Train loss:0.010872, valid loss:0.004159
Epoch:6, Train loss:0.010472, valid loss:0.003496
Epoch:7, Train loss:0.010024, valid loss:0.003426
Epoch:8, Train loss:0.008812, valid loss:0.003537
Epoch:9, Train loss:0.007169, valid loss:0.002914
Epoch:10, Train loss:0.005883, valid loss:0.002289
Epoch:11, Train loss:0.005021, valid loss:0.001851
Epoch:12, Train loss:0.004835, valid loss:0.001890
Epoch:13, Train loss:0.004735, valid loss:0.001911
Epoch:14, Train loss:0.004589, valid loss:0.001812
Epoch:15, Train loss:0.004546, valid loss:0.002198
Epoch:16, Train loss:0.004389, valid loss:0.001742
Epoch:17, Train loss:0.004382, valid loss:0.001814
Epoch:18, Train loss:0.004157, valid loss:0.001793
Epoch:19, Train loss:0.004063, valid loss:0.001602
Epoch:20, Train loss:0.004156, valid loss:0.001841
Epoch:21, Train loss:0.003617, valid loss:0.001467
Epoch:22, Train loss:0.003404, valid loss:0.001463
Epoch:23, Train loss:0.003412, valid loss:0.001376
Epoch:24, Train loss:0.003310, valid loss:0.001406
Epoch:25, Train loss:0.003308, valid loss:0.001451
Epoch:26, Train loss:0.003306, valid loss:0.001299
Epoch:27, Train loss:0.003207, valid loss:0.001265
Epoch:28, Train loss:0.003165, valid loss:0.001268
Epoch:29, Train loss:0.003142, valid loss:0.001192
Epoch:30, Train loss:0.003134, valid loss:0.001370
Epoch:31, Train loss:0.002850, valid loss:0.001193
Epoch:32, Train loss:0.002773, valid loss:0.001167
Epoch:33, Train loss:0.002759, valid loss:0.001165
Epoch:34, Train loss:0.002747, valid loss:0.001224
Epoch:35, Train loss:0.002748, valid loss:0.001165
Epoch:36, Train loss:0.002713, valid loss:0.001216
Epoch:37, Train loss:0.002725, valid loss:0.001188
Epoch:38, Train loss:0.002703, valid loss:0.001176
Epoch:39, Train loss:0.002670, valid loss:0.001130
Epoch:40, Train loss:0.002651, valid loss:0.001133
Epoch:41, Train loss:0.002512, valid loss:0.001167
Epoch:42, Train loss:0.002492, valid loss:0.001106
Epoch:43, Train loss:0.002483, valid loss:0.001108
Epoch:44, Train loss:0.002469, valid loss:0.001096
Epoch:45, Train loss:0.002457, valid loss:0.001111
Epoch:46, Train loss:0.002453, valid loss:0.001115
Epoch:47, Train loss:0.002453, valid loss:0.001070
Epoch:48, Train loss:0.002424, valid loss:0.001078
Epoch:49, Train loss:0.002426, valid loss:0.001172
Epoch:50, Train loss:0.002439, valid loss:0.001057
Epoch:51, Train loss:0.002346, valid loss:0.001045
Epoch:52, Train loss:0.002340, valid loss:0.001041
Epoch:53, Train loss:0.002336, valid loss:0.001040
Epoch:54, Train loss:0.002334, valid loss:0.001046
Epoch:55, Train loss:0.002333, valid loss:0.001039
Epoch:56, Train loss:0.002332, valid loss:0.001041
Epoch:57, Train loss:0.002331, valid loss:0.001043
Epoch:58, Train loss:0.002330, valid loss:0.001043
Epoch:59, Train loss:0.002329, valid loss:0.001038
Epoch:60, Train loss:0.002328, valid loss:0.001042
training time 11976.54171204567
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.4195174575723519
plot_id,batch_id 0 1 miss% 0.42684530456519665
plot_id,batch_id 0 2 miss% 0.5221430829739501
plot_id,batch_id 0 3 miss% 0.5139958958489264
plot_id,batch_id 0 4 miss% 0.41727312981596004
plot_id,batch_id 0 5 miss% 0.5198928470389184
plot_id,batch_id 0 6 miss% 0.46871078554434004
plot_id,batch_id 0 7 miss% 0.5449349691295183
plot_id,batch_id 0 8 miss% 0.5098166247288716
plot_id,batch_id 0 9 miss% 0.5168545704274365
plot_id,batch_id 0 10 miss% 0.35347489970277157
plot_id,batch_id 0 11 miss% 0.5080746143166591
plot_id,batch_id 0 12 miss% 0.5141981837499694
plot_id,batch_id 0 13 miss% 0.4640733450630129
plot_id,batch_id 0 14 miss% 0.5108323027407586
plot_id,batch_id 0 15 miss% 0.5189689244415848
plot_id,batch_id 0 16 miss% 0.5700012126367229
plot_id,batch_id 0 17 miss% 0.5040026055008291
plot_id,batch_id 0 18 miss% 0.5008620097733231
plot_id,batch_id 0 19 miss% 0.4667813021934913
plot_id,batch_id 0 20 miss% 0.439698792566223
plot_id,batch_id 0 21 miss% 0.5023229278397671
plot_id,batch_id 0 22 miss% 0.5190927953674523
plot_id,batch_id 0 23 miss% 0.45699896497776166
plot_id,batch_id 0 24 miss% 0.4189592476196379
plot_id,batch_id 0 25 miss% 0.4466912021406904
plot_id,batch_id 0 26 miss% 0.531432516934691
plot_id,batch_id 0 27 miss% 0.4920877639032322
plot_id,batch_id 0 28 miss% 0.5733362815247973
plot_id,batch_id 0 29 miss% 0.5124335277704916
plot_id,batch_id 0 30 miss% 0.4412223667960901
plot_id,batch_id 0 31 miss% 0.5216160271508925
plot_id,batch_id 0 32 miss% 0.4773592920774722
plot_id,batch_id 0 33 miss% 0.4752883465161003
plot_id,batch_id 0 34 miss% 0.5546145553654529
plot_id,batch_id 0 35 miss% 0.4475854147391325
plot_id,batch_id 0 36 miss% 0.5573717212161462
plot_id,batch_id 0 37 miss% 0.48579128582908343
plot_id,batch_id 0 38 miss% 0.5286058686044638
plot_id,batch_id 0 39 miss% 0.43022049850442784
plot_id,batch_id 0 40 miss% 0.4626434378304488
plot_id,batch_id 0 41 miss% 0.5580176450566208
plot_id,batch_id 0 42 miss% 0.3024567601460977
plot_id,batch_id 0 43 miss% 0.4820140655936574
plot_id,batch_id 0 44 miss% 0.5755366042898604
plot_id,batch_id 0 45 miss% 0.4638840605597506
plot_id,batch_id 0 46 miss% 0.6859076232359064
plot_id,batch_id 0 47 miss% 0.6050018222392379
plot_id,batch_id 0 48 miss% 0.497286913362282
plot_id,batch_id 0 49 miss% 0.7378392049214146
plot_id,batch_id 0 50 miss% 0.6345103965266448
plot_id,batch_id 0 51 miss% 0.534029089292589
plot_id,batch_id 0 52 miss% 0.6327230082159339
plot_id,batch_id 0 53 miss% 0.5189951449194126
plot_id,batch_id 0 54 miss% 0.4606924282099887
plot_id,batch_id 0 55 miss% 0.4733764760341538
plot_id,batch_id 0 56 miss% 0.6565427881977464
plot_id,batch_id 0 57 miss% 0.6535933343980701
plot_id,batch_id 0 58 miss% 0.6821390262125946
plot_id,batch_id 0 59 miss% 0.6142170991528912
plot_id,batch_id 0 60 miss% 0.3897783431004194
plot_id,batch_id 0 61 miss% 0.407227024115538
plot_id,batch_id 0 62 miss% 0.45659043833466323
plot_id,batch_id 0 63 miss% 0.5039667453420217
plot_id,batch_id 0 64 miss% 0.5356054156174247
plot_id,batch_id 0 65 miss% 0.40218966901567466
plot_id,batch_id 0 66 miss% 0.5063529595148063
plot_id,batch_id 0 67 miss% 0.41644475562646077
plot_id,batch_id 0 68 miss% 0.5379137292527649
plot_id,batch_id 0 69 miss% 0.5224808449084534
plot_id,batch_id 0 70 miss% plot_id,batch_id 0 67 miss% 0.029363016558108548
plot_id,batch_id 0 68 miss% 0.057392760874918035
plot_id,batch_id 0 69 miss% 0.13502676487826348
plot_id,batch_id 0 70 miss% 0.05178641435920553
plot_id,batch_id 0 71 miss% 0.03542617104309838
plot_id,batch_id 0 72 miss% 0.12959576606139708
plot_id,batch_id 0 73 miss% 0.04630573916836508
plot_id,batch_id 0 74 miss% 0.13563645113588377
plot_id,batch_id 0 75 miss% 0.0429472091083451
plot_id,batch_id 0 76 miss% 0.06681369724281949
plot_id,batch_id 0 77 miss% 0.049206572258902605
plot_id,batch_id 0 78 miss% 0.04221407251999714
plot_id,batch_id 0 79 miss% 0.11370922243487772
plot_id,batch_id 0 80 miss% 0.02919850704358269
plot_id,batch_id 0 81 miss% 0.08763721191770382
plot_id,batch_id 0 82 miss% 0.07648595260203703
plot_id,batch_id 0 83 miss% 0.06920985040996373
plot_id,batch_id 0 84 miss% 0.11607852500589357
plot_id,batch_id 0 85 miss% 0.03419461447340015
plot_id,batch_id 0 86 miss% 0.09051824294016572
plot_id,batch_id 0 87 miss% 0.06756326510444387
plot_id,batch_id 0 88 miss% 0.10081260993617686
plot_id,batch_id 0 89 miss% 0.05385810742745885
plot_id,batch_id 0 90 miss% 0.025292700629433815
plot_id,batch_id 0 91 miss% 0.08424023272399891
plot_id,batch_id 0 92 miss% 0.032262947411668734
plot_id,batch_id 0 93 miss% 0.04198486341795603
plot_id,batch_id 0 94 miss% 0.11447783784971505
plot_id,batch_id 0 95 miss% 0.05476264718540546
plot_id,batch_id 0 96 miss% 0.0495326484278271
plot_id,batch_id 0 97 miss% 0.044457386823937024
plot_id,batch_id 0 98 miss% 0.031059336108176835
plot_id,batch_id 0 99 miss% 0.0351978973285435
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04098654 0.04513484 0.13228087 0.06253787 0.04481787 0.06107504
 0.04724086 0.10338893 0.08763519 0.07230839 0.10945334 0.10068751
 0.0874186  0.07284073 0.09434923 0.06769255 0.14558105 0.02615214
 0.04760206 0.12510437 0.10800813 0.03735621 0.03915855 0.02904605
 0.05866304 0.07038698 0.06452748 0.04265792 0.02729048 0.02570015
 0.04588007 0.10530363 0.09826407 0.04385171 0.06664005 0.0477655
 0.11887075 0.11566218 0.04679898 0.05522336 0.19121209 0.0928556
 0.02451801 0.05943021 0.02801196 0.05766089 0.02168892 0.04138378
 0.03430659 0.01847482 0.15304997 0.03634894 0.04682766 0.02666702
 0.03191794 0.10240625 0.07451037 0.05232803 0.0408423  0.02715976
 0.04156837 0.01907219 0.04610941 0.06431822 0.0652661  0.0473001
 0.05446941 0.02936302 0.05739276 0.13502676 0.05178641 0.03542617
 0.12959577 0.04630574 0.13563645 0.04294721 0.0668137  0.04920657
 0.04221407 0.11370922 0.02919851 0.08763721 0.07648595 0.06920985
 0.11607853 0.03419461 0.09051824 0.06756327 0.10081261 0.05385811
 0.0252927  0.08424023 0.03226295 0.04198486 0.11447784 0.05476265
 0.04953265 0.04445739 0.03105934 0.0351979 ]
for model  84 the mean error 0.06495297412270605
all id 84 hidden_dim 24 learning_rate 0.0025 num_layers 3 frames 25 out win 4 err 0.06495297412270605 time 11902.699295282364
Launcher: Job 85 completed in 12169 seconds.
Launcher: Task 74 done. Exiting.
0.3870492200971822
plot_id,batch_id 0 71 miss% 0.498648324922384
plot_id,batch_id 0 72 miss% 0.5268612512132849
plot_id,batch_id 0 73 miss% 0.5016438961195631
plot_id,batch_id 0 74 miss% 0.5831339260860716
plot_id,batch_id 0 75 miss% 0.41614308242138703
plot_id,batch_id 0 76 miss% 0.5475176590488615
plot_id,batch_id 0 77 miss% 0.4405476093880643
plot_id,batch_id 0 78 miss% 0.4323083957805637
plot_id,batch_id 0 79 miss% 0.46539157174764395
plot_id,batch_id 0 80 miss% 0.43028708396202825
plot_id,batch_id 0 81 miss% 0.5747553428055834
plot_id,batch_id 0 82 miss% 0.4301000788688393
plot_id,batch_id 0 83 miss% 0.5177647524160977
plot_id,batch_id 0 84 miss% 0.4383217759486001
plot_id,batch_id 0 85 miss% 0.43171620156415663
plot_id,batch_id 0 86 miss% 0.5322877533019748
plot_id,batch_id 0 87 miss% 0.5251906859613357
plot_id,batch_id 0 88 miss% 0.5436029125132028
plot_id,batch_id 0 89 miss% 0.5177380435917421
plot_id,batch_id 0 90 miss% 0.3985691001477262
plot_id,batch_id 0 91 miss% 0.5003618054393341
plot_id,batch_id 0 92 miss% 0.40928279157542585
plot_id,batch_id 0 93 miss% 0.4216697426572809
plot_id,batch_id 0 94 miss% 0.5613899407277844
plot_id,batch_id 0 95 miss% 0.4017132603653532
plot_id,batch_id 0 96 miss% 0.5240124209277158
plot_id,batch_id 0 97 miss% 0.5831869517967113
plot_id,batch_id 0 98 miss% 0.5776901854461921
plot_id,batch_id 0 99 miss% 0.46540997076096846
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.41951746 0.4268453  0.52214308 0.5139959  0.41727313 0.51989285
 0.46871079 0.54493497 0.50981662 0.51685457 0.3534749  0.50807461
 0.51419818 0.46407335 0.5108323  0.51896892 0.57000121 0.50400261
 0.50086201 0.4667813  0.43969879 0.50232293 0.5190928  0.45699896
 0.41895925 0.4466912  0.53143252 0.49208776 0.57333628 0.51243353
 0.44122237 0.52161603 0.47735929 0.47528835 0.55461456 0.44758541
 0.55737172 0.48579129 0.52860587 0.4302205  0.46264344 0.55801765
 0.30245676 0.48201407 0.5755366  0.46388406 0.68590762 0.60500182
 0.49728691 0.7378392  0.6345104  0.53402909 0.63272301 0.51899514
 0.46069243 0.47337648 0.65654279 0.65359333 0.68213903 0.6142171
 0.38977834 0.40722702 0.45659044 0.50396675 0.53560542 0.40218967
 0.50635296 0.41644476 0.53791373 0.52248084 0.38704922 0.49864832
 0.52686125 0.5016439  0.58313393 0.41614308 0.54751766 0.44054761
 0.4323084  0.46539157 0.43028708 0.57475534 0.43010008 0.51776475
 0.43832178 0.4317162  0.53228775 0.52519069 0.54360291 0.51773804
 0.3985691  0.50036181 0.40928279 0.42166974 0.56138994 0.40171326
 0.52401242 0.58318695 0.57769019 0.46540997]
for model  68 the mean error 0.500782360560352
all id 68 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 21 out win 6 err 0.500782360560352 time 11976.54171204567
Launcher: Job 69 completed in 12200 seconds.
Launcher: Task 142 done. Exiting.
67 miss% 0.27367666461260703
plot_id,batch_id 0 68 miss% 0.401493505008773
plot_id,batch_id 0 69 miss% 0.4142625729573794
plot_id,batch_id 0 70 miss% 0.24813683519521398
plot_id,batch_id 0 71 miss% 0.37347631059406294
plot_id,batch_id 0 72 miss% 0.4007825631096532
plot_id,batch_id 0 73 miss% 0.31616249653392947
plot_id,batch_id 0 74 miss% 0.29799537360744566
plot_id,batch_id 0 75 miss% 0.22756952119365606
plot_id,batch_id 0 76 miss% 0.2872094780481889
plot_id,batch_id 0 77 miss% 0.28146141064749414
plot_id,batch_id 0 78 miss% 0.3205914787408476
plot_id,batch_id 0 79 miss% 0.3120086236881929
plot_id,batch_id 0 80 miss% 0.2969208350334951
plot_id,batch_id 0 81 miss% 0.4868989274435523
plot_id,batch_id 0 82 miss% 0.28325543559864697
plot_id,batch_id 0 83 miss% 0.350973983735653
plot_id,batch_id 0 84 miss% 0.3568319702947313
plot_id,batch_id 0 85 miss% 0.22607574515913
plot_id,batch_id 0 86 miss% 0.30982391922326036
plot_id,batch_id 0 87 miss% 0.32048621684232637
plot_id,batch_id 0 88 miss% 0.36820065340622576
plot_id,batch_id 0 89 miss% 0.36802245470025596
plot_id,batch_id 0 90 miss% 0.21712375479085372
plot_id,batch_id 0 91 miss% 0.2766876935706663
plot_id,batch_id 0 92 miss% 0.2785486701588735
plot_id,batch_id 0 93 miss% 0.2638435963038153
plot_id,batch_id 0 94 miss% 0.3626878636307229
plot_id,batch_id 0 95 miss% 0.26860006935914643
plot_id,batch_id 0 96 miss% 0.28845788631738495
plot_id,batch_id 0 97 miss% 0.3924229866089829
plot_id,batch_id 0 98 miss% 0.3056503912683008
plot_id,batch_id 0 99 miss% 0.40264981558351504
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.2955362  0.34398745 0.43264078 0.28156748 0.34777535 0.26931487
 0.36577085 0.48019971 0.61673284 0.46611733 0.2866036  0.30305185
 0.29208514 0.33613891 0.46085761 0.3540625  0.45596965 0.42637394
 0.41114626 0.39946205 0.42055628 0.41412203 0.40183708 0.4088908
 0.33600921 0.28266012 0.36603612 0.373921   0.41681842 0.40084929
 0.33823984 0.38348066 0.42481018 0.45751149 0.42097915 0.29520304
 0.48869586 0.44216621 0.39655899 0.44835178 0.32894649 0.40398791
 0.30897584 0.28794261 0.34108061 0.25920339 0.33693995 0.43684078
 0.38643982 0.28554343 0.52728681 0.44826769 0.45188526 0.29489135
 0.43714326 0.44126109 0.53379639 0.43831689 0.43925124 0.43837878
 0.21459058 0.23690243 0.33780238 0.33022495 0.37380007 0.29849261
 0.38956839 0.27367666 0.40149351 0.41426257 0.24813684 0.37347631
 0.40078256 0.3161625  0.29799537 0.22756952 0.28720948 0.28146141
 0.32059148 0.31200862 0.29692084 0.48689893 0.28325544 0.35097398
 0.35683197 0.22607575 0.30982392 0.32048622 0.36820065 0.36802245
 0.21712375 0.27668769 0.27854867 0.2638436  0.36268786 0.26860007
 0.28845789 0.39242299 0.30565039 0.40264982]
for model  190 the mean error 0.36129842632000025
all id 190 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 31 out win 5 err 0.36129842632000025 time 11960.005131721497
Launcher: Job 191 completed in 12199 seconds.
Launcher: Task 182 done. Exiting.
plot_id,batch_id 0 69 miss% 0.42232309430485443
plot_id,batch_id 0 70 miss% 0.27880391091437307
plot_id,batch_id 0 71 miss% 0.40331333967840527
plot_id,batch_id 0 72 miss% 0.40977634343343783
plot_id,batch_id 0 73 miss% 0.34076375150823335
plot_id,batch_id 0 74 miss% 0.4097345126480272
plot_id,batch_id 0 75 miss% 0.29472585629593345
plot_id,batch_id 0 76 miss% 0.33644723468875193
plot_id,batch_id 0 77 miss% 0.32002697014662956
plot_id,batch_id 0 78 miss% 0.3103383774442667
plot_id,batch_id 0 79 miss% 0.3643105297241373
plot_id,batch_id 0 80 miss% 0.3204944097072076
plot_id,batch_id 0 81 miss% 0.4725005210066217
plot_id,batch_id 0 82 miss% 0.435573087905208
plot_id,batch_id 0 83 miss% 0.4098571648607231
plot_id,batch_id 0 84 miss% 0.4033384363666265
plot_id,batch_id 0 85 miss% 0.30332119258587736
plot_id,batch_id 0 86 miss% 0.38295795092835005
plot_id,batch_id 0 87 miss% 0.4005862246083639
plot_id,batch_id 0 88 miss% 0.4492515293337982
plot_id,batch_id 0 89 miss% 0.3835537467248761
plot_id,batch_id 0 90 miss% 0.26124537344536536
plot_id,batch_id 0 91 miss% 0.3040727413947222
plot_id,batch_id 0 92 miss% 0.2992854235523916
plot_id,batch_id 0 93 miss% 0.27628498158260967
plot_id,batch_id 0 94 miss% 0.46362003041541205
plot_id,batch_id 0 95 miss% 0.24608797781166664
plot_id,batch_id 0 96 miss% 0.3070168828490076
plot_id,batch_id 0 97 miss% 0.4451963182335748
plot_id,batch_id 0 98 miss% 0.48175204002649874
plot_id,batch_id 0 99 miss% 0.3708871310332918
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.32110489 0.38947285 0.47831142 0.42410665 0.3232642  0.38072099
 0.33913662 0.47646614 0.62722022 0.51451146 0.29651493 0.41952134
 0.36157638 0.36350922 0.48444034 0.34652413 0.48544497 0.41841969
 0.47267972 0.44600463 0.59518195 0.43003561 0.47535007 0.41785838
 0.34863207 0.35022735 0.40654878 0.36851078 0.41719034 0.37588556
 0.34983818 0.42393111 0.46054009 0.45732118 0.35354271 0.36200821
 0.50240769 0.48507952 0.40895402 0.45679013 0.29562028 0.41713839
 0.30358226 0.31595993 0.30752019 0.30575877 0.41328516 0.38139117
 0.38670715 0.33697944 0.45998398 0.43823563 0.46602612 0.36726473
 0.33056803 0.43610004 0.5220339  0.44968977 0.42020801 0.42229159
 0.33221271 0.29633891 0.40841086 0.36189392 0.35539572 0.39998911
 0.41834353 0.33040028 0.43541407 0.42232309 0.27880391 0.40331334
 0.40977634 0.34076375 0.40973451 0.29472586 0.33644723 0.32002697
 0.31033838 0.36431053 0.32049441 0.47250052 0.43557309 0.40985716
 0.40333844 0.30332119 0.38295795 0.40058622 0.44925153 0.38355375
 0.26124537 0.30407274 0.29928542 0.27628498 0.46362003 0.24608798
 0.30701688 0.44519632 0.48175204 0.37088713]
for model  112 the mean error 0.3923697525469318
all id 112 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 25 out win 5 err 0.3923697525469318 time 11966.500348806381
Launcher: Job 113 completed in 12204 seconds.
Launcher: Task 235 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  154129
Epoch:0, Train loss:0.725999, valid loss:0.745131
Epoch:1, Train loss:0.143257, valid loss:0.004323
Epoch:2, Train loss:0.009860, valid loss:0.003676
Epoch:3, Train loss:0.008328, valid loss:0.003108
Epoch:4, Train loss:0.007233, valid loss:0.002717
Epoch:5, Train loss:0.006064, valid loss:0.002247
Epoch:6, Train loss:0.005731, valid loss:0.002425
Epoch:7, Train loss:0.005452, valid loss:0.002112
Epoch:8, Train loss:0.005180, valid loss:0.002320
Epoch:9, Train loss:0.004863, valid loss:0.001801
Epoch:10, Train loss:0.004675, valid loss:0.002082
Epoch:11, Train loss:0.003716, valid loss:0.001468
Epoch:12, Train loss:0.003563, valid loss:0.001350
Epoch:13, Train loss:0.003529, valid loss:0.001573
Epoch:14, Train loss:0.002488, valid loss:0.001242
Epoch:15, Train loss:0.002122, valid loss:0.001184
Epoch:16, Train loss:0.001988, valid loss:0.001147
Epoch:17, Train loss:0.001852, valid loss:0.001182
Epoch:18, Train loss:0.001977, valid loss:0.001194
Epoch:19, Train loss:0.001834, valid loss:0.001068
Epoch:20, Train loss:0.001685, valid loss:0.001060
Epoch:21, Train loss:0.001293, valid loss:0.000924
Epoch:22, Train loss:0.001229, valid loss:0.000887
Epoch:23, Train loss:0.001180, valid loss:0.000800
Epoch:24, Train loss:0.001208, valid loss:0.000842
Epoch:25, Train loss:0.001123, valid loss:0.000806
Epoch:26, Train loss:0.001144, valid loss:0.000767
Epoch:27, Train loss:0.001107, valid loss:0.000781
Epoch:28, Train loss:0.001101, valid loss:0.000800
Epoch:29, Train loss:0.001059, valid loss:0.000762
Epoch:30, Train loss:0.001064, valid loss:0.000815
Epoch:31, Train loss:0.000819, valid loss:0.000746
Epoch:32, Train loss:0.000774, valid loss:0.000775
Epoch:33, Train loss:0.000780, valid loss:0.000712
Epoch:34, Train loss:0.000750, valid loss:0.000731
Epoch:35, Train loss:0.000750, valid loss:0.000707
Epoch:36, Train loss:0.000750, valid loss:0.000690
Epoch:37, Train loss:0.000739, valid loss:0.000702
Epoch:38, Train loss:0.000725, valid loss:0.000794
Epoch:39, Train loss:0.000716, valid loss:0.000721
Epoch:40, Train loss:0.000705, valid loss:0.000755
Epoch:41, Train loss:0.000610, valid loss:0.000641
Epoch:42, Train loss:0.000593, valid loss:0.000673
Epoch:43, Train loss:0.000583, valid loss:0.000658
Epoch:44, Train loss:0.000585, valid loss:0.000635
Epoch:45, Train loss:0.000580, valid loss:0.000621
Epoch:46, Train loss:0.000581, valid loss:0.000638
Epoch:47, Train loss:0.000565, valid loss:0.000642
Epoch:48, Train loss:0.000567, valid loss:0.000637
Epoch:49, Train loss:0.000563, valid loss:0.000678
Epoch:50, Train loss:0.000560, valid loss:0.000644
Epoch:51, Train loss:0.000513, valid loss:0.000637
Epoch:52, Train loss:0.000509, valid loss:0.000635
Epoch:53, Train loss:0.000507, valid loss:0.000634
Epoch:54, Train loss:0.000506, valid loss:0.000636
Epoch:55, Train loss:0.000506, valid loss:0.000633
Epoch:56, Train loss:0.000505, valid loss:0.000633
Epoch:57, Train loss:0.000504, valid loss:0.000633
Epoch:58, Train loss:0.000504, valid loss:0.000633
Epoch:59, Train loss:0.000503, valid loss:0.000635
Epoch:60, Train loss:0.000502, valid loss:0.000633
training time 12038.119781970978
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08219256824726794
plot_id,batch_id 0 1 miss% 0.05027831285233554
plot_id,batch_id 0 2 miss% 0.08371613473012816
plot_id,batch_id 0 3 miss% 0.05614718359788734
plot_id,batch_id 0 4 miss% 0.021587695400195204
plot_id,batch_id 0 5 miss% 0.08296885146352691
plot_id,batch_id 0 6 miss% 0.02907960978444755
plot_id,batch_id 0 7 miss% 0.05424551482082407
plot_id,batch_id 0 8 miss% 0.053962147538080533
plot_id,batch_id 0 9 miss% 0.04888913976998025
plot_id,batch_id 0 10 miss% 0.04024734593120983
plot_id,batch_id 0 11 miss% 0.07053162338664626
plot_id,batch_id 0 12 miss% 0.04492067365974837
plot_id,batch_id 0 13 miss% 0.054439505151498764
plot_id,batch_id 0 14 miss% 0.0698635270706667
plot_id,batch_id 0 15 miss% 0.04895473495646844
plot_id,batch_id 0 16 miss% 0.06843680813427129
plot_id,batch_id 0 17 miss% 0.05904479414360569
plot_id,batch_id 0 18 miss% 0.05723719706516154
plot_id,batch_id 0 19 miss% 0.042494060565896274
plot_id,batch_id 0 20 miss% 0.0769429913151388
plot_id,batch_id 0 21 miss% 0.08102973014498799
plot_id,batch_id 0 22 miss% 0.026893666673312888
plot_id,batch_id 0 23 miss% 0.03199076203117077
plot_id,batch_id 0 24 miss% 0.03107232665057754
plot_id,batch_id 0 25 miss% 0.058942573956960086
plot_id,batch_id 0 26 miss% 0.06851620215169392
plot_id,batch_id 0 27 miss% 0.04282554102831371
plot_id,batch_id 0 28 miss% 0.029206948706477855
plot_id,batch_id 0 29 miss% 0.02695582377808086
plot_id,batch_id 0 30 miss% 0.03350660367584049
plot_id,batch_id 0 31 miss% 0.06906055118762754
plot_id,batch_id 0 32 miss% 0.08013315891446288
plot_id,batch_id 0 33 miss% 0.04687659766904749
plot_id,batch_id 0 34 miss% 0.030613711605282704
plot_id,batch_id 0 35 miss% 0.07402437546696913
plot_id,batch_id 0 36 miss% 0.06271673053349507
plot_id,batch_id 0 37 miss% 0.12872959466586856
plot_id,batch_id 0 38 miss% 0.04704920729157867
plot_id,batch_id 0 39 miss% 0.03272820779649741
plot_id,batch_id 0 40 miss% 0.05553496380544411
plot_id,batch_id 0 41 miss% 0.05229930641974148
plot_id,batch_id 0 42 miss% 0.05686125296589567
plot_id,batch_id 0 43 miss% 0.04141790379914637
plot_id,batch_id 0 44 miss% 0.021265353586735796
plot_id,batch_id 0 45 miss% 0.0708390974754082
plot_id,batch_id 0 46 miss% 0.03127173242511612
plot_id,batch_id 0 47 miss% 0.012865102712428673
plot_id,batch_id 0 48 miss% 0.01993332599741329
plot_id,batch_id 0 49 miss% 0.03650202695904289
plot_id,batch_id 0 50 miss% 0.13360506378670506
plot_id,batch_id 0 51 miss% 0.03526265065016421
plot_id,batch_id 0 52 miss% 0.03185199911973005
plot_id,batch_id 0 53 miss% 0.029821318344647265
plot_id,batch_id 0 54 miss% 0.02872576546592484
plot_id,batch_id 0 55 miss% 0.05950190826573814
plot_id,batch_id 0 56 miss% 0.10480404893782762
plot_id,batch_id 0 57 miss% 0.031792584217818644
plot_id,batch_id 0 58 miss% 0.03760857767670219
plot_id,batch_id 0 59 miss% 0.024576286349013005
plot_id,batch_id 0 60 miss% 0.03176616070655355
plot_id,batch_id 0 61 miss% 0.018761076371959166
plot_id,batch_id 0 62 miss% 0.07115117560842396
plot_id,batch_id 0 63 miss% 0.060145599077221
plot_id,batch_id 0 64 miss% 0.05866397367070236
plot_id,batch_id 0 65 miss% 0.0625080158074409
plot_id,batch_id 0 66 miss% 0.1165170493371973
plot_id,batch_id 0 67 miss% 0.02437681685371169
plot_id,batch_id 0 68 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  120401
Epoch:0, Train loss:0.757522, valid loss:0.743895
Epoch:1, Train loss:0.185435, valid loss:0.005789
Epoch:2, Train loss:0.015437, valid loss:0.005518
Epoch:3, Train loss:0.013167, valid loss:0.004678
Epoch:4, Train loss:0.011119, valid loss:0.004381
Epoch:5, Train loss:0.007669, valid loss:0.002359
Epoch:6, Train loss:0.004759, valid loss:0.002099
Epoch:7, Train loss:0.004476, valid loss:0.002151
Epoch:8, Train loss:0.004209, valid loss:0.002062
Epoch:9, Train loss:0.003818, valid loss:0.002384
Epoch:10, Train loss:0.003760, valid loss:0.001763
Epoch:11, Train loss:0.002759, valid loss:0.001344
Epoch:12, Train loss:0.002626, valid loss:0.001303
Epoch:13, Train loss:0.002437, valid loss:0.001331
Epoch:14, Train loss:0.002375, valid loss:0.001204
Epoch:15, Train loss:0.002257, valid loss:0.001293
Epoch:16, Train loss:0.002142, valid loss:0.001291
Epoch:17, Train loss:0.002077, valid loss:0.001046
Epoch:18, Train loss:0.002053, valid loss:0.001134
Epoch:19, Train loss:0.002011, valid loss:0.001184
Epoch:20, Train loss:0.001871, valid loss:0.001117
Epoch:21, Train loss:0.001482, valid loss:0.000925
Epoch:22, Train loss:0.001426, valid loss:0.000970
Epoch:23, Train loss:0.001357, valid loss:0.000980
Epoch:24, Train loss:0.001381, valid loss:0.001039
Epoch:25, Train loss:0.001318, valid loss:0.000986
Epoch:26, Train loss:0.001324, valid loss:0.000944
Epoch:27, Train loss:0.001244, valid loss:0.000989
Epoch:28, Train loss:0.001261, valid loss:0.000875
Epoch:29, Train loss:0.001243, valid loss:0.000951
Epoch:30, Train loss:0.001215, valid loss:0.000906
Epoch:31, Train loss:0.001019, valid loss:0.000831
Epoch:32, Train loss:0.000977, valid loss:0.000817
Epoch:33, Train loss:0.000981, valid loss:0.000864
Epoch:34, Train loss:0.000954, valid loss:0.000842
Epoch:35, Train loss:0.000930, valid loss:0.000805
Epoch:36, Train loss:0.000925, valid loss:0.000793
Epoch:37, Train loss:0.000939, valid loss:0.000811
Epoch:38, Train loss:0.000950, valid loss:0.000735
Epoch:39, Train loss:0.000895, valid loss:0.000826
Epoch:40, Train loss:0.000881, valid loss:0.000799
Epoch:41, Train loss:0.000788, valid loss:0.000764
Epoch:42, Train loss:0.000777, valid loss:0.000735
Epoch:43, Train loss:0.000763, valid loss:0.000765
Epoch:44, Train loss:0.000771, valid loss:0.000762
Epoch:45, Train loss:0.000769, valid loss:0.000732
Epoch:46, Train loss:0.000749, valid loss:0.000765
Epoch:47, Train loss:0.000748, valid loss:0.000779
Epoch:48, Train loss:0.000741, valid loss:0.000791
Epoch:49, Train loss:0.000734, valid loss:0.000762
Epoch:50, Train loss:0.000735, valid loss:0.000757
Epoch:51, Train loss:0.000686, valid loss:0.000736
Epoch:52, Train loss:0.000677, valid loss:0.000739
Epoch:53, Train loss:0.000672, valid loss:0.000730
Epoch:54, Train loss:0.000670, valid loss:0.000721
Epoch:55, Train loss:0.000668, valid loss:0.000726
Epoch:56, Train loss:0.000667, valid loss:0.000729
Epoch:57, Train loss:0.000666, valid loss:0.000721
Epoch:58, Train loss:0.000664, valid loss:0.000738
Epoch:59, Train loss:0.000664, valid loss:0.000727
Epoch:60, Train loss:0.000663, valid loss:0.000722
training time 12063.239634990692
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07741578364299583
plot_id,batch_id 0 1 miss% 0.0796941635190007
plot_id,batch_id 0 2 miss% 0.06469865789688446
plot_id,batch_id 0 3 miss% 0.054358671641653256
plot_id,batch_id 0 4 miss% 0.050134788021097554
plot_id,batch_id 0 5 miss% 0.03544262962107835
plot_id,batch_id 0 6 miss% 0.047994466710877995
plot_id,batch_id 0 7 miss% 0.09144791510497817
plot_id,batch_id 0 8 miss% 0.11030325113644378
plot_id,batch_id 0 9 miss% 0.04503659530240593
plot_id,batch_id 0 10 miss% 0.04080908509424017
plot_id,batch_id 0 11 miss% 0.08008962549359515
plot_id,batch_id 0 12 miss% 0.09099014534004023
plot_id,batch_id 0 13 miss% 0.08930187590739593
plot_id,batch_id 0 14 miss% 0.06747701186139646
plot_id,batch_id 0 15 miss% 0.08387542489561955
plot_id,batch_id 0 16 miss% 0.05173813729662826
plot_id,batch_id 0 17 miss% 0.03985671443190702
plot_id,batch_id 0 18 miss% 0.06844077514133162
plot_id,batch_id 0 19 miss% 0.0812032206112575
plot_id,batch_id 0 20 miss% 0.03263669119151193
plot_id,batch_id 0 21 miss% 0.0644763264830848
plot_id,batch_id 0 22 miss% 0.05487028076051665
plot_id,batch_id 0 23 miss% 0.06003023173466198
plot_id,batch_id 0 24 miss% 0.05789209739067006
plot_id,batch_id 0 25 miss% 0.06471975314649245
plot_id,batch_id 0 26 miss% 0.031083305227815793
plot_id,batch_id 0 27 miss% 0.038733118825407205
plot_id,batch_id 0 28 miss% 0.03383315691584652
plot_id,batch_id 0 29 miss% 0.05594912187668267
plot_id,batch_id 0 30 miss% 0.05349251538865025
plot_id,batch_id 0 31 miss% 0.11210353279219802
plot_id,batch_id 0 32 miss% 0.07158246087511129
plot_id,batch_id 0 33 miss% 0.06034043052889298
plot_id,batch_id 0 34 miss% 0.035052538782369205
plot_id,batch_id 0 35 miss% 0.08951582673845276
plot_id,batch_id 0 36 miss% 0.09053912597808852
plot_id,batch_id 0 37 miss% 0.043536832736585145
plot_id,batch_id 0 38 miss% 0.05222682001780889
plot_id,batch_id 0 39 miss% 0.12013946824757314
plot_id,batch_id 0 40 miss% 0.07711777835715501
plot_id,batch_id 0 41 miss% 0.035004702925075015
plot_id,batch_id 0 42 miss% 0.04511026053658884
plot_id,batch_id 0 43 miss% 0.04401531063909419
plot_id,batch_id 0 44 miss% 0.037949240009794466
plot_id,batch_id 0 45 miss% 0.02190814650932425
plot_id,batch_id 0 46 miss% 0.043145331706564954
plot_id,batch_id 0 47 miss% 0.03925282616728737
plot_id,batch_id 0 48 miss% 0.050442816905027416
plot_id,batch_id 0 49 miss% 0.03242446812310663
plot_id,batch_id 0 50 miss% 0.12625429987387363
plot_id,batch_id 0 51 miss% 0.023439458410810743
plot_id,batch_id 0 52 miss% 0.04213067011138402
plot_id,batch_id 0 53 miss% 0.03891746779378695
plot_id,batch_id 0 54 miss% 0.022492488930104672
plot_id,batch_id 0 55 miss% 0.058295710175916234
plot_id,batch_id 0 56 miss% 0.06522541462816876
plot_id,batch_id 0 57 miss% 0.03915563382256405
plot_id,batch_id 0 58 miss% 0.06475921514724077
plot_id,batch_id 0 59 miss% 0.05544853183585086
plot_id,batch_id 0 60 miss% 0.050218870321057686
plot_id,batch_id 0 61 miss% 0.020992544119172448
plot_id,batch_id 0 62 miss% 0.10379515744375589
plot_id,batch_id 0 63 miss% 0.055358058944667006
plot_id,batch_id 0 64 miss% 0.05398611051502692
plot_id,batch_id 0 65 miss% 0.059343603969681316
plot_id,batch_id 0 66 miss% 0.05170220240016828
plot_id,batch_id 0 67 miss% 0.023364541025216873
plot_id,batch_id 0 68 miss% 0.03623739384096716
0.05433296134588143
plot_id,batch_id 0 69 miss% 0.0300096870913206
plot_id,batch_id 0 70 miss% 0.03785853003098278
plot_id,batch_id 0 71 miss% 0.045306330139126307
plot_id,batch_id 0 72 miss% 0.0931805249207884
plot_id,batch_id 0 73 miss% 0.056975713712615124
plot_id,batch_id 0 74 miss% 0.07229019817018817
plot_id,batch_id 0 75 miss% 0.0924816477846446
plot_id,batch_id 0 76 miss% 0.06304964983883984
plot_id,batch_id 0 77 miss% 0.07706601693628859
plot_id,batch_id 0 78 miss% 0.04111709817096691
plot_id,batch_id 0 79 miss% 0.12657684618400133
plot_id,batch_id 0 80 miss% 0.044049281220957685
plot_id,batch_id 0 81 miss% 0.0905603249777168
plot_id,batch_id 0 82 miss% 0.07326219299770456
plot_id,batch_id 0 83 miss% 0.09445700551497195
plot_id,batch_id 0 84 miss% 0.048649968948520465
plot_id,batch_id 0 85 miss% 0.04583346665366182
plot_id,batch_id 0 86 miss% 0.04288875570213192
plot_id,batch_id 0 87 miss% 0.04940307957157605
plot_id,batch_id 0 88 miss% 0.06416804354387172
plot_id,batch_id 0 89 miss% 0.048955015536808456
plot_id,batch_id 0 90 miss% 0.02989520550457417
plot_id,batch_id 0 91 miss% 0.06152579353587001
plot_id,batch_id 0 92 miss% 0.04357747925934986
plot_id,batch_id 0 93 miss% 0.030340761796822917
plot_id,batch_id 0 94 miss% 0.0631560120391097
plot_id,batch_id 0 95 miss% 0.043527147111089584
plot_id,batch_id 0 96 miss% 0.030396425217440355
plot_id,batch_id 0 97 miss% 0.042288302151108355
plot_id,batch_id 0 98 miss% 0.026675850559923175
plot_id,batch_id 0 99 miss% 0.06434414034934507
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08219257 0.05027831 0.08371613 0.05614718 0.0215877  0.08296885
 0.02907961 0.05424551 0.05396215 0.04888914 0.04024735 0.07053162
 0.04492067 0.05443951 0.06986353 0.04895473 0.06843681 0.05904479
 0.0572372  0.04249406 0.07694299 0.08102973 0.02689367 0.03199076
 0.03107233 0.05894257 0.0685162  0.04282554 0.02920695 0.02695582
 0.0335066  0.06906055 0.08013316 0.0468766  0.03061371 0.07402438
 0.06271673 0.12872959 0.04704921 0.03272821 0.05553496 0.05229931
 0.05686125 0.0414179  0.02126535 0.0708391  0.03127173 0.0128651
 0.01993333 0.03650203 0.13360506 0.03526265 0.031852   0.02982132
 0.02872577 0.05950191 0.10480405 0.03179258 0.03760858 0.02457629
 0.03176616 0.01876108 0.07115118 0.0601456  0.05866397 0.06250802
 0.11651705 0.02437682 0.05433296 0.03000969 0.03785853 0.04530633
 0.09318052 0.05697571 0.0722902  0.09248165 0.06304965 0.07706602
 0.0411171  0.12657685 0.04404928 0.09056032 0.07326219 0.09445701
 0.04864997 0.04583347 0.04288876 0.04940308 0.06416804 0.04895502
 0.02989521 0.06152579 0.04357748 0.03034076 0.06315601 0.04352715
 0.03039643 0.0422883  0.02667585 0.06434414]
for model  60 the mean error 0.05387482326425283
all id 60 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 21 out win 4 err 0.05387482326425283 time 12038.119781970978
Launcher: Job 61 completed in 12303 seconds.
Launcher: Task 196 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  154129
Epoch:0, Train loss:0.708836, valid loss:0.710832
Epoch:1, Train loss:0.063510, valid loss:0.006026
Epoch:2, Train loss:0.015825, valid loss:0.005108
Epoch:3, Train loss:0.013864, valid loss:0.004012
Epoch:4, Train loss:0.009787, valid loss:0.003382
Epoch:5, Train loss:0.005949, valid loss:0.002373
Epoch:6, Train loss:0.005147, valid loss:0.002403
Epoch:7, Train loss:0.004745, valid loss:0.002334
Epoch:8, Train loss:0.004521, valid loss:0.002289
Epoch:9, Train loss:0.004271, valid loss:0.002394
Epoch:10, Train loss:0.004086, valid loss:0.002058
Epoch:11, Train loss:0.003006, valid loss:0.001395
Epoch:12, Train loss:0.002736, valid loss:0.001662
Epoch:13, Train loss:0.002626, valid loss:0.001671
Epoch:14, Train loss:0.002622, valid loss:0.001665
Epoch:15, Train loss:0.002541, valid loss:0.001342
Epoch:16, Train loss:0.002286, valid loss:0.001252
Epoch:17, Train loss:0.002235, valid loss:0.001312
Epoch:18, Train loss:0.002295, valid loss:0.001348
Epoch:19, Train loss:0.002027, valid loss:0.001261
Epoch:20, Train loss:0.002048, valid loss:0.001380
Epoch:21, Train loss:0.001509, valid loss:0.001048
Epoch:22, Train loss:0.001400, valid loss:0.000966
Epoch:23, Train loss:0.001361, valid loss:0.001113
Epoch:24, Train loss:0.001330, valid loss:0.001190
Epoch:25, Train loss:0.001367, valid loss:0.001067
Epoch:26, Train loss:0.001271, valid loss:0.001005
Epoch:27, Train loss:0.001334, valid loss:0.001123
Epoch:28, Train loss:0.001300, valid loss:0.001032
Epoch:29, Train loss:0.001176, valid loss:0.001065
Epoch:30, Train loss:0.001233, valid loss:0.001014
Epoch:31, Train loss:0.000924, valid loss:0.000963
Epoch:32, Train loss:0.000898, valid loss:0.000912
Epoch:33, Train loss:0.000882, valid loss:0.000934
Epoch:34, Train loss:0.000874, valid loss:0.000971
Epoch:35, Train loss:0.000843, valid loss:0.000960
Epoch:36, Train loss:0.000865, valid loss:0.000905
Epoch:37, Train loss:0.000863, valid loss:0.000864
Epoch:38, Train loss:0.000828, valid loss:0.000896
Epoch:39, Train loss:0.000803, valid loss:0.000887
Epoch:40, Train loss:0.000784, valid loss:0.000972
Epoch:41, Train loss:0.000679, valid loss:0.000911
Epoch:42, Train loss:0.000669, valid loss:0.000867
Epoch:43, Train loss:0.000652, valid loss:0.000863
Epoch:44, Train loss:0.000640, valid loss:0.000862
Epoch:45, Train loss:0.000636, valid loss:0.000905
Epoch:46, Train loss:0.000630, valid loss:0.000867
Epoch:47, Train loss:0.000633, valid loss:0.000888
Epoch:48, Train loss:0.000640, valid loss:0.000899
Epoch:49, Train loss:0.000624, valid loss:0.000890
Epoch:50, Train loss:0.000606, valid loss:0.000885
Epoch:51, Train loss:0.000577, valid loss:0.000873
Epoch:52, Train loss:0.000566, valid loss:0.000873
Epoch:53, Train loss:0.000563, valid loss:0.000874
Epoch:54, Train loss:0.000560, valid loss:0.000875
Epoch:55, Train loss:0.000559, valid loss:0.000874
Epoch:56, Train loss:0.000557, valid loss:0.000875
Epoch:57, Train loss:0.000556, valid loss:0.000874
Epoch:58, Train loss:0.000555, valid loss:0.000876
Epoch:59, Train loss:0.000554, valid loss:0.000874
Epoch:60, Train loss:0.000552, valid loss:0.000874
training time 12088.504332304
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.08989651294170076
plot_id,batch_id 0 1 miss% 0.07893537634487445
plot_id,batch_id 0 2 miss% 0.07893820661509113
plot_id,batch_id 0 3 miss% 0.04079645610049156
plot_id,batch_id 0 4 miss% 0.03638245436020191
plot_id,batch_id 0 5 miss% 0.04945021223518982
plot_id,batch_id 0 6 miss% 0.04509168549293629
plot_id,batch_id 0 7 miss% 0.08138671317922254
plot_id,batch_id 0 8 miss% 0.07076548680549664
plot_id,batch_id 0 9 miss% 0.035249565852009244
plot_id,batch_id 0 10 miss% 0.05521109195629987
plot_id,batch_id 0 11 miss% 0.05594825437687748
plot_id,batch_id 0 12 miss% 0.04646592876657295
plot_id,batch_id 0 13 miss% 0.07843536917014597
plot_id,batch_id 0 14 miss% 0.0354882212293347
plot_id,batch_id 0 15 miss% 0.05845801678629965
plot_id,batch_id 0 16 miss% 0.09229343549013089
plot_id,batch_id 0 17 miss% 0.08290453693784633
plot_id,batch_id 0 18 miss% 0.0514660422088823
plot_id,batch_id 0 19 miss% 0.07048944356296657
plot_id,batch_id 0 20 miss% 0.09049912231867371
plot_id,batch_id 0 21 miss% 0.04065252427187766
plot_id,batch_id 0 22 miss% 0.03626654551231829
plot_id,batch_id 0 23 miss% 0.03010961851612291
plot_id,batch_id 0 24 miss% 0.05834166335122486
plot_id,batch_id 0 25 miss% 0.04312859828116751
plot_id,batch_id 0 26 miss% 0.031402680901478824
plot_id,batch_id 0 27 miss% 0.06399184783121875
plot_id,batch_id 0 28 miss% 0.025656738140098286
plot_id,batch_id 0 29 miss% 0.027718706799686924
plot_id,batch_id 0 30 miss% 0.055859097484656645
plot_id,batch_id 0 31 miss% 0.11234285422253862
plot_id,batch_id 0 32 miss% 0.08857678719720784
plot_id,batch_id 0 33 miss% 0.03831666587636125
plot_id,batch_id 0 34 miss% 0.025144710397973075
plot_id,batch_id 0 35 miss% 0.04517126620857968
plot_id,batch_id 0 36 miss% 0.09162762983969047
plot_id,batch_id 0 37 miss% 0.041821188944533454
plot_id,batch_id 0 38 miss% 0.030073456233367803
plot_id,batch_id 0 39 miss% 0.0424404114629519
plot_id,batch_id 0 40 miss% 0.0633137703858444
plot_id,batch_id 0 41 miss% 0.04298159565181508
plot_id,batch_id 0 42 miss% 0.03926040981510242
plot_id,batch_id 0 43 miss% 0.03522699770076297
plot_id,batch_id 0 44 miss% 0.026660490227316213
plot_id,batch_id 0 45 miss% 0.044020018248852974
plot_id,batch_id 0 46 miss% 0.02986186038149648
plot_id,batch_id 0 47 miss% 0.02959596331968243
plot_id,batch_id 0 48 miss% 0.026448629322230147
plot_id,batch_id 0 49 miss% 0.03839573626972672
plot_id,batch_id 0 50 miss% 0.1442619192214552
plot_id,batch_id 0 51 miss% 0.03659281773436379
plot_id,batch_id 0 52 miss% 0.02876498686457922
plot_id,batch_id 0 53 miss% 0.03734504541045844
plot_id,batch_id 0 54 miss% 0.06189548250476604
plot_id,batch_id 0 55 miss% 0.06771098583616643
plot_id,batch_id 0 56 miss% 0.03609152952063563
plot_id,batch_id 0 57 miss% 0.021942484340453216
plot_id,batch_id 0 58 miss% 0.02556078121349448
plot_id,batch_id 0 59 miss% 0.022273023288076826
plot_id,batch_id 0 60 miss% 0.040386741160878065
plot_id,batch_id 0 61 miss% 0.028290466044514614
plot_id,batch_id 0 62 miss% 0.10866763789547663
plot_id,batch_id 0 63 miss% 0.03547025395082907
plot_id,batch_id 0 64 miss% 0.04156147974648376
plot_id,batch_id 0 65 miss% 0.05808682562994642
plot_id,batch_id 0 66 miss% 0.119385286256564
plot_id,batch_id 0 67 miss% 0.027908891916439353
plot_id,batch_id 0 68 miss% 0.025730527851713075
plot_id,batch_id 0 69 miss% 0.047071963720966004
plot_id,batch_id 0 70 miss% 0.05329053769027574
plot_id,batch_id 0 71 miss% 0.028652997335894607
plot_id,batch_id 0 72 miss% 0.09529508794163918
plot_id,batch_id 0 73 miss% 0.07868548003915447
plot_id,batch_id 0 74 miss% 0.1416257808712971
plot_id,batch_id 0 75 miss% 0.032427309144871144
plot_id,batch_id 0 76 miss% 0.08838083305033306
plot_id,batch_id 0 77 miss% 0.040080315907353066
plot_id,batch_id 0 78 miss% 0.04440302010454937
plot_id,batch_id 0 79 miss% 0.09557240913182458
plot_id,batch_id 0 80 miss% 0.04788003568278692
plot_id,batch_id 0 81 miss% 0.06779030627555055
plot_id,batch_id 0 82 miss% 0.050670707873011436
plot_id,batch_id 0 83 miss% 0.09383595911438841
plot_id,batch_id 0 84 miss% 0.055066740573604354
plot_id,batch_id 0 85 miss% 0.024778712886991065
plot_id,batch_id 0 86 miss% 0.047205545637579734
plot_id,batch_id 0 87 miss% 0.08551971213964236
plot_id,batch_id 0 88 miss% 0.14796947551238065
plot_id,batch_id 0 89 miss% 0.0754724716865193
plot_id,batch_id 0 90 miss% 0.037129181898035116
plot_id,batch_id 0 91 miss% 0.07284444191426209
plot_id,batch_id 0 92 miss% 0.058902939040534445
plot_id,batch_id 0 93 miss% 0.04718386027068904
plot_id,batch_id 0 94 miss% 0.0602737693417568
plot_id,batch_id 0 95 miss% 0.05908350047595963
plot_id,batch_id 0 96 miss% 0.0495365389534493
plot_id,batch_id 0 97 miss% 0.06898387918354049
plot_id,batch_id 0 98 miss% 0.02369363737427088
plot_id,batch_id 0 99 miss% 0.07382177726589491
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07741578 0.07969416 0.06469866 0.05435867 0.05013479 0.03544263
 0.04799447 0.09144792 0.11030325 0.0450366  0.04080909 0.08008963
 0.09099015 0.08930188 0.06747701 0.08387542 0.05173814 0.03985671
 0.06844078 0.08120322 0.03263669 0.06447633 0.05487028 0.06003023
 0.0578921  0.06471975 0.03108331 0.03873312 0.03383316 0.05594912
 0.05349252 0.11210353 0.07158246 0.06034043 0.03505254 0.08951583
 0.09053913 0.04353683 0.05222682 0.12013947 0.07711778 0.0350047
 0.04511026 0.04401531 0.03794924 0.02190815 0.04314533 0.03925283
 0.05044282 0.03242447 0.1262543  0.02343946 0.04213067 0.03891747
 0.02249249 0.05829571 0.06522541 0.03915563 0.06475922 0.05544853
 0.05021887 0.02099254 0.10379516 0.05535806 0.05398611 0.0593436
 0.0517022  0.02336454 0.03623739 0.04707196 0.05329054 0.028653
 0.09529509 0.07868548 0.14162578 0.03242731 0.08838083 0.04008032
 0.04440302 0.09557241 0.04788004 0.06779031 0.05067071 0.09383596
 0.05506674 0.02477871 0.04720555 0.08551971 0.14796948 0.07547247
 0.03712918 0.07284444 0.05890294 0.04718386 0.06027377 0.0590835
 0.04953654 0.06898388 0.02369364 0.07382178]
for model  40 the mean error 0.05983679757536686
all id 40 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 21 out win 5 err 0.05983679757536686 time 12063.239634990692
Launcher: Job 41 completed in 12332 seconds.
Launcher: Task 224 done. Exiting.
plot_id,batch_id 0 69 miss% 0.08102736892997216
plot_id,batch_id 0 70 miss% 0.05725661820073634
plot_id,batch_id 0 71 miss% 0.02646432204539638
plot_id,batch_id 0 72 miss% 0.10629438385594552
plot_id,batch_id 0 73 miss% 0.05875545079030466
plot_id,batch_id 0 74 miss% 0.11304031022196082
plot_id,batch_id 0 75 miss% 0.045619568197462136
plot_id,batch_id 0 76 miss% 0.1302443335920634
plot_id,batch_id 0 77 miss% 0.030123959510551093
plot_id,batch_id 0 78 miss% 0.062499620451102474
plot_id,batch_id 0 79 miss% 0.03279922226588823
plot_id,batch_id 0 80 miss% 0.08206463595620664
plot_id,batch_id 0 81 miss% 0.08207692003794254
plot_id,batch_id 0 82 miss% 0.05785728813924781
plot_id,batch_id 0 83 miss% 0.05695973124263619
plot_id,batch_id 0 84 miss% 0.06277434123280391
plot_id,batch_id 0 85 miss% 0.034549950236867515
plot_id,batch_id 0 86 miss% 0.0671378565395275
plot_id,batch_id 0 87 miss% 0.10449356119453643
plot_id,batch_id 0 88 miss% 0.0673623315483307
plot_id,batch_id 0 89 miss% 0.06212567961049251
plot_id,batch_id 0 90 miss% 0.02609760039517974
plot_id,batch_id 0 91 miss% 0.06397741330780309
plot_id,batch_id 0 92 miss% 0.06584830516992149
plot_id,batch_id 0 93 miss% 0.056945984023739425
plot_id,batch_id 0 94 miss% 0.07191410490200482
plot_id,batch_id 0 95 miss% 0.08825389011494336
plot_id,batch_id 0 96 miss% 0.07670902313139219
plot_id,batch_id 0 97 miss% 0.06106388423026586
plot_id,batch_id 0 98 miss% 0.019301078510246872
plot_id,batch_id 0 99 miss% 0.03147058356378336
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08989651 0.07893538 0.07893821 0.04079646 0.03638245 0.04945021
 0.04509169 0.08138671 0.07076549 0.03524957 0.05521109 0.05594825
 0.04646593 0.07843537 0.03548822 0.05845802 0.09229344 0.08290454
 0.05146604 0.07048944 0.09049912 0.04065252 0.03626655 0.03010962
 0.05834166 0.0431286  0.03140268 0.06399185 0.02565674 0.02771871
 0.0558591  0.11234285 0.08857679 0.03831667 0.02514471 0.04517127
 0.09162763 0.04182119 0.03007346 0.04244041 0.06331377 0.0429816
 0.03926041 0.035227   0.02666049 0.04402002 0.02986186 0.02959596
 0.02644863 0.03839574 0.14426192 0.03659282 0.02876499 0.03734505
 0.06189548 0.06771099 0.03609153 0.02194248 0.02556078 0.02227302
 0.04038674 0.02829047 0.10866764 0.03547025 0.04156148 0.05808683
 0.11938529 0.02790889 0.02573053 0.08102737 0.05725662 0.02646432
 0.10629438 0.05875545 0.11304031 0.04561957 0.13024433 0.03012396
 0.06249962 0.03279922 0.08206464 0.08207692 0.05785729 0.05695973
 0.06277434 0.03454995 0.06713786 0.10449356 0.06736233 0.06212568
 0.0260976  0.06397741 0.06584831 0.05694598 0.0719141  0.08825389
 0.07670902 0.06106388 0.01930108 0.03147058]
for model  62 the mean error 0.05579997083063679
all id 62 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 21 out win 6 err 0.05579997083063679 time 12088.504332304
Launcher: Job 63 completed in 12353 seconds.
Launcher: Task 135 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  154129
Epoch:0, Train loss:0.725999, valid loss:0.745131
Epoch:1, Train loss:0.126457, valid loss:0.005762
Epoch:2, Train loss:0.014077, valid loss:0.003982
Epoch:3, Train loss:0.006673, valid loss:0.002845
Epoch:4, Train loss:0.005425, valid loss:0.002489
Epoch:5, Train loss:0.004495, valid loss:0.001922
Epoch:6, Train loss:0.003954, valid loss:0.001780
Epoch:7, Train loss:0.003578, valid loss:0.001800
Epoch:8, Train loss:0.003357, valid loss:0.001505
Epoch:9, Train loss:0.002918, valid loss:0.001606
Epoch:10, Train loss:0.002792, valid loss:0.001653
Epoch:11, Train loss:0.002076, valid loss:0.001336
Epoch:12, Train loss:0.002005, valid loss:0.001186
Epoch:13, Train loss:0.001975, valid loss:0.001067
Epoch:14, Train loss:0.001893, valid loss:0.001209
Epoch:15, Train loss:0.001832, valid loss:0.001092
Epoch:16, Train loss:0.001764, valid loss:0.001142
Epoch:17, Train loss:0.001642, valid loss:0.001144
Epoch:18, Train loss:0.001643, valid loss:0.000989
Epoch:19, Train loss:0.001607, valid loss:0.001068
Epoch:20, Train loss:0.001534, valid loss:0.001041
Epoch:21, Train loss:0.001227, valid loss:0.000923
Epoch:22, Train loss:0.001193, valid loss:0.000832
Epoch:23, Train loss:0.001177, valid loss:0.000856
Epoch:24, Train loss:0.001134, valid loss:0.000739
Epoch:25, Train loss:0.001110, valid loss:0.000842
Epoch:26, Train loss:0.001109, valid loss:0.000788
Epoch:27, Train loss:0.001077, valid loss:0.000730
Epoch:28, Train loss:0.001058, valid loss:0.000730
Epoch:29, Train loss:0.001048, valid loss:0.000760
Epoch:30, Train loss:0.001023, valid loss:0.000797
Epoch:31, Train loss:0.000864, valid loss:0.000713
Epoch:32, Train loss:0.000845, valid loss:0.000700
Epoch:33, Train loss:0.000841, valid loss:0.000668
Epoch:34, Train loss:0.000823, valid loss:0.000655
Epoch:35, Train loss:0.000827, valid loss:0.000667
Epoch:36, Train loss:0.000813, valid loss:0.000723
Epoch:37, Train loss:0.000815, valid loss:0.000669
Epoch:38, Train loss:0.000796, valid loss:0.000663
Epoch:39, Train loss:0.000790, valid loss:0.000643
Epoch:40, Train loss:0.000788, valid loss:0.000698
Epoch:41, Train loss:0.000709, valid loss:0.000668
Epoch:42, Train loss:0.000703, valid loss:0.000649
Epoch:43, Train loss:0.000697, valid loss:0.000626
Epoch:44, Train loss:0.000692, valid loss:0.000649
Epoch:45, Train loss:0.000689, valid loss:0.000627
Epoch:46, Train loss:0.000686, valid loss:0.000623
Epoch:47, Train loss:0.000682, valid loss:0.000636
Epoch:48, Train loss:0.000674, valid loss:0.000631
Epoch:49, Train loss:0.000671, valid loss:0.000624
Epoch:50, Train loss:0.000670, valid loss:0.000606
Epoch:51, Train loss:0.000630, valid loss:0.000603
Epoch:52, Train loss:0.000625, valid loss:0.000630
Epoch:53, Train loss:0.000622, valid loss:0.000609
Epoch:54, Train loss:0.000621, valid loss:0.000617
Epoch:55, Train loss:0.000620, valid loss:0.000626
Epoch:56, Train loss:0.000619, valid loss:0.000620
Epoch:57, Train loss:0.000618, valid loss:0.000630
Epoch:58, Train loss:0.000617, valid loss:0.000608
Epoch:59, Train loss:0.000618, valid loss:0.000604
Epoch:60, Train loss:0.000617, valid loss:0.000610
training time 12177.124764680862
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08479530520405003
plot_id,batch_id 0 1 miss% 0.04341110450517636
plot_id,batch_id 0 2 miss% 0.09768482453185127
plot_id,batch_id 0 3 miss% 0.09003273161507623
plot_id,batch_id 0 4 miss% 0.04597511443921971
plot_id,batch_id 0 5 miss% 0.035836718329230605
plot_id,batch_id 0 6 miss% 0.035814387984977894
plot_id,batch_id 0 7 miss% 0.0807415410022486
plot_id,batch_id 0 8 miss% 0.06961832337864442
plot_id,batch_id 0 9 miss% 0.08253788764587
plot_id,batch_id 0 10 miss% 0.0334671459043137
plot_id,batch_id 0 11 miss% 0.08199724476809252
plot_id,batch_id 0 12 miss% 0.08531384522646872
plot_id,batch_id 0 13 miss% 0.044971349591337806
plot_id,batch_id 0 14 miss% 0.10393819504175147
plot_id,batch_id 0 15 miss% 0.09871197135022126
plot_id,batch_id 0 16 miss% 0.06493644839476528
plot_id,batch_id 0 17 miss% 0.022437480802960062
plot_id,batch_id 0 18 miss% 0.07248808669779357
plot_id,batch_id 0 19 miss% 0.12412996159324671
plot_id,batch_id 0 20 miss% 0.05114594209159994
plot_id,batch_id 0 21 miss% 0.03242171874772827
plot_id,batch_id 0 22 miss% 0.05589793150206414
plot_id,batch_id 0 23 miss% 0.05068615068038492
plot_id,batch_id 0 24 miss% 0.062425084120343644
plot_id,batch_id 0 25 miss% 0.0659211653810207
plot_id,batch_id 0 26 miss% 0.05807146640533812
plot_id,batch_id 0 27 miss% 0.051048958458055876
plot_id,batch_id 0 28 miss% 0.057807632590339664
plot_id,batch_id 0 29 miss% 0.036784208943380275
plot_id,batch_id 0 30 miss% 0.05943169296734009
plot_id,batch_id 0 31 miss% 0.08672857397730102
plot_id,batch_id 0 32 miss% 0.11800249296610892
plot_id,batch_id 0 33 miss% 0.07953494362711792
plot_id,batch_id 0 34 miss% 0.04946578097251893
plot_id,batch_id 0 35 miss% 0.046047019553137614
plot_id,batch_id 0 36 miss% 0.07134081781259693
plot_id,batch_id 0 37 miss% 0.059260764502200636
plot_id,batch_id 0 38 miss% 0.039090350642903594
plot_id,batch_id 0 39 miss% 0.029726141475047565
plot_id,batch_id 0 40 miss% 0.04333214381161429
plot_id,batch_id 0 41 miss% 0.0402466806248396
plot_id,batch_id 0 42 miss% 0.03479062359567821
plot_id,batch_id 0 43 miss% 0.05367359167542804
plot_id,batch_id 0 44 miss% 0.039645232001401284
plot_id,batch_id 0 45 miss% 0.02605085064704869
plot_id,batch_id 0 46 miss% 0.041730456734964894
plot_id,batch_id 0 47 miss% 0.03651525759382713
plot_id,batch_id 0 48 miss% 0.047208545262840566
plot_id,batch_id 0 49 miss% 0.04133197845681034
plot_id,batch_id 0 50 miss% 0.12592557058197615
plot_id,batch_id 0 51 miss% 0.03670502322031307
plot_id,batch_id 0 52 miss% 0.04958051429133896
plot_id,batch_id 0 53 miss% 0.021764104780460782
plot_id,batch_id 0 54 miss% 0.03609618199983998
plot_id,batch_id 0 55 miss% 0.03079766999878849
plot_id,batch_id 0 56 miss% 0.0763519102262012
plot_id,batch_id 0 57 miss% 0.031531412804293744
plot_id,batch_id 0 58 miss% 0.06150880467943225
plot_id,batch_id 0 59 miss% 0.06842114746586148
plot_id,batch_id 0 60 miss% 0.04918746823641265
plot_id,batch_id 0 61 miss% 0.030569585089410947
plot_id,batch_id 0 62 miss% 0.035348079334738144
plot_id,batch_id 0 63 miss% 0.04302207046774558
plot_id,batch_id 0 64 miss% 0.06891401602662728
plot_id,batch_id 0 65 miss% 0.05835327682973409
plot_id,batch_id 0 66 miss% 0.04791444152687308
plot_id,batch_id 0 67 miss% 0.059127531735703814
plot_id,batch_id 0 68 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  55697
Epoch:0, Train loss:0.573817, valid loss:0.538962
Epoch:1, Train loss:0.136990, valid loss:0.004486
Epoch:2, Train loss:0.011375, valid loss:0.003589
Epoch:3, Train loss:0.009774, valid loss:0.003024
Epoch:4, Train loss:0.008304, valid loss:0.002990
Epoch:5, Train loss:0.005879, valid loss:0.002040
Epoch:6, Train loss:0.005196, valid loss:0.002059
Epoch:7, Train loss:0.004848, valid loss:0.001793
Epoch:8, Train loss:0.004573, valid loss:0.001800
Epoch:9, Train loss:0.004406, valid loss:0.001515
Epoch:10, Train loss:0.004171, valid loss:0.001537
Epoch:11, Train loss:0.003577, valid loss:0.001349
Epoch:12, Train loss:0.003572, valid loss:0.001333
Epoch:13, Train loss:0.003417, valid loss:0.001313
Epoch:14, Train loss:0.003352, valid loss:0.001346
Epoch:15, Train loss:0.003285, valid loss:0.001352
Epoch:16, Train loss:0.003235, valid loss:0.001241
Epoch:17, Train loss:0.003183, valid loss:0.001199
Epoch:18, Train loss:0.003130, valid loss:0.001104
Epoch:19, Train loss:0.003062, valid loss:0.001218
Epoch:20, Train loss:0.003046, valid loss:0.001054
Epoch:21, Train loss:0.002718, valid loss:0.000945
Epoch:22, Train loss:0.002681, valid loss:0.001018
Epoch:23, Train loss:0.002536, valid loss:0.000860
Epoch:24, Train loss:0.001315, valid loss:0.000887
Epoch:25, Train loss:0.001245, valid loss:0.000787
Epoch:26, Train loss:0.001225, valid loss:0.000721
Epoch:27, Train loss:0.001211, valid loss:0.000752
Epoch:28, Train loss:0.001182, valid loss:0.000795
Epoch:29, Train loss:0.001164, valid loss:0.000718
Epoch:30, Train loss:0.001143, valid loss:0.000757
Epoch:31, Train loss:0.000988, valid loss:0.000687
Epoch:32, Train loss:0.000982, valid loss:0.000682
Epoch:33, Train loss:0.000965, valid loss:0.000681
Epoch:34, Train loss:0.000963, valid loss:0.000672
Epoch:35, Train loss:0.000961, valid loss:0.000656
Epoch:36, Train loss:0.000947, valid loss:0.000662
Epoch:37, Train loss:0.000934, valid loss:0.000656
Epoch:38, Train loss:0.000919, valid loss:0.000690
Epoch:39, Train loss:0.000918, valid loss:0.000663
Epoch:40, Train loss:0.000919, valid loss:0.000700
Epoch:41, Train loss:0.000838, valid loss:0.000652
Epoch:42, Train loss:0.000828, valid loss:0.000667
Epoch:43, Train loss:0.000826, valid loss:0.000644
Epoch:44, Train loss:0.000823, valid loss:0.000634
Epoch:45, Train loss:0.000822, valid loss:0.000637
Epoch:46, Train loss:0.000810, valid loss:0.000654
Epoch:47, Train loss:0.000804, valid loss:0.000621
Epoch:48, Train loss:0.000798, valid loss:0.000632
Epoch:49, Train loss:0.000803, valid loss:0.000624
Epoch:50, Train loss:0.000795, valid loss:0.000651
Epoch:51, Train loss:0.000754, valid loss:0.000618
Epoch:52, Train loss:0.000747, valid loss:0.000624
Epoch:53, Train loss:0.000745, valid loss:0.000619
Epoch:54, Train loss:0.000744, valid loss:0.000617
Epoch:55, Train loss:0.000743, valid loss:0.000629
Epoch:56, Train loss:0.000741, valid loss:0.000641
Epoch:57, Train loss:0.000741, valid loss:0.000625
Epoch:58, Train loss:0.000740, valid loss:0.000620
Epoch:59, Train loss:0.000739, valid loss:0.000618
Epoch:60, Train loss:0.000740, valid loss:0.000622
training time 12218.617328166962
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.08009281174628746
plot_id,batch_id 0 1 miss% 0.08351291655898065
plot_id,batch_id 0 2 miss% 0.07926776087120281
plot_id,batch_id 0 3 miss% 0.08765631541352101
plot_id,batch_id 0 4 miss% 0.08576377020437587
plot_id,batch_id 0 5 miss% 0.05322114585929718
plot_id,batch_id 0 6 miss% 0.05163575350161583
plot_id,batch_id 0 7 miss% 0.09777825746580984
plot_id,batch_id 0 8 miss% 0.10342775458828105
plot_id,batch_id 0 9 miss% 0.04756455102287807
plot_id,batch_id 0 10 miss% 0.05092449934692566
plot_id,batch_id 0 11 miss% 0.10665440981900778
plot_id,batch_id 0 12 miss% 0.060340885762135574
plot_id,batch_id 0 13 miss% 0.04458179579881918
plot_id,batch_id 0 14 miss% 0.11567751985034523
plot_id,batch_id 0 15 miss% 0.03690062938233316
plot_id,batch_id 0 16 miss% 0.17150968983839482
plot_id,batch_id 0 17 miss% 0.05491007166394324
plot_id,batch_id 0 18 miss% 0.06653020766458118
plot_id,batch_id 0 19 miss% 0.0987213555599919
plot_id,batch_id 0 20 miss% 0.0546433612608522
plot_id,batch_id 0 21 miss% 0.05924072729222236
plot_id,batch_id 0 22 miss% 0.0731085314794166
plot_id,batch_id 0 23 miss% 0.03143322798935124
plot_id,batch_id 0 24 miss% 0.09821581146362884
plot_id,batch_id 0 25 miss% 0.09876631644403149
plot_id,batch_id 0 26 miss% 0.044952222056339156
plot_id,batch_id 0 27 miss% 0.030681770933398685
plot_id,batch_id 0 28 miss% 0.04360980511275299
plot_id,batch_id 0 29 miss% 0.02878081724436422
plot_id,batch_id 0 30 miss% 0.055809058434651775
plot_id,batch_id 0 31 miss% 0.11580154289068391
plot_id,batch_id 0 32 miss% 0.09827369613949428
plot_id,batch_id 0 33 miss% 0.06662055925278282
plot_id,batch_id 0 34 miss% 0.03817181156410727
plot_id,batch_id 0 35 miss% 0.05954973635155145
plot_id,batch_id 0 36 miss% 0.08882335176230943
plot_id,batch_id 0 37 miss% 0.08426170218603869
plot_id,batch_id 0 38 miss% 0.08124025636670044
plot_id,batch_id 0 39 miss% 0.03184988714752555
plot_id,batch_id 0 40 miss% 0.10474498079182382
plot_id,batch_id 0 41 miss% 0.07364642147075606
plot_id,batch_id 0 42 miss% 0.03468303471310162
plot_id,batch_id 0 43 miss% 0.07455749700068909
plot_id,batch_id 0 44 miss% 0.02179087365024092
plot_id,batch_id 0 45 miss% 0.08557768015598817
plot_id,batch_id 0 46 miss% 0.03963423217979456
plot_id,batch_id 0 47 miss% 0.036343447561888724
plot_id,batch_id 0 48 miss% 0.031783749597931156
plot_id,batch_id 0 49 miss% 0.031450272006167464
plot_id,batch_id 0 50 miss% 0.09490546328648826
plot_id,batch_id 0 51 miss% 0.03653333154483858
plot_id,batch_id 0 52 miss% 0.030270933090731107
plot_id,batch_id 0 53 miss% 0.02620244985467708
plot_id,batch_id 0 54 miss% 0.048723982973829474
plot_id,batch_id 0 55 miss% 0.04669140509609492
plot_id,batch_id 0 56 miss% 0.07376268576437954
plot_id,batch_id 0 57 miss% 0.07256748077659993
plot_id,batch_id 0 58 miss% 0.03564609543964318
plot_id,batch_id 0 59 miss% 0.033920161936474284
plot_id,batch_id 0 60 miss% 0.04635980011193121
plot_id,batch_id 0 61 miss% 0.037690851363544706
plot_id,batch_id 0 62 miss% 0.08178976668688037
plot_id,batch_id 0 63 miss% 0.09051689548070213
plot_id,batch_id 0 64 miss% 0.05395992151249096
plot_id,batch_id 0 65 miss% 0.13865843187842383
plot_id,batch_id 0 66 miss% 0.0420707650546675
plot_id,batch_id 0 67 miss% 0.04768749937675986
0.0423504787362085
plot_id,batch_id 0 69 miss% 0.0709155587420593
plot_id,batch_id 0 70 miss% 0.03354186885972468
plot_id,batch_id 0 71 miss% 0.04478019388264596
plot_id,batch_id 0 72 miss% 0.11979995031906823
plot_id,batch_id 0 73 miss% 0.07793413675029728
plot_id,batch_id 0 74 miss% 0.06607164231595107
plot_id,batch_id 0 75 miss% 0.04771634190626649
plot_id,batch_id 0 76 miss% 0.06688471494164418
plot_id,batch_id 0 77 miss% 0.030183061248063064
plot_id,batch_id 0 78 miss% 0.035793474946505574
plot_id,batch_id 0 79 miss% 0.13554265805801205
plot_id,batch_id 0 80 miss% 0.02980696600273543
plot_id,batch_id 0 81 miss% 0.0961251816283159
plot_id,batch_id 0 82 miss% 0.046491103745857104
plot_id,batch_id 0 83 miss% 0.05740721766867412
plot_id,batch_id 0 84 miss% 0.0615970155670061
plot_id,batch_id 0 85 miss% 0.04050075899406562
plot_id,batch_id 0 86 miss% 0.07067746162639459
plot_id,batch_id 0 87 miss% 0.06705867516714403
plot_id,batch_id 0 88 miss% 0.09659308332911565
plot_id,batch_id 0 89 miss% 0.07104999262847249
plot_id,batch_id 0 90 miss% 0.04100278921907318
plot_id,batch_id 0 91 miss% 0.056045193477733596
plot_id,batch_id 0 92 miss% 0.040074312805685956
plot_id,batch_id 0 93 miss% 0.0750548378112912
plot_id,batch_id 0 94 miss% 0.06828228500104758
plot_id,batch_id 0 95 miss% 0.04540397032085494
plot_id,batch_id 0 96 miss% 0.06184954554202268
plot_id,batch_id 0 97 miss% 0.05960325070627838
plot_id,batch_id 0 98 miss% 0.026104441032220455
plot_id,batch_id 0 99 miss% 0.04472304972813313
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08479531 0.0434111  0.09768482 0.09003273 0.04597511 0.03583672
 0.03581439 0.08074154 0.06961832 0.08253789 0.03346715 0.08199724
 0.08531385 0.04497135 0.1039382  0.09871197 0.06493645 0.02243748
 0.07248809 0.12412996 0.05114594 0.03242172 0.05589793 0.05068615
 0.06242508 0.06592117 0.05807147 0.05104896 0.05780763 0.03678421
 0.05943169 0.08672857 0.11800249 0.07953494 0.04946578 0.04604702
 0.07134082 0.05926076 0.03909035 0.02972614 0.04333214 0.04024668
 0.03479062 0.05367359 0.03964523 0.02605085 0.04173046 0.03651526
 0.04720855 0.04133198 0.12592557 0.03670502 0.04958051 0.0217641
 0.03609618 0.03079767 0.07635191 0.03153141 0.0615088  0.06842115
 0.04918747 0.03056959 0.03534808 0.04302207 0.06891402 0.05835328
 0.04791444 0.05912753 0.04235048 0.07091556 0.03354187 0.04478019
 0.11979995 0.07793414 0.06607164 0.04771634 0.06688471 0.03018306
 0.03579347 0.13554266 0.02980697 0.09612518 0.0464911  0.05740722
 0.06159702 0.04050076 0.07067746 0.06705868 0.09659308 0.07104999
 0.04100279 0.05604519 0.04007431 0.07505484 0.06828229 0.04540397
 0.06184955 0.05960325 0.02610444 0.04472305]
for model  6 the mean error 0.05822287887832599
all id 6 hidden_dim 32 learning_rate 0.0025 num_layers 3 frames 21 out win 4 err 0.05822287887832599 time 12177.124764680862
Launcher: Job 7 completed in 12446 seconds.
Launcher: Task 126 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  41745
Epoch:0, Train loss:0.449807, valid loss:0.435074
Epoch:1, Train loss:0.023370, valid loss:0.002296
Epoch:2, Train loss:0.004529, valid loss:0.002049
Epoch:3, Train loss:0.003574, valid loss:0.001623
Epoch:4, Train loss:0.003043, valid loss:0.001509
Epoch:5, Train loss:0.002732, valid loss:0.001370
Epoch:6, Train loss:0.002518, valid loss:0.001208
Epoch:7, Train loss:0.002337, valid loss:0.001073
Epoch:8, Train loss:0.002187, valid loss:0.001034
Epoch:9, Train loss:0.002039, valid loss:0.001194
Epoch:10, Train loss:0.001951, valid loss:0.001032
Epoch:11, Train loss:0.001619, valid loss:0.000827
Epoch:12, Train loss:0.001550, valid loss:0.000869
Epoch:13, Train loss:0.001533, valid loss:0.000852
Epoch:14, Train loss:0.001468, valid loss:0.000835
Epoch:15, Train loss:0.001413, valid loss:0.000869
Epoch:16, Train loss:0.001382, valid loss:0.000722
Epoch:17, Train loss:0.001362, valid loss:0.000768
Epoch:18, Train loss:0.001310, valid loss:0.000743
Epoch:19, Train loss:0.001292, valid loss:0.000697
Epoch:20, Train loss:0.001260, valid loss:0.000681
Epoch:21, Train loss:0.001083, valid loss:0.000620
Epoch:22, Train loss:0.001056, valid loss:0.000640
Epoch:23, Train loss:0.001057, valid loss:0.000632
Epoch:24, Train loss:0.001037, valid loss:0.000638
Epoch:25, Train loss:0.001024, valid loss:0.000648
Epoch:26, Train loss:0.001010, valid loss:0.000656
Epoch:27, Train loss:0.000990, valid loss:0.000657
Epoch:28, Train loss:0.000975, valid loss:0.000653
Epoch:29, Train loss:0.000978, valid loss:0.000630
Epoch:30, Train loss:0.000956, valid loss:0.000591
Epoch:31, Train loss:0.000872, valid loss:0.000593
Epoch:32, Train loss:0.000854, valid loss:0.000564
Epoch:33, Train loss:0.000853, valid loss:0.000613
Epoch:34, Train loss:0.000840, valid loss:0.000630
Epoch:35, Train loss:0.000839, valid loss:0.000585
Epoch:36, Train loss:0.000835, valid loss:0.000559
Epoch:37, Train loss:0.000829, valid loss:0.000569
Epoch:38, Train loss:0.000815, valid loss:0.000565
Epoch:39, Train loss:0.000822, valid loss:0.000596
Epoch:40, Train loss:0.000815, valid loss:0.000572
Epoch:41, Train loss:0.000761, valid loss:0.000539
Epoch:42, Train loss:0.000760, valid loss:0.000528
Epoch:43, Train loss:0.000752, valid loss:0.000528
Epoch:44, Train loss:0.000750, valid loss:0.000552
Epoch:45, Train loss:0.000750, valid loss:0.000538
Epoch:46, Train loss:0.000750, valid loss:0.000533
Epoch:47, Train loss:0.000746, valid loss:0.000540
Epoch:48, Train loss:0.000742, valid loss:0.000525
Epoch:49, Train loss:0.000733, valid loss:0.000542
Epoch:50, Train loss:0.000736, valid loss:0.000529
Epoch:51, Train loss:0.000702, valid loss:0.000528
Epoch:52, Train loss:0.000698, valid loss:0.000531
Epoch:53, Train loss:0.000698, valid loss:0.000531
Epoch:54, Train loss:0.000697, valid loss:0.000524
Epoch:55, Train loss:0.000697, valid loss:0.000523
Epoch:56, Train loss:0.000696, valid loss:0.000522
Epoch:57, Train loss:0.000696, valid loss:0.000521
Epoch:58, Train loss:0.000695, valid loss:0.000525
Epoch:59, Train loss:0.000695, valid loss:0.000525
Epoch:60, Train loss:0.000695, valid loss:0.000519
training time 12253.960482597351
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3405812533291066
plot_id,batch_id 0 1 miss% 0.33649060249551543
plot_id,batch_id 0 2 miss% 0.4212950884775294
plot_id,batch_id 0 3 miss% 0.2779723902468868
plot_id,batch_id 0 4 miss% 0.3389657631284219
plot_id,batch_id 0 5 miss% 0.3108497315670977
plot_id,batch_id 0 6 miss% 0.3219609932285687
plot_id,batch_id 0 7 miss% 0.4751407027780258
plot_id,batch_id 0 8 miss% 0.5355892811921997
plot_id,batch_id 0 9 miss% 0.560535111582768
plot_id,batch_id 0 10 miss% 0.3016235404430442
plot_id,batch_id 0 11 miss% 0.3842630699252675
plot_id,batch_id 0 12 miss% 0.4105941370532976
plot_id,batch_id 0 13 miss% 0.37652083860637764
plot_id,batch_id 0 14 miss% 0.4784473734966608
plot_id,batch_id 0 15 miss% 0.34254131197025217
plot_id,batch_id 0 16 miss% 0.49160841352429124
plot_id,batch_id 0 17 miss% 0.5053668191817092
plot_id,batch_id 0 18 miss% 0.5011144674022182
plot_id,batch_id 0 19 miss% 0.3882164772539098
plot_id,batch_id 0 20 miss% 0.33162835610395436
plot_id,batch_id 0 21 miss% 0.41826017058358167
plot_id,batch_id 0 22 miss% 0.39862484357093303
plot_id,batch_id 0 23 miss% 0.35606529643138773
plot_id,batch_id 0 24 miss% 0.31067120669410186
plot_id,batch_id 0 25 miss% 0.4982402988027083
plot_id,batch_id 0 26 miss% 0.4636616845878763
plot_id,batch_id 0 27 miss% 0.36082882127122956
plot_id,batch_id 0 28 miss% 0.43220916798764236
plot_id,batch_id 0 29 miss% 0.41358876246236453
plot_id,batch_id 0 30 miss% 0.39285534319890103
plot_id,batch_id 0 31 miss% 0.4756839940630018
plot_id,batch_id 0 32 miss% 0.5255546371306883
plot_id,batch_id 0 33 miss% 0.4861962465317114
plot_id,batch_id 0 34 miss% 0.443489836714276
plot_id,batch_id 0 35 miss% 0.3487136801825097
plot_id,batch_id 0 36 miss% 0.5188682693104462
plot_id,batch_id 0 37 miss% 0.5126414740137735
plot_id,batch_id 0 38 miss% 0.4904289287152309
plot_id,batch_id 0 39 miss% 0.5736368396979706
plot_id,batch_id 0 40 miss% 0.42125804971835923
plot_id,batch_id 0 41 miss% 0.41302414123101056
plot_id,batch_id 0 42 miss% 0.35159372280566004
plot_id,batch_id 0 43 miss% 0.2654189098223959
plot_id,batch_id 0 44 miss% 0.2892016366522301
plot_id,batch_id 0 45 miss% 0.38265269153069564
plot_id,batch_id 0 46 miss% 0.38177862861920386
plot_id,batch_id 0 47 miss% 0.46506535061755455
plot_id,batch_id 0 48 miss% 0.4252551695208632
plot_id,batch_id 0 49 miss% 0.34447965002006214
plot_id,batch_id 0 50 miss% 0.5589924692110592
plot_id,batch_id 0 51 miss% 0.5280409840835713
plot_id,batch_id 0 52 miss% 0.4767441725228894
plot_id,batch_id 0 53 miss% 0.37973519181436544
plot_id,batch_id 0 54 miss% 0.47750635294272237
plot_id,batch_id 0 55 miss% 0.5591084993846135
plot_id,batch_id 0 56 miss% 0.5489419779157433
plot_id,batch_id 0 57 miss% 0.5489642438950808
plot_id,batch_id 0 58 miss% 0.4692557447788583
plot_id,batch_id 0 59 miss% 0.47223948224326917
plot_id,batch_id 0 60 miss% 0.2993253507944071
plot_id,batch_id 0 61 miss% 0.26853009992195737
plot_id,batch_id 0 62 miss% 0.33869154859502154
plot_id,batch_id 0 63 miss% 0.39631861420407755
plot_id,batch_id 0 64 miss% 0.3613271741619263
plot_id,batch_id 0 65 miss% 0.30089334898781434
plot_id,batch_id 0 66 miss% 0.40724180312693586
plot_id,batch_id 0 67 miss% 0.2889063182861842
plot_id,batch_id 0 68 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  89105
Epoch:0, Train loss:0.423104, valid loss:0.381201
Epoch:1, Train loss:0.122509, valid loss:0.006461
Epoch:2, Train loss:0.012130, valid loss:0.003360
Epoch:3, Train loss:0.007070, valid loss:0.001946
Epoch:4, Train loss:0.004771, valid loss:0.001949
Epoch:5, Train loss:0.004278, valid loss:0.001851
Epoch:6, Train loss:0.003962, valid loss:0.001562
Epoch:7, Train loss:0.003771, valid loss:0.001531
Epoch:8, Train loss:0.003577, valid loss:0.001419
Epoch:9, Train loss:0.003311, valid loss:0.001343
Epoch:10, Train loss:0.002377, valid loss:0.001185
Epoch:11, Train loss:0.001891, valid loss:0.000891
Epoch:12, Train loss:0.001813, valid loss:0.000976
Epoch:13, Train loss:0.001729, valid loss:0.000793
Epoch:14, Train loss:0.001649, valid loss:0.000840
Epoch:15, Train loss:0.001590, valid loss:0.000767
Epoch:16, Train loss:0.001498, valid loss:0.000741
Epoch:17, Train loss:0.001434, valid loss:0.000804
Epoch:18, Train loss:0.001364, valid loss:0.000722
Epoch:19, Train loss:0.001323, valid loss:0.000687
Epoch:20, Train loss:0.001249, valid loss:0.000687
Epoch:21, Train loss:0.001020, valid loss:0.000571
Epoch:22, Train loss:0.000985, valid loss:0.000666
Epoch:23, Train loss:0.000962, valid loss:0.000560
Epoch:24, Train loss:0.000946, valid loss:0.000581
Epoch:25, Train loss:0.000918, valid loss:0.000588
Epoch:26, Train loss:0.000896, valid loss:0.000626
Epoch:27, Train loss:0.000887, valid loss:0.000570
Epoch:28, Train loss:0.000875, valid loss:0.000588
Epoch:29, Train loss:0.000865, valid loss:0.000571
Epoch:30, Train loss:0.000849, valid loss:0.000577
Epoch:31, Train loss:0.000715, valid loss:0.000511
Epoch:32, Train loss:0.000702, valid loss:0.000522
Epoch:33, Train loss:0.000693, valid loss:0.000506
Epoch:34, Train loss:0.000678, valid loss:0.000522
Epoch:35, Train loss:0.000675, valid loss:0.000521
Epoch:36, Train loss:0.000665, valid loss:0.000506
Epoch:37, Train loss:0.000653, valid loss:0.000501
Epoch:38, Train loss:0.000650, valid loss:0.000511
Epoch:39, Train loss:0.000652, valid loss:0.000484
Epoch:40, Train loss:0.000643, valid loss:0.000501
Epoch:41, Train loss:0.000576, valid loss:0.000483
Epoch:42, Train loss:0.000571, valid loss:0.000487
Epoch:43, Train loss:0.000569, valid loss:0.000485
Epoch:44, Train loss:0.000561, valid loss:0.000481
Epoch:45, Train loss:0.000561, valid loss:0.000456
Epoch:46, Train loss:0.000554, valid loss:0.000465
Epoch:47, Train loss:0.000548, valid loss:0.000505
Epoch:48, Train loss:0.000553, valid loss:0.000459
Epoch:49, Train loss:0.000542, valid loss:0.000465
Epoch:50, Train loss:0.000546, valid loss:0.000464
Epoch:51, Train loss:0.000504, valid loss:0.000461
Epoch:52, Train loss:0.000503, valid loss:0.000459
Epoch:53, Train loss:0.000502, valid loss:0.000458
Epoch:54, Train loss:0.000501, valid loss:0.000460
Epoch:55, Train loss:0.000500, valid loss:0.000458
Epoch:56, Train loss:0.000500, valid loss:0.000458
Epoch:57, Train loss:0.000499, valid loss:0.000457
Epoch:58, Train loss:0.000499, valid loss:0.000458
Epoch:59, Train loss:0.000498, valid loss:0.000457
Epoch:60, Train loss:0.000498, valid loss:0.000456
training time 12268.263126373291
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.3137815962468069
plot_id,batch_id 0 1 miss% 0.4266458785890263
plot_id,batch_id 0 2 miss% 0.5509502125066962
plot_id,batch_id 0 3 miss% 0.3746981958715174
plot_id,batch_id 0 4 miss% 0.3216154589243774
plot_id,batch_id 0 5 miss% 0.20581009147658227
plot_id,batch_id 0 6 miss% 0.48907969304650567
plot_id,batch_id 0 7 miss% 0.5432287555994936
plot_id,batch_id 0 8 miss% 0.4467437118858652
plot_id,batch_id 0 9 miss% 0.5334702300643399
plot_id,batch_id 0 10 miss% 0.46862702613974005
plot_id,batch_id 0 11 miss% 0.37321988786101346
plot_id,batch_id 0 12 miss% 0.633114349064368
plot_id,batch_id 0 13 miss% 0.4732158228108128
plot_id,batch_id 0 14 miss% 0.6284660732616877
plot_id,batch_id 0 15 miss% 0.3023381530686434
plot_id,batch_id 0 16 miss% 0.4380551040030053
plot_id,batch_id 0 17 miss% 0.5538667021578016
plot_id,batch_id 0 18 miss% 0.4317210779313828
plot_id,batch_id 0 19 miss% 0.5279053965049678
plot_id,batch_id 0 20 miss% 0.31437793282732895
plot_id,batch_id 0 21 miss% 0.5824442789236984
plot_id,batch_id 0 22 miss% 0.564480286670347
plot_id,batch_id 0 23 miss% 0.39949037613921484
plot_id,batch_id 0 24 miss% 0.30684053845688763
plot_id,batch_id 0 25 miss% 0.3704330571655142
plot_id,batch_id 0 26 miss% 0.45775451119681
plot_id,batch_id 0 27 miss% 0.4633140573360112
plot_id,batch_id 0 28 miss% 0.3701319165260025
plot_id,batch_id 0 29 miss% 0.2822716303691341
plot_id,batch_id 0 30 miss% 0.2081808606406902
plot_id,batch_id 0 31 miss% 0.5175114015999343
plot_id,batch_id 0 32 miss% 0.3928676652716298
plot_id,batch_id 0 33 miss% 0.4416673937190661
plot_id,batch_id 0 34 miss% 0.3293040274066593
plot_id,batch_id 0 35 miss% 0.31703557068056837
plot_id,batch_id 0 36 miss% 0.39228408190265757
plot_id,batch_id 0 37 miss% 0.5428835268460426
plot_id,batch_id 0 38 miss% 0.42425540981133003
plot_id,batch_id 0 39 miss% 0.5332618784072441
plot_id,batch_id 0 40 miss% 0.254295540421599
plot_id,batch_id 0 41 miss% 0.2856509442511819
plot_id,batch_id 0 42 miss% 0.13865792941830982
plot_id,batch_id 0 43 miss% 0.44160866679037697
plot_id,batch_id 0 44 miss% 0.3806572799000422
plot_id,batch_id 0 45 miss% 0.2671536046372497
plot_id,batch_id 0 46 miss% 0.42699438817026586
plot_id,batch_id 0 47 miss% 0.3427203011507431
plot_id,batch_id 0 48 miss% 0.35480523279762455
plot_id,batch_id 0 49 miss% 0.5400635905738203
plot_id,batch_id 0 50 miss% 0.3608102430505269
plot_id,batch_id 0 51 miss% 0.4228846805019332
plot_id,batch_id 0 52 miss% 0.2741093998257055
plot_id,batch_id 0 53 miss% 0.26929386455669313
plot_id,batch_id 0 54 miss% 0.28996289884882503
plot_id,batch_id 0 55 miss% 0.5974120836504883
plot_id,batch_id 0 56 miss% 0.5499192357456627
plot_id,batch_id 0 57 miss% 0.6412428084685826
plot_id,batch_id 0 58 miss% 0.4378448297121416
plot_id,batch_id 0 59 miss% 0.30514784997886796
plot_id,batch_id 0 60 miss% 0.19938495553891683
plot_id,batch_id 0 61 miss% 0.21446994953267054
plot_id,batch_id 0 62 miss% 0.3966617040607626
plot_id,batch_id 0 63 miss% 0.6519851431232595
plot_id,batch_id 0 64 miss% 0.4458662568686468
plot_id,batch_id 0 65 miss% 0.5081301567356794
plot_id,batch_id 0 66 miss% plot_id,batch_id 0 68 miss% 0.060609204596874806
plot_id,batch_id 0 69 miss% 0.0861018067169634
plot_id,batch_id 0 70 miss% 0.08460045050027579
plot_id,batch_id 0 71 miss% 0.04429777674549404
plot_id,batch_id 0 72 miss% 0.12498661273004488
plot_id,batch_id 0 73 miss% 0.0577863998444235
plot_id,batch_id 0 74 miss% 0.07689792498860086
plot_id,batch_id 0 75 miss% 0.07502686215227931
plot_id,batch_id 0 76 miss% 0.057188844242637454
plot_id,batch_id 0 77 miss% 0.05470212964962245
plot_id,batch_id 0 78 miss% 0.03154700681293219
plot_id,batch_id 0 79 miss% 0.041368154655052024
plot_id,batch_id 0 80 miss% 0.03453233548002288
plot_id,batch_id 0 81 miss% 0.10297890939150531
plot_id,batch_id 0 82 miss% 0.041444225532192264
plot_id,batch_id 0 83 miss% 0.06810317854298653
plot_id,batch_id 0 84 miss% 0.07842776676263692
plot_id,batch_id 0 85 miss% 0.029875809949719414
plot_id,batch_id 0 86 miss% 0.04631777973463267
plot_id,batch_id 0 87 miss% 0.066060965731799
plot_id,batch_id 0 88 miss% 0.06243025326934232
plot_id,batch_id 0 89 miss% 0.08207891107303453
plot_id,batch_id 0 90 miss% 0.03582170318047146
plot_id,batch_id 0 91 miss% 0.11425734909769514
plot_id,batch_id 0 92 miss% 0.051763035543204515
plot_id,batch_id 0 93 miss% 0.06651761027980219
plot_id,batch_id 0 94 miss% 0.0932754111817351
plot_id,batch_id 0 95 miss% 0.03419060633308897
plot_id,batch_id 0 96 miss% 0.06877792811934906
plot_id,batch_id 0 97 miss% 0.045385400302570336
plot_id,batch_id 0 98 miss% 0.03121269552123335
plot_id,batch_id 0 99 miss% 0.05461495771261962
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08009281 0.08351292 0.07926776 0.08765632 0.08576377 0.05322115
 0.05163575 0.09777826 0.10342775 0.04756455 0.0509245  0.10665441
 0.06034089 0.0445818  0.11567752 0.03690063 0.17150969 0.05491007
 0.06653021 0.09872136 0.05464336 0.05924073 0.07310853 0.03143323
 0.09821581 0.09876632 0.04495222 0.03068177 0.04360981 0.02878082
 0.05580906 0.11580154 0.0982737  0.06662056 0.03817181 0.05954974
 0.08882335 0.0842617  0.08124026 0.03184989 0.10474498 0.07364642
 0.03468303 0.0745575  0.02179087 0.08557768 0.03963423 0.03634345
 0.03178375 0.03145027 0.09490546 0.03653333 0.03027093 0.02620245
 0.04872398 0.04669141 0.07376269 0.07256748 0.0356461  0.03392016
 0.0463598  0.03769085 0.08178977 0.0905169  0.05395992 0.13865843
 0.04207077 0.0476875  0.0606092  0.08610181 0.08460045 0.04429778
 0.12498661 0.0577864  0.07689792 0.07502686 0.05718884 0.05470213
 0.03154701 0.04136815 0.03453234 0.10297891 0.04144423 0.06810318
 0.07842777 0.02987581 0.04631778 0.06606097 0.06243025 0.08207891
 0.0358217  0.11425735 0.05176304 0.06651761 0.09327541 0.03419061
 0.06877793 0.0453854  0.0312127  0.05461496]
for model  118 the mean error 0.06435856412023312
all id 118 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 25 out win 5 err 0.06435856412023312 time 12218.617328166962
Launcher: Job 119 completed in 12483 seconds.
Launcher: Task 33 done. Exiting.
0.41626462179317897
plot_id,batch_id 0 69 miss% 0.4314121681469856
plot_id,batch_id 0 70 miss% 0.2662532466852013
plot_id,batch_id 0 71 miss% 0.34357866469280796
plot_id,batch_id 0 72 miss% 0.4567251113182985
plot_id,batch_id 0 73 miss% 0.3745949537778421
plot_id,batch_id 0 74 miss% 0.3518878189222307
plot_id,batch_id 0 75 miss% 0.3477258196376766
plot_id,batch_id 0 76 miss% 0.35041608251670964
plot_id,batch_id 0 77 miss% 0.367517433234459
plot_id,batch_id 0 78 miss% 0.3546221314530617
plot_id,batch_id 0 79 miss% 0.370539304652706
plot_id,batch_id 0 80 miss% 0.33388831458754575
plot_id,batch_id 0 81 miss% 0.3942963043916972
plot_id,batch_id 0 82 miss% 0.31724056447604015
plot_id,batch_id 0 83 miss% 0.4068301655124189
plot_id,batch_id 0 84 miss% 0.384988294800066
plot_id,batch_id 0 85 miss% 0.27891460660670636
plot_id,batch_id 0 86 miss% 0.31881010248259095
plot_id,batch_id 0 87 miss% 0.36479523016862087
plot_id,batch_id 0 88 miss% 0.37740146797201235
plot_id,batch_id 0 89 miss% 0.3569354928101268
plot_id,batch_id 0 90 miss% 0.23322443873864815
plot_id,batch_id 0 91 miss% 0.31103408137611066
plot_id,batch_id 0 92 miss% 0.2874015820969667
plot_id,batch_id 0 93 miss% 0.30170346952008453
plot_id,batch_id 0 94 miss% 0.46521714687066473
plot_id,batch_id 0 95 miss% 0.3050255420178784
plot_id,batch_id 0 96 miss% 0.3333664328042674
plot_id,batch_id 0 97 miss% 0.41705503838737645
plot_id,batch_id 0 98 miss% 0.4427932721204807
plot_id,batch_id 0 99 miss% 0.4645805227999079
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.34058125 0.3364906  0.42129509 0.27797239 0.33896576 0.31084973
 0.32196099 0.4751407  0.53558928 0.56053511 0.30162354 0.38426307
 0.41059414 0.37652084 0.47844737 0.34254131 0.49160841 0.50536682
 0.50111447 0.38821648 0.33162836 0.41826017 0.39862484 0.3560653
 0.31067121 0.4982403  0.46366168 0.36082882 0.43220917 0.41358876
 0.39285534 0.47568399 0.52555464 0.48619625 0.44348984 0.34871368
 0.51886827 0.51264147 0.49042893 0.57363684 0.42125805 0.41302414
 0.35159372 0.26541891 0.28920164 0.38265269 0.38177863 0.46506535
 0.42525517 0.34447965 0.55899247 0.52804098 0.47674417 0.37973519
 0.47750635 0.5591085  0.54894198 0.54896424 0.46925574 0.47223948
 0.29932535 0.2685301  0.33869155 0.39631861 0.36132717 0.30089335
 0.4072418  0.28890632 0.41626462 0.43141217 0.26625325 0.34357866
 0.45672511 0.37459495 0.35188782 0.34772582 0.35041608 0.36751743
 0.35462213 0.3705393  0.33388831 0.3942963  0.31724056 0.40683017
 0.38498829 0.27891461 0.3188101  0.36479523 0.37740147 0.35693549
 0.23322444 0.31103408 0.28740158 0.30170347 0.46521715 0.30502554
 0.33336643 0.41705504 0.44279327 0.46458052]
for model  164 the mean error 0.39799055979717335
all id 164 hidden_dim 16 learning_rate 0.0025 num_layers 3 frames 31 out win 6 err 0.39799055979717335 time 12253.960482597351
Launcher: Job 165 completed in 12488 seconds.
Launcher: Task 202 done. Exiting.
0.28501265804231285
plot_id,batch_id 0 67 miss% 0.3021955226777046
plot_id,batch_id 0 68 miss% 0.4477596652705336
plot_id,batch_id 0 69 miss% 0.5619859251387989
plot_id,batch_id 0 70 miss% 0.37127436133962505
plot_id,batch_id 0 71 miss% 0.4160015470623396
plot_id,batch_id 0 72 miss% 0.4436083291079494
plot_id,batch_id 0 73 miss% 0.5447864594267002
plot_id,batch_id 0 74 miss% 0.44474195329582583
plot_id,batch_id 0 75 miss% 0.11487760252661221
plot_id,batch_id 0 76 miss% 0.3430553986130562
plot_id,batch_id 0 77 miss% 0.2324251219496133
plot_id,batch_id 0 78 miss% 0.23946188389044734
plot_id,batch_id 0 79 miss% 0.3699743791430186
plot_id,batch_id 0 80 miss% 0.22824515387008734
plot_id,batch_id 0 81 miss% 0.357100486596374
plot_id,batch_id 0 82 miss% 0.6143792355619463
plot_id,batch_id 0 83 miss% 0.5024266488561067
plot_id,batch_id 0 84 miss% 0.5240407247548802
plot_id,batch_id 0 85 miss% 0.30084230899643055
plot_id,batch_id 0 86 miss% 0.2762230739276575
plot_id,batch_id 0 87 miss% 0.37233390497257496
plot_id,batch_id 0 88 miss% 0.43613373006710876
plot_id,batch_id 0 89 miss% 0.5315852562007235
plot_id,batch_id 0 90 miss% 0.20505084600174006
plot_id,batch_id 0 91 miss% 0.5338785337614992
plot_id,batch_id 0 92 miss% 0.5408790049602196
plot_id,batch_id 0 93 miss% 0.2927019017806755
plot_id,batch_id 0 94 miss% 0.3754907739860024
plot_id,batch_id 0 95 miss% 0.25866585819988575
plot_id,batch_id 0 96 miss% 0.3927234860261075
plot_id,batch_id 0 97 miss% 0.5700513809275012
plot_id,batch_id 0 98 miss% 0.6385123954066224
plot_id,batch_id 0 99 miss% 0.32943132004780074
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.3137816  0.42664588 0.55095021 0.3746982  0.32161546 0.20581009
 0.48907969 0.54322876 0.44674371 0.53347023 0.46862703 0.37321989
 0.63311435 0.47321582 0.62846607 0.30233815 0.4380551  0.5538667
 0.43172108 0.5279054  0.31437793 0.58244428 0.56448029 0.39949038
 0.30684054 0.37043306 0.45775451 0.46331406 0.37013192 0.28227163
 0.20818086 0.5175114  0.39286767 0.44166739 0.32930403 0.31703557
 0.39228408 0.54288353 0.42425541 0.53326188 0.25429554 0.28565094
 0.13865793 0.44160867 0.38065728 0.2671536  0.42699439 0.3427203
 0.35480523 0.54006359 0.36081024 0.42288468 0.2741094  0.26929386
 0.2899629  0.59741208 0.54991924 0.64124281 0.43784483 0.30514785
 0.19938496 0.21446995 0.3966617  0.65198514 0.44586626 0.50813016
 0.28501266 0.30219552 0.44775967 0.56198593 0.37127436 0.41600155
 0.44360833 0.54478646 0.44474195 0.1148776  0.3430554  0.23242512
 0.23946188 0.36997438 0.22824515 0.35710049 0.61437924 0.50242665
 0.52404072 0.30084231 0.27622307 0.3723339  0.43613373 0.53158526
 0.20505085 0.53387853 0.540879   0.2927019  0.37549077 0.25866586
 0.39272349 0.57005138 0.6385124  0.32943132]
for model  219 the mean error 0.4064293418960847
all id 219 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 31 out win 4 err 0.4064293418960847 time 12268.263126373291
Launcher: Job 220 completed in 12515 seconds.
Launcher: Task 147 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  89105
Epoch:0, Train loss:0.515416, valid loss:0.475198
Epoch:1, Train loss:0.120949, valid loss:0.003707
Epoch:2, Train loss:0.010525, valid loss:0.003284
Epoch:3, Train loss:0.008132, valid loss:0.002646
Epoch:4, Train loss:0.006908, valid loss:0.002296
Epoch:5, Train loss:0.005642, valid loss:0.002245
Epoch:6, Train loss:0.004991, valid loss:0.001676
Epoch:7, Train loss:0.003671, valid loss:0.001763
Epoch:8, Train loss:0.003374, valid loss:0.001787
Epoch:9, Train loss:0.003201, valid loss:0.001731
Epoch:10, Train loss:0.003018, valid loss:0.001511
Epoch:11, Train loss:0.002313, valid loss:0.001168
Epoch:12, Train loss:0.002236, valid loss:0.001224
Epoch:13, Train loss:0.002172, valid loss:0.001058
Epoch:14, Train loss:0.002035, valid loss:0.001104
Epoch:15, Train loss:0.002112, valid loss:0.001058
Epoch:16, Train loss:0.001927, valid loss:0.001273
Epoch:17, Train loss:0.001951, valid loss:0.001140
Epoch:18, Train loss:0.001735, valid loss:0.000916
Epoch:19, Train loss:0.001779, valid loss:0.000946
Epoch:20, Train loss:0.001714, valid loss:0.001056
Epoch:21, Train loss:0.001324, valid loss:0.000806
Epoch:22, Train loss:0.001258, valid loss:0.000759
Epoch:23, Train loss:0.001236, valid loss:0.000850
Epoch:24, Train loss:0.001232, valid loss:0.000776
Epoch:25, Train loss:0.001187, valid loss:0.000871
Epoch:26, Train loss:0.001201, valid loss:0.000844
Epoch:27, Train loss:0.001171, valid loss:0.000816
Epoch:28, Train loss:0.001099, valid loss:0.000737
Epoch:29, Train loss:0.001094, valid loss:0.000809
Epoch:30, Train loss:0.001106, valid loss:0.000821
Epoch:31, Train loss:0.000905, valid loss:0.000720
Epoch:32, Train loss:0.000855, valid loss:0.000642
Epoch:33, Train loss:0.000845, valid loss:0.000659
Epoch:34, Train loss:0.000847, valid loss:0.000703
Epoch:35, Train loss:0.000844, valid loss:0.000668
Epoch:36, Train loss:0.000819, valid loss:0.000659
Epoch:37, Train loss:0.000820, valid loss:0.000609
Epoch:38, Train loss:0.000789, valid loss:0.000645
Epoch:39, Train loss:0.000794, valid loss:0.000607
Epoch:40, Train loss:0.000797, valid loss:0.000676
Epoch:41, Train loss:0.000684, valid loss:0.000621
Epoch:42, Train loss:0.000671, valid loss:0.000637
Epoch:43, Train loss:0.000662, valid loss:0.000629
Epoch:44, Train loss:0.000667, valid loss:0.000652
Epoch:45, Train loss:0.000677, valid loss:0.000622
Epoch:46, Train loss:0.000659, valid loss:0.000637
Epoch:47, Train loss:0.000646, valid loss:0.000650
Epoch:48, Train loss:0.000653, valid loss:0.000625
Epoch:49, Train loss:0.000637, valid loss:0.000610
Epoch:50, Train loss:0.000646, valid loss:0.000633
Epoch:51, Train loss:0.000580, valid loss:0.000604
Epoch:52, Train loss:0.000576, valid loss:0.000597
Epoch:53, Train loss:0.000573, valid loss:0.000597
Epoch:54, Train loss:0.000572, valid loss:0.000598
Epoch:55, Train loss:0.000570, valid loss:0.000603
Epoch:56, Train loss:0.000570, valid loss:0.000596
Epoch:57, Train loss:0.000569, valid loss:0.000598
Epoch:58, Train loss:0.000568, valid loss:0.000598
Epoch:59, Train loss:0.000568, valid loss:0.000595
Epoch:60, Train loss:0.000567, valid loss:0.000593
training time 12473.590178012848
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.35664716858072154
plot_id,batch_id 0 1 miss% 0.3494767398231241
plot_id,batch_id 0 2 miss% 0.3927156578167456
plot_id,batch_id 0 3 miss% 0.37353563872531625
plot_id,batch_id 0 4 miss% 0.32000382810043915
plot_id,batch_id 0 5 miss% 0.41810625383032785
plot_id,batch_id 0 6 miss% 0.37521759313617714
plot_id,batch_id 0 7 miss% 0.46141727983112746
plot_id,batch_id 0 8 miss% 0.3947541887464196
plot_id,batch_id 0 9 miss% 0.32974801945043186
plot_id,batch_id 0 10 miss% 0.3661315238214842
plot_id,batch_id 0 11 miss% 0.4333853636632919
plot_id,batch_id 0 12 miss% 0.4113706923271197
plot_id,batch_id 0 13 miss% 0.4051882027481374
plot_id,batch_id 0 14 miss% 0.38347105109244095
plot_id,batch_id 0 15 miss% 0.3441407091942119
plot_id,batch_id 0 16 miss% 0.5312755667623421
plot_id,batch_id 0 17 miss% 0.4597716583810779
plot_id,batch_id 0 18 miss% 0.4227853442106854
plot_id,batch_id 0 19 miss% 0.4348967429305388
plot_id,batch_id 0 20 miss% 0.33072235545330564
plot_id,batch_id 0 21 miss% 0.35806771549327765
plot_id,batch_id 0 22 miss% 0.4075811100911684
plot_id,batch_id 0 23 miss% 0.39358487720182117
plot_id,batch_id 0 24 miss% 0.27694798157614237
plot_id,batch_id 0 25 miss% 0.4301974385353952
plot_id,batch_id 0 26 miss% 0.381216368518556
plot_id,batch_id 0 27 miss% 0.3301943413375445
plot_id,batch_id 0 28 miss% 0.4071256812106469
plot_id,batch_id 0 29 miss% 0.34443281392286296
plot_id,batch_id 0 30 miss% 0.41556322969892723
plot_id,batch_id 0 31 miss% 0.394996504395894
plot_id,batch_id 0 32 miss% 0.3803651345877615
plot_id,batch_id 0 33 miss% 0.5259484604311279
plot_id,batch_id 0 34 miss% 0.5029475694481945
plot_id,batch_id 0 35 miss% 0.44067872413040315
plot_id,batch_id 0 36 miss% 0.4759087607045592
plot_id,batch_id 0 37 miss% 0.3981393281524279
plot_id,batch_id 0 38 miss% 0.4243318545594685
plot_id,batch_id 0 39 miss% 0.35561199166447266
plot_id,batch_id 0 40 miss% 0.31088106950705824
plot_id,batch_id 0 41 miss% 0.3064010653676234
plot_id,batch_id 0 42 miss% 0.2791332698024742
plot_id,batch_id 0 43 miss% 0.2932957946442833
plot_id,batch_id 0 44 miss% 0.3357603967670839
plot_id,batch_id 0 45 miss% 0.41632805576503024
plot_id,batch_id 0 46 miss% 0.41552787324806983
plot_id,batch_id 0 47 miss% 0.38345933131652726
plot_id,batch_id 0 48 miss% 0.3685921029915471
plot_id,batch_id 0 49 miss% 0.34918656268153814
plot_id,batch_id 0 50 miss% 0.42981420076521276
plot_id,batch_id 0 51 miss% 0.4314232434224043
plot_id,batch_id 0 52 miss% 0.3881032610221071
plot_id,batch_id 0 53 miss% 0.3847771229997151
plot_id,batch_id 0 54 miss% 0.514955016845559
plot_id,batch_id 0 55 miss% 0.43902610983364226
plot_id,batch_id 0 56 miss% 0.401899997822878
plot_id,batch_id 0 57 miss% 0.38799005496433675
plot_id,batch_id 0 58 miss% 0.36084531574007367
plot_id,batch_id 0 59 miss% 0.41370225015580236
plot_id,batch_id 0 60 miss% 0.2974819857901053
plot_id,batch_id 0 61 miss% 0.33430420436724195
plot_id,batch_id 0 62 miss% 0.37620063293626804
plot_id,batch_id 0 63 miss% 0.402749013248666
plot_id,batch_id 0 64 miss% 0.4266893277376938
plot_id,batch_id 0 65 miss% 0.4108454785780334
plot_id,batch_id 0 66 miss% 0.4335913952678803
plot_id,batch_id 0 67 miss% 0.35424831580332133
plot_id,batch_id 0 68 miss% 0.4726038077553757
plot_id,batch_id 0 69 miss% 0.4422331381347238
plot_id,batch_id the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  55697
Epoch:0, Train loss:0.573817, valid loss:0.538962
Epoch:1, Train loss:0.369173, valid loss:0.365315
Epoch:2, Train loss:0.360258, valid loss:0.365034
Epoch:3, Train loss:0.359496, valid loss:0.364786
Epoch:4, Train loss:0.359071, valid loss:0.364598
Epoch:5, Train loss:0.358850, valid loss:0.364622
Epoch:6, Train loss:0.358628, valid loss:0.364716
Epoch:7, Train loss:0.358465, valid loss:0.364446
Epoch:8, Train loss:0.358324, valid loss:0.364406
Epoch:9, Train loss:0.358196, valid loss:0.364296
Epoch:10, Train loss:0.358122, valid loss:0.364434
Epoch:11, Train loss:0.357507, valid loss:0.364123
Epoch:12, Train loss:0.357577, valid loss:0.364057
Epoch:13, Train loss:0.357402, valid loss:0.363993
Epoch:14, Train loss:0.357341, valid loss:0.363923
Epoch:15, Train loss:0.357307, valid loss:0.363954
Epoch:16, Train loss:0.357332, valid loss:0.364018
Epoch:17, Train loss:0.357252, valid loss:0.364058
Epoch:18, Train loss:0.357210, valid loss:0.363877
Epoch:19, Train loss:0.357260, valid loss:0.363954
Epoch:20, Train loss:0.357200, valid loss:0.363971
Epoch:21, Train loss:0.356905, valid loss:0.363763
Epoch:22, Train loss:0.356871, valid loss:0.363748
Epoch:23, Train loss:0.356854, valid loss:0.363886
Epoch:24, Train loss:0.356854, valid loss:0.363867
Epoch:25, Train loss:0.356844, valid loss:0.363836
Epoch:26, Train loss:0.356805, valid loss:0.363768
Epoch:27, Train loss:0.356807, valid loss:0.363767
Epoch:28, Train loss:0.356790, valid loss:0.363820
Epoch:29, Train loss:0.356790, valid loss:0.363867
Epoch:30, Train loss:0.356784, valid loss:0.363726
Epoch:31, Train loss:0.356635, valid loss:0.363730
Epoch:32, Train loss:0.356608, valid loss:0.363714
Epoch:33, Train loss:0.356599, valid loss:0.363773
Epoch:34, Train loss:0.356613, valid loss:0.363719
Epoch:35, Train loss:0.356595, valid loss:0.363681
Epoch:36, Train loss:0.356577, valid loss:0.363697
Epoch:37, Train loss:0.356574, valid loss:0.363701
Epoch:38, Train loss:0.356586, valid loss:0.363712
Epoch:39, Train loss:0.356584, valid loss:0.363716
Epoch:40, Train loss:0.356554, valid loss:0.363713
Epoch:41, Train loss:0.356495, valid loss:0.363713
Epoch:42, Train loss:0.356485, valid loss:0.363708
Epoch:43, Train loss:0.356485, valid loss:0.363687
Epoch:44, Train loss:0.356475, valid loss:0.363677
Epoch:45, Train loss:0.356471, valid loss:0.363682
Epoch:46, Train loss:0.356468, valid loss:0.363694
Epoch:47, Train loss:0.356466, valid loss:0.363691
Epoch:48, Train loss:0.356464, valid loss:0.363695
Epoch:49, Train loss:0.356459, valid loss:0.363687
Epoch:50, Train loss:0.356459, valid loss:0.363666
Epoch:51, Train loss:0.356425, valid loss:0.363661
Epoch:52, Train loss:0.356420, valid loss:0.363661
Epoch:53, Train loss:0.356418, valid loss:0.363660
Epoch:54, Train loss:0.356416, valid loss:0.363664
Epoch:55, Train loss:0.356415, valid loss:0.363660
Epoch:56, Train loss:0.356414, valid loss:0.363658
Epoch:57, Train loss:0.356414, valid loss:0.363664
Epoch:58, Train loss:0.356413, valid loss:0.363665
Epoch:59, Train loss:0.356413, valid loss:0.363663
Epoch:60, Train loss:0.356412, valid loss:0.363659
training time 12571.016433477402
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.853193390153448
plot_id,batch_id 0 1 miss% 0.8247538244829664
plot_id,batch_id 0 2 miss% 0.8217327813430179
plot_id,batch_id 0 3 miss% 0.815827734998257
plot_id,batch_id 0 4 miss% 0.8137329863031553
plot_id,batch_id 0 5 miss% 0.8539280835685649
plot_id,batch_id 0 6 miss% 0.8242828120538724
plot_id,batch_id 0 7 miss% 0.8220918773526086
plot_id,batch_id 0 8 miss% 0.8191005986074688
plot_id,batch_id 0 9 miss% 0.8173990937864222
plot_id,batch_id 0 10 miss% 0.8588530437028082
plot_id,batch_id 0 11 miss% 0.8268184452551745
plot_id,batch_id 0 12 miss% 0.8191943129144375
plot_id,batch_id 0 13 miss% 0.8151754574289096
plot_id,batch_id 0 14 miss% 0.8163352221404429
plot_id,batch_id 0 15 miss% 0.8761076009064487
plot_id,batch_id 0 16 miss% 0.8219764927409821
plot_id,batch_id 0 17 miss% 0.8249280600819471
plot_id,batch_id 0 18 miss% 0.8185757174606342
plot_id,batch_id 0 19 miss% 0.8216622819633935
plot_id,batch_id 0 20 miss% 0.8402186865154737
plot_id,batch_id 0 21 miss% 0.8216800688925607
plot_id,batch_id 0 22 miss% 0.8185685187421263
plot_id,batch_id 0 23 miss% 0.8124960084152816
plot_id,batch_id 0 24 miss% 0.8123880276922737
plot_id,batch_id 0 25 miss% 0.835656091361218
plot_id,batch_id 0 26 miss% 0.8216182818335815
plot_id,batch_id 0 27 miss% 0.816852722844461
plot_id,batch_id 0 28 miss% 0.8125995539428394
plot_id,batch_id 0 29 miss% 0.8137264752950366
plot_id,batch_id 0 30 miss% 0.8374805760659354
plot_id,batch_id 0 31 miss% 0.8196958364694367
plot_id,batch_id 0 32 miss% 0.8190847569882321
plot_id,batch_id 0 33 miss% 0.8128832236988962
plot_id,batch_id 0 34 miss% 0.8126624037868294
plot_id,batch_id 0 35 miss% 0.8374208408425651
plot_id,batch_id 0 36 miss% 0.8217952368973123
plot_id,batch_id 0 37 miss% 0.8147248125784262
plot_id,batch_id 0 38 miss% 0.814003426023696
plot_id,batch_id 0 39 miss% 0.8130436783282116
plot_id,batch_id 0 40 miss% 0.8341473920371429
plot_id,batch_id 0 41 miss% 0.814584708378146
plot_id,batch_id 0 42 miss% 0.8113972138214708
plot_id,batch_id 0 43 miss% 0.8082219042738038
plot_id,batch_id 0 44 miss% 0.8108294383832494
plot_id,batch_id 0 45 miss% 0.8241953032050496
plot_id,batch_id 0 46 miss% 0.8153766029213028
plot_id,batch_id 0 47 miss% 0.8146107678848653
plot_id,batch_id 0 48 miss% 0.8125143149503202
plot_id,batch_id 0 49 miss% 0.808262185049074
plot_id,batch_id 0 50 miss% 0.8281658497677874
plot_id,batch_id 0 51 miss% 0.8170538337760291
plot_id,batch_id 0 52 miss% 0.8149893658800423
plot_id,batch_id 0 53 miss% 0.8118432863348122
plot_id,batch_id 0 54 miss% 0.8081584235049012
plot_id,batch_id 0 55 miss% 0.8286380357748436
plot_id,batch_id 0 56 miss% 0.8186024079532662
plot_id,batch_id 0 57 miss% 0.8152403260771496
plot_id,batch_id 0 58 miss% 0.8114670693848789
plot_id,batch_id 0 59 miss% 0.8137309235040202
plot_id,batch_id 0 60 miss% 0.8878381394383984
plot_id,batch_id 0 61 miss% 0.8420530467742413
plot_id,batch_id 0 62 miss% 0.8319594417747638
plot_id,batch_id 0 63 miss% 0.8254252031879614
plot_id,batch_id 0 64 miss% 0.8243527806748162
plot_id,batch_id 0 65 miss% 0.887448033407538
plot_id,batch_id 0 66 miss% 0.8489923132644
plot_id,batch_id 0 67 miss% 0.8348016625933197
plot_id,batch_id 0 68 miss% 0.8243844878650928
plot_id,batch_id 0 69 miss% 0.8192276540609295
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  151697
Epoch:0, Train loss:0.707612, valid loss:0.660109
Epoch:1, Train loss:0.529748, valid loss:0.525488
Epoch:2, Train loss:0.516788, valid loss:0.524951
Epoch:3, Train loss:0.515449, valid loss:0.524350
Epoch:4, Train loss:0.514646, valid loss:0.524210
Epoch:5, Train loss:0.514339, valid loss:0.524269
Epoch:6, Train loss:0.513817, valid loss:0.523940
Epoch:7, Train loss:0.513651, valid loss:0.524423
Epoch:8, Train loss:0.513394, valid loss:0.523767
Epoch:9, Train loss:0.513086, valid loss:0.523712
Epoch:10, Train loss:0.512916, valid loss:0.523586
Epoch:11, Train loss:0.512285, valid loss:0.523316
Epoch:12, Train loss:0.512203, valid loss:0.523350
Epoch:13, Train loss:0.512208, valid loss:0.523525
Epoch:14, Train loss:0.512116, valid loss:0.523254
Epoch:15, Train loss:0.512043, valid loss:0.523221
Epoch:16, Train loss:0.512051, valid loss:0.523251
Epoch:17, Train loss:0.511949, valid loss:0.523286
Epoch:18, Train loss:0.511893, valid loss:0.523238
Epoch:19, Train loss:0.511898, valid loss:0.523141
Epoch:20, Train loss:0.511859, valid loss:0.523187
Epoch:21, Train loss:0.511536, valid loss:0.523050
Epoch:22, Train loss:0.511497, valid loss:0.523031
Epoch:23, Train loss:0.511511, valid loss:0.523103
Epoch:24, Train loss:0.511487, valid loss:0.523077
Epoch:25, Train loss:0.511454, valid loss:0.523055
Epoch:26, Train loss:0.511440, valid loss:0.523092
Epoch:27, Train loss:0.511443, valid loss:0.523020
Epoch:28, Train loss:0.511420, valid loss:0.523130
Epoch:29, Train loss:0.511408, valid loss:0.523064
Epoch:30, Train loss:0.511428, valid loss:0.523117
Epoch:31, Train loss:0.511270, valid loss:0.522959
Epoch:32, Train loss:0.511256, valid loss:0.522981
Epoch:33, Train loss:0.511239, valid loss:0.522970
Epoch:34, Train loss:0.511235, valid loss:0.522967
Epoch:35, Train loss:0.511232, valid loss:0.522985
Epoch:36, Train loss:0.511222, valid loss:0.523042
Epoch:37, Train loss:0.511227, valid loss:0.522978
Epoch:38, Train loss:0.511204, valid loss:0.523001
Epoch:39, Train loss:0.511202, valid loss:0.522965
Epoch:40, Train loss:0.511212, valid loss:0.522976
Epoch:41, Train loss:0.511135, valid loss:0.522936
Epoch:42, Train loss:0.511127, valid loss:0.522919
Epoch:43, Train loss:0.511128, valid loss:0.522969
Epoch:44, Train loss:0.511122, valid loss:0.522943
Epoch:45, Train loss:0.511121, valid loss:0.522930
Epoch:46, Train loss:0.511126, valid loss:0.522966
Epoch:47, Train loss:0.511117, valid loss:0.522939
Epoch:48, Train loss:0.511111, valid loss:0.522959
Epoch:49, Train loss:0.511104, valid loss:0.522975
Epoch:50, Train loss:0.511104, valid loss:0.522931
Epoch:51, Train loss:0.511076, valid loss:0.522940
Epoch:52, Train loss:0.511073, valid loss:0.522931
Epoch:53, Train loss:0.511071, valid loss:0.522930
Epoch:54, Train loss:0.511071, valid loss:0.522929
Epoch:55, Train loss:0.511070, valid loss:0.522930
Epoch:56, Train loss:0.511069, valid loss:0.522933
Epoch:57, Train loss:0.511069, valid loss:0.522930
Epoch:58, Train loss:0.511068, valid loss:0.522927
Epoch:59, Train loss:0.511068, valid loss:0.522933
Epoch:60, Train loss:0.511068, valid loss:0.522930
training time 12577.639124155045
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.8681261653048955
plot_id,batch_id 0 1 miss% 0.8392345456984814
plot_id,batch_id 0 2 miss% 0.8339433764498058
plot_id,batch_id 0 3 miss% 0.8297380060045384
plot_id,batch_id 0 4 miss% 0.8303407175874701
plot_id,batch_id 0 5 miss% 0.8713873563915242
plot_id,batch_id 0 6 miss% 0.8421320263165188
plot_id,batch_id 0 7 miss% 0.8356626696526286
plot_id,batch_id 0 8 miss% 0.8308133199058922
plot_id,batch_id 0 9 miss% 0.8312258456339461
plot_id,batch_id 0 10 miss% 0.872100348460447
plot_id,batch_id 0 11 miss% 0.8400773627006554
plot_id,batch_id 0 12 miss% 0.8340127005132673
plot_id,batch_id 0 13 miss% 0.8283997095375677
plot_id,batch_id 0 14 miss% 0.8295051642520074
plot_id,batch_id 0 15 miss% 0.8722168001671374
plot_id,batch_id 0 16 miss% 0.8388927400036502
plot_id,batch_id 0 17 miss% 0.8315449150471247
plot_id,batch_id 0 18 miss% 0.8322734450136885
plot_id,batch_id 0 19 miss% 0.8312586467084954
plot_id,batch_id 0 20 miss% 0.8505924119145667
plot_id,batch_id 0 21 miss% 0.8342226631305468
plot_id,batch_id 0 22 miss% 0.8295417240930314
plot_id,batch_id 0 23 miss% 0.8275550413888358
plot_id,batch_id 0 24 miss% 0.8257981580737074
plot_id,batch_id 0 25 miss% 0.8513097768123625
plot_id,batch_id 0 26 miss% 0.8388636588430758
plot_id,batch_id 0 27 miss% 0.8303630199725371
plot_id,batch_id 0 28 miss% 0.8299676328715309
plot_id,batch_id 0 29 miss% 0.8264802841532035
plot_id,batch_id 0 30 miss% 0.8538513334613103
plot_id,batch_id 0 31 miss% 0.8366063606598774
plot_id,batch_id 0 32 miss% 0.8351142873029943
plot_id,batch_id 0 33 miss% 0.8305991052915689
plot_id,batch_id 0 34 miss% 0.8281192887420132
plot_id,batch_id 0 35 miss% 0.8580632948980412
plot_id,batch_id 0 36 miss% 0.8345799608873193
plot_id,batch_id 0 37 miss% 0.8303855580411629
plot_id,batch_id 0 38 miss% 0.8316309274238416
plot_id,batch_id 0 39 miss% 0.8283269976146878
plot_id,batch_id 0 40 miss% 0.8413004473098943
plot_id,batch_id 0 41 miss% 0.8321661592906395
plot_id,batch_id 0 42 miss% 0.8258541360095909
plot_id,batch_id 0 43 miss% 0.8258072538564153
plot_id,batch_id 0 44 miss% 0.8258396740846311
plot_id,batch_id 0 45 miss% 0.8448954784840003
plot_id,batch_id 0 46 miss% 0.8323803384028018
plot_id,batch_id 0 47 miss% 0.829365558784883
plot_id,batch_id 0 48 miss% 0.8272625730990204
plot_id,batch_id 0 49 miss% 0.825476100949194
plot_id,batch_id 0 50 miss% 0.8392384119285746
plot_id,batch_id 0 51 miss% 0.8357615529109146
plot_id,batch_id 0 52 miss% 0.8298660444383253
plot_id,batch_id 0 53 miss% 0.8266014462619545
plot_id,batch_id 0 54 miss% 0.8283590662509617
plot_id,batch_id 0 55 miss% 0.8354486686246698
plot_id,batch_id 0 56 miss% 0.8369002090033416
plot_id,batch_id 0 57 miss% 0.8314399059104696
plot_id,batch_id 0 58 miss% 0.8290307720938106
plot_id,batch_id 0 59 miss% 0.8300616136548847
plot_id,batch_id 0 60 miss% 0.8960683525424692
plot_id,batch_id 0 61 miss% 0.857186919284731
plot_id,batch_id 0 62 miss% 0.844095014723103
plot_id,batch_id 0 63 miss% 0.8377008209110628
plot_id,batch_id 0 64 miss% 0.8360669986573346
plot_id,batch_id 0 65 miss% 0.8987090644583552
plot_id,batch_id 0 66 miss% 0.8607751223162975
plot_id,batch_id 0 67 miss% 0.8469666019480466
plot_id,batch_id 0 68 miss% 0.8386674288346344
plot_id,batch_id 0 69 miss% 0.8344785099010117
plot_id,batch_id 0 70 miss%plot_id,batch_id 0 70 miss% 0.8886203252615402
plot_id,batch_id 0 71 miss% 0.8484896501443976
plot_id,batch_id 0 72 miss% 0.8305611000960011
plot_id,batch_id 0 73 miss% 0.8290909262131669
plot_id,batch_id 0 74 miss% 0.8232047764288843
plot_id,batch_id 0 75 miss% 0.8869318891090203
plot_id,batch_id 0 76 miss% 0.8471869354000621
plot_id,batch_id 0 77 miss% 0.8271246688784724
plot_id,batch_id 0 78 miss% 0.824462509215541
plot_id,batch_id 0 79 miss% 0.8238606966677823
plot_id,batch_id 0 80 miss% 0.8766759469588011
plot_id,batch_id 0 81 miss% 0.8370568805083733
plot_id,batch_id 0 82 miss% 0.8319961919248454
plot_id,batch_id 0 83 miss% 0.8211813606134647
plot_id,batch_id 0 84 miss% 0.8208442842085046
plot_id,batch_id 0 85 miss% 0.8754515504208017
plot_id,batch_id 0 86 miss% 0.8321152003896448
plot_id,batch_id 0 87 miss% 0.8245515733883945
plot_id,batch_id 0 88 miss% 0.8203053158657869
plot_id,batch_id 0 89 miss% 0.8181482297467392
plot_id,batch_id 0 90 miss% 0.8834736976983776
plot_id,batch_id 0 91 miss% 0.8341320926772567
plot_id,batch_id 0 92 miss% 0.8229807539924822
plot_id,batch_id 0 93 miss% 0.8231375257243432
plot_id,batch_id 0 94 miss% 0.8240129889357224
plot_id,batch_id 0 95 miss% 0.8934225884015331
plot_id,batch_id 0 96 miss% 0.8376350138375412
plot_id,batch_id 0 97 miss% 0.8215068733882627
plot_id,batch_id 0 98 miss% 0.8176805309550594
plot_id,batch_id 0 99 miss% 0.8187484600782816
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.85319339 0.82475382 0.82173278 0.81582773 0.81373299 0.85392808
 0.82428281 0.82209188 0.8191006  0.81739909 0.85885304 0.82681845
 0.81919431 0.81517546 0.81633522 0.8761076  0.82197649 0.82492806
 0.81857572 0.82166228 0.84021869 0.82168007 0.81856852 0.81249601
 0.81238803 0.83565609 0.82161828 0.81685272 0.81259955 0.81372648
 0.83748058 0.81969584 0.81908476 0.81288322 0.8126624  0.83742084
 0.82179524 0.81472481 0.81400343 0.81304368 0.83414739 0.81458471
 0.81139721 0.8082219  0.81082944 0.8241953  0.8153766  0.81461077
 0.81251431 0.80826219 0.82816585 0.81705383 0.81498937 0.81184329
 0.80815842 0.82863804 0.81860241 0.81524033 0.81146707 0.81373092
 0.88783814 0.84205305 0.83195944 0.8254252  0.82435278 0.88744803
 0.84899231 0.83480166 0.82438449 0.81922765 0.88862033 0.84848965
 0.8305611  0.82909093 0.82320478 0.88693189 0.84718694 0.82712467
 0.82446251 0.8238607  0.87667595 0.83705688 0.83199619 0.82118136
 0.82084428 0.87545155 0.8321152  0.82455157 0.82030532 0.81814823
 0.8834737  0.83413209 0.82298075 0.82313753 0.82401299 0.89342259
 0.83763501 0.82150687 0.81768053 0.81874846]
for model  145 the mean error 0.8288737169749204
all id 145 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 25 out win 5 err 0.8288737169749204 time 12571.016433477402
Launcher: Job 146 completed in 12704 seconds.
Launcher: Task 5 done. Exiting.
 0.8971445113789384
plot_id,batch_id 0 71 miss% 0.8596810600228709
plot_id,batch_id 0 72 miss% 0.8473970830846231
plot_id,batch_id 0 73 miss% 0.8429180606381261
plot_id,batch_id 0 74 miss% 0.8356700005984262
plot_id,batch_id 0 75 miss% 0.8960133046573118
plot_id,batch_id 0 76 miss% 0.8610333120256848
plot_id,batch_id 0 77 miss% 0.8465393288639911
plot_id,batch_id 0 78 miss% 0.840799899809631
plot_id,batch_id 0 79 miss% 0.837117801211338
plot_id,batch_id 0 80 miss% 0.8882575374142931
plot_id,batch_id 0 81 miss% 0.847212779859082
plot_id,batch_id 0 82 miss% 0.8386773045240984
plot_id,batch_id 0 83 miss% 0.8341571264853866
plot_id,batch_id 0 84 miss% 0.8318394990863208
plot_id,batch_id 0 85 miss% 0.888693013252899
plot_id,batch_id 0 86 miss% 0.8488786709972612
plot_id,batch_id 0 87 miss% 0.8401973048195592
plot_id,batch_id 0 88 miss% 0.8320187398589688
plot_id,batch_id 0 89 miss% 0.8322726551872069
plot_id,batch_id 0 90 miss% 0.893103777075989
plot_id,batch_id 0 91 miss% 0.8518086547984033
plot_id,batch_id 0 92 miss% 0.8405312024511232
plot_id,batch_id 0 93 miss% 0.8351226254803765
plot_id,batch_id 0 94 miss% 0.8339512069625086
plot_id,batch_id 0 95 miss% 0.8948830929766488
plot_id,batch_id 0 96 miss% 0.848050480327242
plot_id,batch_id 0 97 miss% 0.8415762288471347
plot_id,batch_id 0 98 miss% 0.8326670169933439
plot_id,batch_id 0 99 miss% 0.8306694803043796
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.86812617 0.83923455 0.83394338 0.82973801 0.83034072 0.87138736
 0.84213203 0.83566267 0.83081332 0.83122585 0.87210035 0.84007736
 0.8340127  0.82839971 0.82950516 0.8722168  0.83889274 0.83154492
 0.83227345 0.83125865 0.85059241 0.83422266 0.82954172 0.82755504
 0.82579816 0.85130978 0.83886366 0.83036302 0.82996763 0.82648028
 0.85385133 0.83660636 0.83511429 0.83059911 0.82811929 0.85806329
 0.83457996 0.83038556 0.83163093 0.828327   0.84130045 0.83216616
 0.82585414 0.82580725 0.82583967 0.84489548 0.83238034 0.82936556
 0.82726257 0.8254761  0.83923841 0.83576155 0.82986604 0.82660145
 0.82835907 0.83544867 0.83690021 0.83143991 0.82903077 0.83006161
 0.89606835 0.85718692 0.84409501 0.83770082 0.836067   0.89870906
 0.86077512 0.8469666  0.83866743 0.83447851 0.89714451 0.85968106
 0.84739708 0.84291806 0.83567    0.8960133  0.86103331 0.84653933
 0.8407999  0.8371178  0.88825754 0.84721278 0.8386773  0.83415713
 0.8318395  0.88869301 0.84887867 0.8401973  0.83201874 0.83227266
 0.89310378 0.85180865 0.8405312  0.83512263 0.83395121 0.89488309
 0.84805048 0.84157623 0.83266702 0.83066948]
for model  48 the mean error 0.8426751035184514
all id 48 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 21 out win 4 err 0.8426751035184514 time 12577.639124155045
Launcher: Job 49 completed in 12709 seconds.
Launcher: Task 131 done. Exiting.
0 70 miss% 0.326552160328628
plot_id,batch_id 0 71 miss% 0.4748508015599771
plot_id,batch_id 0 72 miss% 0.38102704357453343
plot_id,batch_id 0 73 miss% 0.43655847180831503
plot_id,batch_id 0 74 miss% 0.45691186086301805
plot_id,batch_id 0 75 miss% 0.42179577352870645
plot_id,batch_id 0 76 miss% 0.358601507671548
plot_id,batch_id 0 77 miss% 0.37186130479574425
plot_id,batch_id 0 78 miss% 0.3533168635209628
plot_id,batch_id 0 79 miss% 0.3452007428053666
plot_id,batch_id 0 80 miss% 0.3500065653347087
plot_id,batch_id 0 81 miss% 0.3983573062167595
plot_id,batch_id 0 82 miss% 0.4039842142609845
plot_id,batch_id 0 83 miss% 0.43258785672867345
plot_id,batch_id 0 84 miss% 0.3095314533584113
plot_id,batch_id 0 85 miss% 0.30968135213496367
plot_id,batch_id 0 86 miss% 0.4067344263475823
plot_id,batch_id 0 87 miss% 0.40392102383899564
plot_id,batch_id 0 88 miss% 0.4429749973732405
plot_id,batch_id 0 89 miss% 0.4322483166054704
plot_id,batch_id 0 90 miss% 0.2792885035538718
plot_id,batch_id 0 91 miss% 0.3232258786321719
plot_id,batch_id 0 92 miss% 0.38108142331679545
plot_id,batch_id 0 93 miss% 0.2906402846881076
plot_id,batch_id 0 94 miss% 0.5040954333915776
plot_id,batch_id 0 95 miss% 0.3370328327274878
plot_id,batch_id 0 96 miss% 0.39695318434091875
plot_id,batch_id 0 97 miss% 0.5046414555171668
plot_id,batch_id 0 98 miss% 0.4717724265855481
plot_id,batch_id 0 99 miss% 0.38204161715745116
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.35664717 0.34947674 0.39271566 0.37353564 0.32000383 0.41810625
 0.37521759 0.46141728 0.39475419 0.32974802 0.36613152 0.43338536
 0.41137069 0.4051882  0.38347105 0.34414071 0.53127557 0.45977166
 0.42278534 0.43489674 0.33072236 0.35806772 0.40758111 0.39358488
 0.27694798 0.43019744 0.38121637 0.33019434 0.40712568 0.34443281
 0.41556323 0.3949965  0.38036513 0.52594846 0.50294757 0.44067872
 0.47590876 0.39813933 0.42433185 0.35561199 0.31088107 0.30640107
 0.27913327 0.29329579 0.3357604  0.41632806 0.41552787 0.38345933
 0.3685921  0.34918656 0.4298142  0.43142324 0.38810326 0.38477712
 0.51495502 0.43902611 0.4019     0.38799005 0.36084532 0.41370225
 0.29748199 0.3343042  0.37620063 0.40274901 0.42668933 0.41084548
 0.4335914  0.35424832 0.47260381 0.44223314 0.32655216 0.4748508
 0.38102704 0.43655847 0.45691186 0.42179577 0.35860151 0.3718613
 0.35331686 0.34520074 0.35000657 0.39835731 0.40398421 0.43258786
 0.30953145 0.30968135 0.40673443 0.40392102 0.442975   0.43224832
 0.2792885  0.32322588 0.38108142 0.29064028 0.50409543 0.33703283
 0.39695318 0.50464146 0.47177243 0.38204162]
for model  140 the mean error 0.3918812794213607
all id 140 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 25 out win 6 err 0.3918812794213607 time 12473.590178012848
Launcher: Job 141 completed in 12708 seconds.
Launcher: Task 129 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  154129
Epoch:0, Train loss:0.717944, valid loss:0.727560
Epoch:1, Train loss:0.153582, valid loss:0.006495
Epoch:2, Train loss:0.017322, valid loss:0.005525
Epoch:3, Train loss:0.015900, valid loss:0.005012
Epoch:4, Train loss:0.013345, valid loss:0.003716
Epoch:5, Train loss:0.008078, valid loss:0.002624
Epoch:6, Train loss:0.006127, valid loss:0.002292
Epoch:7, Train loss:0.004605, valid loss:0.002158
Epoch:8, Train loss:0.004092, valid loss:0.001907
Epoch:9, Train loss:0.003807, valid loss:0.001775
Epoch:10, Train loss:0.003568, valid loss:0.001850
Epoch:11, Train loss:0.002812, valid loss:0.001567
Epoch:12, Train loss:0.002651, valid loss:0.001386
Epoch:13, Train loss:0.002503, valid loss:0.001313
Epoch:14, Train loss:0.002369, valid loss:0.001422
Epoch:15, Train loss:0.002302, valid loss:0.001330
Epoch:16, Train loss:0.002172, valid loss:0.001417
Epoch:17, Train loss:0.002137, valid loss:0.001359
Epoch:18, Train loss:0.002055, valid loss:0.001365
Epoch:19, Train loss:0.002019, valid loss:0.001215
Epoch:20, Train loss:0.001893, valid loss:0.001189
Epoch:21, Train loss:0.001517, valid loss:0.001017
Epoch:22, Train loss:0.001517, valid loss:0.000985
Epoch:23, Train loss:0.001460, valid loss:0.000998
Epoch:24, Train loss:0.001447, valid loss:0.000998
Epoch:25, Train loss:0.001405, valid loss:0.000976
Epoch:26, Train loss:0.001350, valid loss:0.000982
Epoch:27, Train loss:0.001322, valid loss:0.001005
Epoch:28, Train loss:0.001302, valid loss:0.001040
Epoch:29, Train loss:0.001310, valid loss:0.000962
Epoch:30, Train loss:0.001248, valid loss:0.001169
Epoch:31, Train loss:0.001083, valid loss:0.000870
Epoch:32, Train loss:0.001049, valid loss:0.000879
Epoch:33, Train loss:0.001046, valid loss:0.000890
Epoch:34, Train loss:0.001024, valid loss:0.000930
Epoch:35, Train loss:0.001016, valid loss:0.000896
Epoch:36, Train loss:0.001000, valid loss:0.000942
Epoch:37, Train loss:0.001000, valid loss:0.000951
Epoch:38, Train loss:0.000984, valid loss:0.000914
Epoch:39, Train loss:0.000977, valid loss:0.000920
Epoch:40, Train loss:0.000953, valid loss:0.000871
Epoch:41, Train loss:0.000870, valid loss:0.000885
Epoch:42, Train loss:0.000852, valid loss:0.000845
Epoch:43, Train loss:0.000854, valid loss:0.000859
Epoch:44, Train loss:0.000837, valid loss:0.000877
Epoch:45, Train loss:0.000837, valid loss:0.000875
Epoch:46, Train loss:0.000824, valid loss:0.000853
Epoch:47, Train loss:0.000831, valid loss:0.000862
Epoch:48, Train loss:0.000819, valid loss:0.000875
Epoch:49, Train loss:0.000810, valid loss:0.000837
Epoch:50, Train loss:0.000809, valid loss:0.000830
Epoch:51, Train loss:0.000752, valid loss:0.000827
Epoch:52, Train loss:0.000747, valid loss:0.000827
Epoch:53, Train loss:0.000745, valid loss:0.000827
Epoch:54, Train loss:0.000743, valid loss:0.000835
Epoch:55, Train loss:0.000743, valid loss:0.000829
Epoch:56, Train loss:0.000741, valid loss:0.000829
Epoch:57, Train loss:0.000741, valid loss:0.000826
Epoch:58, Train loss:0.000741, valid loss:0.000830
Epoch:59, Train loss:0.000741, valid loss:0.000825
Epoch:60, Train loss:0.000740, valid loss:0.000831
training time 12637.652930498123
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07992852822040766
plot_id,batch_id 0 1 miss% 0.031181584363056174
plot_id,batch_id 0 2 miss% 0.07757204943251626
plot_id,batch_id 0 3 miss% 0.06405149239098624
plot_id,batch_id 0 4 miss% 0.07600240293400372
plot_id,batch_id 0 5 miss% 0.07298290814899669
plot_id,batch_id 0 6 miss% 0.03802180251899019
plot_id,batch_id 0 7 miss% 0.1205293711231959
plot_id,batch_id 0 8 miss% 0.06422290714337059
plot_id,batch_id 0 9 miss% 0.04243340548901582
plot_id,batch_id 0 10 miss% 0.024792420220064303
plot_id,batch_id 0 11 miss% 0.04986030567407188
plot_id,batch_id 0 12 miss% 0.04465001373124734
plot_id,batch_id 0 13 miss% 0.06100397327380689
plot_id,batch_id 0 14 miss% 0.10143166884517399
plot_id,batch_id 0 15 miss% 0.03302144452819116
plot_id,batch_id 0 16 miss% 0.09381501951963288
plot_id,batch_id 0 17 miss% 0.03233286723384949
plot_id,batch_id 0 18 miss% 0.06356910810717427
plot_id,batch_id 0 19 miss% 0.07029777531602739
plot_id,batch_id 0 20 miss% 0.07705079667522405
plot_id,batch_id 0 21 miss% 0.04366953194250111
plot_id,batch_id 0 22 miss% 0.06825723438258313
plot_id,batch_id 0 23 miss% 0.06827796298480301
plot_id,batch_id 0 24 miss% 0.0642695926569623
plot_id,batch_id 0 25 miss% 0.08564866016752354
plot_id,batch_id 0 26 miss% 0.03601453351577114
plot_id,batch_id 0 27 miss% 0.03755713834219353
plot_id,batch_id 0 28 miss% 0.039856521621548346
plot_id,batch_id 0 29 miss% 0.05386693570146242
plot_id,batch_id 0 30 miss% 0.08123038358808897
plot_id,batch_id 0 31 miss% 0.10485481531702469
plot_id,batch_id 0 32 miss% 0.09194633525924191
plot_id,batch_id 0 33 miss% 0.06375654195511367
plot_id,batch_id 0 34 miss% 0.06068934248925798
plot_id,batch_id 0 35 miss% 0.04274788383539015
plot_id,batch_id 0 36 miss% 0.08288143965949536
plot_id,batch_id 0 37 miss% 0.08079613970797087
plot_id,batch_id 0 38 miss% 0.044946794670833776
plot_id,batch_id 0 39 miss% 0.023201299714232848
plot_id,batch_id 0 40 miss% 0.09668822531054007
plot_id,batch_id 0 41 miss% 0.06060763013273222
plot_id,batch_id 0 42 miss% 0.02708201725252587
plot_id,batch_id 0 43 miss% 0.05953086675784322
plot_id,batch_id 0 44 miss% 0.026723786426871874
plot_id,batch_id 0 45 miss% 0.04776648116503028
plot_id,batch_id 0 46 miss% 0.028033816546954884
plot_id,batch_id 0 47 miss% 0.027917345583758513
plot_id,batch_id 0 48 miss% 0.029840535668826215
plot_id,batch_id 0 49 miss% 0.020725085177460804
plot_id,batch_id 0 50 miss% 0.16360490283269932
plot_id,batch_id 0 51 miss% 0.019409521385291457
plot_id,batch_id 0 52 miss% 0.025045649384234012
plot_id,batch_id 0 53 miss% 0.041150379670999875
plot_id,batch_id 0 54 miss% 0.04736436147908085
plot_id,batch_id 0 55 miss% 0.061967983983334904
plot_id,batch_id 0 56 miss% 0.06561695998807278
plot_id,batch_id 0 57 miss% 0.03907449458627409
plot_id,batch_id 0 58 miss% 0.04143095340991653
plot_id,batch_id 0 59 miss% 0.03228394739049743
plot_id,batch_id 0 60 miss% 0.0603127476143533
plot_id,batch_id 0 61 miss% 0.05040735359556142
plot_id,batch_id 0 62 miss% 0.043327761236842705
plot_id,batch_id 0 63 miss% 0.06513874775974486
plot_id,batch_id 0 64 miss% 0.06294665736997859
plot_id,batch_id 0 65 miss% 0.07238469864246787
plot_id,batch_id 0 66 miss% 0.03986508016508905
plot_id,batch_id 0 67 miss% 0.039665894903258746
plot_id,batch_id 0 68 miss% 0.05481794595509538
plot_id,batch_id 0 69 miss% 0.030624809504933643
plot_id,batch_id 0 70 miss% 0.13834793313491767
plot_id,batch_id 0 71 miss% 0.02809696076402083
plot_id,batch_id 0 72 miss% 0.08623182855516894
plot_id,batch_id 0 73 miss% 0.07860480917965594
plot_id,batch_id 0 74 miss% 0.13251501966929968
plot_id,batch_id 0 75 miss% 0.02970299411430338
plot_id,batch_id 0 76 miss% 0.062084241470556537
plot_id,batch_id 0 77 miss% 0.0736876220348038
plot_id,batch_id 0 78 miss% 0.05811815880496536
plot_id,batch_id 0 79 miss% 0.07464989842383299
plot_id,batch_id 0 80 miss% 0.09960522795809373
plot_id,batch_id 0 81 miss% 0.09523588774258233
plot_id,batch_id 0 82 miss% 0.0658757626311565
plot_id,batch_id 0 83 miss% 0.054006599971525314
plot_id,batch_id 0 84 miss% 0.06090970724273923
plot_id,batch_id 0 85 miss% 0.03979754653346585
plot_id,batch_id 0 86 miss% 0.07335513902450497
plot_id,batch_id 0 87 miss% 0.06876342049005796
plot_id,batch_id 0 88 miss% 0.09090887045284153
plot_id,batch_id 0 89 miss% 0.06124162622840148
plot_id,batch_id 0 90 miss% 0.05041374262432188
plot_id,batch_id 0 91 miss% 0.06643480546240946
plot_id,batch_id 0 92 miss% 0.038602424741134046
plot_id,batch_id 0 93 miss% 0.06700896508948452
plot_id,batch_id 0 94 miss% 0.07494843441441464
plot_id,batch_id 0 95 miss% 0.07426956783107222
plot_id,batch_id 0 96 miss% 0.06109908200620765
plot_id,batch_id 0 97 miss% 0.028249709749496626
plot_id,batch_id 0 98 miss% 0.02687113730904472
plot_id,batch_id 0 99 miss% 0.056779648723859495
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07992853 0.03118158 0.07757205 0.06405149 0.0760024  0.07298291
 0.0380218  0.12052937 0.06422291 0.04243341 0.02479242 0.04986031
 0.04465001 0.06100397 0.10143167 0.03302144 0.09381502 0.03233287
 0.06356911 0.07029778 0.0770508  0.04366953 0.06825723 0.06827796
 0.06426959 0.08564866 0.03601453 0.03755714 0.03985652 0.05386694
 0.08123038 0.10485482 0.09194634 0.06375654 0.06068934 0.04274788
 0.08288144 0.08079614 0.04494679 0.0232013  0.09668823 0.06060763
 0.02708202 0.05953087 0.02672379 0.04776648 0.02803382 0.02791735
 0.02984054 0.02072509 0.1636049  0.01940952 0.02504565 0.04115038
 0.04736436 0.06196798 0.06561696 0.03907449 0.04143095 0.03228395
 0.06031275 0.05040735 0.04332776 0.06513875 0.06294666 0.0723847
 0.03986508 0.03966589 0.05481795 0.03062481 0.13834793 0.02809696
 0.08623183 0.07860481 0.13251502 0.02970299 0.06208424 0.07368762
 0.05811816 0.0746499  0.09960523 0.09523589 0.06587576 0.0540066
 0.06090971 0.03979755 0.07335514 0.06876342 0.09090887 0.06124163
 0.05041374 0.06643481 0.03860242 0.06700897 0.07494843 0.07426957
 0.06109908 0.02824971 0.02687114 0.05677965]
for model  7 the mean error 0.05990994345659611
all id 7 hidden_dim 32 learning_rate 0.0025 num_layers 3 frames 21 out win 5 err 0.05990994345659611 time 12637.652930498123
Launcher: Job 8 completed in 12908 seconds.
Launcher: Task 125 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  41745
Epoch:0, Train loss:0.449807, valid loss:0.435074
Epoch:1, Train loss:0.021237, valid loss:0.002335
Epoch:2, Train loss:0.004931, valid loss:0.001557
Epoch:3, Train loss:0.003202, valid loss:0.001415
Epoch:4, Train loss:0.002763, valid loss:0.001310
Epoch:5, Train loss:0.002475, valid loss:0.001204
Epoch:6, Train loss:0.002302, valid loss:0.001064
Epoch:7, Train loss:0.002170, valid loss:0.001021
Epoch:8, Train loss:0.002064, valid loss:0.000947
Epoch:9, Train loss:0.001930, valid loss:0.001000
Epoch:10, Train loss:0.001858, valid loss:0.000925
Epoch:11, Train loss:0.001447, valid loss:0.000777
Epoch:12, Train loss:0.001385, valid loss:0.000694
Epoch:13, Train loss:0.001365, valid loss:0.000792
Epoch:14, Train loss:0.001329, valid loss:0.000696
Epoch:15, Train loss:0.001282, valid loss:0.000766
Epoch:16, Train loss:0.001243, valid loss:0.000777
Epoch:17, Train loss:0.001213, valid loss:0.000694
Epoch:18, Train loss:0.001185, valid loss:0.000708
Epoch:19, Train loss:0.001148, valid loss:0.000661
Epoch:20, Train loss:0.001134, valid loss:0.000686
Epoch:21, Train loss:0.000949, valid loss:0.000628
Epoch:22, Train loss:0.000925, valid loss:0.000595
Epoch:23, Train loss:0.000916, valid loss:0.000608
Epoch:24, Train loss:0.000896, valid loss:0.000570
Epoch:25, Train loss:0.000890, valid loss:0.000585
Epoch:26, Train loss:0.000877, valid loss:0.000601
Epoch:27, Train loss:0.000868, valid loss:0.000606
Epoch:28, Train loss:0.000864, valid loss:0.000612
Epoch:29, Train loss:0.000847, valid loss:0.000674
Epoch:30, Train loss:0.000844, valid loss:0.000598
Epoch:31, Train loss:0.000736, valid loss:0.000559
Epoch:32, Train loss:0.000734, valid loss:0.000563
Epoch:33, Train loss:0.000723, valid loss:0.000575
Epoch:34, Train loss:0.000723, valid loss:0.000598
Epoch:35, Train loss:0.000721, valid loss:0.000548
Epoch:36, Train loss:0.000711, valid loss:0.000538
Epoch:37, Train loss:0.000713, valid loss:0.000547
Epoch:38, Train loss:0.000706, valid loss:0.000572
Epoch:39, Train loss:0.000700, valid loss:0.000542
Epoch:40, Train loss:0.000698, valid loss:0.000567
Epoch:41, Train loss:0.000645, valid loss:0.000546
Epoch:42, Train loss:0.000640, valid loss:0.000534
Epoch:43, Train loss:0.000642, valid loss:0.000544
Epoch:44, Train loss:0.000635, valid loss:0.000544
Epoch:45, Train loss:0.000637, valid loss:0.000551
Epoch:46, Train loss:0.000634, valid loss:0.000534
Epoch:47, Train loss:0.000629, valid loss:0.000548
Epoch:48, Train loss:0.000624, valid loss:0.000545
Epoch:49, Train loss:0.000623, valid loss:0.000540
Epoch:50, Train loss:0.000621, valid loss:0.000524
Epoch:51, Train loss:0.000591, valid loss:0.000521
Epoch:52, Train loss:0.000587, valid loss:0.000524
Epoch:53, Train loss:0.000585, valid loss:0.000520
Epoch:54, Train loss:0.000584, valid loss:0.000521
Epoch:55, Train loss:0.000583, valid loss:0.000521
Epoch:56, Train loss:0.000582, valid loss:0.000523
Epoch:57, Train loss:0.000582, valid loss:0.000523
Epoch:58, Train loss:0.000582, valid loss:0.000523
Epoch:59, Train loss:0.000581, valid loss:0.000523
Epoch:60, Train loss:0.000580, valid loss:0.000520
training time 12745.747241973877
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.34903408178927015
plot_id,batch_id 0 1 miss% 0.38929617967009156
plot_id,batch_id 0 2 miss% 0.5381282455134825
plot_id,batch_id 0 3 miss% 0.38856989080527826
plot_id,batch_id 0 4 miss% 0.3603509324884144
plot_id,batch_id 0 5 miss% 0.37218673220615545
plot_id,batch_id 0 6 miss% 0.43329352004276767
plot_id,batch_id 0 7 miss% 0.5031408332039679
plot_id,batch_id 0 8 miss% 0.566800859911028
plot_id,batch_id 0 9 miss% 0.5194825302164966
plot_id,batch_id 0 10 miss% 0.3015301727349074
plot_id,batch_id 0 11 miss% 0.3526965750280468
plot_id,batch_id 0 12 miss% 0.42991247728522525
plot_id,batch_id 0 13 miss% 0.40938587886202227
plot_id,batch_id 0 14 miss% 0.48962121105954665
plot_id,batch_id 0 15 miss% 0.3772179100937326
plot_id,batch_id 0 16 miss% 0.4406758621972192
plot_id,batch_id 0 17 miss% 0.5746815440846613
plot_id,batch_id 0 18 miss% 0.46350105564311167
plot_id,batch_id 0 19 miss% 0.42305915141294714
plot_id,batch_id 0 20 miss% 0.3831104097983766
plot_id,batch_id 0 21 miss% 0.37249239148185065
plot_id,batch_id 0 22 miss% 0.43839174065266623
plot_id,batch_id 0 23 miss% 0.4864114600012672
plot_id,batch_id 0 24 miss% 0.36182405725309946
plot_id,batch_id 0 25 miss% 0.3811975372882823
plot_id,batch_id 0 26 miss% 0.5023020650772331
plot_id,batch_id 0 27 miss% 0.3366511363989017
plot_id,batch_id 0 28 miss% 0.40657350121522756
plot_id,batch_id 0 29 miss% 0.4445057650222471
plot_id,batch_id 0 30 miss% 0.3248245625642968
plot_id,batch_id 0 31 miss% 0.4507302399165403
plot_id,batch_id 0 32 miss% 0.4272985420185049
plot_id,batch_id 0 33 miss% 0.4902644850715951
plot_id,batch_id 0 34 miss% 0.4100713116708301
plot_id,batch_id 0 35 miss% 0.34289115580826873
plot_id,batch_id 0 36 miss% 0.4672479588688073
plot_id,batch_id 0 37 miss% 0.4648128637741419
plot_id,batch_id 0 38 miss% 0.4984752722987289
plot_id,batch_id 0 39 miss% 0.4423405695986871
plot_id,batch_id 0 40 miss% 0.3536164992183337
plot_id,batch_id 0 41 miss% 0.4112289739053463
plot_id,batch_id 0 42 miss% 0.36233195806844054
plot_id,batch_id 0 43 miss% 0.36079803637817365
plot_id,batch_id 0 44 miss% 0.36727128294355077
plot_id,batch_id 0 45 miss% 0.4350117573144912
plot_id,batch_id 0 46 miss% 0.37729725954730287
plot_id,batch_id 0 47 miss% 0.4574958686120883
plot_id,batch_id 0 48 miss% 0.3970139885772322
plot_id,batch_id 0 49 miss% 0.37986729783181106
plot_id,batch_id 0 50 miss% 0.5527220863506358
plot_id,batch_id 0 51 miss% 0.5151759828782084
plot_id,batch_id 0 52 miss% 0.49172370022584755
plot_id,batch_id 0 53 miss% 0.3167990577286275
plot_id,batch_id 0 54 miss% 0.31154308284983273
plot_id,batch_id 0 55 miss% 0.2944698955500166
plot_id,batch_id 0 56 miss% 0.45545626168943254
plot_id,batch_id 0 57 miss% 0.49875808047670095
plot_id,batch_id 0 58 miss% 0.43233504245590176
plot_id,batch_id 0 59 miss% 0.46039374241364606
plot_id,batch_id 0 60 miss% 0.3173519085753236
plot_id,batch_id 0 61 miss% 0.26530344358888724
plot_id,batch_id 0 62 miss% 0.3536620010624927
plot_id,batch_id 0 63 miss% 0.4084798844618984
plot_id,batch_id 0 64 miss% 0.43900403284311806
plot_id,batch_id 0 65 miss% 0.28945892243632776
plot_id,batch_id 0 66 miss% 0.3943328519600624
plot_id,batch_id 0 67 miss% 0.33008829492649583
plot_id,batch_id 0 68 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  89105
Epoch:0, Train loss:0.423104, valid loss:0.381201
Epoch:1, Train loss:0.050486, valid loss:0.002498
Epoch:2, Train loss:0.005446, valid loss:0.001798
Epoch:3, Train loss:0.003267, valid loss:0.001204
Epoch:4, Train loss:0.002240, valid loss:0.001072
Epoch:5, Train loss:0.001951, valid loss:0.000894
Epoch:6, Train loss:0.001731, valid loss:0.000844
Epoch:7, Train loss:0.001624, valid loss:0.000856
Epoch:8, Train loss:0.001461, valid loss:0.000752
Epoch:9, Train loss:0.001379, valid loss:0.000757
Epoch:10, Train loss:0.001330, valid loss:0.000670
Epoch:11, Train loss:0.001031, valid loss:0.000611
Epoch:12, Train loss:0.000987, valid loss:0.000649
Epoch:13, Train loss:0.000953, valid loss:0.000609
Epoch:14, Train loss:0.000944, valid loss:0.000534
Epoch:15, Train loss:0.000920, valid loss:0.000600
Epoch:16, Train loss:0.000899, valid loss:0.000517
Epoch:17, Train loss:0.000870, valid loss:0.000553
Epoch:18, Train loss:0.000858, valid loss:0.000583
Epoch:19, Train loss:0.000830, valid loss:0.000510
Epoch:20, Train loss:0.000816, valid loss:0.000572
Epoch:21, Train loss:0.000674, valid loss:0.000451
Epoch:22, Train loss:0.000662, valid loss:0.000513
Epoch:23, Train loss:0.000664, valid loss:0.000467
Epoch:24, Train loss:0.000635, valid loss:0.000473
Epoch:25, Train loss:0.000636, valid loss:0.000480
Epoch:26, Train loss:0.000621, valid loss:0.000599
Epoch:27, Train loss:0.000637, valid loss:0.000443
Epoch:28, Train loss:0.000613, valid loss:0.000479
Epoch:29, Train loss:0.000614, valid loss:0.000478
Epoch:30, Train loss:0.000603, valid loss:0.000472
Epoch:31, Train loss:0.000527, valid loss:0.000426
Epoch:32, Train loss:0.000520, valid loss:0.000475
Epoch:33, Train loss:0.000519, valid loss:0.000457
Epoch:34, Train loss:0.000513, valid loss:0.000463
Epoch:35, Train loss:0.000516, valid loss:0.000478
Epoch:36, Train loss:0.000510, valid loss:0.000446
Epoch:37, Train loss:0.000503, valid loss:0.000454
Epoch:38, Train loss:0.000495, valid loss:0.000495
Epoch:39, Train loss:0.000507, valid loss:0.000433
Epoch:40, Train loss:0.000501, valid loss:0.000440
Epoch:41, Train loss:0.000456, valid loss:0.000429
Epoch:42, Train loss:0.000457, valid loss:0.000434
Epoch:43, Train loss:0.000455, valid loss:0.000443
Epoch:44, Train loss:0.000452, valid loss:0.000426
Epoch:45, Train loss:0.000449, valid loss:0.000423
Epoch:46, Train loss:0.000450, valid loss:0.000427
Epoch:47, Train loss:0.000448, valid loss:0.000438
Epoch:48, Train loss:0.000448, valid loss:0.000432
Epoch:49, Train loss:0.000446, valid loss:0.000442
Epoch:50, Train loss:0.000442, valid loss:0.000416
Epoch:51, Train loss:0.000420, valid loss:0.000413
Epoch:52, Train loss:0.000418, valid loss:0.000421
Epoch:53, Train loss:0.000418, valid loss:0.000433
Epoch:54, Train loss:0.000417, valid loss:0.000423
Epoch:55, Train loss:0.000417, valid loss:0.000420
Epoch:56, Train loss:0.000416, valid loss:0.000422
Epoch:57, Train loss:0.000416, valid loss:0.000419
Epoch:58, Train loss:0.000415, valid loss:0.000419
Epoch:59, Train loss:0.000416, valid loss:0.000417
Epoch:60, Train loss:0.000415, valid loss:0.000414
training time 12767.973995447159
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.0584631973151415
plot_id,batch_id 0 1 miss% 0.06218431377735569
plot_id,batch_id 0 2 miss% 0.10272631974308709
plot_id,batch_id 0 3 miss% 0.07409434306634788
plot_id,batch_id 0 4 miss% 0.0330055578503408
plot_id,batch_id 0 5 miss% 0.05767767635618093
plot_id,batch_id 0 6 miss% 0.05168578984668118
plot_id,batch_id 0 7 miss% 0.10367833513713526
plot_id,batch_id 0 8 miss% 0.06636932374664811
plot_id,batch_id 0 9 miss% 0.059612900094706295
plot_id,batch_id 0 10 miss% 0.05038839302465384
plot_id,batch_id 0 11 miss% 0.08690637798318733
plot_id,batch_id 0 12 miss% 0.08160523235203769
plot_id,batch_id 0 13 miss% 0.08943325347358806
plot_id,batch_id 0 14 miss% 0.10890153716474212
plot_id,batch_id 0 15 miss% 0.04100027143081168
plot_id,batch_id 0 16 miss% 0.11519420573657038
plot_id,batch_id 0 17 miss% 0.044622252017339895
plot_id,batch_id 0 18 miss% 0.06505418038894152
plot_id,batch_id 0 19 miss% 0.07500718362716603
plot_id,batch_id 0 20 miss% 0.12877024973199075
plot_id,batch_id 0 21 miss% 0.03775336375727814
plot_id,batch_id 0 22 miss% 0.02766080335162673
plot_id,batch_id 0 23 miss% 0.0499021745019184
plot_id,batch_id 0 24 miss% 0.0312387334629445
plot_id,batch_id 0 25 miss% 0.06570581726548347
plot_id,batch_id 0 26 miss% 0.08075799476181474
plot_id,batch_id 0 27 miss% 0.04749497229836324
plot_id,batch_id 0 28 miss% 0.051498857475758886
plot_id,batch_id 0 29 miss% 0.03304088627631267
plot_id,batch_id 0 30 miss% 0.06445296943488282
plot_id,batch_id 0 31 miss% 0.1370867015963811
plot_id,batch_id 0 32 miss% 0.11594365734195149
plot_id,batch_id 0 33 miss% 0.07400221300658773
plot_id,batch_id 0 34 miss% 0.0497227010010249
plot_id,batch_id 0 35 miss% 0.0568885521685626
plot_id,batch_id 0 36 miss% 0.13189027786141894
plot_id,batch_id 0 37 miss% 0.10068930080392427
plot_id,batch_id 0 38 miss% 0.07641148783632942
plot_id,batch_id 0 39 miss% 0.0468432727240752
plot_id,batch_id 0 40 miss% 0.08198233438363571
plot_id,batch_id 0 41 miss% 0.061654035592236846
plot_id,batch_id 0 42 miss% 0.054495865012103545
plot_id,batch_id 0 43 miss% 0.06115381553513393
plot_id,batch_id 0 44 miss% 0.02860328735027954
plot_id,batch_id 0 45 miss% 0.09368890825744672
plot_id,batch_id 0 46 miss% 0.03137978134676224
plot_id,batch_id 0 47 miss% 0.027335603411913014
plot_id,batch_id 0 48 miss% 0.01608919100526914
plot_id,batch_id 0 49 miss% 0.02538708947146388
plot_id,batch_id 0 50 miss% 0.12639510081431046
plot_id,batch_id 0 51 miss% 0.04285761426738893
plot_id,batch_id 0 52 miss% 0.04031745190176955
plot_id,batch_id 0 53 miss% 0.02740134743932011
plot_id,batch_id 0 54 miss% 0.026974692873019377
plot_id,batch_id 0 55 miss% 0.08090264324896371
plot_id,batch_id 0 56 miss% 0.1272544644428679
plot_id,batch_id 0 57 miss% 0.044402839813301995
plot_id,batch_id 0 58 miss% 0.037015774407755234
plot_id,batch_id 0 59 miss% 0.03924805577018847
plot_id,batch_id 0 60 miss% 0.052281991273718895
plot_id,batch_id 0 61 miss% 0.026452966957898727
plot_id,batch_id 0 62 miss% 0.046992457764800655
plot_id,batch_id 0 63 miss% 0.055488511644839644
plot_id,batch_id 0 64 miss% 0.05851900590177778
plot_id,batch_id 0 65 0.4708345318654477
plot_id,batch_id 0 69 miss% 0.48589752316626444
plot_id,batch_id 0 70 miss% 0.20920473170702375
plot_id,batch_id 0 71 miss% 0.3592458106612403
plot_id,batch_id 0 72 miss% 0.455416340346294
plot_id,batch_id 0 73 miss% 0.32044360357183344
plot_id,batch_id 0 74 miss% 0.4764693102306209
plot_id,batch_id 0 75 miss% 0.2622715662258329
plot_id,batch_id 0 76 miss% 0.3648861747949445
plot_id,batch_id 0 77 miss% 0.3963622987080494
plot_id,batch_id 0 78 miss% 0.3257153052254875
plot_id,batch_id 0 79 miss% 0.3313729334539144
plot_id,batch_id 0 80 miss% 0.3116914919042281
plot_id,batch_id 0 81 miss% 0.432936537101318
plot_id,batch_id 0 82 miss% 0.38710570039949255
plot_id,batch_id 0 83 miss% 0.4042493633725622
plot_id,batch_id 0 84 miss% 0.3705723176890621
plot_id,batch_id 0 85 miss% 0.31728269447577157
plot_id,batch_id 0 86 miss% 0.4185205202584891
plot_id,batch_id 0 87 miss% 0.3727107404824111
plot_id,batch_id 0 88 miss% 0.46026458278755056
plot_id,batch_id 0 89 miss% 0.3946389123629376
plot_id,batch_id 0 90 miss% 0.26804843663553235
plot_id,batch_id 0 91 miss% 0.4229011876478459
plot_id,batch_id 0 92 miss% 0.3924557945397006
plot_id,batch_id 0 93 miss% 0.3268452426731448
plot_id,batch_id 0 94 miss% 0.5320360871520765
plot_id,batch_id 0 95 miss% 0.3508880930205647
plot_id,batch_id 0 96 miss% 0.36180805135173394
plot_id,batch_id 0 97 miss% 0.4676229109727273
plot_id,batch_id 0 98 miss% 0.4355376254872102
plot_id,batch_id 0 99 miss% 0.44193304779043885
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.34903408 0.38929618 0.53812825 0.38856989 0.36035093 0.37218673
 0.43329352 0.50314083 0.56680086 0.51948253 0.30153017 0.35269658
 0.42991248 0.40938588 0.48962121 0.37721791 0.44067586 0.57468154
 0.46350106 0.42305915 0.38311041 0.37249239 0.43839174 0.48641146
 0.36182406 0.38119754 0.50230207 0.33665114 0.4065735  0.44450577
 0.32482456 0.45073024 0.42729854 0.49026449 0.41007131 0.34289116
 0.46724796 0.46481286 0.49847527 0.44234057 0.3536165  0.41122897
 0.36233196 0.36079804 0.36727128 0.43501176 0.37729726 0.45749587
 0.39701399 0.3798673  0.55272209 0.51517598 0.4917237  0.31679906
 0.31154308 0.2944699  0.45545626 0.49875808 0.43233504 0.46039374
 0.31735191 0.26530344 0.353662   0.40847988 0.43900403 0.28945892
 0.39433285 0.33008829 0.47083453 0.48589752 0.20920473 0.35924581
 0.45541634 0.3204436  0.47646931 0.26227157 0.36488617 0.3963623
 0.32571531 0.33137293 0.31169149 0.43293654 0.3871057  0.40424936
 0.37057232 0.31728269 0.41852052 0.37271074 0.46026458 0.39463891
 0.26804844 0.42290119 0.39245579 0.32684524 0.53203609 0.35088809
 0.36180805 0.46762291 0.43553763 0.44193305]
for model  191 the mean error 0.40472143332959903
all id 191 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 31 out win 6 err 0.40472143332959903 time 12745.747241973877
Launcher: Job 192 completed in 12978 seconds.
Launcher: Task 165 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  154129
Epoch:0, Train loss:0.708836, valid loss:0.710832
Epoch:1, Train loss:0.161227, valid loss:0.005929
Epoch:2, Train loss:0.017144, valid loss:0.005605
Epoch:3, Train loss:0.015844, valid loss:0.004942
Epoch:4, Train loss:0.015082, valid loss:0.004619
Epoch:5, Train loss:0.014228, valid loss:0.004187
Epoch:6, Train loss:0.013129, valid loss:0.004025
Epoch:7, Train loss:0.012481, valid loss:0.003795
Epoch:8, Train loss:0.011948, valid loss:0.003887
Epoch:9, Train loss:0.011486, valid loss:0.003358
Epoch:10, Train loss:0.010039, valid loss:0.003758
Epoch:11, Train loss:0.009196, valid loss:0.002831
Epoch:12, Train loss:0.008970, valid loss:0.003088
Epoch:13, Train loss:0.008259, valid loss:0.002652
Epoch:14, Train loss:0.005859, valid loss:0.002244
Epoch:15, Train loss:0.004825, valid loss:0.002146
Epoch:16, Train loss:0.003969, valid loss:0.001528
Epoch:17, Train loss:0.003488, valid loss:0.001578
Epoch:18, Train loss:0.002886, valid loss:0.001323
Epoch:19, Train loss:0.002195, valid loss:0.001332
Epoch:20, Train loss:0.002079, valid loss:0.001250
Epoch:21, Train loss:0.001653, valid loss:0.001099
Epoch:22, Train loss:0.001593, valid loss:0.001043
Epoch:23, Train loss:0.001534, valid loss:0.001079
Epoch:24, Train loss:0.001535, valid loss:0.001128
Epoch:25, Train loss:0.001509, valid loss:0.001041
Epoch:26, Train loss:0.001456, valid loss:0.001021
Epoch:27, Train loss:0.001426, valid loss:0.001075
Epoch:28, Train loss:0.001391, valid loss:0.001002
Epoch:29, Train loss:0.001333, valid loss:0.001058
Epoch:30, Train loss:0.001327, valid loss:0.001040
Epoch:31, Train loss:0.001123, valid loss:0.000908
Epoch:32, Train loss:0.001088, valid loss:0.000910
Epoch:33, Train loss:0.001074, valid loss:0.000884
Epoch:34, Train loss:0.001062, valid loss:0.000913
Epoch:35, Train loss:0.001074, valid loss:0.000899
Epoch:36, Train loss:0.001057, valid loss:0.000878
Epoch:37, Train loss:0.001012, valid loss:0.000862
Epoch:38, Train loss:0.000995, valid loss:0.000893
Epoch:39, Train loss:0.000998, valid loss:0.000889
Epoch:40, Train loss:0.000997, valid loss:0.000892
Epoch:41, Train loss:0.000884, valid loss:0.000833
Epoch:42, Train loss:0.000876, valid loss:0.000833
Epoch:43, Train loss:0.000862, valid loss:0.000820
Epoch:44, Train loss:0.000858, valid loss:0.000834
Epoch:45, Train loss:0.000851, valid loss:0.000822
Epoch:46, Train loss:0.000849, valid loss:0.000835
Epoch:47, Train loss:0.000840, valid loss:0.000865
Epoch:48, Train loss:0.000845, valid loss:0.000836
Epoch:49, Train loss:0.000820, valid loss:0.000846
Epoch:50, Train loss:0.000821, valid loss:0.000848
Epoch:51, Train loss:0.000772, valid loss:0.000820
Epoch:52, Train loss:0.000761, valid loss:0.000813
Epoch:53, Train loss:0.000758, valid loss:0.000820
Epoch:54, Train loss:0.000756, valid loss:0.000818
Epoch:55, Train loss:0.000755, valid loss:0.000811
Epoch:56, Train loss:0.000755, valid loss:0.000811
Epoch:57, Train loss:0.000752, valid loss:0.000813
Epoch:58, Train loss:0.000752, valid loss:0.000813
Epoch:59, Train loss:0.000750, valid loss:0.000803
Epoch:60, Train loss:0.000751, valid loss:0.000807
training time 12803.917968988419
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.09138500164033574
plot_id,batch_id 0 1 miss% 0.04174929075942534
plot_id,batch_id 0 2 miss% 0.12382085128328481
plot_id,batch_id 0 3 miss% 0.05722258829434361
plot_id,batch_id 0 4 miss% 0.0546107893380257
plot_id,batch_id 0 5 miss% 0.05065574335174219
plot_id,batch_id 0 6 miss% 0.04694130770155689
plot_id,batch_id 0 7 miss% 0.13719450401147248
plot_id,batch_id 0 8 miss% 0.0886839749891462
plot_id,batch_id 0 9 miss% 0.055884009813849465
plot_id,batch_id 0 10 miss% 0.06728042139237622
plot_id,batch_id 0 11 miss% 0.05554176602101778
plot_id,batch_id 0 12 miss% 0.11868569826284905
plot_id,batch_id 0 13 miss% 0.06675106851860657
plot_id,batch_id 0 14 miss% 0.06672546393547234
plot_id,batch_id 0 15 miss% 0.05821295758533103
plot_id,batch_id 0 16 miss% 0.19615169288513312
plot_id,batch_id 0 17 miss% 0.13754269031612043
plot_id,batch_id 0 18 miss% 0.058007442388592505
plot_id,batch_id 0 19 miss% 0.13052375192994325
plot_id,batch_id 0 20 miss% 0.06079162306100353
plot_id,batch_id 0 21 miss% 0.03990544619709458
plot_id,batch_id 0 22 miss% 0.053725290976456494
plot_id,batch_id 0 23 miss% 0.04306810098034698
plot_id,batch_id 0 24 miss% 0.052508091301237236
plot_id,batch_id 0 25 miss% 0.039630765422365925
plot_id,batch_id 0 26 miss% 0.053254644898321116
plot_id,batch_id 0 27 miss% 0.049752312051165516
plot_id,batch_id 0 28 miss% 0.02830715984106313
plot_id,batch_id 0 29 miss% 0.02578435259562244
plot_id,batch_id 0 30 miss% 0.054240719847176044
plot_id,batch_id 0 31 miss% 0.12437206928710294
plot_id,batch_id 0 32 miss% 0.10783382465805612
plot_id,batch_id 0 33 miss% 0.0686995466349886
plot_id,batch_id 0 34 miss% 0.044330446113384374
plot_id,batch_id 0 35 miss% 0.03989962944555334
plot_id,batch_id 0 36 miss% 0.1312942096376016
plot_id,batch_id 0 37 miss% 0.10023819289978528
plot_id,batch_id 0 38 miss% 0.054439348471768775
plot_id,batch_id 0 39 miss% 0.029434785882164256
plot_id,batch_id 0 40 miss% 0.0653458319002267
plot_id,batch_id 0 41 miss% 0.05025225933116764
plot_id,batch_id 0 42 miss% 0.029883114149613508
plot_id,batch_id 0 43 miss% 0.07774030831947622
plot_id,batch_id 0 44 miss% 0.028125611003464265
plot_id,batch_id 0 45 miss% 0.028207565441795097
plot_id,batch_id 0 46 miss% 0.040989951200717266
plot_id,batch_id 0 47 miss% 0.03483659524557723
plot_id,batch_id 0 48 miss% 0.01991625012669448
plot_id,batch_id 0 49 miss% 0.08437202904494077
plot_id,batch_id 0 50 miss% 0.15274387949703974
plot_id,batch_id 0 51 miss% 0.03420747527124584
plot_id,batch_id 0 52 miss% 0.04254929670590321
plot_id,batch_id 0 53 miss% 0.031164350455961908
plot_id,batch_id 0 54 miss% 0.028719671554793103
plot_id,batch_id 0 55 miss% 0.058017447662696696
plot_id,batch_id 0 56 miss% 0.06078168671553762
plot_id,batch_id 0 57 miss% 0.04486627809574009
plot_id,batch_id 0 58 miss% 0.02882105685713499
plot_id,batch_id 0 59 miss% 0.028902670466772144
plot_id,batch_id 0 60 miss% 0.03536863866813242
plot_id,batch_id 0 61 miss% 0.03312720622018548
plot_id,batch_id 0 62 miss% 0.07192597641717653
plot_id,batch_id 0 63 miss% 0.0419135305521676
plot_id,batch_id 0 64 miss% 0.07763441003346282
plot_id,batch_id 0 65 miss% 0.07314318682980336
plot_id,batch_id 0 66 miss% 0.06402561386617771
plot_id,batch_id 0 67 miss% 0.025120374939155105
plot_id,batch_id 0 68 miss% 0.02652905665499237
miss% 0.0601896129052779
plot_id,batch_id 0 66 miss% 0.05567095972245838
plot_id,batch_id 0 67 miss% 0.03142376217181087
plot_id,batch_id 0 68 miss% 0.048921851412225055
plot_id,batch_id 0 69 miss% 0.11095147797054677
plot_id,batch_id 0 70 miss% 0.05284022003452821
plot_id,batch_id 0 71 miss% 0.030800959830551346
plot_id,batch_id 0 72 miss% 0.14389938946912212
plot_id,batch_id 0 73 miss% 0.06350079444493363
plot_id,batch_id 0 74 miss% 0.08016732756089266
plot_id,batch_id 0 75 miss% 0.07839077886623415
plot_id,batch_id 0 76 miss% 0.041403541355775106
plot_id,batch_id 0 77 miss% 0.06336421018358446
plot_id,batch_id 0 78 miss% 0.03499746869994204
plot_id,batch_id 0 79 miss% 0.07767121550619183
plot_id,batch_id 0 80 miss% 0.06239572171564995
plot_id,batch_id 0 81 miss% 0.09210917026036446
plot_id,batch_id 0 82 miss% 0.03772535753697703
plot_id,batch_id 0 83 miss% 0.06920427333016221
plot_id,batch_id 0 84 miss% 0.09322563733344198
plot_id,batch_id 0 85 miss% 0.044644268331893444
plot_id,batch_id 0 86 miss% 0.04393279063809634
plot_id,batch_id 0 87 miss% 0.05875565621088436
plot_id,batch_id 0 88 miss% 0.08810612187335969
plot_id,batch_id 0 89 miss% 0.06201073552972277
plot_id,batch_id 0 90 miss% 0.031619489149225806
plot_id,batch_id 0 91 miss% 0.09195632218053364
plot_id,batch_id 0 92 miss% 0.0651529788867318
plot_id,batch_id 0 93 miss% 0.05314601112991579
plot_id,batch_id 0 94 miss% 0.08606953204989097
plot_id,batch_id 0 95 miss% 0.04035165272784383
plot_id,batch_id 0 96 miss% 0.042883934201655156
plot_id,batch_id 0 97 miss% 0.05726284167095545
plot_id,batch_id 0 98 miss% 0.036826226592014136
plot_id,batch_id 0 99 miss% 0.0807570615868454
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0584632  0.06218431 0.10272632 0.07409434 0.03300556 0.05767768
 0.05168579 0.10367834 0.06636932 0.0596129  0.05038839 0.08690638
 0.08160523 0.08943325 0.10890154 0.04100027 0.11519421 0.04462225
 0.06505418 0.07500718 0.12877025 0.03775336 0.0276608  0.04990217
 0.03123873 0.06570582 0.08075799 0.04749497 0.05149886 0.03304089
 0.06445297 0.1370867  0.11594366 0.07400221 0.0497227  0.05688855
 0.13189028 0.1006893  0.07641149 0.04684327 0.08198233 0.06165404
 0.05449587 0.06115382 0.02860329 0.09368891 0.03137978 0.0273356
 0.01608919 0.02538709 0.1263951  0.04285761 0.04031745 0.02740135
 0.02697469 0.08090264 0.12725446 0.04440284 0.03701577 0.03924806
 0.05228199 0.02645297 0.04699246 0.05548851 0.05851901 0.06018961
 0.05567096 0.03142376 0.04892185 0.11095148 0.05284022 0.03080096
 0.14389939 0.06350079 0.08016733 0.07839078 0.04140354 0.06336421
 0.03499747 0.07767122 0.06239572 0.09210917 0.03772536 0.06920427
 0.09322564 0.04464427 0.04393279 0.05875566 0.08810612 0.06201074
 0.03161949 0.09195632 0.06515298 0.05314601 0.08606953 0.04035165
 0.04288393 0.05726284 0.03682623 0.08075706]
for model  165 the mean error 0.06361969813749697
all id 165 hidden_dim 24 learning_rate 0.0025 num_layers 3 frames 31 out win 4 err 0.06361969813749697 time 12767.973995447159
Launcher: Job 166 completed in 13030 seconds.
Launcher: Task 146 done. Exiting.
plot_id,batch_id 0 69 miss% 0.09366905348826672
plot_id,batch_id 0 70 miss% 0.06504037699365986
plot_id,batch_id 0 71 miss% 0.04424076449296084
plot_id,batch_id 0 72 miss% 0.07903053787949547
plot_id,batch_id 0 73 miss% 0.11128434799654219
plot_id,batch_id 0 74 miss% 0.08315434348589118
plot_id,batch_id 0 75 miss% 0.03695353846166859
plot_id,batch_id 0 76 miss% 0.07997233030720377
plot_id,batch_id 0 77 miss% 0.06045932076004668
plot_id,batch_id 0 78 miss% 0.024778761783524526
plot_id,batch_id 0 79 miss% 0.08051566263627202
plot_id,batch_id 0 80 miss% 0.03147292371441261
plot_id,batch_id 0 81 miss% 0.08204622169126909
plot_id,batch_id 0 82 miss% 0.05612085905268854
plot_id,batch_id 0 83 miss% 0.0775980248282799
plot_id,batch_id 0 84 miss% 0.08146546074608402
plot_id,batch_id 0 85 miss% 0.04729150426684055
plot_id,batch_id 0 86 miss% 0.0632861506165656
plot_id,batch_id 0 87 miss% 0.09825305994083908
plot_id,batch_id 0 88 miss% 0.09191169365213812
plot_id,batch_id 0 89 miss% 0.09769269878727517
plot_id,batch_id 0 90 miss% 0.0302093168676962
plot_id,batch_id 0 91 miss% 0.049324072952887994
plot_id,batch_id 0 92 miss% 0.03489191798298318
plot_id,batch_id 0 93 miss% 0.08818968286558475
plot_id,batch_id 0 94 miss% 0.10292567550446963
plot_id,batch_id 0 95 miss% 0.058521283709178876
plot_id,batch_id 0 96 miss% 0.04179084482772938
plot_id,batch_id 0 97 miss% 0.05485906365200154
plot_id,batch_id 0 98 miss% 0.0648082855927234
plot_id,batch_id 0 99 miss% 0.09300957096366783
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.091385   0.04174929 0.12382085 0.05722259 0.05461079 0.05065574
 0.04694131 0.1371945  0.08868397 0.05588401 0.06728042 0.05554177
 0.1186857  0.06675107 0.06672546 0.05821296 0.19615169 0.13754269
 0.05800744 0.13052375 0.06079162 0.03990545 0.05372529 0.0430681
 0.05250809 0.03963077 0.05325464 0.04975231 0.02830716 0.02578435
 0.05424072 0.12437207 0.10783382 0.06869955 0.04433045 0.03989963
 0.13129421 0.10023819 0.05443935 0.02943479 0.06534583 0.05025226
 0.02988311 0.07774031 0.02812561 0.02820757 0.04098995 0.0348366
 0.01991625 0.08437203 0.15274388 0.03420748 0.0425493  0.03116435
 0.02871967 0.05801745 0.06078169 0.04486628 0.02882106 0.02890267
 0.03536864 0.03312721 0.07192598 0.04191353 0.07763441 0.07314319
 0.06402561 0.02512037 0.02652906 0.09366905 0.06504038 0.04424076
 0.07903054 0.11128435 0.08315434 0.03695354 0.07997233 0.06045932
 0.02477876 0.08051566 0.03147292 0.08204622 0.05612086 0.07759802
 0.08146546 0.0472915  0.06328615 0.09825306 0.09191169 0.0976927
 0.03020932 0.04932407 0.03489192 0.08818968 0.10292568 0.05852128
 0.04179084 0.05485906 0.06480829 0.09300957]
for model  8 the mean error 0.06429080248349481
all id 8 hidden_dim 32 learning_rate 0.0025 num_layers 3 frames 21 out win 6 err 0.06429080248349481 time 12803.917968988419
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  55697
Epoch:0, Train loss:0.547327, valid loss:0.513868
Epoch:1, Train loss:0.149481, valid loss:0.004827
Epoch:2, Train loss:0.011859, valid loss:0.002707
Epoch:3, Train loss:0.005886, valid loss:0.002507
Epoch:4, Train loss:0.004803, valid loss:0.001965
Epoch:5, Train loss:0.004225, valid loss:0.001922
Epoch:6, Train loss:0.003901, valid loss:0.001758
Epoch:7, Train loss:0.003596, valid loss:0.001567
Epoch:8, Train loss:0.003496, valid loss:0.001826
Epoch:9, Train loss:0.003240, valid loss:0.001403
Epoch:10, Train loss:0.003028, valid loss:0.001474
Epoch:11, Train loss:0.002395, valid loss:0.001142
Epoch:12, Train loss:0.002311, valid loss:0.001272
Epoch:13, Train loss:0.002276, valid loss:0.001198
Epoch:14, Train loss:0.002149, valid loss:0.001160
Epoch:15, Train loss:0.002148, valid loss:0.001349
Epoch:16, Train loss:0.002068, valid loss:0.001145
Epoch:17, Train loss:0.002017, valid loss:0.001240
Epoch:18, Train loss:0.001970, valid loss:0.001094
Epoch:19, Train loss:0.001929, valid loss:0.001134
Epoch:20, Train loss:0.001868, valid loss:0.001030
Epoch:21, Train loss:0.001508, valid loss:0.000866
Epoch:22, Train loss:0.001455, valid loss:0.000907
Epoch:23, Train loss:0.001440, valid loss:0.000887
Epoch:24, Train loss:0.001411, valid loss:0.000875
Epoch:25, Train loss:0.001408, valid loss:0.000877
Epoch:26, Train loss:0.001336, valid loss:0.000865
Epoch:27, Train loss:0.001339, valid loss:0.000936
Epoch:28, Train loss:0.001309, valid loss:0.000832
Epoch:29, Train loss:0.001289, valid loss:0.000833
Epoch:30, Train loss:0.001288, valid loss:0.000827
Epoch:31, Train loss:0.001081, valid loss:0.000789
Epoch:32, Train loss:0.001053, valid loss:0.000803
Epoch:33, Train loss:0.001041, valid loss:0.000872
Epoch:34, Train loss:0.001041, valid loss:0.000774
Epoch:35, Train loss:0.001026, valid loss:0.000752
Epoch:36, Train loss:0.001004, valid loss:0.000740
Epoch:37, Train loss:0.001008, valid loss:0.000755
Epoch:38, Train loss:0.000992, valid loss:0.000749
Epoch:39, Train loss:0.000983, valid loss:0.000786
Epoch:40, Train loss:0.000976, valid loss:0.000765
Epoch:41, Train loss:0.000873, valid loss:0.000720
Epoch:42, Train loss:0.000859, valid loss:0.000719
Epoch:43, Train loss:0.000854, valid loss:0.000711
Epoch:44, Train loss:0.000875, valid loss:0.000708
Epoch:45, Train loss:0.000852, valid loss:0.000742
Epoch:46, Train loss:0.000840, valid loss:0.000720
Epoch:47, Train loss:0.000829, valid loss:0.000722
Epoch:48, Train loss:0.000848, valid loss:0.000722
Epoch:49, Train loss:0.000822, valid loss:0.000706
Epoch:50, Train loss:0.000824, valid loss:0.000731
Epoch:51, Train loss:0.000770, valid loss:0.000700
Epoch:52, Train loss:0.000761, valid loss:0.000694
Epoch:53, Train loss:0.000759, valid loss:0.000695
Epoch:54, Train loss:0.000756, valid loss:0.000699
Epoch:55, Train loss:0.000755, valid loss:0.000695
Epoch:56, Train loss:0.000754, valid loss:0.000693
Epoch:57, Train loss:0.000753, valid loss:0.000697
Epoch:58, Train loss:0.000752, valid loss:0.000698
Epoch:59, Train loss:0.000751, valid loss:0.000698
Epoch:60, Train loss:0.000751, valid loss:0.000697
training time 12856.760600566864
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.1059408927458796
plot_id,batch_id 0 1 miss% 0.06353803498763042
plot_id,batch_id 0 2 miss% 0.10197353302154491
plot_id,batch_id 0 3 miss% 0.06945317705798214
plot_id,batch_id 0 4 miss% 0.0980690441010621
plot_id,batch_id 0 5 miss% 0.09051503387430715
plot_id,batch_id 0 6 miss% 0.07764665087628132
plot_id,batch_id 0 7 miss% 0.11519317684646566
plot_id,batch_id 0 8 miss% 0.10002759036794151
plot_id,batch_id 0 9 miss% 0.08662155312180636
plot_id,batch_id 0 10 miss% 0.07916377564872502
plot_id,batch_id 0 11 miss% 0.08922768189327825
plot_id,batch_id 0 12 miss% 0.04834128231947823
plot_id,batch_id 0 13 miss% 0.052486343198437126
plot_id,batch_id 0 14 miss% 0.11048414575203201
plot_id,batch_id 0 15 miss% 0.07053246735653937
plot_id,batch_id 0 16 miss% 0.09731027722398862
plot_id,batch_id 0 17 miss% 0.037360729364476604
plot_id,batch_id 0 18 miss% 0.03971616044478447
plot_id,batch_id 0 19 miss% 0.07932558499498021
plot_id,batch_id 0 20 miss% 0.08701994667952669
plot_id,batch_id 0 21 miss% 0.04401093945453982
plot_id,batch_id 0 22 miss% 0.06314416354437037
plot_id,batch_id 0 23 miss% 0.05413905187376407
plot_id,batch_id 0 24 miss% 0.04604077818644605
plot_id,batch_id 0 25 miss% 0.04776661553815596
plot_id,batch_id 0 26 miss% 0.07652070413281385
plot_id,batch_id 0 27 miss% 0.0780908867101739
plot_id,batch_id 0 28 miss% 0.060744599327657495
plot_id,batch_id 0 29 miss% 0.06450181095822538
plot_id,batch_id 0 30 miss% 0.05391445948681253
plot_id,batch_id 0 31 miss% 0.1082394187836034
plot_id,batch_id 0 32 miss% 0.12775660275466968
plot_id,batch_id 0 33 miss% 0.07322370012576093
plot_id,batch_id 0 34 miss% 0.12868304879303827
plot_id,batch_id 0 35 miss% 0.07868162780186605
plot_id,batch_id 0 36 miss% 0.1036908668335852
plot_id,batch_id 0 37 miss% 0.12259603102319731
plot_id,batch_id 0 38 miss% 0.061408427080460924
plot_id,batch_id 0 39 miss% 0.050121473692300744
plot_id,batch_id 0 40 miss% 0.05346361137955334
plot_id,batch_id 0 41 miss% 0.0444851067737055
plot_id,batch_id 0 42 miss% 0.027047661062993278
plot_id,batch_id 0 43 miss% 0.04812468642810883
plot_id,batch_id 0 44 miss% 0.05615186747452209
plot_id,batch_id 0 45 miss% 0.06943521539701433
plot_id,batch_id 0 46 miss% 0.05164473069948732
plot_id,batch_id 0 47 miss% 0.050886442695487186
plot_id,batch_id 0 48 miss% 0.06907938295121352
plot_id,batch_id 0 49 miss% 0.03810635691401107
plot_id,batch_id 0 50 miss% 0.047795397425831186
plot_id,batch_id 0 51 miss% 0.053757230338779355
plot_id,batch_id 0 52 miss% 0.06431872696898161
plot_id,batch_id 0 53 miss% 0.04039520707663231
plot_id,batch_id 0 54 miss% 0.09537053003216603
plot_id,batch_id 0 55 miss% 0.09780726532683474
plot_id,batch_id 0 56 miss% 0.10784020501507552
plot_id,batch_id 0 57 miss% 0.07535654795295565
plot_id,batch_id 0 58 miss% 0.08683406496731765
plot_id,batch_id 0 59 miss% 0.05944434388466723
plot_id,batch_id 0 60 miss% 0.057813844666171746
plot_id,batch_id 0 61 miss% 0.06712306820357239
plot_id,batch_id 0 62 miss% 0.08475592498603154
plot_id,batch_id 0 63 miss% 0.05439341023994942
plot_id,batch_id 0 64 miss% 0.07294230109798587
plot_id,batch_id 0 65 miss% 0.18297515996718652
plot_id,batch_id 0 66 miss% 0.1380011768050022
plot_id,batch_id 0 67 miss% 0.04098809831519107
plot_id,batch_id 0 68 miss% 0.04722639002142996
plot_id,batch_idLauncher: Job 9 completed in 13076 seconds.
Launcher: Task 68 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  89105
Epoch:0, Train loss:0.536096, valid loss:0.493196
Epoch:1, Train loss:0.093232, valid loss:0.004975
Epoch:2, Train loss:0.013032, valid loss:0.003779
Epoch:3, Train loss:0.008048, valid loss:0.002424
Epoch:4, Train loss:0.004405, valid loss:0.001846
Epoch:5, Train loss:0.003549, valid loss:0.001658
Epoch:6, Train loss:0.003106, valid loss:0.001490
Epoch:7, Train loss:0.002825, valid loss:0.001403
Epoch:8, Train loss:0.002575, valid loss:0.001204
Epoch:9, Train loss:0.002513, valid loss:0.001363
Epoch:10, Train loss:0.002236, valid loss:0.001286
Epoch:11, Train loss:0.001786, valid loss:0.000998
Epoch:12, Train loss:0.001712, valid loss:0.000997
Epoch:13, Train loss:0.001634, valid loss:0.001183
Epoch:14, Train loss:0.001606, valid loss:0.001300
Epoch:15, Train loss:0.001527, valid loss:0.000968
Epoch:16, Train loss:0.001493, valid loss:0.000978
Epoch:17, Train loss:0.001430, valid loss:0.000943
Epoch:18, Train loss:0.001392, valid loss:0.000796
Epoch:19, Train loss:0.001338, valid loss:0.000837
Epoch:20, Train loss:0.001316, valid loss:0.000841
Epoch:21, Train loss:0.001082, valid loss:0.000787
Epoch:22, Train loss:0.001067, valid loss:0.000696
Epoch:23, Train loss:0.001030, valid loss:0.000735
Epoch:24, Train loss:0.001024, valid loss:0.000748
Epoch:25, Train loss:0.001004, valid loss:0.000757
Epoch:26, Train loss:0.000980, valid loss:0.000726
Epoch:27, Train loss:0.001009, valid loss:0.000774
Epoch:28, Train loss:0.000939, valid loss:0.000786
Epoch:29, Train loss:0.000982, valid loss:0.000682
Epoch:30, Train loss:0.000932, valid loss:0.000692
Epoch:31, Train loss:0.000814, valid loss:0.000668
Epoch:32, Train loss:0.000806, valid loss:0.000649
Epoch:33, Train loss:0.000797, valid loss:0.000643
Epoch:34, Train loss:0.000800, valid loss:0.000630
Epoch:35, Train loss:0.000782, valid loss:0.000613
Epoch:36, Train loss:0.000775, valid loss:0.000611
Epoch:37, Train loss:0.000774, valid loss:0.000611
Epoch:38, Train loss:0.000757, valid loss:0.000640
Epoch:39, Train loss:0.000755, valid loss:0.000605
Epoch:40, Train loss:0.000751, valid loss:0.000635
Epoch:41, Train loss:0.000689, valid loss:0.000585
Epoch:42, Train loss:0.000687, valid loss:0.000618
Epoch:43, Train loss:0.000679, valid loss:0.000590
Epoch:44, Train loss:0.000678, valid loss:0.000582
Epoch:45, Train loss:0.000681, valid loss:0.000613
Epoch:46, Train loss:0.000677, valid loss:0.000601
Epoch:47, Train loss:0.000668, valid loss:0.000608
Epoch:48, Train loss:0.000660, valid loss:0.000587
Epoch:49, Train loss:0.000658, valid loss:0.000622
Epoch:50, Train loss:0.000659, valid loss:0.000600
Epoch:51, Train loss:0.000628, valid loss:0.000583
Epoch:52, Train loss:0.000623, valid loss:0.000576
Epoch:53, Train loss:0.000621, valid loss:0.000585
Epoch:54, Train loss:0.000619, valid loss:0.000579
Epoch:55, Train loss:0.000617, valid loss:0.000578
Epoch:56, Train loss:0.000616, valid loss:0.000579
Epoch:57, Train loss:0.000616, valid loss:0.000580
Epoch:58, Train loss:0.000616, valid loss:0.000576
Epoch:59, Train loss:0.000615, valid loss:0.000578
Epoch:60, Train loss:0.000614, valid loss:0.000579
training time 12880.196979045868
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.3259121295963513
plot_id,batch_id 0 1 miss% 0.3930770751459912
plot_id,batch_id 0 2 miss% 0.46740369466088133
plot_id,batch_id 0 3 miss% 0.37454519605614206
plot_id,batch_id 0 4 miss% 0.42964373367691916
plot_id,batch_id 0 5 miss% 0.34950686228951366
plot_id,batch_id 0 6 miss% 0.318350133406749
plot_id,batch_id 0 7 miss% 0.4968806578370459
plot_id,batch_id 0 8 miss% 0.5788962907567939
plot_id,batch_id 0 9 miss% 0.5966839829397407
plot_id,batch_id 0 10 miss% 0.30638794343244313
plot_id,batch_id 0 11 miss% 0.3325298845835098
plot_id,batch_id 0 12 miss% 0.33560424963691915
plot_id,batch_id 0 13 miss% 0.31832365007078484
plot_id,batch_id 0 14 miss% 0.49193109559720155
plot_id,batch_id 0 15 miss% 0.3682703251637218
plot_id,batch_id 0 16 miss% 0.5113733607638835
plot_id,batch_id 0 17 miss% 0.45669291701592063
plot_id,batch_id 0 18 miss% 0.43626292097005215
plot_id,batch_id 0 19 miss% 0.40563315595660804
plot_id,batch_id 0 20 miss% 0.3429016824247604
plot_id,batch_id 0 21 miss% 0.574200982992223
plot_id,batch_id 0 22 miss% 0.5017217963647589
plot_id,batch_id 0 23 miss% 0.4706014947280211
plot_id,batch_id 0 24 miss% 0.3981738974261049
plot_id,batch_id 0 25 miss% 0.32003104411979943
plot_id,batch_id 0 26 miss% 0.45765120790473307
plot_id,batch_id 0 27 miss% 0.42283842258894966
plot_id,batch_id 0 28 miss% 0.46410620739403496
plot_id,batch_id 0 29 miss% 0.38238227626155397
plot_id,batch_id 0 30 miss% 0.3011891570928892
plot_id,batch_id 0 31 miss% 0.4464544934890817
plot_id,batch_id 0 32 miss% 0.466940503251915
plot_id,batch_id 0 33 miss% 0.46157236068041313
plot_id,batch_id 0 34 miss% 0.42015418596961845
plot_id,batch_id 0 35 miss% 0.3886658102060926
plot_id,batch_id 0 36 miss% 0.44682407633627397
plot_id,batch_id 0 37 miss% 0.5444843921331018
plot_id,batch_id 0 38 miss% 0.4526606563805339
plot_id,batch_id 0 39 miss% 0.6051392259916666
plot_id,batch_id 0 40 miss% 0.3433579769223917
plot_id,batch_id 0 41 miss% 0.40316774706080705
plot_id,batch_id 0 42 miss% 0.3768091475999005
plot_id,batch_id 0 43 miss% 0.4813645479813188
plot_id,batch_id 0 44 miss% 0.39774066712995676
plot_id,batch_id 0 45 miss% 0.3426535446697981
plot_id,batch_id 0 46 miss% 0.42693554094062397
plot_id,batch_id 0 47 miss% 0.47223464636173124
plot_id,batch_id 0 48 miss% 0.5013553635679757
plot_id,batch_id 0 49 miss% 0.3707406039905847
plot_id,batch_id 0 50 miss% 0.5985639279843568
plot_id,batch_id 0 51 miss% 0.5099971999839494
plot_id,batch_id 0 52 miss% 0.4614955002995744
plot_id,batch_id 0 53 miss% 0.4392713503599021
plot_id,batch_id 0 54 miss% 0.4547661537783048
plot_id,batch_id 0 55 miss% 0.46939522308463544
plot_id,batch_id 0 56 miss% 0.5818468828782415
plot_id,batch_id 0 57 miss% 0.5381556100811469
plot_id,batch_id 0 58 miss% 0.4855380579612127
plot_id,batch_id 0 59 miss% 0.4944415868687457
plot_id,batch_id 0 60 miss% 0.3123936645767913
plot_id,batch_id 0 61 miss% 0.29283722566200066
plot_id,batch_id 0 62 miss% 0.45386820593461813
plot_id,batch_id 0 63 miss% 0.41981417171842744
plot_id,batch_id 0 64 miss% 0.42596568731799095
plot_id,batch_id 0 65 miss% 0.35568212901258395
plot_id,batch_id 0 66 miss% 0.4065665744056736
plot_id,batch_id 0 67 miss% 0.32851373801703065
plot_id,batch_id 0 68 miss% 0.4350379314764935
plot_id,batch_id 0 69 miss% 0.4585249217675601
plot_id,batch_id 0 70 miss% 0.3374623613064744
plot_id,batch_id 0 71 miss% 0.33832012239231224
plot_id,batch_id 0 72 miss% 0.3933727440011021
plot_id,batch_id 0 73 miss% 0.33870384415817567
plot_id,batch_id 0 74 miss% 0.32581264877029736
plot_id,batch_id 0 75 miss% 0.2757332490723553
plot_id,batch_id 0 76 miss% 0.3623562260037822
plot_id,batch_id 0 77 miss% 0.2968628081907771
plot_id,batch_id 0 78 miss% 0.3603061919262699
plot_id,batch_id 0 79 miss% 0.4706158343225108
plot_id,batch_id 0 80 miss% 0.3303507648348718
plot_id,batch_id 0 81 miss% 0.5024104049495761
plot_id,batch_id 0 82 miss% 0.392082324172468
plot_id,batch_id 0 83 miss% 0.431936050353483
plot_id,batch_id 0 84 miss% 0.43934849224636463
plot_id,batch_id 0 85 miss% 0.2678574227113277
plot_id,batch_id 0 86 miss% 0.3260401783823221
plot_id,batch_id 0 87 miss% 0.40376093559780346
plot_id,batch_id 0 88 miss% 0.3872861374375666
plot_id,batch_id 0 89 miss% 0.34865595257547216
plot_id,batch_id 0 90 miss% 0.2386762523223875
plot_id,batch_id 0 91 miss% 0.3714677824725366
plot_id,batch_id 0 92 miss% 0.3780736787803586
plot_id,batch_id 0 93 miss% 0.2821481121828998
plot_id,batch_id 0 94 miss% 0.5094913133301663
plot_id,batch_id 0 95 miss% 0.273438120389069
plot_id,batch_id 0 96 miss% 0.35597529524363947
plot_id,batch_id 0 97 miss% 0.43577191997530323
plot_id,batch_id 0 98 miss% 0.36012267317634483
plot_id,batch_id 0 99 miss% 0.3860528235725525
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.32591213 0.39307708 0.46740369 0.3745452  0.42964373 0.34950686
 0.31835013 0.49688066 0.57889629 0.59668398 0.30638794 0.33252988
 0.33560425 0.31832365 0.4919311  0.36827033 0.51137336 0.45669292
 0.43626292 0.40563316 0.34290168 0.57420098 0.5017218  0.47060149
 0.3981739  0.32003104 0.45765121 0.42283842 0.46410621 0.38238228
 0.30118916 0.44645449 0.4669405  0.46157236 0.42015419 0.38866581
 0.44682408 0.54448439 0.45266066 0.60513923 0.34335798 0.40316775
 0.37680915 0.48136455 0.39774067 0.34265354 0.42693554 0.47223465
 0.50135536 0.3707406  0.59856393 0.5099972  0.4614955  0.43927135
 0.45476615 0.46939522 0.58184688 0.53815561 0.48553806 0.49444159
 0.31239366 0.29283723 0.45386821 0.41981417 0.42596569 0.35568213
 0.40656657 0.32851374 0.43503793 0.45852492 0.33746236 0.33832012
 0.39337274 0.33870384 0.32581265 0.27573325 0.36235623 0.29686281
 0.36030619 0.47061583 0.33035076 0.5024104  0.39208232 0.43193605
 0.43934849 0.26785742 0.32604018 0.40376094 0.38728614 0.34865595
 0.23867625 0.37146778 0.37807368 0.28214811 0.50949131 0.27343812
 0.3559753  0.43577192 0.36012267 0.38605282]
for model  85 the mean error 0.41122131327538597
all id 85 hidden_dim 24 learning_rate 0.0025 num_layers 3 frames 25 out win 5 err 0.41122131327538597 time 12880.196979045868
Launcher: Job 86 completed in 13118 seconds.
Launcher: Task 121 done. Exiting.
 0 69 miss% 0.11285031774818273
plot_id,batch_id 0 70 miss% 0.06410016341289901
plot_id,batch_id 0 71 miss% 0.04329524943350882
plot_id,batch_id 0 72 miss% 0.09767826581284306
plot_id,batch_id 0 73 miss% 0.052574725437912466
plot_id,batch_id 0 74 miss% 0.11659643196376764
plot_id,batch_id 0 75 miss% 0.10437200757529162
plot_id,batch_id 0 76 miss% 0.09990090881149677
plot_id,batch_id 0 77 miss% 0.04787989616973114
plot_id,batch_id 0 78 miss% 0.05788671799402939
plot_id,batch_id 0 79 miss% 0.13479681581701167
plot_id,batch_id 0 80 miss% 0.03628990901137224
plot_id,batch_id 0 81 miss% 0.0631342061475412
plot_id,batch_id 0 82 miss% 0.08557449358233846
plot_id,batch_id 0 83 miss% 0.07425358286172996
plot_id,batch_id 0 84 miss% 0.13627569630784
plot_id,batch_id 0 85 miss% 0.04941179000236039
plot_id,batch_id 0 86 miss% 0.04879923666420597
plot_id,batch_id 0 87 miss% 0.091835251736255
plot_id,batch_id 0 88 miss% 0.06730373466536932
plot_id,batch_id 0 89 miss% 0.08778094507350895
plot_id,batch_id 0 90 miss% 0.044362473499910425
plot_id,batch_id 0 91 miss% 0.06346375831375643
plot_id,batch_id 0 92 miss% 0.050437330439569605
plot_id,batch_id 0 93 miss% 0.06797835607863895
plot_id,batch_id 0 94 miss% 0.0674236287757283
plot_id,batch_id 0 95 miss% 0.06099206598599843
plot_id,batch_id 0 96 miss% 0.056773641865227854
plot_id,batch_id 0 97 miss% 0.0591420531301163
plot_id,batch_id 0 98 miss% 0.058463019966527245
plot_id,batch_id 0 99 miss% 0.09870816537606714
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10594089 0.06353803 0.10197353 0.06945318 0.09806904 0.09051503
 0.07764665 0.11519318 0.10002759 0.08662155 0.07916378 0.08922768
 0.04834128 0.05248634 0.11048415 0.07053247 0.09731028 0.03736073
 0.03971616 0.07932558 0.08701995 0.04401094 0.06314416 0.05413905
 0.04604078 0.04776662 0.0765207  0.07809089 0.0607446  0.06450181
 0.05391446 0.10823942 0.1277566  0.0732237  0.12868305 0.07868163
 0.10369087 0.12259603 0.06140843 0.05012147 0.05346361 0.04448511
 0.02704766 0.04812469 0.05615187 0.06943522 0.05164473 0.05088644
 0.06907938 0.03810636 0.0477954  0.05375723 0.06431873 0.04039521
 0.09537053 0.09780727 0.10784021 0.07535655 0.08683406 0.05944434
 0.05781384 0.06712307 0.08475592 0.05439341 0.0729423  0.18297516
 0.13800118 0.0409881  0.04722639 0.11285032 0.06410016 0.04329525
 0.09767827 0.05257473 0.11659643 0.10437201 0.09990091 0.0478799
 0.05788672 0.13479682 0.03628991 0.06313421 0.08557449 0.07425358
 0.1362757  0.04941179 0.04879924 0.09183525 0.06730373 0.08778095
 0.04436247 0.06346376 0.05043733 0.06797836 0.06742363 0.06099207
 0.05677364 0.05914205 0.05846302 0.09870817]
for model  146 the mean error 0.07427121082707183
all id 146 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 25 out win 6 err 0.07427121082707183 time 12856.760600566864
Launcher: Job 147 completed in 13118 seconds.
Launcher: Task 130 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  41745
Epoch:0, Train loss:0.449807, valid loss:0.435074
Epoch:1, Train loss:0.016465, valid loss:0.001888
Epoch:2, Train loss:0.003686, valid loss:0.001628
Epoch:3, Train loss:0.003023, valid loss:0.001281
Epoch:4, Train loss:0.002609, valid loss:0.001325
Epoch:5, Train loss:0.002405, valid loss:0.001092
Epoch:6, Train loss:0.002216, valid loss:0.001004
Epoch:7, Train loss:0.002094, valid loss:0.001121
Epoch:8, Train loss:0.002015, valid loss:0.000899
Epoch:9, Train loss:0.001917, valid loss:0.000990
Epoch:10, Train loss:0.001836, valid loss:0.000921
Epoch:11, Train loss:0.001415, valid loss:0.000725
Epoch:12, Train loss:0.001360, valid loss:0.000863
Epoch:13, Train loss:0.001306, valid loss:0.000743
Epoch:14, Train loss:0.001318, valid loss:0.000727
Epoch:15, Train loss:0.001263, valid loss:0.000704
Epoch:16, Train loss:0.001231, valid loss:0.000661
Epoch:17, Train loss:0.001172, valid loss:0.000656
Epoch:18, Train loss:0.001194, valid loss:0.000705
Epoch:19, Train loss:0.001136, valid loss:0.000710
Epoch:20, Train loss:0.001133, valid loss:0.000639
Epoch:21, Train loss:0.000901, valid loss:0.000554
Epoch:22, Train loss:0.000890, valid loss:0.000589
Epoch:23, Train loss:0.000891, valid loss:0.000556
Epoch:24, Train loss:0.000872, valid loss:0.000580
Epoch:25, Train loss:0.000838, valid loss:0.000644
Epoch:26, Train loss:0.000839, valid loss:0.000564
Epoch:27, Train loss:0.000830, valid loss:0.000591
Epoch:28, Train loss:0.000819, valid loss:0.000578
Epoch:29, Train loss:0.000807, valid loss:0.000577
Epoch:30, Train loss:0.000805, valid loss:0.000580
Epoch:31, Train loss:0.000688, valid loss:0.000510
Epoch:32, Train loss:0.000676, valid loss:0.000534
Epoch:33, Train loss:0.000669, valid loss:0.000539
Epoch:34, Train loss:0.000675, valid loss:0.000569
Epoch:35, Train loss:0.000668, valid loss:0.000526
Epoch:36, Train loss:0.000657, valid loss:0.000535
Epoch:37, Train loss:0.000649, valid loss:0.000567
Epoch:38, Train loss:0.000643, valid loss:0.000564
Epoch:39, Train loss:0.000650, valid loss:0.000542
Epoch:40, Train loss:0.000644, valid loss:0.000565
Epoch:41, Train loss:0.000576, valid loss:0.000518
Epoch:42, Train loss:0.000573, valid loss:0.000514
Epoch:43, Train loss:0.000570, valid loss:0.000512
Epoch:44, Train loss:0.000568, valid loss:0.000502
Epoch:45, Train loss:0.000568, valid loss:0.000504
Epoch:46, Train loss:0.000563, valid loss:0.000523
Epoch:47, Train loss:0.000561, valid loss:0.000516
Epoch:48, Train loss:0.000561, valid loss:0.000509
Epoch:49, Train loss:0.000555, valid loss:0.000497
Epoch:50, Train loss:0.000553, valid loss:0.000503
Epoch:51, Train loss:0.000519, valid loss:0.000502
Epoch:52, Train loss:0.000515, valid loss:0.000500
Epoch:53, Train loss:0.000514, valid loss:0.000498
Epoch:54, Train loss:0.000513, valid loss:0.000494
Epoch:55, Train loss:0.000513, valid loss:0.000493
Epoch:56, Train loss:0.000512, valid loss:0.000494
Epoch:57, Train loss:0.000512, valid loss:0.000492
Epoch:58, Train loss:0.000511, valid loss:0.000492
Epoch:59, Train loss:0.000511, valid loss:0.000493
Epoch:60, Train loss:0.000511, valid loss:0.000491
training time 13061.485087633133
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3296258822140525
plot_id,batch_id 0 1 miss% 0.45740270542527117
plot_id,batch_id 0 2 miss% 0.4317989475109353
plot_id,batch_id 0 3 miss% 0.452830358840735
plot_id,batch_id 0 4 miss% 0.37610711404484837
plot_id,batch_id 0 5 miss% 0.42224856198067584
plot_id,batch_id 0 6 miss% 0.3233071940140102
plot_id,batch_id 0 7 miss% 0.4828050977164313
plot_id,batch_id 0 8 miss% 0.4653977433520644
plot_id,batch_id 0 9 miss% 0.43525875014574705
plot_id,batch_id 0 10 miss% 0.3149230057818168
plot_id,batch_id 0 11 miss% 0.3907684176645346
plot_id,batch_id 0 12 miss% 0.35577300403279083
plot_id,batch_id 0 13 miss% 0.29958613305166815
plot_id,batch_id 0 14 miss% 0.4419920424589766
plot_id,batch_id 0 15 miss% 0.27519593526736796
plot_id,batch_id 0 16 miss% 0.5115751748575124
plot_id,batch_id 0 17 miss% 0.40436368268839884
plot_id,batch_id 0 18 miss% 0.3839979721295209
plot_id,batch_id 0 19 miss% 0.39950924206471866
plot_id,batch_id 0 20 miss% 0.38232623723067044
plot_id,batch_id 0 21 miss% 0.48722929618036265
plot_id,batch_id 0 22 miss% 0.44547636418625053
plot_id,batch_id 0 23 miss% 0.5127607314147846
plot_id,batch_id 0 24 miss% 0.5026659881080847
plot_id,batch_id 0 25 miss% 0.3550223430003104
plot_id,batch_id 0 26 miss% 0.4918078789877754
plot_id,batch_id 0 27 miss% 0.4170771497744328
plot_id,batch_id 0 28 miss% 0.4541807477046965
plot_id,batch_id 0 29 miss% 0.4594812859916773
plot_id,batch_id 0 30 miss% 0.3138167936089207
plot_id,batch_id 0 31 miss% 0.39277294345346
plot_id,batch_id 0 32 miss% 0.4030864854762085
plot_id,batch_id 0 33 miss% 0.4339709736925548
plot_id,batch_id 0 34 miss% 0.4097061625406845
plot_id,batch_id 0 35 miss% 0.38027410883730417
plot_id,batch_id 0 36 miss% 0.395223309999972
plot_id,batch_id 0 37 miss% 0.38629521994933985
plot_id,batch_id 0 38 miss% 0.4646407095113432
plot_id,batch_id 0 39 miss% 0.33352589169247443
plot_id,batch_id 0 40 miss% 0.26190375776013375
plot_id,batch_id 0 41 miss% 0.3245982682894308
plot_id,batch_id 0 42 miss% 0.41209050517553775
plot_id,batch_id 0 43 miss% 0.4990965883316893
plot_id,batch_id 0 44 miss% 0.3873382800392271
plot_id,batch_id 0 45 miss% 0.3231156309818525
plot_id,batch_id 0 46 miss% 0.41421054183812084
plot_id,batch_id 0 47 miss% 0.3597828929547892
plot_id,batch_id 0 48 miss% 0.38031386795184896
plot_id,batch_id 0 49 miss% 0.2314742331149331
plot_id,batch_id 0 50 miss% 0.6054197843377691
plot_id,batch_id 0 51 miss% 0.449010784232321
plot_id,batch_id 0 52 miss% 0.44892006815627783
plot_id,batch_id 0 53 miss% 0.22515956138380572
plot_id,batch_id 0 54 miss% 0.33995005782492677
plot_id,batch_id 0 55 miss% 0.3314967244389382
plot_id,batch_id 0 56 miss% 0.4139812723519046
plot_id,batch_id 0 57 miss% 0.3844651688652115
plot_id,batch_id 0 58 miss% 0.37529160241374904
plot_id,batch_id 0 59 miss% 0.41315034300595693
plot_id,batch_id 0 60 miss% 0.3598821241711889
plot_id,batch_id 0 61 miss% 0.2777367355672455
plot_id,batch_id 0 62 miss% 0.38922031446052846
plot_id,batch_id 0 63 miss% 0.3738883610263534
plot_id,batch_id 0 64 miss% 0.4834520092292871
plot_id,batch_id 0 65 miss% 0.2976507872053542
plot_id,batch_id 0 66 miss% 0.37578028623633164
plot_id,batch_id 0 67 miss% 0.28378778112769304
plot_id,batch_id 0 68 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  154129
Epoch:0, Train loss:0.560482, valid loss:0.581801
Epoch:1, Train loss:0.062491, valid loss:0.004011
Epoch:2, Train loss:0.008689, valid loss:0.002510
Epoch:3, Train loss:0.005345, valid loss:0.001784
Epoch:4, Train loss:0.004426, valid loss:0.001487
Epoch:5, Train loss:0.004069, valid loss:0.001561
Epoch:6, Train loss:0.003040, valid loss:0.001117
Epoch:7, Train loss:0.002294, valid loss:0.001174
Epoch:8, Train loss:0.002105, valid loss:0.001022
Epoch:9, Train loss:0.001956, valid loss:0.001053
Epoch:10, Train loss:0.001863, valid loss:0.001114
Epoch:11, Train loss:0.001353, valid loss:0.000705
Epoch:12, Train loss:0.001283, valid loss:0.000902
Epoch:13, Train loss:0.001276, valid loss:0.000720
Epoch:14, Train loss:0.001212, valid loss:0.000946
Epoch:15, Train loss:0.001199, valid loss:0.000737
Epoch:16, Train loss:0.001168, valid loss:0.000819
Epoch:17, Train loss:0.001118, valid loss:0.000686
Epoch:18, Train loss:0.001067, valid loss:0.000695
Epoch:19, Train loss:0.001095, valid loss:0.000677
Epoch:20, Train loss:0.001048, valid loss:0.000722
Epoch:21, Train loss:0.000799, valid loss:0.000559
Epoch:22, Train loss:0.000760, valid loss:0.000530
Epoch:23, Train loss:0.000745, valid loss:0.000549
Epoch:24, Train loss:0.000722, valid loss:0.000561
Epoch:25, Train loss:0.000760, valid loss:0.000544
Epoch:26, Train loss:0.000722, valid loss:0.000523
Epoch:27, Train loss:0.000700, valid loss:0.000676
Epoch:28, Train loss:0.000690, valid loss:0.000549
Epoch:29, Train loss:0.000694, valid loss:0.000503
Epoch:30, Train loss:0.000681, valid loss:0.000556
Epoch:31, Train loss:0.000558, valid loss:0.000511
Epoch:32, Train loss:0.000547, valid loss:0.000480
Epoch:33, Train loss:0.000538, valid loss:0.000490
Epoch:34, Train loss:0.000537, valid loss:0.000478
Epoch:35, Train loss:0.000536, valid loss:0.000481
Epoch:36, Train loss:0.000544, valid loss:0.000508
Epoch:37, Train loss:0.000511, valid loss:0.000491
Epoch:38, Train loss:0.000508, valid loss:0.000518
Epoch:39, Train loss:0.000517, valid loss:0.000486
Epoch:40, Train loss:0.000497, valid loss:0.000523
Epoch:41, Train loss:0.000458, valid loss:0.000498
Epoch:42, Train loss:0.000455, valid loss:0.000482
Epoch:43, Train loss:0.000444, valid loss:0.000494
Epoch:44, Train loss:0.000445, valid loss:0.000462
Epoch:45, Train loss:0.000442, valid loss:0.000483
Epoch:46, Train loss:0.000437, valid loss:0.000499
Epoch:47, Train loss:0.000439, valid loss:0.000471
Epoch:48, Train loss:0.000432, valid loss:0.000475
Epoch:49, Train loss:0.000432, valid loss:0.000459
Epoch:50, Train loss:0.000424, valid loss:0.000492
Epoch:51, Train loss:0.000410, valid loss:0.000478
Epoch:52, Train loss:0.000406, valid loss:0.000479
Epoch:53, Train loss:0.000404, valid loss:0.000476
Epoch:54, Train loss:0.000403, valid loss:0.000472
Epoch:55, Train loss:0.000401, valid loss:0.000469
Epoch:56, Train loss:0.000401, valid loss:0.000479
Epoch:57, Train loss:0.000399, valid loss:0.000470
Epoch:58, Train loss:0.000399, valid loss:0.000469
Epoch:59, Train loss:0.000398, valid loss:0.000470
Epoch:60, Train loss:0.000397, valid loss:0.000468
training time 13052.279372215271
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.0610328719591144
plot_id,batch_id 0 1 miss% 0.06334207363944647
plot_id,batch_id 0 2 miss% 0.0953960621022293
plot_id,batch_id 0 3 miss% 0.045947378400054806
plot_id,batch_id 0 4 miss% 0.05671671290619654
plot_id,batch_id 0 5 miss% 0.052742397446888765
plot_id,batch_id 0 6 miss% 0.044363361382779326
plot_id,batch_id 0 7 miss% 0.08738704644549958
plot_id,batch_id 0 8 miss% 0.08516323246747914
plot_id,batch_id 0 9 miss% 0.018160791475021847
plot_id,batch_id 0 10 miss% 0.028656914181292
plot_id,batch_id 0 11 miss% 0.048006393160373924
plot_id,batch_id 0 12 miss% 0.07745100488176802
plot_id,batch_id 0 13 miss% 0.04180364647191301
plot_id,batch_id 0 14 miss% 0.07001210784948753
plot_id,batch_id 0 15 miss% 0.03971211417940089
plot_id,batch_id 0 16 miss% 0.07098993190013658
plot_id,batch_id 0 17 miss% 0.044897567631425314
plot_id,batch_id 0 18 miss% 0.057161869641596114
plot_id,batch_id 0 19 miss% 0.1030642307912989
plot_id,batch_id 0 20 miss% 0.041423708360556505
plot_id,batch_id 0 21 miss% 0.041658210326291675
plot_id,batch_id 0 22 miss% 0.06244079303126418
plot_id,batch_id 0 23 miss% 0.03833836522567895
plot_id,batch_id 0 24 miss% 0.05128194849708646
plot_id,batch_id 0 25 miss% 0.04447464512479744
plot_id,batch_id 0 26 miss% 0.04710575580699686
plot_id,batch_id 0 27 miss% 0.04693831235318586
plot_id,batch_id 0 28 miss% 0.02870720366336612
plot_id,batch_id 0 29 miss% 0.025071108895609318
plot_id,batch_id 0 30 miss% 0.022051450653441985
plot_id,batch_id 0 31 miss% 0.09686080579195072
plot_id,batch_id 0 32 miss% 0.11256918088642585
plot_id,batch_id 0 33 miss% 0.06648764145259259
plot_id,batch_id 0 34 miss% 0.021048347738623574
plot_id,batch_id 0 35 miss% 0.03653265663678523
plot_id,batch_id 0 36 miss% 0.06320352299444659
plot_id,batch_id 0 37 miss% 0.09132132451945567
plot_id,batch_id 0 38 miss% 0.05400532614787172
plot_id,batch_id 0 39 miss% 0.042495207477194964
plot_id,batch_id 0 40 miss% 0.05466695851176661
plot_id,batch_id 0 41 miss% 0.04966515861557278
plot_id,batch_id 0 42 miss% 0.018655279865418616
plot_id,batch_id 0 43 miss% 0.03329074735504463
plot_id,batch_id 0 44 miss% 0.03673891537664003
plot_id,batch_id 0 45 miss% 0.046065903088790265
plot_id,batch_id 0 46 miss% 0.031234130188306886
plot_id,batch_id 0 47 miss% 0.020510346887339683
plot_id,batch_id 0 48 miss% 0.030881101884102206
plot_id,batch_id 0 49 miss% 0.023528773909612135
plot_id,batch_id 0 50 miss% 0.12438863354569925
plot_id,batch_id 0 51 miss% 0.044337344653874365
plot_id,batch_id 0 52 miss% 0.03846388331932374
plot_id,batch_id 0 53 miss% 0.028532219160477545
plot_id,batch_id 0 54 miss% 0.0482717260030536
plot_id,batch_id 0 55 miss% 0.056462198703292235
plot_id,batch_id 0 56 miss% 0.08493139638958078
plot_id,batch_id 0 57 miss% 0.024990669919079226
plot_id,batch_id 0 58 miss% 0.0335759453504731
plot_id,batch_id 0 59 miss% 0.03970373858653633
plot_id,batch_id 0 60 miss% 0.04738359370462403
plot_id,batch_id 0 61 miss% 0.020982148119683575
plot_id,batch_id 0 62 miss% 0.03165705310941581
plot_id,batch_id 0 63 miss% 0.03410792441635202
plot_id,batch_id 0 64 miss% 0.05592484146987143
plot_id,batch_id 0 65 miss% 0.03176984755800242
plot_id,batch_id 0 66 miss% 0.12537347744775884
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  89105
Epoch:0, Train loss:0.423104, valid loss:0.381201
Epoch:1, Train loss:0.034574, valid loss:0.001837
Epoch:2, Train loss:0.003587, valid loss:0.001367
Epoch:3, Train loss:0.002494, valid loss:0.001073
Epoch:4, Train loss:0.002164, valid loss:0.001063
Epoch:5, Train loss:0.001920, valid loss:0.000941
Epoch:6, Train loss:0.001761, valid loss:0.001043
Epoch:7, Train loss:0.001709, valid loss:0.000806
Epoch:8, Train loss:0.001504, valid loss:0.000719
Epoch:9, Train loss:0.001416, valid loss:0.000824
Epoch:10, Train loss:0.001356, valid loss:0.000704
Epoch:11, Train loss:0.001019, valid loss:0.000516
Epoch:12, Train loss:0.000993, valid loss:0.000644
Epoch:13, Train loss:0.000964, valid loss:0.000497
Epoch:14, Train loss:0.000931, valid loss:0.000515
Epoch:15, Train loss:0.000923, valid loss:0.000540
Epoch:16, Train loss:0.000898, valid loss:0.000511
Epoch:17, Train loss:0.000866, valid loss:0.000585
Epoch:18, Train loss:0.000854, valid loss:0.000571
Epoch:19, Train loss:0.000844, valid loss:0.000565
Epoch:20, Train loss:0.000814, valid loss:0.000542
Epoch:21, Train loss:0.000652, valid loss:0.000409
Epoch:22, Train loss:0.000640, valid loss:0.000464
Epoch:23, Train loss:0.000631, valid loss:0.000452
Epoch:24, Train loss:0.000612, valid loss:0.000460
Epoch:25, Train loss:0.000613, valid loss:0.000432
Epoch:26, Train loss:0.000591, valid loss:0.000448
Epoch:27, Train loss:0.000596, valid loss:0.000394
Epoch:28, Train loss:0.000588, valid loss:0.000453
Epoch:29, Train loss:0.000581, valid loss:0.000445
Epoch:30, Train loss:0.000583, valid loss:0.000425
Epoch:31, Train loss:0.000494, valid loss:0.000394
Epoch:32, Train loss:0.000481, valid loss:0.000428
Epoch:33, Train loss:0.000483, valid loss:0.000410
Epoch:34, Train loss:0.000479, valid loss:0.000397
Epoch:35, Train loss:0.000474, valid loss:0.000407
Epoch:36, Train loss:0.000465, valid loss:0.000408
Epoch:37, Train loss:0.000461, valid loss:0.000388
Epoch:38, Train loss:0.000458, valid loss:0.000421
Epoch:39, Train loss:0.000464, valid loss:0.000393
Epoch:40, Train loss:0.000463, valid loss:0.000411
Epoch:41, Train loss:0.000416, valid loss:0.000373
Epoch:42, Train loss:0.000410, valid loss:0.000391
Epoch:43, Train loss:0.000413, valid loss:0.000379
Epoch:44, Train loss:0.000407, valid loss:0.000389
Epoch:45, Train loss:0.000410, valid loss:0.000375
Epoch:46, Train loss:0.000407, valid loss:0.000382
Epoch:47, Train loss:0.000403, valid loss:0.000379
Epoch:48, Train loss:0.000401, valid loss:0.000373
Epoch:49, Train loss:0.000400, valid loss:0.000384
Epoch:50, Train loss:0.000397, valid loss:0.000362
Epoch:51, Train loss:0.000377, valid loss:0.000363
Epoch:52, Train loss:0.000374, valid loss:0.000364
Epoch:53, Train loss:0.000373, valid loss:0.000366
Epoch:54, Train loss:0.000372, valid loss:0.000369
Epoch:55, Train loss:0.000372, valid loss:0.000363
Epoch:56, Train loss:0.000371, valid loss:0.000366
Epoch:57, Train loss:0.000371, valid loss:0.000362
Epoch:58, Train loss:0.000370, valid loss:0.000363
Epoch:59, Train loss:0.000370, valid loss:0.000364
Epoch:60, Train loss:0.000370, valid loss:0.000364
training time 13092.698837041855
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.21760457601253466
plot_id,batch_id 0 1 miss% 0.2375673294775365
plot_id,batch_id 0 2 miss% 0.3078560770851284
plot_id,batch_id 0 3 miss% 0.2594845006676364
plot_id,batch_id 0 4 miss% 0.2974580533659014
plot_id,batch_id 0 5 miss% 0.14997934438760926
plot_id,batch_id 0 6 miss% 0.20436136726368234
plot_id,batch_id 0 7 miss% 0.35106720258152907
plot_id,batch_id 0 8 miss% 0.4860496087391473
plot_id,batch_id 0 9 miss% 0.2133749824841477
plot_id,batch_id 0 10 miss% 0.12221100262018843
plot_id,batch_id 0 11 miss% 0.2137387309053247
plot_id,batch_id 0 12 miss% 0.27483316337673164
plot_id,batch_id 0 13 miss% 0.24665222437261225
plot_id,batch_id 0 14 miss% 0.332590573416766
plot_id,batch_id 0 15 miss% 0.12278112684381437
plot_id,batch_id 0 16 miss% 0.351357961342376
plot_id,batch_id 0 17 miss% 0.28256635652102485
plot_id,batch_id 0 18 miss% 0.2748611418366692
plot_id,batch_id 0 19 miss% 0.32671339146163497
plot_id,batch_id 0 20 miss% 0.18328497531313825
plot_id,batch_id 0 21 miss% 0.2781246534124328
plot_id,batch_id 0 22 miss% 0.3110773236117118
plot_id,batch_id 0 23 miss% 0.2736690872989905
plot_id,batch_id 0 24 miss% 0.2038904083688582
plot_id,batch_id 0 25 miss% 0.18818921520109233
plot_id,batch_id 0 26 miss% 0.2898879671095302
plot_id,batch_id 0 27 miss% 0.29146730300702056
plot_id,batch_id 0 28 miss% 0.32579077865816275
plot_id,batch_id 0 29 miss% 0.28204667052887733
plot_id,batch_id 0 30 miss% 0.16049663402931524
plot_id,batch_id 0 31 miss% 0.361502935637164
plot_id,batch_id 0 32 miss% 0.4222208055467257
plot_id,batch_id 0 33 miss% 0.3967633871502294
plot_id,batch_id 0 34 miss% 0.2637835650124375
plot_id,batch_id 0 35 miss% 0.13354086592826928
plot_id,batch_id 0 36 miss% 0.33882983758858093
plot_id,batch_id 0 37 miss% 0.3995794080182826
plot_id,batch_id 0 38 miss% 0.28248463518778266
plot_id,batch_id 0 39 miss% 0.22193651033488654
plot_id,batch_id 0 40 miss% 0.19380010625382219
plot_id,batch_id 0 41 miss% 0.2216506139358652
plot_id,batch_id 0 42 miss% 0.18761235058595357
plot_id,batch_id 0 43 miss% 0.23673648459145435
plot_id,batch_id 0 44 miss% 0.2039549555762134
plot_id,batch_id 0 45 miss% 0.18083334134614557
plot_id,batch_id 0 46 miss% 0.27500565400992066
plot_id,batch_id 0 47 miss% 0.2605727086803237
plot_id,batch_id 0 48 miss% 0.25544087731179055
plot_id,batch_id 0 49 miss% 0.1862794922042401
plot_id,batch_id 0 50 miss% 0.3597953987107572
plot_id,batch_id 0 51 miss% 0.3773504908585756
plot_id,batch_id 0 52 miss% 0.2960216191458486
plot_id,batch_id 0 53 miss% 0.18370935398341567
plot_id,batch_id 0 54 miss% 0.28171949309669087
plot_id,batch_id 0 55 miss% 0.25855397617051057
plot_id,batch_id 0 56 miss% 0.4164787088827977
plot_id,batch_id 0 57 miss% 0.2157204562563199
plot_id,batch_id 0 58 miss% 0.25097993510892336
plot_id,batch_id 0 59 miss% 0.24164073488899138
plot_id,batch_id 0 60 miss% 0.09492061559378712
plot_id,batch_id 0 61 miss% 0.1644174928521081
plot_id,batch_id 0 62 miss% 0.21506178595178652
plot_id,batch_id 0 63 miss% 0.22392457719389383
plot_id,batch_id 0 64 miss% 0.24344134759244637
plot_id,batch_id 0 65 miss% 0.25918712929258964
0.37335546994871427
plot_id,batch_id 0 69 miss% 0.44485998321603215
plot_id,batch_id 0 70 miss% 0.257916257348888
plot_id,batch_id 0 71 miss% 0.3726686452378429
plot_id,batch_id 0 72 miss% 0.4234094275731251
plot_id,batch_id 0 73 miss% 0.44893547232206915
plot_id,batch_id 0 74 miss% 0.41044954478278795
plot_id,batch_id 0 75 miss% 0.2901436799968744
plot_id,batch_id 0 76 miss% 0.3596696026430085
plot_id,batch_id 0 77 miss% 0.41862989800537015
plot_id,batch_id 0 78 miss% 0.3711234377779885
plot_id,batch_id 0 79 miss% 0.3469497747759263
plot_id,batch_id 0 80 miss% 0.3701189535460964
plot_id,batch_id 0 81 miss% 0.5109364995491399
plot_id,batch_id 0 82 miss% 0.37880455053781925
plot_id,batch_id 0 83 miss% 0.39487982817895817
plot_id,batch_id 0 84 miss% 0.3468841007513099
plot_id,batch_id 0 85 miss% 0.30300648260875185
plot_id,batch_id 0 86 miss% 0.3212151044912709
plot_id,batch_id 0 87 miss% 0.4511976086589191
plot_id,batch_id 0 88 miss% 0.463010437261468
plot_id,batch_id 0 89 miss% 0.4448695400431088
plot_id,batch_id 0 90 miss% 0.228204993085931
plot_id,batch_id 0 91 miss% 0.30711288100591005
plot_id,batch_id 0 92 miss% 0.3387679988519209
plot_id,batch_id 0 93 miss% 0.3860975712684661
plot_id,batch_id 0 94 miss% 0.5101378444644316
plot_id,batch_id 0 95 miss% 0.3282471770448323
plot_id,batch_id 0 96 miss% 0.31860325205945345
plot_id,batch_id 0 97 miss% 0.4650717971926056
plot_id,batch_id 0 98 miss% 0.4300373453870371
plot_id,batch_id 0 99 miss% 0.373500190051095
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.32962588 0.45740271 0.43179895 0.45283036 0.37610711 0.42224856
 0.32330719 0.4828051  0.46539774 0.43525875 0.31492301 0.39076842
 0.355773   0.29958613 0.44199204 0.27519594 0.51157517 0.40436368
 0.38399797 0.39950924 0.38232624 0.4872293  0.44547636 0.51276073
 0.50266599 0.35502234 0.49180788 0.41707715 0.45418075 0.45948129
 0.31381679 0.39277294 0.40308649 0.43397097 0.40970616 0.38027411
 0.39522331 0.38629522 0.46464071 0.33352589 0.26190376 0.32459827
 0.41209051 0.49909659 0.38733828 0.32311563 0.41421054 0.35978289
 0.38031387 0.23147423 0.60541978 0.44901078 0.44892007 0.22515956
 0.33995006 0.33149672 0.41398127 0.38446517 0.3752916  0.41315034
 0.35988212 0.27773674 0.38922031 0.37388836 0.48345201 0.29765079
 0.37578029 0.28378778 0.37335547 0.44485998 0.25791626 0.37266865
 0.42340943 0.44893547 0.41044954 0.29014368 0.3596696  0.4186299
 0.37112344 0.34694977 0.37011895 0.5109365  0.37880455 0.39487983
 0.3468841  0.30300648 0.3212151  0.45119761 0.46301044 0.44486954
 0.22820499 0.30711288 0.338768   0.38609757 0.51013784 0.32824718
 0.31860325 0.4650718  0.43003735 0.37350019]
for model  218 the mean error 0.38986791268722937
all id 218 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 31 out win 6 err 0.38986791268722937 time 13061.485087633133
Launcher: Job 219 completed in 13294 seconds.
Launcher: Task 136 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  55697
Epoch:0, Train loss:0.573817, valid loss:0.538962
Epoch:1, Train loss:0.326439, valid loss:0.004500
Epoch:2, Train loss:0.009129, valid loss:0.003300
Epoch:3, Train loss:0.007475, valid loss:0.002792
Epoch:4, Train loss:0.006103, valid loss:0.002482
Epoch:5, Train loss:0.005135, valid loss:0.002440
Epoch:6, Train loss:0.004056, valid loss:0.001794
Epoch:7, Train loss:0.003495, valid loss:0.001710
Epoch:8, Train loss:0.003229, valid loss:0.001666
Epoch:9, Train loss:0.003028, valid loss:0.001388
Epoch:10, Train loss:0.002809, valid loss:0.001346
Epoch:11, Train loss:0.002305, valid loss:0.001282
Epoch:12, Train loss:0.002229, valid loss:0.001129
Epoch:13, Train loss:0.002134, valid loss:0.001126
Epoch:14, Train loss:0.002050, valid loss:0.001131
Epoch:15, Train loss:0.001978, valid loss:0.001121
Epoch:16, Train loss:0.001921, valid loss:0.001019
Epoch:17, Train loss:0.001904, valid loss:0.001008
Epoch:18, Train loss:0.001817, valid loss:0.001000
Epoch:19, Train loss:0.001765, valid loss:0.000964
Epoch:20, Train loss:0.001731, valid loss:0.001026
Epoch:21, Train loss:0.001476, valid loss:0.000917
Epoch:22, Train loss:0.001442, valid loss:0.000881
Epoch:23, Train loss:0.001425, valid loss:0.000852
Epoch:24, Train loss:0.001421, valid loss:0.000929
Epoch:25, Train loss:0.001385, valid loss:0.000842
Epoch:26, Train loss:0.001342, valid loss:0.000855
Epoch:27, Train loss:0.001346, valid loss:0.000856
Epoch:28, Train loss:0.001309, valid loss:0.000832
Epoch:29, Train loss:0.001309, valid loss:0.000827
Epoch:30, Train loss:0.001275, valid loss:0.000810
Epoch:31, Train loss:0.001159, valid loss:0.000814
Epoch:32, Train loss:0.001156, valid loss:0.000787
Epoch:33, Train loss:0.001139, valid loss:0.000822
Epoch:34, Train loss:0.001131, valid loss:0.000802
Epoch:35, Train loss:0.001126, valid loss:0.000760
Epoch:36, Train loss:0.001104, valid loss:0.000806
Epoch:37, Train loss:0.001095, valid loss:0.000753
Epoch:38, Train loss:0.001098, valid loss:0.000764
Epoch:39, Train loss:0.001086, valid loss:0.000749
Epoch:40, Train loss:0.001072, valid loss:0.000738
Epoch:41, Train loss:0.001013, valid loss:0.000730
Epoch:42, Train loss:0.001008, valid loss:0.000733
Epoch:43, Train loss:0.001002, valid loss:0.000720
Epoch:44, Train loss:0.001003, valid loss:0.000732
Epoch:45, Train loss:0.000993, valid loss:0.000700
Epoch:46, Train loss:0.000993, valid loss:0.000729
Epoch:47, Train loss:0.000991, valid loss:0.000713
Epoch:48, Train loss:0.000986, valid loss:0.000754
Epoch:49, Train loss:0.000979, valid loss:0.000708
Epoch:50, Train loss:0.000970, valid loss:0.000773
Epoch:51, Train loss:0.000937, valid loss:0.000708
Epoch:52, Train loss:0.000930, valid loss:0.000698
Epoch:53, Train loss:0.000930, valid loss:0.000699
Epoch:54, Train loss:0.000926, valid loss:0.000697
Epoch:55, Train loss:0.000927, valid loss:0.000700
Epoch:56, Train loss:0.000926, valid loss:0.000715
Epoch:57, Train loss:0.000925, valid loss:0.000709
Epoch:58, Train loss:0.000925, valid loss:0.000700
Epoch:59, Train loss:0.000925, valid loss:0.000698
Epoch:60, Train loss:0.000923, valid loss:0.000701
training time 13099.834425926208
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.10207210376160578
plot_id,batch_id 0 1 miss% 0.038657002154784355
plot_id,batch_id 0 2 miss% 0.11122401567694921
plot_id,batch_id 0 3 miss% 0.06750764889698177
plot_id,batch_id 0 4 miss% 0.09467616798564447
plot_id,batch_id 0 5 miss% 0.04503109971617844
plot_id,batch_id 0 6 miss% 0.08794626816814322
plot_id,batch_id 0 7 miss% 0.12203989236660516
plot_id,batch_id 0 8 miss% 0.0796015554106274
plot_id,batch_id 0 9 miss% 0.062419880166181066
plot_id,batch_id 0 10 miss% 0.05623338631891051
plot_id,batch_id 0 11 miss% 0.09322391777719777
plot_id,batch_id 0 12 miss% 0.07691603525133893
plot_id,batch_id 0 13 miss% 0.06505368336523815
plot_id,batch_id 0 14 miss% 0.12963904367432372
plot_id,batch_id 0 15 miss% 0.044092905008350836
plot_id,batch_id 0 16 miss% 0.1631952113670612
plot_id,batch_id 0 17 miss% 0.03658724757762321
plot_id,batch_id 0 18 miss% 0.06301397177991597
plot_id,batch_id 0 19 miss% 0.1070204612847395
plot_id,batch_id 0 20 miss% 0.09425155327713894
plot_id,batch_id 0 21 miss% 0.07490150465456483
plot_id,batch_id 0 22 miss% 0.061624864499968714
plot_id,batch_id 0 23 miss% 0.06242707022072402
plot_id,batch_id 0 24 miss% 0.06893657896433882
plot_id,batch_id 0 25 miss% 0.06316447381510645
plot_id,batch_id 0 26 miss% 0.05346832311129376
plot_id,batch_id 0 27 miss% 0.05492605624545786
plot_id,batch_id 0 28 miss% 0.04665015308585484
plot_id,batch_id 0 29 miss% 0.03245171050985439
plot_id,batch_id 0 30 miss% 0.03951165264558003
plot_id,batch_id 0 31 miss% 0.14295419672227327
plot_id,batch_id 0 32 miss% 0.1253846704026901
plot_id,batch_id 0 33 miss% 0.06661885595344304
plot_id,batch_id 0 34 miss% 0.07757866639152143
plot_id,batch_id 0 35 miss% 0.06742289696810627
plot_id,batch_id 0 36 miss% 0.09279630123795851
plot_id,batch_id 0 37 miss% 0.0700320404640661
plot_id,batch_id 0 38 miss% 0.07660855965642803
plot_id,batch_id 0 39 miss% 0.08665581418677855
plot_id,batch_id 0 40 miss% 0.07479667792785782
plot_id,batch_id 0 41 miss% 0.06458625580004324
plot_id,batch_id 0 42 miss% 0.05009502951681412
plot_id,batch_id 0 43 miss% 0.06097449976589585
plot_id,batch_id 0 44 miss% 0.08894613503744869
plot_id,batch_id 0 45 miss% 0.04862593077147171
plot_id,batch_id 0 46 miss% 0.03600405460489548
plot_id,batch_id 0 47 miss% 0.03832361844559802
plot_id,batch_id 0 48 miss% 0.03883659607534917
plot_id,batch_id 0 49 miss% 0.05145186142342971
plot_id,batch_id 0 50 miss% 0.13898305017500823
plot_id,batch_id 0 51 miss% 0.05216485146647554
plot_id,batch_id 0 52 miss% 0.0361338012270333
plot_id,batch_id 0 53 miss% 0.04148142572933773
plot_id,batch_id 0 54 miss% 0.032959613664044575
plot_id,batch_id 0 55 miss% 0.16127022825211274
plot_id,batch_id 0 56 miss% 0.10184908115447007
plot_id,batch_id 0 57 miss% 0.03201475840548786
plot_id,batch_id 0 58 miss% 0.05654630119745318
plot_id,batch_id 0 59 miss% 0.04177928578022955
plot_id,batch_id 0 60 miss% 0.051138999203821645
plot_id,batch_id 0 61 miss% 0.061858881662564716
plot_id,batch_id 0 62 miss% 0.06062121379854359
plot_id,batch_id 0 63 miss% 0.06931818853269354
plot_id,batch_id 0 64 miss% 0.075747333931563
plot_id,batch_id 0 65 miss% 0.0435077362419507
plot_id,batch_id 0 66 miss% 0.04369094024359084
plot_id,batch_id 0 67 miss% 0.08582286591083062
plot_id,batch_id 0 plot_id,batch_id 0 67 miss% 0.02571149210418543
plot_id,batch_id 0 68 miss% 0.04458604850383716
plot_id,batch_id 0 69 miss% 0.06482744563388586
plot_id,batch_id 0 70 miss% 0.027739047610025252
plot_id,batch_id 0 71 miss% 0.04276276417529843
plot_id,batch_id 0 72 miss% 0.061205159050648335
plot_id,batch_id 0 73 miss% 0.028099144185417813
plot_id,batch_id 0 74 miss% 0.08644571823428779
plot_id,batch_id 0 75 miss% 0.039216064528815264
plot_id,batch_id 0 76 miss% 0.05639897351223361
plot_id,batch_id 0 77 miss% 0.04343541351027655
plot_id,batch_id 0 78 miss% 0.02753354459475245
plot_id,batch_id 0 79 miss% 0.07844509656665102
plot_id,batch_id 0 80 miss% 0.04455738132766461
plot_id,batch_id 0 81 miss% 0.0782882896291768
plot_id,batch_id 0 82 miss% 0.05569388509484787
plot_id,batch_id 0 83 miss% 0.09366138399597895
plot_id,batch_id 0 84 miss% 0.06088285057688267
plot_id,batch_id 0 85 miss% 0.05371163996634499
plot_id,batch_id 0 86 miss% 0.05904615503162811
plot_id,batch_id 0 87 miss% 0.0607904234110662
plot_id,batch_id 0 88 miss% 0.06712470337913258
plot_id,batch_id 0 89 miss% 0.05454403736432741
plot_id,batch_id 0 90 miss% 0.024716958532586576
plot_id,batch_id 0 91 miss% 0.04343628895486527
plot_id,batch_id 0 92 miss% 0.06861674810493827
plot_id,batch_id 0 93 miss% 0.051784200901282836
plot_id,batch_id 0 94 miss% 0.07640959961711474
plot_id,batch_id 0 95 miss% 0.033928969566057884
plot_id,batch_id 0 96 miss% 0.08660540619866773
plot_id,batch_id 0 97 miss% 0.052190237598794806
plot_id,batch_id 0 98 miss% 0.04396992749485662
plot_id,batch_id 0 99 miss% 0.08568345228787762
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06103287 0.06334207 0.09539606 0.04594738 0.05671671 0.0527424
 0.04436336 0.08738705 0.08516323 0.01816079 0.02865691 0.04800639
 0.077451   0.04180365 0.07001211 0.03971211 0.07098993 0.04489757
 0.05716187 0.10306423 0.04142371 0.04165821 0.06244079 0.03833837
 0.05128195 0.04447465 0.04710576 0.04693831 0.0287072  0.02507111
 0.02205145 0.09686081 0.11256918 0.06648764 0.02104835 0.03653266
 0.06320352 0.09132132 0.05400533 0.04249521 0.05466696 0.04966516
 0.01865528 0.03329075 0.03673892 0.0460659  0.03123413 0.02051035
 0.0308811  0.02352877 0.12438863 0.04433734 0.03846388 0.02853222
 0.04827173 0.0564622  0.0849314  0.02499067 0.03357595 0.03970374
 0.04738359 0.02098215 0.03165705 0.03410792 0.05592484 0.03176985
 0.12537348 0.02571149 0.04458605 0.06482745 0.02773905 0.04276276
 0.06120516 0.02809914 0.08644572 0.03921606 0.05639897 0.04343541
 0.02753354 0.0784451  0.04455738 0.07828829 0.05569389 0.09366138
 0.06088285 0.05371164 0.05904616 0.06079042 0.0671247  0.05454404
 0.02471696 0.04343629 0.06861675 0.0517842  0.0764096  0.03392897
 0.08660541 0.05219024 0.04396993 0.08568345]
for model  114 the mean error 0.052641656328811244
all id 114 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 25 out win 4 err 0.052641656328811244 time 13052.279372215271
Launcher: Job 115 completed in 13319 seconds.
Launcher: Task 166 done. Exiting.
plot_id,batch_id 0 66 miss% 0.18824067911871847
plot_id,batch_id 0 67 miss% 0.16898700979388157
plot_id,batch_id 0 68 miss% 0.2295115388144803
plot_id,batch_id 0 69 miss% 0.24444093839370598
plot_id,batch_id 0 70 miss% 0.2102247372621463
plot_id,batch_id 0 71 miss% 0.18367854290550142
plot_id,batch_id 0 72 miss% 0.22041074757064527
plot_id,batch_id 0 73 miss% 0.2671448274716126
plot_id,batch_id 0 74 miss% 0.24623947202095234
plot_id,batch_id 0 75 miss% 0.1099786223957564
plot_id,batch_id 0 76 miss% 0.19282813888428332
plot_id,batch_id 0 77 miss% 0.19734586676854754
plot_id,batch_id 0 78 miss% 0.17653314673822731
plot_id,batch_id 0 79 miss% 0.2063341631522666
plot_id,batch_id 0 80 miss% 0.15023582799637542
plot_id,batch_id 0 81 miss% 0.2207561645998643
plot_id,batch_id 0 82 miss% 0.22255489529674027
plot_id,batch_id 0 83 miss% 0.2623106902172335
plot_id,batch_id 0 84 miss% 0.1984205036266284
plot_id,batch_id 0 85 miss% 0.10194461472773217
plot_id,batch_id 0 86 miss% 0.2297633486575743
plot_id,batch_id 0 87 miss% 0.24824620772301567
plot_id,batch_id 0 88 miss% 0.2778109141814957
plot_id,batch_id 0 89 miss% 0.32039222404508894
plot_id,batch_id 0 90 miss% 0.12946981886066788
plot_id,batch_id 0 91 miss% 0.14535154739993547
plot_id,batch_id 0 92 miss% 0.2213850292905354
plot_id,batch_id 0 93 miss% 0.16093258019167989
plot_id,batch_id 0 94 miss% 0.2771902832349761
plot_id,batch_id 0 95 miss% 0.15320701355762434
plot_id,batch_id 0 96 miss% 0.1853936500096057
plot_id,batch_id 0 97 miss% 0.2454365144939365
plot_id,batch_id 0 98 miss% 0.22820716873027994
plot_id,batch_id 0 99 miss% 0.23011272616042416
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.21760458 0.23756733 0.30785608 0.2594845  0.29745805 0.14997934
 0.20436137 0.3510672  0.48604961 0.21337498 0.122211   0.21373873
 0.27483316 0.24665222 0.33259057 0.12278113 0.35135796 0.28256636
 0.27486114 0.32671339 0.18328498 0.27812465 0.31107732 0.27366909
 0.20389041 0.18818922 0.28988797 0.2914673  0.32579078 0.28204667
 0.16049663 0.36150294 0.42222081 0.39676339 0.26378357 0.13354087
 0.33882984 0.39957941 0.28248464 0.22193651 0.19380011 0.22165061
 0.18761235 0.23673648 0.20395496 0.18083334 0.27500565 0.26057271
 0.25544088 0.18627949 0.3597954  0.37735049 0.29602162 0.18370935
 0.28171949 0.25855398 0.41647871 0.21572046 0.25097994 0.24164073
 0.09492062 0.16441749 0.21506179 0.22392458 0.24344135 0.25918713
 0.18824068 0.16898701 0.22951154 0.24444094 0.21022474 0.18367854
 0.22041075 0.26714483 0.24623947 0.10997862 0.19282814 0.19734587
 0.17653315 0.20633416 0.15023583 0.22075616 0.2225549  0.26231069
 0.1984205  0.10194461 0.22976335 0.24824621 0.27781091 0.32039222
 0.12946982 0.14535155 0.22138503 0.16093258 0.27719028 0.15320701
 0.18539365 0.24543651 0.22820717 0.23011273]
for model  192 the mean error 0.2421750553607279
all id 192 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 31 out win 4 err 0.2421750553607279 time 13092.698837041855
Launcher: Job 193 completed in 13340 seconds.
Launcher: Task 197 done. Exiting.
68 miss% 0.06739936154615588
plot_id,batch_id 0 69 miss% 0.14549732239126292
plot_id,batch_id 0 70 miss% 0.13569605950054578
plot_id,batch_id 0 71 miss% 0.05526522495057835
plot_id,batch_id 0 72 miss% 0.09405654877136196
plot_id,batch_id 0 73 miss% 0.119468598329693
plot_id,batch_id 0 74 miss% 0.1597634641189236
plot_id,batch_id 0 75 miss% 0.06398562882850097
plot_id,batch_id 0 76 miss% 0.07247909525753476
plot_id,batch_id 0 77 miss% 0.0352026569924281
plot_id,batch_id 0 78 miss% 0.030461866656718198
plot_id,batch_id 0 79 miss% 0.19946732073402626
plot_id,batch_id 0 80 miss% 0.11781866078277875
plot_id,batch_id 0 81 miss% 0.13008490187417293
plot_id,batch_id 0 82 miss% 0.05822373890269125
plot_id,batch_id 0 83 miss% 0.1002819725275494
plot_id,batch_id 0 84 miss% 0.11901705929366693
plot_id,batch_id 0 85 miss% 0.02903882128343336
plot_id,batch_id 0 86 miss% 0.08246124418732091
plot_id,batch_id 0 87 miss% 0.10891312443817835
plot_id,batch_id 0 88 miss% 0.11283942867748732
plot_id,batch_id 0 89 miss% 0.06487314777061559
plot_id,batch_id 0 90 miss% 0.017977543710084923
plot_id,batch_id 0 91 miss% 0.1046147701534628
plot_id,batch_id 0 92 miss% 0.05645458739250862
plot_id,batch_id 0 93 miss% 0.07289712289627034
plot_id,batch_id 0 94 miss% 0.156444644640365
plot_id,batch_id 0 95 miss% 0.04256037614601183
plot_id,batch_id 0 96 miss% 0.03858815023799021
plot_id,batch_id 0 97 miss% 0.06382876713041599
plot_id,batch_id 0 98 miss% 0.04916019900558996
plot_id,batch_id 0 99 miss% 0.12546729875907459
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.1020721  0.038657   0.11122402 0.06750765 0.09467617 0.0450311
 0.08794627 0.12203989 0.07960156 0.06241988 0.05623339 0.09322392
 0.07691604 0.06505368 0.12963904 0.04409291 0.16319521 0.03658725
 0.06301397 0.10702046 0.09425155 0.0749015  0.06162486 0.06242707
 0.06893658 0.06316447 0.05346832 0.05492606 0.04665015 0.03245171
 0.03951165 0.1429542  0.12538467 0.06661886 0.07757867 0.0674229
 0.0927963  0.07003204 0.07660856 0.08665581 0.07479668 0.06458626
 0.05009503 0.0609745  0.08894614 0.04862593 0.03600405 0.03832362
 0.0388366  0.05145186 0.13898305 0.05216485 0.0361338  0.04148143
 0.03295961 0.16127023 0.10184908 0.03201476 0.0565463  0.04177929
 0.051139   0.06185888 0.06062121 0.06931819 0.07574733 0.04350774
 0.04369094 0.08582287 0.06739936 0.14549732 0.13569606 0.05526522
 0.09405655 0.1194686  0.15976346 0.06398563 0.0724791  0.03520266
 0.03046187 0.19946732 0.11781866 0.1300849  0.05822374 0.10028197
 0.11901706 0.02903882 0.08246124 0.10891312 0.11283943 0.06487315
 0.01797754 0.10461477 0.05645459 0.07289712 0.15644464 0.04256038
 0.03858815 0.06382877 0.0491602  0.1254673 ]
for model  91 the mean error 0.07674335364554963
all id 91 hidden_dim 16 learning_rate 0.0025 num_layers 4 frames 25 out win 5 err 0.07674335364554963 time 13099.834425926208
Launcher: Job 92 completed in 13363 seconds.
Launcher: Task 59 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  55697
Epoch:0, Train loss:0.547327, valid loss:0.513868
Epoch:1, Train loss:0.144548, valid loss:0.004483
Epoch:2, Train loss:0.010182, valid loss:0.003236
Epoch:3, Train loss:0.008546, valid loss:0.002974
Epoch:4, Train loss:0.007968, valid loss:0.002800
Epoch:5, Train loss:0.007477, valid loss:0.002647
Epoch:6, Train loss:0.007165, valid loss:0.002497
Epoch:7, Train loss:0.006945, valid loss:0.002646
Epoch:8, Train loss:0.006677, valid loss:0.002270
Epoch:9, Train loss:0.006476, valid loss:0.002614
Epoch:10, Train loss:0.005481, valid loss:0.002224
Epoch:11, Train loss:0.004398, valid loss:0.001444
Epoch:12, Train loss:0.003650, valid loss:0.001398
Epoch:13, Train loss:0.003477, valid loss:0.001102
Epoch:14, Train loss:0.002115, valid loss:0.001090
Epoch:15, Train loss:0.001983, valid loss:0.001048
Epoch:16, Train loss:0.001927, valid loss:0.001137
Epoch:17, Train loss:0.001907, valid loss:0.001122
Epoch:18, Train loss:0.001793, valid loss:0.001033
Epoch:19, Train loss:0.001750, valid loss:0.001043
Epoch:20, Train loss:0.001729, valid loss:0.001084
Epoch:21, Train loss:0.001414, valid loss:0.000885
Epoch:22, Train loss:0.001400, valid loss:0.000875
Epoch:23, Train loss:0.001355, valid loss:0.000848
Epoch:24, Train loss:0.001325, valid loss:0.000880
Epoch:25, Train loss:0.001304, valid loss:0.000861
Epoch:26, Train loss:0.001285, valid loss:0.000880
Epoch:27, Train loss:0.001284, valid loss:0.000928
Epoch:28, Train loss:0.001230, valid loss:0.000854
Epoch:29, Train loss:0.001202, valid loss:0.000870
Epoch:30, Train loss:0.001205, valid loss:0.000875
Epoch:31, Train loss:0.001053, valid loss:0.000744
Epoch:32, Train loss:0.001031, valid loss:0.000745
Epoch:33, Train loss:0.001016, valid loss:0.000737
Epoch:34, Train loss:0.001014, valid loss:0.000759
Epoch:35, Train loss:0.001013, valid loss:0.000762
Epoch:36, Train loss:0.000983, valid loss:0.000753
Epoch:37, Train loss:0.000972, valid loss:0.000717
Epoch:38, Train loss:0.000971, valid loss:0.000763
Epoch:39, Train loss:0.000976, valid loss:0.000795
Epoch:40, Train loss:0.000960, valid loss:0.000754
Epoch:41, Train loss:0.000879, valid loss:0.000712
Epoch:42, Train loss:0.000859, valid loss:0.000714
Epoch:43, Train loss:0.000858, valid loss:0.000716
Epoch:44, Train loss:0.000862, valid loss:0.000697
Epoch:45, Train loss:0.000847, valid loss:0.000721
Epoch:46, Train loss:0.000839, valid loss:0.000733
Epoch:47, Train loss:0.000844, valid loss:0.000733
Epoch:48, Train loss:0.000835, valid loss:0.000712
Epoch:49, Train loss:0.000829, valid loss:0.000701
Epoch:50, Train loss:0.000822, valid loss:0.000721
Epoch:51, Train loss:0.000782, valid loss:0.000695
Epoch:52, Train loss:0.000773, valid loss:0.000693
Epoch:53, Train loss:0.000769, valid loss:0.000693
Epoch:54, Train loss:0.000767, valid loss:0.000693
Epoch:55, Train loss:0.000766, valid loss:0.000691
Epoch:56, Train loss:0.000764, valid loss:0.000687
Epoch:57, Train loss:0.000763, valid loss:0.000691
Epoch:58, Train loss:0.000762, valid loss:0.000690
Epoch:59, Train loss:0.000762, valid loss:0.000691
Epoch:60, Train loss:0.000761, valid loss:0.000687
training time 13174.909881353378
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07048677439595301
plot_id,batch_id 0 1 miss% 0.05962170052673328
plot_id,batch_id 0 2 miss% 0.10929035850411131
plot_id,batch_id 0 3 miss% 0.04558254514577605
plot_id,batch_id 0 4 miss% 0.06505481511760708
plot_id,batch_id 0 5 miss% 0.03673856125572458
plot_id,batch_id 0 6 miss% 0.06895454513922011
plot_id,batch_id 0 7 miss% 0.13359715895702984
plot_id,batch_id 0 8 miss% 0.06808504888013941
plot_id,batch_id 0 9 miss% 0.05491463392090494
plot_id,batch_id 0 10 miss% 0.09389798840101791
plot_id,batch_id 0 11 miss% 0.10241881884676607
plot_id,batch_id 0 12 miss% 0.10047696040260634
plot_id,batch_id 0 13 miss% 0.06450388246063327
plot_id,batch_id 0 14 miss% 0.11177909482580847
plot_id,batch_id 0 15 miss% 0.04990804075953374
plot_id,batch_id 0 16 miss% 0.09041314456696374
plot_id,batch_id 0 17 miss% 0.06186281245111083
plot_id,batch_id 0 18 miss% 0.06465691847177019
plot_id,batch_id 0 19 miss% 0.06446429630521941
plot_id,batch_id 0 20 miss% 0.07968131071558691
plot_id,batch_id 0 21 miss% 0.0593841123468717
plot_id,batch_id 0 22 miss% 0.0693848718708765
plot_id,batch_id 0 23 miss% 0.036646119759410685
plot_id,batch_id 0 24 miss% 0.04271255211048065
plot_id,batch_id 0 25 miss% 0.037885487152841374
plot_id,batch_id 0 26 miss% 0.04500378933319637
plot_id,batch_id 0 27 miss% 0.04572745935802713
plot_id,batch_id 0 28 miss% 0.02467656128082927
plot_id,batch_id 0 29 miss% 0.05400647214681458
plot_id,batch_id 0 30 miss% 0.0491172654184514
plot_id,batch_id 0 31 miss% 0.10774267999771356
plot_id,batch_id 0 32 miss% 0.12377820400611289
plot_id,batch_id 0 33 miss% 0.06026062101468677
plot_id,batch_id 0 34 miss% 0.07020226926245
plot_id,batch_id 0 35 miss% 0.04528658759856473
plot_id,batch_id 0 36 miss% 0.0821899839500776
plot_id,batch_id 0 37 miss% 0.08693525871142271
plot_id,batch_id 0 38 miss% 0.05580286924278854
plot_id,batch_id 0 39 miss% 0.035251908780817784
plot_id,batch_id 0 40 miss% 0.060698918090431986
plot_id,batch_id 0 41 miss% 0.035296260870763804
plot_id,batch_id 0 42 miss% 0.018231468914287452
plot_id,batch_id 0 43 miss% 0.051474542014161205
plot_id,batch_id 0 44 miss% 0.04449373493932507
plot_id,batch_id 0 45 miss% 0.036852094523512256
plot_id,batch_id 0 46 miss% 0.04670696382107476
plot_id,batch_id 0 47 miss% 0.028295582307224926
plot_id,batch_id 0 48 miss% 0.044599338232950744
plot_id,batch_id 0 49 miss% 0.025530197225173697
plot_id,batch_id 0 50 miss% 0.10636196300158711
plot_id,batch_id 0 51 miss% 0.03631741579308036
plot_id,batch_id 0 52 miss% 0.02841582230823058
plot_id,batch_id 0 53 miss% 0.022235820556686724
plot_id,batch_id 0 54 miss% 0.03129355041808528
plot_id,batch_id 0 55 miss% 0.07811119037124775
plot_id,batch_id 0 56 miss% 0.06451100378547597
plot_id,batch_id 0 57 miss% 0.08172198955975635
plot_id,batch_id 0 58 miss% 0.05914106171446169
plot_id,batch_id 0 59 miss% 0.03795831102490137
plot_id,batch_id 0 60 miss% 0.06926309297004497
plot_id,batch_id 0 61 miss% 0.07047864258536428
plot_id,batch_id 0 62 miss% 0.04682417813466259
plot_id,batch_id 0 63 miss% 0.0569247353581129
plot_id,batch_id 0 64 miss% 0.052121753351386316
plot_id,batch_id 0 65 miss% 0.18558562968573863
plot_id,batch_id 0 66 miss% 0.11508799172975939
plot_id,batch_id 0 67 miss% 0.03275612293223543
plot_id,batch_id 0 68 miss% 0.03503814336749756
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  69649
Epoch:0, Train loss:0.593215, valid loss:0.570251
Epoch:1, Train loss:0.046289, valid loss:0.006420
Epoch:2, Train loss:0.014007, valid loss:0.004381
Epoch:3, Train loss:0.011050, valid loss:0.003275
Epoch:4, Train loss:0.009376, valid loss:0.003068
Epoch:5, Train loss:0.008254, valid loss:0.002886
Epoch:6, Train loss:0.006956, valid loss:0.002632
Epoch:7, Train loss:0.005203, valid loss:0.001608
Epoch:8, Train loss:0.003278, valid loss:0.001442
Epoch:9, Train loss:0.002882, valid loss:0.001539
Epoch:10, Train loss:0.002740, valid loss:0.001410
Epoch:11, Train loss:0.002172, valid loss:0.001075
Epoch:12, Train loss:0.002069, valid loss:0.001110
Epoch:13, Train loss:0.001975, valid loss:0.001022
Epoch:14, Train loss:0.001914, valid loss:0.001082
Epoch:15, Train loss:0.001842, valid loss:0.001028
Epoch:16, Train loss:0.001771, valid loss:0.001020
Epoch:17, Train loss:0.001698, valid loss:0.000969
Epoch:18, Train loss:0.001700, valid loss:0.001010
Epoch:19, Train loss:0.001612, valid loss:0.000933
Epoch:20, Train loss:0.001545, valid loss:0.000903
Epoch:21, Train loss:0.001305, valid loss:0.000806
Epoch:22, Train loss:0.001288, valid loss:0.000885
Epoch:23, Train loss:0.001265, valid loss:0.000844
Epoch:24, Train loss:0.001244, valid loss:0.000812
Epoch:25, Train loss:0.001221, valid loss:0.000794
Epoch:26, Train loss:0.001228, valid loss:0.000784
Epoch:27, Train loss:0.001182, valid loss:0.000833
Epoch:28, Train loss:0.001177, valid loss:0.000813
Epoch:29, Train loss:0.001146, valid loss:0.000802
Epoch:30, Train loss:0.001136, valid loss:0.000882
Epoch:31, Train loss:0.001025, valid loss:0.000755
Epoch:32, Train loss:0.001004, valid loss:0.000754
Epoch:33, Train loss:0.001002, valid loss:0.000736
Epoch:34, Train loss:0.000988, valid loss:0.000720
Epoch:35, Train loss:0.000986, valid loss:0.000758
Epoch:36, Train loss:0.000977, valid loss:0.000727
Epoch:37, Train loss:0.000967, valid loss:0.000700
Epoch:38, Train loss:0.000971, valid loss:0.000721
Epoch:39, Train loss:0.000957, valid loss:0.000693
Epoch:40, Train loss:0.000941, valid loss:0.000721
Epoch:41, Train loss:0.000886, valid loss:0.000689
Epoch:42, Train loss:0.000877, valid loss:0.000695
Epoch:43, Train loss:0.000873, valid loss:0.000709
Epoch:44, Train loss:0.000870, valid loss:0.000694
Epoch:45, Train loss:0.000866, valid loss:0.000684
Epoch:46, Train loss:0.000862, valid loss:0.000667
Epoch:47, Train loss:0.000860, valid loss:0.000673
Epoch:48, Train loss:0.000859, valid loss:0.000669
Epoch:49, Train loss:0.000850, valid loss:0.000671
Epoch:50, Train loss:0.000848, valid loss:0.000703
Epoch:51, Train loss:0.000810, valid loss:0.000665
Epoch:52, Train loss:0.000803, valid loss:0.000665
Epoch:53, Train loss:0.000801, valid loss:0.000665
Epoch:54, Train loss:0.000800, valid loss:0.000660
Epoch:55, Train loss:0.000799, valid loss:0.000659
Epoch:56, Train loss:0.000798, valid loss:0.000662
Epoch:57, Train loss:0.000798, valid loss:0.000657
Epoch:58, Train loss:0.000797, valid loss:0.000656
Epoch:59, Train loss:0.000798, valid loss:0.000666
Epoch:60, Train loss:0.000797, valid loss:0.000665
training time 13179.152706384659
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.07007456476206471
plot_id,batch_id 0 1 miss% 0.046388762247519234
plot_id,batch_id 0 2 miss% 0.10998595354129254
plot_id,batch_id 0 3 miss% 0.07573535573034024
plot_id,batch_id 0 4 miss% 0.09199275190120923
plot_id,batch_id 0 5 miss% 0.05880408627468418
plot_id,batch_id 0 6 miss% 0.10127319501226303
plot_id,batch_id 0 7 miss% 0.11624827307070612
plot_id,batch_id 0 8 miss% 0.08043863530083914
plot_id,batch_id 0 9 miss% 0.034133324868075486
plot_id,batch_id 0 10 miss% 0.05121678008615573
plot_id,batch_id 0 11 miss% 0.09004919449795333
plot_id,batch_id 0 12 miss% 0.0706228800460975
plot_id,batch_id 0 13 miss% 0.07743397445024564
plot_id,batch_id 0 14 miss% 0.10472242620057161
plot_id,batch_id 0 15 miss% 0.06292087727233839
plot_id,batch_id 0 16 miss% 0.09783849381727407
plot_id,batch_id 0 17 miss% 0.08464196269244836
plot_id,batch_id 0 18 miss% 0.08651898985538166
plot_id,batch_id 0 19 miss% 0.0799902501880251
plot_id,batch_id 0 20 miss% 0.06920842805912605
plot_id,batch_id 0 21 miss% 0.04167068286518935
plot_id,batch_id 0 22 miss% 0.0516918043522951
plot_id,batch_id 0 23 miss% 0.03408079422926308
plot_id,batch_id 0 24 miss% 0.06345217874121523
plot_id,batch_id 0 25 miss% 0.046093519756095415
plot_id,batch_id 0 26 miss% 0.04754063333310651
plot_id,batch_id 0 27 miss% 0.054671540868091714
plot_id,batch_id 0 28 miss% 0.02600834464477257
plot_id,batch_id 0 29 miss% 0.04331444811492496
plot_id,batch_id 0 30 miss% 0.06360716171769813
plot_id,batch_id 0 31 miss% 0.10249381609053225
plot_id,batch_id 0 32 miss% 0.13490041235761588
plot_id,batch_id 0 33 miss% 0.0674176610004387
plot_id,batch_id 0 34 miss% 0.06402372831559429
plot_id,batch_id 0 35 miss% 0.06733821127639905
plot_id,batch_id 0 36 miss% 0.07946479773243037
plot_id,batch_id 0 37 miss% 0.07054099621196197
plot_id,batch_id 0 38 miss% 0.0706949905893564
plot_id,batch_id 0 39 miss% 0.0624576089099137
plot_id,batch_id 0 40 miss% 0.05876936730020116
plot_id,batch_id 0 41 miss% 0.05938209610646051
plot_id,batch_id 0 42 miss% 0.02153689550578465
plot_id,batch_id 0 43 miss% 0.08421773925836513
plot_id,batch_id 0 44 miss% 0.06192514508311469
plot_id,batch_id 0 45 miss% 0.060380308575012524
plot_id,batch_id 0 46 miss% 0.041019856542991416
plot_id,batch_id 0 47 miss% 0.05090508291790496
plot_id,batch_id 0 48 miss% 0.0374055185735954
plot_id,batch_id 0 49 miss% 0.0239518320250073
plot_id,batch_id 0 50 miss% 0.18777264705324478
plot_id,batch_id 0 51 miss% 0.07164152535485169
plot_id,batch_id 0 52 miss% 0.03348999939265614
plot_id,batch_id 0 53 miss% 0.03575487043461453
plot_id,batch_id 0 54 miss% 0.0324309363187632
plot_id,batch_id 0 55 miss% 0.04043359272383259
plot_id,batch_id 0 56 miss% 0.07302313079826171
plot_id,batch_id 0 57 miss% 0.03247737780693694
plot_id,batch_id 0 58 miss% 0.03454856713576218
plot_id,batch_id 0 59 miss% 0.0366405762685139
plot_id,batch_id 0 60 miss% 0.052597146308224596
plot_id,batch_id 0 61 miss% 0.037055763113301714
plot_id,batch_id 0 62 miss% 0.04702663932115928
plot_id,batch_id 0 63 miss% 0.043981903664001384
plot_id,batch_id 0 64 miss% 0.06472153586683871
plot_id,batch_id 0 65 miss% 0.15592879713606478
plot_id,batch_id 0 66 miss% 0.07623975759083211
plot_id,batch_id 0 67 miss% 0.033310673183037605
plot_id,batch_id 0the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  89105
Epoch:0, Train loss:0.515416, valid loss:0.475198
Epoch:1, Train loss:0.112545, valid loss:0.005003
Epoch:2, Train loss:0.014531, valid loss:0.004446
Epoch:3, Train loss:0.012986, valid loss:0.003450
Epoch:4, Train loss:0.009245, valid loss:0.003074
Epoch:5, Train loss:0.006779, valid loss:0.002367
Epoch:6, Train loss:0.005494, valid loss:0.002232
Epoch:7, Train loss:0.004553, valid loss:0.001612
Epoch:8, Train loss:0.002913, valid loss:0.001421
Epoch:9, Train loss:0.002723, valid loss:0.001332
Epoch:10, Train loss:0.002573, valid loss:0.001255
Epoch:11, Train loss:0.002019, valid loss:0.001108
Epoch:12, Train loss:0.001911, valid loss:0.001106
Epoch:13, Train loss:0.001860, valid loss:0.001128
Epoch:14, Train loss:0.001809, valid loss:0.000948
Epoch:15, Train loss:0.001721, valid loss:0.000990
Epoch:16, Train loss:0.001687, valid loss:0.000924
Epoch:17, Train loss:0.001641, valid loss:0.001025
Epoch:18, Train loss:0.001561, valid loss:0.000961
Epoch:19, Train loss:0.001537, valid loss:0.000878
Epoch:20, Train loss:0.001448, valid loss:0.000904
Epoch:21, Train loss:0.001221, valid loss:0.000836
Epoch:22, Train loss:0.001186, valid loss:0.000919
Epoch:23, Train loss:0.001185, valid loss:0.000924
Epoch:24, Train loss:0.001157, valid loss:0.000801
Epoch:25, Train loss:0.001114, valid loss:0.000817
Epoch:26, Train loss:0.001126, valid loss:0.000925
Epoch:27, Train loss:0.001101, valid loss:0.000766
Epoch:28, Train loss:0.001048, valid loss:0.000773
Epoch:29, Train loss:0.001057, valid loss:0.000824
Epoch:30, Train loss:0.001060, valid loss:0.000753
Epoch:31, Train loss:0.000908, valid loss:0.000701
Epoch:32, Train loss:0.000892, valid loss:0.000733
Epoch:33, Train loss:0.000888, valid loss:0.000747
Epoch:34, Train loss:0.000879, valid loss:0.000719
Epoch:35, Train loss:0.000879, valid loss:0.000735
Epoch:36, Train loss:0.000867, valid loss:0.000694
Epoch:37, Train loss:0.000847, valid loss:0.000698
Epoch:38, Train loss:0.000840, valid loss:0.000724
Epoch:39, Train loss:0.000839, valid loss:0.000762
Epoch:40, Train loss:0.000833, valid loss:0.000700
Epoch:41, Train loss:0.000763, valid loss:0.000662
Epoch:42, Train loss:0.000760, valid loss:0.000650
Epoch:43, Train loss:0.000750, valid loss:0.000657
Epoch:44, Train loss:0.000751, valid loss:0.000667
Epoch:45, Train loss:0.000744, valid loss:0.000654
Epoch:46, Train loss:0.000744, valid loss:0.000650
Epoch:47, Train loss:0.000743, valid loss:0.000631
Epoch:48, Train loss:0.000738, valid loss:0.000665
Epoch:49, Train loss:0.000729, valid loss:0.000664
Epoch:50, Train loss:0.000721, valid loss:0.000654
Epoch:51, Train loss:0.000688, valid loss:0.000639
Epoch:52, Train loss:0.000682, valid loss:0.000656
Epoch:53, Train loss:0.000678, valid loss:0.000636
Epoch:54, Train loss:0.000680, valid loss:0.000637
Epoch:55, Train loss:0.000677, valid loss:0.000640
Epoch:56, Train loss:0.000675, valid loss:0.000643
Epoch:57, Train loss:0.000677, valid loss:0.000658
Epoch:58, Train loss:0.000677, valid loss:0.000643
Epoch:59, Train loss:0.000676, valid loss:0.000647
Epoch:60, Train loss:0.000676, valid loss:0.000633
training time 13211.46215891838
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.37737350268715664
plot_id,batch_id 0 1 miss% 0.44558264873326503
plot_id,batch_id 0 2 miss% 0.5596165378531487
plot_id,batch_id 0 3 miss% 0.4412751394106622
plot_id,batch_id 0 4 miss% 0.4047323845075826
plot_id,batch_id 0 5 miss% 0.4382093842567664
plot_id,batch_id 0 6 miss% 0.3877782976565378
plot_id,batch_id 0 7 miss% 0.5536768902967706
plot_id,batch_id 0 8 miss% 0.6271658418073842
plot_id,batch_id 0 9 miss% 0.7119583908690711
plot_id,batch_id 0 10 miss% 0.3686061308112662
plot_id,batch_id 0 11 miss% 0.39393066851086983
plot_id,batch_id 0 12 miss% 0.42906673942982265
plot_id,batch_id 0 13 miss% 0.4127153254742714
plot_id,batch_id 0 14 miss% 0.492385051272325
plot_id,batch_id 0 15 miss% 0.4656233922801217
plot_id,batch_id 0 16 miss% 0.5470739009166417
plot_id,batch_id 0 17 miss% 0.4844559689125236
plot_id,batch_id 0 18 miss% 0.51241550803079
plot_id,batch_id 0 19 miss% 0.481193201871208
plot_id,batch_id 0 20 miss% 0.3788206619466365
plot_id,batch_id 0 21 miss% 0.4522966930249933
plot_id,batch_id 0 22 miss% 0.4894013230085735
plot_id,batch_id 0 23 miss% 0.498902342776824
plot_id,batch_id 0 24 miss% 0.48464545702401024
plot_id,batch_id 0 25 miss% 0.38685362799118067
plot_id,batch_id 0 26 miss% 0.4646321675877708
plot_id,batch_id 0 27 miss% 0.4707783534291849
plot_id,batch_id 0 28 miss% 0.56469202394325
plot_id,batch_id 0 29 miss% 0.5011714765325351
plot_id,batch_id 0 30 miss% 0.42902283573406985
plot_id,batch_id 0 31 miss% 0.5059376630568571
plot_id,batch_id 0 32 miss% 0.5255284558146129
plot_id,batch_id 0 33 miss% 0.5044601140768248
plot_id,batch_id 0 34 miss% 0.39598492753283027
plot_id,batch_id 0 35 miss% 0.46350504319220215
plot_id,batch_id 0 36 miss% 0.5971069474568598
plot_id,batch_id 0 37 miss% 0.443754541130445
plot_id,batch_id 0 38 miss% 0.5864453541849
plot_id,batch_id 0 39 miss% 0.5487539215832867
plot_id,batch_id 0 40 miss% 0.45872737088643456
plot_id,batch_id 0 41 miss% 0.47526298724417204
plot_id,batch_id 0 42 miss% 0.4132164322222241
plot_id,batch_id 0 43 miss% 0.3712670464649995
plot_id,batch_id 0 44 miss% 0.4605596282974448
plot_id,batch_id 0 45 miss% 0.41527551557468917
plot_id,batch_id 0 46 miss% 0.4553570116864652
plot_id,batch_id 0 47 miss% 0.50568761616114
plot_id,batch_id 0 48 miss% 0.5347564649695374
plot_id,batch_id 0 49 miss% 0.5506273203914682
plot_id,batch_id 0 50 miss% 0.5334838955239531
plot_id,batch_id 0 51 miss% 0.49100439649348193
plot_id,batch_id 0 52 miss% 0.5267440196123845
plot_id,batch_id 0 53 miss% 0.4728805936943175
plot_id,batch_id 0 54 miss% 0.498625429565606
plot_id,batch_id 0 55 miss% 0.41251236813901204
plot_id,batch_id 0 56 miss% 0.5662788272475316
plot_id,batch_id 0 57 miss% 0.5634219565506541
plot_id,batch_id 0 58 miss% 0.5170804828955781
plot_id,batch_id 0 59 miss% 0.5586691017195407
plot_id,batch_id 0 60 miss% 0.30696894190786544
plot_id,batch_id 0 61 miss% 0.34686220546439533
plot_id,batch_id 0 62 miss% 0.5501641801424599
plot_id,batch_id 0 63 miss% 0.4396936916509022
plot_id,batch_id 0 64 miss% 0.4720766769169084
plot_id,batch_id 0 65 miss% 0.3871080265055968
plot_id,batch_id 0 66 miss% 0.43264901318096244
plot_id,batch_id 0 67 miss% 0.36249242805687604
plot_id,batch_id 0 68 miss% 0.5146803962063924
plot_id,batch_id 0 69 miss% 0.5578631489514457
plot_id,batch_id 0 70 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  209681
Epoch:0, Train loss:0.623425, valid loss:0.630479
Epoch:1, Train loss:0.045064, valid loss:0.007979
Epoch:2, Train loss:0.014114, valid loss:0.004294
Epoch:3, Train loss:0.011316, valid loss:0.003564
Epoch:4, Train loss:0.008109, valid loss:0.002538
Epoch:5, Train loss:0.005586, valid loss:0.001829
Epoch:6, Train loss:0.003640, valid loss:0.001506
Epoch:7, Train loss:0.003124, valid loss:0.001560
Epoch:8, Train loss:0.003059, valid loss:0.001623
Epoch:9, Train loss:0.002614, valid loss:0.001321
Epoch:10, Train loss:0.002436, valid loss:0.001337
Epoch:11, Train loss:0.001761, valid loss:0.001004
Epoch:12, Train loss:0.001686, valid loss:0.000968
Epoch:13, Train loss:0.001615, valid loss:0.000947
Epoch:14, Train loss:0.001557, valid loss:0.000985
Epoch:15, Train loss:0.001544, valid loss:0.000984
Epoch:16, Train loss:0.001520, valid loss:0.001059
Epoch:17, Train loss:0.001425, valid loss:0.001039
Epoch:18, Train loss:0.001350, valid loss:0.000967
Epoch:19, Train loss:0.001308, valid loss:0.000972
Epoch:20, Train loss:0.001326, valid loss:0.000831
Epoch:21, Train loss:0.000940, valid loss:0.000755
Epoch:22, Train loss:0.000889, valid loss:0.000685
Epoch:23, Train loss:0.000930, valid loss:0.000805
Epoch:24, Train loss:0.000888, valid loss:0.000714
Epoch:25, Train loss:0.000846, valid loss:0.000741
Epoch:26, Train loss:0.000852, valid loss:0.000694
Epoch:27, Train loss:0.000817, valid loss:0.000785
Epoch:28, Train loss:0.000831, valid loss:0.000705
Epoch:29, Train loss:0.000779, valid loss:0.000714
Epoch:30, Train loss:0.000793, valid loss:0.000784
Epoch:31, Train loss:0.000635, valid loss:0.000672
Epoch:32, Train loss:0.000600, valid loss:0.000645
Epoch:33, Train loss:0.000595, valid loss:0.000649
Epoch:34, Train loss:0.000596, valid loss:0.000612
Epoch:35, Train loss:0.000576, valid loss:0.000677
Epoch:36, Train loss:0.000582, valid loss:0.000676
Epoch:37, Train loss:0.000579, valid loss:0.000709
Epoch:38, Train loss:0.000570, valid loss:0.000694
Epoch:39, Train loss:0.000564, valid loss:0.000631
Epoch:40, Train loss:0.000543, valid loss:0.000696
Epoch:41, Train loss:0.000485, valid loss:0.000613
Epoch:42, Train loss:0.000474, valid loss:0.000652
Epoch:43, Train loss:0.000475, valid loss:0.000604
Epoch:44, Train loss:0.000471, valid loss:0.000632
Epoch:45, Train loss:0.000470, valid loss:0.000674
Epoch:46, Train loss:0.000462, valid loss:0.000632
Epoch:47, Train loss:0.000459, valid loss:0.000608
Epoch:48, Train loss:0.000458, valid loss:0.000612
Epoch:49, Train loss:0.000454, valid loss:0.000605
Epoch:50, Train loss:0.000447, valid loss:0.000621
Epoch:51, Train loss:0.000426, valid loss:0.000618
Epoch:52, Train loss:0.000421, valid loss:0.000612
Epoch:53, Train loss:0.000418, valid loss:0.000610
Epoch:54, Train loss:0.000417, valid loss:0.000610
Epoch:55, Train loss:0.000415, valid loss:0.000617
Epoch:56, Train loss:0.000414, valid loss:0.000621
Epoch:57, Train loss:0.000414, valid loss:0.000618
Epoch:58, Train loss:0.000413, valid loss:0.000613
Epoch:59, Train loss:0.000412, valid loss:0.000617
Epoch:60, Train loss:0.000412, valid loss:0.000609
training time 13207.631562232971
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.06646389895079516
plot_id,batch_id 0 1 miss% 0.031292373066535685
plot_id,batch_id 0 2 miss% 0.08517511499336534
plot_id,batch_id 0 3 miss% 0.03674372222690387
plot_id,batch_id 0 4 miss% 0.03462617012492607
plot_id,batch_id 0 5 miss% 0.06614742835789889
plot_id,batch_id 0 6 miss% 0.04648164469868895
plot_id,batch_id 0 7 miss% 0.08564269379203468
plot_id,batch_id 0 8 miss% 0.08039007516629128
plot_id,batch_id 0 9 miss% 0.018500572743005903
plot_id,batch_id 0 10 miss% 0.024260024856509393
plot_id,batch_id 0 11 miss% 0.07350672681170131
plot_id,batch_id 0 12 miss% 0.07275846464412002
plot_id,batch_id 0 13 miss% 0.05398466595024488
plot_id,batch_id 0 14 miss% 0.053341649300292705
plot_id,batch_id 0 15 miss% 0.03596778112257618
plot_id,batch_id 0 16 miss% 0.17213040717229125
plot_id,batch_id 0 17 miss% 0.044282999880413766
plot_id,batch_id 0 18 miss% 0.049545868698637155
plot_id,batch_id 0 19 miss% 0.09830693630539214
plot_id,batch_id 0 20 miss% 0.08332830206176184
plot_id,batch_id 0 21 miss% 0.03800046109177449
plot_id,batch_id 0 22 miss% 0.03680401636672025
plot_id,batch_id 0 23 miss% 0.04463604882647368
plot_id,batch_id 0 24 miss% 0.02078213645127191
plot_id,batch_id 0 25 miss% 0.031144859812524395
plot_id,batch_id 0 26 miss% 0.03834654624451583
plot_id,batch_id 0 27 miss% 0.03714422221347318
plot_id,batch_id 0 28 miss% 0.02297421684123478
plot_id,batch_id 0 29 miss% 0.032416497927980875
plot_id,batch_id 0 30 miss% 0.04178848111702907
plot_id,batch_id 0 31 miss% 0.05943752727374807
plot_id,batch_id 0 32 miss% 0.10095225325302286
plot_id,batch_id 0 33 miss% 0.029357058194760702
plot_id,batch_id 0 34 miss% 0.026305563017225204
plot_id,batch_id 0 35 miss% 0.05785050052658023
plot_id,batch_id 0 36 miss% 0.061508949248860655
plot_id,batch_id 0 37 miss% 0.07436127222094135
plot_id,batch_id 0 38 miss% 0.04344251982314335
plot_id,batch_id 0 39 miss% 0.02977490320872315
plot_id,batch_id 0 40 miss% 0.09132610030157892
plot_id,batch_id 0 41 miss% 0.05173646637355125
plot_id,batch_id 0 42 miss% 0.03003959100864335
plot_id,batch_id 0 43 miss% 0.03428798385600581
plot_id,batch_id 0 44 miss% 0.03129701340963991
plot_id,batch_id 0 45 miss% 0.03539671683796169
plot_id,batch_id 0 46 miss% 0.029870407798282827
plot_id,batch_id 0 47 miss% 0.030052428532704865
plot_id,batch_id 0 48 miss% 0.027687967813479375
plot_id,batch_id 0 49 miss% 0.016147163154149137
plot_id,batch_id 0 50 miss% 0.11482560284481357
plot_id,batch_id 0 51 miss% 0.02852454007166042
plot_id,batch_id 0 52 miss% 0.021347454144432704
plot_id,batch_id 0 53 miss% 0.015574314360980454
plot_id,batch_id 0 54 miss% 0.02320517646071873
plot_id,batch_id 0 55 miss% 0.06685420112640691
plot_id,batch_id 0 56 miss% 0.05185376825558037
plot_id,batch_id 0 57 miss% 0.02782198568623281
plot_id,batch_id 0 58 miss% 0.03257645832404614
plot_id,batch_id 0 59 miss% 0.020953658327582003
plot_id,batch_id 0 60 miss% 0.03471992762421292
plot_id,batch_id 0 61 miss% 0.03284448389983845
plot_id,batch_id 0 62 miss% 0.05504486141864348
plot_id,batch_id 0 63 miss% 0.06270164528916324
plot_id,batch_id 0 64 miss% 0.03202285194070717
plot_id,batch_id 0 65 miss% 0.13023113646274653
plot_id,batch_id 0 66 miss% 0.05549778671843572
plot_id,batch_id 0 67 miss% 0.03582155046778806
plot_id,batch_id 0 68 miss% 0.2852662650032068
plot_id,batch_id 0 71 miss% 0.43797174159832236
plot_id,batch_id 0 72 miss% 0.4680141655452923
plot_id,batch_id 0 73 miss% 0.42695696743913175
plot_id,batch_id 0 74 miss% 0.42359617133893923
plot_id,batch_id 0 75 miss% 0.2895827025443822
plot_id,batch_id 0 76 miss% 0.42706790102565373
plot_id,batch_id 0 77 miss% 0.43769057672576783
plot_id,batch_id 0 78 miss% 0.4046587798193938
plot_id,batch_id 0 79 miss% 0.4744130771929618
plot_id,batch_id 0 80 miss% 0.3619303043661541
plot_id,batch_id 0 81 miss% 0.5189959976451594
plot_id,batch_id 0 82 miss% 0.3924418616812979
plot_id,batch_id 0 83 miss% 0.5211456789576715
plot_id,batch_id 0 84 miss% 0.5047692455754723
plot_id,batch_id 0 85 miss% 0.3244903053019288
plot_id,batch_id 0 86 miss% 0.4401595274379007
plot_id,batch_id 0 87 miss% 0.4461041424509035
plot_id,batch_id 0 88 miss% 0.4865218575413849
plot_id,batch_id 0 89 miss% 0.47801304449658005
plot_id,batch_id 0 90 miss% 0.32612277037945236
plot_id,batch_id 0 91 miss% 0.3754004509507808
plot_id,batch_id 0 92 miss% 0.3572774497746722
plot_id,batch_id 0 93 miss% 0.339648120959155
plot_id,batch_id 0 94 miss% 0.5524206840817472
plot_id,batch_id 0 95 miss% 0.337774524714128
plot_id,batch_id 0 96 miss% 0.40163253250305525
plot_id,batch_id 0 97 miss% 0.5087795389592704
plot_id,batch_id 0 98 miss% 0.4230642139851449
plot_id,batch_id 0 99 miss% 0.42037159955237846
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.3773735  0.44558265 0.55961654 0.44127514 0.40473238 0.43820938
 0.3877783  0.55367689 0.62716584 0.71195839 0.36860613 0.39393067
 0.42906674 0.41271533 0.49238505 0.46562339 0.5470739  0.48445597
 0.51241551 0.4811932  0.37882066 0.45229669 0.48940132 0.49890234
 0.48464546 0.38685363 0.46463217 0.47077835 0.56469202 0.50117148
 0.42902284 0.50593766 0.52552846 0.50446011 0.39598493 0.46350504
 0.59710695 0.44375454 0.58644535 0.54875392 0.45872737 0.47526299
 0.41321643 0.37126705 0.46055963 0.41527552 0.45535701 0.50568762
 0.53475646 0.55062732 0.5334839  0.4910044  0.52674402 0.47288059
 0.49862543 0.41251237 0.56627883 0.56342196 0.51708048 0.5586691
 0.30696894 0.34686221 0.55016418 0.43969369 0.47207668 0.38710803
 0.43264901 0.36249243 0.5146804  0.55786315 0.28526627 0.43797174
 0.46801417 0.42695697 0.42359617 0.2895827  0.4270679  0.43769058
 0.40465878 0.47441308 0.3619303  0.518996   0.39244186 0.52114568
 0.50476925 0.32449031 0.44015953 0.44610414 0.48652186 0.47801304
 0.32612277 0.37540045 0.35727745 0.33964812 0.55242068 0.33777452
 0.40163253 0.50877954 0.42306421 0.4203716 ]
for model  86 the mean error 0.4597180821048776
all id 86 hidden_dim 24 learning_rate 0.0025 num_layers 3 frames 25 out win 6 err 0.4597180821048776 time 13211.46215891838
Launcher: Job 87 completed in 13439 seconds.
Launcher: Task 53 done. Exiting.
plot_id,batch_id 0 69 miss% 0.1010407874683435
plot_id,batch_id 0 70 miss% 0.12565189073880406
plot_id,batch_id 0 71 miss% 0.0615623704001882
plot_id,batch_id 0 72 miss% 0.11144838282994592
plot_id,batch_id 0 73 miss% 0.12986268584527447
plot_id,batch_id 0 74 miss% 0.10398173899221735
plot_id,batch_id 0 75 miss% 0.20898302955553072
plot_id,batch_id 0 76 miss% 0.1519989712587568
plot_id,batch_id 0 77 miss% 0.08748719243506804
plot_id,batch_id 0 78 miss% 0.03410288473494608
plot_id,batch_id 0 79 miss% 0.04983910410451119
plot_id,batch_id 0 80 miss% 0.12435987054279891
plot_id,batch_id 0 81 miss% 0.10111074496771652
plot_id,batch_id 0 82 miss% 0.057708424150095666
plot_id,batch_id 0 83 miss% 0.06033207403002377
plot_id,batch_id 0 84 miss% 0.13800729854715288
plot_id,batch_id 0 85 miss% 0.0673954111965061
plot_id,batch_id 0 86 miss% 0.07428687868506663
plot_id,batch_id 0 87 miss% 0.09316015026786634
plot_id,batch_id 0 88 miss% 0.06641696469379246
plot_id,batch_id 0 89 miss% 0.0837353903454203
plot_id,batch_id 0 90 miss% 0.021767897396566475
plot_id,batch_id 0 91 miss% 0.07760701619017601
plot_id,batch_id 0 92 miss% 0.06887284349775263
plot_id,batch_id 0 93 miss% 0.05801622918444501
plot_id,batch_id 0 94 miss% 0.07933387986292229
plot_id,batch_id 0 95 miss% 0.05792350266139308
plot_id,batch_id 0 96 miss% 0.07865148105191727
plot_id,batch_id 0 97 miss% 0.0988323010021324
plot_id,batch_id 0 98 miss% 0.04912171758702672
plot_id,batch_id 0 99 miss% 0.11638321778668943
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07048677 0.0596217  0.10929036 0.04558255 0.06505482 0.03673856
 0.06895455 0.13359716 0.06808505 0.05491463 0.09389799 0.10241882
 0.10047696 0.06450388 0.11177909 0.04990804 0.09041314 0.06186281
 0.06465692 0.0644643  0.07968131 0.05938411 0.06938487 0.03664612
 0.04271255 0.03788549 0.04500379 0.04572746 0.02467656 0.05400647
 0.04911727 0.10774268 0.1237782  0.06026062 0.07020227 0.04528659
 0.08218998 0.08693526 0.05580287 0.03525191 0.06069892 0.03529626
 0.01823147 0.05147454 0.04449373 0.03685209 0.04670696 0.02829558
 0.04459934 0.0255302  0.10636196 0.03631742 0.02841582 0.02223582
 0.03129355 0.07811119 0.064511   0.08172199 0.05914106 0.03795831
 0.06926309 0.07047864 0.04682418 0.05692474 0.05212175 0.18558563
 0.11508799 0.03275612 0.03503814 0.10104079 0.12565189 0.06156237
 0.11144838 0.12986269 0.10398174 0.20898303 0.15199897 0.08748719
 0.03410288 0.0498391  0.12435987 0.10111074 0.05770842 0.06033207
 0.1380073  0.06739541 0.07428688 0.09316015 0.06641696 0.08373539
 0.0217679  0.07760702 0.06887284 0.05801623 0.07933388 0.0579235
 0.07865148 0.0988323  0.04912172 0.11638322]
for model  119 the mean error 0.07069694334990918
all id 119 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 25 out win 6 err 0.07069694334990918 time 13174.909881353378
Launcher: Job 120 completed in 13442 seconds.
Launcher: Task 3 done. Exiting.
 68 miss% 0.051454610025821
plot_id,batch_id 0 69 miss% 0.07345478313251033
plot_id,batch_id 0 70 miss% 0.0771542352713373
plot_id,batch_id 0 71 miss% 0.05506172471427174
plot_id,batch_id 0 72 miss% 0.07429964184524032
plot_id,batch_id 0 73 miss% 0.05240781562130893
plot_id,batch_id 0 74 miss% 0.10499653758128741
plot_id,batch_id 0 75 miss% 0.1094821767405713
plot_id,batch_id 0 76 miss% 0.09372339030311977
plot_id,batch_id 0 77 miss% 0.045243175961153154
plot_id,batch_id 0 78 miss% 0.06422567838752151
plot_id,batch_id 0 79 miss% 0.03878662059228492
plot_id,batch_id 0 80 miss% 0.08718282999360447
plot_id,batch_id 0 81 miss% 0.1093651205524553
plot_id,batch_id 0 82 miss% 0.06701755193270482
plot_id,batch_id 0 83 miss% 0.08284299672488418
plot_id,batch_id 0 84 miss% 0.10316234130352195
plot_id,batch_id 0 85 miss% 0.03512191526798674
plot_id,batch_id 0 86 miss% 0.058455508195188445
plot_id,batch_id 0 87 miss% 0.051779108069501485
plot_id,batch_id 0 88 miss% 0.07744116657651838
plot_id,batch_id 0 89 miss% 0.06388421006389672
plot_id,batch_id 0 90 miss% 0.052372093725278644
plot_id,batch_id 0 91 miss% 0.07185766636213727
plot_id,batch_id 0 92 miss% 0.04905733901535632
plot_id,batch_id 0 93 miss% 0.09096710162389161
plot_id,batch_id 0 94 miss% 0.10018753128277143
plot_id,batch_id 0 95 miss% 0.05251526706747326
plot_id,batch_id 0 96 miss% 0.07853021435576987
plot_id,batch_id 0 97 miss% 0.10524033204774343
plot_id,batch_id 0 98 miss% 0.07148589797005943
plot_id,batch_id 0 99 miss% 0.08568244957746052
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07007456 0.04638876 0.10998595 0.07573536 0.09199275 0.05880409
 0.1012732  0.11624827 0.08043864 0.03413332 0.05121678 0.09004919
 0.07062288 0.07743397 0.10472243 0.06292088 0.09783849 0.08464196
 0.08651899 0.07999025 0.06920843 0.04167068 0.0516918  0.03408079
 0.06345218 0.04609352 0.04754063 0.05467154 0.02600834 0.04331445
 0.06360716 0.10249382 0.13490041 0.06741766 0.06402373 0.06733821
 0.0794648  0.070541   0.07069499 0.06245761 0.05876937 0.0593821
 0.0215369  0.08421774 0.06192515 0.06038031 0.04101986 0.05090508
 0.03740552 0.02395183 0.18777265 0.07164153 0.03349    0.03575487
 0.03243094 0.04043359 0.07302313 0.03247738 0.03454857 0.03664058
 0.05259715 0.03705576 0.04702664 0.0439819  0.06472154 0.1559288
 0.07623976 0.03331067 0.05145461 0.07345478 0.07715424 0.05506172
 0.07429964 0.05240782 0.10499654 0.10948218 0.09372339 0.04524318
 0.06422568 0.03878662 0.08718283 0.10936512 0.06701755 0.082843
 0.10316234 0.03512192 0.05845551 0.05177911 0.07744117 0.06388421
 0.05237209 0.07185767 0.04905734 0.0909671  0.10018753 0.05251527
 0.07853021 0.10524033 0.0714859  0.08568245]
for model  100 the mean error 0.06774710804225474
all id 100 hidden_dim 16 learning_rate 0.0025 num_layers 5 frames 25 out win 5 err 0.06774710804225474 time 13179.152706384659
Launcher: Job 101 completed in 13445 seconds.
Launcher: Task 232 done. Exiting.
0.023929606886018254
plot_id,batch_id 0 69 miss% 0.05271088168304374
plot_id,batch_id 0 70 miss% 0.10586878940184223
plot_id,batch_id 0 71 miss% 0.0491907818618962
plot_id,batch_id 0 72 miss% 0.09419027426497323
plot_id,batch_id 0 73 miss% 0.04836774870152447
plot_id,batch_id 0 74 miss% 0.07510989385900678
plot_id,batch_id 0 75 miss% 0.02419419527963216
plot_id,batch_id 0 76 miss% 0.11329747846231869
plot_id,batch_id 0 77 miss% 0.0615462641751366
plot_id,batch_id 0 78 miss% 0.03686874514911118
plot_id,batch_id 0 79 miss% 0.06879478223390904
plot_id,batch_id 0 80 miss% 0.09223909470972363
plot_id,batch_id 0 81 miss% 0.09152574444769508
plot_id,batch_id 0 82 miss% 0.055461175531233066
plot_id,batch_id 0 83 miss% 0.06983646611568228
plot_id,batch_id 0 84 miss% 0.03344946265959008
plot_id,batch_id 0 85 miss% 0.06866326667493539
plot_id,batch_id 0 86 miss% 0.04883508418786603
plot_id,batch_id 0 87 miss% 0.07659125079086683
plot_id,batch_id 0 88 miss% 0.07656546076833148
plot_id,batch_id 0 89 miss% 0.04317117810379408
plot_id,batch_id 0 90 miss% 0.026807839689264583
plot_id,batch_id 0 91 miss% 0.03931714398090667
plot_id,batch_id 0 92 miss% 0.055279990093574025
plot_id,batch_id 0 93 miss% 0.025295655969037826
plot_id,batch_id 0 94 miss% 0.05875455718523807
plot_id,batch_id 0 95 miss% 0.06718130835237512
plot_id,batch_id 0 96 miss% 0.04713315528887342
plot_id,batch_id 0 97 miss% 0.061414602071188824
plot_id,batch_id 0 98 miss% 0.039882012527372265
plot_id,batch_id 0 99 miss% 0.0732871747110216
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0664639  0.03129237 0.08517511 0.03674372 0.03462617 0.06614743
 0.04648164 0.08564269 0.08039008 0.01850057 0.02426002 0.07350673
 0.07275846 0.05398467 0.05334165 0.03596778 0.17213041 0.044283
 0.04954587 0.09830694 0.0833283  0.03800046 0.03680402 0.04463605
 0.02078214 0.03114486 0.03834655 0.03714422 0.02297422 0.0324165
 0.04178848 0.05943753 0.10095225 0.02935706 0.02630556 0.0578505
 0.06150895 0.07436127 0.04344252 0.0297749  0.0913261  0.05173647
 0.03003959 0.03428798 0.03129701 0.03539672 0.02987041 0.03005243
 0.02768797 0.01614716 0.1148256  0.02852454 0.02134745 0.01557431
 0.02320518 0.0668542  0.05185377 0.02782199 0.03257646 0.02095366
 0.03471993 0.03284448 0.05504486 0.06270165 0.03202285 0.13023114
 0.05549779 0.03582155 0.02392961 0.05271088 0.10586879 0.04919078
 0.09419027 0.04836775 0.07510989 0.0241942  0.11329748 0.06154626
 0.03686875 0.06879478 0.09223909 0.09152574 0.05546118 0.06983647
 0.03344946 0.06866327 0.04883508 0.07659125 0.07656546 0.04317118
 0.02680784 0.03931714 0.05527999 0.02529566 0.05875456 0.06718131
 0.04713316 0.0614146  0.03988201 0.07328717]
for model  42 the mean error 0.052649298629113606
all id 42 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 21 out win 4 err 0.052649298629113606 time 13207.631562232971
Launcher: Job 43 completed in 13473 seconds.
Launcher: Task 234 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  55697
Epoch:0, Train loss:0.547327, valid loss:0.513868
Epoch:1, Train loss:0.287998, valid loss:0.003565
Epoch:2, Train loss:0.007081, valid loss:0.002664
Epoch:3, Train loss:0.005803, valid loss:0.002448
Epoch:4, Train loss:0.004906, valid loss:0.002044
Epoch:5, Train loss:0.004270, valid loss:0.002074
Epoch:6, Train loss:0.003892, valid loss:0.001803
Epoch:7, Train loss:0.003539, valid loss:0.001568
Epoch:8, Train loss:0.003285, valid loss:0.001463
Epoch:9, Train loss:0.003064, valid loss:0.001573
Epoch:10, Train loss:0.002881, valid loss:0.001359
Epoch:11, Train loss:0.002415, valid loss:0.001321
Epoch:12, Train loss:0.002338, valid loss:0.001255
Epoch:13, Train loss:0.002244, valid loss:0.001271
Epoch:14, Train loss:0.002182, valid loss:0.001136
Epoch:15, Train loss:0.002091, valid loss:0.001189
Epoch:16, Train loss:0.002069, valid loss:0.001130
Epoch:17, Train loss:0.002024, valid loss:0.001167
Epoch:18, Train loss:0.001918, valid loss:0.001110
Epoch:19, Train loss:0.001844, valid loss:0.001103
Epoch:20, Train loss:0.001828, valid loss:0.001015
Epoch:21, Train loss:0.001567, valid loss:0.000970
Epoch:22, Train loss:0.001554, valid loss:0.000936
Epoch:23, Train loss:0.001510, valid loss:0.000964
Epoch:24, Train loss:0.001490, valid loss:0.001062
Epoch:25, Train loss:0.001474, valid loss:0.000876
Epoch:26, Train loss:0.001452, valid loss:0.000939
Epoch:27, Train loss:0.001425, valid loss:0.000869
Epoch:28, Train loss:0.001398, valid loss:0.000819
Epoch:29, Train loss:0.001368, valid loss:0.000852
Epoch:30, Train loss:0.001340, valid loss:0.000879
Epoch:31, Train loss:0.001232, valid loss:0.000841
Epoch:32, Train loss:0.001220, valid loss:0.000809
Epoch:33, Train loss:0.001200, valid loss:0.000808
Epoch:34, Train loss:0.001197, valid loss:0.000795
Epoch:35, Train loss:0.001173, valid loss:0.000796
Epoch:36, Train loss:0.001174, valid loss:0.000805
Epoch:37, Train loss:0.001162, valid loss:0.000779
Epoch:38, Train loss:0.001148, valid loss:0.000823
Epoch:39, Train loss:0.001157, valid loss:0.000844
Epoch:40, Train loss:0.001134, valid loss:0.000792
Epoch:41, Train loss:0.001076, valid loss:0.000756
Epoch:42, Train loss:0.001062, valid loss:0.000756
Epoch:43, Train loss:0.001068, valid loss:0.000800
Epoch:44, Train loss:0.001058, valid loss:0.000754
Epoch:45, Train loss:0.001047, valid loss:0.000774
Epoch:46, Train loss:0.001041, valid loss:0.000759
Epoch:47, Train loss:0.001042, valid loss:0.000748
Epoch:48, Train loss:0.001039, valid loss:0.000757
Epoch:49, Train loss:0.001032, valid loss:0.000752
Epoch:50, Train loss:0.001024, valid loss:0.000754
Epoch:51, Train loss:0.000988, valid loss:0.000738
Epoch:52, Train loss:0.000981, valid loss:0.000743
Epoch:53, Train loss:0.000978, valid loss:0.000737
Epoch:54, Train loss:0.000976, valid loss:0.000739
Epoch:55, Train loss:0.000976, valid loss:0.000736
Epoch:56, Train loss:0.000975, valid loss:0.000736
Epoch:57, Train loss:0.000975, valid loss:0.000736
Epoch:58, Train loss:0.000974, valid loss:0.000734
Epoch:59, Train loss:0.000973, valid loss:0.000739
Epoch:60, Train loss:0.000974, valid loss:0.000737
training time 13281.984273910522
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.11151868891923554
plot_id,batch_id 0 1 miss% 0.03198980995246405
plot_id,batch_id 0 2 miss% 0.11522918433741829
plot_id,batch_id 0 3 miss% 0.042123919489741146
plot_id,batch_id 0 4 miss% 0.05436908964712989
plot_id,batch_id 0 5 miss% 0.028941831810354444
plot_id,batch_id 0 6 miss% 0.10190589260745957
plot_id,batch_id 0 7 miss% 0.09952733252073548
plot_id,batch_id 0 8 miss% 0.0725199849556342
plot_id,batch_id 0 9 miss% 0.048032274053058474
plot_id,batch_id 0 10 miss% 0.0641378655011298
plot_id,batch_id 0 11 miss% 0.09837210609005159
plot_id,batch_id 0 12 miss% 0.05784720760048885
plot_id,batch_id 0 13 miss% 0.10312889686350996
plot_id,batch_id 0 14 miss% 0.10384218886061482
plot_id,batch_id 0 15 miss% 0.041392001317569704
plot_id,batch_id 0 16 miss% 0.08850020553004426
plot_id,batch_id 0 17 miss% 0.09171508652346831
plot_id,batch_id 0 18 miss% 0.04189995093140923
plot_id,batch_id 0 19 miss% 0.11421911377406341
plot_id,batch_id 0 20 miss% 0.08237872142045244
plot_id,batch_id 0 21 miss% 0.03796400102989633
plot_id,batch_id 0 22 miss% 0.03788118449554917
plot_id,batch_id 0 23 miss% 0.03598775946841101
plot_id,batch_id 0 24 miss% 0.0557536637464098
plot_id,batch_id 0 25 miss% 0.05131292143736498
plot_id,batch_id 0 26 miss% 0.06601205031703736
plot_id,batch_id 0 27 miss% 0.07810512039220929
plot_id,batch_id 0 28 miss% 0.03947394256158901
plot_id,batch_id 0 29 miss% 0.02694667291103488
plot_id,batch_id 0 30 miss% 0.10387192000679539
plot_id,batch_id 0 31 miss% 0.10317871977709954
plot_id,batch_id 0 32 miss% 0.1030981480500293
plot_id,batch_id 0 33 miss% 0.07893860164220255
plot_id,batch_id 0 34 miss% 0.06718688383118993
plot_id,batch_id 0 35 miss% 0.053225181824067304
plot_id,batch_id 0 36 miss% 0.07928861723121292
plot_id,batch_id 0 37 miss% 0.07744500395559721
plot_id,batch_id 0 38 miss% 0.050506374448782344
plot_id,batch_id 0 39 miss% 0.05213324872773551
plot_id,batch_id 0 40 miss% 0.03632836109907905
plot_id,batch_id 0 41 miss% 0.09493868594380117
plot_id,batch_id 0 42 miss% 0.03517734353829687
plot_id,batch_id 0 43 miss% 0.06250293859841455
plot_id,batch_id 0 44 miss% 0.03463354826292688
plot_id,batch_id 0 45 miss% 0.043605007928792766
plot_id,batch_id 0 46 miss% 0.04659306939309596
plot_id,batch_id 0 47 miss% 0.03366444784787818
plot_id,batch_id 0 48 miss% 0.020324382308291828
plot_id,batch_id 0 49 miss% 0.02567675359370281
plot_id,batch_id 0 50 miss% 0.15065534782905335
plot_id,batch_id 0 51 miss% 0.06601924880390993
plot_id,batch_id 0 52 miss% 0.03409793944405057
plot_id,batch_id 0 53 miss% 0.05125633368241088
plot_id,batch_id 0 54 miss% 0.037650265612196206
plot_id,batch_id 0 55 miss% 0.09986396477156537
plot_id,batch_id 0 56 miss% 0.08914007391120446
plot_id,batch_id 0 57 miss% 0.03205766337389894
plot_id,batch_id 0 58 miss% 0.031726123732055043
plot_id,batch_id 0 59 miss% 0.04152084726926645
plot_id,batch_id 0 60 miss% 0.0526073505670209
plot_id,batch_id 0 61 miss% 0.043012852073243214
plot_id,batch_id 0 62 miss% 0.065397312635708
plot_id,batch_id 0 63 miss% 0.040723652697170515
plot_id,batch_id 0 64 miss% 0.05536966187910067
plot_id,batch_id 0 65 miss% 0.21108236735072697
plot_id,batch_id 0 66 miss% 0.07636732992142874
plot_id,batch_id 0 67 miss% 0.04205024675015706
plot_id,batch_id 0 68 miss% 0.03719067534909858
plot_id,batch_id 0 69 miss% 0.13545352346811618
plot_id,batch_id 0 70 miss% 0.1457756244497243
plot_id,batch_id 0 71 miss% 0.049952480689455285
plot_id,batch_id 0 72 miss% 0.07208030627925045
plot_id,batch_id 0 73 miss% 0.054369240771055934
plot_id,batch_id 0 74 miss% 0.05517173894507777
plot_id,batch_id 0 75 miss% 0.11781735472648364
plot_id,batch_id 0 76 miss% 0.11053826985207925
plot_id,batch_id 0 77 miss% 0.08163542292280974
plot_id,batch_id 0 78 miss% 0.0641329134768978
plot_id,batch_id 0 79 miss% 0.06713865878110907
plot_id,batch_id 0 80 miss% 0.07024112074981345
plot_id,batch_id 0 81 miss% 0.10429931525956995
plot_id,batch_id 0 82 miss% 0.06852524622860869
plot_id,batch_id 0 83 miss% 0.062220214804539035
plot_id,batch_id 0 84 miss% 0.11963121132685318
plot_id,batch_id 0 85 miss% 0.08304250261315378
plot_id,batch_id 0 86 miss% 0.08625696376049154
plot_id,batch_id 0 87 miss% 0.06325904286822928
plot_id,batch_id 0 88 miss% 0.08143675455215567
plot_id,batch_id 0 89 miss% 0.09106795664167869
plot_id,batch_id 0 90 miss% 0.032095169364567994
plot_id,batch_id 0 91 miss% 0.08361022479426308
plot_id,batch_id 0 92 miss% 0.07584051953599127
plot_id,batch_id 0 93 miss% 0.07199583792380267
plot_id,batch_id 0 94 miss% 0.09667014755623733
plot_id,batch_id 0 95 miss% 0.05724838702744862
plot_id,batch_id 0 96 miss% 0.050940186417085094
plot_id,batch_id 0 97 miss% 0.047502756640238675
plot_id,batch_id 0 98 miss% 0.031220354971927176
plot_id,batch_id 0 99 miss% 0.055868927632465946
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.11151869 0.03198981 0.11522918 0.04212392 0.05436909 0.02894183
 0.10190589 0.09952733 0.07251998 0.04803227 0.06413787 0.09837211
 0.05784721 0.1031289  0.10384219 0.041392   0.08850021 0.09171509
 0.04189995 0.11421911 0.08237872 0.037964   0.03788118 0.03598776
 0.05575366 0.05131292 0.06601205 0.07810512 0.03947394 0.02694667
 0.10387192 0.10317872 0.10309815 0.0789386  0.06718688 0.05322518
 0.07928862 0.077445   0.05050637 0.05213325 0.03632836 0.09493869
 0.03517734 0.06250294 0.03463355 0.04360501 0.04659307 0.03366445
 0.02032438 0.02567675 0.15065535 0.06601925 0.03409794 0.05125633
 0.03765027 0.09986396 0.08914007 0.03205766 0.03172612 0.04152085
 0.05260735 0.04301285 0.06539731 0.04072365 0.05536966 0.21108237
 0.07636733 0.04205025 0.03719068 0.13545352 0.14577562 0.04995248
 0.07208031 0.05436924 0.05517174 0.11781735 0.11053827 0.08163542
 0.06413291 0.06713866 0.07024112 0.10429932 0.06852525 0.06222021
 0.11963121 0.0830425  0.08625696 0.06325904 0.08143675 0.09106796
 0.03209517 0.08361022 0.07584052 0.07199584 0.09667015 0.05724839
 0.05094019 0.04750276 0.03122035 0.05586893]
for model  92 the mean error 0.06870173539758974
all id 92 hidden_dim 16 learning_rate 0.0025 num_layers 4 frames 25 out win 6 err 0.06870173539758974 time 13281.984273910522
Launcher: Job 93 completed in 13552 seconds.
Launcher: Task 79 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  209681
Epoch:0, Train loss:0.623425, valid loss:0.630479
Epoch:1, Train loss:0.050296, valid loss:0.005670
Epoch:2, Train loss:0.014641, valid loss:0.004891
Epoch:3, Train loss:0.009139, valid loss:0.002556
Epoch:4, Train loss:0.005375, valid loss:0.002470
Epoch:5, Train loss:0.004625, valid loss:0.002263
Epoch:6, Train loss:0.004347, valid loss:0.002215
Epoch:7, Train loss:0.003867, valid loss:0.001841
Epoch:8, Train loss:0.003625, valid loss:0.002165
Epoch:9, Train loss:0.003399, valid loss:0.001726
Epoch:10, Train loss:0.003076, valid loss:0.001739
Epoch:11, Train loss:0.002089, valid loss:0.001188
Epoch:12, Train loss:0.002055, valid loss:0.001106
Epoch:13, Train loss:0.001953, valid loss:0.001003
Epoch:14, Train loss:0.001934, valid loss:0.001401
Epoch:15, Train loss:0.001876, valid loss:0.001255
Epoch:16, Train loss:0.001798, valid loss:0.001224
Epoch:17, Train loss:0.001714, valid loss:0.001042
Epoch:18, Train loss:0.001691, valid loss:0.001184
Epoch:19, Train loss:0.001678, valid loss:0.001222
Epoch:20, Train loss:0.001530, valid loss:0.000904
Epoch:21, Train loss:0.001094, valid loss:0.000759
Epoch:22, Train loss:0.001045, valid loss:0.000779
Epoch:23, Train loss:0.001024, valid loss:0.000831
Epoch:24, Train loss:0.001014, valid loss:0.000863
Epoch:25, Train loss:0.000995, valid loss:0.000778
Epoch:26, Train loss:0.000940, valid loss:0.000722
Epoch:27, Train loss:0.000949, valid loss:0.000743
Epoch:28, Train loss:0.000950, valid loss:0.000823
Epoch:29, Train loss:0.000919, valid loss:0.000784
Epoch:30, Train loss:0.000910, valid loss:0.000788
Epoch:31, Train loss:0.000682, valid loss:0.000677
Epoch:32, Train loss:0.000656, valid loss:0.000660
Epoch:33, Train loss:0.000649, valid loss:0.000682
Epoch:34, Train loss:0.000628, valid loss:0.000679
Epoch:35, Train loss:0.000629, valid loss:0.000677
Epoch:36, Train loss:0.000620, valid loss:0.000648
Epoch:37, Train loss:0.000639, valid loss:0.000668
Epoch:38, Train loss:0.000607, valid loss:0.000660
Epoch:39, Train loss:0.000615, valid loss:0.000702
Epoch:40, Train loss:0.000583, valid loss:0.000629
Epoch:41, Train loss:0.000504, valid loss:0.000637
Epoch:42, Train loss:0.000489, valid loss:0.000617
Epoch:43, Train loss:0.000488, valid loss:0.000634
Epoch:44, Train loss:0.000484, valid loss:0.000595
Epoch:45, Train loss:0.000474, valid loss:0.000643
Epoch:46, Train loss:0.000469, valid loss:0.000590
Epoch:47, Train loss:0.000471, valid loss:0.000627
Epoch:48, Train loss:0.000472, valid loss:0.000621
Epoch:49, Train loss:0.000463, valid loss:0.000631
Epoch:50, Train loss:0.000451, valid loss:0.000648
Epoch:51, Train loss:0.000429, valid loss:0.000599
Epoch:52, Train loss:0.000423, valid loss:0.000597
Epoch:53, Train loss:0.000421, valid loss:0.000594
Epoch:54, Train loss:0.000420, valid loss:0.000596
Epoch:55, Train loss:0.000419, valid loss:0.000597
Epoch:56, Train loss:0.000417, valid loss:0.000598
Epoch:57, Train loss:0.000417, valid loss:0.000596
Epoch:58, Train loss:0.000416, valid loss:0.000596
Epoch:59, Train loss:0.000415, valid loss:0.000599
Epoch:60, Train loss:0.000415, valid loss:0.000597
training time 13381.745723485947
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08320497348312142
plot_id,batch_id 0 1 miss% 0.07999396400231416
plot_id,batch_id 0 2 miss% 0.08498682665615495
plot_id,batch_id 0 3 miss% 0.02704682127446912
plot_id,batch_id 0 4 miss% 0.04739284002428554
plot_id,batch_id 0 5 miss% 0.047689055901172075
plot_id,batch_id 0 6 miss% 0.04334987169605221
plot_id,batch_id 0 7 miss% 0.058648631981724084
plot_id,batch_id 0 8 miss% 0.16943750623763174
plot_id,batch_id 0 9 miss% 0.0220686314214178
plot_id,batch_id 0 10 miss% 0.021393486819461783
plot_id,batch_id 0 11 miss% 0.06247306512443239
plot_id,batch_id 0 12 miss% 0.05243821168357745
plot_id,batch_id 0 13 miss% 0.03498897991314889
plot_id,batch_id 0 14 miss% 0.04735678232375586
plot_id,batch_id 0 15 miss% 0.027703823335356686
plot_id,batch_id 0 16 miss% 0.1527245339624873
plot_id,batch_id 0 17 miss% 0.03843762276505805
plot_id,batch_id 0 18 miss% 0.04872292931642213
plot_id,batch_id 0 19 miss% 0.06425644723274081
plot_id,batch_id 0 20 miss% 0.05835063936910394
plot_id,batch_id 0 21 miss% 0.02523548629422075
plot_id,batch_id 0 22 miss% 0.056919736491701095
plot_id,batch_id 0 23 miss% 0.0228293364014809
plot_id,batch_id 0 24 miss% 0.04086093912135911
plot_id,batch_id 0 25 miss% 0.0630167343186881
plot_id,batch_id 0 26 miss% 0.028995825446994184
plot_id,batch_id 0 27 miss% 0.02327938298353392
plot_id,batch_id 0 28 miss% 0.02393093765821738
plot_id,batch_id 0 29 miss% 0.02230104926533571
plot_id,batch_id 0 30 miss% 0.013855225401711425
plot_id,batch_id 0 31 miss% 0.05081906018345127
plot_id,batch_id 0 32 miss% 0.044448408312778305
plot_id,batch_id 0 33 miss% 0.03567501655871124
plot_id,batch_id 0 34 miss% 0.028926557916074257
plot_id,batch_id 0 35 miss% 0.05706174007123048
plot_id,batch_id 0 36 miss% 0.02199831779171651
plot_id,batch_id 0 37 miss% 0.03859735108385311
plot_id,batch_id 0 38 miss% 0.07717417847445558
plot_id,batch_id 0 39 miss% 0.041370845571241224
plot_id,batch_id 0 40 miss% 0.07073685629400257
plot_id,batch_id 0 41 miss% 0.035118303974393125
plot_id,batch_id 0 42 miss% 0.025067699545620038
plot_id,batch_id 0 43 miss% 0.039592892108331196
plot_id,batch_id 0 44 miss% 0.024046264148040078
plot_id,batch_id 0 45 miss% 0.03730705617126545
plot_id,batch_id 0 46 miss% 0.025390279531324098
plot_id,batch_id 0 47 miss% 0.020868189505386544
plot_id,batch_id 0 48 miss% 0.016991369434538852
plot_id,batch_id 0 49 miss% 0.019022409207069404
plot_id,batch_id 0 50 miss% 0.0988918892673407
plot_id,batch_id 0 51 miss% 0.02142447315597765
plot_id,batch_id 0 52 miss% 0.030166656965751437
plot_id,batch_id 0 53 miss% 0.03062247141058548
plot_id,batch_id 0 54 miss% 0.02102822696827471
plot_id,batch_id 0 55 miss% 0.04222382927876166
plot_id,batch_id 0 56 miss% 0.0953024933168547
plot_id,batch_id 0 57 miss% 0.038887529621274425
plot_id,batch_id 0 58 miss% 0.02494125652950198
plot_id,batch_id 0 59 miss% 0.024979785608332992
plot_id,batch_id 0 60 miss% 0.05883735255344298
plot_id,batch_id 0 61 miss% 0.020320924783239745
plot_id,batch_id 0 62 miss% 0.08765635114707562
plot_id,batch_id 0 63 miss% 0.044414332958217305
plot_id,batch_id 0 64 miss% 0.06048186458271217
plot_id,batch_id 0 65 miss% 0.07279935062012281
plot_id,batch_id 0 66 miss% 0.029238378213042208
plot_id,batch_id 0 67 miss% 0.028280759440747226
plot_id,batch_id 0 68 miss% 0.04199593969821767
plot_id,batch_id 0 69 miss% 0.0567064649503752
plot_id,batch_id 0 70 miss% 0.055765349639221244
plot_id,batch_id 0 71 miss% 0.038467212545068144
plot_id,batch_id 0 72 miss% 0.07846381377212037
plot_id,batch_id 0 73 miss% 0.030888650434222068
plot_id,batch_id 0 74 miss% 0.07318171489350543
plot_id,batch_id 0 75 miss% 0.048474245566136255
plot_id,batch_id 0 76 miss% 0.047109147294680784
plot_id,batch_id 0 77 miss% 0.050092819819383634
plot_id,batch_id 0 78 miss% 0.01955805624717829
plot_id,batch_id 0 79 miss% 0.10706885424526107
plot_id,batch_id 0 80 miss% 0.06121074759595723
plot_id,batch_id 0 81 miss% 0.06271103755922706
plot_id,batch_id 0 82 miss% 0.06477204315471678
plot_id,batch_id 0 83 miss% 0.10468481868504884
plot_id,batch_id 0 84 miss% 0.03431055143739298
plot_id,batch_id 0 85 miss% 0.034219630586397536
plot_id,batch_id 0 86 miss% 0.03856173808049529
plot_id,batch_id 0 87 miss% 0.03330340035564501
plot_id,batch_id 0 88 miss% 0.05680187492653077
plot_id,batch_id 0 89 miss% 0.04013651984114125
plot_id,batch_id 0 90 miss% 0.021954987102774232
plot_id,batch_id 0 91 miss% 0.047195183436430664
plot_id,batch_id 0 92 miss% 0.03573284628560102
plot_id,batch_id 0 93 miss% 0.04258572544967028
plot_id,batch_id 0 94 miss% 0.05502016902909689
plot_id,batch_id 0 95 miss% 0.11802626407540016
plot_id,batch_id 0 96 miss% 0.04845027843339386
plot_id,batch_id 0 97 miss% 0.05036266135973897
plot_id,batch_id 0 98 miss% 0.05021355362079323
plot_id,batch_id 0 99 miss% 0.07398943354116057
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08320497 0.07999396 0.08498683 0.02704682 0.04739284 0.04768906
 0.04334987 0.05864863 0.16943751 0.02206863 0.02139349 0.06247307
 0.05243821 0.03498898 0.04735678 0.02770382 0.15272453 0.03843762
 0.04872293 0.06425645 0.05835064 0.02523549 0.05691974 0.02282934
 0.04086094 0.06301673 0.02899583 0.02327938 0.02393094 0.02230105
 0.01385523 0.05081906 0.04444841 0.03567502 0.02892656 0.05706174
 0.02199832 0.03859735 0.07717418 0.04137085 0.07073686 0.0351183
 0.0250677  0.03959289 0.02404626 0.03730706 0.02539028 0.02086819
 0.01699137 0.01902241 0.09889189 0.02142447 0.03016666 0.03062247
 0.02102823 0.04222383 0.09530249 0.03888753 0.02494126 0.02497979
 0.05883735 0.02032092 0.08765635 0.04441433 0.06048186 0.07279935
 0.02923838 0.02828076 0.04199594 0.05670646 0.05576535 0.03846721
 0.07846381 0.03088865 0.07318171 0.04847425 0.04710915 0.05009282
 0.01955806 0.10706885 0.06121075 0.06271104 0.06477204 0.10468482
 0.03431055 0.03421963 0.03856174 0.0333034  0.05680187 0.04013652
 0.02195499 0.04719518 0.03573285 0.04258573 0.05502017 0.11802626
 0.04845028 0.05036266 0.05021355 0.07398943]
for model  69 the mean error 0.04856616753873852
all id 69 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 21 out win 4 err 0.04856616753873852 time 13381.745723485947
Launcher: Job 70 completed in 13644 seconds.
Launcher: Task 248 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  55697
Epoch:0, Train loss:0.434513, valid loss:0.402725
Epoch:1, Train loss:0.097358, valid loss:0.002970
Epoch:2, Train loss:0.005277, valid loss:0.002300
Epoch:3, Train loss:0.004248, valid loss:0.001405
Epoch:4, Train loss:0.002936, valid loss:0.001276
Epoch:5, Train loss:0.002633, valid loss:0.001144
Epoch:6, Train loss:0.002427, valid loss:0.001174
Epoch:7, Train loss:0.002275, valid loss:0.001120
Epoch:8, Train loss:0.002154, valid loss:0.001086
Epoch:9, Train loss:0.002053, valid loss:0.001030
Epoch:10, Train loss:0.001971, valid loss:0.001109
Epoch:11, Train loss:0.001496, valid loss:0.000760
Epoch:12, Train loss:0.001449, valid loss:0.000752
Epoch:13, Train loss:0.001406, valid loss:0.000744
Epoch:14, Train loss:0.001394, valid loss:0.000731
Epoch:15, Train loss:0.001341, valid loss:0.000678
Epoch:16, Train loss:0.001269, valid loss:0.000899
Epoch:17, Train loss:0.001269, valid loss:0.000713
Epoch:18, Train loss:0.001214, valid loss:0.000742
Epoch:19, Train loss:0.001177, valid loss:0.000689
Epoch:20, Train loss:0.001166, valid loss:0.000689
Epoch:21, Train loss:0.000934, valid loss:0.000643
Epoch:22, Train loss:0.000912, valid loss:0.000607
Epoch:23, Train loss:0.000894, valid loss:0.000667
Epoch:24, Train loss:0.000891, valid loss:0.000591
Epoch:25, Train loss:0.000876, valid loss:0.000569
Epoch:26, Train loss:0.000870, valid loss:0.000558
Epoch:27, Train loss:0.000833, valid loss:0.000566
Epoch:28, Train loss:0.000836, valid loss:0.000598
Epoch:29, Train loss:0.000827, valid loss:0.000565
Epoch:30, Train loss:0.000822, valid loss:0.000576
Epoch:31, Train loss:0.000701, valid loss:0.000532
Epoch:32, Train loss:0.000690, valid loss:0.000538
Epoch:33, Train loss:0.000678, valid loss:0.000518
Epoch:34, Train loss:0.000687, valid loss:0.000526
Epoch:35, Train loss:0.000670, valid loss:0.000516
Epoch:36, Train loss:0.000662, valid loss:0.000519
Epoch:37, Train loss:0.000657, valid loss:0.000539
Epoch:38, Train loss:0.000654, valid loss:0.000620
Epoch:39, Train loss:0.000655, valid loss:0.000525
Epoch:40, Train loss:0.000642, valid loss:0.000504
Epoch:41, Train loss:0.000582, valid loss:0.000507
Epoch:42, Train loss:0.000581, valid loss:0.000509
Epoch:43, Train loss:0.000572, valid loss:0.000504
Epoch:44, Train loss:0.000574, valid loss:0.000533
Epoch:45, Train loss:0.000567, valid loss:0.000492
Epoch:46, Train loss:0.000566, valid loss:0.000504
Epoch:47, Train loss:0.000562, valid loss:0.000507
Epoch:48, Train loss:0.000565, valid loss:0.000516
Epoch:49, Train loss:0.000559, valid loss:0.000501
Epoch:50, Train loss:0.000559, valid loss:0.000495
Epoch:51, Train loss:0.000527, valid loss:0.000488
Epoch:52, Train loss:0.000524, valid loss:0.000489
Epoch:53, Train loss:0.000522, valid loss:0.000488
Epoch:54, Train loss:0.000521, valid loss:0.000488
Epoch:55, Train loss:0.000520, valid loss:0.000488
Epoch:56, Train loss:0.000519, valid loss:0.000488
Epoch:57, Train loss:0.000518, valid loss:0.000488
Epoch:58, Train loss:0.000518, valid loss:0.000488
Epoch:59, Train loss:0.000518, valid loss:0.000487
Epoch:60, Train loss:0.000517, valid loss:0.000489
training time 13431.301610946655
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07515093926758194
plot_id,batch_id 0 1 miss% 0.04038096688905719
plot_id,batch_id 0 2 miss% 0.13893957417539096
plot_id,batch_id 0 3 miss% 0.07972208743323887
plot_id,batch_id 0 4 miss% 0.04689559613519861
plot_id,batch_id 0 5 miss% 0.07026712966867299
plot_id,batch_id 0 6 miss% 0.05987157382349422
plot_id,batch_id 0 7 miss% 0.049758167201173924
plot_id,batch_id 0 8 miss% 0.08972774710658132
plot_id,batch_id 0 9 miss% 0.05157076140151934
plot_id,batch_id 0 10 miss% 0.04897820885218751
plot_id,batch_id 0 11 miss% 0.09898196882782176
plot_id,batch_id 0 12 miss% 0.07233788553180949
plot_id,batch_id 0 13 miss% 0.07370671101487221
plot_id,batch_id 0 14 miss% 0.08607722063024749
plot_id,batch_id 0 15 miss% 0.045110254713215135
plot_id,batch_id 0 16 miss% 0.09405931414450655
plot_id,batch_id 0 17 miss% 0.042741674701419805
plot_id,batch_id 0 18 miss% 0.06569665568234863
plot_id,batch_id 0 19 miss% 0.07360905964951504
plot_id,batch_id 0 20 miss% 0.05146301416146847
plot_id,batch_id 0 21 miss% 0.04039139770606115
plot_id,batch_id 0 22 miss% 0.044580183604969806
plot_id,batch_id 0 23 miss% 0.029104651840872922
plot_id,batch_id 0 24 miss% 0.06533723729128167
plot_id,batch_id 0 25 miss% 0.05235810058928179
plot_id,batch_id 0 26 miss% 0.0788363622190771
plot_id,batch_id 0 27 miss% 0.04256468981439427
plot_id,batch_id 0 28 miss% 0.07250312625520405
plot_id,batch_id 0 29 miss% 0.05018839484039497
plot_id,batch_id 0 30 miss% 0.05479377957515994
plot_id,batch_id 0 31 miss% 0.12208201620191934
plot_id,batch_id 0 32 miss% 0.08473710824245992
plot_id,batch_id 0 33 miss% 0.04295795217621942
plot_id,batch_id 0 34 miss% 0.04899133747141499
plot_id,batch_id 0 35 miss% 0.052310172603614445
plot_id,batch_id 0 36 miss% 0.14637901383495133
plot_id,batch_id 0 37 miss% 0.09840102918085586
plot_id,batch_id 0 38 miss% 0.07951022648843008
plot_id,batch_id 0 39 miss% 0.03190910193602561
plot_id,batch_id 0 40 miss% 0.12482359680193808
plot_id,batch_id 0 41 miss% 0.05751079667902927
plot_id,batch_id 0 42 miss% 0.03113579797318838
plot_id,batch_id 0 43 miss% 0.04346964308340096
plot_id,batch_id 0 44 miss% 0.029721049022786987
plot_id,batch_id 0 45 miss% 0.05604695672884747
plot_id,batch_id 0 46 miss% 0.023736426602981083
plot_id,batch_id 0 47 miss% 0.025821753691413315
plot_id,batch_id 0 48 miss% 0.04661338751702978
plot_id,batch_id 0 49 miss% 0.02602335461910482
plot_id,batch_id 0 50 miss% 0.08591904717021913
plot_id,batch_id 0 51 miss% 0.028666125964174536
plot_id,batch_id 0 52 miss% 0.02435420325612565
plot_id,batch_id 0 53 miss% 0.024447764247110988
plot_id,batch_id 0 54 miss% 0.050482889504881015
plot_id,batch_id 0 55 miss% 0.0422472315560737
plot_id,batch_id 0 56 miss% 0.05672897220320769
plot_id,batch_id 0 57 miss% 0.035367218346129305
plot_id,batch_id 0 58 miss% 0.033853907150994816
plot_id,batch_id 0 59 miss% 0.09416562499793897
plot_id,batch_id 0 60 miss% 0.04658754460556901
plot_id,batch_id 0 61 miss% 0.0313469482844237
plot_id,batch_id 0 62 miss% 0.04718975325265401
plot_id,batch_id 0 63 miss% 0.03491272758578629
plot_id,batch_id 0 64 miss% 0.07518455483408437
plot_id,batch_id 0 65 miss% 0.11622111421792454
plot_id,batch_id 0 66 miss% 0.03822895894107843
plot_id,batch_id 0 67 miss% 0.025828607077231596
plot_id,batch_id 0 68 miss% 0.05324860801454906
plot_id,batch_id 0 69 miss% 0.07773709565338496
plot_id,batch_id 0 70 miss% 0.0830594367594212
plot_id,batch_id 0 71 miss% 0.04470180429798769
plot_id,batch_id 0 72 miss% 0.1131354612850405
plot_id,batch_id 0 73 miss% 0.06165900417613551
plot_id,batch_id 0 74 miss% 0.1148970198904506
plot_id,batch_id 0 75 miss% 0.09597654977041357
plot_id,batch_id 0 76 miss% 0.08342186413586222
plot_id,batch_id 0 77 miss% 0.0751715144323876
plot_id,batch_id 0 78 miss% 0.0475909239702349
plot_id,batch_id 0 79 miss% 0.11476481704632581
plot_id,batch_id 0 80 miss% 0.03645561877056643
plot_id,batch_id 0 81 miss% 0.07980232058511891
plot_id,batch_id 0 82 miss% 0.09165466152763611
plot_id,batch_id 0 83 miss% 0.060842036412544896
plot_id,batch_id 0 84 miss% 0.12482327452868136
plot_id,batch_id 0 85 miss% 0.0603000702286568
plot_id,batch_id 0 86 miss% 0.08259537211839149
plot_id,batch_id 0 87 miss% 0.07725796823650788
plot_id,batch_id 0 88 miss% 0.06418747052343979
plot_id,batch_id 0 89 miss% 0.05601254247789632
plot_id,batch_id 0 90 miss% 0.045512610917040404
plot_id,batch_id 0 91 miss% 0.07808977349714169
plot_id,batch_id 0 92 miss% 0.02796862383253929
plot_id,batch_id 0 93 miss% 0.024455816224712747
plot_id,batch_id 0 94 miss% 0.0808513806627476
plot_id,batch_id 0 95 miss% 0.06997090037489499
plot_id,batch_id 0 96 miss% 0.05823759599974371
plot_id,batch_id 0 97 miss% 0.06128828473944693
plot_id,batch_id 0 98 miss% 0.026545568036965776
plot_id,batch_id 0 99 miss% 0.03969276871767006
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07515094 0.04038097 0.13893957 0.07972209 0.0468956  0.07026713
 0.05987157 0.04975817 0.08972775 0.05157076 0.04897821 0.09898197
 0.07233789 0.07370671 0.08607722 0.04511025 0.09405931 0.04274167
 0.06569666 0.07360906 0.05146301 0.0403914  0.04458018 0.02910465
 0.06533724 0.0523581  0.07883636 0.04256469 0.07250313 0.05018839
 0.05479378 0.12208202 0.08473711 0.04295795 0.04899134 0.05231017
 0.14637901 0.09840103 0.07951023 0.0319091  0.1248236  0.0575108
 0.0311358  0.04346964 0.02972105 0.05604696 0.02373643 0.02582175
 0.04661339 0.02602335 0.08591905 0.02866613 0.0243542  0.02444776
 0.05048289 0.04224723 0.05672897 0.03536722 0.03385391 0.09416562
 0.04658754 0.03134695 0.04718975 0.03491273 0.07518455 0.11622111
 0.03822896 0.02582861 0.05324861 0.0777371  0.08305944 0.0447018
 0.11313546 0.061659   0.11489702 0.09597655 0.08342186 0.07517151
 0.04759092 0.11476482 0.03645562 0.07980232 0.09165466 0.06084204
 0.12482327 0.06030007 0.08259537 0.07725797 0.06418747 0.05601254
 0.04551261 0.07808977 0.02796862 0.02445582 0.08085138 0.0699709
 0.0582376  0.06128828 0.02654557 0.03969277]
for model  226 the mean error 0.06261527106643776
all id 226 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 31 out win 5 err 0.06261527106643776 time 13431.301610946655
Launcher: Job 227 completed in 13693 seconds.
Launcher: Task 150 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  151697
Epoch:0, Train loss:0.707612, valid loss:0.660109
Epoch:1, Train loss:0.441845, valid loss:0.005298
Epoch:2, Train loss:0.009352, valid loss:0.003557
Epoch:3, Train loss:0.007387, valid loss:0.003018
Epoch:4, Train loss:0.006421, valid loss:0.002595
Epoch:5, Train loss:0.005781, valid loss:0.002539
Epoch:6, Train loss:0.004853, valid loss:0.001940
Epoch:7, Train loss:0.003968, valid loss:0.001753
Epoch:8, Train loss:0.003555, valid loss:0.001424
Epoch:9, Train loss:0.003292, valid loss:0.001748
Epoch:10, Train loss:0.002940, valid loss:0.001424
Epoch:11, Train loss:0.002236, valid loss:0.001295
Epoch:12, Train loss:0.002110, valid loss:0.001269
Epoch:13, Train loss:0.002093, valid loss:0.001210
Epoch:14, Train loss:0.001952, valid loss:0.001074
Epoch:15, Train loss:0.001907, valid loss:0.001056
Epoch:16, Train loss:0.001849, valid loss:0.001125
Epoch:17, Train loss:0.001779, valid loss:0.001020
Epoch:18, Train loss:0.001700, valid loss:0.001039
Epoch:19, Train loss:0.001654, valid loss:0.000922
Epoch:20, Train loss:0.001587, valid loss:0.001015
Epoch:21, Train loss:0.001256, valid loss:0.000960
Epoch:22, Train loss:0.001218, valid loss:0.000757
Epoch:23, Train loss:0.001203, valid loss:0.000803
Epoch:24, Train loss:0.001200, valid loss:0.000788
Epoch:25, Train loss:0.001156, valid loss:0.000771
Epoch:26, Train loss:0.001145, valid loss:0.000868
Epoch:27, Train loss:0.001114, valid loss:0.000826
Epoch:28, Train loss:0.001087, valid loss:0.000804
Epoch:29, Train loss:0.001120, valid loss:0.000811
Epoch:30, Train loss:0.001077, valid loss:0.000716
Epoch:31, Train loss:0.000910, valid loss:0.000717
Epoch:32, Train loss:0.000900, valid loss:0.000768
Epoch:33, Train loss:0.000884, valid loss:0.000695
Epoch:34, Train loss:0.000880, valid loss:0.000667
Epoch:35, Train loss:0.000871, valid loss:0.000691
Epoch:36, Train loss:0.000870, valid loss:0.000760
Epoch:37, Train loss:0.000856, valid loss:0.000695
Epoch:38, Train loss:0.000843, valid loss:0.000706
Epoch:39, Train loss:0.000822, valid loss:0.000709
Epoch:40, Train loss:0.000822, valid loss:0.000670
Epoch:41, Train loss:0.000750, valid loss:0.000656
Epoch:42, Train loss:0.000749, valid loss:0.000639
Epoch:43, Train loss:0.000746, valid loss:0.000654
Epoch:44, Train loss:0.000739, valid loss:0.000650
Epoch:45, Train loss:0.000736, valid loss:0.000666
Epoch:46, Train loss:0.000742, valid loss:0.000701
Epoch:47, Train loss:0.000727, valid loss:0.000680
Epoch:48, Train loss:0.000724, valid loss:0.000672
Epoch:49, Train loss:0.000716, valid loss:0.000665
Epoch:50, Train loss:0.000718, valid loss:0.000658
Epoch:51, Train loss:0.000674, valid loss:0.000665
Epoch:52, Train loss:0.000669, valid loss:0.000637
Epoch:53, Train loss:0.000666, valid loss:0.000638
Epoch:54, Train loss:0.000665, valid loss:0.000636
Epoch:55, Train loss:0.000664, valid loss:0.000632
Epoch:56, Train loss:0.000663, valid loss:0.000633
Epoch:57, Train loss:0.000663, valid loss:0.000639
Epoch:58, Train loss:0.000661, valid loss:0.000634
Epoch:59, Train loss:0.000661, valid loss:0.000638
Epoch:60, Train loss:0.000661, valid loss:0.000634
training time 13493.509044408798
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08338117697793668
plot_id,batch_id 0 1 miss% 0.10907106682908572
plot_id,batch_id 0 2 miss% 0.11663735363096289
plot_id,batch_id 0 3 miss% 0.07337660693925933
plot_id,batch_id 0 4 miss% 0.03090045204955291
plot_id,batch_id 0 5 miss% 0.028977072159341314
plot_id,batch_id 0 6 miss% 0.04679798415759477
plot_id,batch_id 0 7 miss% 0.10577037680036193
plot_id,batch_id 0 8 miss% 0.10658411726526378
plot_id,batch_id 0 9 miss% 0.07157624456286718
plot_id,batch_id 0 10 miss% 0.06548297958352446
plot_id,batch_id 0 11 miss% 0.06695400905778173
plot_id,batch_id 0 12 miss% 0.04779186559094777
plot_id,batch_id 0 13 miss% 0.05033412582561235
plot_id,batch_id 0 14 miss% 0.08698955984177655
plot_id,batch_id 0 15 miss% 0.03216912152213549
plot_id,batch_id 0 16 miss% 0.19836893202383393
plot_id,batch_id 0 17 miss% 0.0909534682103268
plot_id,batch_id 0 18 miss% 0.05250566528391095
plot_id,batch_id 0 19 miss% 0.07059151814914026
plot_id,batch_id 0 20 miss% 0.06315176780438306
plot_id,batch_id 0 21 miss% 0.07557585256097221
plot_id,batch_id 0 22 miss% 0.05210169018571318
plot_id,batch_id 0 23 miss% 0.07190208297779
plot_id,batch_id 0 24 miss% 0.024781980175173895
plot_id,batch_id 0 25 miss% 0.05510672266360413
plot_id,batch_id 0 26 miss% 0.06133468902587503
plot_id,batch_id 0 27 miss% 0.08784130086302543
plot_id,batch_id 0 28 miss% 0.024333607427416984
plot_id,batch_id 0 29 miss% 0.03378493855023397
plot_id,batch_id 0 30 miss% 0.08450873805369971
plot_id,batch_id 0 31 miss% 0.08389469192939049
plot_id,batch_id 0 32 miss% 0.09669658733321586
plot_id,batch_id 0 33 miss% 0.04233729773255468
plot_id,batch_id 0 34 miss% 0.029220626070884055
plot_id,batch_id 0 35 miss% 0.1049938007667982
plot_id,batch_id 0 36 miss% 0.04133195579365557
plot_id,batch_id 0 37 miss% 0.053302607691117755
plot_id,batch_id 0 38 miss% 0.058460122235830525
plot_id,batch_id 0 39 miss% 0.025735501796596102
plot_id,batch_id 0 40 miss% 0.08602487422081244
plot_id,batch_id 0 41 miss% 0.04017760463063778
plot_id,batch_id 0 42 miss% 0.02450547674588216
plot_id,batch_id 0 43 miss% 0.051692607036756044
plot_id,batch_id 0 44 miss% 0.03632008144035252
plot_id,batch_id 0 45 miss% 0.06280728148277732
plot_id,batch_id 0 46 miss% 0.02393619834863101
plot_id,batch_id 0 47 miss% 0.027123409128722707
plot_id,batch_id 0 48 miss% 0.0519899795801046
plot_id,batch_id 0 49 miss% 0.040880669096012026
plot_id,batch_id 0 50 miss% 0.13499415101833181
plot_id,batch_id 0 51 miss% 0.0319729997498367
plot_id,batch_id 0 52 miss% 0.030774804673007736
plot_id,batch_id 0 53 miss% 0.02168346028421686
plot_id,batch_id 0 54 miss% 0.06199639273069446
plot_id,batch_id 0 55 miss% 0.10785243472822074
plot_id,batch_id 0 56 miss% 0.06992359228135142
plot_id,batch_id 0 57 miss% 0.05191696548413554
plot_id,batch_id 0 58 miss% 0.06100256494689962
plot_id,batch_id 0 59 miss% 0.033344540933447325
plot_id,batch_id 0 60 miss% 0.03719764283216022
plot_id,batch_id 0 61 miss% 0.02468162252243081
plot_id,batch_id 0 62 miss% 0.047235590750905165
plot_id,batch_id 0 63 miss% 0.07966896706876768
plot_id,batch_id 0 64 miss% 0.06181886087047239
plot_id,batch_id 0 65 miss% 0.03809032054323844
plot_id,batch_id 0 66 miss% 0.04811776843316227
plot_id,batch_id 0 67 miss% 0.044425389296403876
plot_id,batch_id 0 68 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  69649
Epoch:0, Train loss:0.630832, valid loss:0.612268
Epoch:1, Train loss:0.032111, valid loss:0.004477
Epoch:2, Train loss:0.008668, valid loss:0.002246
Epoch:3, Train loss:0.005416, valid loss:0.001797
Epoch:4, Train loss:0.003598, valid loss:0.001764
Epoch:5, Train loss:0.003190, valid loss:0.001678
Epoch:6, Train loss:0.002975, valid loss:0.001264
Epoch:7, Train loss:0.002750, valid loss:0.001571
Epoch:8, Train loss:0.002546, valid loss:0.001154
Epoch:9, Train loss:0.002342, valid loss:0.001238
Epoch:10, Train loss:0.002310, valid loss:0.001125
Epoch:11, Train loss:0.001592, valid loss:0.001005
Epoch:12, Train loss:0.001567, valid loss:0.000846
Epoch:13, Train loss:0.001555, valid loss:0.000990
Epoch:14, Train loss:0.001533, valid loss:0.000951
Epoch:15, Train loss:0.001476, valid loss:0.000898
Epoch:16, Train loss:0.001443, valid loss:0.000804
Epoch:17, Train loss:0.001400, valid loss:0.000907
Epoch:18, Train loss:0.001343, valid loss:0.000716
Epoch:19, Train loss:0.001348, valid loss:0.000905
Epoch:20, Train loss:0.001344, valid loss:0.001123
Epoch:21, Train loss:0.001036, valid loss:0.000620
Epoch:22, Train loss:0.000979, valid loss:0.000698
Epoch:23, Train loss:0.000969, valid loss:0.000685
Epoch:24, Train loss:0.000956, valid loss:0.000670
Epoch:25, Train loss:0.000946, valid loss:0.000666
Epoch:26, Train loss:0.000942, valid loss:0.000684
Epoch:27, Train loss:0.000910, valid loss:0.000684
Epoch:28, Train loss:0.000926, valid loss:0.000654
Epoch:29, Train loss:0.000881, valid loss:0.000668
Epoch:30, Train loss:0.000882, valid loss:0.000636
Epoch:31, Train loss:0.000725, valid loss:0.000584
Epoch:32, Train loss:0.000721, valid loss:0.000589
Epoch:33, Train loss:0.000726, valid loss:0.000569
Epoch:34, Train loss:0.000725, valid loss:0.000630
Epoch:35, Train loss:0.000709, valid loss:0.000594
Epoch:36, Train loss:0.000708, valid loss:0.000586
Epoch:37, Train loss:0.000697, valid loss:0.000581
Epoch:38, Train loss:0.000712, valid loss:0.000592
Epoch:39, Train loss:0.000698, valid loss:0.000555
Epoch:40, Train loss:0.000663, valid loss:0.000581
Epoch:41, Train loss:0.000606, valid loss:0.000552
Epoch:42, Train loss:0.000606, valid loss:0.000545
Epoch:43, Train loss:0.000595, valid loss:0.000536
Epoch:44, Train loss:0.000595, valid loss:0.000532
Epoch:45, Train loss:0.000597, valid loss:0.000582
Epoch:46, Train loss:0.000592, valid loss:0.000529
Epoch:47, Train loss:0.000595, valid loss:0.000532
Epoch:48, Train loss:0.000589, valid loss:0.000529
Epoch:49, Train loss:0.000584, valid loss:0.000511
Epoch:50, Train loss:0.000577, valid loss:0.000522
Epoch:51, Train loss:0.000544, valid loss:0.000514
Epoch:52, Train loss:0.000540, valid loss:0.000515
Epoch:53, Train loss:0.000537, valid loss:0.000512
Epoch:54, Train loss:0.000536, valid loss:0.000513
Epoch:55, Train loss:0.000535, valid loss:0.000512
Epoch:56, Train loss:0.000534, valid loss:0.000515
Epoch:57, Train loss:0.000533, valid loss:0.000513
Epoch:58, Train loss:0.000533, valid loss:0.000513
Epoch:59, Train loss:0.000532, valid loss:0.000513
Epoch:60, Train loss:0.000531, valid loss:0.000510
training time 13495.481040239334
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.06711778935307597
plot_id,batch_id 0 1 miss% 0.033703628439359826
plot_id,batch_id 0 2 miss% 0.09285340906471949
plot_id,batch_id 0 3 miss% 0.05895765905164941
plot_id,batch_id 0 4 miss% 0.06761213200056344
plot_id,batch_id 0 5 miss% 0.0507386586800761
plot_id,batch_id 0 6 miss% 0.044325129333785976
plot_id,batch_id 0 7 miss% 0.12123496737836997
plot_id,batch_id 0 8 miss% 0.077812904337904
plot_id,batch_id 0 9 miss% 0.061993687894960285
plot_id,batch_id 0 10 miss% 0.04811361934948098
plot_id,batch_id 0 11 miss% 0.09373090198289323
plot_id,batch_id 0 12 miss% 0.10735156603242944
plot_id,batch_id 0 13 miss% 0.03769216037508962
plot_id,batch_id 0 14 miss% 0.07098996204862083
plot_id,batch_id 0 15 miss% 0.020593121168527684
plot_id,batch_id 0 16 miss% 0.07098468018556095
plot_id,batch_id 0 17 miss% 0.05711945189390914
plot_id,batch_id 0 18 miss% 0.0927697333032308
plot_id,batch_id 0 19 miss% 0.0818529516469597
plot_id,batch_id 0 20 miss% 0.20280794225339888
plot_id,batch_id 0 21 miss% 0.059602447082020456
plot_id,batch_id 0 22 miss% 0.047053196576217815
plot_id,batch_id 0 23 miss% 0.050341299211119066
plot_id,batch_id 0 24 miss% 0.04771997811594317
plot_id,batch_id 0 25 miss% 0.08601844090602573
plot_id,batch_id 0 26 miss% 0.04869069851384787
plot_id,batch_id 0 27 miss% 0.04583699334069197
plot_id,batch_id 0 28 miss% 0.07965771758680455
plot_id,batch_id 0 29 miss% 0.04822548737208445
plot_id,batch_id 0 30 miss% 0.0557811019989004
plot_id,batch_id 0 31 miss% 0.07857628261590073
plot_id,batch_id 0 32 miss% 0.09847565701947186
plot_id,batch_id 0 33 miss% 0.05229234017384339
plot_id,batch_id 0 34 miss% 0.062274132656154474
plot_id,batch_id 0 35 miss% 0.08668455778110024
plot_id,batch_id 0 36 miss% 0.09258825838263789
plot_id,batch_id 0 37 miss% 0.0558434633503949
plot_id,batch_id 0 38 miss% 0.058252821773970266
plot_id,batch_id 0 39 miss% 0.04167608306885191
plot_id,batch_id 0 40 miss% 0.09578685567541984
plot_id,batch_id 0 41 miss% 0.08470056560284041
plot_id,batch_id 0 42 miss% 0.030917738057568075
plot_id,batch_id 0 43 miss% 0.07754967597493276
plot_id,batch_id 0 44 miss% 0.019484132032480853
plot_id,batch_id 0 45 miss% 0.09759941957527205
plot_id,batch_id 0 46 miss% 0.09282150546462409
plot_id,batch_id 0 47 miss% 0.04837805710805607
plot_id,batch_id 0 48 miss% 0.038599916239153736
plot_id,batch_id 0 49 miss% 0.05887621574991716
plot_id,batch_id 0 50 miss% 0.12075009789806007
plot_id,batch_id 0 51 miss% 0.02443517287156351
plot_id,batch_id 0 52 miss% 0.038575980670260655
plot_id,batch_id 0 53 miss% 0.02871243201064701
plot_id,batch_id 0 54 miss% 0.08646656905842895
plot_id,batch_id 0 55 miss% 0.05529139937385813
plot_id,batch_id 0 56 miss% 0.08486011206899177
plot_id,batch_id 0 57 miss% 0.04421424149057857
plot_id,batch_id 0 58 miss% 0.04270830830497142
plot_id,batch_id 0 59 miss% 0.05041632880904065
plot_id,batch_id 0 60 miss% 0.06489804442048334
plot_id,batch_id 0 61 miss% 0.023101815736977383
plot_id,batch_id 0 62 miss% 0.04905901858746204
plot_id,batch_id 0 63 miss% 0.03554970109151091
plot_id,batch_id 0 64 miss% 0.05502231035466326
plot_id,batch_id 0 65 miss% 0.07447825598793945
plot_id,batch_id 0 66 miss% 0.034560128785499716
plot_id,batch_idthe mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  209681
Epoch:0, Train loss:0.623425, valid loss:0.630479
Epoch:1, Train loss:0.049738, valid loss:0.006725
Epoch:2, Train loss:0.014540, valid loss:0.004629
Epoch:3, Train loss:0.011177, valid loss:0.003628
Epoch:4, Train loss:0.007217, valid loss:0.002196
Epoch:5, Train loss:0.004557, valid loss:0.002315
Epoch:6, Train loss:0.004049, valid loss:0.001809
Epoch:7, Train loss:0.003539, valid loss:0.001761
Epoch:8, Train loss:0.003088, valid loss:0.001908
Epoch:9, Train loss:0.002933, valid loss:0.001538
Epoch:10, Train loss:0.002748, valid loss:0.001190
Epoch:11, Train loss:0.001954, valid loss:0.001111
Epoch:12, Train loss:0.001900, valid loss:0.001089
Epoch:13, Train loss:0.001825, valid loss:0.001057
Epoch:14, Train loss:0.001763, valid loss:0.000998
Epoch:15, Train loss:0.001676, valid loss:0.001290
Epoch:16, Train loss:0.001688, valid loss:0.001103
Epoch:17, Train loss:0.001582, valid loss:0.001075
Epoch:18, Train loss:0.001503, valid loss:0.001130
Epoch:19, Train loss:0.001508, valid loss:0.001019
Epoch:20, Train loss:0.001494, valid loss:0.000886
Epoch:21, Train loss:0.001118, valid loss:0.000780
Epoch:22, Train loss:0.001076, valid loss:0.000822
Epoch:23, Train loss:0.001106, valid loss:0.000777
Epoch:24, Train loss:0.001052, valid loss:0.000880
Epoch:25, Train loss:0.001035, valid loss:0.000820
Epoch:26, Train loss:0.001057, valid loss:0.000779
Epoch:27, Train loss:0.000981, valid loss:0.000779
Epoch:28, Train loss:0.000964, valid loss:0.000736
Epoch:29, Train loss:0.000961, valid loss:0.000820
Epoch:30, Train loss:0.000922, valid loss:0.000775
Epoch:31, Train loss:0.000773, valid loss:0.000699
Epoch:32, Train loss:0.000765, valid loss:0.000739
Epoch:33, Train loss:0.000769, valid loss:0.000689
Epoch:34, Train loss:0.000750, valid loss:0.000676
Epoch:35, Train loss:0.000758, valid loss:0.000669
Epoch:36, Train loss:0.000727, valid loss:0.000701
Epoch:37, Train loss:0.000721, valid loss:0.000760
Epoch:38, Train loss:0.000708, valid loss:0.000702
Epoch:39, Train loss:0.000717, valid loss:0.000719
Epoch:40, Train loss:0.000700, valid loss:0.000706
Epoch:41, Train loss:0.000628, valid loss:0.000640
Epoch:42, Train loss:0.000615, valid loss:0.000636
Epoch:43, Train loss:0.000617, valid loss:0.000614
Epoch:44, Train loss:0.000608, valid loss:0.000631
Epoch:45, Train loss:0.000603, valid loss:0.000649
Epoch:46, Train loss:0.000606, valid loss:0.000613
Epoch:47, Train loss:0.000598, valid loss:0.000606
Epoch:48, Train loss:0.000593, valid loss:0.000609
Epoch:49, Train loss:0.000585, valid loss:0.000602
Epoch:50, Train loss:0.000590, valid loss:0.000604
Epoch:51, Train loss:0.000560, valid loss:0.000598
Epoch:52, Train loss:0.000554, valid loss:0.000595
Epoch:53, Train loss:0.000551, valid loss:0.000600
Epoch:54, Train loss:0.000549, valid loss:0.000598
Epoch:55, Train loss:0.000548, valid loss:0.000599
Epoch:56, Train loss:0.000546, valid loss:0.000605
Epoch:57, Train loss:0.000546, valid loss:0.000603
Epoch:58, Train loss:0.000545, valid loss:0.000598
Epoch:59, Train loss:0.000544, valid loss:0.000600
Epoch:60, Train loss:0.000544, valid loss:0.000597
training time 13501.506478309631
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.05838168373254617
plot_id,batch_id 0 1 miss% 0.037001367692205904
plot_id,batch_id 0 2 miss% 0.10440475744490878
plot_id,batch_id 0 3 miss% 0.0456363065224219
plot_id,batch_id 0 4 miss% 0.056087914903918824
plot_id,batch_id 0 5 miss% 0.03983932020244863
plot_id,batch_id 0 6 miss% 0.028856516087903204
plot_id,batch_id 0 7 miss% 0.07779789711752395
plot_id,batch_id 0 8 miss% 0.06516066147869738
plot_id,batch_id 0 9 miss% 0.03417321248628411
plot_id,batch_id 0 10 miss% 0.02477855296351568
plot_id,batch_id 0 11 miss% 0.03494382096716391
plot_id,batch_id 0 12 miss% 0.049885301548811496
plot_id,batch_id 0 13 miss% 0.05775395214844919
plot_id,batch_id 0 14 miss% 0.03356137030611405
plot_id,batch_id 0 15 miss% 0.06569152239967675
plot_id,batch_id 0 16 miss% 0.18438193704266903
plot_id,batch_id 0 17 miss% 0.06985467064344228
plot_id,batch_id 0 18 miss% 0.0539937425221578
plot_id,batch_id 0 19 miss% 0.11006300558244342
plot_id,batch_id 0 20 miss% 0.038473193888344755
plot_id,batch_id 0 21 miss% 0.05213856554677757
plot_id,batch_id 0 22 miss% 0.028132978735035726
plot_id,batch_id 0 23 miss% 0.07715855645669527
plot_id,batch_id 0 24 miss% 0.03478643584069235
plot_id,batch_id 0 25 miss% 0.04218153198691956
plot_id,batch_id 0 26 miss% 0.061638115620143645
plot_id,batch_id 0 27 miss% 0.05084092051878033
plot_id,batch_id 0 28 miss% 0.03631791322450567
plot_id,batch_id 0 29 miss% 0.04312425017337722
plot_id,batch_id 0 30 miss% 0.030836149112011314
plot_id,batch_id 0 31 miss% 0.08792763108958114
plot_id,batch_id 0 32 miss% 0.07393839181815737
plot_id,batch_id 0 33 miss% 0.06331144395434492
plot_id,batch_id 0 34 miss% 0.04088806870386879
plot_id,batch_id 0 35 miss% 0.024909681831310235
plot_id,batch_id 0 36 miss% 0.08786301165645632
plot_id,batch_id 0 37 miss% 0.03430488624885721
plot_id,batch_id 0 38 miss% 0.045593193913563035
plot_id,batch_id 0 39 miss% 0.030331952640407278
plot_id,batch_id 0 40 miss% 0.07004509476855564
plot_id,batch_id 0 41 miss% 0.032210112692886705
plot_id,batch_id 0 42 miss% 0.016906414078566377
plot_id,batch_id 0 43 miss% 0.05775036284148363
plot_id,batch_id 0 44 miss% 0.03834311657427646
plot_id,batch_id 0 45 miss% 0.04763254949684606
plot_id,batch_id 0 46 miss% 0.0264500607141107
plot_id,batch_id 0 47 miss% 0.026117497394551135
plot_id,batch_id 0 48 miss% 0.03365190715231645
plot_id,batch_id 0 49 miss% 0.018824498683226904
plot_id,batch_id 0 50 miss% 0.10389951670102601
plot_id,batch_id 0 51 miss% 0.03416105363781308
plot_id,batch_id 0 52 miss% 0.03521938863487387
plot_id,batch_id 0 53 miss% 0.025437831267658608
plot_id,batch_id 0 54 miss% 0.04145694012828063
plot_id,batch_id 0 55 miss% 0.040720582489413085
plot_id,batch_id 0 56 miss% 0.0999056363522247
plot_id,batch_id 0 57 miss% 0.02377940052643178
plot_id,batch_id 0 58 miss% 0.030747764156373453
plot_id,batch_id 0 59 miss% 0.04507281281088076
plot_id,batch_id 0 60 miss% 0.06207039698417779
plot_id,batch_id 0 61 miss% 0.015599855067139198
plot_id,batch_id 0 62 miss% 0.06653394612003721
plot_id,batch_id 0 63 miss% 0.05528005272261512
plot_id,batch_id 0 64 miss% 0.0721919115499059
plot_id,batch_id 0 65 miss% 0.04234237068095691
plot_id,batch_id 0 66 miss% 0.08365030099072851
plot_id,batch_id 0 67 miss% 0.036132859402182284
plot_id,batch_id 0 68 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  69649
Epoch:0, Train loss:0.630832, valid loss:0.612268
Epoch:1, Train loss:0.035314, valid loss:0.004095
Epoch:2, Train loss:0.010491, valid loss:0.003114
Epoch:3, Train loss:0.006185, valid loss:0.002473
Epoch:4, Train loss:0.004429, valid loss:0.001710
Epoch:5, Train loss:0.003189, valid loss:0.001548
Epoch:6, Train loss:0.002898, valid loss:0.001255
Epoch:7, Train loss:0.002583, valid loss:0.001273
Epoch:8, Train loss:0.002378, valid loss:0.001209
Epoch:9, Train loss:0.002308, valid loss:0.001208
Epoch:10, Train loss:0.002089, valid loss:0.001288
Epoch:11, Train loss:0.001584, valid loss:0.000946
Epoch:12, Train loss:0.001537, valid loss:0.000877
Epoch:13, Train loss:0.001492, valid loss:0.000913
Epoch:14, Train loss:0.001486, valid loss:0.000947
Epoch:15, Train loss:0.001424, valid loss:0.000872
Epoch:16, Train loss:0.001386, valid loss:0.000753
Epoch:17, Train loss:0.001345, valid loss:0.000873
Epoch:18, Train loss:0.001297, valid loss:0.000712
Epoch:19, Train loss:0.001290, valid loss:0.000909
Epoch:20, Train loss:0.001276, valid loss:0.000909
Epoch:21, Train loss:0.001008, valid loss:0.000648
Epoch:22, Train loss:0.000991, valid loss:0.000637
Epoch:23, Train loss:0.000964, valid loss:0.000711
Epoch:24, Train loss:0.000971, valid loss:0.000645
Epoch:25, Train loss:0.000954, valid loss:0.000638
Epoch:26, Train loss:0.000930, valid loss:0.000661
Epoch:27, Train loss:0.000913, valid loss:0.000714
Epoch:28, Train loss:0.000894, valid loss:0.000631
Epoch:29, Train loss:0.000883, valid loss:0.000649
Epoch:30, Train loss:0.000877, valid loss:0.000626
Epoch:31, Train loss:0.000760, valid loss:0.000583
Epoch:32, Train loss:0.000756, valid loss:0.000608
Epoch:33, Train loss:0.000753, valid loss:0.000605
Epoch:34, Train loss:0.000745, valid loss:0.000572
Epoch:35, Train loss:0.000733, valid loss:0.000622
Epoch:36, Train loss:0.000738, valid loss:0.000656
Epoch:37, Train loss:0.000728, valid loss:0.000556
Epoch:38, Train loss:0.000718, valid loss:0.000564
Epoch:39, Train loss:0.000710, valid loss:0.000537
Epoch:40, Train loss:0.000703, valid loss:0.000601
Epoch:41, Train loss:0.000646, valid loss:0.000541
Epoch:42, Train loss:0.000649, valid loss:0.000529
Epoch:43, Train loss:0.000645, valid loss:0.000539
Epoch:44, Train loss:0.000639, valid loss:0.000537
Epoch:45, Train loss:0.000642, valid loss:0.000565
Epoch:46, Train loss:0.000629, valid loss:0.000527
Epoch:47, Train loss:0.000633, valid loss:0.000558
Epoch:48, Train loss:0.000628, valid loss:0.000566
Epoch:49, Train loss:0.000624, valid loss:0.000569
Epoch:50, Train loss:0.000618, valid loss:0.000560
Epoch:51, Train loss:0.000591, valid loss:0.000559
Epoch:52, Train loss:0.000585, valid loss:0.000550
Epoch:53, Train loss:0.000583, valid loss:0.000550
Epoch:54, Train loss:0.000581, valid loss:0.000551
Epoch:55, Train loss:0.000581, valid loss:0.000538
Epoch:56, Train loss:0.000579, valid loss:0.000542
Epoch:57, Train loss:0.000579, valid loss:0.000533
Epoch:58, Train loss:0.000578, valid loss:0.000541
Epoch:59, Train loss:0.000578, valid loss:0.000530
Epoch:60, Train loss:0.000578, valid loss:0.000545
training time 13512.705030679703
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08040691299615532
plot_id,batch_id 0 1 miss% 0.052385697185492
plot_id,batch_id 0 2 miss% 0.1210952707888934
plot_id,batch_id 0 3 miss% 0.060960685412160244
plot_id,batch_id 0 4 miss% 0.05562499829787284
plot_id,batch_id 0 5 miss% 0.04644060863027329
plot_id,batch_id 0 6 miss% 0.07457025633605208
plot_id,batch_id 0 7 miss% 0.1055274518407842
plot_id,batch_id 0 8 miss% 0.08150686908507722
plot_id,batch_id 0 9 miss% 0.0739331800485126
plot_id,batch_id 0 10 miss% 0.04490243573191156
plot_id,batch_id 0 11 miss% 0.04502995032862884
plot_id,batch_id 0 12 miss% 0.06399220464735418
plot_id,batch_id 0 13 miss% 0.04986108095990151
plot_id,batch_id 0 14 miss% 0.09896984831843285
plot_id,batch_id 0 15 miss% 0.054538605634947494
plot_id,batch_id 0 16 miss% 0.15889003545676844
plot_id,batch_id 0 17 miss% 0.03803155411738032
plot_id,batch_id 0 18 miss% 0.07213390954366256
plot_id,batch_id 0 19 miss% 0.07673653353427014
plot_id,batch_id 0 20 miss% 0.051283442301020804
plot_id,batch_id 0 21 miss% 0.07024390729430297
plot_id,batch_id 0 22 miss% 0.08848168161306678
plot_id,batch_id 0 23 miss% 0.04536671193706215
plot_id,batch_id 0 24 miss% 0.06763017947390641
plot_id,batch_id 0 25 miss% 0.0829307086276159
plot_id,batch_id 0 26 miss% 0.05896487076145806
plot_id,batch_id 0 27 miss% 0.052190584278489736
plot_id,batch_id 0 28 miss% 0.031403423110286766
plot_id,batch_id 0 29 miss% 0.022690913794158486
plot_id,batch_id 0 30 miss% 0.03554515362510421
plot_id,batch_id 0 31 miss% 0.07122906383701426
plot_id,batch_id 0 32 miss% 0.10788075342772958
plot_id,batch_id 0 33 miss% 0.06772106722869753
plot_id,batch_id 0 34 miss% 0.03709236387358507
plot_id,batch_id 0 35 miss% 0.05713604549416495
plot_id,batch_id 0 36 miss% 0.07697523128361415
plot_id,batch_id 0 37 miss% 0.06120498088297139
plot_id,batch_id 0 38 miss% 0.11925844590814118
plot_id,batch_id 0 39 miss% 0.1110569505540986
plot_id,batch_id 0 40 miss% 0.15421836127681915
plot_id,batch_id 0 41 miss% 0.05464365767098521
plot_id,batch_id 0 42 miss% 0.019115091340749256
plot_id,batch_id 0 43 miss% 0.08894881277483
plot_id,batch_id 0 44 miss% 0.055721046538772065
plot_id,batch_id 0 45 miss% 0.04386703974041483
plot_id,batch_id 0 46 miss% 0.033448381618022646
plot_id,batch_id 0 47 miss% 0.02840986761831398
plot_id,batch_id 0 48 miss% 0.03584863875185797
plot_id,batch_id 0 49 miss% 0.06050484888112665
plot_id,batch_id 0 50 miss% 0.10890963509721145
plot_id,batch_id 0 51 miss% 0.029211803387907655
plot_id,batch_id 0 52 miss% 0.03495171101675819
plot_id,batch_id 0 53 miss% 0.04234513955829772
plot_id,batch_id 0 54 miss% 0.03160475241949031
plot_id,batch_id 0 55 miss% 0.06127448031886497
plot_id,batch_id 0 56 miss% 0.08909335143249363
plot_id,batch_id 0 57 miss% 0.03714695132411965
plot_id,batch_id 0 58 miss% 0.040423544321829104
plot_id,batch_id 0 59 miss% 0.04685374079502315
plot_id,batch_id 0 60 miss% 0.03673884122692501
plot_id,batch_id 0 61 miss% 0.02095070324171434
plot_id,batch_id 0 62 miss% 0.044043817164220164
plot_id,batch_id 0 63 miss% 0.05891206988987547
plot_id,batch_id 0 64 miss% 0.07426356584018359
plot_id,batch_id 0 65 miss% 0.09395942061450158
plot_id,batch_id 0 66 miss% 0.14234040633021683
plot_id,batch_id 0 67the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  120401
Epoch:0, Train loss:0.590279, valid loss:0.579846
Epoch:1, Train loss:0.111343, valid loss:0.005227
Epoch:2, Train loss:0.013254, valid loss:0.004537
Epoch:3, Train loss:0.011334, valid loss:0.003519
Epoch:4, Train loss:0.009731, valid loss:0.003361
Epoch:5, Train loss:0.006795, valid loss:0.001683
Epoch:6, Train loss:0.003268, valid loss:0.001625
Epoch:7, Train loss:0.002840, valid loss:0.001539
Epoch:8, Train loss:0.002574, valid loss:0.001362
Epoch:9, Train loss:0.002358, valid loss:0.001316
Epoch:10, Train loss:0.002210, valid loss:0.001089
Epoch:11, Train loss:0.001684, valid loss:0.000937
Epoch:12, Train loss:0.001609, valid loss:0.000948
Epoch:13, Train loss:0.001549, valid loss:0.001000
Epoch:14, Train loss:0.001496, valid loss:0.001082
Epoch:15, Train loss:0.001469, valid loss:0.000916
Epoch:16, Train loss:0.001414, valid loss:0.000877
Epoch:17, Train loss:0.001348, valid loss:0.000853
Epoch:18, Train loss:0.001342, valid loss:0.000968
Epoch:19, Train loss:0.001275, valid loss:0.000762
Epoch:20, Train loss:0.001252, valid loss:0.000846
Epoch:21, Train loss:0.001031, valid loss:0.000687
Epoch:22, Train loss:0.001003, valid loss:0.000735
Epoch:23, Train loss:0.000980, valid loss:0.000715
Epoch:24, Train loss:0.000961, valid loss:0.000737
Epoch:25, Train loss:0.000951, valid loss:0.000699
Epoch:26, Train loss:0.000937, valid loss:0.000675
Epoch:27, Train loss:0.000914, valid loss:0.000665
Epoch:28, Train loss:0.000900, valid loss:0.000670
Epoch:29, Train loss:0.000892, valid loss:0.000709
Epoch:30, Train loss:0.000874, valid loss:0.000640
Epoch:31, Train loss:0.000756, valid loss:0.000622
Epoch:32, Train loss:0.000753, valid loss:0.000650
Epoch:33, Train loss:0.000743, valid loss:0.000615
Epoch:34, Train loss:0.000732, valid loss:0.000642
Epoch:35, Train loss:0.000732, valid loss:0.000613
Epoch:36, Train loss:0.000722, valid loss:0.000614
Epoch:37, Train loss:0.000712, valid loss:0.000600
Epoch:38, Train loss:0.000711, valid loss:0.000614
Epoch:39, Train loss:0.000712, valid loss:0.000645
Epoch:40, Train loss:0.000699, valid loss:0.000678
Epoch:41, Train loss:0.000633, valid loss:0.000588
Epoch:42, Train loss:0.000634, valid loss:0.000584
Epoch:43, Train loss:0.000631, valid loss:0.000587
Epoch:44, Train loss:0.000628, valid loss:0.000581
Epoch:45, Train loss:0.000616, valid loss:0.000595
Epoch:46, Train loss:0.000622, valid loss:0.000582
Epoch:47, Train loss:0.000619, valid loss:0.000591
Epoch:48, Train loss:0.000605, valid loss:0.000599
Epoch:49, Train loss:0.000611, valid loss:0.000606
Epoch:50, Train loss:0.000606, valid loss:0.000579
Epoch:51, Train loss:0.000573, valid loss:0.000565
Epoch:52, Train loss:0.000567, valid loss:0.000557
Epoch:53, Train loss:0.000565, valid loss:0.000559
Epoch:54, Train loss:0.000563, valid loss:0.000560
Epoch:55, Train loss:0.000563, valid loss:0.000566
Epoch:56, Train loss:0.000562, valid loss:0.000556
Epoch:57, Train loss:0.000561, valid loss:0.000556
Epoch:58, Train loss:0.000561, valid loss:0.000562
Epoch:59, Train loss:0.000560, valid loss:0.000559
Epoch:60, Train loss:0.000560, valid loss:0.000562
training time 13542.758927106857
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.0660971459305151
plot_id,batch_id 0 1 miss% 0.06295523505700407
plot_id,batch_id 0 2 miss% 0.11501970536678828
plot_id,batch_id 0 3 miss% 0.05708279108612486
plot_id,batch_id 0 4 miss% 0.061358883832478284
plot_id,batch_id 0 5 miss% 0.040053106203842025
plot_id,batch_id 0 6 miss% 0.07567833085334372
plot_id,batch_id 0 7 miss% 0.10511231483791797
plot_id,batch_id 0 8 miss% 0.07292314517375799
plot_id,batch_id 0 9 miss% 0.04073605938848388
plot_id,batch_id 0 10 miss% 0.02832560789366272
plot_id,batch_id 0 11 miss% 0.06322390447962893
plot_id,batch_id 0 12 miss% 0.07611328958039762
plot_id,batch_id 0 13 miss% 0.04941675752156716
plot_id,batch_id 0 14 miss% 0.10694555317642078
plot_id,batch_id 0 15 miss% 0.03739452732487737
plot_id,batch_id 0 16 miss% 0.18811074859645613
plot_id,batch_id 0 17 miss% 0.04214171068382664
plot_id,batch_id 0 18 miss% 0.06838451396921105
plot_id,batch_id 0 19 miss% 0.10915045064361412
plot_id,batch_id 0 20 miss% 0.07789249674273917
plot_id,batch_id 0 21 miss% 0.03398674623202078
plot_id,batch_id 0 22 miss% 0.022260439817341177
plot_id,batch_id 0 23 miss% 0.04054953871208211
plot_id,batch_id 0 24 miss% 0.03007113968729427
plot_id,batch_id 0 25 miss% 0.049517033334335665
plot_id,batch_id 0 26 miss% 0.05665096262183504
plot_id,batch_id 0 27 miss% 0.05312776031058863
plot_id,batch_id 0 28 miss% 0.04586666800562897
plot_id,batch_id 0 29 miss% 0.02991203574498107
plot_id,batch_id 0 30 miss% 0.06938900921244984
plot_id,batch_id 0 31 miss% 0.08147142507556347
plot_id,batch_id 0 32 miss% 0.11917895186034105
plot_id,batch_id 0 33 miss% 0.060026463646479886
plot_id,batch_id 0 34 miss% 0.07753889534849846
plot_id,batch_id 0 35 miss% 0.028894473525581027
plot_id,batch_id 0 36 miss% 0.08816514847084157
plot_id,batch_id 0 37 miss% 0.11119722982054399
plot_id,batch_id 0 38 miss% 0.029116628103913935
plot_id,batch_id 0 39 miss% 0.03165326264363807
plot_id,batch_id 0 40 miss% 0.06533893455838982
plot_id,batch_id 0 41 miss% 0.06168933487157571
plot_id,batch_id 0 42 miss% 0.03154748736534441
plot_id,batch_id 0 43 miss% 0.0460548290493304
plot_id,batch_id 0 44 miss% 0.037249599885466216
plot_id,batch_id 0 45 miss% 0.06187475401117693
plot_id,batch_id 0 46 miss% 0.03506698759718578
plot_id,batch_id 0 47 miss% 0.036337125990604464
plot_id,batch_id 0 48 miss% 0.03291604866965902
plot_id,batch_id 0 49 miss% 0.027308441253845964
plot_id,batch_id 0 50 miss% 0.1279078031557047
plot_id,batch_id 0 51 miss% 0.05789689110664847
plot_id,batch_id 0 52 miss% 0.06425414867048938
plot_id,batch_id 0 53 miss% 0.0403118204620322
plot_id,batch_id 0 54 miss% 0.030408431385252503
plot_id,batch_id 0 55 miss% 0.07482582913093465
plot_id,batch_id 0 56 miss% 0.06778357449125003
plot_id,batch_id 0 57 miss% 0.06467453455792564
plot_id,batch_id 0 58 miss% 0.05150537865694324
plot_id,batch_id 0 59 miss% 0.03991574129058715
plot_id,batch_id 0 60 miss% 0.06426770640686996
plot_id,batch_id 0 61 miss% 0.04501455797051574
plot_id,batch_id 0 62 miss% 0.051925165425272306
plot_id,batch_id 0 63 miss% 0.05916130799905336
plot_id,batch_id 0 64 miss% 0.05746248367768477
plot_id,batch_id 0 65 miss% 0.1172802024713977
plot_id,batch_id 0 66 miss% 0.056403554656463205
plot_id,batch_id 0 67 miss% 0.02929148888996946
0.04631913861049548
plot_id,batch_id 0 69 miss% 0.06164597474431281
plot_id,batch_id 0 70 miss% 0.06820465632061545
plot_id,batch_id 0 71 miss% 0.019469067921255197
plot_id,batch_id 0 72 miss% 0.13154987091772172
plot_id,batch_id 0 73 miss% 0.045668968448689694
plot_id,batch_id 0 74 miss% 0.07404449327340447
plot_id,batch_id 0 75 miss% 0.03440344014045605
plot_id,batch_id 0 76 miss% 0.08298395756996092
plot_id,batch_id 0 77 miss% 0.02415376785794353
plot_id,batch_id 0 78 miss% 0.03287089183734293
plot_id,batch_id 0 79 miss% 0.05942093280747171
plot_id,batch_id 0 80 miss% 0.03995976834004556
plot_id,batch_id 0 81 miss% 0.07863667692704
plot_id,batch_id 0 82 miss% 0.05166760137615914
plot_id,batch_id 0 83 miss% 0.09531839908187278
plot_id,batch_id 0 84 miss% 0.043771516440858756
plot_id,batch_id 0 85 miss% 0.046924481558969784
plot_id,batch_id 0 86 miss% 0.045095110373971535
plot_id,batch_id 0 87 miss% 0.045784875068938254
plot_id,batch_id 0 88 miss% 0.10375122448624252
plot_id,batch_id 0 89 miss% 0.058516926728175996
plot_id,batch_id 0 90 miss% 0.020619948891981342
plot_id,batch_id 0 91 miss% 0.046396385560752706
plot_id,batch_id 0 92 miss% 0.06101088397022661
plot_id,batch_id 0 93 miss% 0.051009555811874795
plot_id,batch_id 0 94 miss% 0.03974710503945706
plot_id,batch_id 0 95 miss% 0.05207657878385004
plot_id,batch_id 0 96 miss% 0.07324747639744233
plot_id,batch_id 0 97 miss% 0.03913879839059352
plot_id,batch_id 0 98 miss% 0.03174699308545256
plot_id,batch_id 0 99 miss% 0.08942389836313562
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08338118 0.10907107 0.11663735 0.07337661 0.03090045 0.02897707
 0.04679798 0.10577038 0.10658412 0.07157624 0.06548298 0.06695401
 0.04779187 0.05033413 0.08698956 0.03216912 0.19836893 0.09095347
 0.05250567 0.07059152 0.06315177 0.07557585 0.05210169 0.07190208
 0.02478198 0.05510672 0.06133469 0.0878413  0.02433361 0.03378494
 0.08450874 0.08389469 0.09669659 0.0423373  0.02922063 0.1049938
 0.04133196 0.05330261 0.05846012 0.0257355  0.08602487 0.0401776
 0.02450548 0.05169261 0.03632008 0.06280728 0.0239362  0.02712341
 0.05198998 0.04088067 0.13499415 0.031973   0.0307748  0.02168346
 0.06199639 0.10785243 0.06992359 0.05191697 0.06100256 0.03334454
 0.03719764 0.02468162 0.04723559 0.07966897 0.06181886 0.03809032
 0.04811777 0.04442539 0.04631914 0.06164597 0.06820466 0.01946907
 0.13154987 0.04566897 0.07404449 0.03440344 0.08298396 0.02415377
 0.03287089 0.05942093 0.03995977 0.07863668 0.0516676  0.0953184
 0.04377152 0.04692448 0.04509511 0.04578488 0.10375122 0.05851693
 0.02061995 0.04639639 0.06101088 0.05100956 0.03974711 0.05207658
 0.07324748 0.0391388  0.03174699 0.0894239 ]
for model  21 the mean error 0.05932345874084231
all id 21 hidden_dim 24 learning_rate 0.0025 num_layers 5 frames 21 out win 4 err 0.05932345874084231 time 13493.509044408798
Launcher: Job 22 completed in 13763 seconds.
Launcher: Task 64 done. Exiting.
 0 67 miss% 0.03259300205911186
plot_id,batch_id 0 68 miss% 0.05282205131707947
plot_id,batch_id 0 69 miss% 0.060790114439194826
plot_id,batch_id 0 70 miss% 0.06087291206807376
plot_id,batch_id 0 71 miss% 0.029666286877353266
plot_id,batch_id 0 72 miss% 0.0678752595711865
plot_id,batch_id 0 73 miss% 0.054965563220977474
plot_id,batch_id 0 74 miss% 0.13037293437379863
plot_id,batch_id 0 75 miss% 0.05655073546246851
plot_id,batch_id 0 76 miss% 0.06356761947669638
plot_id,batch_id 0 77 miss% 0.06790773078483775
plot_id,batch_id 0 78 miss% 0.03591272697528943
plot_id,batch_id 0 79 miss% 0.07788625051055323
plot_id,batch_id 0 80 miss% 0.04864165202808826
plot_id,batch_id 0 81 miss% 0.09975976706236754
plot_id,batch_id 0 82 miss% 0.08625249357402845
plot_id,batch_id 0 83 miss% 0.08721197375607112
plot_id,batch_id 0 84 miss% 0.04931172111217783
plot_id,batch_id 0 85 miss% 0.04042936551139378
plot_id,batch_id 0 86 miss% 0.0450192695782492
plot_id,batch_id 0 87 miss% 0.06540306068632237
plot_id,batch_id 0 88 miss% 0.08535791535560348
plot_id,batch_id 0 89 miss% 0.08897449452953841
plot_id,batch_id 0 90 miss% 0.043976343429106196
plot_id,batch_id 0 91 miss% 0.07400616394791114
plot_id,batch_id 0 92 miss% 0.04072382475169353
plot_id,batch_id 0 93 miss% 0.05492859223498864
plot_id,batch_id 0 94 miss% 0.0855684210765601
plot_id,batch_id 0 95 miss% 0.05069586068355488
plot_id,batch_id 0 96 miss% 0.06912535154201105
plot_id,batch_id 0 97 miss% 0.04786364756265253
plot_id,batch_id 0 98 miss% 0.03868121897826541
plot_id,batch_id 0 99 miss% 0.08953227922437262
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06711779 0.03370363 0.09285341 0.05895766 0.06761213 0.05073866
 0.04432513 0.12123497 0.0778129  0.06199369 0.04811362 0.0937309
 0.10735157 0.03769216 0.07098996 0.02059312 0.07098468 0.05711945
 0.09276973 0.08185295 0.20280794 0.05960245 0.0470532  0.0503413
 0.04771998 0.08601844 0.0486907  0.04583699 0.07965772 0.04822549
 0.0557811  0.07857628 0.09847566 0.05229234 0.06227413 0.08668456
 0.09258826 0.05584346 0.05825282 0.04167608 0.09578686 0.08470057
 0.03091774 0.07754968 0.01948413 0.09759942 0.09282151 0.04837806
 0.03859992 0.05887622 0.1207501  0.02443517 0.03857598 0.02871243
 0.08646657 0.0552914  0.08486011 0.04421424 0.04270831 0.05041633
 0.06489804 0.02310182 0.04905902 0.0355497  0.05502231 0.07447826
 0.03456013 0.032593   0.05282205 0.06079011 0.06087291 0.02966629
 0.06787526 0.05496556 0.13037293 0.05655074 0.06356762 0.06790773
 0.03591273 0.07788625 0.04864165 0.09975977 0.08625249 0.08721197
 0.04931172 0.04042937 0.04501927 0.06540306 0.08535792 0.08897449
 0.04397634 0.07400616 0.04072382 0.05492859 0.08556842 0.05069586
 0.06912535 0.04786365 0.03868122 0.08953228]
for model  153 the mean error 0.06397007616063327
all id 153 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 25 out win 4 err 0.06397007616063327 time 13495.481040239334
Launcher: Job 154 completed in 13761 seconds.
Launcher: Task 40 done. Exiting.
0.055017086250622266
plot_id,batch_id 0 69 miss% 0.07131210060948628
plot_id,batch_id 0 70 miss% 0.05181372640724777
plot_id,batch_id 0 71 miss% 0.047209803362206544
plot_id,batch_id 0 72 miss% 0.07618757404215165
plot_id,batch_id 0 73 miss% 0.06721704049458034
plot_id,batch_id 0 74 miss% 0.06902789251458608
plot_id,batch_id 0 75 miss% 0.061458641347542656
plot_id,batch_id 0 76 miss% 0.07046777543581044
plot_id,batch_id 0 77 miss% 0.05524663331087366
plot_id,batch_id 0 78 miss% 0.04119383910382023
plot_id,batch_id 0 79 miss% 0.08360534835650325
plot_id,batch_id 0 80 miss% 0.07988725520970745
plot_id,batch_id 0 81 miss% 0.061047185838664725
plot_id,batch_id 0 82 miss% 0.0579747269736063
plot_id,batch_id 0 83 miss% 0.06749827156723
plot_id,batch_id 0 84 miss% 0.062448718012398895
plot_id,batch_id 0 85 miss% 0.03326121630415902
plot_id,batch_id 0 86 miss% 0.05574370425176174
plot_id,batch_id 0 87 miss% 0.06537450376237344
plot_id,batch_id 0 88 miss% 0.10417812051988576
plot_id,batch_id 0 89 miss% 0.0642501643903807
plot_id,batch_id 0 90 miss% 0.02040394988004929
plot_id,batch_id 0 91 miss% 0.04571049053640174
plot_id,batch_id 0 92 miss% 0.02861865312879458
plot_id,batch_id 0 93 miss% 0.03214213537592059
plot_id,batch_id 0 94 miss% 0.06318476914876102
plot_id,batch_id 0 95 miss% 0.032473039190858394
plot_id,batch_id 0 96 miss% 0.05318630268977804
plot_id,batch_id 0 97 miss% 0.041729509899735104
plot_id,batch_id 0 98 miss% 0.0415656028828347
plot_id,batch_id 0 99 miss% 0.12699690516969372
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05838168 0.03700137 0.10440476 0.04563631 0.05608791 0.03983932
 0.02885652 0.0777979  0.06516066 0.03417321 0.02477855 0.03494382
 0.0498853  0.05775395 0.03356137 0.06569152 0.18438194 0.06985467
 0.05399374 0.11006301 0.03847319 0.05213857 0.02813298 0.07715856
 0.03478644 0.04218153 0.06163812 0.05084092 0.03631791 0.04312425
 0.03083615 0.08792763 0.07393839 0.06331144 0.04088807 0.02490968
 0.08786301 0.03430489 0.04559319 0.03033195 0.07004509 0.03221011
 0.01690641 0.05775036 0.03834312 0.04763255 0.02645006 0.0261175
 0.03365191 0.0188245  0.10389952 0.03416105 0.03521939 0.02543783
 0.04145694 0.04072058 0.09990564 0.0237794  0.03074776 0.04507281
 0.0620704  0.01559986 0.06653395 0.05528005 0.07219191 0.04234237
 0.0836503  0.03613286 0.05501709 0.0713121  0.05181373 0.0472098
 0.07618757 0.06721704 0.06902789 0.06145864 0.07046778 0.05524663
 0.04119384 0.08360535 0.07988726 0.06104719 0.05797473 0.06749827
 0.06244872 0.03326122 0.0557437  0.0653745  0.10417812 0.06425016
 0.02040395 0.04571049 0.02861865 0.03214214 0.06318477 0.03247304
 0.0531863  0.04172951 0.0415656  0.12699691]
for model  15 the mean error 0.05386511307341097
all id 15 hidden_dim 32 learning_rate 0.0025 num_layers 4 frames 21 out win 4 err 0.05386511307341097 time 13501.506478309631
Launcher: Job 16 completed in 13770 seconds.
Launcher: Task 213 done. Exiting.
 miss% 0.028294648330650023
plot_id,batch_id 0 68 miss% 0.034472917664521534
plot_id,batch_id 0 69 miss% 0.06441570223926882
plot_id,batch_id 0 70 miss% 0.07973732234350883
plot_id,batch_id 0 71 miss% 0.046689378032625364
plot_id,batch_id 0 72 miss% 0.10566870049420309
plot_id,batch_id 0 73 miss% 0.07333373179101192
plot_id,batch_id 0 74 miss% 0.1495220481527307
plot_id,batch_id 0 75 miss% 0.06015815656744178
plot_id,batch_id 0 76 miss% 0.04466295383067727
plot_id,batch_id 0 77 miss% 0.035765484403423566
plot_id,batch_id 0 78 miss% 0.03093985729422252
plot_id,batch_id 0 79 miss% 0.1197611024139702
plot_id,batch_id 0 80 miss% 0.05350897802625932
plot_id,batch_id 0 81 miss% 0.08441440156603902
plot_id,batch_id 0 82 miss% 0.054971458618926244
plot_id,batch_id 0 83 miss% 0.06455728960791933
plot_id,batch_id 0 84 miss% 0.08342277731736181
plot_id,batch_id 0 85 miss% 0.05182410663124327
plot_id,batch_id 0 86 miss% 0.05929205372221731
plot_id,batch_id 0 87 miss% 0.042558077980178174
plot_id,batch_id 0 88 miss% 0.09031017906005981
plot_id,batch_id 0 89 miss% 0.07050694809007484
plot_id,batch_id 0 90 miss% 0.05205953659542255
plot_id,batch_id 0 91 miss% 0.06764425804237371
plot_id,batch_id 0 92 miss% 0.037475726545974225
plot_id,batch_id 0 93 miss% 0.035433663545692315
plot_id,batch_id 0 94 miss% 0.07448255755138014
plot_id,batch_id 0 95 miss% 0.0352795554476
plot_id,batch_id 0 96 miss% 0.05816666684002662
plot_id,batch_id 0 97 miss% 0.04831129946707253
plot_id,batch_id 0 98 miss% 0.02219945612520429
plot_id,batch_id 0 99 miss% 0.04853168701229581
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08040691 0.0523857  0.12109527 0.06096069 0.055625   0.04644061
 0.07457026 0.10552745 0.08150687 0.07393318 0.04490244 0.04502995
 0.0639922  0.04986108 0.09896985 0.05453861 0.15889004 0.03803155
 0.07213391 0.07673653 0.05128344 0.07024391 0.08848168 0.04536671
 0.06763018 0.08293071 0.05896487 0.05219058 0.03140342 0.02269091
 0.03554515 0.07122906 0.10788075 0.06772107 0.03709236 0.05713605
 0.07697523 0.06120498 0.11925845 0.11105695 0.15421836 0.05464366
 0.01911509 0.08894881 0.05572105 0.04386704 0.03344838 0.02840987
 0.03584864 0.06050485 0.10890964 0.0292118  0.03495171 0.04234514
 0.03160475 0.06127448 0.08909335 0.03714695 0.04042354 0.04685374
 0.03673884 0.0209507  0.04404382 0.05891207 0.07426357 0.09395942
 0.14234041 0.02829465 0.03447292 0.0644157  0.07973732 0.04668938
 0.1056687  0.07333373 0.14952205 0.06015816 0.04466295 0.03576548
 0.03093986 0.1197611  0.05350898 0.0844144  0.05497146 0.06455729
 0.08342278 0.05182411 0.05929205 0.04255808 0.09031018 0.07050695
 0.05205954 0.06764426 0.03747573 0.03543366 0.07448256 0.03527956
 0.05816667 0.0483113  0.02219946 0.04853169]
for model  126 the mean error 0.06347946929744121
all id 126 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 25 out win 4 err 0.06347946929744121 time 13512.705030679703
Launcher: Job 127 completed in 13779 seconds.
Launcher: Task 80 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  55697
Epoch:0, Train loss:0.469126, valid loss:0.435657
Epoch:1, Train loss:0.103443, valid loss:0.003130
Epoch:2, Train loss:0.006414, valid loss:0.001835
Epoch:3, Train loss:0.003277, valid loss:0.001422
Epoch:4, Train loss:0.002499, valid loss:0.001191
Epoch:5, Train loss:0.002199, valid loss:0.001005
Epoch:6, Train loss:0.001964, valid loss:0.000937
Epoch:7, Train loss:0.001767, valid loss:0.000891
Epoch:8, Train loss:0.001654, valid loss:0.000906
Epoch:9, Train loss:0.001541, valid loss:0.000833
Epoch:10, Train loss:0.001452, valid loss:0.000760
Epoch:11, Train loss:0.001161, valid loss:0.000658
Epoch:12, Train loss:0.001125, valid loss:0.000606
Epoch:13, Train loss:0.001101, valid loss:0.000601
Epoch:14, Train loss:0.001045, valid loss:0.000744
Epoch:15, Train loss:0.001050, valid loss:0.000735
Epoch:16, Train loss:0.001007, valid loss:0.000670
Epoch:17, Train loss:0.001003, valid loss:0.000644
Epoch:18, Train loss:0.000980, valid loss:0.000532
Epoch:19, Train loss:0.000944, valid loss:0.000545
Epoch:20, Train loss:0.000929, valid loss:0.000640
Epoch:21, Train loss:0.000795, valid loss:0.000527
Epoch:22, Train loss:0.000777, valid loss:0.000506
Epoch:23, Train loss:0.000764, valid loss:0.000524
Epoch:24, Train loss:0.000761, valid loss:0.000487
Epoch:25, Train loss:0.000750, valid loss:0.000533
Epoch:26, Train loss:0.000737, valid loss:0.000493
Epoch:27, Train loss:0.000730, valid loss:0.000493
Epoch:28, Train loss:0.000727, valid loss:0.000490
Epoch:29, Train loss:0.000712, valid loss:0.000529
Epoch:30, Train loss:0.000720, valid loss:0.000509
Epoch:31, Train loss:0.000635, valid loss:0.000479
Epoch:32, Train loss:0.000631, valid loss:0.000473
Epoch:33, Train loss:0.000626, valid loss:0.000463
Epoch:34, Train loss:0.000625, valid loss:0.000511
Epoch:35, Train loss:0.000619, valid loss:0.000425
Epoch:36, Train loss:0.000616, valid loss:0.000489
Epoch:37, Train loss:0.000615, valid loss:0.000468
Epoch:38, Train loss:0.000607, valid loss:0.000474
Epoch:39, Train loss:0.000608, valid loss:0.000463
Epoch:40, Train loss:0.000600, valid loss:0.000450
Epoch:41, Train loss:0.000563, valid loss:0.000464
Epoch:42, Train loss:0.000562, valid loss:0.000447
Epoch:43, Train loss:0.000561, valid loss:0.000458
Epoch:44, Train loss:0.000560, valid loss:0.000448
Epoch:45, Train loss:0.000554, valid loss:0.000452
Epoch:46, Train loss:0.000555, valid loss:0.000436
Epoch:47, Train loss:0.000551, valid loss:0.000447
Epoch:48, Train loss:0.000549, valid loss:0.000453
Epoch:49, Train loss:0.000548, valid loss:0.000450
Epoch:50, Train loss:0.000549, valid loss:0.000454
Epoch:51, Train loss:0.000525, valid loss:0.000439
Epoch:52, Train loss:0.000522, valid loss:0.000438
Epoch:53, Train loss:0.000521, valid loss:0.000440
Epoch:54, Train loss:0.000520, valid loss:0.000434
Epoch:55, Train loss:0.000520, valid loss:0.000439
Epoch:56, Train loss:0.000519, valid loss:0.000438
Epoch:57, Train loss:0.000519, valid loss:0.000443
Epoch:58, Train loss:0.000519, valid loss:0.000435
Epoch:59, Train loss:0.000519, valid loss:0.000432
Epoch:60, Train loss:0.000519, valid loss:0.000437
training time 13587.651848316193
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.03640857262333635
plot_id,batch_id 0 1 miss% 0.03730189764789333
plot_id,batch_id 0 2 miss% 0.10214012972277593
plot_id,batch_id 0 3 miss% 0.07362694922667241
plot_id,batch_id 0 4 miss% 0.04333314676811434
plot_id,batch_id 0 5 miss% 0.02214901530242475
plot_id,batch_id 0 6 miss% 0.053040352689183466
plot_id,batch_id 0 7 miss% 0.11393236624405421
plot_id,batch_id 0 8 miss% 0.08020983049964536
plot_id,batch_id 0 9 miss% 0.04514747928502543
plot_id,batch_id 0 10 miss% 0.0725861120171758
plot_id,batch_id 0 11 miss% 0.057613269847311785
plot_id,batch_id 0 12 miss% 0.06257018729653264
plot_id,batch_id 0 13 miss% 0.046547609862494156
plot_id,batch_id 0 14 miss% 0.10863417508118288
plot_id,batch_id 0 15 miss% 0.04530813734852585
plot_id,batch_id 0 16 miss% 0.058327696103935386
plot_id,batch_id 0 17 miss% 0.024234854066651088
plot_id,batch_id 0 18 miss% 0.0708325467404362
plot_id,batch_id 0 19 miss% 0.08523485560917753
plot_id,batch_id 0 20 miss% 0.11261197973198321
plot_id,batch_id 0 21 miss% 0.03763262015914625
plot_id,batch_id 0 22 miss% 0.0804961308301777
plot_id,batch_id 0 23 miss% 0.06744883980489134
plot_id,batch_id 0 24 miss% 0.03490282278353907
plot_id,batch_id 0 25 miss% 0.049546082094421615
plot_id,batch_id 0 26 miss% 0.12546739748254257
plot_id,batch_id 0 27 miss% 0.0648185214595739
plot_id,batch_id 0 28 miss% 0.05179793796876866
plot_id,batch_id 0 29 miss% 0.07210343835883407
plot_id,batch_id 0 30 miss% 0.04322441717580103
plot_id,batch_id 0 31 miss% 0.10391675876900346
plot_id,batch_id 0 32 miss% 0.15454455568887002
plot_id,batch_id 0 33 miss% 0.07372874285005068
plot_id,batch_id 0 34 miss% 0.08758460228822805
plot_id,batch_id 0 35 miss% 0.08988789575811247
plot_id,batch_id 0 36 miss% 0.07860263258808556
plot_id,batch_id 0 37 miss% 0.09477851536208816
plot_id,batch_id 0 38 miss% 0.0751621837214461
plot_id,batch_id 0 39 miss% 0.0575648450689691
plot_id,batch_id 0 40 miss% 0.07078203509368731
plot_id,batch_id 0 41 miss% 0.05161688450326104
plot_id,batch_id 0 42 miss% 0.07439179113975168
plot_id,batch_id 0 43 miss% 0.05159520667827132
plot_id,batch_id 0 44 miss% 0.0570432549784441
plot_id,batch_id 0 45 miss% 0.07023776376183452
plot_id,batch_id 0 46 miss% 0.05192111746603994
plot_id,batch_id 0 47 miss% 0.03756741298342501
plot_id,batch_id 0 48 miss% 0.04399874062216629
plot_id,batch_id 0 49 miss% 0.04243266825049967
plot_id,batch_id 0 50 miss% 0.19368338860728596
plot_id,batch_id 0 51 miss% 0.03402787107455337
plot_id,batch_id 0 52 miss% 0.030221390709185496
plot_id,batch_id 0 53 miss% 0.06448861945877293
plot_id,batch_id 0 54 miss% 0.06171119594832527
plot_id,batch_id 0 55 miss% 0.12411174872099327
plot_id,batch_id 0 56 miss% 0.1380599790291549
plot_id,batch_id 0 57 miss% 0.058614793827803956
plot_id,batch_id 0 58 miss% 0.05325269584007255
plot_id,batch_id 0 59 miss% 0.04525243180718641
plot_id,batch_id 0 60 miss% 0.05188895639890941
plot_id,batch_id 0 61 miss% 0.03299187090233707
plot_id,batch_id 0 62 miss% 0.0488186917647616
plot_id,batch_id 0 63 miss% 0.02800616606871284
plot_id,batch_id 0 64 miss% 0.06037824296154541
plot_id,batch_id 0 65 miss%the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  89105
Epoch:0, Train loss:0.396860, valid loss:0.357440
Epoch:1, Train loss:0.040379, valid loss:0.002514
Epoch:2, Train loss:0.005993, valid loss:0.001621
Epoch:3, Train loss:0.002953, valid loss:0.001327
Epoch:4, Train loss:0.002442, valid loss:0.001189
Epoch:5, Train loss:0.002147, valid loss:0.001184
Epoch:6, Train loss:0.001947, valid loss:0.000975
Epoch:7, Train loss:0.001786, valid loss:0.000812
Epoch:8, Train loss:0.001676, valid loss:0.000999
Epoch:9, Train loss:0.001549, valid loss:0.000735
Epoch:10, Train loss:0.001479, valid loss:0.001112
Epoch:11, Train loss:0.001125, valid loss:0.000568
Epoch:12, Train loss:0.001081, valid loss:0.000591
Epoch:13, Train loss:0.001040, valid loss:0.000661
Epoch:14, Train loss:0.001012, valid loss:0.000592
Epoch:15, Train loss:0.000984, valid loss:0.000755
Epoch:16, Train loss:0.000952, valid loss:0.000603
Epoch:17, Train loss:0.000914, valid loss:0.000575
Epoch:18, Train loss:0.000915, valid loss:0.000658
Epoch:19, Train loss:0.000870, valid loss:0.000535
Epoch:20, Train loss:0.000899, valid loss:0.000601
Epoch:21, Train loss:0.000706, valid loss:0.000479
Epoch:22, Train loss:0.000679, valid loss:0.000502
Epoch:23, Train loss:0.000658, valid loss:0.000444
Epoch:24, Train loss:0.000669, valid loss:0.000444
Epoch:25, Train loss:0.000631, valid loss:0.000490
Epoch:26, Train loss:0.000630, valid loss:0.000492
Epoch:27, Train loss:0.000620, valid loss:0.000571
Epoch:28, Train loss:0.000630, valid loss:0.000434
Epoch:29, Train loss:0.000597, valid loss:0.000431
Epoch:30, Train loss:0.000601, valid loss:0.000465
Epoch:31, Train loss:0.000514, valid loss:0.000427
Epoch:32, Train loss:0.000500, valid loss:0.000428
Epoch:33, Train loss:0.000504, valid loss:0.000434
Epoch:34, Train loss:0.000495, valid loss:0.000440
Epoch:35, Train loss:0.000492, valid loss:0.000421
Epoch:36, Train loss:0.000494, valid loss:0.000420
Epoch:37, Train loss:0.000485, valid loss:0.000397
Epoch:38, Train loss:0.000485, valid loss:0.000457
Epoch:39, Train loss:0.000480, valid loss:0.000422
Epoch:40, Train loss:0.000476, valid loss:0.000414
Epoch:41, Train loss:0.000429, valid loss:0.000404
Epoch:42, Train loss:0.000424, valid loss:0.000398
Epoch:43, Train loss:0.000422, valid loss:0.000411
Epoch:44, Train loss:0.000421, valid loss:0.000398
Epoch:45, Train loss:0.000424, valid loss:0.000401
Epoch:46, Train loss:0.000421, valid loss:0.000461
Epoch:47, Train loss:0.000417, valid loss:0.000425
Epoch:48, Train loss:0.000412, valid loss:0.000412
Epoch:49, Train loss:0.000417, valid loss:0.000420
Epoch:50, Train loss:0.000408, valid loss:0.000403
Epoch:51, Train loss:0.000387, valid loss:0.000392
Epoch:52, Train loss:0.000384, valid loss:0.000394
Epoch:53, Train loss:0.000383, valid loss:0.000389
Epoch:54, Train loss:0.000383, valid loss:0.000390
Epoch:55, Train loss:0.000382, valid loss:0.000390
Epoch:56, Train loss:0.000382, valid loss:0.000389
Epoch:57, Train loss:0.000381, valid loss:0.000388
Epoch:58, Train loss:0.000381, valid loss:0.000388
Epoch:59, Train loss:0.000381, valid loss:0.000391
Epoch:60, Train loss:0.000381, valid loss:0.000391
training time 13607.051013946533
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.25794190851364285
plot_id,batch_id 0 1 miss% 0.33472325571800515
plot_id,batch_id 0 2 miss% 0.41787620841577705
plot_id,batch_id 0 3 miss% 0.3361557164198529
plot_id,batch_id 0 4 miss% 0.3205788359943714
plot_id,batch_id 0 5 miss% 0.28702994252353986
plot_id,batch_id 0 6 miss% 0.3007301007151316
plot_id,batch_id 0 7 miss% 0.46591285592191445
plot_id,batch_id 0 8 miss% 0.5209709398773499
plot_id,batch_id 0 9 miss% 0.5117235446500252
plot_id,batch_id 0 10 miss% 0.22360925552162841
plot_id,batch_id 0 11 miss% 0.349649368658033
plot_id,batch_id 0 12 miss% 0.35811224748631476
plot_id,batch_id 0 13 miss% 0.31281058079302193
plot_id,batch_id 0 14 miss% 0.4405340036888831
plot_id,batch_id 0 15 miss% 0.3273590138216685
plot_id,batch_id 0 16 miss% 0.4645229958656066
plot_id,batch_id 0 17 miss% 0.40956721418270137
plot_id,batch_id 0 18 miss% 0.4249876209505046
plot_id,batch_id 0 19 miss% 0.37922420476394264
plot_id,batch_id 0 20 miss% 0.3191260133993213
plot_id,batch_id 0 21 miss% 0.41319170324980936
plot_id,batch_id 0 22 miss% 0.3994196471140127
plot_id,batch_id 0 23 miss% 0.32637362914463003
plot_id,batch_id 0 24 miss% 0.3474049513456965
plot_id,batch_id 0 25 miss% 0.28034834799839825
plot_id,batch_id 0 26 miss% 0.38339400171293614
plot_id,batch_id 0 27 miss% 0.33621781526923866
plot_id,batch_id 0 28 miss% 0.39160679273991633
plot_id,batch_id 0 29 miss% 0.36388025940712565
plot_id,batch_id 0 30 miss% 0.2617533757322524
plot_id,batch_id 0 31 miss% 0.3862464641620139
plot_id,batch_id 0 32 miss% 0.3931495683614244
plot_id,batch_id 0 33 miss% 0.41969730336753397
plot_id,batch_id 0 34 miss% 0.3135927174842028
plot_id,batch_id 0 35 miss% 0.32342809797868327
plot_id,batch_id 0 36 miss% 0.4568948098789218
plot_id,batch_id 0 37 miss% 0.34213872839792414
plot_id,batch_id 0 38 miss% 0.48507463359387043
plot_id,batch_id 0 39 miss% 0.3998434262830921
plot_id,batch_id 0 40 miss% 0.3123097851262396
plot_id,batch_id 0 41 miss% 0.265774829369717
plot_id,batch_id 0 42 miss% 0.2497014206904185
plot_id,batch_id 0 43 miss% 0.3110769037407599
plot_id,batch_id 0 44 miss% 0.23787309315265423
plot_id,batch_id 0 45 miss% 0.21826759718460678
plot_id,batch_id 0 46 miss% 0.3080602476682985
plot_id,batch_id 0 47 miss% 0.28172702987420284
plot_id,batch_id 0 48 miss% 0.2918430582059346
plot_id,batch_id 0 49 miss% 0.25866011661619226
plot_id,batch_id 0 50 miss% 0.40523847148425163
plot_id,batch_id 0 51 miss% 0.34050983267311663
plot_id,batch_id 0 52 miss% 0.335754524329472
plot_id,batch_id 0 53 miss% 0.26256583052866933
plot_id,batch_id 0 54 miss% 0.20596856626508142
plot_id,batch_id 0 55 miss% 0.31832117516800346
plot_id,batch_id 0 56 miss% 0.34279079337997004
plot_id,batch_id 0 57 miss% 0.4612055145696178
plot_id,batch_id 0 58 miss% 0.2813526345591715
plot_id,batch_id 0 59 miss% 0.3603933834404363
plot_id,batch_id 0 60 miss% 0.2217021316887482
plot_id,batch_id 0 61 miss% 0.2277398763311975
plot_id,batch_id 0 62 miss% 0.40859886731853673
plot_id,batch_id 0 63 miss% 0.36475290839037766
plot_id,batch_id 0 64 miss% 0.37921561301864704
plot_id,batch_id 0 65 miss% 0.24888636184700927
plot_id,batch_id 0 66 miss% plot_id,batch_id 0 68 miss% 0.04356201382294261
plot_id,batch_id 0 69 miss% 0.05964270894970262
plot_id,batch_id 0 70 miss% 0.040537410661607085
plot_id,batch_id 0 71 miss% 0.023644851149322414
plot_id,batch_id 0 72 miss% 0.05065415932782207
plot_id,batch_id 0 73 miss% 0.06792535909622986
plot_id,batch_id 0 74 miss% 0.04940967542341528
plot_id,batch_id 0 75 miss% 0.1194252888429927
plot_id,batch_id 0 76 miss% 0.13252860590353452
plot_id,batch_id 0 77 miss% 0.030855485473500574
plot_id,batch_id 0 78 miss% 0.05335520829167855
plot_id,batch_id 0 79 miss% 0.07098674868635656
plot_id,batch_id 0 80 miss% 0.053462697498173076
plot_id,batch_id 0 81 miss% 0.06512238529106304
plot_id,batch_id 0 82 miss% 0.040428151000616384
plot_id,batch_id 0 83 miss% 0.06156596870300299
plot_id,batch_id 0 84 miss% 0.09483653534486886
plot_id,batch_id 0 85 miss% 0.02908739895253045
plot_id,batch_id 0 86 miss% 0.082517305076188
plot_id,batch_id 0 87 miss% 0.052040684541101875
plot_id,batch_id 0 88 miss% 0.14540715603201598
plot_id,batch_id 0 89 miss% 0.05819652484522936
plot_id,batch_id 0 90 miss% 0.033950128229198656
plot_id,batch_id 0 91 miss% 0.10335386900141898
plot_id,batch_id 0 92 miss% 0.03274535952799307
plot_id,batch_id 0 93 miss% 0.04340455369077945
plot_id,batch_id 0 94 miss% 0.08975092043634192
plot_id,batch_id 0 95 miss% 0.04312494272004838
plot_id,batch_id 0 96 miss% 0.038203323606191726
plot_id,batch_id 0 97 miss% 0.05806848683376059
plot_id,batch_id 0 98 miss% 0.018607827698167007
plot_id,batch_id 0 99 miss% 0.07481667724829351
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06609715 0.06295524 0.11501971 0.05708279 0.06135888 0.04005311
 0.07567833 0.10511231 0.07292315 0.04073606 0.02832561 0.0632239
 0.07611329 0.04941676 0.10694555 0.03739453 0.18811075 0.04214171
 0.06838451 0.10915045 0.0778925  0.03398675 0.02226044 0.04054954
 0.03007114 0.04951703 0.05665096 0.05312776 0.04586667 0.02991204
 0.06938901 0.08147143 0.11917895 0.06002646 0.0775389  0.02889447
 0.08816515 0.11119723 0.02911663 0.03165326 0.06533893 0.06168933
 0.03154749 0.04605483 0.0372496  0.06187475 0.03506699 0.03633713
 0.03291605 0.02730844 0.1279078  0.05789689 0.06425415 0.04031182
 0.03040843 0.07482583 0.06778357 0.06467453 0.05150538 0.03991574
 0.06426771 0.04501456 0.05192517 0.05916131 0.05746248 0.1172802
 0.05640355 0.02929149 0.04356201 0.05964271 0.04053741 0.02364485
 0.05065416 0.06792536 0.04940968 0.11942529 0.13252861 0.03085549
 0.05335521 0.07098675 0.0534627  0.06512239 0.04042815 0.06156597
 0.09483654 0.0290874  0.08251731 0.05204068 0.14540716 0.05819652
 0.03395013 0.10335387 0.03274536 0.04340455 0.08975092 0.04312494
 0.03820332 0.05806849 0.01860783 0.07481668]
for model  94 the mean error 0.06099582666080278
all id 94 hidden_dim 24 learning_rate 0.0025 num_layers 4 frames 25 out win 5 err 0.06099582666080278 time 13542.758927106857
Launcher: Job 95 completed in 13809 seconds.
Launcher: Task 160 done. Exiting.
0.33157874354603534
plot_id,batch_id 0 67 miss% 0.2608702902596713
plot_id,batch_id 0 68 miss% 0.3721502033063364
plot_id,batch_id 0 69 miss% 0.4486075259160984
plot_id,batch_id 0 70 miss% 0.20393347778661058
plot_id,batch_id 0 71 miss% 0.27647862859292527
plot_id,batch_id 0 72 miss% 0.36578344710173694
plot_id,batch_id 0 73 miss% 0.2945277994712503
plot_id,batch_id 0 74 miss% 0.4100791762141096
plot_id,batch_id 0 75 miss% 0.19730642618000216
plot_id,batch_id 0 76 miss% 0.2808159954704865
plot_id,batch_id 0 77 miss% 0.3333133409585374
plot_id,batch_id 0 78 miss% 0.30049231209088173
plot_id,batch_id 0 79 miss% 0.3623647703853145
plot_id,batch_id 0 80 miss% 0.2649042933060343
plot_id,batch_id 0 81 miss% 0.33451892693980495
plot_id,batch_id 0 82 miss% 0.3785742965834938
plot_id,batch_id 0 83 miss% 0.3422543620612562
plot_id,batch_id 0 84 miss% 0.29995823464185617
plot_id,batch_id 0 85 miss% 0.21447674700694255
plot_id,batch_id 0 86 miss% 0.34602552532269026
plot_id,batch_id 0 87 miss% 0.28152531621430027
plot_id,batch_id 0 88 miss% 0.39062157515703283
plot_id,batch_id 0 89 miss% 0.3964302459535335
plot_id,batch_id 0 90 miss% 0.18606350877822742
plot_id,batch_id 0 91 miss% 0.24805854434939845
plot_id,batch_id 0 92 miss% 0.2574139268465222
plot_id,batch_id 0 93 miss% 0.25677490087461746
plot_id,batch_id 0 94 miss% 0.4851160043999374
plot_id,batch_id 0 95 miss% 0.2215622846192771
plot_id,batch_id 0 96 miss% 0.24875163176697213
plot_id,batch_id 0 97 miss% 0.38008925410574884
plot_id,batch_id 0 98 miss% 0.40470581944121664
plot_id,batch_id 0 99 miss% 0.3551650009570499
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.25794191 0.33472326 0.41787621 0.33615572 0.32057884 0.28702994
 0.3007301  0.46591286 0.52097094 0.51172354 0.22360926 0.34964937
 0.35811225 0.31281058 0.440534   0.32735901 0.464523   0.40956721
 0.42498762 0.3792242  0.31912601 0.4131917  0.39941965 0.32637363
 0.34740495 0.28034835 0.383394   0.33621782 0.39160679 0.36388026
 0.26175338 0.38624646 0.39314957 0.4196973  0.31359272 0.3234281
 0.45689481 0.34213873 0.48507463 0.39984343 0.31230979 0.26577483
 0.24970142 0.3110769  0.23787309 0.2182676  0.30806025 0.28172703
 0.29184306 0.25866012 0.40523847 0.34050983 0.33575452 0.26256583
 0.20596857 0.31832118 0.34279079 0.46120551 0.28135263 0.36039338
 0.22170213 0.22773988 0.40859887 0.36475291 0.37921561 0.24888636
 0.33157874 0.26087029 0.3721502  0.44860753 0.20393348 0.27647863
 0.36578345 0.2945278  0.41007918 0.19730643 0.280816   0.33331334
 0.30049231 0.36236477 0.26490429 0.33451893 0.3785743  0.34225436
 0.29995823 0.21447675 0.34602553 0.28152532 0.39062158 0.39643025
 0.18606351 0.24805854 0.25741393 0.2567749  0.485116   0.22156228
 0.24875163 0.38008925 0.40470582 0.355165  ]
for model  193 the mean error 0.3341838520433016
all id 193 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 31 out win 5 err 0.3341838520433016 time 13607.051013946533
Launcher: Job 194 completed in 13844 seconds.
Launcher: Task 157 done. Exiting.
 0.036994001051676204
plot_id,batch_id 0 66 miss% 0.032665488309301265
plot_id,batch_id 0 67 miss% 0.031332089344202306
plot_id,batch_id 0 68 miss% 0.045795151370228625
plot_id,batch_id 0 69 miss% 0.0690854025847576
plot_id,batch_id 0 70 miss% 0.05937562635952194
plot_id,batch_id 0 71 miss% 0.03898980095750908
plot_id,batch_id 0 72 miss% 0.0751740610278928
plot_id,batch_id 0 73 miss% 0.045300900812158755
plot_id,batch_id 0 74 miss% 0.1434886757289435
plot_id,batch_id 0 75 miss% 0.03958236719043423
plot_id,batch_id 0 76 miss% 0.08020850100461817
plot_id,batch_id 0 77 miss% 0.04173250877936926
plot_id,batch_id 0 78 miss% 0.03412642365131633
plot_id,batch_id 0 79 miss% 0.11218765359777944
plot_id,batch_id 0 80 miss% 0.05610884918738238
plot_id,batch_id 0 81 miss% 0.09602327976012923
plot_id,batch_id 0 82 miss% 0.09040102383745607
plot_id,batch_id 0 83 miss% 0.09966861525812139
plot_id,batch_id 0 84 miss% 0.04719657179306829
plot_id,batch_id 0 85 miss% 0.03940489693555159
plot_id,batch_id 0 86 miss% 0.04125298177578907
plot_id,batch_id 0 87 miss% 0.04582631864937722
plot_id,batch_id 0 88 miss% 0.09077822732121085
plot_id,batch_id 0 89 miss% 0.05790254613249014
plot_id,batch_id 0 90 miss% 0.025030741399327646
plot_id,batch_id 0 91 miss% 0.08616004701227392
plot_id,batch_id 0 92 miss% 0.03198840712991876
plot_id,batch_id 0 93 miss% 0.05231493919920943
plot_id,batch_id 0 94 miss% 0.08360733154846657
plot_id,batch_id 0 95 miss% 0.04068450774868401
plot_id,batch_id 0 96 miss% 0.07051844413085594
plot_id,batch_id 0 97 miss% 0.05155013506489301
plot_id,batch_id 0 98 miss% 0.07490984855215349
plot_id,batch_id 0 99 miss% 0.08703624677241369
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03640857 0.0373019  0.10214013 0.07362695 0.04333315 0.02214902
 0.05304035 0.11393237 0.08020983 0.04514748 0.07258611 0.05761327
 0.06257019 0.04654761 0.10863418 0.04530814 0.0583277  0.02423485
 0.07083255 0.08523486 0.11261198 0.03763262 0.08049613 0.06744884
 0.03490282 0.04954608 0.1254674  0.06481852 0.05179794 0.07210344
 0.04322442 0.10391676 0.15454456 0.07372874 0.0875846  0.0898879
 0.07860263 0.09477852 0.07516218 0.05756485 0.07078204 0.05161688
 0.07439179 0.05159521 0.05704325 0.07023776 0.05192112 0.03756741
 0.04399874 0.04243267 0.19368339 0.03402787 0.03022139 0.06448862
 0.0617112  0.12411175 0.13805998 0.05861479 0.0532527  0.04525243
 0.05188896 0.03299187 0.04881869 0.02800617 0.06037824 0.036994
 0.03266549 0.03133209 0.04579515 0.0690854  0.05937563 0.0389898
 0.07517406 0.0453009  0.14348868 0.03958237 0.0802085  0.04173251
 0.03412642 0.11218765 0.05610885 0.09602328 0.09040102 0.09966862
 0.04719657 0.0394049  0.04125298 0.04582632 0.09077823 0.05790255
 0.02503074 0.08616005 0.03198841 0.05231494 0.08360733 0.04068451
 0.07051844 0.05155014 0.07490985 0.08703625]
for model  171 the mean error 0.06526497633502545
all id 171 hidden_dim 16 learning_rate 0.0025 num_layers 4 frames 31 out win 4 err 0.06526497633502545 time 13587.651848316193
Launcher: Job 172 completed in 13852 seconds.
Launcher: Task 22 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  209681
Epoch:0, Train loss:0.619391, valid loss:0.620696
Epoch:1, Train loss:0.053590, valid loss:0.011855
Epoch:2, Train loss:0.017711, valid loss:0.004943
Epoch:3, Train loss:0.013328, valid loss:0.004605
Epoch:4, Train loss:0.012160, valid loss:0.004229
Epoch:5, Train loss:0.010777, valid loss:0.003727
Epoch:6, Train loss:0.009500, valid loss:0.002886
Epoch:7, Train loss:0.006865, valid loss:0.002525
Epoch:8, Train loss:0.004275, valid loss:0.001853
Epoch:9, Train loss:0.003436, valid loss:0.001500
Epoch:10, Train loss:0.003073, valid loss:0.001472
Epoch:11, Train loss:0.002325, valid loss:0.001250
Epoch:12, Train loss:0.002173, valid loss:0.001255
Epoch:13, Train loss:0.002088, valid loss:0.001113
Epoch:14, Train loss:0.002035, valid loss:0.001197
Epoch:15, Train loss:0.001892, valid loss:0.001166
Epoch:16, Train loss:0.001847, valid loss:0.001305
Epoch:17, Train loss:0.001799, valid loss:0.001144
Epoch:18, Train loss:0.001748, valid loss:0.001112
Epoch:19, Train loss:0.001657, valid loss:0.001083
Epoch:20, Train loss:0.001604, valid loss:0.001106
Epoch:21, Train loss:0.001272, valid loss:0.000952
Epoch:22, Train loss:0.001197, valid loss:0.000947
Epoch:23, Train loss:0.001229, valid loss:0.000932
Epoch:24, Train loss:0.001147, valid loss:0.000935
Epoch:25, Train loss:0.001141, valid loss:0.000954
Epoch:26, Train loss:0.001103, valid loss:0.000895
Epoch:27, Train loss:0.001088, valid loss:0.000946
Epoch:28, Train loss:0.001100, valid loss:0.000937
Epoch:29, Train loss:0.001093, valid loss:0.001086
Epoch:30, Train loss:0.001038, valid loss:0.000902
Epoch:31, Train loss:0.000865, valid loss:0.000864
Epoch:32, Train loss:0.000852, valid loss:0.000832
Epoch:33, Train loss:0.000828, valid loss:0.000814
Epoch:34, Train loss:0.000839, valid loss:0.000884
Epoch:35, Train loss:0.000828, valid loss:0.000833
Epoch:36, Train loss:0.000808, valid loss:0.000830
Epoch:37, Train loss:0.000815, valid loss:0.000834
Epoch:38, Train loss:0.000788, valid loss:0.000940
Epoch:39, Train loss:0.000781, valid loss:0.000786
Epoch:40, Train loss:0.000782, valid loss:0.000842
Epoch:41, Train loss:0.000692, valid loss:0.000815
Epoch:42, Train loss:0.000683, valid loss:0.000779
Epoch:43, Train loss:0.000672, valid loss:0.000789
Epoch:44, Train loss:0.000676, valid loss:0.000772
Epoch:45, Train loss:0.000664, valid loss:0.000793
Epoch:46, Train loss:0.000672, valid loss:0.000787
Epoch:47, Train loss:0.000655, valid loss:0.000795
Epoch:48, Train loss:0.000654, valid loss:0.000788
Epoch:49, Train loss:0.000645, valid loss:0.000806
Epoch:50, Train loss:0.000644, valid loss:0.000792
Epoch:51, Train loss:0.000605, valid loss:0.000773
Epoch:52, Train loss:0.000597, valid loss:0.000783
Epoch:53, Train loss:0.000594, valid loss:0.000802
Epoch:54, Train loss:0.000592, valid loss:0.000766
Epoch:55, Train loss:0.000591, valid loss:0.000764
Epoch:56, Train loss:0.000590, valid loss:0.000768
Epoch:57, Train loss:0.000589, valid loss:0.000775
Epoch:58, Train loss:0.000588, valid loss:0.000774
Epoch:59, Train loss:0.000588, valid loss:0.000767
Epoch:60, Train loss:0.000587, valid loss:0.000767
training time 13653.510697364807
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.09523174698711334
plot_id,batch_id 0 1 miss% 0.052518069732354664
plot_id,batch_id 0 2 miss% 0.06868535994785836
plot_id,batch_id 0 3 miss% 0.04073110993038164
plot_id,batch_id 0 4 miss% 0.037200402389411774
plot_id,batch_id 0 5 miss% 0.061101717096966046
plot_id,batch_id 0 6 miss% 0.05910248079030681
plot_id,batch_id 0 7 miss% 0.07727161552889993
plot_id,batch_id 0 8 miss% 0.0861250322740978
plot_id,batch_id 0 9 miss% 0.053732723909526256
plot_id,batch_id 0 10 miss% 0.028473979061835568
plot_id,batch_id 0 11 miss% 0.05644114230333132
plot_id,batch_id 0 12 miss% 0.07150347899261335
plot_id,batch_id 0 13 miss% 0.10395214985608532
plot_id,batch_id 0 14 miss% 0.05064603242070434
plot_id,batch_id 0 15 miss% 0.047957514000488234
plot_id,batch_id 0 16 miss% 0.13704075394121618
plot_id,batch_id 0 17 miss% 0.07547513610773707
plot_id,batch_id 0 18 miss% 0.05752046673464338
plot_id,batch_id 0 19 miss% 0.0996858096693725
plot_id,batch_id 0 20 miss% 0.055904777657431036
plot_id,batch_id 0 21 miss% 0.07063573363143165
plot_id,batch_id 0 22 miss% 0.0302291207943289
plot_id,batch_id 0 23 miss% 0.026637153222171917
plot_id,batch_id 0 24 miss% 0.03079632012806972
plot_id,batch_id 0 25 miss% 0.10797263226777137
plot_id,batch_id 0 26 miss% 0.03164020254916625
plot_id,batch_id 0 27 miss% 0.05381299437993856
plot_id,batch_id 0 28 miss% 0.02329018437589936
plot_id,batch_id 0 29 miss% 0.03270213795846391
plot_id,batch_id 0 30 miss% 0.02325945860367478
plot_id,batch_id 0 31 miss% 0.06593382834930721
plot_id,batch_id 0 32 miss% 0.06894453601176356
plot_id,batch_id 0 33 miss% 0.04825478422712184
plot_id,batch_id 0 34 miss% 0.04877097060543339
plot_id,batch_id 0 35 miss% 0.13155612300863753
plot_id,batch_id 0 36 miss% 0.10133727304637458
plot_id,batch_id 0 37 miss% 0.050968332035813885
plot_id,batch_id 0 38 miss% 0.04683254130635629
plot_id,batch_id 0 39 miss% 0.03979481214431789
plot_id,batch_id 0 40 miss% 0.0908094429550519
plot_id,batch_id 0 41 miss% 0.04126585026768617
plot_id,batch_id 0 42 miss% 0.01895993510905231
plot_id,batch_id 0 43 miss% 0.056789980491055915
plot_id,batch_id 0 44 miss% 0.05547465045475304
plot_id,batch_id 0 45 miss% 0.03133747643997525
plot_id,batch_id 0 46 miss% 0.039074652115642663
plot_id,batch_id 0 47 miss% 0.040032739143459055
plot_id,batch_id 0 48 miss% 0.030580128495785276
plot_id,batch_id 0 49 miss% 0.024436443618515342
plot_id,batch_id 0 50 miss% 0.1158601269456768
plot_id,batch_id 0 51 miss% 0.023249326422639747
plot_id,batch_id 0 52 miss% 0.050420398408044666
plot_id,batch_id 0 53 miss% 0.021457432125981824
plot_id,batch_id 0 54 miss% 0.036086323846994284
plot_id,batch_id 0 55 miss% 0.0555959919127808
plot_id,batch_id 0 56 miss% 0.08376413064475091
plot_id,batch_id 0 57 miss% 0.05866176786681883
plot_id,batch_id 0 58 miss% 0.05353353070466348
plot_id,batch_id 0 59 miss% 0.03835470769116382
plot_id,batch_id 0 60 miss% 0.03531669555390667
plot_id,batch_id 0 61 miss% 0.04208066204985782
plot_id,batch_id 0 62 miss% 0.06552261707567805
plot_id,batch_id 0 63 miss% 0.06657614777797581
plot_id,batch_id 0 64 miss% 0.0633572868917868
plot_id,batch_id 0 65 miss% 0.05417951526925394
plot_id,batch_id 0 66 miss% 0.05346219050590295
plot_id,batch_id 0 67 miss% 0.01766290370296956
plot_id,batch_id 0 68 miss% 0.03701395465812804
plot_id,batch_id 0 69 miss% 0.05480420094513062
plot_id,batch_id 0 70 miss% 0.09662732197604611
plot_id,batch_id 0 71 miss% 0.04411988455881915
plot_id,batch_id 0 72 miss% 0.08528019629544442
plot_id,batch_id 0 73 miss% 0.0694086835742893
plot_id,batch_id 0 74 miss% 0.0904780051033135
plot_id,batch_id 0 75 miss% 0.09952297087522569
plot_id,batch_id 0 76 miss% 0.13061823904343875
plot_id,batch_id 0 77 miss% 0.047894060686907325
plot_id,batch_id 0 78 miss% 0.029942155288630045
plot_id,batch_id 0 79 miss% 0.05006889825438971
plot_id,batch_id 0 80 miss% 0.04012834612493107
plot_id,batch_id 0 81 miss% 0.0690060105548296
plot_id,batch_id 0 82 miss% 0.06330326284340335
plot_id,batch_id 0 83 miss% 0.03707957387397215
plot_id,batch_id 0 84 miss% 0.088144605708319
plot_id,batch_id 0 85 miss% 0.05969279159996714
plot_id,batch_id 0 86 miss% 0.05004405647282189
plot_id,batch_id 0 87 miss% 0.07316919455825899
plot_id,batch_id 0 88 miss% 0.07938276527365018
plot_id,batch_id 0 89 miss% 0.072012946850389
plot_id,batch_id 0 90 miss% 0.04014085711075617
plot_id,batch_id 0 91 miss% 0.07464583491796606
plot_id,batch_id 0 92 miss% 0.047083067952611246
plot_id,batch_id 0 93 miss% 0.06419143276139849
plot_id,batch_id 0 94 miss% 0.07307256988708827
plot_id,batch_id 0 95 miss% 0.07064847838954579
plot_id,batch_id 0 96 miss% 0.04239918407979089
plot_id,batch_id 0 97 miss% 0.05401888544805074
plot_id,batch_id 0 98 miss% 0.03839336099426608
plot_id,batch_id 0 99 miss% 0.12851584635738572
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09523175 0.05251807 0.06868536 0.04073111 0.0372004  0.06110172
 0.05910248 0.07727162 0.08612503 0.05373272 0.02847398 0.05644114
 0.07150348 0.10395215 0.05064603 0.04795751 0.13704075 0.07547514
 0.05752047 0.09968581 0.05590478 0.07063573 0.03022912 0.02663715
 0.03079632 0.10797263 0.0316402  0.05381299 0.02329018 0.03270214
 0.02325946 0.06593383 0.06894454 0.04825478 0.04877097 0.13155612
 0.10133727 0.05096833 0.04683254 0.03979481 0.09080944 0.04126585
 0.01895994 0.05678998 0.05547465 0.03133748 0.03907465 0.04003274
 0.03058013 0.02443644 0.11586013 0.02324933 0.0504204  0.02145743
 0.03608632 0.05559599 0.08376413 0.05866177 0.05353353 0.03835471
 0.0353167  0.04208066 0.06552262 0.06657615 0.06335729 0.05417952
 0.05346219 0.0176629  0.03701395 0.0548042  0.09662732 0.04411988
 0.0852802  0.06940868 0.09047801 0.09952297 0.13061824 0.04789406
 0.02994216 0.0500689  0.04012835 0.06900601 0.06330326 0.03707957
 0.08814461 0.05969279 0.05004406 0.07316919 0.07938277 0.07201295
 0.04014086 0.07464583 0.04708307 0.06419143 0.07307257 0.07064848
 0.04239918 0.05401889 0.03839336 0.12851585]
for model  16 the mean error 0.059144253354854064
all id 16 hidden_dim 32 learning_rate 0.0025 num_layers 4 frames 21 out win 5 err 0.059144253354854064 time 13653.510697364807
Launcher: Job 17 completed in 13921 seconds.
Launcher: Task 243 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  69649
Epoch:0, Train loss:0.562909, valid loss:0.538390
Epoch:1, Train loss:0.048139, valid loss:0.008868
Epoch:2, Train loss:0.015170, valid loss:0.004171
Epoch:3, Train loss:0.011127, valid loss:0.003822
Epoch:4, Train loss:0.010179, valid loss:0.003198
Epoch:5, Train loss:0.008701, valid loss:0.002960
Epoch:6, Train loss:0.008332, valid loss:0.002958
Epoch:7, Train loss:0.008068, valid loss:0.002923
Epoch:8, Train loss:0.007720, valid loss:0.002483
Epoch:9, Train loss:0.007552, valid loss:0.002580
Epoch:10, Train loss:0.007328, valid loss:0.002752
Epoch:11, Train loss:0.006868, valid loss:0.002238
Epoch:12, Train loss:0.006744, valid loss:0.002212
Epoch:13, Train loss:0.006690, valid loss:0.002214
Epoch:14, Train loss:0.006499, valid loss:0.001741
Epoch:15, Train loss:0.005691, valid loss:0.001567
Epoch:16, Train loss:0.004564, valid loss:0.001605
Epoch:17, Train loss:0.004373, valid loss:0.001509
Epoch:18, Train loss:0.004278, valid loss:0.001521
Epoch:19, Train loss:0.004237, valid loss:0.001429
Epoch:20, Train loss:0.004198, valid loss:0.001497
Epoch:21, Train loss:0.003938, valid loss:0.001365
Epoch:22, Train loss:0.003898, valid loss:0.001345
Epoch:23, Train loss:0.003894, valid loss:0.001366
Epoch:24, Train loss:0.003874, valid loss:0.001332
Epoch:25, Train loss:0.003839, valid loss:0.001316
Epoch:26, Train loss:0.003819, valid loss:0.001302
Epoch:27, Train loss:0.003805, valid loss:0.001330
Epoch:28, Train loss:0.003786, valid loss:0.001273
Epoch:29, Train loss:0.003783, valid loss:0.001287
Epoch:30, Train loss:0.003746, valid loss:0.001270
Epoch:31, Train loss:0.003627, valid loss:0.001244
Epoch:32, Train loss:0.003615, valid loss:0.001199
Epoch:33, Train loss:0.003591, valid loss:0.001244
Epoch:34, Train loss:0.003607, valid loss:0.001203
Epoch:35, Train loss:0.003590, valid loss:0.001207
Epoch:36, Train loss:0.003572, valid loss:0.001206
Epoch:37, Train loss:0.003575, valid loss:0.001302
Epoch:38, Train loss:0.003558, valid loss:0.001214
Epoch:39, Train loss:0.003548, valid loss:0.001211
Epoch:40, Train loss:0.003541, valid loss:0.001090
Epoch:41, Train loss:0.002772, valid loss:0.000920
Epoch:42, Train loss:0.002500, valid loss:0.000898
Epoch:43, Train loss:0.002438, valid loss:0.000923
Epoch:44, Train loss:0.002370, valid loss:0.000859
Epoch:45, Train loss:0.001254, valid loss:0.000766
Epoch:46, Train loss:0.001047, valid loss:0.000759
Epoch:47, Train loss:0.001003, valid loss:0.000726
Epoch:48, Train loss:0.000980, valid loss:0.000692
Epoch:49, Train loss:0.000965, valid loss:0.000701
Epoch:50, Train loss:0.000947, valid loss:0.000701
Epoch:51, Train loss:0.000903, valid loss:0.000686
Epoch:52, Train loss:0.000897, valid loss:0.000685
Epoch:53, Train loss:0.000894, valid loss:0.000684
Epoch:54, Train loss:0.000893, valid loss:0.000683
Epoch:55, Train loss:0.000892, valid loss:0.000687
Epoch:56, Train loss:0.000892, valid loss:0.000695
Epoch:57, Train loss:0.000890, valid loss:0.000685
Epoch:58, Train loss:0.000890, valid loss:0.000684
Epoch:59, Train loss:0.000890, valid loss:0.000679
Epoch:60, Train loss:0.000890, valid loss:0.000684
training time 13835.001140117645
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.04687943012790966
plot_id,batch_id 0 1 miss% 0.05563673549441473
plot_id,batch_id 0 2 miss% 0.09135533815083073
plot_id,batch_id 0 3 miss% 0.04717277632338183
plot_id,batch_id 0 4 miss% 0.04660923943811597
plot_id,batch_id 0 5 miss% 0.05793913068031768
plot_id,batch_id 0 6 miss% 0.0580333665421834
plot_id,batch_id 0 7 miss% 0.1224668857669575
plot_id,batch_id 0 8 miss% 0.0914779300257336
plot_id,batch_id 0 9 miss% 0.06792891810767447
plot_id,batch_id 0 10 miss% 0.050687816168276015
plot_id,batch_id 0 11 miss% 0.07236339879662902
plot_id,batch_id 0 12 miss% 0.06693643601243973
plot_id,batch_id 0 13 miss% 0.060397825479981994
plot_id,batch_id 0 14 miss% 0.11761448865447675
plot_id,batch_id 0 15 miss% 0.029795728518339365
plot_id,batch_id 0 16 miss% 0.07135656414770994
plot_id,batch_id 0 17 miss% 0.044102860868340525
plot_id,batch_id 0 18 miss% 0.08718779452452072
plot_id,batch_id 0 19 miss% 0.12268934083030944
plot_id,batch_id 0 20 miss% 0.06982054593999586
plot_id,batch_id 0 21 miss% 0.03206059837996757
plot_id,batch_id 0 22 miss% 0.036354409079045934
plot_id,batch_id 0 23 miss% 0.024207560762108036
plot_id,batch_id 0 24 miss% 0.029297401002925376
plot_id,batch_id 0 25 miss% 0.07902403288809882
plot_id,batch_id 0 26 miss% 0.05466713717972164
plot_id,batch_id 0 27 miss% 0.052186049583294584
plot_id,batch_id 0 28 miss% 0.051282676131616495
plot_id,batch_id 0 29 miss% 0.039384948986782804
plot_id,batch_id 0 30 miss% 0.05027432412052567
plot_id,batch_id 0 31 miss% 0.11316832013501314
plot_id,batch_id 0 32 miss% 0.10837725272571994
plot_id,batch_id 0 33 miss% 0.05738826069635323
plot_id,batch_id 0 34 miss% 0.07021995924130044
plot_id,batch_id 0 35 miss% 0.041962370137349686
plot_id,batch_id 0 36 miss% 0.14980391050926065
plot_id,batch_id 0 37 miss% 0.07183056650710691
plot_id,batch_id 0 38 miss% 0.04624821459697166
plot_id,batch_id 0 39 miss% 0.042893645685098465
plot_id,batch_id 0 40 miss% 0.08043598530936678
plot_id,batch_id 0 41 miss% 0.0441960038467754
plot_id,batch_id 0 42 miss% 0.023792129844415148
plot_id,batch_id 0 43 miss% 0.012588308485184245
plot_id,batch_id 0 44 miss% 0.03208080653852652
plot_id,batch_id 0 45 miss% 0.053677022563645975
plot_id,batch_id 0 46 miss% 0.027584867453468283
plot_id,batch_id 0 47 miss% 0.026675965389742175
plot_id,batch_id 0 48 miss% 0.0351524536610836
plot_id,batch_id 0 49 miss% 0.02950565768693695
plot_id,batch_id 0 50 miss% 0.1707686809055131
plot_id,batch_id 0 51 miss% 0.03201046987336761
plot_id,batch_id 0 52 miss% 0.05108134512095664
plot_id,batch_id 0 53 miss% 0.022537340847125862
plot_id,batch_id 0 54 miss% 0.05343812677831584
plot_id,batch_id 0 55 miss% 0.09321023297638717
plot_id,batch_id 0 56 miss% 0.05752379954115868
plot_id,batch_id 0 57 miss% 0.06326215818679365
plot_id,batch_id 0 58 miss% 0.03597889887874439
plot_id,batch_id 0 59 miss% 0.03171381509083561
plot_id,batch_id 0 60 miss% 0.03902613333115543
plot_id,batch_id 0 61 miss% 0.1039865772833854
plot_id,batch_id 0 62 miss% 0.09158362890874708
plot_id,batch_id 0 63 miss% 0.058077418178670966
plot_id,batch_id 0 64 miss% 0.09478059378766035
plot_id,batch_id 0 65 miss% 0.10330954318098794
plot_id,batch_id 0 66 miss% 0.14141147984905936
plot_id,batch_id 0 67 miss% 0.060986346337483316
plot_id,batch_id 0 68 miss% 0.057788010898691664
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  89105
Epoch:0, Train loss:0.396860, valid loss:0.357440
Epoch:1, Train loss:0.060914, valid loss:0.002741
Epoch:2, Train loss:0.004043, valid loss:0.001768
Epoch:3, Train loss:0.003317, valid loss:0.001499
Epoch:4, Train loss:0.002974, valid loss:0.001486
Epoch:5, Train loss:0.002647, valid loss:0.001285
Epoch:6, Train loss:0.002453, valid loss:0.001271
Epoch:7, Train loss:0.002293, valid loss:0.001493
Epoch:8, Train loss:0.002119, valid loss:0.001124
Epoch:9, Train loss:0.002009, valid loss:0.001083
Epoch:10, Train loss:0.001890, valid loss:0.000887
Epoch:11, Train loss:0.001435, valid loss:0.000778
Epoch:12, Train loss:0.001390, valid loss:0.000671
Epoch:13, Train loss:0.001345, valid loss:0.000722
Epoch:14, Train loss:0.001312, valid loss:0.000778
Epoch:15, Train loss:0.001239, valid loss:0.000771
Epoch:16, Train loss:0.001229, valid loss:0.000669
Epoch:17, Train loss:0.001197, valid loss:0.000664
Epoch:18, Train loss:0.001175, valid loss:0.000622
Epoch:19, Train loss:0.001110, valid loss:0.000594
Epoch:20, Train loss:0.001097, valid loss:0.000727
Epoch:21, Train loss:0.000860, valid loss:0.000563
Epoch:22, Train loss:0.000839, valid loss:0.000623
Epoch:23, Train loss:0.000815, valid loss:0.000522
Epoch:24, Train loss:0.000813, valid loss:0.000541
Epoch:25, Train loss:0.000789, valid loss:0.000546
Epoch:26, Train loss:0.000789, valid loss:0.000565
Epoch:27, Train loss:0.000754, valid loss:0.000582
Epoch:28, Train loss:0.000759, valid loss:0.000513
Epoch:29, Train loss:0.000751, valid loss:0.000617
Epoch:30, Train loss:0.000732, valid loss:0.000522
Epoch:31, Train loss:0.000616, valid loss:0.000491
Epoch:32, Train loss:0.000612, valid loss:0.000492
Epoch:33, Train loss:0.000597, valid loss:0.000498
Epoch:34, Train loss:0.000587, valid loss:0.000540
Epoch:35, Train loss:0.000577, valid loss:0.000492
Epoch:36, Train loss:0.000577, valid loss:0.000491
Epoch:37, Train loss:0.000570, valid loss:0.000484
Epoch:38, Train loss:0.000576, valid loss:0.000525
Epoch:39, Train loss:0.000550, valid loss:0.000506
Epoch:40, Train loss:0.000556, valid loss:0.000490
Epoch:41, Train loss:0.000495, valid loss:0.000473
Epoch:42, Train loss:0.000487, valid loss:0.000473
Epoch:43, Train loss:0.000490, valid loss:0.000473
Epoch:44, Train loss:0.000491, valid loss:0.000467
Epoch:45, Train loss:0.000480, valid loss:0.000474
Epoch:46, Train loss:0.000479, valid loss:0.000486
Epoch:47, Train loss:0.000477, valid loss:0.000466
Epoch:48, Train loss:0.000470, valid loss:0.000489
Epoch:49, Train loss:0.000473, valid loss:0.000528
Epoch:50, Train loss:0.000471, valid loss:0.000487
Epoch:51, Train loss:0.000443, valid loss:0.000479
Epoch:52, Train loss:0.000440, valid loss:0.000477
Epoch:53, Train loss:0.000438, valid loss:0.000473
Epoch:54, Train loss:0.000437, valid loss:0.000473
Epoch:55, Train loss:0.000436, valid loss:0.000472
Epoch:56, Train loss:0.000435, valid loss:0.000471
Epoch:57, Train loss:0.000434, valid loss:0.000470
Epoch:58, Train loss:0.000434, valid loss:0.000470
Epoch:59, Train loss:0.000433, valid loss:0.000470
Epoch:60, Train loss:0.000433, valid loss:0.000468
training time 13872.18343424797
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.30230493402502
plot_id,batch_id 0 1 miss% 0.370635043021389
plot_id,batch_id 0 2 miss% 0.35092336384842704
plot_id,batch_id 0 3 miss% 0.3613575039086975
plot_id,batch_id 0 4 miss% 0.3332223122833759
plot_id,batch_id 0 5 miss% 0.26814756103179493
plot_id,batch_id 0 6 miss% 0.3313526817125124
plot_id,batch_id 0 7 miss% 0.36891910956504637
plot_id,batch_id 0 8 miss% 0.31861241910930294
plot_id,batch_id 0 9 miss% 0.3157318601523741
plot_id,batch_id 0 10 miss% 0.24126482940743613
plot_id,batch_id 0 11 miss% 0.2962808561912487
plot_id,batch_id 0 12 miss% 0.3145525931454967
plot_id,batch_id 0 13 miss% 0.3021858274929605
plot_id,batch_id 0 14 miss% 0.3491518107660319
plot_id,batch_id 0 15 miss% 0.3391379356651826
plot_id,batch_id 0 16 miss% 0.4391252628968915
plot_id,batch_id 0 17 miss% 0.45159086263622245
plot_id,batch_id 0 18 miss% 0.42798869214681723
plot_id,batch_id 0 19 miss% 0.38777650021322996
plot_id,batch_id 0 20 miss% 0.33910550664626515
plot_id,batch_id 0 21 miss% 0.3290798232049918
plot_id,batch_id 0 22 miss% 0.28346135470709083
plot_id,batch_id 0 23 miss% 0.32676843647053805
plot_id,batch_id 0 24 miss% 0.2655119934117475
plot_id,batch_id 0 25 miss% 0.2949316294370151
plot_id,batch_id 0 26 miss% 0.34703402748529466
plot_id,batch_id 0 27 miss% 0.3527372992888434
plot_id,batch_id 0 28 miss% 0.4613058965358629
plot_id,batch_id 0 29 miss% 0.3171244530907929
plot_id,batch_id 0 30 miss% 0.29726923658505133
plot_id,batch_id 0 31 miss% 0.43240825119999177
plot_id,batch_id 0 32 miss% 0.35188215227335573
plot_id,batch_id 0 33 miss% 0.47349689938873185
plot_id,batch_id 0 34 miss% 0.3076349431653543
plot_id,batch_id 0 35 miss% 0.23457157759969025
plot_id,batch_id 0 36 miss% 0.4466005392948494
plot_id,batch_id 0 37 miss% 0.32723513161799594
plot_id,batch_id 0 38 miss% 0.3838394189346655
plot_id,batch_id 0 39 miss% 0.4285277098719697
plot_id,batch_id 0 40 miss% 0.3833379352509426
plot_id,batch_id 0 41 miss% 0.3041516137929158
plot_id,batch_id 0 42 miss% 0.3124050076063533
plot_id,batch_id 0 43 miss% 0.35243521612838125
plot_id,batch_id 0 44 miss% 0.32264618639001175
plot_id,batch_id 0 45 miss% 0.2093830848438148
plot_id,batch_id 0 46 miss% 0.5115354299434662
plot_id,batch_id 0 47 miss% 0.3541932297715231
plot_id,batch_id 0 48 miss% 0.3837083272105507
plot_id,batch_id 0 49 miss% 0.5758193091073947
plot_id,batch_id 0 50 miss% 0.48956571005479027
plot_id,batch_id 0 51 miss% 0.4416333222004365
plot_id,batch_id 0 52 miss% 0.3933325543050276
plot_id,batch_id 0 53 miss% 0.33027916037574123
plot_id,batch_id 0 54 miss% 0.6761754768607148
plot_id,batch_id 0 55 miss% 0.3372946379176322
plot_id,batch_id 0 56 miss% 0.540918889622075
plot_id,batch_id 0 57 miss% 0.45997886276340777
plot_id,batch_id 0 58 miss% 0.4045530309191544
plot_id,batch_id 0 59 miss% 0.47873692509446364
plot_id,batch_id 0 60 miss% 0.21685311942446034
plot_id,batch_id 0 61 miss% 0.2543383972526242
plot_id,batch_id 0 62 miss% 0.4036047091784859
plot_id,batch_id 0 63 miss% 0.3790980276451196
plot_id,batch_id 0 64 miss% 0.3495504455232908
plot_id,batch_id 0 65 miss% 0.3772809757498466
plot_id,batch_id 0 66 miss% 0.35860651167285856
plot_id,batch_id 0 69 miss% 0.10885123670271499
plot_id,batch_id 0 70 miss% 0.1316649116369806
plot_id,batch_id 0 71 miss% 0.03388035928271335
plot_id,batch_id 0 72 miss% 0.08311125533415399
plot_id,batch_id 0 73 miss% 0.07878543982207431
plot_id,batch_id 0 74 miss% 0.11657513363506049
plot_id,batch_id 0 75 miss% 0.09287603979465658
plot_id,batch_id 0 76 miss% 0.13155325774391735
plot_id,batch_id 0 77 miss% 0.05737635211504981
plot_id,batch_id 0 78 miss% 0.052624935533636806
plot_id,batch_id 0 79 miss% 0.060615045788184566
plot_id,batch_id 0 80 miss% 0.11505368053748606
plot_id,batch_id 0 81 miss% 0.11035466966692395
plot_id,batch_id 0 82 miss% 0.10167977683068789
plot_id,batch_id 0 83 miss% 0.062283531951269316
plot_id,batch_id 0 84 miss% 0.1161618434114788
plot_id,batch_id 0 85 miss% 0.08261872315208718
plot_id,batch_id 0 86 miss% 0.0733502518447921
plot_id,batch_id 0 87 miss% 0.09063960574197001
plot_id,batch_id 0 88 miss% 0.06453944439625009
plot_id,batch_id 0 89 miss% 0.08953322382994577
plot_id,batch_id 0 90 miss% 0.03323572977747182
plot_id,batch_id 0 91 miss% 0.07592127094228715
plot_id,batch_id 0 92 miss% 0.052800216279063596
plot_id,batch_id 0 93 miss% 0.06533495728105583
plot_id,batch_id 0 94 miss% 0.050125070977029
plot_id,batch_id 0 95 miss% 0.07257446773617411
plot_id,batch_id 0 96 miss% 0.04449656180897539
plot_id,batch_id 0 97 miss% 0.046701434146261364
plot_id,batch_id 0 98 miss% 0.055322486479325465
plot_id,batch_id 0 99 miss% 0.06469410867527779
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04687943 0.05563674 0.09135534 0.04717278 0.04660924 0.05793913
 0.05803337 0.12246689 0.09147793 0.06792892 0.05068782 0.0723634
 0.06693644 0.06039783 0.11761449 0.02979573 0.07135656 0.04410286
 0.08718779 0.12268934 0.06982055 0.0320606  0.03635441 0.02420756
 0.0292974  0.07902403 0.05466714 0.05218605 0.05128268 0.03938495
 0.05027432 0.11316832 0.10837725 0.05738826 0.07021996 0.04196237
 0.14980391 0.07183057 0.04624821 0.04289365 0.08043599 0.044196
 0.02379213 0.01258831 0.03208081 0.05367702 0.02758487 0.02667597
 0.03515245 0.02950566 0.17076868 0.03201047 0.05108135 0.02253734
 0.05343813 0.09321023 0.0575238  0.06326216 0.0359789  0.03171382
 0.03902613 0.10398658 0.09158363 0.05807742 0.09478059 0.10330954
 0.14141148 0.06098635 0.05778801 0.10885124 0.13166491 0.03388036
 0.08311126 0.07878544 0.11657513 0.09287604 0.13155326 0.05737635
 0.05262494 0.06061505 0.11505368 0.11035467 0.10167978 0.06228353
 0.11616184 0.08261872 0.07335025 0.09063961 0.06453944 0.08953322
 0.03323573 0.07592127 0.05280022 0.06533496 0.05012507 0.07257447
 0.04449656 0.04670143 0.05532249 0.06469411]
for model  101 the mean error 0.06772583012535942
all id 101 hidden_dim 16 learning_rate 0.0025 num_layers 5 frames 25 out win 6 err 0.06772583012535942 time 13835.001140117645
Launcher: Job 102 completed in 14099 seconds.
Launcher: Task 250 done. Exiting.
plot_id,batch_id 0 67 miss% 0.2673131796527426
plot_id,batch_id 0 68 miss% 0.40702741300044576
plot_id,batch_id 0 69 miss% 0.3328416154304611
plot_id,batch_id 0 70 miss% 0.20169659314423405
plot_id,batch_id 0 71 miss% 0.29489102795835137
plot_id,batch_id 0 72 miss% 0.40171405262547927
plot_id,batch_id 0 73 miss% 0.35801729348329403
plot_id,batch_id 0 74 miss% 0.3851715912200637
plot_id,batch_id 0 75 miss% 0.1889752346518327
plot_id,batch_id 0 76 miss% 0.26483178340933283
plot_id,batch_id 0 77 miss% 0.29156178563535096
plot_id,batch_id 0 78 miss% 0.2657039109012678
plot_id,batch_id 0 79 miss% 0.3283798816187965
plot_id,batch_id 0 80 miss% 0.23358622670127366
plot_id,batch_id 0 81 miss% 0.4048523870438289
plot_id,batch_id 0 82 miss% 0.3163180301228859
plot_id,batch_id 0 83 miss% 0.3492667592883195
plot_id,batch_id 0 84 miss% 0.25663409633972833
plot_id,batch_id 0 85 miss% 0.22702044923784503
plot_id,batch_id 0 86 miss% 0.28772750427591837
plot_id,batch_id 0 87 miss% 0.399393678340022
plot_id,batch_id 0 88 miss% 0.3342004327294501
plot_id,batch_id 0 89 miss% 0.3255389685623293
plot_id,batch_id 0 90 miss% 0.21521563621121395
plot_id,batch_id 0 91 miss% 0.2651566788978641
plot_id,batch_id 0 92 miss% 0.2635103177014967
plot_id,batch_id 0 93 miss% 0.2338680966400225
plot_id,batch_id 0 94 miss% 0.48654588880891153
plot_id,batch_id 0 95 miss% 0.23106498882900856
plot_id,batch_id 0 96 miss% 0.26646543495532343
plot_id,batch_id 0 97 miss% 0.38526046243979856
plot_id,batch_id 0 98 miss% 0.38946762729550616
plot_id,batch_id 0 99 miss% 0.2817507226161277
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.30230493 0.37063504 0.35092336 0.3613575  0.33322231 0.26814756
 0.33135268 0.36891911 0.31861242 0.31573186 0.24126483 0.29628086
 0.31455259 0.30218583 0.34915181 0.33913794 0.43912526 0.45159086
 0.42798869 0.3877765  0.33910551 0.32907982 0.28346135 0.32676844
 0.26551199 0.29493163 0.34703403 0.3527373  0.4613059  0.31712445
 0.29726924 0.43240825 0.35188215 0.4734969  0.30763494 0.23457158
 0.44660054 0.32723513 0.38383942 0.42852771 0.38333794 0.30415161
 0.31240501 0.35243522 0.32264619 0.20938308 0.51153543 0.35419323
 0.38370833 0.57581931 0.48956571 0.44163332 0.39333255 0.33027916
 0.67617548 0.33729464 0.54091889 0.45997886 0.40455303 0.47873693
 0.21685312 0.2543384  0.40360471 0.37909803 0.34955045 0.37728098
 0.35860651 0.26731318 0.40702741 0.33284162 0.20169659 0.29489103
 0.40171405 0.35801729 0.38517159 0.18897523 0.26483178 0.29156179
 0.26570391 0.32837988 0.23358623 0.40485239 0.31631803 0.34926676
 0.2566341  0.22702045 0.2877275  0.39939368 0.33420043 0.32553897
 0.21521564 0.26515668 0.26351032 0.2338681  0.48654589 0.23106499
 0.26646543 0.38526046 0.38946763 0.28175072]
for model  220 the mean error 0.34613174085803566
all id 220 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 31 out win 5 err 0.34613174085803566 time 13872.18343424797
Launcher: Job 221 completed in 14108 seconds.
Launcher: Task 225 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  209681
Epoch:0, Train loss:0.619391, valid loss:0.620696
Epoch:1, Train loss:0.051269, valid loss:0.007000
Epoch:2, Train loss:0.017082, valid loss:0.005268
Epoch:3, Train loss:0.014369, valid loss:0.005478
Epoch:4, Train loss:0.011855, valid loss:0.003883
Epoch:5, Train loss:0.008263, valid loss:0.002765
Epoch:6, Train loss:0.006679, valid loss:0.002142
Epoch:7, Train loss:0.004517, valid loss:0.001645
Epoch:8, Train loss:0.003662, valid loss:0.001694
Epoch:9, Train loss:0.003233, valid loss:0.001661
Epoch:10, Train loss:0.002939, valid loss:0.001833
Epoch:11, Train loss:0.002154, valid loss:0.001130
Epoch:12, Train loss:0.001987, valid loss:0.001154
Epoch:13, Train loss:0.001827, valid loss:0.001044
Epoch:14, Train loss:0.002004, valid loss:0.001115
Epoch:15, Train loss:0.001709, valid loss:0.001116
Epoch:16, Train loss:0.001700, valid loss:0.000998
Epoch:17, Train loss:0.001631, valid loss:0.001017
Epoch:18, Train loss:0.001626, valid loss:0.001185
Epoch:19, Train loss:0.001526, valid loss:0.000955
Epoch:20, Train loss:0.001482, valid loss:0.001077
Epoch:21, Train loss:0.001079, valid loss:0.000820
Epoch:22, Train loss:0.001065, valid loss:0.000837
Epoch:23, Train loss:0.001020, valid loss:0.000903
Epoch:24, Train loss:0.000998, valid loss:0.000846
Epoch:25, Train loss:0.000974, valid loss:0.000820
Epoch:26, Train loss:0.000963, valid loss:0.000822
Epoch:27, Train loss:0.000907, valid loss:0.000936
Epoch:28, Train loss:0.000919, valid loss:0.000769
Epoch:29, Train loss:0.000919, valid loss:0.000918
Epoch:30, Train loss:0.000869, valid loss:0.000758
Epoch:31, Train loss:0.000689, valid loss:0.000803
Epoch:32, Train loss:0.000684, valid loss:0.000719
Epoch:33, Train loss:0.000666, valid loss:0.000743
Epoch:34, Train loss:0.000669, valid loss:0.000745
Epoch:35, Train loss:0.000658, valid loss:0.000738
Epoch:36, Train loss:0.000663, valid loss:0.000711
Epoch:37, Train loss:0.000633, valid loss:0.000742
Epoch:38, Train loss:0.000632, valid loss:0.000772
Epoch:39, Train loss:0.000617, valid loss:0.000705
Epoch:40, Train loss:0.000610, valid loss:0.000743
Epoch:41, Train loss:0.000532, valid loss:0.000714
Epoch:42, Train loss:0.000521, valid loss:0.000723
Epoch:43, Train loss:0.000510, valid loss:0.000710
Epoch:44, Train loss:0.000513, valid loss:0.000701
Epoch:45, Train loss:0.000513, valid loss:0.000719
Epoch:46, Train loss:0.000523, valid loss:0.000703
Epoch:47, Train loss:0.000494, valid loss:0.000714
Epoch:48, Train loss:0.000497, valid loss:0.000715
Epoch:49, Train loss:0.000500, valid loss:0.000736
Epoch:50, Train loss:0.000483, valid loss:0.000706
Epoch:51, Train loss:0.000458, valid loss:0.000696
Epoch:52, Train loss:0.000452, valid loss:0.000697
Epoch:53, Train loss:0.000450, valid loss:0.000700
Epoch:54, Train loss:0.000448, valid loss:0.000692
Epoch:55, Train loss:0.000447, valid loss:0.000693
Epoch:56, Train loss:0.000446, valid loss:0.000693
Epoch:57, Train loss:0.000445, valid loss:0.000695
Epoch:58, Train loss:0.000444, valid loss:0.000695
Epoch:59, Train loss:0.000444, valid loss:0.000691
Epoch:60, Train loss:0.000443, valid loss:0.000695
training time 13988.478967905045
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.05942104017509659
plot_id,batch_id 0 1 miss% 0.08519256782976273
plot_id,batch_id 0 2 miss% 0.08077166934435194
plot_id,batch_id 0 3 miss% 0.05790341925135174
plot_id,batch_id 0 4 miss% 0.05606985135782794
plot_id,batch_id 0 5 miss% 0.03074111582472881
plot_id,batch_id 0 6 miss% 0.03515380980264039
plot_id,batch_id 0 7 miss% 0.05151712437735911
plot_id,batch_id 0 8 miss% 0.07101430254582541
plot_id,batch_id 0 9 miss% 0.03684799446717433
plot_id,batch_id 0 10 miss% 0.03365512120896954
plot_id,batch_id 0 11 miss% 0.05343113948234787
plot_id,batch_id 0 12 miss% 0.03587536214489328
plot_id,batch_id 0 13 miss% 0.04301452993771081
plot_id,batch_id 0 14 miss% 0.031971433888661166
plot_id,batch_id 0 15 miss% 0.024857592727701595
plot_id,batch_id 0 16 miss% 0.0625760138298921
plot_id,batch_id 0 17 miss% 0.029853220675277615
plot_id,batch_id 0 18 miss% 0.05208005705912643
plot_id,batch_id 0 19 miss% 0.04589907661763718
plot_id,batch_id 0 20 miss% 0.13858740603511158
plot_id,batch_id 0 21 miss% 0.0526324799333488
plot_id,batch_id 0 22 miss% 0.04078439716306978
plot_id,batch_id 0 23 miss% 0.03627254846068371
plot_id,batch_id 0 24 miss% 0.0320197807094082
plot_id,batch_id 0 25 miss% 0.06109903145861341
plot_id,batch_id 0 26 miss% 0.04576609045583713
plot_id,batch_id 0 27 miss% 0.06924048087831475
plot_id,batch_id 0 28 miss% 0.03548986808979202
plot_id,batch_id 0 29 miss% 0.0192924379154337
plot_id,batch_id 0 30 miss% 0.057183122319126395
plot_id,batch_id 0 31 miss% 0.07312026839941506
plot_id,batch_id 0 32 miss% 0.0985155778396683
plot_id,batch_id 0 33 miss% 0.048173929614762905
plot_id,batch_id 0 34 miss% 0.04085612928565575
plot_id,batch_id 0 35 miss% 0.030182250981233758
plot_id,batch_id 0 36 miss% 0.058903618697605534
plot_id,batch_id 0 37 miss% 0.048219237085229356
plot_id,batch_id 0 38 miss% 0.0563021159032806
plot_id,batch_id 0 39 miss% 0.027287863898993087
plot_id,batch_id 0 40 miss% 0.0640077616617003
plot_id,batch_id 0 41 miss% 0.0402480946982505
plot_id,batch_id 0 42 miss% 0.021408224940353233
plot_id,batch_id 0 43 miss% 0.02925862475507321
plot_id,batch_id 0 44 miss% 0.02610278196123979
plot_id,batch_id 0 45 miss% 0.033543580262689957
plot_id,batch_id 0 46 miss% 0.03958116465999754
plot_id,batch_id 0 47 miss% 0.02671092273689313
plot_id,batch_id 0 48 miss% 0.023847321899523925
plot_id,batch_id 0 49 miss% 0.019768957004123483
plot_id,batch_id 0 50 miss% 0.14417124981795373
plot_id,batch_id 0 51 miss% 0.046818929512844096
plot_id,batch_id 0 52 miss% 0.031542927273331044
plot_id,batch_id 0 53 miss% 0.029150358980292296
plot_id,batch_id 0 54 miss% 0.02271892726933409
plot_id,batch_id 0 55 miss% 0.06651007755244559
plot_id,batch_id 0 56 miss% 0.032783558012581246
plot_id,batch_id 0 57 miss% 0.03736425821777679
plot_id,batch_id 0 58 miss% 0.025426637681068436
plot_id,batch_id 0 59 miss% 0.024535669760760645
plot_id,batch_id 0 60 miss% 0.02956464962283881
plot_id,batch_id 0 61 miss% 0.025960094804000907
plot_id,batch_id 0 62 miss% 0.20953253619825932
plot_id,batch_id 0 63 miss% 0.05169368022568982
plot_id,batch_id 0 64 miss% 0.039772046780704576
plot_id,batch_id 0 65 miss% 0.055676832465039815
plot_id,batch_id 0 66 miss% 0.03789824588670233
plot_id,batch_id 0 67 miss% 0.033982421101018984
plot_id,batch_id 0 68 miss% 0.04501376524142817
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  120401
Epoch:0, Train loss:0.567214, valid loss:0.551781
Epoch:1, Train loss:0.114892, valid loss:0.004472
Epoch:2, Train loss:0.012433, valid loss:0.004211
Epoch:3, Train loss:0.011120, valid loss:0.003715
Epoch:4, Train loss:0.009527, valid loss:0.003197
Epoch:5, Train loss:0.007485, valid loss:0.002637
Epoch:6, Train loss:0.006830, valid loss:0.002439
Epoch:7, Train loss:0.006628, valid loss:0.002264
Epoch:8, Train loss:0.006313, valid loss:0.002311
Epoch:9, Train loss:0.006090, valid loss:0.002267
Epoch:10, Train loss:0.003470, valid loss:0.001322
Epoch:11, Train loss:0.001955, valid loss:0.001122
Epoch:12, Train loss:0.001869, valid loss:0.001066
Epoch:13, Train loss:0.001772, valid loss:0.001199
Epoch:14, Train loss:0.001723, valid loss:0.000925
Epoch:15, Train loss:0.001649, valid loss:0.001008
Epoch:16, Train loss:0.001555, valid loss:0.001026
Epoch:17, Train loss:0.001512, valid loss:0.000940
Epoch:18, Train loss:0.001474, valid loss:0.001160
Epoch:19, Train loss:0.001435, valid loss:0.000931
Epoch:20, Train loss:0.001409, valid loss:0.001206
Epoch:21, Train loss:0.001146, valid loss:0.000807
Epoch:22, Train loss:0.001101, valid loss:0.000820
Epoch:23, Train loss:0.001094, valid loss:0.000814
Epoch:24, Train loss:0.001043, valid loss:0.000801
Epoch:25, Train loss:0.001025, valid loss:0.000781
Epoch:26, Train loss:0.001031, valid loss:0.000778
Epoch:27, Train loss:0.000991, valid loss:0.000752
Epoch:28, Train loss:0.001000, valid loss:0.000750
Epoch:29, Train loss:0.000980, valid loss:0.000863
Epoch:30, Train loss:0.000946, valid loss:0.000804
Epoch:31, Train loss:0.000812, valid loss:0.000748
Epoch:32, Train loss:0.000809, valid loss:0.000800
Epoch:33, Train loss:0.000795, valid loss:0.000739
Epoch:34, Train loss:0.000782, valid loss:0.000744
Epoch:35, Train loss:0.000797, valid loss:0.000807
Epoch:36, Train loss:0.000771, valid loss:0.000751
Epoch:37, Train loss:0.000767, valid loss:0.000716
Epoch:38, Train loss:0.000766, valid loss:0.000693
Epoch:39, Train loss:0.000737, valid loss:0.000741
Epoch:40, Train loss:0.000738, valid loss:0.000750
Epoch:41, Train loss:0.000681, valid loss:0.000709
Epoch:42, Train loss:0.000669, valid loss:0.000777
Epoch:43, Train loss:0.000662, valid loss:0.000732
Epoch:44, Train loss:0.000660, valid loss:0.000716
Epoch:45, Train loss:0.000654, valid loss:0.000707
Epoch:46, Train loss:0.000654, valid loss:0.000703
Epoch:47, Train loss:0.000652, valid loss:0.000687
Epoch:48, Train loss:0.000642, valid loss:0.000702
Epoch:49, Train loss:0.000643, valid loss:0.000699
Epoch:50, Train loss:0.000639, valid loss:0.000714
Epoch:51, Train loss:0.000601, valid loss:0.000682
Epoch:52, Train loss:0.000595, valid loss:0.000688
Epoch:53, Train loss:0.000591, valid loss:0.000688
Epoch:54, Train loss:0.000590, valid loss:0.000689
Epoch:55, Train loss:0.000589, valid loss:0.000693
Epoch:56, Train loss:0.000588, valid loss:0.000687
Epoch:57, Train loss:0.000588, valid loss:0.000681
Epoch:58, Train loss:0.000586, valid loss:0.000681
Epoch:59, Train loss:0.000586, valid loss:0.000686
Epoch:60, Train loss:0.000585, valid loss:0.000678
training time 13992.96946811676
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.05798037152969541
plot_id,batch_id 0 1 miss% 0.04165480383062047
plot_id,batch_id 0 2 miss% 0.08874330661648772
plot_id,batch_id 0 3 miss% 0.04845787718775201
plot_id,batch_id 0 4 miss% 0.06530811790301468
plot_id,batch_id 0 5 miss% 0.055295994460836606
plot_id,batch_id 0 6 miss% 0.04930545775665886
plot_id,batch_id 0 7 miss% 0.11540619567258288
plot_id,batch_id 0 8 miss% 0.08060720965624704
plot_id,batch_id 0 9 miss% 0.048858550535783266
plot_id,batch_id 0 10 miss% 0.02794474153006446
plot_id,batch_id 0 11 miss% 0.07200970347651156
plot_id,batch_id 0 12 miss% 0.07250494411329039
plot_id,batch_id 0 13 miss% 0.05920496396172786
plot_id,batch_id 0 14 miss% 0.08574281262210633
plot_id,batch_id 0 15 miss% 0.04996708886149793
plot_id,batch_id 0 16 miss% 0.09150535927062611
plot_id,batch_id 0 17 miss% 0.06003289490042268
plot_id,batch_id 0 18 miss% 0.05208403254001126
plot_id,batch_id 0 19 miss% 0.10750883389493254
plot_id,batch_id 0 20 miss% 0.04946362052055694
plot_id,batch_id 0 21 miss% 0.07328411223159285
plot_id,batch_id 0 22 miss% 0.052237689752158034
plot_id,batch_id 0 23 miss% 0.04163881687697038
plot_id,batch_id 0 24 miss% 0.0626364906341354
plot_id,batch_id 0 25 miss% 0.10468165480954329
plot_id,batch_id 0 26 miss% 0.05682906695944183
plot_id,batch_id 0 27 miss% 0.055599441114699874
plot_id,batch_id 0 28 miss% 0.04327366727677413
plot_id,batch_id 0 29 miss% 0.030311350730096283
plot_id,batch_id 0 30 miss% 0.034046174901008905
plot_id,batch_id 0 31 miss% 0.08284733395642642
plot_id,batch_id 0 32 miss% 0.13567527245503755
plot_id,batch_id 0 33 miss% 0.08607969546835628
plot_id,batch_id 0 34 miss% 0.07942839093359487
plot_id,batch_id 0 35 miss% 0.043642298635462276
plot_id,batch_id 0 36 miss% 0.04606319476953828
plot_id,batch_id 0 37 miss% 0.08316073726539426
plot_id,batch_id 0 38 miss% 0.06738493935312287
plot_id,batch_id 0 39 miss% 0.05947953378084081
plot_id,batch_id 0 40 miss% 0.06532694551375116
plot_id,batch_id 0 41 miss% 0.055027461873446995
plot_id,batch_id 0 42 miss% 0.02178558422700623
plot_id,batch_id 0 43 miss% 0.027433499274477947
plot_id,batch_id 0 44 miss% 0.016805222571391876
plot_id,batch_id 0 45 miss% 0.07444565463290798
plot_id,batch_id 0 46 miss% 0.0310111954777009
plot_id,batch_id 0 47 miss% 0.025611052349040604
plot_id,batch_id 0 48 miss% 0.028147364254773276
plot_id,batch_id 0 49 miss% 0.027260095037379802
plot_id,batch_id 0 50 miss% 0.14863696156555986
plot_id,batch_id 0 51 miss% 0.050964952126230915
plot_id,batch_id 0 52 miss% 0.02878990231764194
plot_id,batch_id 0 53 miss% 0.032087540323253845
plot_id,batch_id 0 54 miss% 0.03633398936039348
plot_id,batch_id 0 55 miss% 0.11803425340165262
plot_id,batch_id 0 56 miss% 0.10611285930068677
plot_id,batch_id 0 57 miss% 0.05881735338003291
plot_id,batch_id 0 58 miss% 0.03215029875823819
plot_id,batch_id 0 59 miss% 0.026974807204057812
plot_id,batch_id 0 60 miss% 0.0214636261352337
plot_id,batch_id 0 61 miss% 0.07059679207911243
plot_id,batch_id 0 62 miss% 0.0673222615297271
plot_id,batch_id 0 63 miss% 0.051596112018195625
plot_id,batch_id 0 64 miss% 0.04494971704519545
plot_id,batch_id 0 65 miss% 0.10852250818130264
plot_id,batch_id 0 66 miss% 0.05000120243377891
plot_id,batch_id 0 67 miss% 0.026921009731792508
plot_id,batch_id 0 68 miss% 0.0722212603424167
plot_id,batch_id 0 69 miss% 0.04551756562677226
plot_id,batch_id 0 70 miss% 0.0707361500945371
plot_id,batch_id 0 71 miss% 0.03850417308504929
plot_id,batch_id 0 72 miss% 0.1408765075041552
plot_id,batch_id 0 73 miss% 0.05202697360342064
plot_id,batch_id 0 74 miss% 0.06378618876582352
plot_id,batch_id 0 75 miss% 0.04852207458918913
plot_id,batch_id 0 76 miss% 0.10137482113788221
plot_id,batch_id 0 77 miss% 0.07787033335916012
plot_id,batch_id 0 78 miss% 0.034779122944340615
plot_id,batch_id 0 79 miss% 0.07857945324347117
plot_id,batch_id 0 80 miss% 0.06792697972337673
plot_id,batch_id 0 81 miss% 0.04661425288877218
plot_id,batch_id 0 82 miss% 0.05090573816694486
plot_id,batch_id 0 83 miss% 0.0350425900315616
plot_id,batch_id 0 84 miss% 0.06987887383999426
plot_id,batch_id 0 85 miss% 0.04294938962885835
plot_id,batch_id 0 86 miss% 0.05746282121813764
plot_id,batch_id 0 87 miss% 0.06960265448153426
plot_id,batch_id 0 88 miss% 0.03945276462174581
plot_id,batch_id 0 89 miss% 0.04242213592563499
plot_id,batch_id 0 90 miss% 0.02619745211699477
plot_id,batch_id 0 91 miss% 0.06357760379748036
plot_id,batch_id 0 92 miss% 0.04577311810788285
plot_id,batch_id 0 93 miss% 0.03132615299495412
plot_id,batch_id 0 94 miss% 0.10370858214265526
plot_id,batch_id 0 95 miss% 0.06545963730585952
plot_id,batch_id 0 96 miss% 0.0651031590452543
plot_id,batch_id 0 97 miss% 0.05220502600174153
plot_id,batch_id 0 98 miss% 0.030048732968231413
plot_id,batch_id 0 99 miss% 0.05349513971369673
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05942104 0.08519257 0.08077167 0.05790342 0.05606985 0.03074112
 0.03515381 0.05151712 0.0710143  0.03684799 0.03365512 0.05343114
 0.03587536 0.04301453 0.03197143 0.02485759 0.06257601 0.02985322
 0.05208006 0.04589908 0.13858741 0.05263248 0.0407844  0.03627255
 0.03201978 0.06109903 0.04576609 0.06924048 0.03548987 0.01929244
 0.05718312 0.07312027 0.09851558 0.04817393 0.04085613 0.03018225
 0.05890362 0.04821924 0.05630212 0.02728786 0.06400776 0.04024809
 0.02140822 0.02925862 0.02610278 0.03354358 0.03958116 0.02671092
 0.02384732 0.01976896 0.14417125 0.04681893 0.03154293 0.02915036
 0.02271893 0.06651008 0.03278356 0.03736426 0.02542664 0.02453567
 0.02956465 0.02596009 0.20953254 0.05169368 0.03977205 0.05567683
 0.03789825 0.03398242 0.04501377 0.04551757 0.07073615 0.03850417
 0.14087651 0.05202697 0.06378619 0.04852207 0.10137482 0.07787033
 0.03477912 0.07857945 0.06792698 0.04661425 0.05090574 0.03504259
 0.06987887 0.04294939 0.05746282 0.06960265 0.03945276 0.04242214
 0.02619745 0.0635776  0.04577312 0.03132615 0.10370858 0.06545964
 0.06510316 0.05220503 0.03004873 0.05349514]
for model  43 the mean error 0.05174095547331949
all id 43 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 21 out win 5 err 0.05174095547331949 time 13988.478967905045
Launcher: Job 44 completed in 14251 seconds.
Launcher: Task 236 done. Exiting.
plot_id,batch_id 0 69 miss% 0.08948912818787436
plot_id,batch_id 0 70 miss% 0.07520302610860752
plot_id,batch_id 0 71 miss% 0.022243198456445127
plot_id,batch_id 0 72 miss% 0.09973220252979317
plot_id,batch_id 0 73 miss% 0.07273609252782197
plot_id,batch_id 0 74 miss% 0.10503144326231065
plot_id,batch_id 0 75 miss% 0.14762146270977167
plot_id,batch_id 0 76 miss% 0.10398335994205078
plot_id,batch_id 0 77 miss% 0.055781910026208324
plot_id,batch_id 0 78 miss% 0.03391790558494738
plot_id,batch_id 0 79 miss% 0.09429688226891456
plot_id,batch_id 0 80 miss% 0.07267861452704784
plot_id,batch_id 0 81 miss% 0.07510826984154276
plot_id,batch_id 0 82 miss% 0.053607722498224056
plot_id,batch_id 0 83 miss% 0.08313188685151385
plot_id,batch_id 0 84 miss% 0.07054582508814161
plot_id,batch_id 0 85 miss% 0.03765249768118898
plot_id,batch_id 0 86 miss% 0.05433028520439546
plot_id,batch_id 0 87 miss% 0.07790688424483715
plot_id,batch_id 0 88 miss% 0.10304772881196282
plot_id,batch_id 0 89 miss% 0.05936078842751529
plot_id,batch_id 0 90 miss% 0.022587743145064024
plot_id,batch_id 0 91 miss% 0.05039750613638791
plot_id,batch_id 0 92 miss% 0.09804361824221519
plot_id,batch_id 0 93 miss% 0.032207031146041426
plot_id,batch_id 0 94 miss% 0.06817022679940649
plot_id,batch_id 0 95 miss% 0.054286490144257356
plot_id,batch_id 0 96 miss% 0.03525265642165587
plot_id,batch_id 0 97 miss% 0.0589903719472462
plot_id,batch_id 0 98 miss% 0.058199571654817546
plot_id,batch_id 0 99 miss% 0.07793728700480783
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05798037 0.0416548  0.08874331 0.04845788 0.06530812 0.05529599
 0.04930546 0.1154062  0.08060721 0.04885855 0.02794474 0.0720097
 0.07250494 0.05920496 0.08574281 0.04996709 0.09150536 0.06003289
 0.05208403 0.10750883 0.04946362 0.07328411 0.05223769 0.04163882
 0.06263649 0.10468165 0.05682907 0.05559944 0.04327367 0.03031135
 0.03404617 0.08284733 0.13567527 0.0860797  0.07942839 0.0436423
 0.04606319 0.08316074 0.06738494 0.05947953 0.06532695 0.05502746
 0.02178558 0.0274335  0.01680522 0.07444565 0.0310112  0.02561105
 0.02814736 0.0272601  0.14863696 0.05096495 0.0287899  0.03208754
 0.03633399 0.11803425 0.10611286 0.05881735 0.0321503  0.02697481
 0.02146363 0.07059679 0.06732226 0.05159611 0.04494972 0.10852251
 0.0500012  0.02692101 0.07222126 0.08948913 0.07520303 0.0222432
 0.0997322  0.07273609 0.10503144 0.14762146 0.10398336 0.05578191
 0.03391791 0.09429688 0.07267861 0.07510827 0.05360772 0.08313189
 0.07054583 0.0376525  0.05433029 0.07790688 0.10304773 0.05936079
 0.02258774 0.05039751 0.09804362 0.03220703 0.06817023 0.05428649
 0.03525266 0.05899037 0.05819957 0.07793729]
for model  95 the mean error 0.06286717842615017
all id 95 hidden_dim 24 learning_rate 0.0025 num_layers 4 frames 25 out win 6 err 0.06286717842615017 time 13992.96946811676
Launcher: Job 96 completed in 14253 seconds.
Launcher: Task 216 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  55697
Epoch:0, Train loss:0.469126, valid loss:0.435657
Epoch:1, Train loss:0.054147, valid loss:0.003106
Epoch:2, Train loss:0.008639, valid loss:0.002202
Epoch:3, Train loss:0.004799, valid loss:0.001445
Epoch:4, Train loss:0.002966, valid loss:0.000954
Epoch:5, Train loss:0.002099, valid loss:0.001124
Epoch:6, Train loss:0.001811, valid loss:0.000790
Epoch:7, Train loss:0.001685, valid loss:0.000826
Epoch:8, Train loss:0.001602, valid loss:0.000844
Epoch:9, Train loss:0.001477, valid loss:0.000732
Epoch:10, Train loss:0.001419, valid loss:0.000706
Epoch:11, Train loss:0.001109, valid loss:0.000589
Epoch:12, Train loss:0.001089, valid loss:0.000555
Epoch:13, Train loss:0.001041, valid loss:0.000585
Epoch:14, Train loss:0.001018, valid loss:0.000723
Epoch:15, Train loss:0.000996, valid loss:0.000633
Epoch:16, Train loss:0.000970, valid loss:0.000636
Epoch:17, Train loss:0.000965, valid loss:0.000609
Epoch:18, Train loss:0.000947, valid loss:0.000575
Epoch:19, Train loss:0.000921, valid loss:0.000537
Epoch:20, Train loss:0.000893, valid loss:0.000570
Epoch:21, Train loss:0.000738, valid loss:0.000494
Epoch:22, Train loss:0.000716, valid loss:0.000457
Epoch:23, Train loss:0.000722, valid loss:0.000493
Epoch:24, Train loss:0.000715, valid loss:0.000457
Epoch:25, Train loss:0.000721, valid loss:0.000496
Epoch:26, Train loss:0.000687, valid loss:0.000494
Epoch:27, Train loss:0.000684, valid loss:0.000541
Epoch:28, Train loss:0.000687, valid loss:0.000466
Epoch:29, Train loss:0.000685, valid loss:0.000472
Epoch:30, Train loss:0.000670, valid loss:0.000511
Epoch:31, Train loss:0.000577, valid loss:0.000479
Epoch:32, Train loss:0.000579, valid loss:0.000425
Epoch:33, Train loss:0.000567, valid loss:0.000429
Epoch:34, Train loss:0.000564, valid loss:0.000517
Epoch:35, Train loss:0.000565, valid loss:0.000416
Epoch:36, Train loss:0.000556, valid loss:0.000488
Epoch:37, Train loss:0.000559, valid loss:0.000449
Epoch:38, Train loss:0.000547, valid loss:0.000441
Epoch:39, Train loss:0.000548, valid loss:0.000438
Epoch:40, Train loss:0.000540, valid loss:0.000425
Epoch:41, Train loss:0.000503, valid loss:0.000429
Epoch:42, Train loss:0.000501, valid loss:0.000425
Epoch:43, Train loss:0.000497, valid loss:0.000448
Epoch:44, Train loss:0.000499, valid loss:0.000415
Epoch:45, Train loss:0.000494, valid loss:0.000459
Epoch:46, Train loss:0.000500, valid loss:0.000410
Epoch:47, Train loss:0.000490, valid loss:0.000421
Epoch:48, Train loss:0.000488, valid loss:0.000417
Epoch:49, Train loss:0.000492, valid loss:0.000402
Epoch:50, Train loss:0.000488, valid loss:0.000404
Epoch:51, Train loss:0.000462, valid loss:0.000402
Epoch:52, Train loss:0.000460, valid loss:0.000403
Epoch:53, Train loss:0.000459, valid loss:0.000405
Epoch:54, Train loss:0.000458, valid loss:0.000400
Epoch:55, Train loss:0.000457, valid loss:0.000401
Epoch:56, Train loss:0.000457, valid loss:0.000401
Epoch:57, Train loss:0.000456, valid loss:0.000401
Epoch:58, Train loss:0.000456, valid loss:0.000400
Epoch:59, Train loss:0.000456, valid loss:0.000402
Epoch:60, Train loss:0.000456, valid loss:0.000399
training time 14042.967223644257
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.0777061145534209
plot_id,batch_id 0 1 miss% 0.0597654288957273
plot_id,batch_id 0 2 miss% 0.09721544682255867
plot_id,batch_id 0 3 miss% 0.054648264599813964
plot_id,batch_id 0 4 miss% 0.07078821543125899
plot_id,batch_id 0 5 miss% 0.09739340143517947
plot_id,batch_id 0 6 miss% 0.04247384459348834
plot_id,batch_id 0 7 miss% 0.08635426892095463
plot_id,batch_id 0 8 miss% 0.09370061531936737
plot_id,batch_id 0 9 miss% 0.05686002321708486
plot_id,batch_id 0 10 miss% 0.03714603824504067
plot_id,batch_id 0 11 miss% 0.04905175890959169
plot_id,batch_id 0 12 miss% 0.0904022180472815
plot_id,batch_id 0 13 miss% 0.029215527428293583
plot_id,batch_id 0 14 miss% 0.10697244074556168
plot_id,batch_id 0 15 miss% 0.023868550717666442
plot_id,batch_id 0 16 miss% 0.09631282226948253
plot_id,batch_id 0 17 miss% 0.05309310137014052
plot_id,batch_id 0 18 miss% 0.07935306035253828
plot_id,batch_id 0 19 miss% 0.07134856147622849
plot_id,batch_id 0 20 miss% 0.1484567179297397
plot_id,batch_id 0 21 miss% 0.043520729812114634
plot_id,batch_id 0 22 miss% 0.06321562409670048
plot_id,batch_id 0 23 miss% 0.03901166966420702
plot_id,batch_id 0 24 miss% 0.10379876207214489
plot_id,batch_id 0 25 miss% 0.18292531394134387
plot_id,batch_id 0 26 miss% 0.05832784529515432
plot_id,batch_id 0 27 miss% 0.07549227168476143
plot_id,batch_id 0 28 miss% 0.03349091658432148
plot_id,batch_id 0 29 miss% 0.04347787352072009
plot_id,batch_id 0 30 miss% 0.030388600061083335
plot_id,batch_id 0 31 miss% 0.11496839133541652
plot_id,batch_id 0 32 miss% 0.11303034335623022
plot_id,batch_id 0 33 miss% 0.07755688610211027
plot_id,batch_id 0 34 miss% 0.06773533760149233
plot_id,batch_id 0 35 miss% 0.042955962625732286
plot_id,batch_id 0 36 miss% 0.10457796059090244
plot_id,batch_id 0 37 miss% 0.07898857733919445
plot_id,batch_id 0 38 miss% 0.04860826157371973
plot_id,batch_id 0 39 miss% 0.08209971812273735
plot_id,batch_id 0 40 miss% 0.2151368825353728
plot_id,batch_id 0 41 miss% 0.08537185354720886
plot_id,batch_id 0 42 miss% 0.04395496514335547
plot_id,batch_id 0 43 miss% 0.12708144969727603
plot_id,batch_id 0 44 miss% 0.07004233263388268
plot_id,batch_id 0 45 miss% 0.05642481714618678
plot_id,batch_id 0 46 miss% 0.06621384622980722
plot_id,batch_id 0 47 miss% 0.06224463386064509
plot_id,batch_id 0 48 miss% 0.04386418089115238
plot_id,batch_id 0 49 miss% 0.04163004055620509
plot_id,batch_id 0 50 miss% 0.14507784444018523
plot_id,batch_id 0 51 miss% 0.05432641670867483
plot_id,batch_id 0 52 miss% 0.08337470942919219
plot_id,batch_id 0 53 miss% 0.06054957489604469
plot_id,batch_id 0 54 miss% 0.04514550375245793
plot_id,batch_id 0 55 miss% 0.09166676590576259
plot_id,batch_id 0 56 miss% 0.10952953681414712
plot_id,batch_id 0 57 miss% 0.06408992015062755
plot_id,batch_id 0 58 miss% 0.051635870682434484
plot_id,batch_id 0 59 miss% 0.04379032819000645
plot_id,batch_id 0 60 miss% 0.04091032475663252
plot_id,batch_id 0 61 miss% 0.029665414167454513
plot_id,batch_id 0 62 miss% 0.0333648274969157
plot_id,batch_id 0 63 miss% 0.05026247430558766
plot_id,batch_id 0 64 miss% 0.05494389815000656
plot_id,batch_id 0 65 miss% 0.12801405237397231
plot_id,batch_id 0 66 miss% 0.03859298282879332
plot_id,batch_id 0 67 miss% 0.017118069596386426
plot_id,batch_id 0 68 miss% 0.05530031661900365
plot_id,batch_id 0 69 miss% 0.11568812278049226
plot_id,batch_id 0 70 miss% 0.05949645643810291
plot_id,batch_id 0 71 miss% 0.02503671475943036
plot_id,batch_id 0 72 miss% 0.11357889630374209
plot_id,batch_id 0 73 miss% 0.0494527492390579
plot_id,batch_id 0 74 miss% 0.08435506739268238
plot_id,batch_id 0 75 miss% 0.02478603983542415
plot_id,batch_id 0 76 miss% 0.06008602297206757
plot_id,batch_id 0 77 miss% 0.06234646729696523
plot_id,batch_id 0 78 miss% 0.018027692621206758
plot_id,batch_id 0 79 miss% 0.06257103269492477
plot_id,batch_id 0 80 miss% 0.10051359798686292
plot_id,batch_id 0 81 miss% 0.11943272929475612
plot_id,batch_id 0 82 miss% 0.06310818643343918
plot_id,batch_id 0 83 miss% 0.13980029324553325
plot_id,batch_id 0 84 miss% 0.04000619355144359
plot_id,batch_id 0 85 miss% 0.028221857189518713
plot_id,batch_id 0 86 miss% 0.07937308002596517
plot_id,batch_id 0 87 miss% 0.058683922730681376
plot_id,batch_id 0 88 miss% 0.09471373909825914
plot_id,batch_id 0 89 miss% 0.06142754467795064
plot_id,batch_id 0 90 miss% 0.04317842268552926
plot_id,batch_id 0 91 miss% 0.04901235229148017
plot_id,batch_id 0 92 miss% 0.04164967196008599
plot_id,batch_id 0 93 miss% 0.061833443760519476
plot_id,batch_id 0 94 miss% 0.14077933763919823
plot_id,batch_id 0 95 miss% 0.05386239580812493
plot_id,batch_id 0 96 miss% 0.04768591931715804
plot_id,batch_id 0 97 miss% 0.06707219490965355
plot_id,batch_id 0 98 miss% 0.033538112777446706
plot_id,batch_id 0 99 miss% 0.06413298959511173
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07770611 0.05976543 0.09721545 0.05464826 0.07078822 0.0973934
 0.04247384 0.08635427 0.09370062 0.05686002 0.03714604 0.04905176
 0.09040222 0.02921553 0.10697244 0.02386855 0.09631282 0.0530931
 0.07935306 0.07134856 0.14845672 0.04352073 0.06321562 0.03901167
 0.10379876 0.18292531 0.05832785 0.07549227 0.03349092 0.04347787
 0.0303886  0.11496839 0.11303034 0.07755689 0.06773534 0.04295596
 0.10457796 0.07898858 0.04860826 0.08209972 0.21513688 0.08537185
 0.04395497 0.12708145 0.07004233 0.05642482 0.06621385 0.06224463
 0.04386418 0.04163004 0.14507784 0.05432642 0.08337471 0.06054957
 0.0451455  0.09166677 0.10952954 0.06408992 0.05163587 0.04379033
 0.04091032 0.02966541 0.03336483 0.05026247 0.0549439  0.12801405
 0.03859298 0.01711807 0.05530032 0.11568812 0.05949646 0.02503671
 0.1135789  0.04945275 0.08435507 0.02478604 0.06008602 0.06234647
 0.01802769 0.06257103 0.1005136  0.11943273 0.06310819 0.13980029
 0.04000619 0.02822186 0.07937308 0.05868392 0.09471374 0.06142754
 0.04317842 0.04901235 0.04164967 0.06183344 0.14077934 0.0538624
 0.04768592 0.06707219 0.03353811 0.06413299]
for model  198 the mean error 0.06999072547478699
all id 198 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 31 out win 4 err 0.06999072547478699 time 14042.967223644257
Launcher: Job 199 completed in 14308 seconds.
Launcher: Task 9 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  89105
Epoch:0, Train loss:0.396860, valid loss:0.357440
Epoch:1, Train loss:0.044442, valid loss:0.003697
Epoch:2, Train loss:0.011717, valid loss:0.003559
Epoch:3, Train loss:0.011233, valid loss:0.003426
Epoch:4, Train loss:0.010870, valid loss:0.003104
Epoch:5, Train loss:0.010593, valid loss:0.002930
Epoch:6, Train loss:0.007089, valid loss:0.001559
Epoch:7, Train loss:0.001956, valid loss:0.001001
Epoch:8, Train loss:0.001677, valid loss:0.000802
Epoch:9, Train loss:0.001563, valid loss:0.000857
Epoch:10, Train loss:0.001472, valid loss:0.000823
Epoch:11, Train loss:0.001145, valid loss:0.000603
Epoch:12, Train loss:0.001106, valid loss:0.000601
Epoch:13, Train loss:0.001083, valid loss:0.000604
Epoch:14, Train loss:0.001021, valid loss:0.000586
Epoch:15, Train loss:0.001017, valid loss:0.000641
Epoch:16, Train loss:0.000984, valid loss:0.000572
Epoch:17, Train loss:0.000953, valid loss:0.000563
Epoch:18, Train loss:0.000943, valid loss:0.000712
Epoch:19, Train loss:0.000909, valid loss:0.000552
Epoch:20, Train loss:0.000910, valid loss:0.000535
Epoch:21, Train loss:0.000737, valid loss:0.000488
Epoch:22, Train loss:0.000716, valid loss:0.000510
Epoch:23, Train loss:0.000704, valid loss:0.000495
Epoch:24, Train loss:0.000715, valid loss:0.000472
Epoch:25, Train loss:0.000688, valid loss:0.000521
Epoch:26, Train loss:0.000683, valid loss:0.000484
Epoch:27, Train loss:0.000677, valid loss:0.000482
Epoch:28, Train loss:0.000670, valid loss:0.000460
Epoch:29, Train loss:0.000668, valid loss:0.000494
Epoch:30, Train loss:0.000665, valid loss:0.000524
Epoch:31, Train loss:0.000564, valid loss:0.000458
Epoch:32, Train loss:0.000562, valid loss:0.000459
Epoch:33, Train loss:0.000562, valid loss:0.000444
Epoch:34, Train loss:0.000552, valid loss:0.000455
Epoch:35, Train loss:0.000556, valid loss:0.000434
Epoch:36, Train loss:0.000541, valid loss:0.000448
Epoch:37, Train loss:0.000552, valid loss:0.000453
Epoch:38, Train loss:0.000540, valid loss:0.000474
Epoch:39, Train loss:0.000532, valid loss:0.000494
Epoch:40, Train loss:0.000538, valid loss:0.000445
Epoch:41, Train loss:0.000489, valid loss:0.000440
Epoch:42, Train loss:0.000487, valid loss:0.000453
Epoch:43, Train loss:0.000484, valid loss:0.000448
Epoch:44, Train loss:0.000486, valid loss:0.000449
Epoch:45, Train loss:0.000485, valid loss:0.000430
Epoch:46, Train loss:0.000477, valid loss:0.000446
Epoch:47, Train loss:0.000477, valid loss:0.000430
Epoch:48, Train loss:0.000473, valid loss:0.000447
Epoch:49, Train loss:0.000470, valid loss:0.000443
Epoch:50, Train loss:0.000472, valid loss:0.000447
Epoch:51, Train loss:0.000452, valid loss:0.000430
Epoch:52, Train loss:0.000446, valid loss:0.000436
Epoch:53, Train loss:0.000445, valid loss:0.000433
Epoch:54, Train loss:0.000445, valid loss:0.000430
Epoch:55, Train loss:0.000444, valid loss:0.000430
Epoch:56, Train loss:0.000443, valid loss:0.000425
Epoch:57, Train loss:0.000442, valid loss:0.000427
Epoch:58, Train loss:0.000442, valid loss:0.000435
Epoch:59, Train loss:0.000443, valid loss:0.000432
Epoch:60, Train loss:0.000442, valid loss:0.000434
training time 14204.944429397583
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.2855382580546678
plot_id,batch_id 0 1 miss% 0.3411746488822724
plot_id,batch_id 0 2 miss% 0.4184598618794342
plot_id,batch_id 0 3 miss% 0.315928965839412
plot_id,batch_id 0 4 miss% 0.38976623273391847
plot_id,batch_id 0 5 miss% 0.2954091469290265
plot_id,batch_id 0 6 miss% 0.3055149815916134
plot_id,batch_id 0 7 miss% 0.48215080800667076
plot_id,batch_id 0 8 miss% 0.6393611124836738
plot_id,batch_id 0 9 miss% 0.4260335256122538
plot_id,batch_id 0 10 miss% 0.2234600183260433
plot_id,batch_id 0 11 miss% 0.31497182346053626
plot_id,batch_id 0 12 miss% 0.3648949876617637
plot_id,batch_id 0 13 miss% 0.3208550459013299
plot_id,batch_id 0 14 miss% 0.44924038261611
plot_id,batch_id 0 15 miss% 0.2533154782958293
plot_id,batch_id 0 16 miss% 0.4526816442278345
plot_id,batch_id 0 17 miss% 0.43427413481702465
plot_id,batch_id 0 18 miss% 0.46750186172290326
plot_id,batch_id 0 19 miss% 0.4298189136502787
plot_id,batch_id 0 20 miss% 0.2939322345506492
plot_id,batch_id 0 21 miss% 0.40034407809740985
plot_id,batch_id 0 22 miss% 0.4186651073334551
plot_id,batch_id 0 23 miss% 0.44075597756060586
plot_id,batch_id 0 24 miss% 0.35296100724838825
plot_id,batch_id 0 25 miss% 0.2853745997888757
plot_id,batch_id 0 26 miss% 0.3980250317005572
plot_id,batch_id 0 27 miss% 0.33919610283179424
plot_id,batch_id 0 28 miss% 0.39398476701766577
plot_id,batch_id 0 29 miss% 0.43521046360006793
plot_id,batch_id 0 30 miss% 0.27520668818477734
plot_id,batch_id 0 31 miss% 0.4717197470507923
plot_id,batch_id 0 32 miss% 0.4329573921193466
plot_id,batch_id 0 33 miss% 0.4449627679078242
plot_id,batch_id 0 34 miss% 0.3721247303035404
plot_id,batch_id 0 35 miss% 0.23596243715193252
plot_id,batch_id 0 36 miss% 0.4859421649025091
plot_id,batch_id 0 37 miss% 0.35521352699220654
plot_id,batch_id 0 38 miss% 0.4581805761686614
plot_id,batch_id 0 39 miss% 0.38732736120843536
plot_id,batch_id 0 40 miss% 0.3724309931738474
plot_id,batch_id 0 41 miss% 0.3896192263818997
plot_id,batch_id 0 42 miss% 0.31162285824187436
plot_id,batch_id 0 43 miss% 0.34578270909858255
plot_id,batch_id 0 44 miss% 0.2667365056158822
plot_id,batch_id 0 45 miss% 0.3534408518535684
plot_id,batch_id 0 46 miss% 0.3529881219203064
plot_id,batch_id 0 47 miss% 0.4437252413734768
plot_id,batch_id 0 48 miss% 0.42510173167605914
plot_id,batch_id 0 49 miss% 0.36861594226734523
plot_id,batch_id 0 50 miss% 0.5668712220482744
plot_id,batch_id 0 51 miss% 0.41925831711554273
plot_id,batch_id 0 52 miss% 0.4384905767618406
plot_id,batch_id 0 53 miss% 0.35608648149058053
plot_id,batch_id 0 54 miss% 0.28754048370030916
plot_id,batch_id 0 55 miss% 0.40730698729895753
plot_id,batch_id 0 56 miss% 0.5151571928339151
plot_id,batch_id 0 57 miss% 0.4151364308998445
plot_id,batch_id 0 58 miss% 0.3912424680031955
plot_id,batch_id 0 59 miss% 0.47327268651629023
plot_id,batch_id 0 60 miss% 0.18503246242510787
plot_id,batch_id 0 61 miss% 0.25226967419193325
plot_id,batch_id 0 62 miss% 0.3394944342854095
plot_id,batch_id 0 63 miss% 0.30542915222645484
plot_id,batch_id 0 64 miss% 0.3381007178658228
plot_id,batch_id 0 65 miss% 0.22465671784660593
plot_id,batch_id 0 66 miss% 0.30884391601773875
plot_id,batch_id 0 67 miss% 0.24553634414861575
plot_id,batch_id 0 68 miss% 0.3918641527242379
plot_id,batch_id 0 69 miss% 0.43008263749361647
plot_id,batch_id 0 70 miss% 0.2196780397089141
plot_id,batch_id 0 71 miss% 0.3146492255366853
plot_id,batch_id 0 72 miss% 0.428216829195424
plot_id,batch_id 0 73 miss% 0.2683427599439829
plot_id,batch_id 0 74 miss% 0.335747311067114
plot_id,batch_id 0 75 miss% 0.19808327882615523
plot_id,batch_id 0 76 miss% 0.2552326853363746
plot_id,batch_id 0 77 miss% 0.25561932136450993
plot_id,batch_id 0 78 miss% 0.2550810316059935
plot_id,batch_id 0 79 miss% 0.3161628883903965
plot_id,batch_id 0 80 miss% 0.2142605653503697
plot_id,batch_id 0 81 miss% 0.4235223862637526
plot_id,batch_id 0 82 miss% 0.2938813530771458
plot_id,batch_id 0 83 miss% 0.3635858019241133
plot_id,batch_id 0 84 miss% 0.2864183337887153
plot_id,batch_id 0 85 miss% 0.19637825283381913
plot_id,batch_id 0 86 miss% 0.3387055998387662
plot_id,batch_id 0 87 miss% 0.32958946722401056
plot_id,batch_id 0 88 miss% 0.3950944871693174
plot_id,batch_id 0 89 miss% 0.34920984699570057
plot_id,batch_id 0 90 miss% 0.18180362946540252
plot_id,batch_id 0 91 miss% 0.23959883529336695
plot_id,batch_id 0 92 miss% 0.2695367053461097
plot_id,batch_id 0 93 miss% 0.2877532156103522
plot_id,batch_id 0 94 miss% 0.3610833442379138
plot_id,batch_id 0 95 miss% 0.2235103861743508
plot_id,batch_id 0 96 miss% 0.22548495227510992
plot_id,batch_id 0 97 miss% 0.3769901590065837
plot_id,batch_id 0 98 miss% 0.3280604298336911
plot_id,batch_id 0 99 miss% 0.3614108889465042
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.28553826 0.34117465 0.41845986 0.31592897 0.38976623 0.29540915
 0.30551498 0.48215081 0.63936111 0.42603353 0.22346002 0.31497182
 0.36489499 0.32085505 0.44924038 0.25331548 0.45268164 0.43427413
 0.46750186 0.42981891 0.29393223 0.40034408 0.41866511 0.44075598
 0.35296101 0.2853746  0.39802503 0.3391961  0.39398477 0.43521046
 0.27520669 0.47171975 0.43295739 0.44496277 0.37212473 0.23596244
 0.48594216 0.35521353 0.45818058 0.38732736 0.37243099 0.38961923
 0.31162286 0.34578271 0.26673651 0.35344085 0.35298812 0.44372524
 0.42510173 0.36861594 0.56687122 0.41925832 0.43849058 0.35608648
 0.28754048 0.40730699 0.51515719 0.41513643 0.39124247 0.47327269
 0.18503246 0.25226967 0.33949443 0.30542915 0.33810072 0.22465672
 0.30884392 0.24553634 0.39186415 0.43008264 0.21967804 0.31464923
 0.42821683 0.26834276 0.33574731 0.19808328 0.25523269 0.25561932
 0.25508103 0.31616289 0.21426057 0.42352239 0.29388135 0.3635858
 0.28641833 0.19637825 0.3387056  0.32958947 0.39509449 0.34920985
 0.18180363 0.23959884 0.26953671 0.28775322 0.36108334 0.22351039
 0.22548495 0.37699016 0.32806043 0.36141089]
for model  166 the mean error 0.3510282784353987
all id 166 hidden_dim 24 learning_rate 0.0025 num_layers 3 frames 31 out win 5 err 0.3510282784353987 time 14204.944429397583
Launcher: Job 167 completed in 14444 seconds.
Launcher: Task 155 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  120401
Epoch:0, Train loss:0.617554, valid loss:0.615952
Epoch:1, Train loss:0.090635, valid loss:0.004232
Epoch:2, Train loss:0.011462, valid loss:0.003722
Epoch:3, Train loss:0.008889, valid loss:0.003000
Epoch:4, Train loss:0.006108, valid loss:0.001974
Epoch:5, Train loss:0.003409, valid loss:0.001449
Epoch:6, Train loss:0.002736, valid loss:0.001493
Epoch:7, Train loss:0.002449, valid loss:0.001142
Epoch:8, Train loss:0.002242, valid loss:0.001105
Epoch:9, Train loss:0.002088, valid loss:0.001041
Epoch:10, Train loss:0.001928, valid loss:0.000901
Epoch:11, Train loss:0.001416, valid loss:0.000733
Epoch:12, Train loss:0.001396, valid loss:0.000715
Epoch:13, Train loss:0.001337, valid loss:0.000735
Epoch:14, Train loss:0.001269, valid loss:0.000742
Epoch:15, Train loss:0.001241, valid loss:0.000756
Epoch:16, Train loss:0.001225, valid loss:0.000710
Epoch:17, Train loss:0.001182, valid loss:0.000834
Epoch:18, Train loss:0.001198, valid loss:0.000706
Epoch:19, Train loss:0.001104, valid loss:0.000691
Epoch:20, Train loss:0.001091, valid loss:0.000671
Epoch:21, Train loss:0.000823, valid loss:0.000613
Epoch:22, Train loss:0.000797, valid loss:0.000570
Epoch:23, Train loss:0.000790, valid loss:0.000589
Epoch:24, Train loss:0.000803, valid loss:0.000598
Epoch:25, Train loss:0.000765, valid loss:0.000577
Epoch:26, Train loss:0.000760, valid loss:0.000586
Epoch:27, Train loss:0.000744, valid loss:0.000558
Epoch:28, Train loss:0.000733, valid loss:0.000551
Epoch:29, Train loss:0.000749, valid loss:0.000523
Epoch:30, Train loss:0.000712, valid loss:0.000550
Epoch:31, Train loss:0.000586, valid loss:0.000494
Epoch:32, Train loss:0.000596, valid loss:0.000505
Epoch:33, Train loss:0.000579, valid loss:0.000492
Epoch:34, Train loss:0.000575, valid loss:0.000529
Epoch:35, Train loss:0.000565, valid loss:0.000485
Epoch:36, Train loss:0.000565, valid loss:0.000473
Epoch:37, Train loss:0.000561, valid loss:0.000525
Epoch:38, Train loss:0.000570, valid loss:0.000521
Epoch:39, Train loss:0.000547, valid loss:0.000475
Epoch:40, Train loss:0.000535, valid loss:0.000539
Epoch:41, Train loss:0.000491, valid loss:0.000489
Epoch:42, Train loss:0.000489, valid loss:0.000494
Epoch:43, Train loss:0.000480, valid loss:0.000519
Epoch:44, Train loss:0.000478, valid loss:0.000496
Epoch:45, Train loss:0.000475, valid loss:0.000483
Epoch:46, Train loss:0.000472, valid loss:0.000472
Epoch:47, Train loss:0.000475, valid loss:0.000526
Epoch:48, Train loss:0.000471, valid loss:0.000478
Epoch:49, Train loss:0.000470, valid loss:0.000478
Epoch:50, Train loss:0.000463, valid loss:0.000469
Epoch:51, Train loss:0.000437, valid loss:0.000473
Epoch:52, Train loss:0.000433, valid loss:0.000479
Epoch:53, Train loss:0.000431, valid loss:0.000468
Epoch:54, Train loss:0.000430, valid loss:0.000463
Epoch:55, Train loss:0.000430, valid loss:0.000462
Epoch:56, Train loss:0.000429, valid loss:0.000468
Epoch:57, Train loss:0.000428, valid loss:0.000465
Epoch:58, Train loss:0.000428, valid loss:0.000467
Epoch:59, Train loss:0.000427, valid loss:0.000466
Epoch:60, Train loss:0.000427, valid loss:0.000479
training time 14260.311553478241
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.09349941516315384
plot_id,batch_id 0 1 miss% 0.039070443152546264
plot_id,batch_id 0 2 miss% 0.11130431535638619
plot_id,batch_id 0 3 miss% 0.0447777968422866
plot_id,batch_id 0 4 miss% 0.06848601073509127
plot_id,batch_id 0 5 miss% 0.0382795710405019
plot_id,batch_id 0 6 miss% 0.04334584187822149
plot_id,batch_id 0 7 miss% 0.12719169087508453
plot_id,batch_id 0 8 miss% 0.08064854018965435
plot_id,batch_id 0 9 miss% 0.03793578738053175
plot_id,batch_id 0 10 miss% 0.025155679612410796
plot_id,batch_id 0 11 miss% 0.09794548307966988
plot_id,batch_id 0 12 miss% 0.047561116804253144
plot_id,batch_id 0 13 miss% 0.06413064365495813
plot_id,batch_id 0 14 miss% 0.0970292537522884
plot_id,batch_id 0 15 miss% 0.038902336213126824
plot_id,batch_id 0 16 miss% 0.07998112678276494
plot_id,batch_id 0 17 miss% 0.0608847285253179
plot_id,batch_id 0 18 miss% 0.07197056735438725
plot_id,batch_id 0 19 miss% 0.0655300860789077
plot_id,batch_id 0 20 miss% 0.058603631693939326
plot_id,batch_id 0 21 miss% 0.030354152534749487
plot_id,batch_id 0 22 miss% 0.07182454674438932
plot_id,batch_id 0 23 miss% 0.03285366653201946
plot_id,batch_id 0 24 miss% 0.029601194950633784
plot_id,batch_id 0 25 miss% 0.06909161968392845
plot_id,batch_id 0 26 miss% 0.06567588960059365
plot_id,batch_id 0 27 miss% 0.04559495652787834
plot_id,batch_id 0 28 miss% 0.024989238623076372
plot_id,batch_id 0 29 miss% 0.03699875092077514
plot_id,batch_id 0 30 miss% 0.04198268670139673
plot_id,batch_id 0 31 miss% 0.14024491472875023
plot_id,batch_id 0 32 miss% 0.10793419565723687
plot_id,batch_id 0 33 miss% 0.059895206272703894
plot_id,batch_id 0 34 miss% 0.04565552083132243
plot_id,batch_id 0 35 miss% 0.08775331150910828
plot_id,batch_id 0 36 miss% 0.09536071320227218
plot_id,batch_id 0 37 miss% 0.07689374992933964
plot_id,batch_id 0 38 miss% 0.04033621026342282
plot_id,batch_id 0 39 miss% 0.05709163745316133
plot_id,batch_id 0 40 miss% 0.15329290618383695
plot_id,batch_id 0 41 miss% 0.05407665404160267
plot_id,batch_id 0 42 miss% 0.01982945423642685
plot_id,batch_id 0 43 miss% 0.07545816023563219
plot_id,batch_id 0 44 miss% 0.03157426063182551
plot_id,batch_id 0 45 miss% 0.04648021267070913
plot_id,batch_id 0 46 miss% 0.04519595972413829
plot_id,batch_id 0 47 miss% 0.03744067281444715
plot_id,batch_id 0 48 miss% 0.031423333215866815
plot_id,batch_id 0 49 miss% 0.05542363880055175
plot_id,batch_id 0 50 miss% 0.09902461953067998
plot_id,batch_id 0 51 miss% 0.046492990359589544
plot_id,batch_id 0 52 miss% 0.03553226287948488
plot_id,batch_id 0 53 miss% 0.02250802620472921
plot_id,batch_id 0 54 miss% 0.02046104338175787
plot_id,batch_id 0 55 miss% 0.06121497778334457
plot_id,batch_id 0 56 miss% 0.07508077239924131
plot_id,batch_id 0 57 miss% 0.046220679282446504
plot_id,batch_id 0 58 miss% 0.04049664893322228
plot_id,batch_id 0 59 miss% 0.048162638044786356
plot_id,batch_id 0 60 miss% 0.050883962163797945
plot_id,batch_id 0 61 miss% 0.023465749789463704
plot_id,batch_id 0 62 miss% 0.05226552705492332
plot_id,batch_id 0 63 miss% 0.059202128413007774
plot_id,batch_id 0 64 miss% 0.06466243062057746
plot_id,batch_id 0 65 miss% 0.12308615460964359
plot_id,batch_id 0 66 miss% 0.1333728272695236
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  209681
Epoch:0, Train loss:0.619391, valid loss:0.620696
Epoch:1, Train loss:0.048226, valid loss:0.006367
Epoch:2, Train loss:0.016076, valid loss:0.005128
Epoch:3, Train loss:0.011970, valid loss:0.003974
Epoch:4, Train loss:0.006029, valid loss:0.002420
Epoch:5, Train loss:0.004951, valid loss:0.002426
Epoch:6, Train loss:0.004538, valid loss:0.002000
Epoch:7, Train loss:0.004022, valid loss:0.002028
Epoch:8, Train loss:0.003542, valid loss:0.001918
Epoch:9, Train loss:0.003423, valid loss:0.001639
Epoch:10, Train loss:0.003217, valid loss:0.001432
Epoch:11, Train loss:0.002239, valid loss:0.001557
Epoch:12, Train loss:0.002089, valid loss:0.001164
Epoch:13, Train loss:0.001999, valid loss:0.001239
Epoch:14, Train loss:0.001992, valid loss:0.001260
Epoch:15, Train loss:0.001880, valid loss:0.001016
Epoch:16, Train loss:0.001850, valid loss:0.001107
Epoch:17, Train loss:0.001777, valid loss:0.001034
Epoch:18, Train loss:0.001821, valid loss:0.001115
Epoch:19, Train loss:0.001711, valid loss:0.000975
Epoch:20, Train loss:0.001639, valid loss:0.001018
Epoch:21, Train loss:0.001189, valid loss:0.000866
Epoch:22, Train loss:0.001118, valid loss:0.000866
Epoch:23, Train loss:0.001070, valid loss:0.000842
Epoch:24, Train loss:0.001075, valid loss:0.000874
Epoch:25, Train loss:0.001051, valid loss:0.000889
Epoch:26, Train loss:0.001046, valid loss:0.000865
Epoch:27, Train loss:0.001038, valid loss:0.000863
Epoch:28, Train loss:0.001004, valid loss:0.000958
Epoch:29, Train loss:0.000948, valid loss:0.000961
Epoch:30, Train loss:0.000939, valid loss:0.000837
Epoch:31, Train loss:0.000725, valid loss:0.000829
Epoch:32, Train loss:0.000712, valid loss:0.000777
Epoch:33, Train loss:0.000687, valid loss:0.000800
Epoch:34, Train loss:0.000694, valid loss:0.000789
Epoch:35, Train loss:0.000693, valid loss:0.000788
Epoch:36, Train loss:0.000680, valid loss:0.000795
Epoch:37, Train loss:0.000651, valid loss:0.000814
Epoch:38, Train loss:0.000661, valid loss:0.000851
Epoch:39, Train loss:0.000644, valid loss:0.000760
Epoch:40, Train loss:0.000642, valid loss:0.000749
Epoch:41, Train loss:0.000538, valid loss:0.000777
Epoch:42, Train loss:0.000527, valid loss:0.000775
Epoch:43, Train loss:0.000513, valid loss:0.000730
Epoch:44, Train loss:0.000513, valid loss:0.000747
Epoch:45, Train loss:0.000512, valid loss:0.000735
Epoch:46, Train loss:0.000507, valid loss:0.000762
Epoch:47, Train loss:0.000509, valid loss:0.000764
Epoch:48, Train loss:0.000497, valid loss:0.000753
Epoch:49, Train loss:0.000487, valid loss:0.000739
Epoch:50, Train loss:0.000501, valid loss:0.000796
Epoch:51, Train loss:0.000475, valid loss:0.000755
Epoch:52, Train loss:0.000467, valid loss:0.000759
Epoch:53, Train loss:0.000463, valid loss:0.000762
Epoch:54, Train loss:0.000460, valid loss:0.000750
Epoch:55, Train loss:0.000459, valid loss:0.000748
Epoch:56, Train loss:0.000457, valid loss:0.000745
Epoch:57, Train loss:0.000456, valid loss:0.000747
Epoch:58, Train loss:0.000454, valid loss:0.000749
Epoch:59, Train loss:0.000453, valid loss:0.000745
Epoch:60, Train loss:0.000452, valid loss:0.000747
training time 14277.390532016754
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.057709857688454685
plot_id,batch_id 0 1 miss% 0.048512465970762544
plot_id,batch_id 0 2 miss% 0.07953013149502679
plot_id,batch_id 0 3 miss% 0.0464638495165014
plot_id,batch_id 0 4 miss% 0.035221039403879696
plot_id,batch_id 0 5 miss% 0.04629206679649918
plot_id,batch_id 0 6 miss% 0.026658464632940953
plot_id,batch_id 0 7 miss% 0.052774403764316045
plot_id,batch_id 0 8 miss% 0.07268472544762547
plot_id,batch_id 0 9 miss% 0.05584202615057986
plot_id,batch_id 0 10 miss% 0.0605760402794437
plot_id,batch_id 0 11 miss% 0.03961084874115553
plot_id,batch_id 0 12 miss% 0.048378196990298585
plot_id,batch_id 0 13 miss% 0.038502516518024776
plot_id,batch_id 0 14 miss% 0.05012533675569816
plot_id,batch_id 0 15 miss% 0.05660550306648664
plot_id,batch_id 0 16 miss% 0.13038378182832308
plot_id,batch_id 0 17 miss% 0.041499367669295
plot_id,batch_id 0 18 miss% 0.048044475337364445
plot_id,batch_id 0 19 miss% 0.07040794667376798
plot_id,batch_id 0 20 miss% 0.11927083356458673
plot_id,batch_id 0 21 miss% 0.0376837354201555
plot_id,batch_id 0 22 miss% 0.04746535371598223
plot_id,batch_id 0 23 miss% 0.03644178155261805
plot_id,batch_id 0 24 miss% 0.036566252408400196
plot_id,batch_id 0 25 miss% 0.0416080179322459
plot_id,batch_id 0 26 miss% 0.06512233295217928
plot_id,batch_id 0 27 miss% 0.04329276475135878
plot_id,batch_id 0 28 miss% 0.028864175172787762
plot_id,batch_id 0 29 miss% 0.02199971551416456
plot_id,batch_id 0 30 miss% 0.03279746707333215
plot_id,batch_id 0 31 miss% 0.06662571276725558
plot_id,batch_id 0 32 miss% 0.09350449158688118
plot_id,batch_id 0 33 miss% 0.04594265832759697
plot_id,batch_id 0 34 miss% 0.035477343444140656
plot_id,batch_id 0 35 miss% 0.07916552973623009
plot_id,batch_id 0 36 miss% 0.07015738870608264
plot_id,batch_id 0 37 miss% 0.07522796098684616
plot_id,batch_id 0 38 miss% 0.035916489323808934
plot_id,batch_id 0 39 miss% 0.04408851895443659
plot_id,batch_id 0 40 miss% 0.08123328777250566
plot_id,batch_id 0 41 miss% 0.037646870559714385
plot_id,batch_id 0 42 miss% 0.07546891941496807
plot_id,batch_id 0 43 miss% 0.03389405984369836
plot_id,batch_id 0 44 miss% 0.024551115506458342
plot_id,batch_id 0 45 miss% 0.040162106635323616
plot_id,batch_id 0 46 miss% 0.03244013011548768
plot_id,batch_id 0 47 miss% 0.020291511392445703
plot_id,batch_id 0 48 miss% 0.028644406652964357
plot_id,batch_id 0 49 miss% 0.03830573056062978
plot_id,batch_id 0 50 miss% 0.13085860808724942
plot_id,batch_id 0 51 miss% 0.05730256695716528
plot_id,batch_id 0 52 miss% 0.04469931322718811
plot_id,batch_id 0 53 miss% 0.03749033514311416
plot_id,batch_id 0 54 miss% 0.04493375562868332
plot_id,batch_id 0 55 miss% 0.05842430111856412
plot_id,batch_id 0 56 miss% 0.0447578141985596
plot_id,batch_id 0 57 miss% 0.03552997325975637
plot_id,batch_id 0 58 miss% 0.03354704110167439
plot_id,batch_id 0 59 miss% 0.04768628302184326
plot_id,batch_id 0 60 miss% 0.03444558759965244
plot_id,batch_id 0 61 miss% 0.03969751510288493
plot_id,batch_id 0 62 miss% 0.04265465732967157
plot_id,batch_id 0 63 miss% 0.028796967619364875
plot_id,batch_id 0 64 miss% 0.07613707324597478
plot_id,batch_id 0 65 miss% 0.09466746648722095
plot_id,batch_id 0 66 miss% 0.08207500407369624
plot_id,batch_id 0 67 miss% 0.03571579609986994
plot_id,batch_id 0 68 miss% 0.033241695894125754
plot_id,batch_id plot_id,batch_id 0 67 miss% 0.029721597966896336
plot_id,batch_id 0 68 miss% 0.05857535716777028
plot_id,batch_id 0 69 miss% 0.05375741791669568
plot_id,batch_id 0 70 miss% 0.06940774307086718
plot_id,batch_id 0 71 miss% 0.036256259312727725
plot_id,batch_id 0 72 miss% 0.07890480663331144
plot_id,batch_id 0 73 miss% 0.06079897999520715
plot_id,batch_id 0 74 miss% 0.1700162768346135
plot_id,batch_id 0 75 miss% 0.10179458247503793
plot_id,batch_id 0 76 miss% 0.037664076579524516
plot_id,batch_id 0 77 miss% 0.04080562854711637
plot_id,batch_id 0 78 miss% 0.07851254182731603
plot_id,batch_id 0 79 miss% 0.05055800658025064
plot_id,batch_id 0 80 miss% 0.08040480829182177
plot_id,batch_id 0 81 miss% 0.12288217565125877
plot_id,batch_id 0 82 miss% 0.054390703214769874
plot_id,batch_id 0 83 miss% 0.1139388339016331
plot_id,batch_id 0 84 miss% 0.08162926470275199
plot_id,batch_id 0 85 miss% 0.05218232332258895
plot_id,batch_id 0 86 miss% 0.03806275349153721
plot_id,batch_id 0 87 miss% 0.0648156371420728
plot_id,batch_id 0 88 miss% 0.09555539315859916
plot_id,batch_id 0 89 miss% 0.07425044256502814
plot_id,batch_id 0 90 miss% 0.03640978448152364
plot_id,batch_id 0 91 miss% 0.04590766062645026
plot_id,batch_id 0 92 miss% 0.04226233862110514
plot_id,batch_id 0 93 miss% 0.02256720529111293
plot_id,batch_id 0 94 miss% 0.07914349198007128
plot_id,batch_id 0 95 miss% 0.05736684797054146
plot_id,batch_id 0 96 miss% 0.03038227166003842
plot_id,batch_id 0 97 miss% 0.03716413190015131
plot_id,batch_id 0 98 miss% 0.03887707980873163
plot_id,batch_id 0 99 miss% 0.07225488371313328
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09349942 0.03907044 0.11130432 0.0447778  0.06848601 0.03827957
 0.04334584 0.12719169 0.08064854 0.03793579 0.02515568 0.09794548
 0.04756112 0.06413064 0.09702925 0.03890234 0.07998113 0.06088473
 0.07197057 0.06553009 0.05860363 0.03035415 0.07182455 0.03285367
 0.02960119 0.06909162 0.06567589 0.04559496 0.02498924 0.03699875
 0.04198269 0.14024491 0.1079342  0.05989521 0.04565552 0.08775331
 0.09536071 0.07689375 0.04033621 0.05709164 0.15329291 0.05407665
 0.01982945 0.07545816 0.03157426 0.04648021 0.04519596 0.03744067
 0.03142333 0.05542364 0.09902462 0.04649299 0.03553226 0.02250803
 0.02046104 0.06121498 0.07508077 0.04622068 0.04049665 0.04816264
 0.05088396 0.02346575 0.05226553 0.05920213 0.06466243 0.12308615
 0.13337283 0.0297216  0.05857536 0.05375742 0.06940774 0.03625626
 0.07890481 0.06079898 0.17001628 0.10179458 0.03766408 0.04080563
 0.07851254 0.05055801 0.08040481 0.12288218 0.0543907  0.11393883
 0.08162926 0.05218232 0.03806275 0.06481564 0.09555539 0.07425044
 0.03640978 0.04590766 0.04226234 0.02256721 0.07914349 0.05736685
 0.03038227 0.03716413 0.03887708 0.07225488]
for model  120 the mean error 0.062119162265097526
all id 120 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 25 out win 4 err 0.062119162265097526 time 14260.311553478241
Launcher: Job 121 completed in 14526 seconds.
Launcher: Task 60 done. Exiting.
0 69 miss% 0.048904667567228804
plot_id,batch_id 0 70 miss% 0.07440293177766998
plot_id,batch_id 0 71 miss% 0.034297726919135504
plot_id,batch_id 0 72 miss% 0.11709597452593452
plot_id,batch_id 0 73 miss% 0.03031468156923342
plot_id,batch_id 0 74 miss% 0.10852213951843866
plot_id,batch_id 0 75 miss% 0.09885575277457419
plot_id,batch_id 0 76 miss% 0.09268646522323103
plot_id,batch_id 0 77 miss% 0.08484259185349348
plot_id,batch_id 0 78 miss% 0.034642077010768726
plot_id,batch_id 0 79 miss% 0.0699960690548312
plot_id,batch_id 0 80 miss% 0.04232164558105839
plot_id,batch_id 0 81 miss% 0.06481660107804776
plot_id,batch_id 0 82 miss% 0.060275008111976676
plot_id,batch_id 0 83 miss% 0.047730040626828604
plot_id,batch_id 0 84 miss% 0.08240156895856689
plot_id,batch_id 0 85 miss% 0.06652501482230523
plot_id,batch_id 0 86 miss% 0.03327617317167649
plot_id,batch_id 0 87 miss% 0.06218691995356669
plot_id,batch_id 0 88 miss% 0.06956616918320531
plot_id,batch_id 0 89 miss% 0.05083362874703542
plot_id,batch_id 0 90 miss% 0.04461641794178123
plot_id,batch_id 0 91 miss% 0.04828621878131287
plot_id,batch_id 0 92 miss% 0.04117146727030777
plot_id,batch_id 0 93 miss% 0.04650140518024967
plot_id,batch_id 0 94 miss% 0.08218079661932405
plot_id,batch_id 0 95 miss% 0.05307859649206983
plot_id,batch_id 0 96 miss% 0.05081239474724448
plot_id,batch_id 0 97 miss% 0.04141646449136561
plot_id,batch_id 0 98 miss% 0.02781516751649347
plot_id,batch_id 0 99 miss% 0.07463413256533338
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05770986 0.04851247 0.07953013 0.04646385 0.03522104 0.04629207
 0.02665846 0.0527744  0.07268473 0.05584203 0.06057604 0.03961085
 0.0483782  0.03850252 0.05012534 0.0566055  0.13038378 0.04149937
 0.04804448 0.07040795 0.11927083 0.03768374 0.04746535 0.03644178
 0.03656625 0.04160802 0.06512233 0.04329276 0.02886418 0.02199972
 0.03279747 0.06662571 0.09350449 0.04594266 0.03547734 0.07916553
 0.07015739 0.07522796 0.03591649 0.04408852 0.08123329 0.03764687
 0.07546892 0.03389406 0.02455112 0.04016211 0.03244013 0.02029151
 0.02864441 0.03830573 0.13085861 0.05730257 0.04469931 0.03749034
 0.04493376 0.0584243  0.04475781 0.03552997 0.03354704 0.04768628
 0.03444559 0.03969752 0.04265466 0.02879697 0.07613707 0.09466747
 0.082075   0.0357158  0.0332417  0.04890467 0.07440293 0.03429773
 0.11709597 0.03031468 0.10852214 0.09885575 0.09268647 0.08484259
 0.03464208 0.06999607 0.04232165 0.0648166  0.06027501 0.04773004
 0.08240157 0.06652501 0.03327617 0.06218692 0.06956617 0.05083363
 0.04461642 0.04828622 0.04117147 0.04650141 0.0821808  0.0530786
 0.05081239 0.04141646 0.02781517 0.07463413]
for model  70 the mean error 0.05463348371902279
all id 70 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 21 out win 5 err 0.05463348371902279 time 14277.390532016754
Launcher: Job 71 completed in 14543 seconds.
Launcher: Task 174 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  154129
Epoch:0, Train loss:0.542910, valid loss:0.547726
Epoch:1, Train loss:0.097196, valid loss:0.005200
Epoch:2, Train loss:0.014428, valid loss:0.004525
Epoch:3, Train loss:0.013595, valid loss:0.004311
Epoch:4, Train loss:0.013183, valid loss:0.004309
Epoch:5, Train loss:0.012761, valid loss:0.003872
Epoch:6, Train loss:0.012457, valid loss:0.003606
Epoch:7, Train loss:0.012217, valid loss:0.003674
Epoch:8, Train loss:0.009084, valid loss:0.001988
Epoch:9, Train loss:0.003791, valid loss:0.001411
Epoch:10, Train loss:0.002765, valid loss:0.001214
Epoch:11, Train loss:0.002021, valid loss:0.000998
Epoch:12, Train loss:0.001875, valid loss:0.001062
Epoch:13, Train loss:0.001838, valid loss:0.000969
Epoch:14, Train loss:0.001736, valid loss:0.000937
Epoch:15, Train loss:0.001663, valid loss:0.000914
Epoch:16, Train loss:0.001609, valid loss:0.000955
Epoch:17, Train loss:0.001501, valid loss:0.000956
Epoch:18, Train loss:0.001528, valid loss:0.000854
Epoch:19, Train loss:0.001448, valid loss:0.000988
Epoch:20, Train loss:0.001385, valid loss:0.001085
Epoch:21, Train loss:0.001114, valid loss:0.000790
Epoch:22, Train loss:0.001011, valid loss:0.000758
Epoch:23, Train loss:0.001014, valid loss:0.000756
Epoch:24, Train loss:0.000972, valid loss:0.000771
Epoch:25, Train loss:0.000967, valid loss:0.000756
Epoch:26, Train loss:0.000929, valid loss:0.000740
Epoch:27, Train loss:0.000917, valid loss:0.000741
Epoch:28, Train loss:0.000904, valid loss:0.000724
Epoch:29, Train loss:0.000887, valid loss:0.000711
Epoch:30, Train loss:0.000862, valid loss:0.000696
Epoch:31, Train loss:0.000706, valid loss:0.000688
Epoch:32, Train loss:0.000680, valid loss:0.000683
Epoch:33, Train loss:0.000679, valid loss:0.000695
Epoch:34, Train loss:0.000662, valid loss:0.000698
Epoch:35, Train loss:0.000664, valid loss:0.000692
Epoch:36, Train loss:0.000648, valid loss:0.000661
Epoch:37, Train loss:0.000646, valid loss:0.000683
Epoch:38, Train loss:0.000643, valid loss:0.000730
Epoch:39, Train loss:0.000625, valid loss:0.000638
Epoch:40, Train loss:0.000620, valid loss:0.000736
Epoch:41, Train loss:0.000544, valid loss:0.000639
Epoch:42, Train loss:0.000532, valid loss:0.000634
Epoch:43, Train loss:0.000529, valid loss:0.000657
Epoch:44, Train loss:0.000528, valid loss:0.000640
Epoch:45, Train loss:0.000524, valid loss:0.000634
Epoch:46, Train loss:0.000519, valid loss:0.000642
Epoch:47, Train loss:0.000515, valid loss:0.000634
Epoch:48, Train loss:0.000509, valid loss:0.000647
Epoch:49, Train loss:0.000500, valid loss:0.000652
Epoch:50, Train loss:0.000502, valid loss:0.000639
Epoch:51, Train loss:0.000464, valid loss:0.000635
Epoch:52, Train loss:0.000460, valid loss:0.000635
Epoch:53, Train loss:0.000458, valid loss:0.000635
Epoch:54, Train loss:0.000457, valid loss:0.000632
Epoch:55, Train loss:0.000456, valid loss:0.000632
Epoch:56, Train loss:0.000455, valid loss:0.000631
Epoch:57, Train loss:0.000454, valid loss:0.000631
Epoch:58, Train loss:0.000454, valid loss:0.000631
Epoch:59, Train loss:0.000454, valid loss:0.000630
Epoch:60, Train loss:0.000453, valid loss:0.000630
training time 14354.25519323349
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08293607191394498
plot_id,batch_id 0 1 miss% 0.05743784670680085
plot_id,batch_id 0 2 miss% 0.06120506822832731
plot_id,batch_id 0 3 miss% 0.03812119958273773
plot_id,batch_id 0 4 miss% 0.0408752376573921
plot_id,batch_id 0 5 miss% 0.023027917323833257
plot_id,batch_id 0 6 miss% 0.05457607576869865
plot_id,batch_id 0 7 miss% 0.07784740666516819
plot_id,batch_id 0 8 miss% 0.05019360760133672
plot_id,batch_id 0 9 miss% 0.06104656088102918
plot_id,batch_id 0 10 miss% 0.06409232951875078
plot_id,batch_id 0 11 miss% 0.06400786021079206
plot_id,batch_id 0 12 miss% 0.06019521736424609
plot_id,batch_id 0 13 miss% 0.04523216447209179
plot_id,batch_id 0 14 miss% 0.08760088871420982
plot_id,batch_id 0 15 miss% 0.07118722451314995
plot_id,batch_id 0 16 miss% 0.06462118756368286
plot_id,batch_id 0 17 miss% 0.04042481604344237
plot_id,batch_id 0 18 miss% 0.08143142146484927
plot_id,batch_id 0 19 miss% 0.08120930630123349
plot_id,batch_id 0 20 miss% 0.10039999257706192
plot_id,batch_id 0 21 miss% 0.027713728860534366
plot_id,batch_id 0 22 miss% 0.02887429576246909
plot_id,batch_id 0 23 miss% 0.0446888296869883
plot_id,batch_id 0 24 miss% 0.023132093999692217
plot_id,batch_id 0 25 miss% 0.050900227793055405
plot_id,batch_id 0 26 miss% 0.05001785728288272
plot_id,batch_id 0 27 miss% 0.03382372407509847
plot_id,batch_id 0 28 miss% 0.03479544761283873
plot_id,batch_id 0 29 miss% 0.027657505240416796
plot_id,batch_id 0 30 miss% 0.04976707153325153
plot_id,batch_id 0 31 miss% 0.09251063464609721
plot_id,batch_id 0 32 miss% 0.0795685876305257
plot_id,batch_id 0 33 miss% 0.04419406074170277
plot_id,batch_id 0 34 miss% 0.030131015586381014
plot_id,batch_id 0 35 miss% 0.058015009359753046
plot_id,batch_id 0 36 miss% 0.05427116406576948
plot_id,batch_id 0 37 miss% 0.06759666953370747
plot_id,batch_id 0 38 miss% 0.08909125309081889
plot_id,batch_id 0 39 miss% 0.031408797883468766
plot_id,batch_id 0 40 miss% 0.06224992633714914
plot_id,batch_id 0 41 miss% 0.030416468674152746
plot_id,batch_id 0 42 miss% 0.021766223379664582
plot_id,batch_id 0 43 miss% 0.052366947962733175
plot_id,batch_id 0 44 miss% 0.0938267425487947
plot_id,batch_id 0 45 miss% 0.04486652795615026
plot_id,batch_id 0 46 miss% 0.02240028006855719
plot_id,batch_id 0 47 miss% 0.026967156858343784
plot_id,batch_id 0 48 miss% 0.018074686887882444
plot_id,batch_id 0 49 miss% 0.01972413571340002
plot_id,batch_id 0 50 miss% 0.08177978450468079
plot_id,batch_id 0 51 miss% 0.021823576460863487
plot_id,batch_id 0 52 miss% 0.01480906256195389
plot_id,batch_id 0 53 miss% 0.01229818809082887
plot_id,batch_id 0 54 miss% 0.030892992239003055
plot_id,batch_id 0 55 miss% 0.09236000836689351
plot_id,batch_id 0 56 miss% 0.0697048694396975
plot_id,batch_id 0 57 miss% 0.033899716869604433
plot_id,batch_id 0 58 miss% 0.02084501862797296
plot_id,batch_id 0 59 miss% 0.01941777650917664
plot_id,batch_id 0 60 miss% 0.05834551155756815
plot_id,batch_id 0 61 miss% 0.04867894233944187
plot_id,batch_id 0 62 miss% 0.22636261437731442
plot_id,batch_id 0 63 miss% 0.03773170036650077
plot_id,batch_id 0 64 miss% 0.06623077966278386
plot_id,batch_id 0 65 miss% 0.029014465823312446
plot_id,batch_id 0 66 miss% 0.045149039888191515
plot_id,batch_id 0 67 miss% 0.021603523436626117
plot_id,batch_id 0 68 miss% 0.05862624568783864
plot_id,batch_id 0 69 miss% 0.11384964431104676
plot_id,batch_id 0 70 miss% 0.028042463824505735
plot_id,batch_id 0 71 miss% 0.04668970936042523
plot_id,batch_id 0 72 miss% 0.12111086613297044
plot_id,batch_id 0 73 miss% 0.043358092191313514
plot_id,batch_id 0 74 miss% 0.07242383067485576
plot_id,batch_id 0 75 miss% 0.06059786808818758
plot_id,batch_id 0 76 miss% 0.07326568270197241
plot_id,batch_id 0 77 miss% 0.07742190560997854
plot_id,batch_id 0 78 miss% 0.035404586574051516
plot_id,batch_id 0 79 miss% 0.054245206535225285
plot_id,batch_id 0 80 miss% 0.06256533480145209
plot_id,batch_id 0 81 miss% 0.07504619198368975
plot_id,batch_id 0 82 miss% 0.05265616906204704
plot_id,batch_id 0 83 miss% 0.07777208325805468
plot_id,batch_id 0 84 miss% 0.0975719370072524
plot_id,batch_id 0 85 miss% 0.07416127445677277
plot_id,batch_id 0 86 miss% 0.0767888572227848
plot_id,batch_id 0 87 miss% 0.08175474511378628
plot_id,batch_id 0 88 miss% 0.08763060387133703
plot_id,batch_id 0 89 miss% 0.0643022047856243
plot_id,batch_id 0 90 miss% 0.027046268487726767
plot_id,batch_id 0 91 miss% 0.05084393852689811
plot_id,batch_id 0 92 miss% 0.09590003177624874
plot_id,batch_id 0 93 miss% 0.04876276460215038
plot_id,batch_id 0 94 miss% 0.08353385870717163
plot_id,batch_id 0 95 miss% 0.03703923016648446
plot_id,batch_id 0 96 miss% 0.04434630036401253
plot_id,batch_id 0 97 miss% 0.049753647426858304
plot_id,batch_id 0 98 miss% 0.029522765672361465
plot_id,batch_id 0 99 miss% 0.11453930473668107
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08293607 0.05743785 0.06120507 0.0381212  0.04087524 0.02302792
 0.05457608 0.07784741 0.05019361 0.06104656 0.06409233 0.06400786
 0.06019522 0.04523216 0.08760089 0.07118722 0.06462119 0.04042482
 0.08143142 0.08120931 0.10039999 0.02771373 0.0288743  0.04468883
 0.02313209 0.05090023 0.05001786 0.03382372 0.03479545 0.02765751
 0.04976707 0.09251063 0.07956859 0.04419406 0.03013102 0.05801501
 0.05427116 0.06759667 0.08909125 0.0314088  0.06224993 0.03041647
 0.02176622 0.05236695 0.09382674 0.04486653 0.02240028 0.02696716
 0.01807469 0.01972414 0.08177978 0.02182358 0.01480906 0.01229819
 0.03089299 0.09236001 0.06970487 0.03389972 0.02084502 0.01941778
 0.05834551 0.04867894 0.22636261 0.0377317  0.06623078 0.02901447
 0.04514904 0.02160352 0.05862625 0.11384964 0.02804246 0.04668971
 0.12111087 0.04335809 0.07242383 0.06059787 0.07326568 0.07742191
 0.03540459 0.05424521 0.06256533 0.07504619 0.05265617 0.07777208
 0.09757194 0.07416127 0.07678886 0.08175475 0.0876306  0.0643022
 0.02704627 0.05084394 0.09590003 0.04876276 0.08353386 0.03703923
 0.0443463  0.04975365 0.02952277 0.1145393 ]
for model  116 the mean error 0.056660076566892405
all id 116 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 25 out win 6 err 0.056660076566892405 time 14354.25519323349
Launcher: Job 117 completed in 14615 seconds.
Launcher: Task 192 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  151697
Epoch:0, Train loss:0.707612, valid loss:0.660109
Epoch:1, Train loss:0.531342, valid loss:0.525883
Epoch:2, Train loss:0.516879, valid loss:0.524938
Epoch:3, Train loss:0.515530, valid loss:0.524402
Epoch:4, Train loss:0.514746, valid loss:0.524393
Epoch:5, Train loss:0.514369, valid loss:0.524298
Epoch:6, Train loss:0.513969, valid loss:0.524226
Epoch:7, Train loss:0.513745, valid loss:0.524235
Epoch:8, Train loss:0.513587, valid loss:0.524028
Epoch:9, Train loss:0.513428, valid loss:0.523906
Epoch:10, Train loss:0.513363, valid loss:0.523737
Epoch:11, Train loss:0.512529, valid loss:0.523424
Epoch:12, Train loss:0.512393, valid loss:0.523538
Epoch:13, Train loss:0.512389, valid loss:0.523318
Epoch:14, Train loss:0.512303, valid loss:0.523487
Epoch:15, Train loss:0.512272, valid loss:0.523394
Epoch:16, Train loss:0.512275, valid loss:0.523373
Epoch:17, Train loss:0.512162, valid loss:0.523389
Epoch:18, Train loss:0.512086, valid loss:0.523349
Epoch:19, Train loss:0.512108, valid loss:0.523395
Epoch:20, Train loss:0.512109, valid loss:0.523327
Epoch:21, Train loss:0.511695, valid loss:0.523175
Epoch:22, Train loss:0.511631, valid loss:0.523155
Epoch:23, Train loss:0.511625, valid loss:0.523178
Epoch:24, Train loss:0.511609, valid loss:0.523143
Epoch:25, Train loss:0.511595, valid loss:0.523120
Epoch:26, Train loss:0.511578, valid loss:0.523201
Epoch:27, Train loss:0.511547, valid loss:0.523136
Epoch:28, Train loss:0.511525, valid loss:0.523209
Epoch:29, Train loss:0.511542, valid loss:0.523249
Epoch:30, Train loss:0.511516, valid loss:0.523137
Epoch:31, Train loss:0.511338, valid loss:0.523039
Epoch:32, Train loss:0.511319, valid loss:0.523059
Epoch:33, Train loss:0.511308, valid loss:0.523046
Epoch:34, Train loss:0.511290, valid loss:0.523038
Epoch:35, Train loss:0.511294, valid loss:0.523064
Epoch:36, Train loss:0.511293, valid loss:0.523052
Epoch:37, Train loss:0.511264, valid loss:0.523080
Epoch:38, Train loss:0.511266, valid loss:0.523047
Epoch:39, Train loss:0.511267, valid loss:0.523033
Epoch:40, Train loss:0.511248, valid loss:0.523033
Epoch:41, Train loss:0.511176, valid loss:0.523042
Epoch:42, Train loss:0.511177, valid loss:0.522989
Epoch:43, Train loss:0.511164, valid loss:0.523017
Epoch:44, Train loss:0.511161, valid loss:0.522992
Epoch:45, Train loss:0.511159, valid loss:0.523030
Epoch:46, Train loss:0.511150, valid loss:0.523008
Epoch:47, Train loss:0.511145, valid loss:0.522998
Epoch:48, Train loss:0.511144, valid loss:0.523013
Epoch:49, Train loss:0.511139, valid loss:0.522971
Epoch:50, Train loss:0.511144, valid loss:0.522981
Epoch:51, Train loss:0.511117, valid loss:0.522978
Epoch:52, Train loss:0.511114, valid loss:0.522975
Epoch:53, Train loss:0.511112, valid loss:0.522980
Epoch:54, Train loss:0.511110, valid loss:0.522976
Epoch:55, Train loss:0.511109, valid loss:0.522975
Epoch:56, Train loss:0.511107, valid loss:0.522973
Epoch:57, Train loss:0.511106, valid loss:0.522979
Epoch:58, Train loss:0.511106, valid loss:0.522978
Epoch:59, Train loss:0.511105, valid loss:0.522973
Epoch:60, Train loss:0.511104, valid loss:0.522979
training time 14549.161848068237
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.8696490240573851
plot_id,batch_id 0 1 miss% 0.8392629679873463
plot_id,batch_id 0 2 miss% 0.8351050307924093
plot_id,batch_id 0 3 miss% 0.8313495271118153
plot_id,batch_id 0 4 miss% 0.8262509851268907
plot_id,batch_id 0 5 miss% 0.8725374838008018
plot_id,batch_id 0 6 miss% 0.8390397490224659
plot_id,batch_id 0 7 miss% 0.8336286866621483
plot_id,batch_id 0 8 miss% 0.8312507345246966
plot_id,batch_id 0 9 miss% 0.8284240123161676
plot_id,batch_id 0 10 miss% 0.8764788613372323
plot_id,batch_id 0 11 miss% 0.8407423403420796
plot_id,batch_id 0 12 miss% 0.8346467053192196
plot_id,batch_id 0 13 miss% 0.8291957201866077
plot_id,batch_id 0 14 miss% 0.8297592708512826
plot_id,batch_id 0 15 miss% 0.868268255539609
plot_id,batch_id 0 16 miss% 0.8423103638883948
plot_id,batch_id 0 17 miss% 0.8358544379736889
plot_id,batch_id 0 18 miss% 0.8300454360766164
plot_id,batch_id 0 19 miss% 0.8300558583648889
plot_id,batch_id 0 20 miss% 0.8571834146620444
plot_id,batch_id 0 21 miss% 0.8305980831086712
plot_id,batch_id 0 22 miss% 0.8315828460978308
plot_id,batch_id 0 23 miss% 0.8280713040539175
plot_id,batch_id 0 24 miss% 0.8246026008203686
plot_id,batch_id 0 25 miss% 0.8581579361224749
plot_id,batch_id 0 26 miss% 0.8348118967400828
plot_id,batch_id 0 27 miss% 0.8310990423953784
plot_id,batch_id 0 28 miss% 0.8289270032406086
plot_id,batch_id 0 29 miss% 0.8266322657667614
plot_id,batch_id 0 30 miss% 0.8549319227443696
plot_id,batch_id 0 31 miss% 0.8355096293203086
plot_id,batch_id 0 32 miss% 0.8307414685676464
plot_id,batch_id 0 33 miss% 0.8280930946008735
plot_id,batch_id 0 34 miss% 0.8279768432498436
plot_id,batch_id 0 35 miss% 0.856877753296363
plot_id,batch_id 0 36 miss% 0.8363411151207255
plot_id,batch_id 0 37 miss% 0.831734268789375
plot_id,batch_id 0 38 miss% 0.8292884511282245
plot_id,batch_id 0 39 miss% 0.8276600134801052
plot_id,batch_id 0 40 miss% 0.8456288093119498
plot_id,batch_id 0 41 miss% 0.8292110896934953
plot_id,batch_id 0 42 miss% 0.827358076822816
plot_id,batch_id 0 43 miss% 0.8242734645474802
plot_id,batch_id 0 44 miss% 0.8272546293595499
plot_id,batch_id 0 45 miss% 0.8424633343143768
plot_id,batch_id 0 46 miss% 0.8307732480320212
plot_id,batch_id 0 47 miss% 0.8269389710824893
plot_id,batch_id 0 48 miss% 0.827170961987268
plot_id,batch_id 0 49 miss% 0.8314410364562446
plot_id,batch_id 0 50 miss% 0.8411342548037449
plot_id,batch_id 0 51 miss% 0.8288374886563527
plot_id,batch_id 0 52 miss% 0.8283704676934607
plot_id,batch_id 0 53 miss% 0.82533365013843
plot_id,batch_id 0 54 miss% 0.8254703016217507
plot_id,batch_id 0 55 miss% 0.8445305028198655
plot_id,batch_id 0 56 miss% 0.8321693161315815
plot_id,batch_id 0 57 miss% 0.8284891873315828
plot_id,batch_id 0 58 miss% 0.8263958432691889
plot_id,batch_id 0 59 miss% 0.8274915435172197
plot_id,batch_id 0 60 miss% 0.8968597573239472
plot_id,batch_id 0 61 miss% 0.8559822073838272
plot_id,batch_id 0 62 miss% 0.8471484469759586
plot_id,batch_id 0 63 miss% 0.8386693814197852
plot_id,batch_id 0 64 miss% 0.8365092082023803
plot_id,batch_id 0 65 miss% 0.9001298870876743
plot_id,batch_id 0 66 miss% 0.8610162377958591
plot_id,batch_id 0 67 miss% 0.84747195834327
plot_id,batch_id 0 68 miss% 0.8389647751864455
plot_id,batch_id 0 69 miss% 0.834875371766955
plot_id,batch_id 0 70 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  89105
Epoch:0, Train loss:0.376429, valid loss:0.339860
Epoch:1, Train loss:0.057255, valid loss:0.002995
Epoch:2, Train loss:0.007599, valid loss:0.002530
Epoch:3, Train loss:0.006318, valid loss:0.002265
Epoch:4, Train loss:0.005466, valid loss:0.001841
Epoch:5, Train loss:0.003152, valid loss:0.001208
Epoch:6, Train loss:0.002064, valid loss:0.001170
Epoch:7, Train loss:0.001893, valid loss:0.001157
Epoch:8, Train loss:0.001735, valid loss:0.000869
Epoch:9, Train loss:0.001594, valid loss:0.000872
Epoch:10, Train loss:0.001514, valid loss:0.000793
Epoch:11, Train loss:0.001188, valid loss:0.000726
Epoch:12, Train loss:0.001154, valid loss:0.000633
Epoch:13, Train loss:0.001105, valid loss:0.000622
Epoch:14, Train loss:0.001084, valid loss:0.000676
Epoch:15, Train loss:0.001043, valid loss:0.000644
Epoch:16, Train loss:0.001014, valid loss:0.000653
Epoch:17, Train loss:0.000996, valid loss:0.000655
Epoch:18, Train loss:0.000958, valid loss:0.000598
Epoch:19, Train loss:0.000957, valid loss:0.000569
Epoch:20, Train loss:0.000934, valid loss:0.000608
Epoch:21, Train loss:0.000746, valid loss:0.000552
Epoch:22, Train loss:0.000743, valid loss:0.000540
Epoch:23, Train loss:0.000725, valid loss:0.000543
Epoch:24, Train loss:0.000720, valid loss:0.000533
Epoch:25, Train loss:0.000734, valid loss:0.000500
Epoch:26, Train loss:0.000703, valid loss:0.000564
Epoch:27, Train loss:0.000691, valid loss:0.000524
Epoch:28, Train loss:0.000691, valid loss:0.000522
Epoch:29, Train loss:0.000680, valid loss:0.000544
Epoch:30, Train loss:0.000656, valid loss:0.000552
Epoch:31, Train loss:0.000577, valid loss:0.000468
Epoch:32, Train loss:0.000572, valid loss:0.000489
Epoch:33, Train loss:0.000564, valid loss:0.000463
Epoch:34, Train loss:0.000560, valid loss:0.000473
Epoch:35, Train loss:0.000562, valid loss:0.000478
Epoch:36, Train loss:0.000546, valid loss:0.000487
Epoch:37, Train loss:0.000553, valid loss:0.000484
Epoch:38, Train loss:0.000548, valid loss:0.000465
Epoch:39, Train loss:0.000547, valid loss:0.000470
Epoch:40, Train loss:0.000539, valid loss:0.000462
Epoch:41, Train loss:0.000494, valid loss:0.000491
Epoch:42, Train loss:0.000489, valid loss:0.000461
Epoch:43, Train loss:0.000486, valid loss:0.000459
Epoch:44, Train loss:0.000484, valid loss:0.000472
Epoch:45, Train loss:0.000484, valid loss:0.000481
Epoch:46, Train loss:0.000482, valid loss:0.000445
Epoch:47, Train loss:0.000479, valid loss:0.000459
Epoch:48, Train loss:0.000477, valid loss:0.000477
Epoch:49, Train loss:0.000479, valid loss:0.000454
Epoch:50, Train loss:0.000472, valid loss:0.000479
Epoch:51, Train loss:0.000446, valid loss:0.000455
Epoch:52, Train loss:0.000443, valid loss:0.000458
Epoch:53, Train loss:0.000442, valid loss:0.000453
Epoch:54, Train loss:0.000442, valid loss:0.000454
Epoch:55, Train loss:0.000440, valid loss:0.000453
Epoch:56, Train loss:0.000440, valid loss:0.000455
Epoch:57, Train loss:0.000440, valid loss:0.000451
Epoch:58, Train loss:0.000439, valid loss:0.000451
Epoch:59, Train loss:0.000439, valid loss:0.000451
Epoch:60, Train loss:0.000439, valid loss:0.000451
training time 14466.869171857834
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.09647069037088489
plot_id,batch_id 0 1 miss% 0.06291371996404371
plot_id,batch_id 0 2 miss% 0.10020744723864415
plot_id,batch_id 0 3 miss% 0.10035312821123972
plot_id,batch_id 0 4 miss% 0.17920542099812997
plot_id,batch_id 0 5 miss% 0.08105796015511277
plot_id,batch_id 0 6 miss% 0.036345349333349215
plot_id,batch_id 0 7 miss% 0.08651239403274165
plot_id,batch_id 0 8 miss% 0.082496161825391
plot_id,batch_id 0 9 miss% 0.023367135059466252
plot_id,batch_id 0 10 miss% 0.05423595986983625
plot_id,batch_id 0 11 miss% 0.0693218738194432
plot_id,batch_id 0 12 miss% 0.0687600253698755
plot_id,batch_id 0 13 miss% 0.08104229870501803
plot_id,batch_id 0 14 miss% 0.09468245883882447
plot_id,batch_id 0 15 miss% 0.05706454073042571
plot_id,batch_id 0 16 miss% 0.09856873586235575
plot_id,batch_id 0 17 miss% 0.04482843580288623
plot_id,batch_id 0 18 miss% 0.09196404186814004
plot_id,batch_id 0 19 miss% 0.0893074231640745
plot_id,batch_id 0 20 miss% 0.053705633923858534
plot_id,batch_id 0 21 miss% 0.07430120763078976
plot_id,batch_id 0 22 miss% 0.05270732395130333
plot_id,batch_id 0 23 miss% 0.05343956366507434
plot_id,batch_id 0 24 miss% 0.08366146637127274
plot_id,batch_id 0 25 miss% 0.06490785135151686
plot_id,batch_id 0 26 miss% 0.051387402365830795
plot_id,batch_id 0 27 miss% 0.06389405248372242
plot_id,batch_id 0 28 miss% 0.05275489266787754
plot_id,batch_id 0 29 miss% 0.042863524360690235
plot_id,batch_id 0 30 miss% 0.03629062995906354
plot_id,batch_id 0 31 miss% 0.10520971297170069
plot_id,batch_id 0 32 miss% 0.1379700795713021
plot_id,batch_id 0 33 miss% 0.07280830677395356
plot_id,batch_id 0 34 miss% 0.10289063281986317
plot_id,batch_id 0 35 miss% 0.03938259755947861
plot_id,batch_id 0 36 miss% 0.09769762263201275
plot_id,batch_id 0 37 miss% 0.06287487561644113
plot_id,batch_id 0 38 miss% 0.052221098739095376
plot_id,batch_id 0 39 miss% 0.03853882588190941
plot_id,batch_id 0 40 miss% 0.05687019049367347
plot_id,batch_id 0 41 miss% 0.059948398838896584
plot_id,batch_id 0 42 miss% 0.030884624263427217
plot_id,batch_id 0 43 miss% 0.08608870041179754
plot_id,batch_id 0 44 miss% 0.055567970358652866
plot_id,batch_id 0 45 miss% 0.05415832424674867
plot_id,batch_id 0 46 miss% 0.027903424988729393
plot_id,batch_id 0 47 miss% 0.042239172846457444
plot_id,batch_id 0 48 miss% 0.030686735378327515
plot_id,batch_id 0 49 miss% 0.044940072678808306
plot_id,batch_id 0 50 miss% 0.14773944283073448
plot_id,batch_id 0 51 miss% 0.07485274953056845
plot_id,batch_id 0 52 miss% 0.037140203792988734
plot_id,batch_id 0 53 miss% 0.031725278594129026
plot_id,batch_id 0 54 miss% 0.07781186107691906
plot_id,batch_id 0 55 miss% 0.0694089013868003
plot_id,batch_id 0 56 miss% 0.06778396039537006
plot_id,batch_id 0 57 miss% 0.06483962669963005
plot_id,batch_id 0 58 miss% 0.05662683717367048
plot_id,batch_id 0 59 miss% 0.03085143912208027
plot_id,batch_id 0 60 miss% 0.04553527879194579
plot_id,batch_id 0 61 miss% 0.036697192069559285
plot_id,batch_id 0 62 miss% 0.04486089768310433
plot_id,batch_id 0 63 miss% 0.09754038561589053
plot_id,batch_id 0 64 miss% 0.04330543433276232
plot_id,batch_id 0 65 miss% 0.07165214767228649
plot_id,batch_id 0 66 miss% 0.04467118087188032
plot_id,batch_id 0 67 miss% 0.8950419445381428
plot_id,batch_id 0 71 miss% 0.8596805894975912
plot_id,batch_id 0 72 miss% 0.845487556221766
plot_id,batch_id 0 73 miss% 0.8417879641079712
plot_id,batch_id 0 74 miss% 0.8353320283295134
plot_id,batch_id 0 75 miss% 0.8965296820639742
plot_id,batch_id 0 76 miss% 0.8597526414933144
plot_id,batch_id 0 77 miss% 0.8447904582371575
plot_id,batch_id 0 78 miss% 0.8401875047722067
plot_id,batch_id 0 79 miss% 0.8348074005208747
plot_id,batch_id 0 80 miss% 0.8881297041928823
plot_id,batch_id 0 81 miss% 0.8474771325103461
plot_id,batch_id 0 82 miss% 0.8392482475588459
plot_id,batch_id 0 83 miss% 0.8348130214603046
plot_id,batch_id 0 84 miss% 0.8313245425434176
plot_id,batch_id 0 85 miss% 0.8877641736753576
plot_id,batch_id 0 86 miss% 0.8500438478353897
plot_id,batch_id 0 87 miss% 0.8399873354799333
plot_id,batch_id 0 88 miss% 0.8357023633943972
plot_id,batch_id 0 89 miss% 0.8325384428958025
plot_id,batch_id 0 90 miss% 0.8899408734043002
plot_id,batch_id 0 91 miss% 0.8538563915728977
plot_id,batch_id 0 92 miss% 0.8401262176753246
plot_id,batch_id 0 93 miss% 0.8358148228517148
plot_id,batch_id 0 94 miss% 0.8345883790361833
plot_id,batch_id 0 95 miss% 0.8929613745086792
plot_id,batch_id 0 96 miss% 0.8542240159880503
plot_id,batch_id 0 97 miss% 0.8442376110879948
plot_id,batch_id 0 98 miss% 0.8373204622189849
plot_id,batch_id 0 99 miss% 0.8335352941512114
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.86964902 0.83926297 0.83510503 0.83134953 0.82625099 0.87253748
 0.83903975 0.83362869 0.83125073 0.82842401 0.87647886 0.84074234
 0.83464671 0.82919572 0.82975927 0.86826826 0.84231036 0.83585444
 0.83004544 0.83005586 0.85718341 0.83059808 0.83158285 0.8280713
 0.8246026  0.85815794 0.8348119  0.83109904 0.828927   0.82663227
 0.85493192 0.83550963 0.83074147 0.82809309 0.82797684 0.85687775
 0.83634112 0.83173427 0.82928845 0.82766001 0.84562881 0.82921109
 0.82735808 0.82427346 0.82725463 0.84246333 0.83077325 0.82693897
 0.82717096 0.83144104 0.84113425 0.82883749 0.82837047 0.82533365
 0.8254703  0.8445305  0.83216932 0.82848919 0.82639584 0.82749154
 0.89685976 0.85598221 0.84714845 0.83866938 0.83650921 0.90012989
 0.86101624 0.84747196 0.83896478 0.83487537 0.89504194 0.85968059
 0.84548756 0.84178796 0.83533203 0.89652968 0.85975264 0.84479046
 0.8401875  0.8348074  0.8881297  0.84747713 0.83924825 0.83481302
 0.83132454 0.88776417 0.85004385 0.83998734 0.83570236 0.83253844
 0.88994087 0.85385639 0.84012622 0.83581482 0.83458838 0.89296137
 0.85422402 0.84423761 0.83732046 0.83353529]
for model  75 the mean error 0.8427007183748922
all id 75 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 21 out win 4 err 0.8427007183748922 time 14549.161848068237
Launcher: Job 76 completed in 14682 seconds.
Launcher: Task 23 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  154129
Epoch:0, Train loss:0.552158, valid loss:0.564355
Epoch:1, Train loss:0.078932, valid loss:0.004616
Epoch:2, Train loss:0.013935, valid loss:0.004658
Epoch:3, Train loss:0.013048, valid loss:0.004034
Epoch:4, Train loss:0.012580, valid loss:0.003902
Epoch:5, Train loss:0.012235, valid loss:0.003717
Epoch:6, Train loss:0.012007, valid loss:0.003546
Epoch:7, Train loss:0.011660, valid loss:0.003675
Epoch:8, Train loss:0.011505, valid loss:0.003430
Epoch:9, Train loss:0.011422, valid loss:0.003401
Epoch:10, Train loss:0.011138, valid loss:0.003554
Epoch:11, Train loss:0.010680, valid loss:0.003109
Epoch:12, Train loss:0.010523, valid loss:0.003073
Epoch:13, Train loss:0.010554, valid loss:0.002995
Epoch:14, Train loss:0.010414, valid loss:0.003006
Epoch:15, Train loss:0.010391, valid loss:0.002965
Epoch:16, Train loss:0.010315, valid loss:0.003048
Epoch:17, Train loss:0.010309, valid loss:0.002969
Epoch:18, Train loss:0.010249, valid loss:0.002999
Epoch:19, Train loss:0.010183, valid loss:0.002963
Epoch:20, Train loss:0.010189, valid loss:0.002950
Epoch:21, Train loss:0.009897, valid loss:0.002847
Epoch:22, Train loss:0.009834, valid loss:0.002816
Epoch:23, Train loss:0.009840, valid loss:0.002810
Epoch:24, Train loss:0.009805, valid loss:0.002792
Epoch:25, Train loss:0.009816, valid loss:0.002866
Epoch:26, Train loss:0.009792, valid loss:0.002893
Epoch:27, Train loss:0.009759, valid loss:0.002770
Epoch:28, Train loss:0.009762, valid loss:0.002833
Epoch:29, Train loss:0.009736, valid loss:0.002807
Epoch:30, Train loss:0.009710, valid loss:0.002813
Epoch:31, Train loss:0.009589, valid loss:0.002784
Epoch:32, Train loss:0.009578, valid loss:0.002778
Epoch:33, Train loss:0.009575, valid loss:0.002758
Epoch:34, Train loss:0.009581, valid loss:0.002786
Epoch:35, Train loss:0.009340, valid loss:0.002604
Epoch:36, Train loss:0.008323, valid loss:0.002611
Epoch:37, Train loss:0.008221, valid loss:0.002582
Epoch:38, Train loss:0.008227, valid loss:0.002599
Epoch:39, Train loss:0.008172, valid loss:0.002556
Epoch:40, Train loss:0.008170, valid loss:0.002550
Epoch:41, Train loss:0.008103, valid loss:0.002536
Epoch:42, Train loss:0.008092, valid loss:0.002541
Epoch:43, Train loss:0.008085, valid loss:0.002533
Epoch:44, Train loss:0.008086, valid loss:0.002534
Epoch:45, Train loss:0.008074, valid loss:0.002539
Epoch:46, Train loss:0.008085, valid loss:0.002562
Epoch:47, Train loss:0.008068, valid loss:0.002540
Epoch:48, Train loss:0.007805, valid loss:0.002342
Epoch:49, Train loss:0.003259, valid loss:0.000803
Epoch:50, Train loss:0.000996, valid loss:0.000689
Epoch:51, Train loss:0.000816, valid loss:0.000661
Epoch:52, Train loss:0.000807, valid loss:0.000654
Epoch:53, Train loss:0.000803, valid loss:0.000656
Epoch:54, Train loss:0.000802, valid loss:0.000652
Epoch:55, Train loss:0.000800, valid loss:0.000650
Epoch:56, Train loss:0.000799, valid loss:0.000654
Epoch:57, Train loss:0.000798, valid loss:0.000656
Epoch:58, Train loss:0.000797, valid loss:0.000650
Epoch:59, Train loss:0.000796, valid loss:0.000650
Epoch:60, Train loss:0.000796, valid loss:0.000652
training time 14499.108830690384
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.07209077053475631
plot_id,batch_id 0 1 miss% 0.029017068682238734
plot_id,batch_id 0 2 miss% 0.09370091543051852
plot_id,batch_id 0 3 miss% 0.039440173433031335
plot_id,batch_id 0 4 miss% 0.042956273427912056
plot_id,batch_id 0 5 miss% 0.045862025800276054
plot_id,batch_id 0 6 miss% 0.03648144551883563
plot_id,batch_id 0 7 miss% 0.08315291407211381
plot_id,batch_id 0 8 miss% 0.053754767336613614
plot_id,batch_id 0 9 miss% 0.04194875401919465
plot_id,batch_id 0 10 miss% 0.05570373826335445
plot_id,batch_id 0 11 miss% 0.05966240220826783
plot_id,batch_id 0 12 miss% 0.10006225172546668
plot_id,batch_id 0 13 miss% 0.04498582277584493
plot_id,batch_id 0 14 miss% 0.042540059112119846
plot_id,batch_id 0 15 miss% 0.050250209344234444
plot_id,batch_id 0 16 miss% 0.08534833174240478
plot_id,batch_id 0 17 miss% 0.07329655825251734
plot_id,batch_id 0 18 miss% 0.07415660859753777
plot_id,batch_id 0 19 miss% 0.10249435896864603
plot_id,batch_id 0 20 miss% 0.1378819946737816
plot_id,batch_id 0 21 miss% 0.041066138816418254
plot_id,batch_id 0 22 miss% 0.03744392645602105
plot_id,batch_id 0 23 miss% 0.03022531299306658
plot_id,batch_id 0 24 miss% 0.03868723918782244
plot_id,batch_id 0 25 miss% 0.1287446227357296
plot_id,batch_id 0 26 miss% 0.03902918190386165
plot_id,batch_id 0 27 miss% 0.07630475982065069
plot_id,batch_id 0 28 miss% 0.03845852943087657
plot_id,batch_id 0 29 miss% 0.026287795972218617
plot_id,batch_id 0 30 miss% 0.035604269047408474
plot_id,batch_id 0 31 miss% 0.07752887577393319
plot_id,batch_id 0 32 miss% 0.0747472721495378
plot_id,batch_id 0 33 miss% 0.07017877646080223
plot_id,batch_id 0 34 miss% 0.04177830553961863
plot_id,batch_id 0 35 miss% 0.05497651876368199
plot_id,batch_id 0 36 miss% 0.08994262756766128
plot_id,batch_id 0 37 miss% 0.08592566731937264
plot_id,batch_id 0 38 miss% 0.04411124895484601
plot_id,batch_id 0 39 miss% 0.02759011969843601
plot_id,batch_id 0 40 miss% 0.06844133908290817
plot_id,batch_id 0 41 miss% 0.05834231130105573
plot_id,batch_id 0 42 miss% 0.036023566477873595
plot_id,batch_id 0 43 miss% 0.04254063431954879
plot_id,batch_id 0 44 miss% 0.03034952507825975
plot_id,batch_id 0 45 miss% 0.046150487100916356
plot_id,batch_id 0 46 miss% 0.04025922018886151
plot_id,batch_id 0 47 miss% 0.03698923277360874
plot_id,batch_id 0 48 miss% 0.041265474803190254
plot_id,batch_id 0 49 miss% 0.031968650541496335
plot_id,batch_id 0 50 miss% 0.10809741910094152
plot_id,batch_id 0 51 miss% 0.0441293946100122
plot_id,batch_id 0 52 miss% 0.03090017872694046
plot_id,batch_id 0 53 miss% 0.027029661957684404
plot_id,batch_id 0 54 miss% 0.052951044360034574
plot_id,batch_id 0 55 miss% 0.07953943221382132
plot_id,batch_id 0 56 miss% 0.05824348937953811
plot_id,batch_id 0 57 miss% 0.030410428754281432
plot_id,batch_id 0 58 miss% 0.03130827672199471
plot_id,batch_id 0 59 miss% 0.02895685207883543
plot_id,batch_id 0 60 miss% 0.033945675762776184
plot_id,batch_id 0 61 miss% 0.029166144709321905
plot_id,batch_id 0 62 miss% 0.0863151558251874
plot_id,batch_id 0 63 miss% 0.06162958070084115
plot_id,batch_id 0 64 miss% 0.04049276939132417
plot_id,batch_id 0 65 miss% 0.023248621782455892
plot_id,batch_id 0 66 miss% 0.09333202880281717
plot_id,batch_id 0 67 miss% 0.04026526788322258
0.03003250316384516
plot_id,batch_id 0 68 miss% 0.033778293716666095
plot_id,batch_id 0 69 miss% 0.10807108652099405
plot_id,batch_id 0 70 miss% 0.06532120235818023
plot_id,batch_id 0 71 miss% 0.03623074785906704
plot_id,batch_id 0 72 miss% 0.10098767440668205
plot_id,batch_id 0 73 miss% 0.05225207592764696
plot_id,batch_id 0 74 miss% 0.11683064307474637
plot_id,batch_id 0 75 miss% 0.0865953989437564
plot_id,batch_id 0 76 miss% 0.050039952466861104
plot_id,batch_id 0 77 miss% 0.08325330354612283
plot_id,batch_id 0 78 miss% 0.03890271086371754
plot_id,batch_id 0 79 miss% 0.07969211699018516
plot_id,batch_id 0 80 miss% 0.10019510606739292
plot_id,batch_id 0 81 miss% 0.09212830471491396
plot_id,batch_id 0 82 miss% 0.06773457666114666
plot_id,batch_id 0 83 miss% 0.057609210553426264
plot_id,batch_id 0 84 miss% 0.07468084589455974
plot_id,batch_id 0 85 miss% 0.03138370728373584
plot_id,batch_id 0 86 miss% 0.05720219351343579
plot_id,batch_id 0 87 miss% 0.06774423378508675
plot_id,batch_id 0 88 miss% 0.076045842634393
plot_id,batch_id 0 89 miss% 0.08372107694337844
plot_id,batch_id 0 90 miss% 0.06084473071607324
plot_id,batch_id 0 91 miss% 0.06919941398960719
plot_id,batch_id 0 92 miss% 0.041545098225021324
plot_id,batch_id 0 93 miss% 0.04241686640987592
plot_id,batch_id 0 94 miss% 0.14775548778164468
plot_id,batch_id 0 95 miss% 0.056717660674753145
plot_id,batch_id 0 96 miss% 0.05759910383526046
plot_id,batch_id 0 97 miss% 0.0685459090725522
plot_id,batch_id 0 98 miss% 0.03194133273291825
plot_id,batch_id 0 99 miss% 0.06803376943877491
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09647069 0.06291372 0.10020745 0.10035313 0.17920542 0.08105796
 0.03634535 0.08651239 0.08249616 0.02336714 0.05423596 0.06932187
 0.06876003 0.0810423  0.09468246 0.05706454 0.09856874 0.04482844
 0.09196404 0.08930742 0.05370563 0.07430121 0.05270732 0.05343956
 0.08366147 0.06490785 0.0513874  0.06389405 0.05275489 0.04286352
 0.03629063 0.10520971 0.13797008 0.07280831 0.10289063 0.0393826
 0.09769762 0.06287488 0.0522211  0.03853883 0.05687019 0.0599484
 0.03088462 0.0860887  0.05556797 0.05415832 0.02790342 0.04223917
 0.03068674 0.04494007 0.14773944 0.07485275 0.0371402  0.03172528
 0.07781186 0.0694089  0.06778396 0.06483963 0.05662684 0.03085144
 0.04553528 0.03669719 0.0448609  0.09754039 0.04330543 0.07165215
 0.04467118 0.0300325  0.03377829 0.10807109 0.0653212  0.03623075
 0.10098767 0.05225208 0.11683064 0.0865954  0.05003995 0.0832533
 0.03890271 0.07969212 0.10019511 0.0921283  0.06773458 0.05760921
 0.07468085 0.03138371 0.05720219 0.06774423 0.07604584 0.08372108
 0.06084473 0.06919941 0.0415451  0.04241687 0.14775549 0.05671766
 0.0575991  0.06854591 0.03194133 0.06803377]
for model  167 the mean error 0.067055751154289
all id 167 hidden_dim 24 learning_rate 0.0025 num_layers 3 frames 31 out win 6 err 0.067055751154289 time 14466.869171857834
Launcher: Job 168 completed in 14727 seconds.
Launcher: Task 241 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  120401
Epoch:0, Train loss:0.617554, valid loss:0.615952
Epoch:1, Train loss:0.111187, valid loss:0.003925
Epoch:2, Train loss:0.010532, valid loss:0.003207
Epoch:3, Train loss:0.008962, valid loss:0.002871
Epoch:4, Train loss:0.006968, valid loss:0.002412
Epoch:5, Train loss:0.005356, valid loss:0.002013
Epoch:6, Train loss:0.002894, valid loss:0.001313
Epoch:7, Train loss:0.002580, valid loss:0.001240
Epoch:8, Train loss:0.002325, valid loss:0.001027
Epoch:9, Train loss:0.002189, valid loss:0.001166
Epoch:10, Train loss:0.002009, valid loss:0.001129
Epoch:11, Train loss:0.001553, valid loss:0.000843
Epoch:12, Train loss:0.001514, valid loss:0.000746
Epoch:13, Train loss:0.001453, valid loss:0.000720
Epoch:14, Train loss:0.001371, valid loss:0.000722
Epoch:15, Train loss:0.001344, valid loss:0.000746
Epoch:16, Train loss:0.001321, valid loss:0.000776
Epoch:17, Train loss:0.001266, valid loss:0.001191
Epoch:18, Train loss:0.001252, valid loss:0.000715
Epoch:19, Train loss:0.001200, valid loss:0.000766
Epoch:20, Train loss:0.001152, valid loss:0.000702
Epoch:21, Train loss:0.000918, valid loss:0.000645
Epoch:22, Train loss:0.000921, valid loss:0.000610
Epoch:23, Train loss:0.000897, valid loss:0.000593
Epoch:24, Train loss:0.000891, valid loss:0.000632
Epoch:25, Train loss:0.000848, valid loss:0.000566
Epoch:26, Train loss:0.000861, valid loss:0.000614
Epoch:27, Train loss:0.000851, valid loss:0.000552
Epoch:28, Train loss:0.000830, valid loss:0.000856
Epoch:29, Train loss:0.000822, valid loss:0.000593
Epoch:30, Train loss:0.000813, valid loss:0.000582
Epoch:31, Train loss:0.000694, valid loss:0.000519
Epoch:32, Train loss:0.000681, valid loss:0.000531
Epoch:33, Train loss:0.000688, valid loss:0.000542
Epoch:34, Train loss:0.000672, valid loss:0.000529
Epoch:35, Train loss:0.000667, valid loss:0.000524
Epoch:36, Train loss:0.000657, valid loss:0.000473
Epoch:37, Train loss:0.000652, valid loss:0.000492
Epoch:38, Train loss:0.000660, valid loss:0.000523
Epoch:39, Train loss:0.000648, valid loss:0.000505
Epoch:40, Train loss:0.000632, valid loss:0.000505
Epoch:41, Train loss:0.000582, valid loss:0.000509
Epoch:42, Train loss:0.000579, valid loss:0.000501
Epoch:43, Train loss:0.000574, valid loss:0.000492
Epoch:44, Train loss:0.000573, valid loss:0.000489
Epoch:45, Train loss:0.000569, valid loss:0.000502
Epoch:46, Train loss:0.000571, valid loss:0.000486
Epoch:47, Train loss:0.000567, valid loss:0.000509
Epoch:48, Train loss:0.000559, valid loss:0.000514
Epoch:49, Train loss:0.000553, valid loss:0.000496
Epoch:50, Train loss:0.000551, valid loss:0.000489
Epoch:51, Train loss:0.000523, valid loss:0.000478
Epoch:52, Train loss:0.000518, valid loss:0.000476
Epoch:53, Train loss:0.000517, valid loss:0.000478
Epoch:54, Train loss:0.000516, valid loss:0.000474
Epoch:55, Train loss:0.000516, valid loss:0.000475
Epoch:56, Train loss:0.000515, valid loss:0.000475
Epoch:57, Train loss:0.000515, valid loss:0.000477
Epoch:58, Train loss:0.000515, valid loss:0.000477
Epoch:59, Train loss:0.000515, valid loss:0.000477
Epoch:60, Train loss:0.000514, valid loss:0.000475
training time 14539.950309753418
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.039559987442920046
plot_id,batch_id 0 1 miss% 0.06984210963417892
plot_id,batch_id 0 2 miss% 0.09177531727931049
plot_id,batch_id 0 3 miss% 0.04957111120155138
plot_id,batch_id 0 4 miss% 0.10253154886178015
plot_id,batch_id 0 5 miss% 0.04291944554139338
plot_id,batch_id 0 6 miss% 0.040752726878894634
plot_id,batch_id 0 7 miss% 0.11871523104758278
plot_id,batch_id 0 8 miss% 0.05985341542742312
plot_id,batch_id 0 9 miss% 0.03316234228562468
plot_id,batch_id 0 10 miss% 0.03994349039107045
plot_id,batch_id 0 11 miss% 0.09036199078891416
plot_id,batch_id 0 12 miss% 0.07633660866257011
plot_id,batch_id 0 13 miss% 0.04019820514187497
plot_id,batch_id 0 14 miss% 0.08549556560181223
plot_id,batch_id 0 15 miss% 0.04110153609147295
plot_id,batch_id 0 16 miss% 0.13856030734219668
plot_id,batch_id 0 17 miss% 0.04053741795830063
plot_id,batch_id 0 18 miss% 0.07617868193986084
plot_id,batch_id 0 19 miss% 0.1155497425859009
plot_id,batch_id 0 20 miss% 0.05270984211195339
plot_id,batch_id 0 21 miss% 0.03079763345779977
plot_id,batch_id 0 22 miss% 0.04692403845205909
plot_id,batch_id 0 23 miss% 0.04099533897993022
plot_id,batch_id 0 24 miss% 0.07921928379725285
plot_id,batch_id 0 25 miss% 0.05013950264847439
plot_id,batch_id 0 26 miss% 0.07205512415721516
plot_id,batch_id 0 27 miss% 0.057036695797254136
plot_id,batch_id 0 28 miss% 0.034071362681700634
plot_id,batch_id 0 29 miss% 0.03177853685665044
plot_id,batch_id 0 30 miss% 0.05167226863894647
plot_id,batch_id 0 31 miss% 0.11182913926121776
plot_id,batch_id 0 32 miss% 0.09159162100422538
plot_id,batch_id 0 33 miss% 0.05809010187660732
plot_id,batch_id 0 34 miss% 0.035579596387599846
plot_id,batch_id 0 35 miss% 0.023164507053040323
plot_id,batch_id 0 36 miss% 0.09432052087541562
plot_id,batch_id 0 37 miss% 0.10138581021569094
plot_id,batch_id 0 38 miss% 0.06468163988697516
plot_id,batch_id 0 39 miss% 0.05499154210687111
plot_id,batch_id 0 40 miss% 0.1737839171485827
plot_id,batch_id 0 41 miss% 0.0687456351607655
plot_id,batch_id 0 42 miss% 0.018134619965078494
plot_id,batch_id 0 43 miss% 0.08743316898246654
plot_id,batch_id 0 44 miss% 0.02548946066314818
plot_id,batch_id 0 45 miss% 0.0482921762109149
plot_id,batch_id 0 46 miss% 0.05582799062385647
plot_id,batch_id 0 47 miss% 0.02120001439376488
plot_id,batch_id 0 48 miss% 0.02582361750225572
plot_id,batch_id 0 49 miss% 0.06268698604824682
plot_id,batch_id 0 50 miss% 0.16151572346223636
plot_id,batch_id 0 51 miss% 0.041343586589873124
plot_id,batch_id 0 52 miss% 0.039113766798765465
plot_id,batch_id 0 53 miss% 0.06265483096636376
plot_id,batch_id 0 54 miss% 0.04970972729241633
plot_id,batch_id 0 55 miss% 0.0825141425261631
plot_id,batch_id 0 56 miss% 0.12386316863200811
plot_id,batch_id 0 57 miss% 0.04492954819786243
plot_id,batch_id 0 58 miss% 0.04013660453545578
plot_id,batch_id 0 59 miss% 0.032136958003815386
plot_id,batch_id 0 60 miss% 0.046777481461519124
plot_id,batch_id 0 61 miss% 0.033486921963504244
plot_id,batch_id 0 62 miss% 0.06941545269418654
plot_id,batch_id 0 63 miss% 0.04404979921304565
plot_id,batch_id 0 64 miss% 0.05158683596959931
plot_id,batch_id 0 65 miss% 0.02992105722533586
plot_id,batch_id 0 66 miss% 0.02449877407998118
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  69649
Epoch:0, Train loss:0.593215, valid loss:0.570251
Epoch:1, Train loss:0.037074, valid loss:0.004713
Epoch:2, Train loss:0.010576, valid loss:0.003664
Epoch:3, Train loss:0.006011, valid loss:0.001919
Epoch:4, Train loss:0.003962, valid loss:0.001847
Epoch:5, Train loss:0.003393, valid loss:0.001818
Epoch:6, Train loss:0.003222, valid loss:0.001444
Epoch:7, Train loss:0.002974, valid loss:0.001678
Epoch:8, Train loss:0.002838, valid loss:0.001251
Epoch:9, Train loss:0.002736, valid loss:0.001318
Epoch:10, Train loss:0.002478, valid loss:0.001311
Epoch:11, Train loss:0.001884, valid loss:0.000957
Epoch:12, Train loss:0.001780, valid loss:0.000963
Epoch:13, Train loss:0.001743, valid loss:0.000958
Epoch:14, Train loss:0.001693, valid loss:0.001001
Epoch:15, Train loss:0.001644, valid loss:0.000895
Epoch:16, Train loss:0.001621, valid loss:0.000874
Epoch:17, Train loss:0.001597, valid loss:0.000905
Epoch:18, Train loss:0.001542, valid loss:0.000857
Epoch:19, Train loss:0.001502, valid loss:0.000926
Epoch:20, Train loss:0.001439, valid loss:0.000849
Epoch:21, Train loss:0.001138, valid loss:0.000753
Epoch:22, Train loss:0.001099, valid loss:0.000778
Epoch:23, Train loss:0.001085, valid loss:0.000730
Epoch:24, Train loss:0.001088, valid loss:0.000722
Epoch:25, Train loss:0.001054, valid loss:0.000750
Epoch:26, Train loss:0.001025, valid loss:0.000678
Epoch:27, Train loss:0.001022, valid loss:0.000701
Epoch:28, Train loss:0.001019, valid loss:0.000726
Epoch:29, Train loss:0.001008, valid loss:0.000698
Epoch:30, Train loss:0.000986, valid loss:0.000869
Epoch:31, Train loss:0.000834, valid loss:0.000630
Epoch:32, Train loss:0.000810, valid loss:0.000673
Epoch:33, Train loss:0.000799, valid loss:0.000655
Epoch:34, Train loss:0.000781, valid loss:0.000671
Epoch:35, Train loss:0.000797, valid loss:0.000664
Epoch:36, Train loss:0.000785, valid loss:0.000629
Epoch:37, Train loss:0.000781, valid loss:0.000645
Epoch:38, Train loss:0.000761, valid loss:0.000659
Epoch:39, Train loss:0.000758, valid loss:0.000625
Epoch:40, Train loss:0.000762, valid loss:0.000685
Epoch:41, Train loss:0.000675, valid loss:0.000618
Epoch:42, Train loss:0.000673, valid loss:0.000625
Epoch:43, Train loss:0.000660, valid loss:0.000636
Epoch:44, Train loss:0.000661, valid loss:0.000626
Epoch:45, Train loss:0.000651, valid loss:0.000630
Epoch:46, Train loss:0.000649, valid loss:0.000619
Epoch:47, Train loss:0.000651, valid loss:0.000629
Epoch:48, Train loss:0.000644, valid loss:0.000627
Epoch:49, Train loss:0.000636, valid loss:0.000616
Epoch:50, Train loss:0.000643, valid loss:0.000604
Epoch:51, Train loss:0.000603, valid loss:0.000610
Epoch:52, Train loss:0.000593, valid loss:0.000612
Epoch:53, Train loss:0.000591, valid loss:0.000611
Epoch:54, Train loss:0.000589, valid loss:0.000608
Epoch:55, Train loss:0.000588, valid loss:0.000606
Epoch:56, Train loss:0.000587, valid loss:0.000612
Epoch:57, Train loss:0.000586, valid loss:0.000606
Epoch:58, Train loss:0.000586, valid loss:0.000608
Epoch:59, Train loss:0.000585, valid loss:0.000604
Epoch:60, Train loss:0.000584, valid loss:0.000606
training time 14541.714903831482
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.1077333286258677
plot_id,batch_id 0 1 miss% 0.05606184070855408
plot_id,batch_id 0 2 miss% 0.079158177442188
plot_id,batch_id 0 3 miss% 0.061268266640068875
plot_id,batch_id 0 4 miss% 0.057482794413687845
plot_id,batch_id 0 5 miss% 0.03081622059063086
plot_id,batch_id 0 6 miss% 0.07679894627368514
plot_id,batch_id 0 7 miss% 0.0688266945818632
plot_id,batch_id 0 8 miss% 0.06752594307238068
plot_id,batch_id 0 9 miss% 0.0380915825196622
plot_id,batch_id 0 10 miss% 0.025227499584958263
plot_id,batch_id 0 11 miss% 0.0717884746145555
plot_id,batch_id 0 12 miss% 0.0982125282559049
plot_id,batch_id 0 13 miss% 0.08521969971181244
plot_id,batch_id 0 14 miss% 0.09105209454916621
plot_id,batch_id 0 15 miss% 0.04107429955963201
plot_id,batch_id 0 16 miss% 0.080734750018734
plot_id,batch_id 0 17 miss% 0.026360547751016337
plot_id,batch_id 0 18 miss% 0.05604777952504617
plot_id,batch_id 0 19 miss% 0.07129992880353951
plot_id,batch_id 0 20 miss% 0.16314306143566615
plot_id,batch_id 0 21 miss% 0.03418182504072203
plot_id,batch_id 0 22 miss% 0.057239115449089044
plot_id,batch_id 0 23 miss% 0.08004347086087323
plot_id,batch_id 0 24 miss% 0.057717430802416606
plot_id,batch_id 0 25 miss% 0.0438155497650625
plot_id,batch_id 0 26 miss% 0.06930618826657038
plot_id,batch_id 0 27 miss% 0.04058097634645187
plot_id,batch_id 0 28 miss% 0.039995537689729986
plot_id,batch_id 0 29 miss% 0.041932905296154045
plot_id,batch_id 0 30 miss% 0.07839460193920565
plot_id,batch_id 0 31 miss% 0.07623251015616839
plot_id,batch_id 0 32 miss% 0.07426403333207572
plot_id,batch_id 0 33 miss% 0.0365056162417714
plot_id,batch_id 0 34 miss% 0.0661538354001992
plot_id,batch_id 0 35 miss% 0.09489818785862193
plot_id,batch_id 0 36 miss% 0.12542962456105874
plot_id,batch_id 0 37 miss% 0.0761273356779451
plot_id,batch_id 0 38 miss% 0.07128308084103559
plot_id,batch_id 0 39 miss% 0.046368815809468486
plot_id,batch_id 0 40 miss% 0.10812095660005207
plot_id,batch_id 0 41 miss% 0.08937360073518649
plot_id,batch_id 0 42 miss% 0.10123193930180671
plot_id,batch_id 0 43 miss% 0.05358473846980434
plot_id,batch_id 0 44 miss% 0.03299795302955009
plot_id,batch_id 0 45 miss% 0.062307549084337166
plot_id,batch_id 0 46 miss% 0.0440930011229121
plot_id,batch_id 0 47 miss% 0.04306775880744032
plot_id,batch_id 0 48 miss% 0.058538352527536
plot_id,batch_id 0 49 miss% 0.03540608249791333
plot_id,batch_id 0 50 miss% 0.14875653256247595
plot_id,batch_id 0 51 miss% 0.045765955697234274
plot_id,batch_id 0 52 miss% 0.039817720395744806
plot_id,batch_id 0 53 miss% 0.03176173015005291
plot_id,batch_id 0 54 miss% 0.034654560229398425
plot_id,batch_id 0 55 miss% 0.04621898358931452
plot_id,batch_id 0 56 miss% 0.09221457580808326
plot_id,batch_id 0 57 miss% 0.048441665581269354
plot_id,batch_id 0 58 miss% 0.03261368367857092
plot_id,batch_id 0 59 miss% 0.036422582487719714
plot_id,batch_id 0 60 miss% 0.057426037784586194
plot_id,batch_id 0 61 miss% 0.04657527007520508
plot_id,batch_id 0 62 miss% 0.07109833737175132
plot_id,batch_id 0 63 miss% 0.0461388650039223
plot_id,batch_id 0 64 miss% 0.07529223531427343
plot_id,batch_id 0 65 miss% 0.059837846335326114
plot_id,batch_id 0 66 miss% 0.13905276746451659
plot_id,batch_id 0 67 miss% 0.06565351408304927
plot_id,batch_id 0 plot_id,batch_id 0 68 miss% 0.05951896214939415
plot_id,batch_id 0 69 miss% 0.09813049956362509
plot_id,batch_id 0 70 miss% 0.13189895023941467
plot_id,batch_id 0 71 miss% 0.06067759854934371
plot_id,batch_id 0 72 miss% 0.0482613262960185
plot_id,batch_id 0 73 miss% 0.06577326291346605
plot_id,batch_id 0 74 miss% 0.10186677629793217
plot_id,batch_id 0 75 miss% 0.04417606468323371
plot_id,batch_id 0 76 miss% 0.08644052344250584
plot_id,batch_id 0 77 miss% 0.09186393482393627
plot_id,batch_id 0 78 miss% 0.023337905810123995
plot_id,batch_id 0 79 miss% 0.10395125435646081
plot_id,batch_id 0 80 miss% 0.04265978784164163
plot_id,batch_id 0 81 miss% 0.09336454326387694
plot_id,batch_id 0 82 miss% 0.06326283640424656
plot_id,batch_id 0 83 miss% 0.06151670474727463
plot_id,batch_id 0 84 miss% 0.0476250079546274
plot_id,batch_id 0 85 miss% 0.04151227653013942
plot_id,batch_id 0 86 miss% 0.050863563159599424
plot_id,batch_id 0 87 miss% 0.07707259284803349
plot_id,batch_id 0 88 miss% 0.08192595940916447
plot_id,batch_id 0 89 miss% 0.04030635165113564
plot_id,batch_id 0 90 miss% 0.03242167664227677
plot_id,batch_id 0 91 miss% 0.06385536849511715
plot_id,batch_id 0 92 miss% 0.06345819614100832
plot_id,batch_id 0 93 miss% 0.08278836915300265
plot_id,batch_id 0 94 miss% 0.07085460332585809
plot_id,batch_id 0 95 miss% 0.03983465137594993
plot_id,batch_id 0 96 miss% 0.07819525390290297
plot_id,batch_id 0 97 miss% 0.050889842757706305
plot_id,batch_id 0 98 miss% 0.03910558482196725
plot_id,batch_id 0 99 miss% 0.09563512721186661
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07209077 0.02901707 0.09370092 0.03944017 0.04295627 0.04586203
 0.03648145 0.08315291 0.05375477 0.04194875 0.05570374 0.0596624
 0.10006225 0.04498582 0.04254006 0.05025021 0.08534833 0.07329656
 0.07415661 0.10249436 0.13788199 0.04106614 0.03744393 0.03022531
 0.03868724 0.12874462 0.03902918 0.07630476 0.03845853 0.0262878
 0.03560427 0.07752888 0.07474727 0.07017878 0.04177831 0.05497652
 0.08994263 0.08592567 0.04411125 0.02759012 0.06844134 0.05834231
 0.03602357 0.04254063 0.03034953 0.04615049 0.04025922 0.03698923
 0.04126547 0.03196865 0.10809742 0.04412939 0.03090018 0.02702966
 0.05295104 0.07953943 0.05824349 0.03041043 0.03130828 0.02895685
 0.03394568 0.02916614 0.08631516 0.06162958 0.04049277 0.02324862
 0.09333203 0.04026527 0.05951896 0.0981305  0.13189895 0.0606776
 0.04826133 0.06577326 0.10186678 0.04417606 0.08644052 0.09186393
 0.02333791 0.10395125 0.04265979 0.09336454 0.06326284 0.0615167
 0.04762501 0.04151228 0.05086356 0.07707259 0.08192596 0.04030635
 0.03242168 0.06385537 0.0634582  0.08278837 0.0708546  0.03983465
 0.07819525 0.05088984 0.03910558 0.09563513]
for model  115 the mean error 0.0588875585370423
all id 115 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 25 out win 5 err 0.0588875585370423 time 14499.108830690384
Launcher: Job 116 completed in 14762 seconds.
Launcher: Task 215 done. Exiting.
68 miss% 0.047124467659947114
plot_id,batch_id 0 69 miss% 0.07958587192950163
plot_id,batch_id 0 70 miss% 0.04200603030765607
plot_id,batch_id 0 71 miss% 0.030641273460984466
plot_id,batch_id 0 72 miss% 0.07334858692113214
plot_id,batch_id 0 73 miss% 0.05611399831802957
plot_id,batch_id 0 74 miss% 0.0844824941693091
plot_id,batch_id 0 75 miss% 0.10886137555678284
plot_id,batch_id 0 76 miss% 0.09304232976871717
plot_id,batch_id 0 77 miss% 0.07780538924364122
plot_id,batch_id 0 78 miss% 0.06219448425536908
plot_id,batch_id 0 79 miss% 0.06282053546972965
plot_id,batch_id 0 80 miss% 0.05643643756598382
plot_id,batch_id 0 81 miss% 0.1162413085951527
plot_id,batch_id 0 82 miss% 0.06466157716359562
plot_id,batch_id 0 83 miss% 0.04780417897234777
plot_id,batch_id 0 84 miss% 0.06948650621818597
plot_id,batch_id 0 85 miss% 0.02937114876553453
plot_id,batch_id 0 86 miss% 0.0828931881743731
plot_id,batch_id 0 87 miss% 0.09501321242671328
plot_id,batch_id 0 88 miss% 0.09651194059232603
plot_id,batch_id 0 89 miss% 0.07412447762408272
plot_id,batch_id 0 90 miss% 0.03288337884313641
plot_id,batch_id 0 91 miss% 0.11341388411781499
plot_id,batch_id 0 92 miss% 0.08152950581062077
plot_id,batch_id 0 93 miss% 0.06619941932715107
plot_id,batch_id 0 94 miss% 0.05519195862595957
plot_id,batch_id 0 95 miss% 0.03818508328291885
plot_id,batch_id 0 96 miss% 0.07348549971415294
plot_id,batch_id 0 97 miss% 0.07872988507918126
plot_id,batch_id 0 98 miss% 0.028487582116426748
plot_id,batch_id 0 99 miss% 0.08405376081899757
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10773333 0.05606184 0.07915818 0.06126827 0.05748279 0.03081622
 0.07679895 0.06882669 0.06752594 0.03809158 0.0252275  0.07178847
 0.09821253 0.0852197  0.09105209 0.0410743  0.08073475 0.02636055
 0.05604778 0.07129993 0.16314306 0.03418183 0.05723912 0.08004347
 0.05771743 0.04381555 0.06930619 0.04058098 0.03999554 0.04193291
 0.0783946  0.07623251 0.07426403 0.03650562 0.06615384 0.09489819
 0.12542962 0.07612734 0.07128308 0.04636882 0.10812096 0.0893736
 0.10123194 0.05358474 0.03299795 0.06230755 0.044093   0.04306776
 0.05853835 0.03540608 0.14875653 0.04576596 0.03981772 0.03176173
 0.03465456 0.04621898 0.09221458 0.04844167 0.03261368 0.03642258
 0.05742604 0.04657527 0.07109834 0.04613887 0.07529224 0.05983785
 0.13905277 0.06565351 0.04712447 0.07958587 0.04200603 0.03064127
 0.07334859 0.056114   0.08448249 0.10886138 0.09304233 0.07780539
 0.06219448 0.06282054 0.05643644 0.11624131 0.06466158 0.04780418
 0.06948651 0.02937115 0.08289319 0.09501321 0.09651194 0.07412448
 0.03288338 0.11341388 0.08152951 0.06619942 0.05519196 0.03818508
 0.0734855  0.07872989 0.02848758 0.08405376]
for model  154 the mean error 0.06613590666697729
all id 154 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 25 out win 5 err 0.06613590666697729 time 14541.714903831482
Launcher: Job 155 completed in 14804 seconds.
Launcher: Task 101 done. Exiting.
plot_id,batch_id 0 67 miss% 0.04261716270319029
plot_id,batch_id 0 68 miss% 0.05082673304194412
plot_id,batch_id 0 69 miss% 0.08596108724110754
plot_id,batch_id 0 70 miss% 0.046816340653053595
plot_id,batch_id 0 71 miss% 0.04381923882567019
plot_id,batch_id 0 72 miss% 0.050528620603287645
plot_id,batch_id 0 73 miss% 0.06195335389623145
plot_id,batch_id 0 74 miss% 0.10468529036619072
plot_id,batch_id 0 75 miss% 0.07060046299501943
plot_id,batch_id 0 76 miss% 0.05643765528040524
plot_id,batch_id 0 77 miss% 0.04002859619010958
plot_id,batch_id 0 78 miss% 0.04950151536481799
plot_id,batch_id 0 79 miss% 0.08746587134672848
plot_id,batch_id 0 80 miss% 0.0402989228345411
plot_id,batch_id 0 81 miss% 0.07359248346687546
plot_id,batch_id 0 82 miss% 0.04712502575811344
plot_id,batch_id 0 83 miss% 0.06838285068633199
plot_id,batch_id 0 84 miss% 0.03878184001017421
plot_id,batch_id 0 85 miss% 0.03478345836283354
plot_id,batch_id 0 86 miss% 0.0403839752474539
plot_id,batch_id 0 87 miss% 0.05520464613579573
plot_id,batch_id 0 88 miss% 0.11616226560769613
plot_id,batch_id 0 89 miss% 0.07629354554871297
plot_id,batch_id 0 90 miss% 0.033069266787555555
plot_id,batch_id 0 91 miss% 0.05897327684144721
plot_id,batch_id 0 92 miss% 0.04711593545919493
plot_id,batch_id 0 93 miss% 0.04150041566846387
plot_id,batch_id 0 94 miss% 0.04951470598331575
plot_id,batch_id 0 95 miss% 0.04265625290229924
plot_id,batch_id 0 96 miss% 0.04169368712372915
plot_id,batch_id 0 97 miss% 0.05877594250575751
plot_id,batch_id 0 98 miss% 0.04770704978074521
plot_id,batch_id 0 99 miss% 0.11741889410631551
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03955999 0.06984211 0.09177532 0.04957111 0.10253155 0.04291945
 0.04075273 0.11871523 0.05985342 0.03316234 0.03994349 0.09036199
 0.07633661 0.04019821 0.08549557 0.04110154 0.13856031 0.04053742
 0.07617868 0.11554974 0.05270984 0.03079763 0.04692404 0.04099534
 0.07921928 0.0501395  0.07205512 0.0570367  0.03407136 0.03177854
 0.05167227 0.11182914 0.09159162 0.0580901  0.0355796  0.02316451
 0.09432052 0.10138581 0.06468164 0.05499154 0.17378392 0.06874564
 0.01813462 0.08743317 0.02548946 0.04829218 0.05582799 0.02120001
 0.02582362 0.06268699 0.16151572 0.04134359 0.03911377 0.06265483
 0.04970973 0.08251414 0.12386317 0.04492955 0.0401366  0.03213696
 0.04677748 0.03348692 0.06941545 0.0440498  0.05158684 0.02992106
 0.02449877 0.04261716 0.05082673 0.08596109 0.04681634 0.04381924
 0.05052862 0.06195335 0.10468529 0.07060046 0.05643766 0.0400286
 0.04950152 0.08746587 0.04029892 0.07359248 0.04712503 0.06838285
 0.03878184 0.03478346 0.04038398 0.05520465 0.11616227 0.07629355
 0.03306927 0.05897328 0.04711594 0.04150042 0.04951471 0.04265625
 0.04169369 0.05877594 0.04770705 0.11741889]
for model  93 the mean error 0.06061729223987834
all id 93 hidden_dim 24 learning_rate 0.0025 num_layers 4 frames 25 out win 4 err 0.06061729223987834 time 14539.950309753418
Launcher: Job 94 completed in 14807 seconds.
Launcher: Task 122 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  151697
Epoch:0, Train loss:0.686398, valid loss:0.640620
Epoch:1, Train loss:0.511472, valid loss:0.013968
Epoch:2, Train loss:0.013158, valid loss:0.003525
Epoch:3, Train loss:0.010464, valid loss:0.003331
Epoch:4, Train loss:0.008623, valid loss:0.003093
Epoch:5, Train loss:0.007429, valid loss:0.002444
Epoch:6, Train loss:0.006821, valid loss:0.002481
Epoch:7, Train loss:0.006449, valid loss:0.002316
Epoch:8, Train loss:0.006079, valid loss:0.002339
Epoch:9, Train loss:0.005836, valid loss:0.001861
Epoch:10, Train loss:0.005411, valid loss:0.002041
Epoch:11, Train loss:0.003780, valid loss:0.001456
Epoch:12, Train loss:0.003578, valid loss:0.001477
Epoch:13, Train loss:0.003489, valid loss:0.001348
Epoch:14, Train loss:0.003330, valid loss:0.001260
Epoch:15, Train loss:0.003296, valid loss:0.001260
Epoch:16, Train loss:0.002382, valid loss:0.001220
Epoch:17, Train loss:0.001973, valid loss:0.001185
Epoch:18, Train loss:0.001872, valid loss:0.001153
Epoch:19, Train loss:0.001801, valid loss:0.001137
Epoch:20, Train loss:0.001731, valid loss:0.001058
Epoch:21, Train loss:0.001396, valid loss:0.001019
Epoch:22, Train loss:0.001357, valid loss:0.000945
Epoch:23, Train loss:0.001306, valid loss:0.000948
Epoch:24, Train loss:0.001293, valid loss:0.000911
Epoch:25, Train loss:0.001261, valid loss:0.000942
Epoch:26, Train loss:0.001249, valid loss:0.000926
Epoch:27, Train loss:0.001220, valid loss:0.000863
Epoch:28, Train loss:0.001206, valid loss:0.001017
Epoch:29, Train loss:0.001157, valid loss:0.000885
Epoch:30, Train loss:0.001169, valid loss:0.001042
Epoch:31, Train loss:0.000987, valid loss:0.000780
Epoch:32, Train loss:0.000975, valid loss:0.000884
Epoch:33, Train loss:0.000960, valid loss:0.000860
Epoch:34, Train loss:0.000947, valid loss:0.000814
Epoch:35, Train loss:0.000934, valid loss:0.000780
Epoch:36, Train loss:0.000916, valid loss:0.000812
Epoch:37, Train loss:0.000922, valid loss:0.000786
Epoch:38, Train loss:0.000908, valid loss:0.000800
Epoch:39, Train loss:0.000911, valid loss:0.000827
Epoch:40, Train loss:0.000865, valid loss:0.000826
Epoch:41, Train loss:0.000802, valid loss:0.000740
Epoch:42, Train loss:0.000793, valid loss:0.000759
Epoch:43, Train loss:0.000792, valid loss:0.000745
Epoch:44, Train loss:0.000779, valid loss:0.000747
Epoch:45, Train loss:0.000768, valid loss:0.000761
Epoch:46, Train loss:0.000771, valid loss:0.000751
Epoch:47, Train loss:0.000778, valid loss:0.000755
Epoch:48, Train loss:0.000772, valid loss:0.000747
Epoch:49, Train loss:0.000751, valid loss:0.000743
Epoch:50, Train loss:0.000751, valid loss:0.000734
Epoch:51, Train loss:0.000701, valid loss:0.000722
Epoch:52, Train loss:0.000696, valid loss:0.000730
Epoch:53, Train loss:0.000693, valid loss:0.000728
Epoch:54, Train loss:0.000692, valid loss:0.000728
Epoch:55, Train loss:0.000691, valid loss:0.000723
Epoch:56, Train loss:0.000690, valid loss:0.000720
Epoch:57, Train loss:0.000690, valid loss:0.000725
Epoch:58, Train loss:0.000689, valid loss:0.000722
Epoch:59, Train loss:0.000689, valid loss:0.000726
Epoch:60, Train loss:0.000689, valid loss:0.000723
training time 14712.331038951874
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.0688060309033826
plot_id,batch_id 0 1 miss% 0.03933929195436035
plot_id,batch_id 0 2 miss% 0.10402951098101837
plot_id,batch_id 0 3 miss% 0.03607854674234366
plot_id,batch_id 0 4 miss% 0.08131904955353796
plot_id,batch_id 0 5 miss% 0.06723253753792324
plot_id,batch_id 0 6 miss% 0.07277016898405955
plot_id,batch_id 0 7 miss% 0.09940770806741064
plot_id,batch_id 0 8 miss% 0.08564675167649813
plot_id,batch_id 0 9 miss% 0.038042593881704084
plot_id,batch_id 0 10 miss% 0.02270393430916161
plot_id,batch_id 0 11 miss% 0.051759410049953054
plot_id,batch_id 0 12 miss% 0.09664260279446381
plot_id,batch_id 0 13 miss% 0.06605690432549301
plot_id,batch_id 0 14 miss% 0.08772519168975984
plot_id,batch_id 0 15 miss% 0.058010433392091255
plot_id,batch_id 0 16 miss% 0.07709813892832258
plot_id,batch_id 0 17 miss% 0.030995878569459453
plot_id,batch_id 0 18 miss% 0.07856401617204802
plot_id,batch_id 0 19 miss% 0.08140825015764411
plot_id,batch_id 0 20 miss% 0.11377033146746642
plot_id,batch_id 0 21 miss% 0.036285896783674845
plot_id,batch_id 0 22 miss% 0.0882633509830138
plot_id,batch_id 0 23 miss% 0.04527121898667565
plot_id,batch_id 0 24 miss% 0.06897014547110686
plot_id,batch_id 0 25 miss% 0.0674829256884515
plot_id,batch_id 0 26 miss% 0.025530456597569934
plot_id,batch_id 0 27 miss% 0.05112546339376808
plot_id,batch_id 0 28 miss% 0.042026978458904025
plot_id,batch_id 0 29 miss% 0.05113636212367821
plot_id,batch_id 0 30 miss% 0.06759858490689487
plot_id,batch_id 0 31 miss% 0.09921781184956963
plot_id,batch_id 0 32 miss% 0.09014115537037343
plot_id,batch_id 0 33 miss% 0.076708680459411
plot_id,batch_id 0 34 miss% 0.08718963592002443
plot_id,batch_id 0 35 miss% 0.06880677284041152
plot_id,batch_id 0 36 miss% 0.06597198838646895
plot_id,batch_id 0 37 miss% 0.07644374383093287
plot_id,batch_id 0 38 miss% 0.046192180659081886
plot_id,batch_id 0 39 miss% 0.05956711813670387
plot_id,batch_id 0 40 miss% 0.09919697187809014
plot_id,batch_id 0 41 miss% 0.08812314721205487
plot_id,batch_id 0 42 miss% 0.05651006941575736
plot_id,batch_id 0 43 miss% 0.07073231391025579
plot_id,batch_id 0 44 miss% 0.06721641949700116
plot_id,batch_id 0 45 miss% 0.02887054021917727
plot_id,batch_id 0 46 miss% 0.051320898892886165
plot_id,batch_id 0 47 miss% 0.0422798117680507
plot_id,batch_id 0 48 miss% 0.04915313934749994
plot_id,batch_id 0 49 miss% 0.04277494810777037
plot_id,batch_id 0 50 miss% 0.1277752748396997
plot_id,batch_id 0 51 miss% 0.06491172228004377
plot_id,batch_id 0 52 miss% 0.05088206664523749
plot_id,batch_id 0 53 miss% 0.038256023548934016
plot_id,batch_id 0 54 miss% 0.07109018977172887
plot_id,batch_id 0 55 miss% 0.05024346716189499
plot_id,batch_id 0 56 miss% 0.06555284635250785
plot_id,batch_id 0 57 miss% 0.06576592725709524
plot_id,batch_id 0 58 miss% 0.03183543260530033
plot_id,batch_id 0 59 miss% 0.054150714351986436
plot_id,batch_id 0 60 miss% 0.04598374535480728
plot_id,batch_id 0 61 miss% 0.027528076025271495
plot_id,batch_id 0 62 miss% 0.07543189617395922
plot_id,batch_id 0 63 miss% 0.04096770545507146
plot_id,batch_id 0 64 miss% 0.0480588045233472
plot_id,batch_id 0 65 miss% 0.08031829187511523
plot_id,batch_id 0 66 miss% 0.061268201115558155
plot_id,batch_id 0 67 miss% 0.03781291382993559
plot_id,batch_id 0 68 miss% 0.03183923300909768
plot_id,batch_id 0 69 the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  120401
Epoch:0, Train loss:0.617554, valid loss:0.615952
Epoch:1, Train loss:0.107453, valid loss:0.004471
Epoch:2, Train loss:0.012966, valid loss:0.003831
Epoch:3, Train loss:0.010643, valid loss:0.003443
Epoch:4, Train loss:0.009105, valid loss:0.002832
Epoch:5, Train loss:0.008075, valid loss:0.002391
Epoch:6, Train loss:0.006241, valid loss:0.001954
Epoch:7, Train loss:0.004948, valid loss:0.001516
Epoch:8, Train loss:0.003908, valid loss:0.001263
Epoch:9, Train loss:0.003705, valid loss:0.001303
Epoch:10, Train loss:0.003542, valid loss:0.001386
Epoch:11, Train loss:0.001703, valid loss:0.000863
Epoch:12, Train loss:0.001588, valid loss:0.000880
Epoch:13, Train loss:0.001570, valid loss:0.000907
Epoch:14, Train loss:0.001520, valid loss:0.000755
Epoch:15, Train loss:0.001431, valid loss:0.000851
Epoch:16, Train loss:0.001473, valid loss:0.000746
Epoch:17, Train loss:0.001431, valid loss:0.000815
Epoch:18, Train loss:0.001404, valid loss:0.001034
Epoch:19, Train loss:0.001303, valid loss:0.000837
Epoch:20, Train loss:0.001332, valid loss:0.000745
Epoch:21, Train loss:0.000984, valid loss:0.000737
Epoch:22, Train loss:0.000954, valid loss:0.000616
Epoch:23, Train loss:0.000948, valid loss:0.000556
Epoch:24, Train loss:0.000945, valid loss:0.000651
Epoch:25, Train loss:0.000883, valid loss:0.000654
Epoch:26, Train loss:0.000895, valid loss:0.000602
Epoch:27, Train loss:0.000887, valid loss:0.000610
Epoch:28, Train loss:0.000847, valid loss:0.000626
Epoch:29, Train loss:0.000844, valid loss:0.000628
Epoch:30, Train loss:0.000826, valid loss:0.000567
Epoch:31, Train loss:0.000670, valid loss:0.000538
Epoch:32, Train loss:0.000669, valid loss:0.000507
Epoch:33, Train loss:0.000648, valid loss:0.000528
Epoch:34, Train loss:0.000644, valid loss:0.000533
Epoch:35, Train loss:0.000632, valid loss:0.000523
Epoch:36, Train loss:0.000637, valid loss:0.000510
Epoch:37, Train loss:0.000622, valid loss:0.000514
Epoch:38, Train loss:0.000622, valid loss:0.000501
Epoch:39, Train loss:0.000619, valid loss:0.000533
Epoch:40, Train loss:0.000591, valid loss:0.000532
Epoch:41, Train loss:0.000531, valid loss:0.000483
Epoch:42, Train loss:0.000534, valid loss:0.000501
Epoch:43, Train loss:0.000520, valid loss:0.000501
Epoch:44, Train loss:0.000526, valid loss:0.000493
Epoch:45, Train loss:0.000515, valid loss:0.000496
Epoch:46, Train loss:0.000516, valid loss:0.000501
Epoch:47, Train loss:0.000513, valid loss:0.000515
Epoch:48, Train loss:0.000507, valid loss:0.000501
Epoch:49, Train loss:0.000503, valid loss:0.000513
Epoch:50, Train loss:0.000515, valid loss:0.000499
Epoch:51, Train loss:0.000472, valid loss:0.000489
Epoch:52, Train loss:0.000467, valid loss:0.000487
Epoch:53, Train loss:0.000465, valid loss:0.000487
Epoch:54, Train loss:0.000464, valid loss:0.000485
Epoch:55, Train loss:0.000462, valid loss:0.000486
Epoch:56, Train loss:0.000461, valid loss:0.000484
Epoch:57, Train loss:0.000461, valid loss:0.000486
Epoch:58, Train loss:0.000460, valid loss:0.000485
Epoch:59, Train loss:0.000459, valid loss:0.000484
Epoch:60, Train loss:0.000459, valid loss:0.000485
training time 14739.905398368835
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07294918759548405
plot_id,batch_id 0 1 miss% 0.04696410455362082
plot_id,batch_id 0 2 miss% 0.1108443355651941
plot_id,batch_id 0 3 miss% 0.042047781336983316
plot_id,batch_id 0 4 miss% 0.04305768328375734
plot_id,batch_id 0 5 miss% 0.03266525184765843
plot_id,batch_id 0 6 miss% 0.02957502256466355
plot_id,batch_id 0 7 miss% 0.09747837047252488
plot_id,batch_id 0 8 miss% 0.0647994938876505
plot_id,batch_id 0 9 miss% 0.08290196360716072
plot_id,batch_id 0 10 miss% 0.029208089815032243
plot_id,batch_id 0 11 miss% 0.0933783933230323
plot_id,batch_id 0 12 miss% 0.07626589989946309
plot_id,batch_id 0 13 miss% 0.061290166878416215
plot_id,batch_id 0 14 miss% 0.10262305475031885
plot_id,batch_id 0 15 miss% 0.03148335660854748
plot_id,batch_id 0 16 miss% 0.12175100859281737
plot_id,batch_id 0 17 miss% 0.04419850349412453
plot_id,batch_id 0 18 miss% 0.106618920278665
plot_id,batch_id 0 19 miss% 0.05687174620987779
plot_id,batch_id 0 20 miss% 0.04444601365176392
plot_id,batch_id 0 21 miss% 0.10578292156359698
plot_id,batch_id 0 22 miss% 0.06291387653176328
plot_id,batch_id 0 23 miss% 0.0363634622042172
plot_id,batch_id 0 24 miss% 0.06626369500533302
plot_id,batch_id 0 25 miss% 0.07189654814341376
plot_id,batch_id 0 26 miss% 0.0553611276015911
plot_id,batch_id 0 27 miss% 0.05152058617188008
plot_id,batch_id 0 28 miss% 0.05582515598368784
plot_id,batch_id 0 29 miss% 0.03100382370516194
plot_id,batch_id 0 30 miss% 0.028899375846948183
plot_id,batch_id 0 31 miss% 0.08268232040795329
plot_id,batch_id 0 32 miss% 0.09840843717135744
plot_id,batch_id 0 33 miss% 0.09689647510597812
plot_id,batch_id 0 34 miss% 0.040754282340075554
plot_id,batch_id 0 35 miss% 0.07726339620387325
plot_id,batch_id 0 36 miss% 0.08540939587466
plot_id,batch_id 0 37 miss% 0.05577636600354187
plot_id,batch_id 0 38 miss% 0.03431117326856509
plot_id,batch_id 0 39 miss% 0.025596691362899562
plot_id,batch_id 0 40 miss% 0.09388617854565137
plot_id,batch_id 0 41 miss% 0.05651407176924382
plot_id,batch_id 0 42 miss% 0.04784281050366973
plot_id,batch_id 0 43 miss% 0.06264533468398409
plot_id,batch_id 0 44 miss% 0.037010625952594745
plot_id,batch_id 0 45 miss% 0.04991038408340562
plot_id,batch_id 0 46 miss% 0.02721508321736022
plot_id,batch_id 0 47 miss% 0.035422007810528025
plot_id,batch_id 0 48 miss% 0.03754994166142607
plot_id,batch_id 0 49 miss% 0.03430841065106615
plot_id,batch_id 0 50 miss% 0.09608146500979117
plot_id,batch_id 0 51 miss% 0.03695319823339133
plot_id,batch_id 0 52 miss% 0.03903123718696943
plot_id,batch_id 0 53 miss% 0.023384582364370445
plot_id,batch_id 0 54 miss% 0.04527477252469159
plot_id,batch_id 0 55 miss% 0.0774983649116171
plot_id,batch_id 0 56 miss% 0.09127649081287156
plot_id,batch_id 0 57 miss% 0.07294610403424444
plot_id,batch_id 0 58 miss% 0.033123510438188015
plot_id,batch_id 0 59 miss% 0.031101606400161033
plot_id,batch_id 0 60 miss% 0.03662619620094868
plot_id,batch_id 0 61 miss% 0.017936000216690395
plot_id,batch_id 0 62 miss% 0.04859684214858616
plot_id,batch_id 0 63 miss% 0.07942855142947292
plot_id,batch_id 0 64 miss% 0.04303158525312922
plot_id,batch_id 0 65 miss% 0.035265830730670224
plot_id,batch_id 0 66 miss% 0.14887791431092642
plot_id,batch_id 0 67the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  154129
Epoch:0, Train loss:0.560482, valid loss:0.581801
Epoch:1, Train loss:0.073506, valid loss:0.003449
Epoch:2, Train loss:0.008785, valid loss:0.002369
Epoch:3, Train loss:0.005999, valid loss:0.001955
Epoch:4, Train loss:0.004656, valid loss:0.001669
Epoch:5, Train loss:0.003751, valid loss:0.001706
Epoch:6, Train loss:0.002674, valid loss:0.001273
Epoch:7, Train loss:0.002401, valid loss:0.001336
Epoch:8, Train loss:0.002166, valid loss:0.001001
Epoch:9, Train loss:0.002017, valid loss:0.001347
Epoch:10, Train loss:0.001939, valid loss:0.001193
Epoch:11, Train loss:0.001449, valid loss:0.000844
Epoch:12, Train loss:0.001385, valid loss:0.000945
Epoch:13, Train loss:0.001327, valid loss:0.000749
Epoch:14, Train loss:0.001279, valid loss:0.001101
Epoch:15, Train loss:0.001260, valid loss:0.000848
Epoch:16, Train loss:0.001214, valid loss:0.000681
Epoch:17, Train loss:0.001154, valid loss:0.000684
Epoch:18, Train loss:0.001150, valid loss:0.000755
Epoch:19, Train loss:0.001106, valid loss:0.000701
Epoch:20, Train loss:0.001077, valid loss:0.000791
Epoch:21, Train loss:0.000870, valid loss:0.000623
Epoch:22, Train loss:0.000837, valid loss:0.000582
Epoch:23, Train loss:0.000821, valid loss:0.000575
Epoch:24, Train loss:0.000817, valid loss:0.000604
Epoch:25, Train loss:0.000797, valid loss:0.000566
Epoch:26, Train loss:0.000766, valid loss:0.000553
Epoch:27, Train loss:0.000750, valid loss:0.000578
Epoch:28, Train loss:0.000743, valid loss:0.000600
Epoch:29, Train loss:0.000743, valid loss:0.000549
Epoch:30, Train loss:0.000708, valid loss:0.000573
Epoch:31, Train loss:0.000608, valid loss:0.000536
Epoch:32, Train loss:0.000604, valid loss:0.000532
Epoch:33, Train loss:0.000592, valid loss:0.000518
Epoch:34, Train loss:0.000601, valid loss:0.000503
Epoch:35, Train loss:0.000579, valid loss:0.000519
Epoch:36, Train loss:0.000588, valid loss:0.000528
Epoch:37, Train loss:0.000570, valid loss:0.000502
Epoch:38, Train loss:0.000568, valid loss:0.000535
Epoch:39, Train loss:0.000562, valid loss:0.000521
Epoch:40, Train loss:0.000561, valid loss:0.000549
Epoch:41, Train loss:0.000508, valid loss:0.000503
Epoch:42, Train loss:0.000504, valid loss:0.000499
Epoch:43, Train loss:0.000502, valid loss:0.000498
Epoch:44, Train loss:0.000502, valid loss:0.000494
Epoch:45, Train loss:0.000497, valid loss:0.000509
Epoch:46, Train loss:0.000490, valid loss:0.000508
Epoch:47, Train loss:0.000488, valid loss:0.000490
Epoch:48, Train loss:0.000490, valid loss:0.000492
Epoch:49, Train loss:0.000484, valid loss:0.000491
Epoch:50, Train loss:0.000483, valid loss:0.000484
Epoch:51, Train loss:0.000455, valid loss:0.000490
Epoch:52, Train loss:0.000452, valid loss:0.000483
Epoch:53, Train loss:0.000450, valid loss:0.000487
Epoch:54, Train loss:0.000450, valid loss:0.000487
Epoch:55, Train loss:0.000449, valid loss:0.000485
Epoch:56, Train loss:0.000448, valid loss:0.000486
Epoch:57, Train loss:0.000448, valid loss:0.000483
Epoch:58, Train loss:0.000448, valid loss:0.000483
Epoch:59, Train loss:0.000448, valid loss:0.000484
Epoch:60, Train loss:0.000447, valid loss:0.000483
training time 14755.319708108902
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07803810982138223
plot_id,batch_id 0 1 miss% 0.03620761510566143
plot_id,batch_id 0 2 miss% 0.09950315982732463
plot_id,batch_id 0 3 miss% 0.07229819683665753
plot_id,batch_id 0 4 miss% 0.055202568790446625
plot_id,batch_id 0 5 miss% 0.06266194117693795
plot_id,batch_id 0 6 miss% 0.05630349402990907
plot_id,batch_id 0 7 miss% 0.07867853717406985
plot_id,batch_id 0 8 miss% 0.07849797020808931
plot_id,batch_id 0 9 miss% 0.025370670164728163
plot_id,batch_id 0 10 miss% 0.036150828335636265
plot_id,batch_id 0 11 miss% 0.08528795817776631
plot_id,batch_id 0 12 miss% 0.09871676517615198
plot_id,batch_id 0 13 miss% 0.049269161494267126
plot_id,batch_id 0 14 miss% 0.10714221580380705
plot_id,batch_id 0 15 miss% 0.06209722059762277
plot_id,batch_id 0 16 miss% 0.1264131229025729
plot_id,batch_id 0 17 miss% 0.03308797138183542
plot_id,batch_id 0 18 miss% 0.057314360050084
plot_id,batch_id 0 19 miss% 0.08884082528316053
plot_id,batch_id 0 20 miss% 0.08347181247072745
plot_id,batch_id 0 21 miss% 0.04835772582915525
plot_id,batch_id 0 22 miss% 0.05779669863653605
plot_id,batch_id 0 23 miss% 0.06583644505252279
plot_id,batch_id 0 24 miss% 0.05774029181941694
plot_id,batch_id 0 25 miss% 0.07369530897416209
plot_id,batch_id 0 26 miss% 0.0903697687842217
plot_id,batch_id 0 27 miss% 0.05172162149669126
plot_id,batch_id 0 28 miss% 0.049827917637457794
plot_id,batch_id 0 29 miss% 0.04214037608353936
plot_id,batch_id 0 30 miss% 0.026178653490572863
plot_id,batch_id 0 31 miss% 0.14431366356344955
plot_id,batch_id 0 32 miss% 0.12467384127844909
plot_id,batch_id 0 33 miss% 0.07239814467593521
plot_id,batch_id 0 34 miss% 0.02996244293553809
plot_id,batch_id 0 35 miss% 0.03251598296514488
plot_id,batch_id 0 36 miss% 0.09286937942313317
plot_id,batch_id 0 37 miss% 0.07161258324199178
plot_id,batch_id 0 38 miss% 0.03655444118931479
plot_id,batch_id 0 39 miss% 0.022374513118337616
plot_id,batch_id 0 40 miss% 0.14223912531898528
plot_id,batch_id 0 41 miss% 0.06584252411046429
plot_id,batch_id 0 42 miss% 0.02026377964125767
plot_id,batch_id 0 43 miss% 0.05205962311933867
plot_id,batch_id 0 44 miss% 0.04877483651210917
plot_id,batch_id 0 45 miss% 0.1141452008639986
plot_id,batch_id 0 46 miss% 0.03984676784702671
plot_id,batch_id 0 47 miss% 0.02978986026627929
plot_id,batch_id 0 48 miss% 0.02652065334268409
plot_id,batch_id 0 49 miss% 0.03802509516035601
plot_id,batch_id 0 50 miss% 0.15518998556150837
plot_id,batch_id 0 51 miss% 0.03607883090151132
plot_id,batch_id 0 52 miss% 0.022825778714707343
plot_id,batch_id 0 53 miss% 0.027864542324749775
plot_id,batch_id 0 54 miss% 0.04814757183650842
plot_id,batch_id 0 55 miss% 0.08320883738804714
plot_id,batch_id 0 56 miss% 0.0824714109016134
plot_id,batch_id 0 57 miss% 0.04731195088190694
plot_id,batch_id 0 58 miss% 0.04588622857524481
plot_id,batch_id 0 59 miss% 0.04966214604920602
plot_id,batch_id 0 60 miss% 0.04273402634589793
plot_id,batch_id 0 61 miss% 0.04180173514980393
plot_id,batch_id 0 62 miss% 0.04467934364911091
plot_id,batch_id 0 63 miss% 0.02954158277734864
plot_id,batch_id 0 64 miss% 0.0671160456135297
plot_id,batch_id 0 65 miss% 0.06578636989864958
plot_id,batch_id 0 66 miss% 0.02592560626948714
plot_id,batch_id 0 miss% 0.03394318637254693
plot_id,batch_id 0 70 miss% 0.03826893239831906
plot_id,batch_id 0 71 miss% 0.04567723553101838
plot_id,batch_id 0 72 miss% 0.13693230628062003
plot_id,batch_id 0 73 miss% 0.06699754861206042
plot_id,batch_id 0 74 miss% 0.06458992933572567
plot_id,batch_id 0 75 miss% 0.06593902724102658
plot_id,batch_id 0 76 miss% 0.04882554889652169
plot_id,batch_id 0 77 miss% 0.06267990904408499
plot_id,batch_id 0 78 miss% 0.03679331871023944
plot_id,batch_id 0 79 miss% 0.05927639156347671
plot_id,batch_id 0 80 miss% 0.05580078342501689
plot_id,batch_id 0 81 miss% 0.09967023852580206
plot_id,batch_id 0 82 miss% 0.05255556866493999
plot_id,batch_id 0 83 miss% 0.047300226899175776
plot_id,batch_id 0 84 miss% 0.06192433250261785
plot_id,batch_id 0 85 miss% 0.04278737351568951
plot_id,batch_id 0 86 miss% 0.07724454351279365
plot_id,batch_id 0 87 miss% 0.10711859833293857
plot_id,batch_id 0 88 miss% 0.0798566642127694
plot_id,batch_id 0 89 miss% 0.05414976020872648
plot_id,batch_id 0 90 miss% 0.03257907372226933
plot_id,batch_id 0 91 miss% 0.048331556016287164
plot_id,batch_id 0 92 miss% 0.07063019746758538
plot_id,batch_id 0 93 miss% 0.05624597132714554
plot_id,batch_id 0 94 miss% 0.0895657595261549
plot_id,batch_id 0 95 miss% 0.08222473253126544
plot_id,batch_id 0 96 miss% 0.03926073177161781
plot_id,batch_id 0 97 miss% 0.03726959708796273
plot_id,batch_id 0 98 miss% 0.04185509221307789
plot_id,batch_id 0 99 miss% 0.08307574734173598
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06880603 0.03933929 0.10402951 0.03607855 0.08131905 0.06723254
 0.07277017 0.09940771 0.08564675 0.03804259 0.02270393 0.05175941
 0.0966426  0.0660569  0.08772519 0.05801043 0.07709814 0.03099588
 0.07856402 0.08140825 0.11377033 0.0362859  0.08826335 0.04527122
 0.06897015 0.06748293 0.02553046 0.05112546 0.04202698 0.05113636
 0.06759858 0.09921781 0.09014116 0.07670868 0.08718964 0.06880677
 0.06597199 0.07644374 0.04619218 0.05956712 0.09919697 0.08812315
 0.05651007 0.07073231 0.06721642 0.02887054 0.0513209  0.04227981
 0.04915314 0.04277495 0.12777527 0.06491172 0.05088207 0.03825602
 0.07109019 0.05024347 0.06555285 0.06576593 0.03183543 0.05415071
 0.04598375 0.02752808 0.0754319  0.04096771 0.0480588  0.08031829
 0.0612682  0.03781291 0.03183923 0.03394319 0.03826893 0.04567724
 0.13693231 0.06699755 0.06458993 0.06593903 0.04882555 0.06267991
 0.03679332 0.05927639 0.05580078 0.09967024 0.05255557 0.04730023
 0.06192433 0.04278737 0.07724454 0.1071186  0.07985666 0.05414976
 0.03257907 0.04833156 0.0706302  0.05624597 0.08956576 0.08222473
 0.03926073 0.0372696  0.04185509 0.08307575]
for model  22 the mean error 0.06256558428201166
all id 22 hidden_dim 24 learning_rate 0.0025 num_layers 5 frames 21 out win 5 err 0.06256558428201166 time 14712.331038951874
Launcher: Job 23 completed in 14979 seconds.
Launcher: Task 78 done. Exiting.
 miss% 0.047026441281794104
plot_id,batch_id 0 68 miss% 0.024274007498186416
plot_id,batch_id 0 69 miss% 0.06314199717482862
plot_id,batch_id 0 70 miss% 0.04554277029367544
plot_id,batch_id 0 71 miss% 0.054967442074287484
plot_id,batch_id 0 72 miss% 0.09929792403491906
plot_id,batch_id 0 73 miss% 0.07740569357174654
plot_id,batch_id 0 74 miss% 0.13377283566245887
plot_id,batch_id 0 75 miss% 0.07252672038025566
plot_id,batch_id 0 76 miss% 0.05533468793937738
plot_id,batch_id 0 77 miss% 0.042737056629257114
plot_id,batch_id 0 78 miss% 0.050727908538448684
plot_id,batch_id 0 79 miss% 0.08579115437777718
plot_id,batch_id 0 80 miss% 0.051531188430071595
plot_id,batch_id 0 81 miss% 0.07965807905342867
plot_id,batch_id 0 82 miss% 0.07408217031654914
plot_id,batch_id 0 83 miss% 0.09114354269448494
plot_id,batch_id 0 84 miss% 0.08831943355815555
plot_id,batch_id 0 85 miss% 0.04094471184156917
plot_id,batch_id 0 86 miss% 0.03624583005229889
plot_id,batch_id 0 87 miss% 0.03987273376455425
plot_id,batch_id 0 88 miss% 0.07839573932798044
plot_id,batch_id 0 89 miss% 0.0926590651470074
plot_id,batch_id 0 90 miss% 0.035729282928262183
plot_id,batch_id 0 91 miss% 0.06912156386530126
plot_id,batch_id 0 92 miss% 0.058577709064486366
plot_id,batch_id 0 93 miss% 0.04651856985399825
plot_id,batch_id 0 94 miss% 0.07400110949262653
plot_id,batch_id 0 95 miss% 0.10451053328591627
plot_id,batch_id 0 96 miss% 0.07018395949487236
plot_id,batch_id 0 97 miss% 0.038362585155233934
plot_id,batch_id 0 98 miss% 0.06056785539798923
plot_id,batch_id 0 99 miss% 0.06966510303212475
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07294919 0.0469641  0.11084434 0.04204778 0.04305768 0.03266525
 0.02957502 0.09747837 0.06479949 0.08290196 0.02920809 0.09337839
 0.0762659  0.06129017 0.10262305 0.03148336 0.12175101 0.0441985
 0.10661892 0.05687175 0.04444601 0.10578292 0.06291388 0.03636346
 0.0662637  0.07189655 0.05536113 0.05152059 0.05582516 0.03100382
 0.02889938 0.08268232 0.09840844 0.09689648 0.04075428 0.0772634
 0.0854094  0.05577637 0.03431117 0.02559669 0.09388618 0.05651407
 0.04784281 0.06264533 0.03701063 0.04991038 0.02721508 0.03542201
 0.03754994 0.03430841 0.09608147 0.0369532  0.03903124 0.02338458
 0.04527477 0.07749836 0.09127649 0.0729461  0.03312351 0.03110161
 0.0366262  0.017936   0.04859684 0.07942855 0.04303159 0.03526583
 0.14887791 0.04702644 0.02427401 0.063142   0.04554277 0.05496744
 0.09929792 0.07740569 0.13377284 0.07252672 0.05533469 0.04273706
 0.05072791 0.08579115 0.05153119 0.07965808 0.07408217 0.09114354
 0.08831943 0.04094471 0.03624583 0.03987273 0.07839574 0.09265907
 0.03572928 0.06912156 0.05857771 0.04651857 0.07400111 0.10451053
 0.07018396 0.03836259 0.06056786 0.0696651 ]
for model  147 the mean error 0.06145723965012828
all id 147 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 25 out win 4 err 0.06145723965012828 time 14739.905398368835
Launcher: Job 148 completed in 15002 seconds.
Launcher: Task 70 done. Exiting.
67 miss% 0.024203246045898588
plot_id,batch_id 0 68 miss% 0.03945770531823653
plot_id,batch_id 0 69 miss% 0.06009718437208505
plot_id,batch_id 0 70 miss% 0.051283192689618694
plot_id,batch_id 0 71 miss% 0.031302156189741755
plot_id,batch_id 0 72 miss% 0.10636162582803661
plot_id,batch_id 0 73 miss% 0.03459865459400083
plot_id,batch_id 0 74 miss% 0.12644436767033448
plot_id,batch_id 0 75 miss% 0.05300371188453119
plot_id,batch_id 0 76 miss% 0.10353387686180517
plot_id,batch_id 0 77 miss% 0.024430125285749497
plot_id,batch_id 0 78 miss% 0.03811015332992209
plot_id,batch_id 0 79 miss% 0.10776331975687518
plot_id,batch_id 0 80 miss% 0.05151336391034775
plot_id,batch_id 0 81 miss% 0.09439807552412427
plot_id,batch_id 0 82 miss% 0.034107714319101326
plot_id,batch_id 0 83 miss% 0.08086523716724905
plot_id,batch_id 0 84 miss% 0.09446927037000215
plot_id,batch_id 0 85 miss% 0.02667983533692257
plot_id,batch_id 0 86 miss% 0.07151759522305264
plot_id,batch_id 0 87 miss% 0.0687922316731862
plot_id,batch_id 0 88 miss% 0.07860875037538528
plot_id,batch_id 0 89 miss% 0.04298022103122662
plot_id,batch_id 0 90 miss% 0.04088271731848521
plot_id,batch_id 0 91 miss% 0.03430600617618659
plot_id,batch_id 0 92 miss% 0.03452144053219463
plot_id,batch_id 0 93 miss% 0.03727805813115475
plot_id,batch_id 0 94 miss% 0.04856901836992759
plot_id,batch_id 0 95 miss% 0.0435084562129274
plot_id,batch_id 0 96 miss% 0.05169923602351013
plot_id,batch_id 0 97 miss% 0.028532111830899633
plot_id,batch_id 0 98 miss% 0.04047374346766088
plot_id,batch_id 0 99 miss% 0.09533950576201453
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07803811 0.03620762 0.09950316 0.0722982  0.05520257 0.06266194
 0.05630349 0.07867854 0.07849797 0.02537067 0.03615083 0.08528796
 0.09871677 0.04926916 0.10714222 0.06209722 0.12641312 0.03308797
 0.05731436 0.08884083 0.08347181 0.04835773 0.0577967  0.06583645
 0.05774029 0.07369531 0.09036977 0.05172162 0.04982792 0.04214038
 0.02617865 0.14431366 0.12467384 0.07239814 0.02996244 0.03251598
 0.09286938 0.07161258 0.03655444 0.02237451 0.14223913 0.06584252
 0.02026378 0.05205962 0.04877484 0.1141452  0.03984677 0.02978986
 0.02652065 0.0380251  0.15518999 0.03607883 0.02282578 0.02786454
 0.04814757 0.08320884 0.08247141 0.04731195 0.04588623 0.04966215
 0.04273403 0.04180174 0.04467934 0.02954158 0.06711605 0.06578637
 0.02592561 0.02420325 0.03945771 0.06009718 0.05128319 0.03130216
 0.10636163 0.03459865 0.12644437 0.05300371 0.10353388 0.02443013
 0.03811015 0.10776332 0.05151336 0.09439808 0.03410771 0.08086524
 0.09446927 0.02667984 0.0715176  0.06879223 0.07860875 0.04298022
 0.04088272 0.03430601 0.03452144 0.03727806 0.04856902 0.04350846
 0.05169924 0.02853211 0.04047374 0.09533951]
for model  87 the mean error 0.06054865672608135
all id 87 hidden_dim 32 learning_rate 0.0025 num_layers 3 frames 25 out win 4 err 0.06054865672608135 time 14755.319708108902
Launcher: Job 88 completed in 15020 seconds.
Launcher: Task 43 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  151697
Epoch:0, Train loss:0.669293, valid loss:0.625895
Epoch:1, Train loss:0.342494, valid loss:0.005491
Epoch:2, Train loss:0.014158, valid loss:0.004072
Epoch:3, Train loss:0.012201, valid loss:0.003859
Epoch:4, Train loss:0.011455, valid loss:0.003618
Epoch:5, Train loss:0.010671, valid loss:0.003198
Epoch:6, Train loss:0.009077, valid loss:0.002851
Epoch:7, Train loss:0.007808, valid loss:0.002641
Epoch:8, Train loss:0.006864, valid loss:0.002221
Epoch:9, Train loss:0.005942, valid loss:0.002343
Epoch:10, Train loss:0.005682, valid loss:0.002202
Epoch:11, Train loss:0.004854, valid loss:0.001723
Epoch:12, Train loss:0.004640, valid loss:0.001747
Epoch:13, Train loss:0.004529, valid loss:0.001855
Epoch:14, Train loss:0.004405, valid loss:0.002110
Epoch:15, Train loss:0.004249, valid loss:0.001690
Epoch:16, Train loss:0.004093, valid loss:0.001624
Epoch:17, Train loss:0.004018, valid loss:0.001638
Epoch:18, Train loss:0.003058, valid loss:0.001414
Epoch:19, Train loss:0.002380, valid loss:0.001388
Epoch:20, Train loss:0.002262, valid loss:0.001351
Epoch:21, Train loss:0.001904, valid loss:0.001224
Epoch:22, Train loss:0.001788, valid loss:0.001197
Epoch:23, Train loss:0.001737, valid loss:0.001147
Epoch:24, Train loss:0.001690, valid loss:0.001273
Epoch:25, Train loss:0.001648, valid loss:0.001224
Epoch:26, Train loss:0.001604, valid loss:0.001116
Epoch:27, Train loss:0.001618, valid loss:0.001157
Epoch:28, Train loss:0.001532, valid loss:0.001124
Epoch:29, Train loss:0.001530, valid loss:0.001144
Epoch:30, Train loss:0.001449, valid loss:0.001096
Epoch:31, Train loss:0.001285, valid loss:0.001042
Epoch:32, Train loss:0.001243, valid loss:0.001090
Epoch:33, Train loss:0.001260, valid loss:0.001056
Epoch:34, Train loss:0.001215, valid loss:0.001062
Epoch:35, Train loss:0.001213, valid loss:0.001083
Epoch:36, Train loss:0.001186, valid loss:0.001083
Epoch:37, Train loss:0.001181, valid loss:0.001064
Epoch:38, Train loss:0.001154, valid loss:0.001024
Epoch:39, Train loss:0.001148, valid loss:0.001029
Epoch:40, Train loss:0.001135, valid loss:0.001121
Epoch:41, Train loss:0.001045, valid loss:0.000985
Epoch:42, Train loss:0.001020, valid loss:0.000958
Epoch:43, Train loss:0.001016, valid loss:0.001002
Epoch:44, Train loss:0.001006, valid loss:0.001027
Epoch:45, Train loss:0.000998, valid loss:0.000978
Epoch:46, Train loss:0.000986, valid loss:0.001017
Epoch:47, Train loss:0.001002, valid loss:0.000992
Epoch:48, Train loss:0.000984, valid loss:0.000962
Epoch:49, Train loss:0.000974, valid loss:0.000988
Epoch:50, Train loss:0.000956, valid loss:0.000972
Epoch:51, Train loss:0.000904, valid loss:0.000957
Epoch:52, Train loss:0.000897, valid loss:0.000963
Epoch:53, Train loss:0.000894, valid loss:0.000952
Epoch:54, Train loss:0.000893, valid loss:0.000954
Epoch:55, Train loss:0.000893, valid loss:0.000959
Epoch:56, Train loss:0.000891, valid loss:0.000965
Epoch:57, Train loss:0.000891, valid loss:0.000957
Epoch:58, Train loss:0.000890, valid loss:0.000956
Epoch:59, Train loss:0.000890, valid loss:0.000957
Epoch:60, Train loss:0.000890, valid loss:0.000956
training time 14868.028490781784
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.056801394507345906
plot_id,batch_id 0 1 miss% 0.06213737771402992
plot_id,batch_id 0 2 miss% 0.049634757004017774
plot_id,batch_id 0 3 miss% 0.035349398852047194
plot_id,batch_id 0 4 miss% 0.060086176846307945
plot_id,batch_id 0 5 miss% 0.06387459304080012
plot_id,batch_id 0 6 miss% 0.048998390991486646
plot_id,batch_id 0 7 miss% 0.09204088004200027
plot_id,batch_id 0 8 miss% 0.08040980364629034
plot_id,batch_id 0 9 miss% 0.040826793837689716
plot_id,batch_id 0 10 miss% 0.03795042121634608
plot_id,batch_id 0 11 miss% 0.05639159649275419
plot_id,batch_id 0 12 miss% 0.07450635197283112
plot_id,batch_id 0 13 miss% 0.052274469956616165
plot_id,batch_id 0 14 miss% 0.07993451717059245
plot_id,batch_id 0 15 miss% 0.02282979584730353
plot_id,batch_id 0 16 miss% 0.08317216677003088
plot_id,batch_id 0 17 miss% 0.047366761323736496
plot_id,batch_id 0 18 miss% 0.04705372212108802
plot_id,batch_id 0 19 miss% 0.09203452889884928
plot_id,batch_id 0 20 miss% 0.0805593002137881
plot_id,batch_id 0 21 miss% 0.046782336054066995
plot_id,batch_id 0 22 miss% 0.08848081911916823
plot_id,batch_id 0 23 miss% 0.052034061059776475
plot_id,batch_id 0 24 miss% 0.10028420549853695
plot_id,batch_id 0 25 miss% 0.044796130878407864
plot_id,batch_id 0 26 miss% 0.039199732839986956
plot_id,batch_id 0 27 miss% 0.04305701749353937
plot_id,batch_id 0 28 miss% 0.03339709573180519
plot_id,batch_id 0 29 miss% 0.03202898356998491
plot_id,batch_id 0 30 miss% 0.03234730047133116
plot_id,batch_id 0 31 miss% 0.07161168130288
plot_id,batch_id 0 32 miss% 0.1032398147655815
plot_id,batch_id 0 33 miss% 0.08324070860123471
plot_id,batch_id 0 34 miss% 0.056023179093876135
plot_id,batch_id 0 35 miss% 0.04792070765371684
plot_id,batch_id 0 36 miss% 0.09950245030018115
plot_id,batch_id 0 37 miss% 0.0571579368779837
plot_id,batch_id 0 38 miss% 0.03403338291359804
plot_id,batch_id 0 39 miss% 0.045242207241178264
plot_id,batch_id 0 40 miss% 0.044100427856866145
plot_id,batch_id 0 41 miss% 0.04742089050226626
plot_id,batch_id 0 42 miss% 0.025877087751473526
plot_id,batch_id 0 43 miss% 0.06751334745690057
plot_id,batch_id 0 44 miss% 0.021434324829476024
plot_id,batch_id 0 45 miss% 0.0383806008115054
plot_id,batch_id 0 46 miss% 0.03256015574713528
plot_id,batch_id 0 47 miss% 0.031738735737536876
plot_id,batch_id 0 48 miss% 0.02677628145594886
plot_id,batch_id 0 49 miss% 0.029061771144209308
plot_id,batch_id 0 50 miss% 0.09411550991414042
plot_id,batch_id 0 51 miss% 0.04061768198055951
plot_id,batch_id 0 52 miss% 0.024470628210382823
plot_id,batch_id 0 53 miss% 0.04596257500519019
plot_id,batch_id 0 54 miss% 0.0284091931406713
plot_id,batch_id 0 55 miss% 0.04032473628125218
plot_id,batch_id 0 56 miss% 0.07995987951202198
plot_id,batch_id 0 57 miss% 0.04916543684799982
plot_id,batch_id 0 58 miss% 0.02324440957195804
plot_id,batch_id 0 59 miss% 0.02079382522286845
plot_id,batch_id 0 60 miss% 0.07010790491844518
plot_id,batch_id 0 61 miss% 0.056525763511658916
plot_id,batch_id 0 62 miss% 0.09456028984438017
plot_id,batch_id 0 63 miss% 0.03776997496894706
plot_id,batch_id 0 64 miss% 0.045545502241807485
plot_id,batch_id 0 65 miss% 0.042087234444654585
plot_id,batch_id 0 66 miss% 0.14394853189586632
plot_id,batch_id 0 67 miss% 0.03434194028271797
plot_id,batch_id 0 68 miss% 0.021903630293877163the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  154129
Epoch:0, Train loss:0.560482, valid loss:0.581801
Epoch:1, Train loss:0.028389, valid loss:0.002688
Epoch:2, Train loss:0.004786, valid loss:0.001867
Epoch:3, Train loss:0.003823, valid loss:0.001871
Epoch:4, Train loss:0.003238, valid loss:0.001463
Epoch:5, Train loss:0.002916, valid loss:0.001522
Epoch:6, Train loss:0.002650, valid loss:0.001396
Epoch:7, Train loss:0.002620, valid loss:0.001404
Epoch:8, Train loss:0.002342, valid loss:0.001271
Epoch:9, Train loss:0.002139, valid loss:0.001238
Epoch:10, Train loss:0.002110, valid loss:0.000926
Epoch:11, Train loss:0.001479, valid loss:0.000789
Epoch:12, Train loss:0.001443, valid loss:0.001149
Epoch:13, Train loss:0.001354, valid loss:0.000843
Epoch:14, Train loss:0.001393, valid loss:0.001131
Epoch:15, Train loss:0.001366, valid loss:0.000767
Epoch:16, Train loss:0.001251, valid loss:0.000826
Epoch:17, Train loss:0.001269, valid loss:0.000801
Epoch:18, Train loss:0.001219, valid loss:0.000782
Epoch:19, Train loss:0.001206, valid loss:0.000690
Epoch:20, Train loss:0.001162, valid loss:0.000738
Epoch:21, Train loss:0.000840, valid loss:0.000667
Epoch:22, Train loss:0.000821, valid loss:0.000600
Epoch:23, Train loss:0.000780, valid loss:0.000612
Epoch:24, Train loss:0.000779, valid loss:0.000542
Epoch:25, Train loss:0.000775, valid loss:0.000576
Epoch:26, Train loss:0.000755, valid loss:0.000588
Epoch:27, Train loss:0.000742, valid loss:0.000615
Epoch:28, Train loss:0.000746, valid loss:0.000594
Epoch:29, Train loss:0.000713, valid loss:0.000596
Epoch:30, Train loss:0.000696, valid loss:0.000604
Epoch:31, Train loss:0.000569, valid loss:0.000556
Epoch:32, Train loss:0.000538, valid loss:0.000467
Epoch:33, Train loss:0.000545, valid loss:0.000497
Epoch:34, Train loss:0.000528, valid loss:0.000496
Epoch:35, Train loss:0.000534, valid loss:0.000498
Epoch:36, Train loss:0.000548, valid loss:0.000501
Epoch:37, Train loss:0.000510, valid loss:0.000526
Epoch:38, Train loss:0.000527, valid loss:0.000528
Epoch:39, Train loss:0.000508, valid loss:0.000502
Epoch:40, Train loss:0.000495, valid loss:0.000531
Epoch:41, Train loss:0.000436, valid loss:0.000489
Epoch:42, Train loss:0.000427, valid loss:0.000452
Epoch:43, Train loss:0.000432, valid loss:0.000504
Epoch:44, Train loss:0.000428, valid loss:0.000494
Epoch:45, Train loss:0.000423, valid loss:0.000488
Epoch:46, Train loss:0.000424, valid loss:0.000523
Epoch:47, Train loss:0.000415, valid loss:0.000473
Epoch:48, Train loss:0.000411, valid loss:0.000472
Epoch:49, Train loss:0.000413, valid loss:0.000457
Epoch:50, Train loss:0.000420, valid loss:0.000461
Epoch:51, Train loss:0.000385, valid loss:0.000464
Epoch:52, Train loss:0.000382, valid loss:0.000464
Epoch:53, Train loss:0.000380, valid loss:0.000461
Epoch:54, Train loss:0.000380, valid loss:0.000461
Epoch:55, Train loss:0.000379, valid loss:0.000460
Epoch:56, Train loss:0.000378, valid loss:0.000462
Epoch:57, Train loss:0.000378, valid loss:0.000463
Epoch:58, Train loss:0.000378, valid loss:0.000463
Epoch:59, Train loss:0.000377, valid loss:0.000461
Epoch:60, Train loss:0.000377, valid loss:0.000461
training time 14896.193536996841
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.0711959408892667
plot_id,batch_id 0 1 miss% 0.05432993585014017
plot_id,batch_id 0 2 miss% 0.06923637601267567
plot_id,batch_id 0 3 miss% 0.05576886112634621
plot_id,batch_id 0 4 miss% 0.0618397229949074
plot_id,batch_id 0 5 miss% 0.05201347810885096
plot_id,batch_id 0 6 miss% 0.07123583597235339
plot_id,batch_id 0 7 miss% 0.08708677018305196
plot_id,batch_id 0 8 miss% 0.09075612776578186
plot_id,batch_id 0 9 miss% 0.10061980759469305
plot_id,batch_id 0 10 miss% 0.0295986311566643
plot_id,batch_id 0 11 miss% 0.10198003804524836
plot_id,batch_id 0 12 miss% 0.06791028420082339
plot_id,batch_id 0 13 miss% 0.07025717923428
plot_id,batch_id 0 14 miss% 0.07114204955968449
plot_id,batch_id 0 15 miss% 0.04587549940985855
plot_id,batch_id 0 16 miss% 0.09808349162407513
plot_id,batch_id 0 17 miss% 0.050959915155154335
plot_id,batch_id 0 18 miss% 0.09058497849626328
plot_id,batch_id 0 19 miss% 0.10244673354408594
plot_id,batch_id 0 20 miss% 0.0738337086958238
plot_id,batch_id 0 21 miss% 0.03490659424566767
plot_id,batch_id 0 22 miss% 0.05462290391870814
plot_id,batch_id 0 23 miss% 0.05056745973709594
plot_id,batch_id 0 24 miss% 0.03938067276097193
plot_id,batch_id 0 25 miss% 0.0792587802194093
plot_id,batch_id 0 26 miss% 0.07491714748399846
plot_id,batch_id 0 27 miss% 0.04835210353301081
plot_id,batch_id 0 28 miss% 0.052045221882266256
plot_id,batch_id 0 29 miss% 0.04785073646707616
plot_id,batch_id 0 30 miss% 0.04565751701640863
plot_id,batch_id 0 31 miss% 0.07703601502702251
plot_id,batch_id 0 32 miss% 0.11997210160553633
plot_id,batch_id 0 33 miss% 0.04844238092496711
plot_id,batch_id 0 34 miss% 0.03479930833275204
plot_id,batch_id 0 35 miss% 0.030241167891892667
plot_id,batch_id 0 36 miss% 0.07396272653706487
plot_id,batch_id 0 37 miss% 0.0738736936653802
plot_id,batch_id 0 38 miss% 0.052324545087782656
plot_id,batch_id 0 39 miss% 0.07579858656022107
plot_id,batch_id 0 40 miss% 0.04523769392737421
plot_id,batch_id 0 41 miss% 0.0453753151046997
plot_id,batch_id 0 42 miss% 0.02088399908531505
plot_id,batch_id 0 43 miss% 0.0496017781409944
plot_id,batch_id 0 44 miss% 0.01818801994305778
plot_id,batch_id 0 45 miss% 0.024435767176278652
plot_id,batch_id 0 46 miss% 0.05281895333682947
plot_id,batch_id 0 47 miss% 0.03401211754413261
plot_id,batch_id 0 48 miss% 0.03894298763118595
plot_id,batch_id 0 49 miss% 0.028038752806530853
plot_id,batch_id 0 50 miss% 0.11352900787936594
plot_id,batch_id 0 51 miss% 0.043629124262833156
plot_id,batch_id 0 52 miss% 0.04961613768431325
plot_id,batch_id 0 53 miss% 0.02308710898800617
plot_id,batch_id 0 54 miss% 0.035334688485595275
plot_id,batch_id 0 55 miss% 0.061963606178898574
plot_id,batch_id 0 56 miss% 0.0854664591905014
plot_id,batch_id 0 57 miss% 0.05646361913958464
plot_id,batch_id 0 58 miss% 0.038847083130412595
plot_id,batch_id 0 59 miss% 0.03372238420388264
plot_id,batch_id 0 60 miss% 0.03536506152751008
plot_id,batch_id 0 61 miss% 0.027472743404746952
plot_id,batch_id 0 62 miss% 0.0584343118589564
plot_id,batch_id 0 63 miss% 0.07271531339379038
plot_id,batch_id 0 64 miss% 0.10837156356459168
plot_id,batch_id 0 65 miss% 0.04426663873340962
plot_id,batch_id 0 66 miss% 0.024843987914572978
plot_id,batch_id 0 67 
plot_id,batch_id 0 69 miss% 0.117571098796479
plot_id,batch_id 0 70 miss% 0.12635534122928
plot_id,batch_id 0 71 miss% 0.03081303962835591
plot_id,batch_id 0 72 miss% 0.0968409240724386
plot_id,batch_id 0 73 miss% 0.04262718588448559
plot_id,batch_id 0 74 miss% 0.10561954245763276
plot_id,batch_id 0 75 miss% 0.06405246892964411
plot_id,batch_id 0 76 miss% 0.10897176208810537
plot_id,batch_id 0 77 miss% 0.05149437823739821
plot_id,batch_id 0 78 miss% 0.025597224039349886
plot_id,batch_id 0 79 miss% 0.06480347088058738
plot_id,batch_id 0 80 miss% 0.03709155171479337
plot_id,batch_id 0 81 miss% 0.08541902988322275
plot_id,batch_id 0 82 miss% 0.040106126077033406
plot_id,batch_id 0 83 miss% 0.04417101169146913
plot_id,batch_id 0 84 miss% 0.059382241281815684
plot_id,batch_id 0 85 miss% 0.040989541342260016
plot_id,batch_id 0 86 miss% 0.06431931513502755
plot_id,batch_id 0 87 miss% 0.09796408264207049
plot_id,batch_id 0 88 miss% 0.08696484561445487
plot_id,batch_id 0 89 miss% 0.06719214276735877
plot_id,batch_id 0 90 miss% 0.03428404418839487
plot_id,batch_id 0 91 miss% 0.0592799843024637
plot_id,batch_id 0 92 miss% 0.029862468175904638
plot_id,batch_id 0 93 miss% 0.058445221409909404
plot_id,batch_id 0 94 miss% 0.058324895097379585
plot_id,batch_id 0 95 miss% 0.06890481508189822
plot_id,batch_id 0 96 miss% 0.03468013582937433
plot_id,batch_id 0 97 miss% 0.05289449165180905
plot_id,batch_id 0 98 miss% 0.038917456985450384
plot_id,batch_id 0 99 miss% 0.05126480390531059
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05680139 0.06213738 0.04963476 0.0353494  0.06008618 0.06387459
 0.04899839 0.09204088 0.0804098  0.04082679 0.03795042 0.0563916
 0.07450635 0.05227447 0.07993452 0.0228298  0.08317217 0.04736676
 0.04705372 0.09203453 0.0805593  0.04678234 0.08848082 0.05203406
 0.10028421 0.04479613 0.03919973 0.04305702 0.0333971  0.03202898
 0.0323473  0.07161168 0.10323981 0.08324071 0.05602318 0.04792071
 0.09950245 0.05715794 0.03403338 0.04524221 0.04410043 0.04742089
 0.02587709 0.06751335 0.02143432 0.0383806  0.03256016 0.03173874
 0.02677628 0.02906177 0.09411551 0.04061768 0.02447063 0.04596258
 0.02840919 0.04032474 0.07995988 0.04916544 0.02324441 0.02079383
 0.0701079  0.05652576 0.09456029 0.03776997 0.0455455  0.04208723
 0.14394853 0.03434194 0.02190363 0.1175711  0.12635534 0.03081304
 0.09684092 0.04262719 0.10561954 0.06405247 0.10897176 0.05149438
 0.02559722 0.06480347 0.03709155 0.08541903 0.04010613 0.04417101
 0.05938224 0.04098954 0.06431932 0.09796408 0.08696485 0.06719214
 0.03428404 0.05927998 0.02986247 0.05844522 0.0583249  0.06890482
 0.03468014 0.05289449 0.03891746 0.0512648 ]
for model  23 the mean error 0.056785358623366616
all id 23 hidden_dim 24 learning_rate 0.0025 num_layers 5 frames 21 out win 6 err 0.056785358623366616 time 14868.028490781784
Launcher: Job 24 completed in 15131 seconds.
Launcher: Task 57 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  151697
Epoch:0, Train loss:0.538751, valid loss:0.495115
Epoch:1, Train loss:0.278155, valid loss:0.003834
Epoch:2, Train loss:0.006305, valid loss:0.002480
Epoch:3, Train loss:0.004331, valid loss:0.001939
Epoch:4, Train loss:0.003638, valid loss:0.001797
Epoch:5, Train loss:0.003196, valid loss:0.001467
Epoch:6, Train loss:0.002757, valid loss:0.001192
Epoch:7, Train loss:0.002504, valid loss:0.001369
Epoch:8, Train loss:0.002261, valid loss:0.001100
Epoch:9, Train loss:0.002086, valid loss:0.001028
Epoch:10, Train loss:0.001898, valid loss:0.001130
Epoch:11, Train loss:0.001438, valid loss:0.000779
Epoch:12, Train loss:0.001410, valid loss:0.000747
Epoch:13, Train loss:0.001335, valid loss:0.000751
Epoch:14, Train loss:0.001304, valid loss:0.000772
Epoch:15, Train loss:0.001241, valid loss:0.000711
Epoch:16, Train loss:0.001216, valid loss:0.000761
Epoch:17, Train loss:0.001165, valid loss:0.000720
Epoch:18, Train loss:0.001147, valid loss:0.000811
Epoch:19, Train loss:0.001097, valid loss:0.000655
Epoch:20, Train loss:0.001055, valid loss:0.000709
Epoch:21, Train loss:0.000842, valid loss:0.000612
Epoch:22, Train loss:0.000826, valid loss:0.000590
Epoch:23, Train loss:0.000815, valid loss:0.000568
Epoch:24, Train loss:0.000797, valid loss:0.000541
Epoch:25, Train loss:0.000796, valid loss:0.000651
Epoch:26, Train loss:0.000781, valid loss:0.000537
Epoch:27, Train loss:0.000755, valid loss:0.000675
Epoch:28, Train loss:0.000750, valid loss:0.000548
Epoch:29, Train loss:0.000742, valid loss:0.000584
Epoch:30, Train loss:0.000725, valid loss:0.000569
Epoch:31, Train loss:0.000634, valid loss:0.000496
Epoch:32, Train loss:0.000605, valid loss:0.000503
Epoch:33, Train loss:0.000616, valid loss:0.000500
Epoch:34, Train loss:0.000608, valid loss:0.000502
Epoch:35, Train loss:0.000595, valid loss:0.000500
Epoch:36, Train loss:0.000597, valid loss:0.000547
Epoch:37, Train loss:0.000582, valid loss:0.000519
Epoch:38, Train loss:0.000589, valid loss:0.000520
Epoch:39, Train loss:0.000571, valid loss:0.000507
Epoch:40, Train loss:0.000579, valid loss:0.000525
Epoch:41, Train loss:0.000523, valid loss:0.000481
Epoch:42, Train loss:0.000519, valid loss:0.000487
Epoch:43, Train loss:0.000522, valid loss:0.000477
Epoch:44, Train loss:0.000516, valid loss:0.000487
Epoch:45, Train loss:0.000511, valid loss:0.000486
Epoch:46, Train loss:0.000510, valid loss:0.000474
Epoch:47, Train loss:0.000501, valid loss:0.000491
Epoch:48, Train loss:0.000509, valid loss:0.000479
Epoch:49, Train loss:0.000497, valid loss:0.000495
Epoch:50, Train loss:0.000495, valid loss:0.000472
Epoch:51, Train loss:0.000469, valid loss:0.000473
Epoch:52, Train loss:0.000465, valid loss:0.000472
Epoch:53, Train loss:0.000463, valid loss:0.000469
Epoch:54, Train loss:0.000462, valid loss:0.000470
Epoch:55, Train loss:0.000462, valid loss:0.000471
Epoch:56, Train loss:0.000461, valid loss:0.000469
Epoch:57, Train loss:0.000461, valid loss:0.000467
Epoch:58, Train loss:0.000461, valid loss:0.000468
Epoch:59, Train loss:0.000460, valid loss:0.000471
Epoch:60, Train loss:0.000460, valid loss:0.000472
training time 14917.824623584747
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.02560940412992857
plot_id,batch_id 0 1 miss% 0.027280785645317843
plot_id,batch_id 0 2 miss% 0.1337910923533457
plot_id,batch_id 0 3 miss% 0.04519043172838269
plot_id,batch_id 0 4 miss% 0.05103529330265824
plot_id,batch_id 0 5 miss% 0.0462859200385935
plot_id,batch_id 0 6 miss% 0.035996973435298306
plot_id,batch_id 0 7 miss% 0.09078861332488765
plot_id,batch_id 0 8 miss% 0.08973312030376844
plot_id,batch_id 0 9 miss% 0.03727924341617198
plot_id,batch_id 0 10 miss% 0.03579964707652071
plot_id,batch_id 0 11 miss% 0.08434232220468697
plot_id,batch_id 0 12 miss% 0.08080514150064032
plot_id,batch_id 0 13 miss% 0.06421110236075506
plot_id,batch_id 0 14 miss% 0.0746510031854539
plot_id,batch_id 0 15 miss% 0.021582297003017527
plot_id,batch_id 0 16 miss% 0.17434455024129147
plot_id,batch_id 0 17 miss% 0.03897696900230143
plot_id,batch_id 0 18 miss% 0.048580466116680564
plot_id,batch_id 0 19 miss% 0.07925496148454086
plot_id,batch_id 0 20 miss% 0.06120837875861796
plot_id,batch_id 0 21 miss% 0.03283948631045574
plot_id,batch_id 0 22 miss% 0.05755934534298463
plot_id,batch_id 0 23 miss% 0.03233523426342431
plot_id,batch_id 0 24 miss% 0.030539469325090697
plot_id,batch_id 0 25 miss% 0.0590709644479257
plot_id,batch_id 0 26 miss% 0.05486661116271571
plot_id,batch_id 0 27 miss% 0.04515901414971852
plot_id,batch_id 0 28 miss% 0.03935866996555284
plot_id,batch_id 0 29 miss% 0.05025895798366912
plot_id,batch_id 0 30 miss% 0.05111432978820103
plot_id,batch_id 0 31 miss% 0.14233040100012376
plot_id,batch_id 0 32 miss% 0.07912658316393886
plot_id,batch_id 0 33 miss% 0.053049397162745725
plot_id,batch_id 0 34 miss% 0.04080001627558021
plot_id,batch_id 0 35 miss% 0.035515704459807465
plot_id,batch_id 0 36 miss% 0.08372166934020094
plot_id,batch_id 0 37 miss% 0.07565447681177406
plot_id,batch_id 0 38 miss% 0.07817660408302879
plot_id,batch_id 0 39 miss% 0.04934148850166966
plot_id,batch_id 0 40 miss% 0.06844895033834258
plot_id,batch_id 0 41 miss% 0.04509788784478133
plot_id,batch_id 0 42 miss% 0.040169380428622066
plot_id,batch_id 0 43 miss% 0.03735043806082289
plot_id,batch_id 0 44 miss% 0.018897164810333697
plot_id,batch_id 0 45 miss% 0.04904659843233346
plot_id,batch_id 0 46 miss% 0.04581974928209375
plot_id,batch_id 0 47 miss% 0.030575923222939446
plot_id,batch_id 0 48 miss% 0.04450071058160419
plot_id,batch_id 0 49 miss% 0.05018944761513574
plot_id,batch_id 0 50 miss% 0.13303026114433794
plot_id,batch_id 0 51 miss% 0.04315162331314556
plot_id,batch_id 0 52 miss% 0.016201177730018976
plot_id,batch_id 0 53 miss% 0.04479793492312427
plot_id,batch_id 0 54 miss% 0.048723819567790594
plot_id,batch_id 0 55 miss% 0.05210464743424763
plot_id,batch_id 0 56 miss% 0.07479530785961903
plot_id,batch_id 0 57 miss% 0.04806375713748099
plot_id,batch_id 0 58 miss% 0.036653684676821535
plot_id,batch_id 0 59 miss% 0.026715072389196386
plot_id,batch_id 0 60 miss% 0.06715262389502595
plot_id,batch_id 0 61 miss% 0.02481860507823074
plot_id,batch_id 0 62 miss% 0.07144157120861085
plot_id,batch_id 0 63 miss% 0.02470777038311797
plot_id,batch_id 0 64 miss% 0.06795175833309323
plot_id,batch_id 0 65 miss% 0.09818910366484108
plot_id,batch_id 0 66 miss% 0.03791954929533828
miss% 0.029044622945878774
plot_id,batch_id 0 68 miss% 0.038669597950091256
plot_id,batch_id 0 69 miss% 0.06363772597397287
plot_id,batch_id 0 70 miss% 0.03460259025850036
plot_id,batch_id 0 71 miss% 0.08276135087027314
plot_id,batch_id 0 72 miss% 0.060270809385121436
plot_id,batch_id 0 73 miss% 0.07926328638343749
plot_id,batch_id 0 74 miss% 0.0639457828942175
plot_id,batch_id 0 75 miss% 0.07461743153394798
plot_id,batch_id 0 76 miss% 0.05637800299072078
plot_id,batch_id 0 77 miss% 0.042805175869707245
plot_id,batch_id 0 78 miss% 0.029567461564460547
plot_id,batch_id 0 79 miss% 0.0845153128741538
plot_id,batch_id 0 80 miss% 0.0631641988563383
plot_id,batch_id 0 81 miss% 0.07980986913550449
plot_id,batch_id 0 82 miss% 0.04498046019396659
plot_id,batch_id 0 83 miss% 0.10209038221597658
plot_id,batch_id 0 84 miss% 0.06581450075700314
plot_id,batch_id 0 85 miss% 0.04163829504919192
plot_id,batch_id 0 86 miss% 0.04101730536078872
plot_id,batch_id 0 87 miss% 0.04627248937774141
plot_id,batch_id 0 88 miss% 0.06413692898557273
plot_id,batch_id 0 89 miss% 0.08598147601397581
plot_id,batch_id 0 90 miss% 0.03260071052235548
plot_id,batch_id 0 91 miss% 0.04200273934325925
plot_id,batch_id 0 92 miss% 0.04110244024270187
plot_id,batch_id 0 93 miss% 0.041918172358662895
plot_id,batch_id 0 94 miss% 0.08407284669555469
plot_id,batch_id 0 95 miss% 0.08938910662245368
plot_id,batch_id 0 96 miss% 0.052697993525060466
plot_id,batch_id 0 97 miss% 0.05064415230965637
plot_id,batch_id 0 98 miss% 0.07318900247490254
plot_id,batch_id 0 99 miss% 0.07892311624764307
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07119594 0.05432994 0.06923638 0.05576886 0.06183972 0.05201348
 0.07123584 0.08708677 0.09075613 0.10061981 0.02959863 0.10198004
 0.06791028 0.07025718 0.07114205 0.0458755  0.09808349 0.05095992
 0.09058498 0.10244673 0.07383371 0.03490659 0.0546229  0.05056746
 0.03938067 0.07925878 0.07491715 0.0483521  0.05204522 0.04785074
 0.04565752 0.07703602 0.1199721  0.04844238 0.03479931 0.03024117
 0.07396273 0.07387369 0.05232455 0.07579859 0.04523769 0.04537532
 0.020884   0.04960178 0.01818802 0.02443577 0.05281895 0.03401212
 0.03894299 0.02803875 0.11352901 0.04362912 0.04961614 0.02308711
 0.03533469 0.06196361 0.08546646 0.05646362 0.03884708 0.03372238
 0.03536506 0.02747274 0.05843431 0.07271531 0.10837156 0.04426664
 0.02484399 0.02904462 0.0386696  0.06363773 0.03460259 0.08276135
 0.06027081 0.07926329 0.06394578 0.07461743 0.056378   0.04280518
 0.02956746 0.08451531 0.0631642  0.07980987 0.04498046 0.10209038
 0.0658145  0.0416383  0.04101731 0.04627249 0.06413693 0.08598148
 0.03260071 0.04200274 0.04110244 0.04191817 0.08407285 0.08938911
 0.05269799 0.05064415 0.073189   0.07892312]
for model  141 the mean error 0.05862954590537425
all id 141 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 25 out win 4 err 0.05862954590537425 time 14896.193536996841
Launcher: Job 142 completed in 15156 seconds.
Launcher: Task 26 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  151697
Epoch:0, Train loss:0.686398, valid loss:0.640620
Epoch:1, Train loss:0.534317, valid loss:0.526761
Epoch:2, Train loss:0.521423, valid loss:0.525682
Epoch:3, Train loss:0.520214, valid loss:0.525601
Epoch:4, Train loss:0.519491, valid loss:0.525522
Epoch:5, Train loss:0.519054, valid loss:0.525321
Epoch:6, Train loss:0.518704, valid loss:0.524991
Epoch:7, Train loss:0.518622, valid loss:0.524900
Epoch:8, Train loss:0.518198, valid loss:0.524997
Epoch:9, Train loss:0.518169, valid loss:0.524938
Epoch:10, Train loss:0.518072, valid loss:0.524952
Epoch:11, Train loss:0.517140, valid loss:0.524493
Epoch:12, Train loss:0.517006, valid loss:0.524444
Epoch:13, Train loss:0.516959, valid loss:0.524454
Epoch:14, Train loss:0.517034, valid loss:0.524426
Epoch:15, Train loss:0.516857, valid loss:0.524477
Epoch:16, Train loss:0.516885, valid loss:0.524451
Epoch:17, Train loss:0.516785, valid loss:0.524471
Epoch:18, Train loss:0.516791, valid loss:0.524522
Epoch:19, Train loss:0.516688, valid loss:0.524384
Epoch:20, Train loss:0.516645, valid loss:0.524376
Epoch:21, Train loss:0.516285, valid loss:0.524347
Epoch:22, Train loss:0.516227, valid loss:0.524139
Epoch:23, Train loss:0.516186, valid loss:0.524237
Epoch:24, Train loss:0.516196, valid loss:0.524150
Epoch:25, Train loss:0.516165, valid loss:0.524229
Epoch:26, Train loss:0.516162, valid loss:0.524269
Epoch:27, Train loss:0.516179, valid loss:0.524182
Epoch:28, Train loss:0.516131, valid loss:0.524375
Epoch:29, Train loss:0.516070, valid loss:0.524167
Epoch:30, Train loss:0.516118, valid loss:0.524196
Epoch:31, Train loss:0.515890, valid loss:0.524095
Epoch:32, Train loss:0.515866, valid loss:0.524067
Epoch:33, Train loss:0.515855, valid loss:0.524110
Epoch:34, Train loss:0.515854, valid loss:0.524096
Epoch:35, Train loss:0.515851, valid loss:0.524066
Epoch:36, Train loss:0.515839, valid loss:0.524135
Epoch:37, Train loss:0.515841, valid loss:0.524065
Epoch:38, Train loss:0.515809, valid loss:0.524181
Epoch:39, Train loss:0.515823, valid loss:0.524110
Epoch:40, Train loss:0.515794, valid loss:0.524086
Epoch:41, Train loss:0.515726, valid loss:0.524086
Epoch:42, Train loss:0.515706, valid loss:0.524051
Epoch:43, Train loss:0.515710, valid loss:0.524047
Epoch:44, Train loss:0.515702, valid loss:0.524079
Epoch:45, Train loss:0.515705, valid loss:0.524074
Epoch:46, Train loss:0.515694, valid loss:0.524063
Epoch:47, Train loss:0.515691, valid loss:0.524081
Epoch:48, Train loss:0.515679, valid loss:0.524042
Epoch:49, Train loss:0.515687, valid loss:0.524133
Epoch:50, Train loss:0.515684, valid loss:0.524030
Epoch:51, Train loss:0.515654, valid loss:0.524045
Epoch:52, Train loss:0.515646, valid loss:0.524046
Epoch:53, Train loss:0.515643, valid loss:0.524042
Epoch:54, Train loss:0.515642, valid loss:0.524042
Epoch:55, Train loss:0.515641, valid loss:0.524045
Epoch:56, Train loss:0.515640, valid loss:0.524040
Epoch:57, Train loss:0.515640, valid loss:0.524044
Epoch:58, Train loss:0.515639, valid loss:0.524041
Epoch:59, Train loss:0.515638, valid loss:0.524043
Epoch:60, Train loss:0.515638, valid loss:0.524041
training time 15046.45157122612
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.8206387054891887
plot_id,batch_id 0 1 miss% 0.7912709446523355
plot_id,batch_id 0 2 miss% 0.7851875410846076
plot_id,batch_id 0 3 miss% 0.7814732696196894
plot_id,batch_id 0 4 miss% 0.7831998426288096
plot_id,batch_id 0 5 miss% 0.8274976651005009
plot_id,batch_id 0 6 miss% 0.7926004709541844
plot_id,batch_id 0 7 miss% 0.7890492945453947
plot_id,batch_id 0 8 miss% 0.7828105938495109
plot_id,batch_id 0 9 miss% 0.7790227032413966
plot_id,batch_id 0 10 miss% 0.8345749571299248
plot_id,batch_id 0 11 miss% 0.7984276540850997
plot_id,batch_id 0 12 miss% 0.7868490426601684
plot_id,batch_id 0 13 miss% 0.7822861374506874
plot_id,batch_id 0 14 miss% 0.7860749542297476
plot_id,batch_id 0 15 miss% 0.8333244531700618
plot_id,batch_id 0 16 miss% 0.7958083172463793
plot_id,batch_id 0 17 miss% 0.789375325642654
plot_id,batch_id 0 18 miss% 0.7843629588551773
plot_id,batch_id 0 19 miss% 0.7908834799982016
plot_id,batch_id 0 20 miss% 0.8033098785712086
plot_id,batch_id 0 21 miss% 0.7868031760257945
plot_id,batch_id 0 22 miss% 0.7848496506021133
plot_id,batch_id 0 23 miss% 0.7820839576572489
plot_id,batch_id 0 24 miss% 0.782175019644454
plot_id,batch_id 0 25 miss% 0.8056634855876432
plot_id,batch_id 0 26 miss% 0.7898774060534476
plot_id,batch_id 0 27 miss% 0.7834112147311815
plot_id,batch_id 0 28 miss% 0.7823948253124223
plot_id,batch_id 0 29 miss% 0.7798062668914817
plot_id,batch_id 0 30 miss% 0.8081730545637023
plot_id,batch_id 0 31 miss% 0.7878728917073068
plot_id,batch_id 0 32 miss% 0.786956777812695
plot_id,batch_id 0 33 miss% 0.7890074222072966
plot_id,batch_id 0 34 miss% 0.7824409852178641
plot_id,batch_id 0 35 miss% 0.8203964847887899
plot_id,batch_id 0 36 miss% 0.7953357953061112
plot_id,batch_id 0 37 miss% 0.784235166505703
plot_id,batch_id 0 38 miss% 0.7871904626537444
plot_id,batch_id 0 39 miss% 0.7812026832660481
plot_id,batch_id 0 40 miss% 0.8026955741160817
plot_id,batch_id 0 41 miss% 0.7832210038788179
plot_id,batch_id 0 42 miss% 0.7797978730055845
plot_id,batch_id 0 43 miss% 0.7795414056058121
plot_id,batch_id 0 44 miss% 0.7776635632230583
plot_id,batch_id 0 45 miss% 0.7903302845349133
plot_id,batch_id 0 46 miss% 0.7824198745386662
plot_id,batch_id 0 47 miss% 0.7813368278108189
plot_id,batch_id 0 48 miss% 0.7813917828695179
plot_id,batch_id 0 49 miss% 0.7801112109497492
plot_id,batch_id 0 50 miss% 0.7910925820248337
plot_id,batch_id 0 51 miss% 0.7829847585176689
plot_id,batch_id 0 52 miss% 0.7830663703147027
plot_id,batch_id 0 53 miss% 0.7809408382614488
plot_id,batch_id 0 54 miss% 0.782807259386517
plot_id,batch_id 0 55 miss% 0.7967408588429621
plot_id,batch_id 0 56 miss% 0.7849941433576872
plot_id,batch_id 0 57 miss% 0.7948588824542842
plot_id,batch_id 0 58 miss% 0.7814618050366297
plot_id,batch_id 0 59 miss% 0.7842630991540601
plot_id,batch_id 0 60 miss% 0.8643905918259318
plot_id,batch_id 0 61 miss% 0.8073300105864092
plot_id,batch_id 0 62 miss% 0.7962608766822966
plot_id,batch_id 0 63 miss% 0.7899654664683434
plot_id,batch_id 0 64 miss% 0.7867644652668869
plot_id,batch_id 0 65 miss% 0.8607694322072681
plot_id,batch_id 0 66 miss% 0.8209026600771689
plot_id,batch_id 0 67 miss% 0.8033343524648502
plot_id,batch_id 0 68 miss% 0.7940233750550595
plot_id,batch_id 0 69 miss% 0.7883268209435281
plot_id,batch_id 0 70 miss% 0.8688043910751482
plot_id,batch_id 0 67 miss% 0.05652874189869261
plot_id,batch_id 0 68 miss% 0.016526589240099417
plot_id,batch_id 0 69 miss% 0.06410837587222054
plot_id,batch_id 0 70 miss% 0.059047897362217765
plot_id,batch_id 0 71 miss% 0.02640490643782554
plot_id,batch_id 0 72 miss% 0.1350039141914753
plot_id,batch_id 0 73 miss% 0.07023426868490207
plot_id,batch_id 0 74 miss% 0.0801485306809302
plot_id,batch_id 0 75 miss% 0.03827870072619969
plot_id,batch_id 0 76 miss% 0.07056588133875963
plot_id,batch_id 0 77 miss% 0.02864972575249463
plot_id,batch_id 0 78 miss% 0.028608883934533747
plot_id,batch_id 0 79 miss% 0.07883911083413157
plot_id,batch_id 0 80 miss% 0.05978109154438237
plot_id,batch_id 0 81 miss% 0.10380400950846683
plot_id,batch_id 0 82 miss% 0.05134444182842766
plot_id,batch_id 0 83 miss% 0.08111957249459147
plot_id,batch_id 0 84 miss% 0.045143725205686526
plot_id,batch_id 0 85 miss% 0.0385487222943302
plot_id,batch_id 0 86 miss% 0.037927828256789434
plot_id,batch_id 0 87 miss% 0.06717112773818062
plot_id,batch_id 0 88 miss% 0.07632824365300554
plot_id,batch_id 0 89 miss% 0.04653420342078987
plot_id,batch_id 0 90 miss% 0.026309287216308378
plot_id,batch_id 0 91 miss% 0.044882738736793944
plot_id,batch_id 0 92 miss% 0.03908196912739031
plot_id,batch_id 0 93 miss% 0.0435567733184381
plot_id,batch_id 0 94 miss% 0.08372349926814346
plot_id,batch_id 0 95 miss% 0.04365270080592183
plot_id,batch_id 0 96 miss% 0.07835181384965982
plot_id,batch_id 0 97 miss% 0.05976530991724324
plot_id,batch_id 0 98 miss% 0.059311053833898134
plot_id,batch_id 0 99 miss% 0.030335230513853492
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0256094  0.02728079 0.13379109 0.04519043 0.05103529 0.04628592
 0.03599697 0.09078861 0.08973312 0.03727924 0.03579965 0.08434232
 0.08080514 0.0642111  0.074651   0.0215823  0.17434455 0.03897697
 0.04858047 0.07925496 0.06120838 0.03283949 0.05755935 0.03233523
 0.03053947 0.05907096 0.05486661 0.04515901 0.03935867 0.05025896
 0.05111433 0.1423304  0.07912658 0.0530494  0.04080002 0.0355157
 0.08372167 0.07565448 0.0781766  0.04934149 0.06844895 0.04509789
 0.04016938 0.03735044 0.01889716 0.0490466  0.04581975 0.03057592
 0.04450071 0.05018945 0.13303026 0.04315162 0.01620118 0.04479793
 0.04872382 0.05210465 0.07479531 0.04806376 0.03665368 0.02671507
 0.06715262 0.02481861 0.07144157 0.02470777 0.06795176 0.0981891
 0.03791955 0.05652874 0.01652659 0.06410838 0.0590479  0.02640491
 0.13500391 0.07023427 0.08014853 0.0382787  0.07056588 0.02864973
 0.02860888 0.07883911 0.05978109 0.10380401 0.05134444 0.08111957
 0.04514373 0.03854872 0.03792783 0.06717113 0.07632824 0.0465342
 0.02630929 0.04488274 0.03908197 0.04355677 0.0837235  0.0436527
 0.07835181 0.05976531 0.05931105 0.03033523]
for model  102 the mean error 0.05653699528289303
all id 102 hidden_dim 24 learning_rate 0.0025 num_layers 5 frames 25 out win 4 err 0.05653699528289303 time 14917.824623584747
Launcher: Job 103 completed in 15181 seconds.
Launcher: Task 154 done. Exiting.
plot_id,batch_id 0 71 miss% 0.8178589121231458
plot_id,batch_id 0 72 miss% 0.7997804217839865
plot_id,batch_id 0 73 miss% 0.7955065489081283
plot_id,batch_id 0 74 miss% 0.790415735983619
plot_id,batch_id 0 75 miss% 0.8611359741769173
plot_id,batch_id 0 76 miss% 0.8112481503855042
plot_id,batch_id 0 77 miss% 0.8030162211627743
plot_id,batch_id 0 78 miss% 0.7951592447406367
plot_id,batch_id 0 79 miss% 0.7918199000680493
plot_id,batch_id 0 80 miss% 0.8507196744378042
plot_id,batch_id 0 81 miss% 0.8015589059039673
plot_id,batch_id 0 82 miss% 0.7889274761377716
plot_id,batch_id 0 83 miss% 0.7895358856473571
plot_id,batch_id 0 84 miss% 0.7815676668170374
plot_id,batch_id 0 85 miss% 0.8461237983503443
plot_id,batch_id 0 86 miss% 0.7985997980610466
plot_id,batch_id 0 87 miss% 0.7932659709148727
plot_id,batch_id 0 88 miss% 0.7868559987899307
plot_id,batch_id 0 89 miss% 0.7835273468213949
plot_id,batch_id 0 90 miss% 0.8641993504449743
plot_id,batch_id 0 91 miss% 0.8069935976312694
plot_id,batch_id 0 92 miss% 0.7962994729342898
plot_id,batch_id 0 93 miss% 0.7910202342924386
plot_id,batch_id 0 94 miss% 0.79372146436777
plot_id,batch_id 0 95 miss% 0.8600398749398119
plot_id,batch_id 0 96 miss% 0.8061259645940209
plot_id,batch_id 0 97 miss% 0.795779726094736
plot_id,batch_id 0 98 miss% 0.788131866964027
plot_id,batch_id 0 99 miss% 0.7900382241217417
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.82063871 0.79127094 0.78518754 0.78147327 0.78319984 0.82749767
 0.79260047 0.78904929 0.78281059 0.7790227  0.83457496 0.79842765
 0.78684904 0.78228614 0.78607495 0.83332445 0.79580832 0.78937533
 0.78436296 0.79088348 0.80330988 0.78680318 0.78484965 0.78208396
 0.78217502 0.80566349 0.78987741 0.78341121 0.78239483 0.77980627
 0.80817305 0.78787289 0.78695678 0.78900742 0.78244099 0.82039648
 0.7953358  0.78423517 0.78719046 0.78120268 0.80269557 0.783221
 0.77979787 0.77954141 0.77766356 0.79033028 0.78241987 0.78133683
 0.78139178 0.78011121 0.79109258 0.78298476 0.78306637 0.78094084
 0.78280726 0.79674086 0.78499414 0.79485888 0.78146181 0.7842631
 0.86439059 0.80733001 0.79626088 0.78996547 0.78676447 0.86076943
 0.82090266 0.80333435 0.79402338 0.78832682 0.86880439 0.81785891
 0.79978042 0.79550655 0.79041574 0.86113597 0.81124815 0.80301622
 0.79515924 0.7918199  0.85071967 0.80155891 0.78892748 0.78953589
 0.78156767 0.8461238  0.7985998  0.79326597 0.786856   0.78352735
 0.86419935 0.8069936  0.79629947 0.79102023 0.79372146 0.86003987
 0.80612596 0.79577973 0.78813187 0.79003822]
for model  76 the mean error 0.7977947076487605
all id 76 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 21 out win 5 err 0.7977947076487605 time 15046.45157122612
Launcher: Job 77 completed in 15185 seconds.
Launcher: Task 29 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  69649
Epoch:0, Train loss:0.593215, valid loss:0.570251
Epoch:1, Train loss:0.043009, valid loss:0.005791
Epoch:2, Train loss:0.012437, valid loss:0.004377
Epoch:3, Train loss:0.009622, valid loss:0.003002
Epoch:4, Train loss:0.008857, valid loss:0.003432
Epoch:5, Train loss:0.006920, valid loss:0.002100
Epoch:6, Train loss:0.003768, valid loss:0.001875
Epoch:7, Train loss:0.003201, valid loss:0.002013
Epoch:8, Train loss:0.002905, valid loss:0.001384
Epoch:9, Train loss:0.002624, valid loss:0.001246
Epoch:10, Train loss:0.002464, valid loss:0.001090
Epoch:11, Train loss:0.001827, valid loss:0.001002
Epoch:12, Train loss:0.001776, valid loss:0.001141
Epoch:13, Train loss:0.001704, valid loss:0.000971
Epoch:14, Train loss:0.001614, valid loss:0.001060
Epoch:15, Train loss:0.001622, valid loss:0.000936
Epoch:16, Train loss:0.001555, valid loss:0.000929
Epoch:17, Train loss:0.001539, valid loss:0.000854
Epoch:18, Train loss:0.001472, valid loss:0.000919
Epoch:19, Train loss:0.001436, valid loss:0.000855
Epoch:20, Train loss:0.001379, valid loss:0.000796
Epoch:21, Train loss:0.001084, valid loss:0.000731
Epoch:22, Train loss:0.001084, valid loss:0.000769
Epoch:23, Train loss:0.001060, valid loss:0.000694
Epoch:24, Train loss:0.001038, valid loss:0.000719
Epoch:25, Train loss:0.001030, valid loss:0.000826
Epoch:26, Train loss:0.001021, valid loss:0.000727
Epoch:27, Train loss:0.000969, valid loss:0.000694
Epoch:28, Train loss:0.000988, valid loss:0.000711
Epoch:29, Train loss:0.000977, valid loss:0.000711
Epoch:30, Train loss:0.000955, valid loss:0.000712
Epoch:31, Train loss:0.000808, valid loss:0.000619
Epoch:32, Train loss:0.000802, valid loss:0.000645
Epoch:33, Train loss:0.000789, valid loss:0.000650
Epoch:34, Train loss:0.000788, valid loss:0.000638
Epoch:35, Train loss:0.000785, valid loss:0.000697
Epoch:36, Train loss:0.000773, valid loss:0.000665
Epoch:37, Train loss:0.000764, valid loss:0.000623
Epoch:38, Train loss:0.000752, valid loss:0.000649
Epoch:39, Train loss:0.000745, valid loss:0.000624
Epoch:40, Train loss:0.000744, valid loss:0.000689
Epoch:41, Train loss:0.000684, valid loss:0.000589
Epoch:42, Train loss:0.000672, valid loss:0.000579
Epoch:43, Train loss:0.000667, valid loss:0.000615
Epoch:44, Train loss:0.000665, valid loss:0.000610
Epoch:45, Train loss:0.000665, valid loss:0.000609
Epoch:46, Train loss:0.000653, valid loss:0.000587
Epoch:47, Train loss:0.000657, valid loss:0.000612
Epoch:48, Train loss:0.000653, valid loss:0.000592
Epoch:49, Train loss:0.000650, valid loss:0.000595
Epoch:50, Train loss:0.000647, valid loss:0.000588
Epoch:51, Train loss:0.000605, valid loss:0.000578
Epoch:52, Train loss:0.000601, valid loss:0.000575
Epoch:53, Train loss:0.000599, valid loss:0.000573
Epoch:54, Train loss:0.000598, valid loss:0.000571
Epoch:55, Train loss:0.000596, valid loss:0.000575
Epoch:56, Train loss:0.000596, valid loss:0.000575
Epoch:57, Train loss:0.000595, valid loss:0.000578
Epoch:58, Train loss:0.000595, valid loss:0.000572
Epoch:59, Train loss:0.000595, valid loss:0.000572
Epoch:60, Train loss:0.000594, valid loss:0.000572
training time 14988.33471941948
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.08422611270400204
plot_id,batch_id 0 1 miss% 0.08500247388405606
plot_id,batch_id 0 2 miss% 0.08353933490865123
plot_id,batch_id 0 3 miss% 0.07025319771874504
plot_id,batch_id 0 4 miss% 0.03863628886932553
plot_id,batch_id 0 5 miss% 0.031549900161259775
plot_id,batch_id 0 6 miss% 0.04342078837389018
plot_id,batch_id 0 7 miss% 0.1119277438961643
plot_id,batch_id 0 8 miss% 0.07042551936045885
plot_id,batch_id 0 9 miss% 0.0320377305422692
plot_id,batch_id 0 10 miss% 0.05008397867347449
plot_id,batch_id 0 11 miss% 0.05466426076736861
plot_id,batch_id 0 12 miss% 0.038269306211356914
plot_id,batch_id 0 13 miss% 0.04506750711489266
plot_id,batch_id 0 14 miss% 0.08899494193993017
plot_id,batch_id 0 15 miss% 0.05577498728063146
plot_id,batch_id 0 16 miss% 0.08877319313095593
plot_id,batch_id 0 17 miss% 0.04769700999795466
plot_id,batch_id 0 18 miss% 0.04936096801653044
plot_id,batch_id 0 19 miss% 0.0715677136202316
plot_id,batch_id 0 20 miss% 0.10743684934476966
plot_id,batch_id 0 21 miss% 0.03818815980341082
plot_id,batch_id 0 22 miss% 0.05212340828443092
plot_id,batch_id 0 23 miss% 0.06172838073483101
plot_id,batch_id 0 24 miss% 0.10202938996781773
plot_id,batch_id 0 25 miss% 0.09145730372916219
plot_id,batch_id 0 26 miss% 0.05834353420338339
plot_id,batch_id 0 27 miss% 0.0409828606958207
plot_id,batch_id 0 28 miss% 0.047216181815865184
plot_id,batch_id 0 29 miss% 0.043260325017780564
plot_id,batch_id 0 30 miss% 0.0669895576044726
plot_id,batch_id 0 31 miss% 0.09194529895407788
plot_id,batch_id 0 32 miss% 0.13873619602257406
plot_id,batch_id 0 33 miss% 0.03238250216198605
plot_id,batch_id 0 34 miss% 0.061759258958343624
plot_id,batch_id 0 35 miss% 0.03700250457363386
plot_id,batch_id 0 36 miss% 0.0746174215677005
plot_id,batch_id 0 37 miss% 0.07207335131291483
plot_id,batch_id 0 38 miss% 0.057859277562666414
plot_id,batch_id 0 39 miss% 0.0565006166313421
plot_id,batch_id 0 40 miss% 0.030227526688407436
plot_id,batch_id 0 41 miss% 0.04272289436134554
plot_id,batch_id 0 42 miss% 0.02794639362876946
plot_id,batch_id 0 43 miss% 0.05553433817871592
plot_id,batch_id 0 44 miss% 0.04872271830606088
plot_id,batch_id 0 45 miss% 0.05859315975692118
plot_id,batch_id 0 46 miss% 0.04182020988726559
plot_id,batch_id 0 47 miss% 0.026213027861895277
plot_id,batch_id 0 48 miss% 0.028754292163844855
plot_id,batch_id 0 49 miss% 0.03349245395243021
plot_id,batch_id 0 50 miss% 0.1654195322703585
plot_id,batch_id 0 51 miss% 0.04473185107238143
plot_id,batch_id 0 52 miss% 0.02786466583887922
plot_id,batch_id 0 53 miss% 0.026866327019694297
plot_id,batch_id 0 54 miss% 0.0627730211214489
plot_id,batch_id 0 55 miss% 0.08643140144920365
plot_id,batch_id 0 56 miss% 0.057288378215406804
plot_id,batch_id 0 57 miss% 0.038801840439990944
plot_id,batch_id 0 58 miss% 0.03490074728825469
plot_id,batch_id 0 59 miss% 0.029154731602350173
plot_id,batch_id 0 60 miss% 0.04682969088537594
plot_id,batch_id 0 61 miss% 0.030699018865354727
plot_id,batch_id 0 62 miss% 0.23330797042129736
plot_id,batch_id 0 63 miss% 0.08793569071278365
plot_id,batch_id 0 64 miss% 0.05229488086706433
plot_id,batch_id 0 65 miss% 0.07390359596634036
plot_id,batch_id 0 66 miss% 0.09630943233867387
plot_id,batch_id 0 67 miss% 0.0265282695283471
plot_id,batch_idthe mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  89105
Epoch:0, Train loss:0.376429, valid loss:0.339860
Epoch:1, Train loss:0.043007, valid loss:0.003581
Epoch:2, Train loss:0.006495, valid loss:0.001716
Epoch:3, Train loss:0.004321, valid loss:0.001529
Epoch:4, Train loss:0.003507, valid loss:0.001208
Epoch:5, Train loss:0.002267, valid loss:0.001008
Epoch:6, Train loss:0.002030, valid loss:0.000909
Epoch:7, Train loss:0.001849, valid loss:0.000895
Epoch:8, Train loss:0.001772, valid loss:0.001103
Epoch:9, Train loss:0.001627, valid loss:0.000890
Epoch:10, Train loss:0.001533, valid loss:0.000878
Epoch:11, Train loss:0.001176, valid loss:0.000669
Epoch:12, Train loss:0.001130, valid loss:0.000690
Epoch:13, Train loss:0.001110, valid loss:0.000641
Epoch:14, Train loss:0.001067, valid loss:0.000647
Epoch:15, Train loss:0.001015, valid loss:0.000620
Epoch:16, Train loss:0.001021, valid loss:0.000642
Epoch:17, Train loss:0.000969, valid loss:0.000642
Epoch:18, Train loss:0.000968, valid loss:0.000685
Epoch:19, Train loss:0.000949, valid loss:0.000661
Epoch:20, Train loss:0.000934, valid loss:0.000561
Epoch:21, Train loss:0.000730, valid loss:0.000530
Epoch:22, Train loss:0.000709, valid loss:0.000569
Epoch:23, Train loss:0.000704, valid loss:0.000521
Epoch:24, Train loss:0.000687, valid loss:0.000506
Epoch:25, Train loss:0.000698, valid loss:0.000496
Epoch:26, Train loss:0.000664, valid loss:0.000498
Epoch:27, Train loss:0.000663, valid loss:0.000505
Epoch:28, Train loss:0.000646, valid loss:0.000532
Epoch:29, Train loss:0.000652, valid loss:0.000504
Epoch:30, Train loss:0.000637, valid loss:0.000723
Epoch:31, Train loss:0.000544, valid loss:0.000480
Epoch:32, Train loss:0.000529, valid loss:0.000454
Epoch:33, Train loss:0.000521, valid loss:0.000431
Epoch:34, Train loss:0.000525, valid loss:0.000511
Epoch:35, Train loss:0.000513, valid loss:0.000438
Epoch:36, Train loss:0.000500, valid loss:0.000464
Epoch:37, Train loss:0.000517, valid loss:0.000443
Epoch:38, Train loss:0.000501, valid loss:0.000499
Epoch:39, Train loss:0.000502, valid loss:0.000438
Epoch:40, Train loss:0.000494, valid loss:0.000433
Epoch:41, Train loss:0.000449, valid loss:0.000427
Epoch:42, Train loss:0.000442, valid loss:0.000429
Epoch:43, Train loss:0.000437, valid loss:0.000453
Epoch:44, Train loss:0.000438, valid loss:0.000414
Epoch:45, Train loss:0.000432, valid loss:0.000436
Epoch:46, Train loss:0.000435, valid loss:0.000407
Epoch:47, Train loss:0.000431, valid loss:0.000423
Epoch:48, Train loss:0.000425, valid loss:0.000436
Epoch:49, Train loss:0.000426, valid loss:0.000419
Epoch:50, Train loss:0.000427, valid loss:0.000474
Epoch:51, Train loss:0.000421, valid loss:0.000426
Epoch:52, Train loss:0.000413, valid loss:0.000421
Epoch:53, Train loss:0.000409, valid loss:0.000415
Epoch:54, Train loss:0.000407, valid loss:0.000416
Epoch:55, Train loss:0.000405, valid loss:0.000411
Epoch:56, Train loss:0.000403, valid loss:0.000413
Epoch:57, Train loss:0.000402, valid loss:0.000416
Epoch:58, Train loss:0.000401, valid loss:0.000411
Epoch:59, Train loss:0.000400, valid loss:0.000409
Epoch:60, Train loss:0.000399, valid loss:0.000408
training time 15028.351480484009
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3456859565800766
plot_id,batch_id 0 1 miss% 0.32935553568143483
plot_id,batch_id 0 2 miss% 0.39447594085824667
plot_id,batch_id 0 3 miss% 0.33611584957431556
plot_id,batch_id 0 4 miss% 0.3093569244583972
plot_id,batch_id 0 5 miss% 0.3550237661776392
plot_id,batch_id 0 6 miss% 0.4135312250109096
plot_id,batch_id 0 7 miss% 0.4546969711233759
plot_id,batch_id 0 8 miss% 0.5246562332556001
plot_id,batch_id 0 9 miss% 0.34623626907968247
plot_id,batch_id 0 10 miss% 0.28298432339087687
plot_id,batch_id 0 11 miss% 0.3674623249696561
plot_id,batch_id 0 12 miss% 0.45498795621734983
plot_id,batch_id 0 13 miss% 0.3133621751247851
plot_id,batch_id 0 14 miss% 0.37972020488058955
plot_id,batch_id 0 15 miss% 0.3758802412140997
plot_id,batch_id 0 16 miss% 0.49342327806639796
plot_id,batch_id 0 17 miss% 0.4322212911889202
plot_id,batch_id 0 18 miss% 0.4492970131550099
plot_id,batch_id 0 19 miss% 0.3387113693384102
plot_id,batch_id 0 20 miss% 0.4792177719589359
plot_id,batch_id 0 21 miss% 0.4652457258496008
plot_id,batch_id 0 22 miss% 0.31038677495428985
plot_id,batch_id 0 23 miss% 0.2904992646145058
plot_id,batch_id 0 24 miss% 0.33139245454980676
plot_id,batch_id 0 25 miss% 0.40479529689488625
plot_id,batch_id 0 26 miss% 0.37061248003328867
plot_id,batch_id 0 27 miss% 0.34332051682216214
plot_id,batch_id 0 28 miss% 0.31860732149522575
plot_id,batch_id 0 29 miss% 0.28469991804943573
plot_id,batch_id 0 30 miss% 0.3671040813307655
plot_id,batch_id 0 31 miss% 0.5039308591804129
plot_id,batch_id 0 32 miss% 0.4311503315256142
plot_id,batch_id 0 33 miss% 0.3307939355225132
plot_id,batch_id 0 34 miss% 0.41349224877693497
plot_id,batch_id 0 35 miss% 0.3060613910577097
plot_id,batch_id 0 36 miss% 0.4949586072962467
plot_id,batch_id 0 37 miss% 0.37835719442433996
plot_id,batch_id 0 38 miss% 0.34717611210752813
plot_id,batch_id 0 39 miss% 0.2827564783392113
plot_id,batch_id 0 40 miss% 0.37536715767732043
plot_id,batch_id 0 41 miss% 0.3620571461072987
plot_id,batch_id 0 42 miss% 0.2712998523055024
plot_id,batch_id 0 43 miss% 0.2839565395208605
plot_id,batch_id 0 44 miss% 0.2806556354623226
plot_id,batch_id 0 45 miss% 0.33586120168779404
plot_id,batch_id 0 46 miss% 0.3705090513699156
plot_id,batch_id 0 47 miss% 0.34335389289095974
plot_id,batch_id 0 48 miss% 0.2741411520222801
plot_id,batch_id 0 49 miss% 0.32801448956447277
plot_id,batch_id 0 50 miss% 0.6046990748101491
plot_id,batch_id 0 51 miss% 0.33654685256927536
plot_id,batch_id 0 52 miss% 0.3556842278255718
plot_id,batch_id 0 53 miss% 0.27572739375164595
plot_id,batch_id 0 54 miss% 0.3400204443076062
plot_id,batch_id 0 55 miss% 0.4144038184727598
plot_id,batch_id 0 56 miss% 0.3928062149388322
plot_id,batch_id 0 57 miss% 0.3011543740573039
plot_id,batch_id 0 58 miss% 0.27604229403976926
plot_id,batch_id 0 59 miss% 0.28186003897005035
plot_id,batch_id 0 60 miss% 0.2700588668885033
plot_id,batch_id 0 61 miss% 0.28405530314944577
plot_id,batch_id 0 62 miss% 0.45721729559723967
plot_id,batch_id 0 63 miss% 0.39043505347095764
plot_id,batch_id 0 64 miss% 0.33033977302085477
plot_id,batch_id 0 65 miss% 0.3105164136085125
plot_id,batch_id 0 66 miss% 0.3799576912300855
plot_id,batch_id 0 67 miss% 0.27729508135921055
plot_id,batch_id 0 68 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  151697
Epoch:0, Train loss:0.669293, valid loss:0.625895
Epoch:1, Train loss:0.339708, valid loss:0.006224
Epoch:2, Train loss:0.017441, valid loss:0.005108
Epoch:3, Train loss:0.015925, valid loss:0.004898
Epoch:4, Train loss:0.013776, valid loss:0.004281
Epoch:5, Train loss:0.012660, valid loss:0.003993
Epoch:6, Train loss:0.012185, valid loss:0.003797
Epoch:7, Train loss:0.011333, valid loss:0.003507
Epoch:8, Train loss:0.010458, valid loss:0.003057
Epoch:9, Train loss:0.010114, valid loss:0.003230
Epoch:10, Train loss:0.009843, valid loss:0.003212
Epoch:11, Train loss:0.009007, valid loss:0.002839
Epoch:12, Train loss:0.008841, valid loss:0.002653
Epoch:13, Train loss:0.007930, valid loss:0.002192
Epoch:14, Train loss:0.007650, valid loss:0.002213
Epoch:15, Train loss:0.007278, valid loss:0.002150
Epoch:16, Train loss:0.006270, valid loss:0.002020
Epoch:17, Train loss:0.006097, valid loss:0.002061
Epoch:18, Train loss:0.005047, valid loss:0.001699
Epoch:19, Train loss:0.004000, valid loss:0.001536
Epoch:20, Train loss:0.003755, valid loss:0.001671
Epoch:21, Train loss:0.003236, valid loss:0.001327
Epoch:22, Train loss:0.001809, valid loss:0.001191
Epoch:23, Train loss:0.001696, valid loss:0.001110
Epoch:24, Train loss:0.001613, valid loss:0.001242
Epoch:25, Train loss:0.001577, valid loss:0.001206
Epoch:26, Train loss:0.001607, valid loss:0.001188
Epoch:27, Train loss:0.001526, valid loss:0.001118
Epoch:28, Train loss:0.001444, valid loss:0.001346
Epoch:29, Train loss:0.001452, valid loss:0.001143
Epoch:30, Train loss:0.001403, valid loss:0.001142
Epoch:31, Train loss:0.001172, valid loss:0.001065
Epoch:32, Train loss:0.001143, valid loss:0.001033
Epoch:33, Train loss:0.001123, valid loss:0.000999
Epoch:34, Train loss:0.001102, valid loss:0.000999
Epoch:35, Train loss:0.001114, valid loss:0.001027
Epoch:36, Train loss:0.001101, valid loss:0.001018
Epoch:37, Train loss:0.001060, valid loss:0.001000
Epoch:38, Train loss:0.001052, valid loss:0.001026
Epoch:39, Train loss:0.001034, valid loss:0.001004
Epoch:40, Train loss:0.001029, valid loss:0.000993
Epoch:41, Train loss:0.000932, valid loss:0.000973
Epoch:42, Train loss:0.000903, valid loss:0.000964
Epoch:43, Train loss:0.000888, valid loss:0.000991
Epoch:44, Train loss:0.000897, valid loss:0.000981
Epoch:45, Train loss:0.000874, valid loss:0.000964
Epoch:46, Train loss:0.000875, valid loss:0.000963
Epoch:47, Train loss:0.000873, valid loss:0.000976
Epoch:48, Train loss:0.000856, valid loss:0.000939
Epoch:49, Train loss:0.000857, valid loss:0.001038
Epoch:50, Train loss:0.000837, valid loss:0.000958
Epoch:51, Train loss:0.000793, valid loss:0.000941
Epoch:52, Train loss:0.000782, valid loss:0.000944
Epoch:53, Train loss:0.000778, valid loss:0.000932
Epoch:54, Train loss:0.000775, valid loss:0.000937
Epoch:55, Train loss:0.000773, valid loss:0.000929
Epoch:56, Train loss:0.000771, valid loss:0.000933
Epoch:57, Train loss:0.000770, valid loss:0.000925
Epoch:58, Train loss:0.000769, valid loss:0.000929
Epoch:59, Train loss:0.000768, valid loss:0.000931
Epoch:60, Train loss:0.000768, valid loss:0.000928
training time 15008.643577575684
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.12114401191563991
plot_id,batch_id 0 1 miss% 0.04492449938659977
plot_id,batch_id 0 2 miss% 0.08217681601195082
plot_id,batch_id 0 3 miss% 0.05293949081554846
plot_id,batch_id 0 4 miss% 0.07319724232194474
plot_id,batch_id 0 5 miss% 0.06577906307977952
plot_id,batch_id 0 6 miss% 0.054071414040860216
plot_id,batch_id 0 7 miss% 0.1397732664311219
plot_id,batch_id 0 8 miss% 0.0517755847611195
plot_id,batch_id 0 9 miss% 0.05491066302945271
plot_id,batch_id 0 10 miss% 0.05883551857769429
plot_id,batch_id 0 11 miss% 0.06927292671224584
plot_id,batch_id 0 12 miss% 0.08539851728245017
plot_id,batch_id 0 13 miss% 0.050160112305026254
plot_id,batch_id 0 14 miss% 0.07506813277569908
plot_id,batch_id 0 15 miss% 0.03530782716718177
plot_id,batch_id 0 16 miss% 0.048078783856739936
plot_id,batch_id 0 17 miss% 0.07298238090723663
plot_id,batch_id 0 18 miss% 0.06378398406113686
plot_id,batch_id 0 19 miss% 0.13161890501024626
plot_id,batch_id 0 20 miss% 0.09505984306635953
plot_id,batch_id 0 21 miss% 0.04838587968692528
plot_id,batch_id 0 22 miss% 0.10386976682528774
plot_id,batch_id 0 23 miss% 0.051286589682711846
plot_id,batch_id 0 24 miss% 0.06098420101258509
plot_id,batch_id 0 25 miss% 0.08289433215257948
plot_id,batch_id 0 26 miss% 0.07656303292964332
plot_id,batch_id 0 27 miss% 0.05273845460465081
plot_id,batch_id 0 28 miss% 0.029949511060668177
plot_id,batch_id 0 29 miss% 0.0371294471358666
plot_id,batch_id 0 30 miss% 0.029044616969691694
plot_id,batch_id 0 31 miss% 0.09914872714122999
plot_id,batch_id 0 32 miss% 0.10706879494173312
plot_id,batch_id 0 33 miss% 0.057163376772377976
plot_id,batch_id 0 34 miss% 0.035134875866711344
plot_id,batch_id 0 35 miss% 0.04231694574330164
plot_id,batch_id 0 36 miss% 0.08311256346536813
plot_id,batch_id 0 37 miss% 0.04840690819596178
plot_id,batch_id 0 38 miss% 0.0943640578403556
plot_id,batch_id 0 39 miss% 0.028535430165765298
plot_id,batch_id 0 40 miss% 0.040231542723734394
plot_id,batch_id 0 41 miss% 0.05484998213116473
plot_id,batch_id 0 42 miss% 0.021312280882557582
plot_id,batch_id 0 43 miss% 0.09758840607175445
plot_id,batch_id 0 44 miss% 0.026927695039373176
plot_id,batch_id 0 45 miss% 0.04211316095928029
plot_id,batch_id 0 46 miss% 0.04978024927674743
plot_id,batch_id 0 47 miss% 0.04460156173808218
plot_id,batch_id 0 48 miss% 0.040173443558766626
plot_id,batch_id 0 49 miss% 0.026453185837873087
plot_id,batch_id 0 50 miss% 0.08983136505092346
plot_id,batch_id 0 51 miss% 0.04699008903636314
plot_id,batch_id 0 52 miss% 0.04182217874587366
plot_id,batch_id 0 53 miss% 0.022360593945643003
plot_id,batch_id 0 54 miss% 0.049895425452539076
plot_id,batch_id 0 55 miss% 0.07702094503097663
plot_id,batch_id 0 56 miss% 0.05498497593112056
plot_id,batch_id 0 57 miss% 0.044094572648519736
plot_id,batch_id 0 58 miss% 0.03973977583441809
plot_id,batch_id 0 59 miss% 0.0345086034536251
plot_id,batch_id 0 60 miss% 0.03056035226763143
plot_id,batch_id 0 61 miss% 0.024346076513580332
plot_id,batch_id 0 62 miss% 0.05946315225159016
plot_id,batch_id 0 63 miss% 0.08000781940182082
plot_id,batch_id 0 64 miss% 0.06640951350054504
plot_id,batch_id 0 65 miss% 0.030764905534605938
plot_id,batch_id 0 66 miss% 0.07096224277531665
plot_id,batch_id 0 67 miss% 0.05679879166546471
plot_id,batch_id 0 68 miss% 0.06029664377760943
 0 68 miss% 0.04068944026637834
plot_id,batch_id 0 69 miss% 0.0976925544642928
plot_id,batch_id 0 70 miss% 0.05905093091676912
plot_id,batch_id 0 71 miss% 0.0765462090735646
plot_id,batch_id 0 72 miss% 0.10023870846095105
plot_id,batch_id 0 73 miss% 0.08048519338896445
plot_id,batch_id 0 74 miss% 0.09997698915292613
plot_id,batch_id 0 75 miss% 0.10725431968916783
plot_id,batch_id 0 76 miss% 0.10685028048532211
plot_id,batch_id 0 77 miss% 0.0490270943863469
plot_id,batch_id 0 78 miss% 0.042789676666828126
plot_id,batch_id 0 79 miss% 0.06784239807149457
plot_id,batch_id 0 80 miss% 0.09719345882965559
plot_id,batch_id 0 81 miss% 0.10378999518576652
plot_id,batch_id 0 82 miss% 0.0442418889653145
plot_id,batch_id 0 83 miss% 0.10073560811178281
plot_id,batch_id 0 84 miss% 0.1101286454439621
plot_id,batch_id 0 85 miss% 0.046849232323571625
plot_id,batch_id 0 86 miss% 0.046920117430546
plot_id,batch_id 0 87 miss% 0.09935038435525757
plot_id,batch_id 0 88 miss% 0.06963979895031679
plot_id,batch_id 0 89 miss% 0.09896331646625435
plot_id,batch_id 0 90 miss% 0.05460409468115725
plot_id,batch_id 0 91 miss% 0.07609298906516532
plot_id,batch_id 0 92 miss% 0.04353365904842385
plot_id,batch_id 0 93 miss% 0.04456261629158935
plot_id,batch_id 0 94 miss% 0.0836417283298906
plot_id,batch_id 0 95 miss% 0.07683727043159137
plot_id,batch_id 0 96 miss% 0.038250486488184444
plot_id,batch_id 0 97 miss% 0.10056453044932398
plot_id,batch_id 0 98 miss% 0.0354690254862517
plot_id,batch_id 0 99 miss% 0.04113156461354683
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08422611 0.08500247 0.08353933 0.0702532  0.03863629 0.0315499
 0.04342079 0.11192774 0.07042552 0.03203773 0.05008398 0.05466426
 0.03826931 0.04506751 0.08899494 0.05577499 0.08877319 0.04769701
 0.04936097 0.07156771 0.10743685 0.03818816 0.05212341 0.06172838
 0.10202939 0.0914573  0.05834353 0.04098286 0.04721618 0.04326033
 0.06698956 0.0919453  0.1387362  0.0323825  0.06175926 0.0370025
 0.07461742 0.07207335 0.05785928 0.05650062 0.03022753 0.04272289
 0.02794639 0.05553434 0.04872272 0.05859316 0.04182021 0.02621303
 0.02875429 0.03349245 0.16541953 0.04473185 0.02786467 0.02686633
 0.06277302 0.0864314  0.05728838 0.03880184 0.03490075 0.02915473
 0.04682969 0.03069902 0.23330797 0.08793569 0.05229488 0.0739036
 0.09630943 0.02652827 0.04068944 0.09769255 0.05905093 0.07654621
 0.10023871 0.08048519 0.09997699 0.10725432 0.10685028 0.04902709
 0.04278968 0.0678424  0.09719346 0.10379    0.04424189 0.10073561
 0.11012865 0.04684923 0.04692012 0.09935038 0.0696398  0.09896332
 0.05460409 0.07609299 0.04353366 0.04456262 0.08364173 0.07683727
 0.03825049 0.10056453 0.03546903 0.04113156]
for model  127 the mean error 0.06530917602778553
all id 127 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 25 out win 5 err 0.06530917602778553 time 14988.33471941948
Launcher: Job 128 completed in 15251 seconds.
Launcher: Task 119 done. Exiting.
0.4225939617705638
plot_id,batch_id 0 69 miss% 0.4270025503581235
plot_id,batch_id 0 70 miss% 0.28622695199129256
plot_id,batch_id 0 71 miss% 0.3795340699549696
plot_id,batch_id 0 72 miss% 0.3800988365377343
plot_id,batch_id 0 73 miss% 0.31772388579384375
plot_id,batch_id 0 74 miss% 0.4519824680845974
plot_id,batch_id 0 75 miss% 0.31775096954615234
plot_id,batch_id 0 76 miss% 0.35316045471216456
plot_id,batch_id 0 77 miss% 0.31607078058447563
plot_id,batch_id 0 78 miss% 0.3100954131404154
plot_id,batch_id 0 79 miss% 0.3242821867679837
plot_id,batch_id 0 80 miss% 0.26976217559395754
plot_id,batch_id 0 81 miss% 0.4375617852225835
plot_id,batch_id 0 82 miss% 0.36753662748132565
plot_id,batch_id 0 83 miss% 0.4225635662392249
plot_id,batch_id 0 84 miss% 0.2777209873829151
plot_id,batch_id 0 85 miss% 0.28796560839937124
plot_id,batch_id 0 86 miss% 0.361251090481822
plot_id,batch_id 0 87 miss% 0.3796321596544435
plot_id,batch_id 0 88 miss% 0.4431281190790929
plot_id,batch_id 0 89 miss% 0.3905391280916199
plot_id,batch_id 0 90 miss% 0.208448078204708
plot_id,batch_id 0 91 miss% 0.3454183494390218
plot_id,batch_id 0 92 miss% 0.30939925363893245
plot_id,batch_id 0 93 miss% 0.261418229247185
plot_id,batch_id 0 94 miss% 0.4946081814885291
plot_id,batch_id 0 95 miss% 0.29890088531208037
plot_id,batch_id 0 96 miss% 0.33787537673227386
plot_id,batch_id 0 97 miss% 0.42910382836482447
plot_id,batch_id 0 98 miss% 0.4738288171240584
plot_id,batch_id 0 99 miss% 0.364014809254136
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.34568596 0.32935554 0.39447594 0.33611585 0.30935692 0.35502377
 0.41353123 0.45469697 0.52465623 0.34623627 0.28298432 0.36746232
 0.45498796 0.31336218 0.3797202  0.37588024 0.49342328 0.43222129
 0.44929701 0.33871137 0.47921777 0.46524573 0.31038677 0.29049926
 0.33139245 0.4047953  0.37061248 0.34332052 0.31860732 0.28469992
 0.36710408 0.50393086 0.43115033 0.33079394 0.41349225 0.30606139
 0.49495861 0.37835719 0.34717611 0.28275648 0.37536716 0.36205715
 0.27129985 0.28395654 0.28065564 0.3358612  0.37050905 0.34335389
 0.27414115 0.32801449 0.60469907 0.33654685 0.35568423 0.27572739
 0.34002044 0.41440382 0.39280621 0.30115437 0.27604229 0.28186004
 0.27005887 0.2840553  0.4572173  0.39043505 0.33033977 0.31051641
 0.37995769 0.27729508 0.42259396 0.42700255 0.28622695 0.37953407
 0.38009884 0.31772389 0.45198247 0.31775097 0.35316045 0.31607078
 0.31009541 0.32428219 0.26976218 0.43756179 0.36753663 0.42256357
 0.27772099 0.28796561 0.36125109 0.37963216 0.44312812 0.39053913
 0.20844808 0.34541835 0.30939925 0.26141823 0.49460818 0.29890089
 0.33787538 0.42910383 0.47382882 0.36401481]
for model  194 the mean error 0.36102979530480106
all id 194 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 31 out win 6 err 0.36102979530480106 time 15028.351480484009
Launcher: Job 195 completed in 15260 seconds.
Launcher: Task 254 done. Exiting.
plot_id,batch_id 0 69 miss% 0.07528841887933439
plot_id,batch_id 0 70 miss% 0.069608488084901
plot_id,batch_id 0 71 miss% 0.02650464356015757
plot_id,batch_id 0 72 miss% 0.1025526622228603
plot_id,batch_id 0 73 miss% 0.05745649963646387
plot_id,batch_id 0 74 miss% 0.14256540373645915
plot_id,batch_id 0 75 miss% 0.05962605359429903
plot_id,batch_id 0 76 miss% 0.0840570916651402
plot_id,batch_id 0 77 miss% 0.06621508538944021
plot_id,batch_id 0 78 miss% 0.05945026101780064
plot_id,batch_id 0 79 miss% 0.03437975703544146
plot_id,batch_id 0 80 miss% 0.0343301610084071
plot_id,batch_id 0 81 miss% 0.05616662686421192
plot_id,batch_id 0 82 miss% 0.08572938215920091
plot_id,batch_id 0 83 miss% 0.05777492734010603
plot_id,batch_id 0 84 miss% 0.07653576442241156
plot_id,batch_id 0 85 miss% 0.05331881098458294
plot_id,batch_id 0 86 miss% 0.054614705040347725
plot_id,batch_id 0 87 miss% 0.0825223207260132
plot_id,batch_id 0 88 miss% 0.06900570330907829
plot_id,batch_id 0 89 miss% 0.06066992701289724
plot_id,batch_id 0 90 miss% 0.05163045101600487
plot_id,batch_id 0 91 miss% 0.08482979342472545
plot_id,batch_id 0 92 miss% 0.04687134245089346
plot_id,batch_id 0 93 miss% 0.06327984695575306
plot_id,batch_id 0 94 miss% 0.1440368349251383
plot_id,batch_id 0 95 miss% 0.06984934213638168
plot_id,batch_id 0 96 miss% 0.03755003811868087
plot_id,batch_id 0 97 miss% 0.03953005154802494
plot_id,batch_id 0 98 miss% 0.06083356512034274
plot_id,batch_id 0 99 miss% 0.036621981404737575
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.12114401 0.0449245  0.08217682 0.05293949 0.07319724 0.06577906
 0.05407141 0.13977327 0.05177558 0.05491066 0.05883552 0.06927293
 0.08539852 0.05016011 0.07506813 0.03530783 0.04807878 0.07298238
 0.06378398 0.13161891 0.09505984 0.04838588 0.10386977 0.05128659
 0.0609842  0.08289433 0.07656303 0.05273845 0.02994951 0.03712945
 0.02904462 0.09914873 0.10706879 0.05716338 0.03513488 0.04231695
 0.08311256 0.04840691 0.09436406 0.02853543 0.04023154 0.05484998
 0.02131228 0.09758841 0.0269277  0.04211316 0.04978025 0.04460156
 0.04017344 0.02645319 0.08983137 0.04699009 0.04182218 0.02236059
 0.04989543 0.07702095 0.05498498 0.04409457 0.03973978 0.0345086
 0.03056035 0.02434608 0.05946315 0.08000782 0.06640951 0.03076491
 0.07096224 0.05679879 0.06029664 0.07528842 0.06960849 0.02650464
 0.10255266 0.0574565  0.1425654  0.05962605 0.08405709 0.06621509
 0.05945026 0.03437976 0.03433016 0.05616663 0.08572938 0.05777493
 0.07653576 0.05331881 0.05461471 0.08252232 0.0690057  0.06066993
 0.05163045 0.08482979 0.04687134 0.06327985 0.14403683 0.06984934
 0.03755004 0.03953005 0.06083357 0.03662198]
for model  50 the mean error 0.06162651967537189
all id 50 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 21 out win 6 err 0.06162651967537189 time 15008.643577575684
Launcher: Job 51 completed in 15272 seconds.
Launcher: Task 112 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  265233
Epoch:0, Train loss:0.669099, valid loss:0.640473
Epoch:1, Train loss:0.324542, valid loss:0.007281
Epoch:2, Train loss:0.012054, valid loss:0.005580
Epoch:3, Train loss:0.008621, valid loss:0.004059
Epoch:4, Train loss:0.007436, valid loss:0.003949
Epoch:5, Train loss:0.006666, valid loss:0.003372
Epoch:6, Train loss:0.006030, valid loss:0.002597
Epoch:7, Train loss:0.005552, valid loss:0.003052
Epoch:8, Train loss:0.005180, valid loss:0.002341
Epoch:9, Train loss:0.004812, valid loss:0.002433
Epoch:10, Train loss:0.004452, valid loss:0.001911
Epoch:11, Train loss:0.003420, valid loss:0.001656
Epoch:12, Train loss:0.003147, valid loss:0.001632
Epoch:13, Train loss:0.003096, valid loss:0.001472
Epoch:14, Train loss:0.002958, valid loss:0.001790
Epoch:15, Train loss:0.002818, valid loss:0.001431
Epoch:16, Train loss:0.002673, valid loss:0.001637
Epoch:17, Train loss:0.002578, valid loss:0.001129
Epoch:18, Train loss:0.002431, valid loss:0.001173
Epoch:19, Train loss:0.002386, valid loss:0.001451
Epoch:20, Train loss:0.002257, valid loss:0.001178
Epoch:21, Train loss:0.001706, valid loss:0.001014
Epoch:22, Train loss:0.001613, valid loss:0.000956
Epoch:23, Train loss:0.001580, valid loss:0.001022
Epoch:24, Train loss:0.001557, valid loss:0.001057
Epoch:25, Train loss:0.001546, valid loss:0.000993
Epoch:26, Train loss:0.001435, valid loss:0.000921
Epoch:27, Train loss:0.001395, valid loss:0.000841
Epoch:28, Train loss:0.001346, valid loss:0.000856
Epoch:29, Train loss:0.001333, valid loss:0.000929
Epoch:30, Train loss:0.001305, valid loss:0.000945
Epoch:31, Train loss:0.001031, valid loss:0.000775
Epoch:32, Train loss:0.000991, valid loss:0.000793
Epoch:33, Train loss:0.000964, valid loss:0.000795
Epoch:34, Train loss:0.000960, valid loss:0.000776
Epoch:35, Train loss:0.000937, valid loss:0.000811
Epoch:36, Train loss:0.000924, valid loss:0.000781
Epoch:37, Train loss:0.000894, valid loss:0.000746
Epoch:38, Train loss:0.000908, valid loss:0.000751
Epoch:39, Train loss:0.000859, valid loss:0.000748
Epoch:40, Train loss:0.000856, valid loss:0.000753
Epoch:41, Train loss:0.000730, valid loss:0.000702
Epoch:42, Train loss:0.000715, valid loss:0.000710
Epoch:43, Train loss:0.000704, valid loss:0.000724
Epoch:44, Train loss:0.000701, valid loss:0.000711
Epoch:45, Train loss:0.000687, valid loss:0.000794
Epoch:46, Train loss:0.000676, valid loss:0.000731
Epoch:47, Train loss:0.000673, valid loss:0.000725
Epoch:48, Train loss:0.000663, valid loss:0.000695
Epoch:49, Train loss:0.000662, valid loss:0.000702
Epoch:50, Train loss:0.000658, valid loss:0.000686
Epoch:51, Train loss:0.000596, valid loss:0.000666
Epoch:52, Train loss:0.000588, valid loss:0.000669
Epoch:53, Train loss:0.000585, valid loss:0.000670
Epoch:54, Train loss:0.000584, valid loss:0.000669
Epoch:55, Train loss:0.000583, valid loss:0.000669
Epoch:56, Train loss:0.000582, valid loss:0.000669
Epoch:57, Train loss:0.000581, valid loss:0.000670
Epoch:58, Train loss:0.000581, valid loss:0.000669
Epoch:59, Train loss:0.000580, valid loss:0.000668
Epoch:60, Train loss:0.000579, valid loss:0.000668
training time 15107.676292181015
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3784399092582635
plot_id,batch_id 0 1 miss% 0.43459703983356135
plot_id,batch_id 0 2 miss% 0.47662927114916964
plot_id,batch_id 0 3 miss% 0.4503536390605364
plot_id,batch_id 0 4 miss% 0.37252778678455123
plot_id,batch_id 0 5 miss% 0.3511336703772165
plot_id,batch_id 0 6 miss% 0.4803325781394215
plot_id,batch_id 0 7 miss% 0.513514686352121
plot_id,batch_id 0 8 miss% 0.4353963321129271
plot_id,batch_id 0 9 miss% 0.42903358932020313
plot_id,batch_id 0 10 miss% 0.2809698669447248
plot_id,batch_id 0 11 miss% 0.3876125754743439
plot_id,batch_id 0 12 miss% 0.43225683104717305
plot_id,batch_id 0 13 miss% 0.4616447881730044
plot_id,batch_id 0 14 miss% 0.5192300972229779
plot_id,batch_id 0 15 miss% 0.46536358246145937
plot_id,batch_id 0 16 miss% 0.5096936379195846
plot_id,batch_id 0 17 miss% 0.6215361685094628
plot_id,batch_id 0 18 miss% 0.4792574718045601
plot_id,batch_id 0 19 miss% 0.5226280125294842
plot_id,batch_id 0 20 miss% 0.3996794646004165
plot_id,batch_id 0 21 miss% 0.3975944239459155
plot_id,batch_id 0 22 miss% 0.48376786725703075
plot_id,batch_id 0 23 miss% 0.4364395517838418
plot_id,batch_id 0 24 miss% 0.375215670319959
plot_id,batch_id 0 25 miss% 0.3803092808590577
plot_id,batch_id 0 26 miss% 0.36567731940590387
plot_id,batch_id 0 27 miss% 0.45141165149235
plot_id,batch_id 0 28 miss% 0.47458922117546926
plot_id,batch_id 0 29 miss% 0.3888233307909666
plot_id,batch_id 0 30 miss% 0.2917305466320955
plot_id,batch_id 0 31 miss% 0.6415610881762158
plot_id,batch_id 0 32 miss% 0.38411827216544353
plot_id,batch_id 0 33 miss% 0.3472295112734378
plot_id,batch_id 0 34 miss% 0.44666584659594066
plot_id,batch_id 0 35 miss% 0.3234204703402696
plot_id,batch_id 0 36 miss% 0.48358455994249677
plot_id,batch_id 0 37 miss% 0.4839588840641563
plot_id,batch_id 0 38 miss% 0.4002169301065267
plot_id,batch_id 0 39 miss% 0.40773624617202997
plot_id,batch_id 0 40 miss% 0.39761400644814676
plot_id,batch_id 0 41 miss% 0.4304638733326477
plot_id,batch_id 0 42 miss% 0.40209423741583483
plot_id,batch_id 0 43 miss% 0.5250805906540028
plot_id,batch_id 0 44 miss% 0.3908375232332532
plot_id,batch_id 0 45 miss% 0.3036833873484164
plot_id,batch_id 0 46 miss% 0.5976099930754942
plot_id,batch_id 0 47 miss% 0.5318986341960991
plot_id,batch_id 0 48 miss% 0.5607685797321946
plot_id,batch_id 0 49 miss% 0.4094676351773467
plot_id,batch_id 0 50 miss% 0.46673852476661826
plot_id,batch_id 0 51 miss% 0.4713602007261832
plot_id,batch_id 0 52 miss% 0.47524035475114745
plot_id,batch_id 0 53 miss% 0.39657593720068274
plot_id,batch_id 0 54 miss% 0.487534925601427
plot_id,batch_id 0 55 miss% 0.5217369498446014
plot_id,batch_id 0 56 miss% 0.5995802850236174
plot_id,batch_id 0 57 miss% 0.5043775076302232
plot_id,batch_id 0 58 miss% 0.5475381629330591
plot_id,batch_id 0 59 miss% 0.6055322698680963
plot_id,batch_id 0 60 miss% 0.28171732224241725
plot_id,batch_id 0 61 miss% 0.35276331063148586
plot_id,batch_id 0 62 miss% 0.3596398674095447
plot_id,batch_id 0 63 miss% 0.49334551059928927
plot_id,batch_id 0 64 miss% 0.520643254031382
plot_id,batch_id 0 65 miss% 0.2818001541727688
plot_id,batch_id 0 66 miss% 0.3893120324620502
plot_id,batch_id 0 67 miss% 0.28699552752180185
plot_id,batch_id 0 68 miss% 0.38390269993651477
plot_id,batch_id 0 69 miss% 0.36740263804103246
plot_id,batch_id 0 70 miss% 0.2546917169863163
plot_id,batch_id 0 71 miss% 0.3277299801259252
plot_id,batch_id 0 72 miss% 0.4741626664138168
plot_id,batch_id 0 73 miss% 0.45945715954810984
plot_id,batch_id 0 74 miss% 0.4222667008884343
plot_id,batch_id 0 75 miss% 0.2278025815557329
plot_id,batch_id 0 76 miss% 0.34147051864205713
plot_id,batch_id 0 77 miss% 0.35663749252058974
plot_id,batch_id 0 78 miss% 0.3262401755279105
plot_id,batch_id 0 79 miss% 0.33681243665883603
plot_id,batch_id 0 80 miss% 0.23081344489131844
plot_id,batch_id 0 81 miss% 0.4560315173340666
plot_id,batch_id 0 82 miss% 0.5600463681179827
plot_id,batch_id 0 83 miss% 0.43295574549709137
plot_id,batch_id 0 84 miss% 0.3298209804474469
plot_id,batch_id 0 85 miss% 0.23493949317711543
plot_id,batch_id 0 86 miss% 0.36980338676891844
plot_id,batch_id 0 87 miss% 0.3737310157573859
plot_id,batch_id 0 88 miss% 0.4141033346209307
plot_id,batch_id 0 89 miss% 0.5241207252598279
plot_id,batch_id 0 90 miss% 0.22747490720165517
plot_id,batch_id 0 91 miss% 0.4298629999271786
plot_id,batch_id 0 92 miss% 0.36544464690237854
plot_id,batch_id 0 93 miss% 0.43837824562205413
plot_id,batch_id 0 94 miss% 0.45709550863833853
plot_id,batch_id 0 95 miss% 0.19178725587799098
plot_id,batch_id 0 96 miss% 0.39572963412433515
plot_id,batch_id 0 97 miss% 0.5051025778862563
plot_id,batch_id 0 98 miss% 0.4044500488906964
plot_id,batch_id 0 99 miss% 0.4220368403461066
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.37843991 0.43459704 0.47662927 0.45035364 0.37252779 0.35113367
 0.48033258 0.51351469 0.43539633 0.42903359 0.28096987 0.38761258
 0.43225683 0.46164479 0.5192301  0.46536358 0.50969364 0.62153617
 0.47925747 0.52262801 0.39967946 0.39759442 0.48376787 0.43643955
 0.37521567 0.38030928 0.36567732 0.45141165 0.47458922 0.38882333
 0.29173055 0.64156109 0.38411827 0.34722951 0.44666585 0.32342047
 0.48358456 0.48395888 0.40021693 0.40773625 0.39761401 0.43046387
 0.40209424 0.52508059 0.39083752 0.30368339 0.59760999 0.53189863
 0.56076858 0.40946764 0.46673852 0.4713602  0.47524035 0.39657594
 0.48753493 0.52173695 0.59958029 0.50437751 0.54753816 0.60553227
 0.28171732 0.35276331 0.35963987 0.49334551 0.52064325 0.28180015
 0.38931203 0.28699553 0.3839027  0.36740264 0.25469172 0.32772998
 0.47416267 0.45945716 0.4222667  0.22780258 0.34147052 0.35663749
 0.32624018 0.33681244 0.23081344 0.45603152 0.56004637 0.43295575
 0.32982098 0.23493949 0.36980339 0.37373102 0.41410333 0.52412073
 0.22747491 0.429863   0.36544465 0.43837825 0.45709551 0.19178726
 0.39572963 0.50510258 0.40445005 0.42203684]
for model  78 the mean error 0.4200013767373846
all id 78 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 21 out win 4 err 0.4200013767373846 time 15107.676292181015
Launcher: Job 79 completed in 15342 seconds.
Launcher: Task 207 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  55697
Epoch:0, Train loss:0.434513, valid loss:0.402725
Epoch:1, Train loss:0.064274, valid loss:0.003543
Epoch:2, Train loss:0.010660, valid loss:0.003397
Epoch:3, Train loss:0.009958, valid loss:0.003137
Epoch:4, Train loss:0.009316, valid loss:0.002985
Epoch:5, Train loss:0.004435, valid loss:0.001205
Epoch:6, Train loss:0.002529, valid loss:0.000962
Epoch:7, Train loss:0.001900, valid loss:0.000850
Epoch:8, Train loss:0.001746, valid loss:0.000855
Epoch:9, Train loss:0.001668, valid loss:0.000939
Epoch:10, Train loss:0.001550, valid loss:0.000881
Epoch:11, Train loss:0.001204, valid loss:0.000649
Epoch:12, Train loss:0.001166, valid loss:0.000663
Epoch:13, Train loss:0.001123, valid loss:0.000681
Epoch:14, Train loss:0.001092, valid loss:0.000725
Epoch:15, Train loss:0.001062, valid loss:0.000572
Epoch:16, Train loss:0.001049, valid loss:0.000697
Epoch:17, Train loss:0.001030, valid loss:0.000611
Epoch:18, Train loss:0.001022, valid loss:0.000647
Epoch:19, Train loss:0.000975, valid loss:0.000583
Epoch:20, Train loss:0.000954, valid loss:0.000609
Epoch:21, Train loss:0.000773, valid loss:0.000578
Epoch:22, Train loss:0.000761, valid loss:0.000528
Epoch:23, Train loss:0.000761, valid loss:0.000494
Epoch:24, Train loss:0.000756, valid loss:0.000490
Epoch:25, Train loss:0.000739, valid loss:0.000553
Epoch:26, Train loss:0.000726, valid loss:0.000483
Epoch:27, Train loss:0.000727, valid loss:0.000548
Epoch:28, Train loss:0.000694, valid loss:0.000505
Epoch:29, Train loss:0.000718, valid loss:0.000514
Epoch:30, Train loss:0.000692, valid loss:0.000497
Epoch:31, Train loss:0.000604, valid loss:0.000473
Epoch:32, Train loss:0.000600, valid loss:0.000492
Epoch:33, Train loss:0.000593, valid loss:0.000457
Epoch:34, Train loss:0.000585, valid loss:0.000482
Epoch:35, Train loss:0.000588, valid loss:0.000460
Epoch:36, Train loss:0.000586, valid loss:0.000449
Epoch:37, Train loss:0.000574, valid loss:0.000490
Epoch:38, Train loss:0.000573, valid loss:0.000499
Epoch:39, Train loss:0.000576, valid loss:0.000459
Epoch:40, Train loss:0.000563, valid loss:0.000448
Epoch:41, Train loss:0.000520, valid loss:0.000455
Epoch:42, Train loss:0.000519, valid loss:0.000454
Epoch:43, Train loss:0.000514, valid loss:0.000455
Epoch:44, Train loss:0.000516, valid loss:0.000453
Epoch:45, Train loss:0.000511, valid loss:0.000440
Epoch:46, Train loss:0.000506, valid loss:0.000450
Epoch:47, Train loss:0.000505, valid loss:0.000454
Epoch:48, Train loss:0.000511, valid loss:0.000448
Epoch:49, Train loss:0.000500, valid loss:0.000452
Epoch:50, Train loss:0.000505, valid loss:0.000448
Epoch:51, Train loss:0.000474, valid loss:0.000441
Epoch:52, Train loss:0.000470, valid loss:0.000442
Epoch:53, Train loss:0.000469, valid loss:0.000442
Epoch:54, Train loss:0.000468, valid loss:0.000440
Epoch:55, Train loss:0.000467, valid loss:0.000441
Epoch:56, Train loss:0.000467, valid loss:0.000440
Epoch:57, Train loss:0.000466, valid loss:0.000441
Epoch:58, Train loss:0.000466, valid loss:0.000441
Epoch:59, Train loss:0.000466, valid loss:0.000442
Epoch:60, Train loss:0.000466, valid loss:0.000442
training time 15311.74541926384
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.082917031040867
plot_id,batch_id 0 1 miss% 0.035056634590902316
plot_id,batch_id 0 2 miss% 0.1038202910558215
plot_id,batch_id 0 3 miss% 0.08069911567375375
plot_id,batch_id 0 4 miss% 0.08762379891308153
plot_id,batch_id 0 5 miss% 0.05544874997759483
plot_id,batch_id 0 6 miss% 0.07416993635255638
plot_id,batch_id 0 7 miss% 0.0903272940644961
plot_id,batch_id 0 8 miss% 0.12526720079040385
plot_id,batch_id 0 9 miss% 0.06491356523091293
plot_id,batch_id 0 10 miss% 0.03467518896684377
plot_id,batch_id 0 11 miss% 0.09103562910676528
plot_id,batch_id 0 12 miss% 0.0726590551367984
plot_id,batch_id 0 13 miss% 0.06004537488799639
plot_id,batch_id 0 14 miss% 0.11354133484678768
plot_id,batch_id 0 15 miss% 0.03965861278663338
plot_id,batch_id 0 16 miss% 0.04675142174963957
plot_id,batch_id 0 17 miss% 0.08265717743228604
plot_id,batch_id 0 18 miss% 0.08940854567127977
plot_id,batch_id 0 19 miss% 0.09334754608549538
plot_id,batch_id 0 20 miss% 0.035985921663074585
plot_id,batch_id 0 21 miss% 0.07212367625745952
plot_id,batch_id 0 22 miss% 0.07112569265085403
plot_id,batch_id 0 23 miss% 0.044848868250550566
plot_id,batch_id 0 24 miss% 0.07580552903131704
plot_id,batch_id 0 25 miss% 0.044889477454862915
plot_id,batch_id 0 26 miss% 0.05425514455926241
plot_id,batch_id 0 27 miss% 0.04984108235844592
plot_id,batch_id 0 28 miss% 0.0385447780561327
plot_id,batch_id 0 29 miss% 0.05014702082568696
plot_id,batch_id 0 30 miss% 0.025656307265913943
plot_id,batch_id 0 31 miss% 0.09654349897895083
plot_id,batch_id 0 32 miss% 0.16387776896579334
plot_id,batch_id 0 33 miss% 0.11259001517533491
plot_id,batch_id 0 34 miss% 0.08996929044556431
plot_id,batch_id 0 35 miss% 0.04643023387076137
plot_id,batch_id 0 36 miss% 0.07790249577855513
plot_id,batch_id 0 37 miss% 0.12551670544761853
plot_id,batch_id 0 38 miss% 0.07531509276239833
plot_id,batch_id 0 39 miss% 0.056200380424594146
plot_id,batch_id 0 40 miss% 0.07695035815504428
plot_id,batch_id 0 41 miss% 0.08674386597440821
plot_id,batch_id 0 42 miss% 0.07274162889634389
plot_id,batch_id 0 43 miss% 0.1236085735727099
plot_id,batch_id 0 44 miss% 0.03416701549745004
plot_id,batch_id 0 45 miss% 0.03360109052219717
plot_id,batch_id 0 46 miss% 0.03502441582637532
plot_id,batch_id 0 47 miss% 0.06495068845995029
plot_id,batch_id 0 48 miss% 0.051399087353525885
plot_id,batch_id 0 49 miss% 0.052567965428473466
plot_id,batch_id 0 50 miss% 0.09475130571192504
plot_id,batch_id 0 51 miss% 0.03698618071446601
plot_id,batch_id 0 52 miss% 0.02555189422750422
plot_id,batch_id 0 53 miss% 0.04697406621179978
plot_id,batch_id 0 54 miss% 0.07057789867249503
plot_id,batch_id 0 55 miss% 0.04138275133945373
plot_id,batch_id 0 56 miss% 0.12455016804609526
plot_id,batch_id 0 57 miss% 0.07333069058953297
plot_id,batch_id 0 58 miss% 0.05076006151122123
plot_id,batch_id 0 59 miss% 0.048818043497851994
plot_id,batch_id 0 60 miss% 0.04289044844127476
plot_id,batch_id 0 61 miss% 0.04051530926215241
plot_id,batch_id 0 62 miss% 0.055336173685674624
plot_id,batch_id 0 63 miss% 0.046455631760286296
plot_id,batch_id 0 64 miss% 0.06381609192712953
plot_id,batch_id 0 65 miss% 0.03791586153481301
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  151697
Epoch:0, Train loss:0.686398, valid loss:0.640620
Epoch:1, Train loss:0.534331, valid loss:0.527101
Epoch:2, Train loss:0.521544, valid loss:0.525758
Epoch:3, Train loss:0.520124, valid loss:0.525643
Epoch:4, Train loss:0.519510, valid loss:0.525465
Epoch:5, Train loss:0.518872, valid loss:0.525160
Epoch:6, Train loss:0.518564, valid loss:0.524940
Epoch:7, Train loss:0.518208, valid loss:0.524862
Epoch:8, Train loss:0.517967, valid loss:0.524692
Epoch:9, Train loss:0.517791, valid loss:0.524682
Epoch:10, Train loss:0.517632, valid loss:0.524675
Epoch:11, Train loss:0.517019, valid loss:0.524413
Epoch:12, Train loss:0.516848, valid loss:0.524342
Epoch:13, Train loss:0.516781, valid loss:0.524316
Epoch:14, Train loss:0.516798, valid loss:0.524563
Epoch:15, Train loss:0.516733, valid loss:0.524333
Epoch:16, Train loss:0.516656, valid loss:0.524307
Epoch:17, Train loss:0.516583, valid loss:0.524266
Epoch:18, Train loss:0.516547, valid loss:0.524277
Epoch:19, Train loss:0.516595, valid loss:0.524319
Epoch:20, Train loss:0.516450, valid loss:0.524320
Epoch:21, Train loss:0.516196, valid loss:0.524213
Epoch:22, Train loss:0.516145, valid loss:0.524186
Epoch:23, Train loss:0.516146, valid loss:0.524235
Epoch:24, Train loss:0.516115, valid loss:0.524148
Epoch:25, Train loss:0.516105, valid loss:0.524194
Epoch:26, Train loss:0.516111, valid loss:0.524259
Epoch:27, Train loss:0.516063, valid loss:0.524139
Epoch:28, Train loss:0.516053, valid loss:0.524193
Epoch:29, Train loss:0.516025, valid loss:0.524177
Epoch:30, Train loss:0.516074, valid loss:0.524185
Epoch:31, Train loss:0.515874, valid loss:0.524093
Epoch:32, Train loss:0.515857, valid loss:0.524097
Epoch:33, Train loss:0.515850, valid loss:0.524117
Epoch:34, Train loss:0.515847, valid loss:0.524110
Epoch:35, Train loss:0.515827, valid loss:0.524094
Epoch:36, Train loss:0.515820, valid loss:0.524086
Epoch:37, Train loss:0.515827, valid loss:0.524082
Epoch:38, Train loss:0.515817, valid loss:0.524150
Epoch:39, Train loss:0.515824, valid loss:0.524111
Epoch:40, Train loss:0.515800, valid loss:0.524083
Epoch:41, Train loss:0.515741, valid loss:0.524076
Epoch:42, Train loss:0.515722, valid loss:0.524060
Epoch:43, Train loss:0.515723, valid loss:0.524072
Epoch:44, Train loss:0.515717, valid loss:0.524081
Epoch:45, Train loss:0.515719, valid loss:0.524071
Epoch:46, Train loss:0.515710, valid loss:0.524091
Epoch:47, Train loss:0.515712, valid loss:0.524078
Epoch:48, Train loss:0.515710, valid loss:0.524107
Epoch:49, Train loss:0.515703, valid loss:0.524085
Epoch:50, Train loss:0.515701, valid loss:0.524080
Epoch:51, Train loss:0.515673, valid loss:0.524069
Epoch:52, Train loss:0.515669, valid loss:0.524071
Epoch:53, Train loss:0.515667, valid loss:0.524065
Epoch:54, Train loss:0.515666, valid loss:0.524068
Epoch:55, Train loss:0.515665, valid loss:0.524069
Epoch:56, Train loss:0.515664, valid loss:0.524070
Epoch:57, Train loss:0.515663, valid loss:0.524065
Epoch:58, Train loss:0.515662, valid loss:0.524067
Epoch:59, Train loss:0.515662, valid loss:0.524065
Epoch:60, Train loss:0.515661, valid loss:0.524071
training time 15423.076433897018
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.8282455661118893
plot_id,batch_id 0 1 miss% 0.7951426791035002
plot_id,batch_id 0 2 miss% 0.7902182754506683
plot_id,batch_id 0 3 miss% 0.7878196678063395
plot_id,batch_id 0 4 miss% 0.7866192618419245
plot_id,batch_id 0 5 miss% 0.834888834675386
plot_id,batch_id 0 6 miss% 0.794935621492979
plot_id,batch_id 0 7 miss% 0.7984103107099488
plot_id,batch_id 0 8 miss% 0.7889029731516223
plot_id,batch_id 0 9 miss% 0.788801258670836
plot_id,batch_id 0 10 miss% 0.8306164584350882
plot_id,batch_id 0 11 miss% 0.798140436648578
plot_id,batch_id 0 12 miss% 0.79134388706368
plot_id,batch_id 0 13 miss% 0.7879736248702256
plot_id,batch_id 0 14 miss% 0.7916153650890116
plot_id,batch_id 0 15 miss% 0.8333489255328292
plot_id,batch_id 0 16 miss% 0.818557932246008
plot_id,batch_id 0 17 miss% 0.7996652926835873
plot_id,batch_id 0 18 miss% 0.8059116563942007
plot_id,batch_id 0 19 miss% 0.8032585741360836
plot_id,batch_id 0 20 miss% 0.8100198617611049
plot_id,batch_id 0 21 miss% 0.7868520419400975
plot_id,batch_id 0 22 miss% 0.7865327701625575
plot_id,batch_id 0 23 miss% 0.785813028242931
plot_id,batch_id 0 24 miss% 0.7835296222990378
plot_id,batch_id 0 25 miss% 0.8160896532937655
plot_id,batch_id 0 26 miss% 0.791778353372404
plot_id,batch_id 0 27 miss% 0.788943738962907
plot_id,batch_id 0 28 miss% 0.7872958287142369
plot_id,batch_id 0 29 miss% 0.7885734575520912
plot_id,batch_id 0 30 miss% 0.815042156665687
plot_id,batch_id 0 31 miss% 0.8002316091033571
plot_id,batch_id 0 32 miss% 0.7947715992220393
plot_id,batch_id 0 33 miss% 0.7903672617042505
plot_id,batch_id 0 34 miss% 0.7903016306311358
plot_id,batch_id 0 35 miss% 0.8293031382258396
plot_id,batch_id 0 36 miss% 0.8012743034307362
plot_id,batch_id 0 37 miss% 0.8093930081788212
plot_id,batch_id 0 38 miss% 0.7974853084067941
plot_id,batch_id 0 39 miss% 0.7861132587303515
plot_id,batch_id 0 40 miss% 0.805365109465593
plot_id,batch_id 0 41 miss% 0.7839660883823049
plot_id,batch_id 0 42 miss% 0.7836054114040135
plot_id,batch_id 0 43 miss% 0.7817062914663148
plot_id,batch_id 0 44 miss% 0.7817285181079364
plot_id,batch_id 0 45 miss% 0.7941015438029119
plot_id,batch_id 0 46 miss% 0.7874520012054764
plot_id,batch_id 0 47 miss% 0.7844353937714964
plot_id,batch_id 0 48 miss% 0.7862192276839985
plot_id,batch_id 0 49 miss% 0.7839904610894405
plot_id,batch_id 0 50 miss% 0.8074303834380778
plot_id,batch_id 0 51 miss% 0.7969719305208757
plot_id,batch_id 0 52 miss% 0.7912917000650405
plot_id,batch_id 0 53 miss% 0.7844154719731947
plot_id,batch_id 0 54 miss% 0.7849125421837353
plot_id,batch_id 0 55 miss% 0.8075453557086999
plot_id,batch_id 0 56 miss% 0.794251181196803
plot_id,batch_id 0 57 miss% 0.7960658726818002
plot_id,batch_id 0 58 miss% 0.7925623962369384
plot_id,batch_id 0 59 miss% 0.7911384814876871
plot_id,batch_id 0 60 miss% 0.8805115866918444
plot_id,batch_id 0 61 miss% 0.8135487684509338
plot_id,batch_id 0 62 miss% 0.8058977034677165
plot_id,batch_id 0 63 miss% 0.7939426799324605
plot_id,batch_id 0 64 miss% 0.7985671203362209
plot_id,batch_id 0 65 miss% 0.8651385480917748
plot_id,batch_id 0 66 miss% 0.8301062621997346
plot_id,batch_id 0 67 miss% 0.8101793778313812
plot_id,batch_id 0 68 miss% 0.8000645057469523
plot_id,batch_id 0 69 miss% 0.7930878387338913
plot_id,batch_id 0 70 miss% 0.8675797780344437
plot_id,batch_id 0 71 miss% 0.8338662643278332
plot_id,batch_id 0 72 miss% 0.8036977929149529
plot_id,batch_id 0 73 miss% 0.8050882050428568
plot_id,batch_id 0 74 miss% 0.7948693939230561
plot_id,batch_id 0 75 miss% 0.8584483834317111
plot_id,batch_id 0 76 miss% 0.8188843957851176
plot_id,batch_id 0 77 miss% 0.8113715076144408
plot_id,batch_id 0 78 miss% 0.7996116330020544
plot_id,batch_id 0 79 miss% 0.800051477520465
plot_id,batch_id 0 80 miss% 0.8506245922212047
plot_id,batch_id 0 81 miss% 0.8057750718835337
plot_id,batch_id 0 82 miss% 0.7954992105257617
plot_id,batch_id 0 83 miss% 0.7917587785703254
plot_id,batch_id 0 84 miss% 0.7869499561932788
plot_id,batch_id 0 85 miss% 0.8470447633118652
plot_id,batch_id 0 86 miss% 0.8053696137298383
plot_id,batch_id 0 87 miss% 0.7951887732660058
plot_id,batch_id 0 88 miss% 0.7937855702574713
plot_id,batch_id 0 89 miss% 0.7917475693885441
plot_id,batch_id 0 90 miss% 0.8750358481213806
plot_id,batch_id 0 91 miss% 0.8112729402590667
plot_id,batch_id 0 92 miss% 0.7957122682875394
plot_id,batch_id 0 93 miss% 0.7959187404161826
plot_id,batch_id 0 94 miss% 0.791039027593476
plot_id,batch_id 0 95 miss% 0.8698568357160914
plot_id,batch_id 0 96 miss% 0.8288737860426264
plot_id,batch_id 0 97 miss% 0.8056469491597635
plot_id,batch_id 0 98 miss% 0.7986890558123483
plot_id,batch_id 0 99 miss% 0.801896959659038
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.82824557 0.79514268 0.79021828 0.78781967 0.78661926 0.83488883
 0.79493562 0.79841031 0.78890297 0.78880126 0.83061646 0.79814044
 0.79134389 0.78797362 0.79161537 0.83334893 0.81855793 0.79966529
 0.80591166 0.80325857 0.81001986 0.78685204 0.78653277 0.78581303
 0.78352962 0.81608965 0.79177835 0.78894374 0.78729583 0.78857346
 0.81504216 0.80023161 0.7947716  0.79036726 0.79030163 0.82930314
 0.8012743  0.80939301 0.79748531 0.78611326 0.80536511 0.78396609
 0.78360541 0.78170629 0.78172852 0.79410154 0.787452   0.78443539
 0.78621923 0.78399046 0.80743038 0.79697193 0.7912917  0.78441547
 0.78491254 0.80754536 0.79425118 0.79606587 0.7925624  0.79113848
 0.88051159 0.81354877 0.8058977  0.79394268 0.79856712 0.86513855
 0.83010626 0.81017938 0.80006451 0.79308784 0.86757978 0.83386626
 0.80369779 0.80508821 0.79486939 0.85844838 0.8188844  0.81137151
 0.79961163 0.80005148 0.85062459 0.80577507 0.79549921 0.79175878
 0.78694996 0.84704476 0.80536961 0.79518877 0.79378557 0.79174757
 0.87503585 0.81127294 0.79571227 0.79591874 0.79103903 0.86985684
 0.82887379 0.80564695 0.79868906 0.80189696]
for model  49 the mean error 0.8043548312808204
all id 49 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 21 out win 5 err 0.8043548312808204 time 15423.076433897018
Launcher: Job 50 completed in 15563 seconds.
Launcher: Task 71 done. Exiting.
plot_id,batch_id 0 66 miss% 0.04550979712572185
plot_id,batch_id 0 67 miss% 0.03825845351018278
plot_id,batch_id 0 68 miss% 0.04574993810056763
plot_id,batch_id 0 69 miss% 0.11045192642485274
plot_id,batch_id 0 70 miss% 0.06510394606770653
plot_id,batch_id 0 71 miss% 0.052187695755576406
plot_id,batch_id 0 72 miss% 0.1177249599115528
plot_id,batch_id 0 73 miss% 0.054592664465814025
plot_id,batch_id 0 74 miss% 0.1111086959698805
plot_id,batch_id 0 75 miss% 0.14024462167307353
plot_id,batch_id 0 76 miss% 0.07098170916824659
plot_id,batch_id 0 77 miss% 0.023621062555561533
plot_id,batch_id 0 78 miss% 0.06176235074489176
plot_id,batch_id 0 79 miss% 0.10855227547809412
plot_id,batch_id 0 80 miss% 0.09071062030690356
plot_id,batch_id 0 81 miss% 0.052850054695320635
plot_id,batch_id 0 82 miss% 0.0665982593683342
plot_id,batch_id 0 83 miss% 0.07637325200153477
plot_id,batch_id 0 84 miss% 0.08256328874603899
plot_id,batch_id 0 85 miss% 0.07466807341822146
plot_id,batch_id 0 86 miss% 0.06518173367684679
plot_id,batch_id 0 87 miss% 0.08703502830947829
plot_id,batch_id 0 88 miss% 0.0646516174465879
plot_id,batch_id 0 89 miss% 0.07023707579368878
plot_id,batch_id 0 90 miss% 0.03843603562863671
plot_id,batch_id 0 91 miss% 0.04622503609457816
plot_id,batch_id 0 92 miss% 0.07051972303592142
plot_id,batch_id 0 93 miss% 0.027838721804258754
plot_id,batch_id 0 94 miss% 0.05524495148773065
plot_id,batch_id 0 95 miss% 0.08274307640614503
plot_id,batch_id 0 96 miss% 0.07180803555020387
plot_id,batch_id 0 97 miss% 0.08850897759388757
plot_id,batch_id 0 98 miss% 0.08317508699400153
plot_id,batch_id 0 99 miss% 0.05481767993826561
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08291703 0.03505663 0.10382029 0.08069912 0.0876238  0.05544875
 0.07416994 0.09032729 0.1252672  0.06491357 0.03467519 0.09103563
 0.07265906 0.06004537 0.11354133 0.03965861 0.04675142 0.08265718
 0.08940855 0.09334755 0.03598592 0.07212368 0.07112569 0.04484887
 0.07580553 0.04488948 0.05425514 0.04984108 0.03854478 0.05014702
 0.02565631 0.0965435  0.16387777 0.11259002 0.08996929 0.04643023
 0.0779025  0.12551671 0.07531509 0.05620038 0.07695036 0.08674387
 0.07274163 0.12360857 0.03416702 0.03360109 0.03502442 0.06495069
 0.05139909 0.05256797 0.09475131 0.03698618 0.02555189 0.04697407
 0.0705779  0.04138275 0.12455017 0.07333069 0.05076006 0.04881804
 0.04289045 0.04051531 0.05533617 0.04645563 0.06381609 0.03791586
 0.0455098  0.03825845 0.04574994 0.11045193 0.06510395 0.0521877
 0.11772496 0.05459266 0.1111087  0.14024462 0.07098171 0.02362106
 0.06176235 0.10855228 0.09071062 0.05285005 0.06659826 0.07637325
 0.08256329 0.07466807 0.06518173 0.08703503 0.06465162 0.07023708
 0.03843604 0.04622504 0.07051972 0.02783872 0.05524495 0.08274308
 0.07180804 0.08850898 0.08317509 0.05481768]
for model  199 the mean error 0.06859996176652508
all id 199 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 31 out win 5 err 0.06859996176652508 time 15311.74541926384
Launcher: Job 200 completed in 15572 seconds.
Launcher: Task 12 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  154129
Epoch:0, Train loss:0.425111, valid loss:0.448003
Epoch:1, Train loss:0.038920, valid loss:0.002567
Epoch:2, Train loss:0.005530, valid loss:0.001973
Epoch:3, Train loss:0.004785, valid loss:0.001773
Epoch:4, Train loss:0.003642, valid loss:0.001019
Epoch:5, Train loss:0.003124, valid loss:0.001072
Epoch:6, Train loss:0.002080, valid loss:0.000850
Epoch:7, Train loss:0.001513, valid loss:0.000815
Epoch:8, Train loss:0.001433, valid loss:0.000678
Epoch:9, Train loss:0.001300, valid loss:0.000809
Epoch:10, Train loss:0.001279, valid loss:0.000682
Epoch:11, Train loss:0.000926, valid loss:0.000569
Epoch:12, Train loss:0.000908, valid loss:0.000577
Epoch:13, Train loss:0.000901, valid loss:0.000664
Epoch:14, Train loss:0.000856, valid loss:0.000559
Epoch:15, Train loss:0.000843, valid loss:0.000550
Epoch:16, Train loss:0.000830, valid loss:0.000542
Epoch:17, Train loss:0.000802, valid loss:0.000562
Epoch:18, Train loss:0.000779, valid loss:0.000515
Epoch:19, Train loss:0.000772, valid loss:0.000521
Epoch:20, Train loss:0.000728, valid loss:0.000505
Epoch:21, Train loss:0.000587, valid loss:0.000488
Epoch:22, Train loss:0.000596, valid loss:0.000454
Epoch:23, Train loss:0.000566, valid loss:0.000444
Epoch:24, Train loss:0.000546, valid loss:0.000432
Epoch:25, Train loss:0.000564, valid loss:0.000494
Epoch:26, Train loss:0.000551, valid loss:0.000449
Epoch:27, Train loss:0.000534, valid loss:0.000468
Epoch:28, Train loss:0.000520, valid loss:0.000456
Epoch:29, Train loss:0.000528, valid loss:0.000509
Epoch:30, Train loss:0.000528, valid loss:0.000437
Epoch:31, Train loss:0.000434, valid loss:0.000427
Epoch:32, Train loss:0.000435, valid loss:0.000440
Epoch:33, Train loss:0.000426, valid loss:0.000421
Epoch:34, Train loss:0.000423, valid loss:0.000433
Epoch:35, Train loss:0.000422, valid loss:0.000409
Epoch:36, Train loss:0.000417, valid loss:0.000414
Epoch:37, Train loss:0.000423, valid loss:0.000417
Epoch:38, Train loss:0.000414, valid loss:0.000429
Epoch:39, Train loss:0.000407, valid loss:0.000454
Epoch:40, Train loss:0.000406, valid loss:0.000417
Epoch:41, Train loss:0.000371, valid loss:0.000415
Epoch:42, Train loss:0.000369, valid loss:0.000412
Epoch:43, Train loss:0.000367, valid loss:0.000410
Epoch:44, Train loss:0.000365, valid loss:0.000418
Epoch:45, Train loss:0.000362, valid loss:0.000408
Epoch:46, Train loss:0.000363, valid loss:0.000420
Epoch:47, Train loss:0.000360, valid loss:0.000403
Epoch:48, Train loss:0.000357, valid loss:0.000407
Epoch:49, Train loss:0.000359, valid loss:0.000411
Epoch:50, Train loss:0.000355, valid loss:0.000424
Epoch:51, Train loss:0.000341, valid loss:0.000410
Epoch:52, Train loss:0.000337, valid loss:0.000403
Epoch:53, Train loss:0.000336, valid loss:0.000403
Epoch:54, Train loss:0.000335, valid loss:0.000404
Epoch:55, Train loss:0.000334, valid loss:0.000405
Epoch:56, Train loss:0.000333, valid loss:0.000410
Epoch:57, Train loss:0.000333, valid loss:0.000408
Epoch:58, Train loss:0.000332, valid loss:0.000397
Epoch:59, Train loss:0.000332, valid loss:0.000409
Epoch:60, Train loss:0.000332, valid loss:0.000404
training time 15367.12719297409
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.07299742462300716
plot_id,batch_id 0 1 miss% 0.03151871273155474
plot_id,batch_id 0 2 miss% 0.0915569579982635
plot_id,batch_id 0 3 miss% 0.06826473439846184
plot_id,batch_id 0 4 miss% 0.04653297285519332
plot_id,batch_id 0 5 miss% 0.03231575515779258
plot_id,batch_id 0 6 miss% 0.03774378740964588
plot_id,batch_id 0 7 miss% 0.08468421466194434
plot_id,batch_id 0 8 miss% 0.07357224594638029
plot_id,batch_id 0 9 miss% 0.042116721979084816
plot_id,batch_id 0 10 miss% 0.036307651703813344
plot_id,batch_id 0 11 miss% 0.04912217720250904
plot_id,batch_id 0 12 miss% 0.04148049792324463
plot_id,batch_id 0 13 miss% 0.038943556986512
plot_id,batch_id 0 14 miss% 0.0924151761215651
plot_id,batch_id 0 15 miss% 0.049380367625712115
plot_id,batch_id 0 16 miss% 0.1280809765024434
plot_id,batch_id 0 17 miss% 0.05880251673942647
plot_id,batch_id 0 18 miss% 0.079854972173772
plot_id,batch_id 0 19 miss% 0.09841409466678581
plot_id,batch_id 0 20 miss% 0.13968897685660128
plot_id,batch_id 0 21 miss% 0.07027197484633171
plot_id,batch_id 0 22 miss% 0.05528437642755219
plot_id,batch_id 0 23 miss% 0.054322268409075634
plot_id,batch_id 0 24 miss% 0.04294244981370601
plot_id,batch_id 0 25 miss% 0.03950983293149116
plot_id,batch_id 0 26 miss% 0.05879071738133538
plot_id,batch_id 0 27 miss% 0.0694460355939931
plot_id,batch_id 0 28 miss% 0.026164009500871947
plot_id,batch_id 0 29 miss% 0.04091676613966229
plot_id,batch_id 0 30 miss% 0.01763760086946956
plot_id,batch_id 0 31 miss% 0.10647898363646194
plot_id,batch_id 0 32 miss% 0.11240120787866269
plot_id,batch_id 0 33 miss% 0.06811164771363286
plot_id,batch_id 0 34 miss% 0.06053865833150309
plot_id,batch_id 0 35 miss% 0.054860502125877765
plot_id,batch_id 0 36 miss% 0.15836084211785884
plot_id,batch_id 0 37 miss% 0.08910681145462182
plot_id,batch_id 0 38 miss% 0.038862585525022926
plot_id,batch_id 0 39 miss% 0.044209451462179396
plot_id,batch_id 0 40 miss% 0.05615203542856727
plot_id,batch_id 0 41 miss% 0.0538218917164726
plot_id,batch_id 0 42 miss% 0.032986574242528335
plot_id,batch_id 0 43 miss% 0.08482539384747348
plot_id,batch_id 0 44 miss% 0.03103896016582333
plot_id,batch_id 0 45 miss% 0.11580125912287344
plot_id,batch_id 0 46 miss% 0.028653743254328762
plot_id,batch_id 0 47 miss% 0.02690310268121638
plot_id,batch_id 0 48 miss% 0.028002951398311642
plot_id,batch_id 0 49 miss% 0.03111537106378998
plot_id,batch_id 0 50 miss% 0.1496215990153654
plot_id,batch_id 0 51 miss% 0.02983920419423228
plot_id,batch_id 0 52 miss% 0.03422626159012105
plot_id,batch_id 0 53 miss% 0.01731682584780557
plot_id,batch_id 0 54 miss% 0.06293883481091218
plot_id,batch_id 0 55 miss% 0.1044704458218068
plot_id,batch_id 0 56 miss% 0.09811508215448839
plot_id,batch_id 0 57 miss% 0.0282043324657279
plot_id,batch_id 0 58 miss% 0.051795688930264816
plot_id,batch_id 0 59 miss% 0.051639040952072744
plot_id,batch_id 0 60 miss% 0.019253422970823348
plot_id,batch_id 0 61 miss% 0.02186240532504969
plot_id,batch_id 0 62 miss% 0.06415328456578705
plot_id,batch_id 0 63 miss% 0.04310839896175898
plot_id,batch_id 0 64 miss% 0.07509819993906616
plot_id,batch_id 0 65 the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  69649
Epoch:0, Train loss:0.562909, valid loss:0.538390
Epoch:1, Train loss:0.036009, valid loss:0.005297
Epoch:2, Train loss:0.011260, valid loss:0.003298
Epoch:3, Train loss:0.008347, valid loss:0.002837
Epoch:4, Train loss:0.006156, valid loss:0.001948
Epoch:5, Train loss:0.003811, valid loss:0.001723
Epoch:6, Train loss:0.003309, valid loss:0.001505
Epoch:7, Train loss:0.003111, valid loss:0.001790
Epoch:8, Train loss:0.002802, valid loss:0.001620
Epoch:9, Train loss:0.002763, valid loss:0.001626
Epoch:10, Train loss:0.002548, valid loss:0.001425
Epoch:11, Train loss:0.001897, valid loss:0.001051
Epoch:12, Train loss:0.001838, valid loss:0.001006
Epoch:13, Train loss:0.001799, valid loss:0.001067
Epoch:14, Train loss:0.001779, valid loss:0.000970
Epoch:15, Train loss:0.001723, valid loss:0.000990
Epoch:16, Train loss:0.001694, valid loss:0.001042
Epoch:17, Train loss:0.001641, valid loss:0.000915
Epoch:18, Train loss:0.001611, valid loss:0.001129
Epoch:19, Train loss:0.001575, valid loss:0.000920
Epoch:20, Train loss:0.001539, valid loss:0.000922
Epoch:21, Train loss:0.001192, valid loss:0.000812
Epoch:22, Train loss:0.001165, valid loss:0.000834
Epoch:23, Train loss:0.001125, valid loss:0.000830
Epoch:24, Train loss:0.001126, valid loss:0.000802
Epoch:25, Train loss:0.001091, valid loss:0.000754
Epoch:26, Train loss:0.001072, valid loss:0.000807
Epoch:27, Train loss:0.001081, valid loss:0.000895
Epoch:28, Train loss:0.001089, valid loss:0.000775
Epoch:29, Train loss:0.001018, valid loss:0.000874
Epoch:30, Train loss:0.001045, valid loss:0.000725
Epoch:31, Train loss:0.000852, valid loss:0.000696
Epoch:32, Train loss:0.000833, valid loss:0.000680
Epoch:33, Train loss:0.000821, valid loss:0.000686
Epoch:34, Train loss:0.000832, valid loss:0.000681
Epoch:35, Train loss:0.000806, valid loss:0.000696
Epoch:36, Train loss:0.000811, valid loss:0.000683
Epoch:37, Train loss:0.000791, valid loss:0.000709
Epoch:38, Train loss:0.000792, valid loss:0.000706
Epoch:39, Train loss:0.000802, valid loss:0.000748
Epoch:40, Train loss:0.000787, valid loss:0.000704
Epoch:41, Train loss:0.000693, valid loss:0.000688
Epoch:42, Train loss:0.000682, valid loss:0.000669
Epoch:43, Train loss:0.000670, valid loss:0.000673
Epoch:44, Train loss:0.000673, valid loss:0.000661
Epoch:45, Train loss:0.000668, valid loss:0.000656
Epoch:46, Train loss:0.000661, valid loss:0.000675
Epoch:47, Train loss:0.000657, valid loss:0.000650
Epoch:48, Train loss:0.000657, valid loss:0.000663
Epoch:49, Train loss:0.000657, valid loss:0.000695
Epoch:50, Train loss:0.000649, valid loss:0.000634
Epoch:51, Train loss:0.000611, valid loss:0.000627
Epoch:52, Train loss:0.000603, valid loss:0.000631
Epoch:53, Train loss:0.000600, valid loss:0.000628
Epoch:54, Train loss:0.000598, valid loss:0.000630
Epoch:55, Train loss:0.000596, valid loss:0.000627
Epoch:56, Train loss:0.000595, valid loss:0.000629
Epoch:57, Train loss:0.000594, valid loss:0.000627
Epoch:58, Train loss:0.000593, valid loss:0.000630
Epoch:59, Train loss:0.000593, valid loss:0.000629
Epoch:60, Train loss:0.000592, valid loss:0.000632
training time 15402.076098442078
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08034698184946341
plot_id,batch_id 0 1 miss% 0.09876919670526946
plot_id,batch_id 0 2 miss% 0.1299105360498983
plot_id,batch_id 0 3 miss% 0.09096789527452785
plot_id,batch_id 0 4 miss% 0.05500809785747028
plot_id,batch_id 0 5 miss% 0.07356979043903929
plot_id,batch_id 0 6 miss% 0.05954343142043504
plot_id,batch_id 0 7 miss% 0.10632182172880288
plot_id,batch_id 0 8 miss% 0.11460240483757536
plot_id,batch_id 0 9 miss% 0.03440754860098415
plot_id,batch_id 0 10 miss% 0.03112752509001878
plot_id,batch_id 0 11 miss% 0.08555015179222136
plot_id,batch_id 0 12 miss% 0.09617281688699143
plot_id,batch_id 0 13 miss% 0.10817828542174111
plot_id,batch_id 0 14 miss% 0.11287114752660045
plot_id,batch_id 0 15 miss% 0.06231715522998875
plot_id,batch_id 0 16 miss% 0.09966537546781287
plot_id,batch_id 0 17 miss% 0.065934633602495
plot_id,batch_id 0 18 miss% 0.07143923535625095
plot_id,batch_id 0 19 miss% 0.08040335688662768
plot_id,batch_id 0 20 miss% 0.07980151115294903
plot_id,batch_id 0 21 miss% 0.1448634623867316
plot_id,batch_id 0 22 miss% 0.045277885313406374
plot_id,batch_id 0 23 miss% 0.07603238925785352
plot_id,batch_id 0 24 miss% 0.04428891835392719
plot_id,batch_id 0 25 miss% 0.05499384579732633
plot_id,batch_id 0 26 miss% 0.0703448356254262
plot_id,batch_id 0 27 miss% 0.05696393321239529
plot_id,batch_id 0 28 miss% 0.044188256600644266
plot_id,batch_id 0 29 miss% 0.03944360322996321
plot_id,batch_id 0 30 miss% 0.04130175561863204
plot_id,batch_id 0 31 miss% 0.04618520842661682
plot_id,batch_id 0 32 miss% 0.11466226354101416
plot_id,batch_id 0 33 miss% 0.06480552064389358
plot_id,batch_id 0 34 miss% 0.03249114495518943
plot_id,batch_id 0 35 miss% 0.057607847023842126
plot_id,batch_id 0 36 miss% 0.1399045533055709
plot_id,batch_id 0 37 miss% 0.050994368326712725
plot_id,batch_id 0 38 miss% 0.04292556630202533
plot_id,batch_id 0 39 miss% 0.04150081298635419
plot_id,batch_id 0 40 miss% 0.0549716761051327
plot_id,batch_id 0 41 miss% 0.05325112821718043
plot_id,batch_id 0 42 miss% 0.05735198878206589
plot_id,batch_id 0 43 miss% 0.05043186578141805
plot_id,batch_id 0 44 miss% 0.07837578805997868
plot_id,batch_id 0 45 miss% 0.08032372823631952
plot_id,batch_id 0 46 miss% 0.07597112430603144
plot_id,batch_id 0 47 miss% 0.04809194880373104
plot_id,batch_id 0 48 miss% 0.03699344373530409
plot_id,batch_id 0 49 miss% 0.0354430631592583
plot_id,batch_id 0 50 miss% 0.16801800403121647
plot_id,batch_id 0 51 miss% 0.04068379544033377
plot_id,batch_id 0 52 miss% 0.0407850687517322
plot_id,batch_id 0 53 miss% 0.027657756992598988
plot_id,batch_id 0 54 miss% 0.04733050735654849
plot_id,batch_id 0 55 miss% 0.14357719940683616
plot_id,batch_id 0 56 miss% 0.05280260231277944
plot_id,batch_id 0 57 miss% 0.06309993177234727
plot_id,batch_id 0 58 miss% 0.04590201701834669
plot_id,batch_id 0 59 miss% 0.034833242734570516
plot_id,batch_id 0 60 miss% 0.04030599551350695
plot_id,batch_id 0 61 miss% 0.05102003539588324
plot_id,batch_id 0 62 miss% 0.08091734817346265
plot_id,batch_id 0 63 miss% 0.055765293298191305
plot_id,batch_id 0 64 miss% 0.05395476932747504
plot_id,batch_id 0 65 miss% 0.13600577387725954
plot_id,batch_id 0 66 miss% 0.07655125340170432
plot_id,batch_id 0 67 miss% 0.0226518717418959
plot_id,batch_id 0 68 miss% 0.028380690491098825
plot_id,batch_id 0 69the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  151697
Epoch:0, Train loss:0.669293, valid loss:0.625895
Epoch:1, Train loss:0.537744, valid loss:0.527389
Epoch:2, Train loss:0.525384, valid loss:0.526699
Epoch:3, Train loss:0.523705, valid loss:0.526367
Epoch:4, Train loss:0.523150, valid loss:0.526141
Epoch:5, Train loss:0.522542, valid loss:0.526150
Epoch:6, Train loss:0.522434, valid loss:0.526148
Epoch:7, Train loss:0.522111, valid loss:0.526047
Epoch:8, Train loss:0.521672, valid loss:0.525509
Epoch:9, Train loss:0.521565, valid loss:0.525613
Epoch:10, Train loss:0.521283, valid loss:0.525616
Epoch:11, Train loss:0.520558, valid loss:0.525190
Epoch:12, Train loss:0.520363, valid loss:0.525408
Epoch:13, Train loss:0.520337, valid loss:0.525101
Epoch:14, Train loss:0.520279, valid loss:0.525163
Epoch:15, Train loss:0.520468, valid loss:0.525357
Epoch:16, Train loss:0.520314, valid loss:0.525069
Epoch:17, Train loss:0.520227, valid loss:0.525090
Epoch:18, Train loss:0.520094, valid loss:0.525049
Epoch:19, Train loss:0.520049, valid loss:0.525149
Epoch:20, Train loss:0.520008, valid loss:0.525049
Epoch:21, Train loss:0.519634, valid loss:0.524860
Epoch:22, Train loss:0.519543, valid loss:0.524897
Epoch:23, Train loss:0.519540, valid loss:0.524868
Epoch:24, Train loss:0.519508, valid loss:0.524962
Epoch:25, Train loss:0.519539, valid loss:0.524903
Epoch:26, Train loss:0.519501, valid loss:0.524873
Epoch:27, Train loss:0.519464, valid loss:0.524848
Epoch:28, Train loss:0.519453, valid loss:0.524877
Epoch:29, Train loss:0.519476, valid loss:0.524873
Epoch:30, Train loss:0.519408, valid loss:0.524909
Epoch:31, Train loss:0.519229, valid loss:0.524799
Epoch:32, Train loss:0.519174, valid loss:0.524759
Epoch:33, Train loss:0.519170, valid loss:0.524805
Epoch:34, Train loss:0.519173, valid loss:0.524777
Epoch:35, Train loss:0.519163, valid loss:0.524788
Epoch:36, Train loss:0.519164, valid loss:0.524792
Epoch:37, Train loss:0.519166, valid loss:0.524774
Epoch:38, Train loss:0.519125, valid loss:0.524769
Epoch:39, Train loss:0.519127, valid loss:0.524916
Epoch:40, Train loss:0.519133, valid loss:0.524799
Epoch:41, Train loss:0.519026, valid loss:0.524746
Epoch:42, Train loss:0.519014, valid loss:0.524756
Epoch:43, Train loss:0.519001, valid loss:0.524764
Epoch:44, Train loss:0.519000, valid loss:0.524780
Epoch:45, Train loss:0.518997, valid loss:0.524741
Epoch:46, Train loss:0.518989, valid loss:0.524775
Epoch:47, Train loss:0.518984, valid loss:0.524775
Epoch:48, Train loss:0.518983, valid loss:0.524762
Epoch:49, Train loss:0.518974, valid loss:0.524760
Epoch:50, Train loss:0.518985, valid loss:0.524817
Epoch:51, Train loss:0.518974, valid loss:0.524778
Epoch:52, Train loss:0.518954, valid loss:0.524767
Epoch:53, Train loss:0.518947, valid loss:0.524752
Epoch:54, Train loss:0.518943, valid loss:0.524754
Epoch:55, Train loss:0.518941, valid loss:0.524756
Epoch:56, Train loss:0.518939, valid loss:0.524752
Epoch:57, Train loss:0.518938, valid loss:0.524751
Epoch:58, Train loss:0.518937, valid loss:0.524749
Epoch:59, Train loss:0.518935, valid loss:0.524747
Epoch:60, Train loss:0.518935, valid loss:0.524749
training time 15484.729948997498
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.7747477952496049
plot_id,batch_id 0 1 miss% 0.747490168841644
plot_id,batch_id 0 2 miss% 0.7404247816530005
plot_id,batch_id 0 3 miss% 0.7394299034117153
plot_id,batch_id 0 4 miss% 0.73576281322092
plot_id,batch_id 0 5 miss% 0.7746442587696981
plot_id,batch_id 0 6 miss% 0.7508065683144164
plot_id,batch_id 0 7 miss% 0.7439000719279734
plot_id,batch_id 0 8 miss% 0.7369262360438705
plot_id,batch_id 0 9 miss% 0.7360536356009448
plot_id,batch_id 0 10 miss% 0.7725359390955644
plot_id,batch_id 0 11 miss% 0.7458936345826463
plot_id,batch_id 0 12 miss% 0.7436204697181922
plot_id,batch_id 0 13 miss% 0.7340879460442542
plot_id,batch_id 0 14 miss% 0.7409380492731475
plot_id,batch_id 0 15 miss% 0.7840845385093054
plot_id,batch_id 0 16 miss% 0.7544973445895892
plot_id,batch_id 0 17 miss% 0.7426613160625918
plot_id,batch_id 0 18 miss% 0.7440183160127378
plot_id,batch_id 0 19 miss% 0.7433433343295236
plot_id,batch_id 0 20 miss% 0.7570181750061089
plot_id,batch_id 0 21 miss% 0.739444496497101
plot_id,batch_id 0 22 miss% 0.7383609846667726
plot_id,batch_id 0 23 miss% 0.7392931279942261
plot_id,batch_id 0 24 miss% 0.7364231034966752
plot_id,batch_id 0 25 miss% 0.760459140444141
plot_id,batch_id 0 26 miss% 0.7430647918472857
plot_id,batch_id 0 27 miss% 0.7382821951580572
plot_id,batch_id 0 28 miss% 0.7390430610158447
plot_id,batch_id 0 29 miss% 0.7369423862593015
plot_id,batch_id 0 30 miss% 0.7580860352444839
plot_id,batch_id 0 31 miss% 0.7447980554054379
plot_id,batch_id 0 32 miss% 0.7409023710706911
plot_id,batch_id 0 33 miss% 0.7380810064388271
plot_id,batch_id 0 34 miss% 0.73466022245169
plot_id,batch_id 0 35 miss% 0.7675056806803074
plot_id,batch_id 0 36 miss% 0.7417539082576534
plot_id,batch_id 0 37 miss% 0.7400808992089046
plot_id,batch_id 0 38 miss% 0.7370396300927572
plot_id,batch_id 0 39 miss% 0.7341449827934294
plot_id,batch_id 0 40 miss% 0.7514551220823579
plot_id,batch_id 0 41 miss% 0.7359618781323131
plot_id,batch_id 0 42 miss% 0.7315059977489536
plot_id,batch_id 0 43 miss% 0.7312122494031095
plot_id,batch_id 0 44 miss% 0.7312739067834323
plot_id,batch_id 0 45 miss% 0.748558423758934
plot_id,batch_id 0 46 miss% 0.741489441815875
plot_id,batch_id 0 47 miss% 0.7336013964642759
plot_id,batch_id 0 48 miss% 0.7367082839201058
plot_id,batch_id 0 49 miss% 0.732560321771809
plot_id,batch_id 0 50 miss% 0.749653202374277
plot_id,batch_id 0 51 miss% 0.7385964361053577
plot_id,batch_id 0 52 miss% 0.7411714270553109
plot_id,batch_id 0 53 miss% 0.7312352571698303
plot_id,batch_id 0 54 miss% 0.7360915360124617
plot_id,batch_id 0 55 miss% 0.7459923624618479
plot_id,batch_id 0 56 miss% 0.7399669514013061
plot_id,batch_id 0 57 miss% 0.740478320510026
plot_id,batch_id 0 58 miss% 0.7362259265905002
plot_id,batch_id 0 59 miss% 0.7373266345401102
plot_id,batch_id 0 60 miss% 0.8134080188877245
plot_id,batch_id 0 61 miss% 0.7582458926872442
plot_id,batch_id 0 62 miss% 0.7480285038047014
plot_id,batch_id 0 63 miss% 0.7454728450766226
plot_id,batch_id 0 64 miss% 0.7456089740668813
plot_id,batch_id 0 65 miss% 0.8089252362011197
plot_id,batch_id 0 66 miss% 0.7802005356384529
plot_id,batch_id 0 67 miss% 0.7511360524342534
plot_id,batch_id 0 68 miss% 0.748404665951442
plot_id,batch_id 0 69 miss% 0.7396832561627015
plot_id,batch_id 0 70 miss% 0.8134547787502753
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  89105
Epoch:0, Train loss:0.376429, valid loss:0.339860
Epoch:1, Train loss:0.046060, valid loss:0.001779
Epoch:2, Train loss:0.003661, valid loss:0.001506
Epoch:3, Train loss:0.003005, valid loss:0.001349
Epoch:4, Train loss:0.002717, valid loss:0.001266
Epoch:5, Train loss:0.002473, valid loss:0.001185
Epoch:6, Train loss:0.002362, valid loss:0.001066
Epoch:7, Train loss:0.002143, valid loss:0.001141
Epoch:8, Train loss:0.002213, valid loss:0.001044
Epoch:9, Train loss:0.001979, valid loss:0.000918
Epoch:10, Train loss:0.001821, valid loss:0.000877
Epoch:11, Train loss:0.001374, valid loss:0.000725
Epoch:12, Train loss:0.001305, valid loss:0.000657
Epoch:13, Train loss:0.001275, valid loss:0.000658
Epoch:14, Train loss:0.001251, valid loss:0.000700
Epoch:15, Train loss:0.001211, valid loss:0.000695
Epoch:16, Train loss:0.001142, valid loss:0.000671
Epoch:17, Train loss:0.001117, valid loss:0.000722
Epoch:18, Train loss:0.001103, valid loss:0.000759
Epoch:19, Train loss:0.001053, valid loss:0.000623
Epoch:20, Train loss:0.001032, valid loss:0.000566
Epoch:21, Train loss:0.000790, valid loss:0.000587
Epoch:22, Train loss:0.000778, valid loss:0.000560
Epoch:23, Train loss:0.000785, valid loss:0.000538
Epoch:24, Train loss:0.000741, valid loss:0.000529
Epoch:25, Train loss:0.000748, valid loss:0.000539
Epoch:26, Train loss:0.000719, valid loss:0.000545
Epoch:27, Train loss:0.000722, valid loss:0.000558
Epoch:28, Train loss:0.000699, valid loss:0.000599
Epoch:29, Train loss:0.000714, valid loss:0.000557
Epoch:30, Train loss:0.000693, valid loss:0.000621
Epoch:31, Train loss:0.000561, valid loss:0.000477
Epoch:32, Train loss:0.000546, valid loss:0.000504
Epoch:33, Train loss:0.000548, valid loss:0.000489
Epoch:34, Train loss:0.000537, valid loss:0.000487
Epoch:35, Train loss:0.000539, valid loss:0.000505
Epoch:36, Train loss:0.000530, valid loss:0.000486
Epoch:37, Train loss:0.000526, valid loss:0.000490
Epoch:38, Train loss:0.000516, valid loss:0.000509
Epoch:39, Train loss:0.000527, valid loss:0.000517
Epoch:40, Train loss:0.000513, valid loss:0.000501
Epoch:41, Train loss:0.000451, valid loss:0.000486
Epoch:42, Train loss:0.000443, valid loss:0.000469
Epoch:43, Train loss:0.000441, valid loss:0.000458
Epoch:44, Train loss:0.000441, valid loss:0.000475
Epoch:45, Train loss:0.000437, valid loss:0.000464
Epoch:46, Train loss:0.000442, valid loss:0.000453
Epoch:47, Train loss:0.000430, valid loss:0.000456
Epoch:48, Train loss:0.000429, valid loss:0.000480
Epoch:49, Train loss:0.000433, valid loss:0.000459
Epoch:50, Train loss:0.000424, valid loss:0.000494
Epoch:51, Train loss:0.000402, valid loss:0.000455
Epoch:52, Train loss:0.000397, valid loss:0.000453
Epoch:53, Train loss:0.000395, valid loss:0.000452
Epoch:54, Train loss:0.000393, valid loss:0.000455
Epoch:55, Train loss:0.000392, valid loss:0.000452
Epoch:56, Train loss:0.000391, valid loss:0.000454
Epoch:57, Train loss:0.000390, valid loss:0.000453
Epoch:58, Train loss:0.000390, valid loss:0.000452
Epoch:59, Train loss:0.000389, valid loss:0.000454
Epoch:60, Train loss:0.000389, valid loss:0.000451
training time 15426.662184000015
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3214268639756466
plot_id,batch_id 0 1 miss% 0.3868760733553758
plot_id,batch_id 0 2 miss% 0.3440333383349766
plot_id,batch_id 0 3 miss% 0.34296173454459694
plot_id,batch_id 0 4 miss% 0.41110761619439806
plot_id,batch_id 0 5 miss% 0.3682333287391687
plot_id,batch_id 0 6 miss% 0.2823556431268735
plot_id,batch_id 0 7 miss% 0.5209651019460484
plot_id,batch_id 0 8 miss% 0.6919809131392438
plot_id,batch_id 0 9 miss% 0.46637123440229555
plot_id,batch_id 0 10 miss% 0.2954765048238072
plot_id,batch_id 0 11 miss% 0.3636183202513788
plot_id,batch_id 0 12 miss% 0.5041519084021
plot_id,batch_id 0 13 miss% 0.3888602330761775
plot_id,batch_id 0 14 miss% 0.4290232566148764
plot_id,batch_id 0 15 miss% 0.32028401763556785
plot_id,batch_id 0 16 miss% 0.49616529285899613
plot_id,batch_id 0 17 miss% 0.4321261991356959
plot_id,batch_id 0 18 miss% 0.5169661424213751
plot_id,batch_id 0 19 miss% 0.4054425261689732
plot_id,batch_id 0 20 miss% 0.38421554982313577
plot_id,batch_id 0 21 miss% 0.29987795672950696
plot_id,batch_id 0 22 miss% 0.45760182046409936
plot_id,batch_id 0 23 miss% 0.45842329742695626
plot_id,batch_id 0 24 miss% 0.27292195099556066
plot_id,batch_id 0 25 miss% 0.38249047679766346
plot_id,batch_id 0 26 miss% 0.40747519383565656
plot_id,batch_id 0 27 miss% 0.4799390681293094
plot_id,batch_id 0 28 miss% 0.29649697085865157
plot_id,batch_id 0 29 miss% 0.38076715062396055
plot_id,batch_id 0 30 miss% 0.2894508366449093
plot_id,batch_id 0 31 miss% 0.4049288217474496
plot_id,batch_id 0 32 miss% 0.4341051842709125
plot_id,batch_id 0 33 miss% 0.47668986369993505
plot_id,batch_id 0 34 miss% 0.4733133399533735
plot_id,batch_id 0 35 miss% 0.3263662334347022
plot_id,batch_id 0 36 miss% 0.57948139946966
plot_id,batch_id 0 37 miss% 0.4311753042139259
plot_id,batch_id 0 38 miss% 0.4198905038515605
plot_id,batch_id 0 39 miss% 0.3458952591457712
plot_id,batch_id 0 40 miss% 0.4035477705779077
plot_id,batch_id 0 41 miss% 0.3989708712341457
plot_id,batch_id 0 42 miss% 0.28828130267604934
plot_id,batch_id 0 43 miss% 0.4108512176873867
plot_id,batch_id 0 44 miss% 0.3521413238824025
plot_id,batch_id 0 45 miss% 0.3151708116967896
plot_id,batch_id 0 46 miss% 0.4741361709295919
plot_id,batch_id 0 47 miss% 0.4467269071796355
plot_id,batch_id 0 48 miss% 0.3911122996638596
plot_id,batch_id 0 49 miss% 0.24614848675536397
plot_id,batch_id 0 50 miss% 0.41340707034602237
plot_id,batch_id 0 51 miss% 0.4740129236845855
plot_id,batch_id 0 52 miss% 0.45531621806977013
plot_id,batch_id 0 53 miss% 0.3261149048645352
plot_id,batch_id 0 54 miss% 0.36401573505815554
plot_id,batch_id 0 55 miss% 0.3642800782015252
plot_id,batch_id 0 56 miss% 0.6090354191152083
plot_id,batch_id 0 57 miss% 0.4729870674202802
plot_id,batch_id 0 58 miss% 0.39825367132323464
plot_id,batch_id 0 59 miss% 0.3617032147333871
plot_id,batch_id 0 60 miss% 0.27011401499251564
plot_id,batch_id 0 61 miss% 0.25883744338211895
plot_id,batch_id 0 62 miss% 0.38229450305220253
plot_id,batch_id 0 63 miss% 0.39680269891210107
plot_id,batch_id 0 64 miss% 0.3662432551512997
plot_id,batch_id 0 65 miss% 0.30322429741684886
plot_id,batch_id 0 66 miss% 0.3660012859746607
plot_id,batch_id 0 67 miss% 0.28369696932769434
plot_id,batch_id 0 68 miss% miss% 0.04330452314417171
plot_id,batch_id 0 66 miss% 0.024861167028052078
plot_id,batch_id 0 67 miss% 0.03118766457366518
plot_id,batch_id 0 68 miss% 0.03796555985042064
plot_id,batch_id 0 69 miss% 0.08053394844078662
plot_id,batch_id 0 70 miss% 0.11134351012420303
plot_id,batch_id 0 71 miss% 0.03960270560190334
plot_id,batch_id 0 72 miss% 0.042781478765520846
plot_id,batch_id 0 73 miss% 0.04629341900633472
plot_id,batch_id 0 74 miss% 0.14440325672588505
plot_id,batch_id 0 75 miss% 0.0516720750831102
plot_id,batch_id 0 76 miss% 0.07827652773116124
plot_id,batch_id 0 77 miss% 0.07460799149120964
plot_id,batch_id 0 78 miss% 0.03217689531460149
plot_id,batch_id 0 79 miss% 0.11371657086342568
plot_id,batch_id 0 80 miss% 0.05765844806117665
plot_id,batch_id 0 81 miss% 0.09000882144828733
plot_id,batch_id 0 82 miss% 0.06197871499631941
plot_id,batch_id 0 83 miss% 0.07621376168501012
plot_id,batch_id 0 84 miss% 0.05120840381480639
plot_id,batch_id 0 85 miss% 0.02799902953328211
plot_id,batch_id 0 86 miss% 0.058694273548097135
plot_id,batch_id 0 87 miss% 0.06679750319770647
plot_id,batch_id 0 88 miss% 0.08292693815165786
plot_id,batch_id 0 89 miss% 0.0691626101486401
plot_id,batch_id 0 90 miss% 0.043900100395660384
plot_id,batch_id 0 91 miss% 0.059525402358105856
plot_id,batch_id 0 92 miss% 0.03975966545868788
plot_id,batch_id 0 93 miss% 0.055516289863535885
plot_id,batch_id 0 94 miss% 0.10046742322112459
plot_id,batch_id 0 95 miss% 0.0372204860614408
plot_id,batch_id 0 96 miss% 0.04433601165389646
plot_id,batch_id 0 97 miss% 0.04103925198667309
plot_id,batch_id 0 98 miss% 0.029250330148819996
plot_id,batch_id 0 99 miss% 0.08172843311864447
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07299742 0.03151871 0.09155696 0.06826473 0.04653297 0.03231576
 0.03774379 0.08468421 0.07357225 0.04211672 0.03630765 0.04912218
 0.0414805  0.03894356 0.09241518 0.04938037 0.12808098 0.05880252
 0.07985497 0.09841409 0.13968898 0.07027197 0.05528438 0.05432227
 0.04294245 0.03950983 0.05879072 0.06944604 0.02616401 0.04091677
 0.0176376  0.10647898 0.11240121 0.06811165 0.06053866 0.0548605
 0.15836084 0.08910681 0.03886259 0.04420945 0.05615204 0.05382189
 0.03298657 0.08482539 0.03103896 0.11580126 0.02865374 0.0269031
 0.02800295 0.03111537 0.1496216  0.0298392  0.03422626 0.01731683
 0.06293883 0.10447045 0.09811508 0.02820433 0.05179569 0.05163904
 0.01925342 0.02186241 0.06415328 0.0431084  0.0750982  0.04330452
 0.02486117 0.03118766 0.03796556 0.08053395 0.11134351 0.03960271
 0.04278148 0.04629342 0.14440326 0.05167208 0.07827653 0.07460799
 0.0321769  0.11371657 0.05765845 0.09000882 0.06197871 0.07621376
 0.0512084  0.02799903 0.05869427 0.0667975  0.08292694 0.06916261
 0.0439001  0.0595254  0.03975967 0.05551629 0.10046742 0.03722049
 0.04433601 0.04103925 0.02925033 0.08172843]
for model  168 the mean error 0.06071074717485716
all id 168 hidden_dim 32 learning_rate 0.0025 num_layers 3 frames 31 out win 4 err 0.06071074717485716 time 15367.12719297409
plot_id,batch_id 0 71 miss% 0.7612647548624082
plot_id,batch_id 0 72 miss% 0.749403945796937
plot_id,batch_id 0 73 miss% 0.7457807536649955
plot_id,batch_id 0 74 miss% 0.7462591794517912
plot_id,batch_id 0 75 miss% 0.8164526829033213
plot_id,batch_id 0 76 miss% 0.7599936905724767
plot_id,batch_id 0 77 miss% 0.7481561811578505
plot_id,batch_id 0 78 miss% 0.7432189404154217
plot_id,batch_id 0 79 miss% 0.7436084795176172
plot_id,batch_id 0 80 miss% 0.8062243057289044
plot_id,batch_id 0 81 miss% 0.7518252967925225
plot_id,batch_id 0 82 miss% 0.7467737099442762
plot_id,batch_id 0 83 miss% 0.7395485108217364
plot_id,batch_id 0 84 miss% 0.7374864276022274
plot_id,batch_id 0 85 miss% 0.8012639877286153
plot_id,batch_id 0 86 miss% 0.7534020928604457
plot_id,batch_id 0 87 miss% 0.7446404388548898
plot_id,batch_id 0 88 miss% 0.7420657126243607
plot_id,batch_id 0 89 miss% 0.7436908043577504
plot_id,batch_id 0 90 miss% 0.8064052564835023
plot_id,batch_id 0 91 miss% 0.7520515058777063
plot_id,batch_id 0 92 miss% 0.7446004697317398
plot_id,batch_id 0 93 miss% 0.7466219198936743
plot_id,batch_id 0 94 miss% 0.7488680486532201
plot_id,batch_id 0 95 miss% 0.8180674429041042
plot_id,batch_id 0 96 miss% 0.7522777349094154
plot_id,batch_id 0 97 miss% 0.7449866107935671
plot_id,batch_id 0 98 miss% 0.7407127679603647
plot_id,batch_id 0 99 miss% 0.7372377201065038
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.7747478  0.74749017 0.74042478 0.7394299  0.73576281 0.77464426
 0.75080657 0.74390007 0.73692624 0.73605364 0.77253594 0.74589363
 0.74362047 0.73408795 0.74093805 0.78408454 0.75449734 0.74266132
 0.74401832 0.74334333 0.75701818 0.7394445  0.73836098 0.73929313
 0.7364231  0.76045914 0.74306479 0.7382822  0.73904306 0.73694239
 0.75808604 0.74479806 0.74090237 0.73808101 0.73466022 0.76750568
 0.74175391 0.7400809  0.73703963 0.73414498 0.75145512 0.73596188
 0.731506   0.73121225 0.73127391 0.74855842 0.74148944 0.7336014
 0.73670828 0.73256032 0.7496532  0.73859644 0.74117143 0.73123526
 0.73609154 0.74599236 0.73996695 0.74047832 0.73622593 0.73732663
 0.81340802 0.75824589 0.7480285  0.74547285 0.74560897 0.80892524
 0.78020054 0.75113605 0.74840467 0.73968326 0.81345478 0.76126475
 0.74940395 0.74578075 0.74625918 0.81645268 0.75999369 0.74815618
 0.74321894 0.74360848 0.80622431 0.7518253  0.74677371 0.73954851
 0.73748643 0.80126399 0.75340209 0.74464044 0.74206571 0.7436908
 0.80640526 0.75205151 0.74460047 0.74662192 0.74886805 0.81806744
 0.75227773 0.74498661 0.74071277 0.73723772]
for model  77 the mean error 0.7504777458401698
all id 77 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 21 out win 6 err 0.7504777458401698 time 15484.729948997498
Launcher: Job 169 completed in 15627 seconds.
Launcher: Task 237 done. Exiting.
Launcher: Job 78 completed in 15631 seconds.
Launcher: Task 15 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  151697
Epoch:0, Train loss:0.517722, valid loss:0.476178
Epoch:1, Train loss:0.370888, valid loss:0.365980
Epoch:2, Train loss:0.018503, valid loss:0.002795
Epoch:3, Train loss:0.004898, valid loss:0.002227
Epoch:4, Train loss:0.004025, valid loss:0.002027
Epoch:5, Train loss:0.003465, valid loss:0.001535
Epoch:6, Train loss:0.003090, valid loss:0.001411
Epoch:7, Train loss:0.002732, valid loss:0.001278
Epoch:8, Train loss:0.002532, valid loss:0.001250
Epoch:9, Train loss:0.002266, valid loss:0.001412
Epoch:10, Train loss:0.002153, valid loss:0.001231
Epoch:11, Train loss:0.001636, valid loss:0.000880
Epoch:12, Train loss:0.001535, valid loss:0.000991
Epoch:13, Train loss:0.001490, valid loss:0.000971
Epoch:14, Train loss:0.001419, valid loss:0.000822
Epoch:15, Train loss:0.001366, valid loss:0.000830
Epoch:16, Train loss:0.001375, valid loss:0.001132
Epoch:17, Train loss:0.001264, valid loss:0.000827
Epoch:18, Train loss:0.001245, valid loss:0.000785
Epoch:19, Train loss:0.001194, valid loss:0.000955
Epoch:20, Train loss:0.001174, valid loss:0.000763
Epoch:21, Train loss:0.000928, valid loss:0.000680
Epoch:22, Train loss:0.000908, valid loss:0.000697
Epoch:23, Train loss:0.000893, valid loss:0.000693
Epoch:24, Train loss:0.000885, valid loss:0.000675
Epoch:25, Train loss:0.000861, valid loss:0.000692
Epoch:26, Train loss:0.000835, valid loss:0.000662
Epoch:27, Train loss:0.000844, valid loss:0.000624
Epoch:28, Train loss:0.000807, valid loss:0.000659
Epoch:29, Train loss:0.000799, valid loss:0.000640
Epoch:30, Train loss:0.000788, valid loss:0.000650
Epoch:31, Train loss:0.000686, valid loss:0.000629
Epoch:32, Train loss:0.000666, valid loss:0.000581
Epoch:33, Train loss:0.000659, valid loss:0.000629
Epoch:34, Train loss:0.000659, valid loss:0.000633
Epoch:35, Train loss:0.000643, valid loss:0.000605
Epoch:36, Train loss:0.000634, valid loss:0.000575
Epoch:37, Train loss:0.000637, valid loss:0.000623
Epoch:38, Train loss:0.000629, valid loss:0.000568
Epoch:39, Train loss:0.000625, valid loss:0.000575
Epoch:40, Train loss:0.000617, valid loss:0.000551
Epoch:41, Train loss:0.000559, valid loss:0.000553
Epoch:42, Train loss:0.000553, valid loss:0.000549
Epoch:43, Train loss:0.000549, valid loss:0.000542
Epoch:44, Train loss:0.000552, valid loss:0.000555
Epoch:45, Train loss:0.000545, valid loss:0.000553
Epoch:46, Train loss:0.000550, valid loss:0.000558
Epoch:47, Train loss:0.000546, valid loss:0.000558
Epoch:48, Train loss:0.000539, valid loss:0.000542
Epoch:49, Train loss:0.000535, valid loss:0.000534
Epoch:50, Train loss:0.000540, valid loss:0.000532
Epoch:51, Train loss:0.000503, valid loss:0.000534
Epoch:52, Train loss:0.000498, valid loss:0.000544
Epoch:53, Train loss:0.000496, valid loss:0.000533
Epoch:54, Train loss:0.000495, valid loss:0.000536
Epoch:55, Train loss:0.000494, valid loss:0.000536
Epoch:56, Train loss:0.000493, valid loss:0.000536
Epoch:57, Train loss:0.000494, valid loss:0.000536
Epoch:58, Train loss:0.000493, valid loss:0.000535
Epoch:59, Train loss:0.000492, valid loss:0.000538
Epoch:60, Train loss:0.000492, valid loss:0.000535
training time 15436.923255205154
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.09199823764161474
plot_id,batch_id 0 1 miss% 0.03355077713820014
plot_id,batch_id 0 2 miss% 0.11532805620867358
plot_id,batch_id 0 3 miss% 0.0433442223689249
plot_id,batch_id 0 4 miss% 0.050134467788834146
plot_id,batch_id 0 5 miss% 0.07161169630919166
plot_id,batch_id 0 6 miss% 0.05687281043665854
plot_id,batch_id 0 7 miss% 0.10342566838069905
plot_id,batch_id 0 8 miss% 0.0653837708615006
plot_id,batch_id 0 9 miss% 0.05820423042354802
plot_id,batch_id 0 10 miss% 0.043726931620280886
plot_id,batch_id 0 11 miss% 0.06285796065792068
plot_id,batch_id 0 12 miss% 0.040526076426121126
plot_id,batch_id 0 13 miss% 0.04007385141764721
plot_id,batch_id 0 14 miss% 0.07500137054666456
plot_id,batch_id 0 15 miss% 0.033388394461648
plot_id,batch_id 0 16 miss% 0.0902248236839082
plot_id,batch_id 0 17 miss% 0.07279330165745715
plot_id,batch_id 0 18 miss% 0.05063659547553758
plot_id,batch_id 0 19 miss% 0.09205963995800125
plot_id,batch_id 0 20 miss% 0.055843407018694696
plot_id,batch_id 0 21 miss% 0.029875324192071753
plot_id,batch_id 0 22 miss% 0.02927226195345653
plot_id,batch_id 0 23 miss% 0.04267399954292468
plot_id,batch_id 0 24 miss% 0.03146357569244608
plot_id,batch_id 0 25 miss% 0.03733343915798534
plot_id,batch_id 0 26 miss% 0.07725203442951108
plot_id,batch_id 0 27 miss% 0.03194887125441938
plot_id,batch_id 0 28 miss% 0.02993211240846162
plot_id,batch_id 0 29 miss% 0.032140492900888225
plot_id,batch_id 0 30 miss% 0.02426863155700415
plot_id,batch_id 0 31 miss% 0.09439488383160094
plot_id,batch_id 0 32 miss% 0.08567240260903507
plot_id,batch_id 0 33 miss% 0.07256353592884014
plot_id,batch_id 0 34 miss% 0.02106461728880107
plot_id,batch_id 0 35 miss% 0.04121813022452118
plot_id,batch_id 0 36 miss% 0.06623894067936907
plot_id,batch_id 0 37 miss% 0.05927321412328752
plot_id,batch_id 0 38 miss% 0.044221054398326236
plot_id,batch_id 0 39 miss% 0.03326912738561495
plot_id,batch_id 0 40 miss% 0.08576501861548806
plot_id,batch_id 0 41 miss% 0.041903244140829445
plot_id,batch_id 0 42 miss% 0.030735092511771794
plot_id,batch_id 0 43 miss% 0.04102510517761759
plot_id,batch_id 0 44 miss% 0.03136066773532872
plot_id,batch_id 0 45 miss% 0.036246380955530476
plot_id,batch_id 0 46 miss% 0.029818712149609593
plot_id,batch_id 0 47 miss% 0.0253472786113809
plot_id,batch_id 0 48 miss% 0.04009231978617148
plot_id,batch_id 0 49 miss% 0.026481511513661876
plot_id,batch_id 0 50 miss% 0.1046632105314567
plot_id,batch_id 0 51 miss% 0.058677145872388634
plot_id,batch_id 0 52 miss% 0.05590755310529165
plot_id,batch_id 0 53 miss% 0.025680391075314453
plot_id,batch_id 0 54 miss% 0.04592574129310438
plot_id,batch_id 0 55 miss% 0.05852204611457093
plot_id,batch_id 0 56 miss% 0.10992021326021661
plot_id,batch_id 0 57 miss% 0.025957697395442705
plot_id,batch_id 0 58 miss% 0.06928318835001275
plot_id,batch_id 0 59 miss% 0.03954719212616364
plot_id,batch_id 0 60 miss% 0.04025072810932067
plot_id,batch_id 0 61 miss% 0.03999029081945115
plot_id,batch_id 0 62 miss% 0.08164054063388165
plot_id,batch_id 0 63 miss% 0.045340562239975755
plot_id,batch_id 0 64 miss% 0.0429606758880432
plot_id,batch_id 0 65 miss% 0.05681816600759422
plot_id,batch_id 0 66 miss% 0.02177973715319616
plot_id,batch_id 0 67 miss% 0.019793004182472606
0.38557202724416373
plot_id,batch_id 0 69 miss% 0.40678839703935255
plot_id,batch_id 0 70 miss% 0.27777599607092357
plot_id,batch_id 0 71 miss% 0.3694949037161301
plot_id,batch_id 0 72 miss% 0.38321040055763106
plot_id,batch_id 0 73 miss% 0.35489057708730254
plot_id,batch_id 0 74 miss% 0.3589430543261405
plot_id,batch_id 0 75 miss% 0.32284982342929464
plot_id,batch_id 0 76 miss% 0.3638061336876906
plot_id,batch_id 0 77 miss% 0.34615123797765496
plot_id,batch_id 0 78 miss% 0.34303894519440603
plot_id,batch_id 0 79 miss% 0.3056972506522601
plot_id,batch_id 0 80 miss% 0.2733834713948458
plot_id,batch_id 0 81 miss% 0.4467702082444677
plot_id,batch_id 0 82 miss% 0.40077642157679155
plot_id,batch_id 0 83 miss% 0.4698534701600247
plot_id,batch_id 0 84 miss% 0.44602199331065223
plot_id,batch_id 0 85 miss% 0.3474525785504015
plot_id,batch_id 0 86 miss% 0.3891244950492326
plot_id,batch_id 0 87 miss% 0.47414631760881787
plot_id,batch_id 0 88 miss% 0.4272148160863404
plot_id,batch_id 0 89 miss% 0.41238562972942994
plot_id,batch_id 0 90 miss% 0.2581085492445668
plot_id,batch_id 0 91 miss% 0.4213928680396657
plot_id,batch_id 0 92 miss% 0.37703221235088813
plot_id,batch_id 0 93 miss% 0.31605482812581104
plot_id,batch_id 0 94 miss% 0.4489160618337468
plot_id,batch_id 0 95 miss% 0.3335248601390506
plot_id,batch_id 0 96 miss% 0.35936769717650346
plot_id,batch_id 0 97 miss% 0.4849509504905389
plot_id,batch_id 0 98 miss% 0.47369313474263514
plot_id,batch_id 0 99 miss% 0.3904906285435216
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.32142686 0.38687607 0.34403334 0.34296173 0.41110762 0.36823333
 0.28235564 0.5209651  0.69198091 0.46637123 0.2954765  0.36361832
 0.50415191 0.38886023 0.42902326 0.32028402 0.49616529 0.4321262
 0.51696614 0.40544253 0.38421555 0.29987796 0.45760182 0.4584233
 0.27292195 0.38249048 0.40747519 0.47993907 0.29649697 0.38076715
 0.28945084 0.40492882 0.43410518 0.47668986 0.47331334 0.32636623
 0.5794814  0.4311753  0.4198905  0.34589526 0.40354777 0.39897087
 0.2882813  0.41085122 0.35214132 0.31517081 0.47413617 0.44672691
 0.3911123  0.24614849 0.41340707 0.47401292 0.45531622 0.3261149
 0.36401574 0.36428008 0.60903542 0.47298707 0.39825367 0.36170321
 0.27011401 0.25883744 0.3822945  0.3968027  0.36624326 0.3032243
 0.36600129 0.28369697 0.38557203 0.4067884  0.277776   0.3694949
 0.3832104  0.35489058 0.35894305 0.32284982 0.36380613 0.34615124
 0.34303895 0.30569725 0.27338347 0.44677021 0.40077642 0.46985347
 0.44602199 0.34745258 0.3891245  0.47414632 0.42721482 0.41238563
 0.25810855 0.42139287 0.37703221 0.31605483 0.44891606 0.33352486
 0.3593677  0.48495095 0.47369313 0.39049063]
for model  221 the mean error 0.3905224030395443
all id 221 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 31 out win 6 err 0.3905224030395443 time 15426.662184000015
Launcher: Job 222 completed in 15659 seconds.
Launcher: Task 195 done. Exiting.
 miss% 0.09659848088796437
plot_id,batch_id 0 70 miss% 0.04457440937927073
plot_id,batch_id 0 71 miss% 0.04331144961882774
plot_id,batch_id 0 72 miss% 0.16531479134527252
plot_id,batch_id 0 73 miss% 0.035160794320252924
plot_id,batch_id 0 74 miss% 0.0892145920178385
plot_id,batch_id 0 75 miss% 0.1115453422527631
plot_id,batch_id 0 76 miss% 0.15772114012494337
plot_id,batch_id 0 77 miss% 0.06982445308814793
plot_id,batch_id 0 78 miss% 0.02824958693115671
plot_id,batch_id 0 79 miss% 0.06455646770985406
plot_id,batch_id 0 80 miss% 0.0648167885323645
plot_id,batch_id 0 81 miss% 0.10038112846001883
plot_id,batch_id 0 82 miss% 0.11772791577676069
plot_id,batch_id 0 83 miss% 0.1280128556441206
plot_id,batch_id 0 84 miss% 0.07355581230953419
plot_id,batch_id 0 85 miss% 0.03922899989408575
plot_id,batch_id 0 86 miss% 0.04942927353633434
plot_id,batch_id 0 87 miss% 0.06811042004704401
plot_id,batch_id 0 88 miss% 0.10554823338784487
plot_id,batch_id 0 89 miss% 0.06848093838923955
plot_id,batch_id 0 90 miss% 0.060759201302522756
plot_id,batch_id 0 91 miss% 0.05665979459118231
plot_id,batch_id 0 92 miss% 0.06721191490026608
plot_id,batch_id 0 93 miss% 0.03596685872218143
plot_id,batch_id 0 94 miss% 0.0560504683868654
plot_id,batch_id 0 95 miss% 0.14388586311480106
plot_id,batch_id 0 96 miss% 0.05767926418915929
plot_id,batch_id 0 97 miss% 0.059481948268672286
plot_id,batch_id 0 98 miss% 0.07064002622113132
plot_id,batch_id 0 99 miss% 0.04836814559894371
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08034698 0.0987692  0.12991054 0.0909679  0.0550081  0.07356979
 0.05954343 0.10632182 0.1146024  0.03440755 0.03112753 0.08555015
 0.09617282 0.10817829 0.11287115 0.06231716 0.09966538 0.06593463
 0.07143924 0.08040336 0.07980151 0.14486346 0.04527789 0.07603239
 0.04428892 0.05499385 0.07034484 0.05696393 0.04418826 0.0394436
 0.04130176 0.04618521 0.11466226 0.06480552 0.03249114 0.05760785
 0.13990455 0.05099437 0.04292557 0.04150081 0.05497168 0.05325113
 0.05735199 0.05043187 0.07837579 0.08032373 0.07597112 0.04809195
 0.03699344 0.03544306 0.168018   0.0406838  0.04078507 0.02765776
 0.04733051 0.1435772  0.0528026  0.06309993 0.04590202 0.03483324
 0.040306   0.05102004 0.08091735 0.05576529 0.05395477 0.13600577
 0.07655125 0.02265187 0.02838069 0.09659848 0.04457441 0.04331145
 0.16531479 0.03516079 0.08921459 0.11154534 0.15772114 0.06982445
 0.02824959 0.06455647 0.06481679 0.10038113 0.11772792 0.12801286
 0.07355581 0.039229   0.04942927 0.06811042 0.10554823 0.06848094
 0.0607592  0.05665979 0.06721191 0.03596686 0.05605047 0.14388586
 0.05767926 0.05948195 0.07064003 0.04836815]
for model  155 the mean error 0.07105201341258263
all id 155 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 25 out win 6 err 0.07105201341258263 time 15402.076098442078
Launcher: Job 156 completed in 15664 seconds.
Launcher: Task 66 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  154129
Epoch:0, Train loss:0.425111, valid loss:0.448003
Epoch:1, Train loss:0.050904, valid loss:0.002695
Epoch:2, Train loss:0.005562, valid loss:0.002511
Epoch:3, Train loss:0.004760, valid loss:0.001416
Epoch:4, Train loss:0.002571, valid loss:0.001165
Epoch:5, Train loss:0.002275, valid loss:0.000990
Epoch:6, Train loss:0.002062, valid loss:0.001294
Epoch:7, Train loss:0.001891, valid loss:0.001068
Epoch:8, Train loss:0.001804, valid loss:0.000930
Epoch:9, Train loss:0.001614, valid loss:0.000850
Epoch:10, Train loss:0.001522, valid loss:0.000697
Epoch:11, Train loss:0.001116, valid loss:0.000642
Epoch:12, Train loss:0.001078, valid loss:0.000589
Epoch:13, Train loss:0.001039, valid loss:0.000737
Epoch:14, Train loss:0.001024, valid loss:0.000633
Epoch:15, Train loss:0.000986, valid loss:0.000559
Epoch:16, Train loss:0.000963, valid loss:0.000574
Epoch:17, Train loss:0.000916, valid loss:0.000563
Epoch:18, Train loss:0.000915, valid loss:0.000575
Epoch:19, Train loss:0.000890, valid loss:0.000602
Epoch:20, Train loss:0.000850, valid loss:0.000589
Epoch:21, Train loss:0.000664, valid loss:0.000543
Epoch:22, Train loss:0.000652, valid loss:0.000457
Epoch:23, Train loss:0.000628, valid loss:0.000483
Epoch:24, Train loss:0.000617, valid loss:0.000460
Epoch:25, Train loss:0.000617, valid loss:0.000523
Epoch:26, Train loss:0.000612, valid loss:0.000476
Epoch:27, Train loss:0.000596, valid loss:0.000566
Epoch:28, Train loss:0.000587, valid loss:0.000533
Epoch:29, Train loss:0.000573, valid loss:0.000498
Epoch:30, Train loss:0.000576, valid loss:0.000456
Epoch:31, Train loss:0.000475, valid loss:0.000444
Epoch:32, Train loss:0.000467, valid loss:0.000447
Epoch:33, Train loss:0.000465, valid loss:0.000426
Epoch:34, Train loss:0.000453, valid loss:0.000423
Epoch:35, Train loss:0.000445, valid loss:0.000424
Epoch:36, Train loss:0.000459, valid loss:0.000451
Epoch:37, Train loss:0.000437, valid loss:0.000417
Epoch:38, Train loss:0.000438, valid loss:0.000454
Epoch:39, Train loss:0.000437, valid loss:0.000443
Epoch:40, Train loss:0.000421, valid loss:0.000451
Epoch:41, Train loss:0.000381, valid loss:0.000417
Epoch:42, Train loss:0.000378, valid loss:0.000428
Epoch:43, Train loss:0.000378, valid loss:0.000427
Epoch:44, Train loss:0.000378, valid loss:0.000415
Epoch:45, Train loss:0.000372, valid loss:0.000402
Epoch:46, Train loss:0.000367, valid loss:0.000404
Epoch:47, Train loss:0.000368, valid loss:0.000411
Epoch:48, Train loss:0.000365, valid loss:0.000397
Epoch:49, Train loss:0.000366, valid loss:0.000414
Epoch:50, Train loss:0.000361, valid loss:0.000430
Epoch:51, Train loss:0.000347, valid loss:0.000411
Epoch:52, Train loss:0.000342, valid loss:0.000410
Epoch:53, Train loss:0.000341, valid loss:0.000407
Epoch:54, Train loss:0.000340, valid loss:0.000408
Epoch:55, Train loss:0.000339, valid loss:0.000407
Epoch:56, Train loss:0.000338, valid loss:0.000408
Epoch:57, Train loss:0.000338, valid loss:0.000407
Epoch:58, Train loss:0.000338, valid loss:0.000405
Epoch:59, Train loss:0.000337, valid loss:0.000404
Epoch:60, Train loss:0.000337, valid loss:0.000404
training time 15487.339247703552
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.058273177046759904
plot_id,batch_id 0 1 miss% 0.06480010351023416
plot_id,batch_id 0 2 miss% 0.1135204071948867
plot_id,batch_id 0 3 miss% 0.055386564606289164
plot_id,batch_id 0 4 miss% 0.07820539296384955
plot_id,batch_id 0 5 miss% 0.035299448394053724
plot_id,batch_id 0 6 miss% 0.05691120603140382
plot_id,batch_id 0 7 miss% 0.07894931609994607
plot_id,batch_id 0 8 miss% 0.0876462168062914
plot_id,batch_id 0 9 miss% 0.04485441247872643
plot_id,batch_id 0 10 miss% 0.07102520594482245
plot_id,batch_id 0 11 miss% 0.06422218787587512
plot_id,batch_id 0 12 miss% 0.09816495967947425
plot_id,batch_id 0 13 miss% 0.07451505638069882
plot_id,batch_id 0 14 miss% 0.06150174275241275
plot_id,batch_id 0 15 miss% 0.038179126386834294
plot_id,batch_id 0 16 miss% 0.08802263568686766
plot_id,batch_id 0 17 miss% 0.053856029655554014
plot_id,batch_id 0 18 miss% 0.07175515652965553
plot_id,batch_id 0 19 miss% 0.08219578402385093
plot_id,batch_id 0 20 miss% 0.2538469495619116
plot_id,batch_id 0 21 miss% 0.03677795730084244
plot_id,batch_id 0 22 miss% 0.028769765306991473
plot_id,batch_id 0 23 miss% 0.04056148622587553
plot_id,batch_id 0 24 miss% 0.04138526929485167
plot_id,batch_id 0 25 miss% 0.06577426093245135
plot_id,batch_id 0 26 miss% 0.06722226094096215
plot_id,batch_id 0 27 miss% 0.032306421384742566
plot_id,batch_id 0 28 miss% 0.03287235977800796
plot_id,batch_id 0 29 miss% 0.03066577294708468
plot_id,batch_id 0 30 miss% 0.029220462887336755
plot_id,batch_id 0 31 miss% 0.12850151759967857
plot_id,batch_id 0 32 miss% 0.06598798224894555
plot_id,batch_id 0 33 miss% 0.0395387516168869
plot_id,batch_id 0 34 miss% 0.01689165439311131
plot_id,batch_id 0 35 miss% 0.0622854820949848
plot_id,batch_id 0 36 miss% 0.07519142486890061
plot_id,batch_id 0 37 miss% 0.08249844396466133
plot_id,batch_id 0 38 miss% 0.07275095078725617
plot_id,batch_id 0 39 miss% 0.03300696084709951
plot_id,batch_id 0 40 miss% 0.08970865406022058
plot_id,batch_id 0 41 miss% 0.05848543186562302
plot_id,batch_id 0 42 miss% 0.028587224382866774
plot_id,batch_id 0 43 miss% 0.03318034503452804
plot_id,batch_id 0 44 miss% 0.025736867264399294
plot_id,batch_id 0 45 miss% 0.06818535962467814
plot_id,batch_id 0 46 miss% 0.026741889257289295
plot_id,batch_id 0 47 miss% 0.024300458587294866
plot_id,batch_id 0 48 miss% 0.022303604728675167
plot_id,batch_id 0 49 miss% 0.031290955971788915
plot_id,batch_id 0 50 miss% 0.12985317107387662
plot_id,batch_id 0 51 miss% 0.0629206122945702
plot_id,batch_id 0 52 miss% 0.02248543879924272
plot_id,batch_id 0 53 miss% 0.03661866986869511
plot_id,batch_id 0 54 miss% 0.029914969712395288
plot_id,batch_id 0 55 miss% 0.06942704062818493
plot_id,batch_id 0 56 miss% 0.10388957767876075
plot_id,batch_id 0 57 miss% 0.057488105042240875
plot_id,batch_id 0 58 miss% 0.04472629042521606
plot_id,batch_id 0 59 miss% 0.019086750044817115
plot_id,batch_id 0 60 miss% 0.05920759954624587
plot_id,batch_id 0 61 miss% 0.022273268330453497
plot_id,batch_id 0 62 miss% 0.02493553133103358
plot_id,batch_id 0 63 miss% 0.03487400877907968
plot_id,batch_id 0 64 miss% 0.058620502679496346
plot_id,batch_id 0 68 miss% 0.0455675793295157
plot_id,batch_id 0 69 miss% 0.05382697071539354
plot_id,batch_id 0 70 miss% 0.04944917380975719
plot_id,batch_id 0 71 miss% 0.04130208208067544
plot_id,batch_id 0 72 miss% 0.06414780413445077
plot_id,batch_id 0 73 miss% 0.05230895792713026
plot_id,batch_id 0 74 miss% 0.09148639747126369
plot_id,batch_id 0 75 miss% 0.05836507130163832
plot_id,batch_id 0 76 miss% 0.04637730327283565
plot_id,batch_id 0 77 miss% 0.047954944827040685
plot_id,batch_id 0 78 miss% 0.046785248870094984
plot_id,batch_id 0 79 miss% 0.041055316317626635
plot_id,batch_id 0 80 miss% 0.07159289003990081
plot_id,batch_id 0 81 miss% 0.0678273940900105
plot_id,batch_id 0 82 miss% 0.05868738859521201
plot_id,batch_id 0 83 miss% 0.07376885952915209
plot_id,batch_id 0 84 miss% 0.05022590045390221
plot_id,batch_id 0 85 miss% 0.044201289932746385
plot_id,batch_id 0 86 miss% 0.054796706933220585
plot_id,batch_id 0 87 miss% 0.06753133244137473
plot_id,batch_id 0 88 miss% 0.08091550086408335
plot_id,batch_id 0 89 miss% 0.07594884814278531
plot_id,batch_id 0 90 miss% 0.020632363609322437
plot_id,batch_id 0 91 miss% 0.025944211907932772
plot_id,batch_id 0 92 miss% 0.04124536458405687
plot_id,batch_id 0 93 miss% 0.055771247794430795
plot_id,batch_id 0 94 miss% 0.04792385024722794
plot_id,batch_id 0 95 miss% 0.042526998628495405
plot_id,batch_id 0 96 miss% 0.05089972457560564
plot_id,batch_id 0 97 miss% 0.05524492313438086
plot_id,batch_id 0 98 miss% 0.03925811996906948
plot_id,batch_id 0 99 miss% 0.092106881151907
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09199824 0.03355078 0.11532806 0.04334422 0.05013447 0.0716117
 0.05687281 0.10342567 0.06538377 0.05820423 0.04372693 0.06285796
 0.04052608 0.04007385 0.07500137 0.03338839 0.09022482 0.0727933
 0.0506366  0.09205964 0.05584341 0.02987532 0.02927226 0.042674
 0.03146358 0.03733344 0.07725203 0.03194887 0.02993211 0.03214049
 0.02426863 0.09439488 0.0856724  0.07256354 0.02106462 0.04121813
 0.06623894 0.05927321 0.04422105 0.03326913 0.08576502 0.04190324
 0.03073509 0.04102511 0.03136067 0.03624638 0.02981871 0.02534728
 0.04009232 0.02648151 0.10466321 0.05867715 0.05590755 0.02568039
 0.04592574 0.05852205 0.10992021 0.0259577  0.06928319 0.03954719
 0.04025073 0.03999029 0.08164054 0.04534056 0.04296068 0.05681817
 0.02177974 0.019793   0.04556758 0.05382697 0.04944917 0.04130208
 0.0641478  0.05230896 0.0914864  0.05836507 0.0463773  0.04795494
 0.04678525 0.04105532 0.07159289 0.06782739 0.05868739 0.07376886
 0.0502259  0.04420129 0.05479671 0.06753133 0.0809155  0.07594885
 0.02063236 0.02594421 0.04124536 0.05577125 0.04792385 0.042527
 0.05089972 0.05524492 0.03925812 0.09210688]
for model  103 the mean error 0.0531817300207782
all id 103 hidden_dim 24 learning_rate 0.0025 num_layers 5 frames 25 out win 5 err 0.0531817300207782 time 15436.923255205154
Launcher: Job 104 completed in 15701 seconds.
Launcher: Task 231 done. Exiting.
plot_id,batch_id 0 65 miss% 0.09309361816912382
plot_id,batch_id 0 66 miss% 0.026376087347162493
plot_id,batch_id 0 67 miss% 0.03362919371077812
plot_id,batch_id 0 68 miss% 0.036946634126870756
plot_id,batch_id 0 69 miss% 0.07081024104527904
plot_id,batch_id 0 70 miss% 0.052915011419828614
plot_id,batch_id 0 71 miss% 0.05262382455501495
plot_id,batch_id 0 72 miss% 0.10332521431992835
plot_id,batch_id 0 73 miss% 0.03999089224330112
plot_id,batch_id 0 74 miss% 0.04789931575692474
plot_id,batch_id 0 75 miss% 0.06567669179336379
plot_id,batch_id 0 76 miss% 0.0438997329058388
plot_id,batch_id 0 77 miss% 0.03068462967323388
plot_id,batch_id 0 78 miss% 0.03552846561775713
plot_id,batch_id 0 79 miss% 0.06204819908020563
plot_id,batch_id 0 80 miss% 0.06415571690749357
plot_id,batch_id 0 81 miss% 0.09519678516623654
plot_id,batch_id 0 82 miss% 0.07111032270253045
plot_id,batch_id 0 83 miss% 0.11269812341047246
plot_id,batch_id 0 84 miss% 0.10759914389879653
plot_id,batch_id 0 85 miss% 0.03381273347328849
plot_id,batch_id 0 86 miss% 0.06760639091023178
plot_id,batch_id 0 87 miss% 0.09990873542259511
plot_id,batch_id 0 88 miss% 0.10152687273923242
plot_id,batch_id 0 89 miss% 0.04038405733143202
plot_id,batch_id 0 90 miss% 0.04158350761055778
plot_id,batch_id 0 91 miss% 0.08733614663037213
plot_id,batch_id 0 92 miss% 0.03290737580820556
plot_id,batch_id 0 93 miss% 0.04797529224772418
plot_id,batch_id 0 94 miss% 0.06936239955524334
plot_id,batch_id 0 95 miss% 0.07649577610836263
plot_id,batch_id 0 96 miss% 0.041331424218267965
plot_id,batch_id 0 97 miss% 0.04913698572210467
plot_id,batch_id 0 98 miss% 0.04058726761759514
plot_id,batch_id 0 99 miss% 0.023827110448332946
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05827318 0.0648001  0.11352041 0.05538656 0.07820539 0.03529945
 0.05691121 0.07894932 0.08764622 0.04485441 0.07102521 0.06422219
 0.09816496 0.07451506 0.06150174 0.03817913 0.08802264 0.05385603
 0.07175516 0.08219578 0.25384695 0.03677796 0.02876977 0.04056149
 0.04138527 0.06577426 0.06722226 0.03230642 0.03287236 0.03066577
 0.02922046 0.12850152 0.06598798 0.03953875 0.01689165 0.06228548
 0.07519142 0.08249844 0.07275095 0.03300696 0.08970865 0.05848543
 0.02858722 0.03318035 0.02573687 0.06818536 0.02674189 0.02430046
 0.0223036  0.03129096 0.12985317 0.06292061 0.02248544 0.03661867
 0.02991497 0.06942704 0.10388958 0.05748811 0.04472629 0.01908675
 0.0592076  0.02227327 0.02493553 0.03487401 0.0586205  0.09309362
 0.02637609 0.03362919 0.03694663 0.07081024 0.05291501 0.05262382
 0.10332521 0.03999089 0.04789932 0.06567669 0.04389973 0.03068463
 0.03552847 0.0620482  0.06415572 0.09519679 0.07111032 0.11269812
 0.10759914 0.03381273 0.06760639 0.09990874 0.10152687 0.04038406
 0.04158351 0.08733615 0.03290738 0.04797529 0.0693624  0.07649578
 0.04133142 0.04913699 0.04058727 0.02382711]
for model  222 the mean error 0.0589817251173643
all id 222 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 31 out win 4 err 0.0589817251173643 time 15487.339247703552
Launcher: Job 223 completed in 15746 seconds.
Launcher: Task 128 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  209681
Epoch:0, Train loss:0.615114, valid loss:0.611850
Epoch:1, Train loss:0.062027, valid loss:0.015749
Epoch:2, Train loss:0.032301, valid loss:0.009944
Epoch:3, Train loss:0.016896, valid loss:0.004318
Epoch:4, Train loss:0.012301, valid loss:0.003955
Epoch:5, Train loss:0.011336, valid loss:0.003443
Epoch:6, Train loss:0.010435, valid loss:0.003410
Epoch:7, Train loss:0.009951, valid loss:0.003049
Epoch:8, Train loss:0.008063, valid loss:0.002559
Epoch:9, Train loss:0.005667, valid loss:0.002041
Epoch:10, Train loss:0.004104, valid loss:0.001881
Epoch:11, Train loss:0.003304, valid loss:0.001684
Epoch:12, Train loss:0.003029, valid loss:0.001562
Epoch:13, Train loss:0.002838, valid loss:0.001509
Epoch:14, Train loss:0.002680, valid loss:0.001559
Epoch:15, Train loss:0.002529, valid loss:0.001521
Epoch:16, Train loss:0.002403, valid loss:0.001421
Epoch:17, Train loss:0.002339, valid loss:0.001605
Epoch:18, Train loss:0.002194, valid loss:0.001369
Epoch:19, Train loss:0.002113, valid loss:0.001461
Epoch:20, Train loss:0.002109, valid loss:0.001409
Epoch:21, Train loss:0.001642, valid loss:0.001207
Epoch:22, Train loss:0.001620, valid loss:0.001167
Epoch:23, Train loss:0.001560, valid loss:0.001194
Epoch:24, Train loss:0.001535, valid loss:0.001183
Epoch:25, Train loss:0.001504, valid loss:0.001165
Epoch:26, Train loss:0.001439, valid loss:0.001205
Epoch:27, Train loss:0.001438, valid loss:0.001087
Epoch:28, Train loss:0.001373, valid loss:0.001186
Epoch:29, Train loss:0.001367, valid loss:0.001129
Epoch:30, Train loss:0.001332, valid loss:0.001118
Epoch:31, Train loss:0.001146, valid loss:0.001059
Epoch:32, Train loss:0.001104, valid loss:0.001046
Epoch:33, Train loss:0.001100, valid loss:0.001049
Epoch:34, Train loss:0.001085, valid loss:0.001124
Epoch:35, Train loss:0.001079, valid loss:0.001040
Epoch:36, Train loss:0.001061, valid loss:0.001012
Epoch:37, Train loss:0.001030, valid loss:0.001125
Epoch:38, Train loss:0.001035, valid loss:0.001043
Epoch:39, Train loss:0.001006, valid loss:0.001029
Epoch:40, Train loss:0.001027, valid loss:0.001026
Epoch:41, Train loss:0.000904, valid loss:0.000979
Epoch:42, Train loss:0.000890, valid loss:0.001019
Epoch:43, Train loss:0.000879, valid loss:0.000984
Epoch:44, Train loss:0.000878, valid loss:0.000989
Epoch:45, Train loss:0.000855, valid loss:0.000996
Epoch:46, Train loss:0.000862, valid loss:0.001012
Epoch:47, Train loss:0.000859, valid loss:0.001003
Epoch:48, Train loss:0.000860, valid loss:0.000962
Epoch:49, Train loss:0.000842, valid loss:0.000968
Epoch:50, Train loss:0.000840, valid loss:0.000952
Epoch:51, Train loss:0.000786, valid loss:0.000948
Epoch:52, Train loss:0.000779, valid loss:0.000944
Epoch:53, Train loss:0.000775, valid loss:0.000948
Epoch:54, Train loss:0.000773, valid loss:0.000949
Epoch:55, Train loss:0.000771, valid loss:0.000945
Epoch:56, Train loss:0.000770, valid loss:0.000946
Epoch:57, Train loss:0.000769, valid loss:0.000944
Epoch:58, Train loss:0.000768, valid loss:0.000945
Epoch:59, Train loss:0.000766, valid loss:0.000943
Epoch:60, Train loss:0.000766, valid loss:0.000953
training time 15534.541383266449
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.10794129084397992
plot_id,batch_id 0 1 miss% 0.07260024560073719
plot_id,batch_id 0 2 miss% 0.0520302825201306
plot_id,batch_id 0 3 miss% 0.0719159011616332
plot_id,batch_id 0 4 miss% 0.05706210744957698
plot_id,batch_id 0 5 miss% 0.09062404001148784
plot_id,batch_id 0 6 miss% 0.05408085858341877
plot_id,batch_id 0 7 miss% 0.10050760116123714
plot_id,batch_id 0 8 miss% 0.08293857317937163
plot_id,batch_id 0 9 miss% 0.062097210661214056
plot_id,batch_id 0 10 miss% 0.027287075426564434
plot_id,batch_id 0 11 miss% 0.07247137447074659
plot_id,batch_id 0 12 miss% 0.05935564642396654
plot_id,batch_id 0 13 miss% 0.06459834276730367
plot_id,batch_id 0 14 miss% 0.09640382804335795
plot_id,batch_id 0 15 miss% 0.04076925739077651
plot_id,batch_id 0 16 miss% 0.07340710973459598
plot_id,batch_id 0 17 miss% 0.11309776245362167
plot_id,batch_id 0 18 miss% 0.05046675064370003
plot_id,batch_id 0 19 miss% 0.12357105497235903
plot_id,batch_id 0 20 miss% 0.07212359357439843
plot_id,batch_id 0 21 miss% 0.07126575133221066
plot_id,batch_id 0 22 miss% 0.034779659284215184
plot_id,batch_id 0 23 miss% 0.0674403694379987
plot_id,batch_id 0 24 miss% 0.0634061138854346
plot_id,batch_id 0 25 miss% 0.04519963159551144
plot_id,batch_id 0 26 miss% 0.05400174068864003
plot_id,batch_id 0 27 miss% 0.055771821288174504
plot_id,batch_id 0 28 miss% 0.04056157093341956
plot_id,batch_id 0 29 miss% 0.0999585835381133
plot_id,batch_id 0 30 miss% 0.05035468891758205
plot_id,batch_id 0 31 miss% 0.0896652568381079
plot_id,batch_id 0 32 miss% 0.1198435800930396
plot_id,batch_id 0 33 miss% 0.07557996374353115
plot_id,batch_id 0 34 miss% 0.027660384158053806
plot_id,batch_id 0 35 miss% 0.050488717556286276
plot_id,batch_id 0 36 miss% 0.08626822970698075
plot_id,batch_id 0 37 miss% 0.07958017291903031
plot_id,batch_id 0 38 miss% 0.0942711657051212
plot_id,batch_id 0 39 miss% 0.06840070287547054
plot_id,batch_id 0 40 miss% 0.06932011262251588
plot_id,batch_id 0 41 miss% 0.05107477312907489
plot_id,batch_id 0 42 miss% 0.06994262474501592
plot_id,batch_id 0 43 miss% 0.07502179360058749
plot_id,batch_id 0 44 miss% 0.05576048921383963
plot_id,batch_id 0 45 miss% 0.0459492630456227
plot_id,batch_id 0 46 miss% 0.030718689557506743
plot_id,batch_id 0 47 miss% 0.043222122623497825
plot_id,batch_id 0 48 miss% 0.04273938814332317
plot_id,batch_id 0 49 miss% 0.03161561230720717
plot_id,batch_id 0 50 miss% 0.13396115171697895
plot_id,batch_id 0 51 miss% 0.02676979366546525
plot_id,batch_id 0 52 miss% 0.02739871443462537
plot_id,batch_id 0 53 miss% 0.029906851942570766
plot_id,batch_id 0 54 miss% 0.0240740894324036
plot_id,batch_id 0 55 miss% 0.07787400811386228
plot_id,batch_id 0 56 miss% 0.05041471163698514
plot_id,batch_id 0 57 miss% 0.032992288813438804
plot_id,batch_id 0 58 miss% 0.047623836143768486
plot_id,batch_id 0 59 miss% 0.04722552938270472
plot_id,batch_id 0 60 miss% 0.04025464470309363
plot_id,batch_id 0 61 miss% 0.042034835185858946
plot_id,batch_id 0 62 miss% 0.0841859434990481
plot_id,batch_id 0 63 miss% 0.0538274261422208
plot_id,batch_id 0 64 miss% 0.04078170290363944
plot_id,batch_id 0 65 miss% 0.11000593507266604
plot_id,batch_id 0 66 miss% 0.06383434161793096
plot_id,batch_id 0 67 miss% 0.04275935217168473
plot_id,batch_id 0 68 miss% 0.06136668645824816
plot_id,batch_id 0 69 miss% 0.10240347153164957
plot_id,batch_id 0 70 miss% 0.06557012357278887
plot_id,batch_id 0 71 miss% 0.0822552083954444
plot_id,batch_id 0 72 miss% 0.08139839480537873
plot_id,batch_id 0 73 miss% 0.10066099530353209
plot_id,batch_id 0 74 miss% 0.10456489809048673
plot_id,batch_id 0 75 miss% 0.06742044204094134
plot_id,batch_id 0 76 miss% 0.09426247182865045
plot_id,batch_id 0 77 miss% 0.049373134143151705
plot_id,batch_id 0 78 miss% 0.0289551664697459
plot_id,batch_id 0 79 miss% 0.038659716515658665
plot_id,batch_id 0 80 miss% 0.04603139315221812
plot_id,batch_id 0 81 miss% 0.09406499833644244
plot_id,batch_id 0 82 miss% 0.05730028522663268
plot_id,batch_id 0 83 miss% 0.05723531627639594
plot_id,batch_id 0 84 miss% 0.03591368281752313
plot_id,batch_id 0 85 miss% 0.05181280282279181
plot_id,batch_id 0 86 miss% 0.031343250843012385
plot_id,batch_id 0 87 miss% 0.10992039592027063
plot_id,batch_id 0 88 miss% 0.0978686286187505
plot_id,batch_id 0 89 miss% 0.0805716746812303
plot_id,batch_id 0 90 miss% 0.05981402265017471
plot_id,batch_id 0 91 miss% 0.06567102319622406
plot_id,batch_id 0 92 miss% 0.05188827823055229
plot_id,batch_id 0 93 miss% 0.08988946856844592
plot_id,batch_id 0 94 miss% 0.06304814103284945
plot_id,batch_id 0 95 miss% 0.0693575676185909
plot_id,batch_id 0 96 miss% 0.049884453554685536
plot_id,batch_id 0 97 miss% 0.0791058708106185
plot_id,batch_id 0 98 miss% 0.045814288037408335
plot_id,batch_id 0 99 miss% 0.056437887735497856
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10794129 0.07260025 0.05203028 0.0719159  0.05706211 0.09062404
 0.05408086 0.1005076  0.08293857 0.06209721 0.02728708 0.07247137
 0.05935565 0.06459834 0.09640383 0.04076926 0.07340711 0.11309776
 0.05046675 0.12357105 0.07212359 0.07126575 0.03477966 0.06744037
 0.06340611 0.04519963 0.05400174 0.05577182 0.04056157 0.09995858
 0.05035469 0.08966526 0.11984358 0.07557996 0.02766038 0.05048872
 0.08626823 0.07958017 0.09427117 0.0684007  0.06932011 0.05107477
 0.06994262 0.07502179 0.05576049 0.04594926 0.03071869 0.04322212
 0.04273939 0.03161561 0.13396115 0.02676979 0.02739871 0.02990685
 0.02407409 0.07787401 0.05041471 0.03299229 0.04762384 0.04722553
 0.04025464 0.04203484 0.08418594 0.05382743 0.0407817  0.11000594
 0.06383434 0.04275935 0.06136669 0.10240347 0.06557012 0.08225521
 0.08139839 0.100661   0.1045649  0.06742044 0.09426247 0.04937313
 0.02895517 0.03865972 0.04603139 0.094065   0.05730029 0.05723532
 0.03591368 0.0518128  0.03134325 0.1099204  0.09786863 0.08057167
 0.05981402 0.06567102 0.05188828 0.08988947 0.06304814 0.06935757
 0.04988445 0.07910587 0.04581429 0.05643789]
for model  17 the mean error 0.06481002176424229
all id 17 hidden_dim 32 learning_rate 0.0025 num_layers 4 frames 21 out win 6 err 0.06481002176424229 time 15534.541383266449
Launcher: Job 18 completed in 15801 seconds.
Launcher: Task 13 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  154129
Epoch:0, Train loss:0.425111, valid loss:0.448003
Epoch:1, Train loss:0.033092, valid loss:0.003502
Epoch:2, Train loss:0.009661, valid loss:0.003137
Epoch:3, Train loss:0.009192, valid loss:0.002819
Epoch:4, Train loss:0.008717, valid loss:0.002532
Epoch:5, Train loss:0.005497, valid loss:0.000943
Epoch:6, Train loss:0.001709, valid loss:0.000840
Epoch:7, Train loss:0.001527, valid loss:0.000857
Epoch:8, Train loss:0.001420, valid loss:0.000787
Epoch:9, Train loss:0.001335, valid loss:0.000707
Epoch:10, Train loss:0.001253, valid loss:0.000673
Epoch:11, Train loss:0.000910, valid loss:0.000558
Epoch:12, Train loss:0.000877, valid loss:0.000563
Epoch:13, Train loss:0.000859, valid loss:0.000750
Epoch:14, Train loss:0.000836, valid loss:0.000700
Epoch:15, Train loss:0.000838, valid loss:0.000515
Epoch:16, Train loss:0.000778, valid loss:0.000525
Epoch:17, Train loss:0.000750, valid loss:0.000521
Epoch:18, Train loss:0.000764, valid loss:0.000533
Epoch:19, Train loss:0.000739, valid loss:0.000468
Epoch:20, Train loss:0.000701, valid loss:0.000484
Epoch:21, Train loss:0.000551, valid loss:0.000577
Epoch:22, Train loss:0.000553, valid loss:0.000419
Epoch:23, Train loss:0.000523, valid loss:0.000437
Epoch:24, Train loss:0.000514, valid loss:0.000421
Epoch:25, Train loss:0.000503, valid loss:0.000428
Epoch:26, Train loss:0.000516, valid loss:0.000448
Epoch:27, Train loss:0.000496, valid loss:0.000435
Epoch:28, Train loss:0.000492, valid loss:0.000440
Epoch:29, Train loss:0.000491, valid loss:0.000552
Epoch:30, Train loss:0.000480, valid loss:0.000421
Epoch:31, Train loss:0.000399, valid loss:0.000421
Epoch:32, Train loss:0.000388, valid loss:0.000422
Epoch:33, Train loss:0.000393, valid loss:0.000392
Epoch:34, Train loss:0.000388, valid loss:0.000439
Epoch:35, Train loss:0.000378, valid loss:0.000382
Epoch:36, Train loss:0.000385, valid loss:0.000380
Epoch:37, Train loss:0.000374, valid loss:0.000393
Epoch:38, Train loss:0.000385, valid loss:0.000430
Epoch:39, Train loss:0.000373, valid loss:0.000402
Epoch:40, Train loss:0.000372, valid loss:0.000391
Epoch:41, Train loss:0.000334, valid loss:0.000390
Epoch:42, Train loss:0.000330, valid loss:0.000391
Epoch:43, Train loss:0.000330, valid loss:0.000386
Epoch:44, Train loss:0.000327, valid loss:0.000384
Epoch:45, Train loss:0.000328, valid loss:0.000374
Epoch:46, Train loss:0.000328, valid loss:0.000400
Epoch:47, Train loss:0.000322, valid loss:0.000405
Epoch:48, Train loss:0.000320, valid loss:0.000396
Epoch:49, Train loss:0.000322, valid loss:0.000402
Epoch:50, Train loss:0.000320, valid loss:0.000400
Epoch:51, Train loss:0.000304, valid loss:0.000396
Epoch:52, Train loss:0.000302, valid loss:0.000396
Epoch:53, Train loss:0.000301, valid loss:0.000394
Epoch:54, Train loss:0.000300, valid loss:0.000394
Epoch:55, Train loss:0.000299, valid loss:0.000394
Epoch:56, Train loss:0.000299, valid loss:0.000396
Epoch:57, Train loss:0.000299, valid loss:0.000393
Epoch:58, Train loss:0.000298, valid loss:0.000392
Epoch:59, Train loss:0.000298, valid loss:0.000392
Epoch:60, Train loss:0.000298, valid loss:0.000390
training time 15597.945578098297
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.07886403561260093
plot_id,batch_id 0 1 miss% 0.06443093287763614
plot_id,batch_id 0 2 miss% 0.08568794419372976
plot_id,batch_id 0 3 miss% 0.04801043270044669
plot_id,batch_id 0 4 miss% 0.03229654878231819
plot_id,batch_id 0 5 miss% 0.026204981799140416
plot_id,batch_id 0 6 miss% 0.047094011777573996
plot_id,batch_id 0 7 miss% 0.06418989452730278
plot_id,batch_id 0 8 miss% 0.08456727801887878
plot_id,batch_id 0 9 miss% 0.08259775007721609
plot_id,batch_id 0 10 miss% 0.03225518469299309
plot_id,batch_id 0 11 miss% 0.05436496257145369
plot_id,batch_id 0 12 miss% 0.06580238943530249
plot_id,batch_id 0 13 miss% 0.0349048544609025
plot_id,batch_id 0 14 miss% 0.07339763643397881
plot_id,batch_id 0 15 miss% 0.03462154144404145
plot_id,batch_id 0 16 miss% 0.16208398541247956
plot_id,batch_id 0 17 miss% 0.04723372961139039
plot_id,batch_id 0 18 miss% 0.07848385735628673
plot_id,batch_id 0 19 miss% 0.07635639260485506
plot_id,batch_id 0 20 miss% 0.06273286402572466
plot_id,batch_id 0 21 miss% 0.050298058118442165
plot_id,batch_id 0 22 miss% 0.07142604786785088
plot_id,batch_id 0 23 miss% 0.03739912314647769
plot_id,batch_id 0 24 miss% 0.03863569125016841
plot_id,batch_id 0 25 miss% 0.036507925478813694
plot_id,batch_id 0 26 miss% 0.0507278407543871
plot_id,batch_id 0 27 miss% 0.03847466857801248
plot_id,batch_id 0 28 miss% 0.03936899187737566
plot_id,batch_id 0 29 miss% 0.034179962658401127
plot_id,batch_id 0 30 miss% 0.038823677235258244
plot_id,batch_id 0 31 miss% 0.09884701201994595
plot_id,batch_id 0 32 miss% 0.11025134342030471
plot_id,batch_id 0 33 miss% 0.059978686704802824
plot_id,batch_id 0 34 miss% 0.021521178985246022
plot_id,batch_id 0 35 miss% 0.02753550150665645
plot_id,batch_id 0 36 miss% 0.113614844607784
plot_id,batch_id 0 37 miss% 0.0997350699474722
plot_id,batch_id 0 38 miss% 0.07924086250174033
plot_id,batch_id 0 39 miss% 0.021298524341009473
plot_id,batch_id 0 40 miss% 0.05800153083977114
plot_id,batch_id 0 41 miss% 0.043873923917374776
plot_id,batch_id 0 42 miss% 0.02481640739237248
plot_id,batch_id 0 43 miss% 0.08196953183049847
plot_id,batch_id 0 44 miss% 0.03471924668890937
plot_id,batch_id 0 45 miss% 0.04591332217978457
plot_id,batch_id 0 46 miss% 0.0366129801682118
plot_id,batch_id 0 47 miss% 0.0386900314822386
plot_id,batch_id 0 48 miss% 0.025534116333402617
plot_id,batch_id 0 49 miss% 0.056951127977438065
plot_id,batch_id 0 50 miss% 0.10664879987260577
plot_id,batch_id 0 51 miss% 0.05403831469655389
plot_id,batch_id 0 52 miss% 0.03679312196947572
plot_id,batch_id 0 53 miss% 0.019481274597067662
plot_id,batch_id 0 54 miss% 0.039564536805803525
plot_id,batch_id 0 55 miss% 0.0658131793069246
plot_id,batch_id 0 56 miss% 0.07621540797847617
plot_id,batch_id 0 57 miss% 0.03574661285246377
plot_id,batch_id 0 58 miss% 0.03823295383508742
plot_id,batch_id 0 59 miss% 0.024724094588482258
plot_id,batch_id 0 60 miss% 0.026577896246574067
plot_id,batch_id 0 61 miss% 0.01983912102113892
plot_id,batch_id 0 62 miss% 0.03849870897871023
plot_id,batch_id 0 63 miss% 0.043159827272197
plot_id,batch_id 0 64 miss% 0.06002108433241456
plot_id,batch_id 0 the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  151697
Epoch:0, Train loss:0.517722, valid loss:0.476178
Epoch:1, Train loss:0.368904, valid loss:0.365561
Epoch:2, Train loss:0.360392, valid loss:0.365213
Epoch:3, Train loss:0.359224, valid loss:0.364631
Epoch:4, Train loss:0.358699, valid loss:0.364536
Epoch:5, Train loss:0.358332, valid loss:0.364380
Epoch:6, Train loss:0.358137, valid loss:0.364256
Epoch:7, Train loss:0.357871, valid loss:0.364226
Epoch:8, Train loss:0.357759, valid loss:0.364017
Epoch:9, Train loss:0.357681, valid loss:0.364012
Epoch:10, Train loss:0.357564, valid loss:0.364402
Epoch:11, Train loss:0.357124, valid loss:0.363828
Epoch:12, Train loss:0.357041, valid loss:0.364027
Epoch:13, Train loss:0.357013, valid loss:0.363756
Epoch:14, Train loss:0.356967, valid loss:0.363764
Epoch:15, Train loss:0.356948, valid loss:0.363809
Epoch:16, Train loss:0.356937, valid loss:0.363783
Epoch:17, Train loss:0.356866, valid loss:0.363749
Epoch:18, Train loss:0.356913, valid loss:0.363706
Epoch:19, Train loss:0.356799, valid loss:0.363922
Epoch:20, Train loss:0.356839, valid loss:0.363760
Epoch:21, Train loss:0.356600, valid loss:0.363640
Epoch:22, Train loss:0.356558, valid loss:0.363681
Epoch:23, Train loss:0.356556, valid loss:0.363664
Epoch:24, Train loss:0.356548, valid loss:0.363691
Epoch:25, Train loss:0.356529, valid loss:0.363600
Epoch:26, Train loss:0.356529, valid loss:0.363649
Epoch:27, Train loss:0.356507, valid loss:0.363655
Epoch:28, Train loss:0.356502, valid loss:0.363618
Epoch:29, Train loss:0.356489, valid loss:0.363636
Epoch:30, Train loss:0.356514, valid loss:0.363646
Epoch:31, Train loss:0.356389, valid loss:0.363615
Epoch:32, Train loss:0.356359, valid loss:0.363631
Epoch:33, Train loss:0.356361, valid loss:0.363646
Epoch:34, Train loss:0.356354, valid loss:0.363631
Epoch:35, Train loss:0.356350, valid loss:0.363613
Epoch:36, Train loss:0.356341, valid loss:0.363596
Epoch:37, Train loss:0.356349, valid loss:0.363604
Epoch:38, Train loss:0.356335, valid loss:0.363607
Epoch:39, Train loss:0.356343, valid loss:0.363631
Epoch:40, Train loss:0.356322, valid loss:0.363598
Epoch:41, Train loss:0.356280, valid loss:0.363603
Epoch:42, Train loss:0.356271, valid loss:0.363575
Epoch:43, Train loss:0.356278, valid loss:0.363595
Epoch:44, Train loss:0.356268, valid loss:0.363589
Epoch:45, Train loss:0.356270, valid loss:0.363578
Epoch:46, Train loss:0.356261, valid loss:0.363605
Epoch:47, Train loss:0.356259, valid loss:0.363590
Epoch:48, Train loss:0.356255, valid loss:0.363584
Epoch:49, Train loss:0.356253, valid loss:0.363579
Epoch:50, Train loss:0.356256, valid loss:0.363561
Epoch:51, Train loss:0.356234, valid loss:0.363573
Epoch:52, Train loss:0.356230, valid loss:0.363570
Epoch:53, Train loss:0.356228, valid loss:0.363575
Epoch:54, Train loss:0.356227, valid loss:0.363574
Epoch:55, Train loss:0.356226, valid loss:0.363574
Epoch:56, Train loss:0.356225, valid loss:0.363572
Epoch:57, Train loss:0.356225, valid loss:0.363576
Epoch:58, Train loss:0.356225, valid loss:0.363572
Epoch:59, Train loss:0.356224, valid loss:0.363581
Epoch:60, Train loss:0.356224, valid loss:0.363576
training time 15730.234388589859
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.8506019027170553
plot_id,batch_id 0 1 miss% 0.8235390610303964
plot_id,batch_id 0 2 miss% 0.8185442187915369
plot_id,batch_id 0 3 miss% 0.81453602165228
plot_id,batch_id 0 4 miss% 0.8140704875884662
plot_id,batch_id 0 5 miss% 0.8591521780296874
plot_id,batch_id 0 6 miss% 0.8252161882107837
plot_id,batch_id 0 7 miss% 0.8212627582075073
plot_id,batch_id 0 8 miss% 0.8168534845286737
plot_id,batch_id 0 9 miss% 0.8168833126688041
plot_id,batch_id 0 10 miss% 0.8527566124686022
plot_id,batch_id 0 11 miss% 0.8245561835003783
plot_id,batch_id 0 12 miss% 0.8176502254635856
plot_id,batch_id 0 13 miss% 0.8120487299322475
plot_id,batch_id 0 14 miss% 0.814690068255654
plot_id,batch_id 0 15 miss% 0.8492106911071142
plot_id,batch_id 0 16 miss% 0.828102962487929
plot_id,batch_id 0 17 miss% 0.818500093100965
plot_id,batch_id 0 18 miss% 0.8206426797441366
plot_id,batch_id 0 19 miss% 0.8219758571769794
plot_id,batch_id 0 20 miss% 0.8440659304452879
plot_id,batch_id 0 21 miss% 0.8191017759870937
plot_id,batch_id 0 22 miss% 0.815714390622811
plot_id,batch_id 0 23 miss% 0.8125344545056108
plot_id,batch_id 0 24 miss% 0.8118498047185864
plot_id,batch_id 0 25 miss% 0.8348873087261115
plot_id,batch_id 0 26 miss% 0.8210836817343666
plot_id,batch_id 0 27 miss% 0.8159917614292151
plot_id,batch_id 0 28 miss% 0.813422922847909
plot_id,batch_id 0 29 miss% 0.8125012345312848
plot_id,batch_id 0 30 miss% 0.831515609674116
plot_id,batch_id 0 31 miss% 0.8195878429354915
plot_id,batch_id 0 32 miss% 0.81646917525008
plot_id,batch_id 0 33 miss% 0.8153464109357402
plot_id,batch_id 0 34 miss% 0.8118267484928469
plot_id,batch_id 0 35 miss% 0.8442648130561615
plot_id,batch_id 0 36 miss% 0.8195677603510175
plot_id,batch_id 0 37 miss% 0.8186410942216459
plot_id,batch_id 0 38 miss% 0.8129924208221669
plot_id,batch_id 0 39 miss% 0.8140121562028829
plot_id,batch_id 0 40 miss% 0.8305232025218734
plot_id,batch_id 0 41 miss% 0.8149483406615239
plot_id,batch_id 0 42 miss% 0.810892500557419
plot_id,batch_id 0 43 miss% 0.8105051656953183
plot_id,batch_id 0 44 miss% 0.8097113738396378
plot_id,batch_id 0 45 miss% 0.822911765026614
plot_id,batch_id 0 46 miss% 0.8154042077993785
plot_id,batch_id 0 47 miss% 0.8122318572053534
plot_id,batch_id 0 48 miss% 0.8133688977902364
plot_id,batch_id 0 49 miss% 0.8097415558089288
plot_id,batch_id 0 50 miss% 0.8277805498321352
plot_id,batch_id 0 51 miss% 0.8137431477558953
plot_id,batch_id 0 52 miss% 0.812591127839156
plot_id,batch_id 0 53 miss% 0.8096138803833032
plot_id,batch_id 0 54 miss% 0.8113363286413973
plot_id,batch_id 0 55 miss% 0.8254228085379471
plot_id,batch_id 0 56 miss% 0.816553586376002
plot_id,batch_id 0 57 miss% 0.8141725218649841
plot_id,batch_id 0 58 miss% 0.8109829718167141
plot_id,batch_id 0 59 miss% 0.8106205107219011
plot_id,batch_id 0 60 miss% 0.8887778810180245
plot_id,batch_id 0 61 miss% 0.8405081699085009
plot_id,batch_id 0 62 miss% 0.8293553536056902
plot_id,batch_id 0 63 miss% 0.8204398233704864
plot_id,batch_id 0 64 miss% 0.8191321518574506
plot_id,batch_id 0 65 miss% 0.8907736548211483
plot_id,batch_id 0 66 miss% 0.8516402142298817
plot_id,batch_id 0 67 miss% 0.8304982347847331
plot_id,batch_id 0 68 miss% 0.8219277514573455
plot_id,batch_id 0 69 miss% 0.8181985913883771
plot_id,batch_id 0 70 miss% 0.8803030170036146
plot_id,batch_id 0 71 miss% 0.8541669719632276
plot_id,batch_id 0 72 miss% 0.8306499792258103
plot_id,batch_id 0 73 miss% 0.8253588183195677
plot_id,batch_id 0 74 miss% 0.8212721191260204
plot_id,batch_id 0 75 miss% 0.8745536285477833
plot_id,batch_id 0 76 miss% 0.8430344328447447
plot_id,batch_id 0 77 miss% 0.8316437342843822
plot_id,batch_id 0 78 miss% 0.8260904007036359
plot_id,batch_id 0 79 miss% 0.823816869680579
plot_id,batch_id 0 80 miss% 0.8731448720367441
plot_id,batch_id 0 81 miss% 0.8368099959314105
plot_id,batch_id 0 82 miss% 0.8237447735481495
plot_id,batch_id 0 83 miss% 0.8186259515967044
plot_id,batch_id 0 84 miss% 0.814756448609282
plot_id,batch_id 0 85 miss% 0.8703657243714893
plot_id,batch_id 0 86 miss% 0.8309260629050346
plot_id,batch_id 0 87 miss% 0.8215280874752913
plot_id,batch_id 0 88 miss% 0.8186365559568705
plot_id,batch_id 0 89 miss% 0.8152097598750775
plot_id,batch_id 0 90 miss% 0.8876071699660959
plot_id,batch_id 0 91 miss% 0.8349921983831826
plot_id,batch_id 0 92 miss% 0.824115784545375
plot_id,batch_id 0 93 miss% 0.8208910916883492
plot_id,batch_id 0 94 miss% 0.8187522651465861
plot_id,batch_id 0 95 miss% 0.90620844134856
plot_id,batch_id 0 96 miss% 0.8389962641069451
plot_id,batch_id 0 97 miss% 0.8255841403696071
plot_id,batch_id 0 98 miss% 0.81983010515015
plot_id,batch_id 0 99 miss% 0.8188867516257701
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.8506019  0.82353906 0.81854422 0.81453602 0.81407049 0.85915218
 0.82521619 0.82126276 0.81685348 0.81688331 0.85275661 0.82455618
 0.81765023 0.81204873 0.81469007 0.84921069 0.82810296 0.81850009
 0.82064268 0.82197586 0.84406593 0.81910178 0.81571439 0.81253445
 0.8118498  0.83488731 0.82108368 0.81599176 0.81342292 0.81250123
 0.83151561 0.81958784 0.81646918 0.81534641 0.81182675 0.84426481
 0.81956776 0.81864109 0.81299242 0.81401216 0.8305232  0.81494834
 0.8108925  0.81050517 0.80971137 0.82291177 0.81540421 0.81223186
 0.8133689  0.80974156 0.82778055 0.81374315 0.81259113 0.80961388
 0.81133633 0.82542281 0.81655359 0.81417252 0.81098297 0.81062051
 0.88877788 0.84050817 0.82935535 0.82043982 0.81913215 0.89077365
 0.85164021 0.83049823 0.82192775 0.81819859 0.88030302 0.85416697
 0.83064998 0.82535882 0.82127212 0.87455363 0.84303443 0.83164373
 0.8260904  0.82381687 0.87314487 0.83681    0.82374477 0.81862595
 0.81475645 0.87036572 0.83092606 0.82152809 0.81863656 0.81520976
 0.88760717 0.8349922  0.82411578 0.82089109 0.81875227 0.90620844
 0.83899626 0.82558414 0.81983011 0.81888675]
for model  130 the mean error 0.827709815576066
all id 130 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 25 out win 5 err 0.827709815576066 time 15730.234388589859
Launcher: Job 131 completed in 15861 seconds.
Launcher: Task 169 done. Exiting.
65 miss% 0.15274306053996542
plot_id,batch_id 0 66 miss% 0.026144621817965636
plot_id,batch_id 0 67 miss% 0.027106576819654792
plot_id,batch_id 0 68 miss% 0.04970175610042451
plot_id,batch_id 0 69 miss% 0.055951660007949794
plot_id,batch_id 0 70 miss% 0.06837415498328597
plot_id,batch_id 0 71 miss% 0.02993361179484407
plot_id,batch_id 0 72 miss% 0.04390565046734
plot_id,batch_id 0 73 miss% 0.06488629538314854
plot_id,batch_id 0 74 miss% 0.08231725688843572
plot_id,batch_id 0 75 miss% 0.050610872880920324
plot_id,batch_id 0 76 miss% 0.0578238969334977
plot_id,batch_id 0 77 miss% 0.02372543779222582
plot_id,batch_id 0 78 miss% 0.023069773326133636
plot_id,batch_id 0 79 miss% 0.17455188783144815
plot_id,batch_id 0 80 miss% 0.03708168332220199
plot_id,batch_id 0 81 miss% 0.08069556876670247
plot_id,batch_id 0 82 miss% 0.04695912145585685
plot_id,batch_id 0 83 miss% 0.09057040015001482
plot_id,batch_id 0 84 miss% 0.05517627765499731
plot_id,batch_id 0 85 miss% 0.0616749643315421
plot_id,batch_id 0 86 miss% 0.07143176071318544
plot_id,batch_id 0 87 miss% 0.05242728589362293
plot_id,batch_id 0 88 miss% 0.0749759820807843
plot_id,batch_id 0 89 miss% 0.076332781767079
plot_id,batch_id 0 90 miss% 0.024308605110440234
plot_id,batch_id 0 91 miss% 0.041594246747960674
plot_id,batch_id 0 92 miss% 0.04258338445624597
plot_id,batch_id 0 93 miss% 0.02364226133284486
plot_id,batch_id 0 94 miss% 0.06802409549445683
plot_id,batch_id 0 95 miss% 0.04213427916848856
plot_id,batch_id 0 96 miss% 0.0405790306321422
plot_id,batch_id 0 97 miss% 0.04054006636372492
plot_id,batch_id 0 98 miss% 0.02662604180796281
plot_id,batch_id 0 99 miss% 0.046748907750416076
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07886404 0.06443093 0.08568794 0.04801043 0.03229655 0.02620498
 0.04709401 0.06418989 0.08456728 0.08259775 0.03225518 0.05436496
 0.06580239 0.03490485 0.07339764 0.03462154 0.16208399 0.04723373
 0.07848386 0.07635639 0.06273286 0.05029806 0.07142605 0.03739912
 0.03863569 0.03650793 0.05072784 0.03847467 0.03936899 0.03417996
 0.03882368 0.09884701 0.11025134 0.05997869 0.02152118 0.0275355
 0.11361484 0.09973507 0.07924086 0.02129852 0.05800153 0.04387392
 0.02481641 0.08196953 0.03471925 0.04591332 0.03661298 0.03869003
 0.02553412 0.05695113 0.1066488  0.05403831 0.03679312 0.01948127
 0.03956454 0.06581318 0.07621541 0.03574661 0.03823295 0.02472409
 0.0265779  0.01983912 0.03849871 0.04315983 0.06002108 0.15274306
 0.02614462 0.02710658 0.04970176 0.05595166 0.06837415 0.02993361
 0.04390565 0.0648863  0.08231726 0.05061087 0.0578239  0.02372544
 0.02306977 0.17455189 0.03708168 0.08069557 0.04695912 0.0905704
 0.05517628 0.06167496 0.07143176 0.05242729 0.07497598 0.07633278
 0.02430861 0.04159425 0.04258338 0.02364226 0.0680241  0.04213428
 0.04057903 0.04054007 0.02662604 0.04674891]
for model  195 the mean error 0.05511436631148289
all id 195 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 31 out win 4 err 0.05511436631148289 time 15597.945578098297
Launcher: Job 196 completed in 15863 seconds.
Launcher: Task 186 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  69649
Epoch:0, Train loss:0.496002, valid loss:0.480569
Epoch:1, Train loss:0.020953, valid loss:0.003037
Epoch:2, Train loss:0.007492, valid loss:0.002322
Epoch:3, Train loss:0.006866, valid loss:0.002331
Epoch:4, Train loss:0.006355, valid loss:0.001480
Epoch:5, Train loss:0.002877, valid loss:0.001147
Epoch:6, Train loss:0.001958, valid loss:0.000904
Epoch:7, Train loss:0.001766, valid loss:0.001150
Epoch:8, Train loss:0.001656, valid loss:0.000835
Epoch:9, Train loss:0.001544, valid loss:0.000818
Epoch:10, Train loss:0.001428, valid loss:0.000693
Epoch:11, Train loss:0.001070, valid loss:0.000692
Epoch:12, Train loss:0.001047, valid loss:0.000650
Epoch:13, Train loss:0.001015, valid loss:0.000648
Epoch:14, Train loss:0.000986, valid loss:0.000594
Epoch:15, Train loss:0.000963, valid loss:0.000699
Epoch:16, Train loss:0.000934, valid loss:0.000599
Epoch:17, Train loss:0.000904, valid loss:0.000583
Epoch:18, Train loss:0.000896, valid loss:0.000554
Epoch:19, Train loss:0.000888, valid loss:0.000532
Epoch:20, Train loss:0.000851, valid loss:0.000562
Epoch:21, Train loss:0.000701, valid loss:0.000459
Epoch:22, Train loss:0.000674, valid loss:0.000492
Epoch:23, Train loss:0.000664, valid loss:0.000478
Epoch:24, Train loss:0.000660, valid loss:0.000494
Epoch:25, Train loss:0.000659, valid loss:0.000490
Epoch:26, Train loss:0.000647, valid loss:0.000670
Epoch:27, Train loss:0.000642, valid loss:0.000460
Epoch:28, Train loss:0.000633, valid loss:0.000473
Epoch:29, Train loss:0.000621, valid loss:0.000451
Epoch:30, Train loss:0.000612, valid loss:0.000453
Epoch:31, Train loss:0.000538, valid loss:0.000442
Epoch:32, Train loss:0.000530, valid loss:0.000420
Epoch:33, Train loss:0.000524, valid loss:0.000417
Epoch:34, Train loss:0.000517, valid loss:0.000451
Epoch:35, Train loss:0.000518, valid loss:0.000452
Epoch:36, Train loss:0.000511, valid loss:0.000441
Epoch:37, Train loss:0.000508, valid loss:0.000421
Epoch:38, Train loss:0.000505, valid loss:0.000431
Epoch:39, Train loss:0.000499, valid loss:0.000445
Epoch:40, Train loss:0.000502, valid loss:0.000447
Epoch:41, Train loss:0.000456, valid loss:0.000441
Epoch:42, Train loss:0.000451, valid loss:0.000441
Epoch:43, Train loss:0.000452, valid loss:0.000430
Epoch:44, Train loss:0.000449, valid loss:0.000430
Epoch:45, Train loss:0.000446, valid loss:0.000433
Epoch:46, Train loss:0.000447, valid loss:0.000445
Epoch:47, Train loss:0.000446, valid loss:0.000437
Epoch:48, Train loss:0.000442, valid loss:0.000437
Epoch:49, Train loss:0.000437, valid loss:0.000441
Epoch:50, Train loss:0.000437, valid loss:0.000448
Epoch:51, Train loss:0.000417, valid loss:0.000439
Epoch:52, Train loss:0.000414, valid loss:0.000431
Epoch:53, Train loss:0.000412, valid loss:0.000430
Epoch:54, Train loss:0.000412, valid loss:0.000429
Epoch:55, Train loss:0.000411, valid loss:0.000431
Epoch:56, Train loss:0.000410, valid loss:0.000429
Epoch:57, Train loss:0.000410, valid loss:0.000428
Epoch:58, Train loss:0.000409, valid loss:0.000427
Epoch:59, Train loss:0.000409, valid loss:0.000428
Epoch:60, Train loss:0.000409, valid loss:0.000431
training time 15666.490195989609
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.10079868882526097
plot_id,batch_id 0 1 miss% 0.029623844657812023
plot_id,batch_id 0 2 miss% 0.09133020128336553
plot_id,batch_id 0 3 miss% 0.056637835247895105
plot_id,batch_id 0 4 miss% 0.10393758722779078
plot_id,batch_id 0 5 miss% 0.04331902972291169
plot_id,batch_id 0 6 miss% 0.06223241811504337
plot_id,batch_id 0 7 miss% 0.07146293671743324
plot_id,batch_id 0 8 miss% 0.05752764478898642
plot_id,batch_id 0 9 miss% 0.02475447233431369
plot_id,batch_id 0 10 miss% 0.03902887033253914
plot_id,batch_id 0 11 miss% 0.07000375891075601
plot_id,batch_id 0 12 miss% 0.09308632139507639
plot_id,batch_id 0 13 miss% 0.054521580907895585
plot_id,batch_id 0 14 miss% 0.10839097749216081
plot_id,batch_id 0 15 miss% 0.03246040436378488
plot_id,batch_id 0 16 miss% 0.052127773438646274
plot_id,batch_id 0 17 miss% 0.04739889828218031
plot_id,batch_id 0 18 miss% 0.06869575439849429
plot_id,batch_id 0 19 miss% 0.08372458600638069
plot_id,batch_id 0 20 miss% 0.08106476572492308
plot_id,batch_id 0 21 miss% 0.08743084569221017
plot_id,batch_id 0 22 miss% 0.0681935356577216
plot_id,batch_id 0 23 miss% 0.05072712892589695
plot_id,batch_id 0 24 miss% 0.07515072961194919
plot_id,batch_id 0 25 miss% 0.07658657772195845
plot_id,batch_id 0 26 miss% 0.053845046393079965
plot_id,batch_id 0 27 miss% 0.05577976088317279
plot_id,batch_id 0 28 miss% 0.03544883385285834
plot_id,batch_id 0 29 miss% 0.03738853202894739
plot_id,batch_id 0 30 miss% 0.04190578356338186
plot_id,batch_id 0 31 miss% 0.11759254641857583
plot_id,batch_id 0 32 miss% 0.13785477721261563
plot_id,batch_id 0 33 miss% 0.06612318310125367
plot_id,batch_id 0 34 miss% 0.10395519865140149
plot_id,batch_id 0 35 miss% 0.05430170861299019
plot_id,batch_id 0 36 miss% 0.10776514974076426
plot_id,batch_id 0 37 miss% 0.09772440428648974
plot_id,batch_id 0 38 miss% 0.07131513547910005
plot_id,batch_id 0 39 miss% 0.06630258722339642
plot_id,batch_id 0 40 miss% 0.12879968549867693
plot_id,batch_id 0 41 miss% 0.08803806333688181
plot_id,batch_id 0 42 miss% 0.05602100619658599
plot_id,batch_id 0 43 miss% 0.08308975579154616
plot_id,batch_id 0 44 miss% 0.03653541482619584
plot_id,batch_id 0 45 miss% 0.04570996167275324
plot_id,batch_id 0 46 miss% 0.05150085409396616
plot_id,batch_id 0 47 miss% 0.07617628280862107
plot_id,batch_id 0 48 miss% 0.030809732119001457
plot_id,batch_id 0 49 miss% 0.03778450514540914
plot_id,batch_id 0 50 miss% 0.12729446771211295
plot_id,batch_id 0 51 miss% 0.05755590690588866
plot_id,batch_id 0 52 miss% 0.036409796372789925
plot_id,batch_id 0 53 miss% 0.05594487995192352
plot_id,batch_id 0 54 miss% 0.06266623781070767
plot_id,batch_id 0 55 miss% 0.07865763609688525
plot_id,batch_id 0 56 miss% 0.11389562538432257
plot_id,batch_id 0 57 miss% 0.05143277957263141
plot_id,batch_id 0 58 miss% 0.03888242878523748
plot_id,batch_id 0 59 miss% 0.07593593944127268
plot_id,batch_id 0 60 miss% 0.02513751654180884
plot_id,batch_id 0 61 miss% 0.043684477115407766
plot_id,batch_id 0 62 miss% 0.057579277647069146
plot_id,batch_id 0 63 miss% 0.04540319491397407
plot_id,batch_id 0 64 miss% 0.045134368556043115
plot_id,batch_id 0the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  69649
Epoch:0, Train loss:0.496002, valid loss:0.480569
Epoch:1, Train loss:0.025402, valid loss:0.002479
Epoch:2, Train loss:0.004392, valid loss:0.001584
Epoch:3, Train loss:0.002887, valid loss:0.001437
Epoch:4, Train loss:0.002377, valid loss:0.001110
Epoch:5, Train loss:0.002069, valid loss:0.001005
Epoch:6, Train loss:0.001853, valid loss:0.000811
Epoch:7, Train loss:0.001699, valid loss:0.000953
Epoch:8, Train loss:0.001552, valid loss:0.000904
Epoch:9, Train loss:0.001480, valid loss:0.000686
Epoch:10, Train loss:0.001364, valid loss:0.000739
Epoch:11, Train loss:0.001083, valid loss:0.000616
Epoch:12, Train loss:0.001060, valid loss:0.000584
Epoch:13, Train loss:0.001010, valid loss:0.000597
Epoch:14, Train loss:0.000991, valid loss:0.000594
Epoch:15, Train loss:0.000970, valid loss:0.000710
Epoch:16, Train loss:0.000942, valid loss:0.000547
Epoch:17, Train loss:0.000920, valid loss:0.000517
Epoch:18, Train loss:0.000880, valid loss:0.000527
Epoch:19, Train loss:0.000892, valid loss:0.000544
Epoch:20, Train loss:0.000867, valid loss:0.000564
Epoch:21, Train loss:0.000731, valid loss:0.000459
Epoch:22, Train loss:0.000721, valid loss:0.000491
Epoch:23, Train loss:0.000704, valid loss:0.000471
Epoch:24, Train loss:0.000692, valid loss:0.000480
Epoch:25, Train loss:0.000697, valid loss:0.000518
Epoch:26, Train loss:0.000682, valid loss:0.000507
Epoch:27, Train loss:0.000679, valid loss:0.000466
Epoch:28, Train loss:0.000677, valid loss:0.000488
Epoch:29, Train loss:0.000660, valid loss:0.000510
Epoch:30, Train loss:0.000666, valid loss:0.000478
Epoch:31, Train loss:0.000588, valid loss:0.000443
Epoch:32, Train loss:0.000582, valid loss:0.000424
Epoch:33, Train loss:0.000573, valid loss:0.000429
Epoch:34, Train loss:0.000572, valid loss:0.000428
Epoch:35, Train loss:0.000568, valid loss:0.000416
Epoch:36, Train loss:0.000570, valid loss:0.000419
Epoch:37, Train loss:0.000560, valid loss:0.000434
Epoch:38, Train loss:0.000556, valid loss:0.000422
Epoch:39, Train loss:0.000556, valid loss:0.000433
Epoch:40, Train loss:0.000550, valid loss:0.000448
Epoch:41, Train loss:0.000515, valid loss:0.000414
Epoch:42, Train loss:0.000511, valid loss:0.000421
Epoch:43, Train loss:0.000513, valid loss:0.000407
Epoch:44, Train loss:0.000507, valid loss:0.000407
Epoch:45, Train loss:0.000506, valid loss:0.000412
Epoch:46, Train loss:0.000507, valid loss:0.000414
Epoch:47, Train loss:0.000505, valid loss:0.000422
Epoch:48, Train loss:0.000503, valid loss:0.000455
Epoch:49, Train loss:0.000499, valid loss:0.000419
Epoch:50, Train loss:0.000498, valid loss:0.000438
Epoch:51, Train loss:0.000477, valid loss:0.000412
Epoch:52, Train loss:0.000474, valid loss:0.000404
Epoch:53, Train loss:0.000473, valid loss:0.000404
Epoch:54, Train loss:0.000473, valid loss:0.000405
Epoch:55, Train loss:0.000472, valid loss:0.000409
Epoch:56, Train loss:0.000472, valid loss:0.000404
Epoch:57, Train loss:0.000471, valid loss:0.000404
Epoch:58, Train loss:0.000471, valid loss:0.000400
Epoch:59, Train loss:0.000471, valid loss:0.000405
Epoch:60, Train loss:0.000471, valid loss:0.000406
training time 15703.095125436783
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.0789200219183571
plot_id,batch_id 0 1 miss% 0.07411892467101802
plot_id,batch_id 0 2 miss% 0.09968979657569028
plot_id,batch_id 0 3 miss% 0.0845645947820255
plot_id,batch_id 0 4 miss% 0.04862189198248187
plot_id,batch_id 0 5 miss% 0.0521352465377657
plot_id,batch_id 0 6 miss% 0.06498544752047343
plot_id,batch_id 0 7 miss% 0.10255604441883351
plot_id,batch_id 0 8 miss% 0.08373499019517383
plot_id,batch_id 0 9 miss% 0.050014696921894274
plot_id,batch_id 0 10 miss% 0.04285433543370133
plot_id,batch_id 0 11 miss% 0.055172822563791316
plot_id,batch_id 0 12 miss% 0.062137553744359024
plot_id,batch_id 0 13 miss% 0.05669288906665708
plot_id,batch_id 0 14 miss% 0.0685346850093678
plot_id,batch_id 0 15 miss% 0.06363448013038944
plot_id,batch_id 0 16 miss% 0.06665347198569871
plot_id,batch_id 0 17 miss% 0.033621591539401016
plot_id,batch_id 0 18 miss% 0.08414888537933272
plot_id,batch_id 0 19 miss% 0.0788741776105423
plot_id,batch_id 0 20 miss% 0.057304870080474156
plot_id,batch_id 0 21 miss% 0.03735181430014352
plot_id,batch_id 0 22 miss% 0.05848461418184117
plot_id,batch_id 0 23 miss% 0.05275858009531679
plot_id,batch_id 0 24 miss% 0.024549950541523247
plot_id,batch_id 0 25 miss% 0.11250854581974842
plot_id,batch_id 0 26 miss% 0.04074353646777953
plot_id,batch_id 0 27 miss% 0.05070928300667326
plot_id,batch_id 0 28 miss% 0.03658345735750084
plot_id,batch_id 0 29 miss% 0.04170764752458517
plot_id,batch_id 0 30 miss% 0.06688848005472903
plot_id,batch_id 0 31 miss% 0.11179438265948898
plot_id,batch_id 0 32 miss% 0.12536737708377785
plot_id,batch_id 0 33 miss% 0.07197255592994557
plot_id,batch_id 0 34 miss% 0.07280372977328872
plot_id,batch_id 0 35 miss% 0.025808031497009446
plot_id,batch_id 0 36 miss% 0.1066796624178517
plot_id,batch_id 0 37 miss% 0.10514443146656562
plot_id,batch_id 0 38 miss% 0.06471755569298103
plot_id,batch_id 0 39 miss% 0.0412413335760837
plot_id,batch_id 0 40 miss% 0.06577668214249138
plot_id,batch_id 0 41 miss% 0.04423529890601206
plot_id,batch_id 0 42 miss% 0.024099738786042157
plot_id,batch_id 0 43 miss% 0.05427581472522264
plot_id,batch_id 0 44 miss% 0.07054489156236797
plot_id,batch_id 0 45 miss% 0.04146370360504767
plot_id,batch_id 0 46 miss% 0.03263332898689648
plot_id,batch_id 0 47 miss% 0.03751540773512834
plot_id,batch_id 0 48 miss% 0.04560993077008118
plot_id,batch_id 0 49 miss% 0.05938392513270951
plot_id,batch_id 0 50 miss% 0.24876006356468303
plot_id,batch_id 0 51 miss% 0.026555162513855646
plot_id,batch_id 0 52 miss% 0.06114776729816718
plot_id,batch_id 0 53 miss% 0.03093738674415431
plot_id,batch_id 0 54 miss% 0.08637763612506262
plot_id,batch_id 0 55 miss% 0.11273286002612751
plot_id,batch_id 0 56 miss% 0.08603425553550623
plot_id,batch_id 0 57 miss% 0.05125169333747283
plot_id,batch_id 0 58 miss% 0.06319539421638319
plot_id,batch_id 0 59 miss% 0.04691025094456716
plot_id,batch_id 0 60 miss% 0.06099703885156582
plot_id,batch_id 0 61 miss% 0.06301677242025011
plot_id,batch_id 0 62 miss% 0.023665736126004436
plot_id,batch_id 0 63 miss% 0.04169014491299896
plot_id,batch_id 0 64 miss% 0.07837305855084296
plot_id,batch_id 0 65 the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  209681
Epoch:0, Train loss:0.459966, valid loss:0.469450
Epoch:1, Train loss:0.030849, valid loss:0.004818
Epoch:2, Train loss:0.011048, valid loss:0.003457
Epoch:3, Train loss:0.006204, valid loss:0.001904
Epoch:4, Train loss:0.003261, valid loss:0.001493
Epoch:5, Train loss:0.002788, valid loss:0.001299
Epoch:6, Train loss:0.002433, valid loss:0.001192
Epoch:7, Train loss:0.002179, valid loss:0.001281
Epoch:8, Train loss:0.002004, valid loss:0.001121
Epoch:9, Train loss:0.001862, valid loss:0.001134
Epoch:10, Train loss:0.001769, valid loss:0.000946
Epoch:11, Train loss:0.001279, valid loss:0.000795
Epoch:12, Train loss:0.001241, valid loss:0.000676
Epoch:13, Train loss:0.001193, valid loss:0.000811
Epoch:14, Train loss:0.001153, valid loss:0.000672
Epoch:15, Train loss:0.001101, valid loss:0.000706
Epoch:16, Train loss:0.001085, valid loss:0.000730
Epoch:17, Train loss:0.001081, valid loss:0.000865
Epoch:18, Train loss:0.001011, valid loss:0.000629
Epoch:19, Train loss:0.000986, valid loss:0.000637
Epoch:20, Train loss:0.000974, valid loss:0.000700
Epoch:21, Train loss:0.000744, valid loss:0.000520
Epoch:22, Train loss:0.000729, valid loss:0.000576
Epoch:23, Train loss:0.000706, valid loss:0.000533
Epoch:24, Train loss:0.000706, valid loss:0.000497
Epoch:25, Train loss:0.000693, valid loss:0.000539
Epoch:26, Train loss:0.000680, valid loss:0.000589
Epoch:27, Train loss:0.000683, valid loss:0.000518
Epoch:28, Train loss:0.000639, valid loss:0.000712
Epoch:29, Train loss:0.000649, valid loss:0.000513
Epoch:30, Train loss:0.000644, valid loss:0.000568
Epoch:31, Train loss:0.000537, valid loss:0.000527
Epoch:32, Train loss:0.000530, valid loss:0.000483
Epoch:33, Train loss:0.000522, valid loss:0.000467
Epoch:34, Train loss:0.000520, valid loss:0.000534
Epoch:35, Train loss:0.000521, valid loss:0.000478
Epoch:36, Train loss:0.000506, valid loss:0.000455
Epoch:37, Train loss:0.000506, valid loss:0.000478
Epoch:38, Train loss:0.000504, valid loss:0.000491
Epoch:39, Train loss:0.000493, valid loss:0.000489
Epoch:40, Train loss:0.000491, valid loss:0.000492
Epoch:41, Train loss:0.000440, valid loss:0.000459
Epoch:42, Train loss:0.000437, valid loss:0.000476
Epoch:43, Train loss:0.000435, valid loss:0.000446
Epoch:44, Train loss:0.000433, valid loss:0.000449
Epoch:45, Train loss:0.000436, valid loss:0.000472
Epoch:46, Train loss:0.000428, valid loss:0.000496
Epoch:47, Train loss:0.000428, valid loss:0.000471
Epoch:48, Train loss:0.000425, valid loss:0.000458
Epoch:49, Train loss:0.000424, valid loss:0.000491
Epoch:50, Train loss:0.000416, valid loss:0.000463
Epoch:51, Train loss:0.000393, valid loss:0.000448
Epoch:52, Train loss:0.000390, valid loss:0.000448
Epoch:53, Train loss:0.000388, valid loss:0.000447
Epoch:54, Train loss:0.000388, valid loss:0.000445
Epoch:55, Train loss:0.000387, valid loss:0.000448
Epoch:56, Train loss:0.000387, valid loss:0.000452
Epoch:57, Train loss:0.000386, valid loss:0.000447
Epoch:58, Train loss:0.000386, valid loss:0.000446
Epoch:59, Train loss:0.000386, valid loss:0.000449
Epoch:60, Train loss:0.000386, valid loss:0.000448
training time 15713.428469657898
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.04227573057608303
plot_id,batch_id 0 1 miss% 0.026223038864618256
plot_id,batch_id 0 2 miss% 0.10204152783534165
plot_id,batch_id 0 3 miss% 0.0348625888001018
plot_id,batch_id 0 4 miss% 0.05531705306741438
plot_id,batch_id 0 5 miss% 0.05952164835964114
plot_id,batch_id 0 6 miss% 0.05474515984665713
plot_id,batch_id 0 7 miss% 0.09730416490806394
plot_id,batch_id 0 8 miss% 0.06621123773952742
plot_id,batch_id 0 9 miss% 0.03539064269761847
plot_id,batch_id 0 10 miss% 0.03259746910322442
plot_id,batch_id 0 11 miss% 0.08300001448283972
plot_id,batch_id 0 12 miss% 0.07718233509692184
plot_id,batch_id 0 13 miss% 0.06530129621740766
plot_id,batch_id 0 14 miss% 0.0820674936880554
plot_id,batch_id 0 15 miss% 0.0443249739327748
plot_id,batch_id 0 16 miss% 0.15981507542058138
plot_id,batch_id 0 17 miss% 0.05260040385987262
plot_id,batch_id 0 18 miss% 0.06516852502535274
plot_id,batch_id 0 19 miss% 0.07899547379189113
plot_id,batch_id 0 20 miss% 0.07240141769669642
plot_id,batch_id 0 21 miss% 0.025614406110123947
plot_id,batch_id 0 22 miss% 0.05085179274416589
plot_id,batch_id 0 23 miss% 0.038752877106756754
plot_id,batch_id 0 24 miss% 0.056255063785628744
plot_id,batch_id 0 25 miss% 0.04331230727087633
plot_id,batch_id 0 26 miss% 0.0328164974230566
plot_id,batch_id 0 27 miss% 0.0326840483166192
plot_id,batch_id 0 28 miss% 0.047386220551191495
plot_id,batch_id 0 29 miss% 0.03210978329290056
plot_id,batch_id 0 30 miss% 0.03840280875411577
plot_id,batch_id 0 31 miss% 0.08705907454771251
plot_id,batch_id 0 32 miss% 0.11602957204360123
plot_id,batch_id 0 33 miss% 0.0453129198332326
plot_id,batch_id 0 34 miss% 0.03812164908750934
plot_id,batch_id 0 35 miss% 0.02719376128039932
plot_id,batch_id 0 36 miss% 0.09321089503235652
plot_id,batch_id 0 37 miss% 0.08151737524997975
plot_id,batch_id 0 38 miss% 0.04980629432338997
plot_id,batch_id 0 39 miss% 0.030143246268422297
plot_id,batch_id 0 40 miss% 0.07332177372647464
plot_id,batch_id 0 41 miss% 0.060352805039431426
plot_id,batch_id 0 42 miss% 0.02569279144424706
plot_id,batch_id 0 43 miss% 0.06728171047703428
plot_id,batch_id 0 44 miss% 0.035110807118566435
plot_id,batch_id 0 45 miss% 0.1134503660633034
plot_id,batch_id 0 46 miss% 0.03514864387817614
plot_id,batch_id 0 47 miss% 0.031889407246437414
plot_id,batch_id 0 48 miss% 0.040185891389663754
plot_id,batch_id 0 49 miss% 0.04862771556469083
plot_id,batch_id 0 50 miss% 0.1207921701216909
plot_id,batch_id 0 51 miss% 0.024892473415069977
plot_id,batch_id 0 52 miss% 0.029877682813460794
plot_id,batch_id 0 53 miss% 0.039165295621628196
plot_id,batch_id 0 54 miss% 0.04227502159187696
plot_id,batch_id 0 55 miss% 0.044294344856299364
plot_id,batch_id 0 56 miss% 0.07127630703484832
plot_id,batch_id 0 57 miss% 0.030129603963512473
plot_id,batch_id 0 58 miss% 0.044823441568566866
plot_id,batch_id 0 59 miss% 0.043429428111345945
plot_id,batch_id 0 60 miss% 0.03838475623324838
plot_id,batch_id 0 61 miss% 0.0248499216061959
plot_id,batch_id 0 62 miss% 0.05496791189301905
plot_id,batch_id 0 63 miss% 0.04963823108709853
plot_id,batch_id 0 64 miss% 0.06410489323343141
plot_id,batch_id 0 65 miss% 0.060282726035566864
plot_id,batch_id 0 66 miss% 0.05222635720988816
 65 miss% 0.09262452178542929
plot_id,batch_id 0 66 miss% 0.029661016603934852
plot_id,batch_id 0 67 miss% 0.05264320002601601
plot_id,batch_id 0 68 miss% 0.050513830983887446
plot_id,batch_id 0 69 miss% 0.10289630044703955
plot_id,batch_id 0 70 miss% 0.07696172005217088
plot_id,batch_id 0 71 miss% 0.03943320769091171
plot_id,batch_id 0 72 miss% 0.15810140148464324
plot_id,batch_id 0 73 miss% 0.07318727744019587
plot_id,batch_id 0 74 miss% 0.14380106430909853
plot_id,batch_id 0 75 miss% 0.041448341701485106
plot_id,batch_id 0 76 miss% 0.06617429741880788
plot_id,batch_id 0 77 miss% 0.04277631251534677
plot_id,batch_id 0 78 miss% 0.03753461147755338
plot_id,batch_id 0 79 miss% 0.09332650223397973
plot_id,batch_id 0 80 miss% 0.03177666413578375
plot_id,batch_id 0 81 miss% 0.06984550092926907
plot_id,batch_id 0 82 miss% 0.07586690326022598
plot_id,batch_id 0 83 miss% 0.064911200698924
plot_id,batch_id 0 84 miss% 0.07043677675955655
plot_id,batch_id 0 85 miss% 0.054552591917183896
plot_id,batch_id 0 86 miss% 0.06835692759248668
plot_id,batch_id 0 87 miss% 0.05620868307583053
plot_id,batch_id 0 88 miss% 0.07759624953793366
plot_id,batch_id 0 89 miss% 0.11604054864742665
plot_id,batch_id 0 90 miss% 0.020546530599956044
plot_id,batch_id 0 91 miss% 0.05985005928627799
plot_id,batch_id 0 92 miss% 0.045104631505583206
plot_id,batch_id 0 93 miss% 0.06853490374344869
plot_id,batch_id 0 94 miss% 0.05875395481470377
plot_id,batch_id 0 95 miss% 0.042701822813977344
plot_id,batch_id 0 96 miss% 0.06053370738352315
plot_id,batch_id 0 97 miss% 0.03861993493865466
plot_id,batch_id 0 98 miss% 0.0633024162831138
plot_id,batch_id 0 99 miss% 0.08428299866168514
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10079869 0.02962384 0.0913302  0.05663784 0.10393759 0.04331903
 0.06223242 0.07146294 0.05752764 0.02475447 0.03902887 0.07000376
 0.09308632 0.05452158 0.10839098 0.0324604  0.05212777 0.0473989
 0.06869575 0.08372459 0.08106477 0.08743085 0.06819354 0.05072713
 0.07515073 0.07658658 0.05384505 0.05577976 0.03544883 0.03738853
 0.04190578 0.11759255 0.13785478 0.06612318 0.1039552  0.05430171
 0.10776515 0.0977244  0.07131514 0.06630259 0.12879969 0.08803806
 0.05602101 0.08308976 0.03653541 0.04570996 0.05150085 0.07617628
 0.03080973 0.03778451 0.12729447 0.05755591 0.0364098  0.05594488
 0.06266624 0.07865764 0.11389563 0.05143278 0.03888243 0.07593594
 0.02513752 0.04368448 0.05757928 0.04540319 0.04513437 0.09262452
 0.02966102 0.0526432  0.05051383 0.1028963  0.07696172 0.03943321
 0.1581014  0.07318728 0.14380106 0.04144834 0.0661743  0.04277631
 0.03753461 0.0933265  0.03177666 0.0698455  0.0758669  0.0649112
 0.07043678 0.05455259 0.06835693 0.05620868 0.07759625 0.11604055
 0.02054653 0.05985006 0.04510463 0.0685349  0.05875395 0.04270182
 0.06053371 0.03861993 0.06330242 0.084283  ]
for model  207 the mean error 0.0665650622228517
all id 207 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 31 out win 4 err 0.0665650622228517 time 15666.490195989609
Launcher: Job 208 completed in 15929 seconds.
Launcher: Task 47 done. Exiting.
miss% 0.09988457576641072
plot_id,batch_id 0 66 miss% 0.039689208444194594
plot_id,batch_id 0 67 miss% 0.028794493954876852
plot_id,batch_id 0 68 miss% 0.04724971628399119
plot_id,batch_id 0 69 miss% 0.12027554185710382
plot_id,batch_id 0 70 miss% 0.04634502724914979
plot_id,batch_id 0 71 miss% 0.07789136279882203
plot_id,batch_id 0 72 miss% 0.08484185872434423
plot_id,batch_id 0 73 miss% 0.04569797568289231
plot_id,batch_id 0 74 miss% 0.19855482413039827
plot_id,batch_id 0 75 miss% 0.022871458280883064
plot_id,batch_id 0 76 miss% 0.035638936236873926
plot_id,batch_id 0 77 miss% 0.045381423619969344
plot_id,batch_id 0 78 miss% 0.04334446077977033
plot_id,batch_id 0 79 miss% 0.09620591274791523
plot_id,batch_id 0 80 miss% 0.049486615503929916
plot_id,batch_id 0 81 miss% 0.07690218803365764
plot_id,batch_id 0 82 miss% 0.06616728263626494
plot_id,batch_id 0 83 miss% 0.09820893975952276
plot_id,batch_id 0 84 miss% 0.06547546541527545
plot_id,batch_id 0 85 miss% 0.036737973803852936
plot_id,batch_id 0 86 miss% 0.04423118956430584
plot_id,batch_id 0 87 miss% 0.0741932535145153
plot_id,batch_id 0 88 miss% 0.10319295280375579
plot_id,batch_id 0 89 miss% 0.050243238576631405
plot_id,batch_id 0 90 miss% 0.034273951937471424
plot_id,batch_id 0 91 miss% 0.06612587838757351
plot_id,batch_id 0 92 miss% 0.030417956192544296
plot_id,batch_id 0 93 miss% 0.057422396201521064
plot_id,batch_id 0 94 miss% 0.1444441857233499
plot_id,batch_id 0 95 miss% 0.06546059744274728
plot_id,batch_id 0 96 miss% 0.05331774228735659
plot_id,batch_id 0 97 miss% 0.031021626144015765
plot_id,batch_id 0 98 miss% 0.03163825295674462
plot_id,batch_id 0 99 miss% 0.048649475148485506
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07892002 0.07411892 0.0996898  0.08456459 0.04862189 0.05213525
 0.06498545 0.10255604 0.08373499 0.0500147  0.04285434 0.05517282
 0.06213755 0.05669289 0.06853469 0.06363448 0.06665347 0.03362159
 0.08414889 0.07887418 0.05730487 0.03735181 0.05848461 0.05275858
 0.02454995 0.11250855 0.04074354 0.05070928 0.03658346 0.04170765
 0.06688848 0.11179438 0.12536738 0.07197256 0.07280373 0.02580803
 0.10667966 0.10514443 0.06471756 0.04124133 0.06577668 0.0442353
 0.02409974 0.05427581 0.07054489 0.0414637  0.03263333 0.03751541
 0.04560993 0.05938393 0.24876006 0.02655516 0.06114777 0.03093739
 0.08637764 0.11273286 0.08603426 0.05125169 0.06319539 0.04691025
 0.06099704 0.06301677 0.02366574 0.04169014 0.07837306 0.09988458
 0.03968921 0.02879449 0.04724972 0.12027554 0.04634503 0.07789136
 0.08484186 0.04569798 0.19855482 0.02287146 0.03563894 0.04538142
 0.04334446 0.09620591 0.04948662 0.07690219 0.06616728 0.09820894
 0.06547547 0.03673797 0.04423119 0.07419325 0.10319295 0.05024324
 0.03427395 0.06612588 0.03041796 0.0574224  0.14444419 0.0654606
 0.05331774 0.03102163 0.03163825 0.04864948]
for model  180 the mean error 0.0647424826962502
all id 180 hidden_dim 16 learning_rate 0.0025 num_layers 5 frames 31 out win 4 err 0.0647424826962502 time 15703.095125436783
Launcher: Job 181 completed in 15964 seconds.
Launcher: Task 93 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  154129
Epoch:0, Train loss:0.552158, valid loss:0.564355
Epoch:1, Train loss:0.080271, valid loss:0.003111
Epoch:2, Train loss:0.005877, valid loss:0.002694
Epoch:3, Train loss:0.004468, valid loss:0.001810
Epoch:4, Train loss:0.003686, valid loss:0.001575
Epoch:5, Train loss:0.003259, valid loss:0.001534
Epoch:6, Train loss:0.002930, valid loss:0.001564
Epoch:7, Train loss:0.002667, valid loss:0.001328
Epoch:8, Train loss:0.002484, valid loss:0.001184
Epoch:9, Train loss:0.002372, valid loss:0.001218
Epoch:10, Train loss:0.002116, valid loss:0.001114
Epoch:11, Train loss:0.001709, valid loss:0.001004
Epoch:12, Train loss:0.001596, valid loss:0.000928
Epoch:13, Train loss:0.001561, valid loss:0.000903
Epoch:14, Train loss:0.001495, valid loss:0.000985
Epoch:15, Train loss:0.001449, valid loss:0.000906
Epoch:16, Train loss:0.001396, valid loss:0.000825
Epoch:17, Train loss:0.001335, valid loss:0.000857
Epoch:18, Train loss:0.001334, valid loss:0.000814
Epoch:19, Train loss:0.001263, valid loss:0.000894
Epoch:20, Train loss:0.001220, valid loss:0.000841
Epoch:21, Train loss:0.000976, valid loss:0.000682
Epoch:22, Train loss:0.000957, valid loss:0.000721
Epoch:23, Train loss:0.000936, valid loss:0.000789
Epoch:24, Train loss:0.000926, valid loss:0.000693
Epoch:25, Train loss:0.000897, valid loss:0.000755
Epoch:26, Train loss:0.000886, valid loss:0.000730
Epoch:27, Train loss:0.000879, valid loss:0.000699
Epoch:28, Train loss:0.000850, valid loss:0.000694
Epoch:29, Train loss:0.000845, valid loss:0.000663
Epoch:30, Train loss:0.000836, valid loss:0.000693
Epoch:31, Train loss:0.000702, valid loss:0.000644
Epoch:32, Train loss:0.000687, valid loss:0.000644
Epoch:33, Train loss:0.000677, valid loss:0.000651
Epoch:34, Train loss:0.000672, valid loss:0.000625
Epoch:35, Train loss:0.000672, valid loss:0.000722
Epoch:36, Train loss:0.000660, valid loss:0.000627
Epoch:37, Train loss:0.000653, valid loss:0.000626
Epoch:38, Train loss:0.000646, valid loss:0.000624
Epoch:39, Train loss:0.000645, valid loss:0.000644
Epoch:40, Train loss:0.000632, valid loss:0.000603
Epoch:41, Train loss:0.000568, valid loss:0.000572
Epoch:42, Train loss:0.000557, valid loss:0.000593
Epoch:43, Train loss:0.000559, valid loss:0.000581
Epoch:44, Train loss:0.000563, valid loss:0.000593
Epoch:45, Train loss:0.000552, valid loss:0.000589
Epoch:46, Train loss:0.000548, valid loss:0.000594
Epoch:47, Train loss:0.000545, valid loss:0.000607
Epoch:48, Train loss:0.000543, valid loss:0.000600
Epoch:49, Train loss:0.000539, valid loss:0.000591
Epoch:50, Train loss:0.000541, valid loss:0.000584
Epoch:51, Train loss:0.000509, valid loss:0.000577
Epoch:52, Train loss:0.000502, valid loss:0.000575
Epoch:53, Train loss:0.000500, valid loss:0.000577
Epoch:54, Train loss:0.000498, valid loss:0.000576
Epoch:55, Train loss:0.000497, valid loss:0.000585
Epoch:56, Train loss:0.000497, valid loss:0.000576
Epoch:57, Train loss:0.000496, valid loss:0.000578
Epoch:58, Train loss:0.000496, valid loss:0.000575
Epoch:59, Train loss:0.000495, valid loss:0.000582
Epoch:60, Train loss:0.000495, valid loss:0.000577
training time 15753.252504348755
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.08514892250240105
plot_id,batch_id 0 1 miss% 0.04017245090707929
plot_id,batch_id 0 2 miss% 0.09767983646746087
plot_id,batch_id 0 3 miss% 0.09617168332958739
plot_id,batch_id 0 4 miss% 0.06705600776399429
plot_id,batch_id 0 5 miss% 0.07092946148134725
plot_id,batch_id 0 6 miss% 0.05992785219337877
plot_id,batch_id 0 7 miss% 0.09594774187423813
plot_id,batch_id 0 8 miss% 0.06089803182230349
plot_id,batch_id 0 9 miss% 0.037518062430096415
plot_id,batch_id 0 10 miss% 0.025534048937489038
plot_id,batch_id 0 11 miss% 0.059865847831672835
plot_id,batch_id 0 12 miss% 0.07524026436373232
plot_id,batch_id 0 13 miss% 0.056609011247189915
plot_id,batch_id 0 14 miss% 0.10870493130642805
plot_id,batch_id 0 15 miss% 0.0366927465465258
plot_id,batch_id 0 16 miss% 0.09886576680272405
plot_id,batch_id 0 17 miss% 0.06540898644787192
plot_id,batch_id 0 18 miss% 0.07625247512144091
plot_id,batch_id 0 19 miss% 0.08000993297048141
plot_id,batch_id 0 20 miss% 0.05870864009128708
plot_id,batch_id 0 21 miss% 0.05871292551688782
plot_id,batch_id 0 22 miss% 0.06625669696077943
plot_id,batch_id 0 23 miss% 0.034017951395128775
plot_id,batch_id 0 24 miss% 0.07303312196893591
plot_id,batch_id 0 25 miss% 0.03216971621433903
plot_id,batch_id 0 26 miss% 0.04854431311111446
plot_id,batch_id 0 27 miss% 0.05504607867477269
plot_id,batch_id 0 28 miss% 0.03617461751064349
plot_id,batch_id 0 29 miss% 0.025267513396148328
plot_id,batch_id 0 30 miss% 0.044322596445082574
plot_id,batch_id 0 31 miss% 0.09909033627147577
plot_id,batch_id 0 32 miss% 0.11513408287792794
plot_id,batch_id 0 33 miss% 0.07353693133181086
plot_id,batch_id 0 34 miss% 0.05921430202509696
plot_id,batch_id 0 35 miss% 0.044054504485370106
plot_id,batch_id 0 36 miss% 0.08840941465249097
plot_id,batch_id 0 37 miss% 0.11157816973355919
plot_id,batch_id 0 38 miss% 0.07108511606327839
plot_id,batch_id 0 39 miss% 0.05545236193273561
plot_id,batch_id 0 40 miss% 0.0810968515317825
plot_id,batch_id 0 41 miss% 0.03934000732839049
plot_id,batch_id 0 42 miss% 0.029307047017016037
plot_id,batch_id 0 43 miss% 0.08073197622138015
plot_id,batch_id 0 44 miss% 0.028742423601108243
plot_id,batch_id 0 45 miss% 0.09647245248968009
plot_id,batch_id 0 46 miss% 0.05558338031565451
plot_id,batch_id 0 47 miss% 0.0360287270269145
plot_id,batch_id 0 48 miss% 0.04541550510226995
plot_id,batch_id 0 49 miss% 0.024975515045643347
plot_id,batch_id 0 50 miss% 0.1590014922978222
plot_id,batch_id 0 51 miss% 0.039492497570002116
plot_id,batch_id 0 52 miss% 0.05748627307974613
plot_id,batch_id 0 53 miss% 0.0228165801502982
plot_id,batch_id 0 54 miss% 0.056731949521113365
plot_id,batch_id 0 55 miss% 0.13522409728021184
plot_id,batch_id 0 56 miss% 0.05486456589212662
plot_id,batch_id 0 57 miss% 0.0357401699727947
plot_id,batch_id 0 58 miss% 0.03301480236474138
plot_id,batch_id 0 59 miss% 0.04978635617093367
plot_id,batch_id 0 60 miss% 0.049996363799240225
plot_id,batch_id 0 61 miss% 0.07551835829295588
plot_id,batch_id 0 62 miss% 0.06543884813313003
plot_id,batch_id 0 63 miss% 0.051561781201197
plot_id,batch_id 0 64 miss% 0.03394461209331186
plot_id,batch_id 0 65 miss% 0.0604854867236766
plot_id,batch_id 0 66 miss% 0.029110768774144476
plot_id,batch_id 0 67 miss% 0.03368681140843523
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  69649
Epoch:0, Train loss:0.496002, valid loss:0.480569
Epoch:1, Train loss:0.018450, valid loss:0.002139
Epoch:2, Train loss:0.003929, valid loss:0.001340
Epoch:3, Train loss:0.002616, valid loss:0.001224
Epoch:4, Train loss:0.002338, valid loss:0.001208
Epoch:5, Train loss:0.002148, valid loss:0.001036
Epoch:6, Train loss:0.001954, valid loss:0.000995
Epoch:7, Train loss:0.001835, valid loss:0.000868
Epoch:8, Train loss:0.001735, valid loss:0.000862
Epoch:9, Train loss:0.001647, valid loss:0.000868
Epoch:10, Train loss:0.001577, valid loss:0.000932
Epoch:11, Train loss:0.001131, valid loss:0.000669
Epoch:12, Train loss:0.001084, valid loss:0.000687
Epoch:13, Train loss:0.001081, valid loss:0.000597
Epoch:14, Train loss:0.001032, valid loss:0.000668
Epoch:15, Train loss:0.001024, valid loss:0.000601
Epoch:16, Train loss:0.000980, valid loss:0.000575
Epoch:17, Train loss:0.000984, valid loss:0.000544
Epoch:18, Train loss:0.000964, valid loss:0.000565
Epoch:19, Train loss:0.000944, valid loss:0.000566
Epoch:20, Train loss:0.000920, valid loss:0.000557
Epoch:21, Train loss:0.000730, valid loss:0.000506
Epoch:22, Train loss:0.000710, valid loss:0.000500
Epoch:23, Train loss:0.000688, valid loss:0.000464
Epoch:24, Train loss:0.000701, valid loss:0.000512
Epoch:25, Train loss:0.000672, valid loss:0.000496
Epoch:26, Train loss:0.000673, valid loss:0.000546
Epoch:27, Train loss:0.000668, valid loss:0.000468
Epoch:28, Train loss:0.000661, valid loss:0.000500
Epoch:29, Train loss:0.000658, valid loss:0.000519
Epoch:30, Train loss:0.000641, valid loss:0.000495
Epoch:31, Train loss:0.000542, valid loss:0.000446
Epoch:32, Train loss:0.000533, valid loss:0.000431
Epoch:33, Train loss:0.000529, valid loss:0.000439
Epoch:34, Train loss:0.000525, valid loss:0.000415
Epoch:35, Train loss:0.000527, valid loss:0.000441
Epoch:36, Train loss:0.000514, valid loss:0.000416
Epoch:37, Train loss:0.000513, valid loss:0.000482
Epoch:38, Train loss:0.000507, valid loss:0.000427
Epoch:39, Train loss:0.000509, valid loss:0.000441
Epoch:40, Train loss:0.000510, valid loss:0.000433
Epoch:41, Train loss:0.000452, valid loss:0.000410
Epoch:42, Train loss:0.000449, valid loss:0.000421
Epoch:43, Train loss:0.000448, valid loss:0.000413
Epoch:44, Train loss:0.000448, valid loss:0.000400
Epoch:45, Train loss:0.000443, valid loss:0.000402
Epoch:46, Train loss:0.000442, valid loss:0.000421
Epoch:47, Train loss:0.000440, valid loss:0.000395
Epoch:48, Train loss:0.000438, valid loss:0.000408
Epoch:49, Train loss:0.000435, valid loss:0.000401
Epoch:50, Train loss:0.000435, valid loss:0.000399
Epoch:51, Train loss:0.000413, valid loss:0.000395
Epoch:52, Train loss:0.000410, valid loss:0.000394
Epoch:53, Train loss:0.000409, valid loss:0.000394
Epoch:54, Train loss:0.000408, valid loss:0.000397
Epoch:55, Train loss:0.000407, valid loss:0.000394
Epoch:56, Train loss:0.000407, valid loss:0.000396
Epoch:57, Train loss:0.000406, valid loss:0.000394
Epoch:58, Train loss:0.000406, valid loss:0.000394
Epoch:59, Train loss:0.000405, valid loss:0.000396
Epoch:60, Train loss:0.000405, valid loss:0.000396
training time 15760.740825414658
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.07785799702904245
plot_id,batch_id 0 1 miss% 0.0753064315584959
plot_id,batch_id 0 2 miss% 0.06306379298527623
plot_id,batch_id 0 3 miss% 0.05663308570453289
plot_id,batch_id 0 4 miss% 0.04701907390815194
plot_id,batch_id 0 5 miss% 0.04792270561885513
plot_id,batch_id 0 6 miss% 0.047278862699447095
plot_id,batch_id 0 7 miss% 0.13676687640385746
plot_id,batch_id 0 8 miss% 0.11116273961019518
plot_id,batch_id 0 9 miss% 0.07984227825429796
plot_id,batch_id 0 10 miss% 0.021957146239559036
plot_id,batch_id 0 11 miss% 0.049407391097608874
plot_id,batch_id 0 12 miss% 0.10270375799986171
plot_id,batch_id 0 13 miss% 0.06559739193417326
plot_id,batch_id 0 14 miss% 0.12405555003423788
plot_id,batch_id 0 15 miss% 0.03993366743692085
plot_id,batch_id 0 16 miss% 0.07372302295646607
plot_id,batch_id 0 17 miss% 0.04949531735708943
plot_id,batch_id 0 18 miss% 0.10776088379027907
plot_id,batch_id 0 19 miss% 0.07252262639248011
plot_id,batch_id 0 20 miss% 0.05872318922863333
plot_id,batch_id 0 21 miss% 0.08388615297070365
plot_id,batch_id 0 22 miss% 0.11623874524129095
plot_id,batch_id 0 23 miss% 0.03651517874187487
plot_id,batch_id 0 24 miss% 0.12222170797951755
plot_id,batch_id 0 25 miss% 0.18385896840135102
plot_id,batch_id 0 26 miss% 0.0725817352890657
plot_id,batch_id 0 27 miss% 0.04457526996986709
plot_id,batch_id 0 28 miss% 0.049912953752378926
plot_id,batch_id 0 29 miss% 0.02619221352851874
plot_id,batch_id 0 30 miss% 0.07241729967386078
plot_id,batch_id 0 31 miss% 0.13240477170340934
plot_id,batch_id 0 32 miss% 0.06524441852993013
plot_id,batch_id 0 33 miss% 0.07687227553768138
plot_id,batch_id 0 34 miss% 0.023201379328750757
plot_id,batch_id 0 35 miss% 0.09696176899091714
plot_id,batch_id 0 36 miss% 0.1429659567792626
plot_id,batch_id 0 37 miss% 0.10030228107813735
plot_id,batch_id 0 38 miss% 0.056501525991870506
plot_id,batch_id 0 39 miss% 0.06091591551468422
plot_id,batch_id 0 40 miss% 0.10297784247192368
plot_id,batch_id 0 41 miss% 0.05249638871765873
plot_id,batch_id 0 42 miss% 0.04982185582541366
plot_id,batch_id 0 43 miss% 0.09599108696860383
plot_id,batch_id 0 44 miss% 0.029425091915276144
plot_id,batch_id 0 45 miss% 0.0666736923216469
plot_id,batch_id 0 46 miss% 0.05734772056177199
plot_id,batch_id 0 47 miss% 0.059667315005814694
plot_id,batch_id 0 48 miss% 0.08000624075936195
plot_id,batch_id 0 49 miss% 0.052114391325726386
plot_id,batch_id 0 50 miss% 0.1365478254864458
plot_id,batch_id 0 51 miss% 0.03450604638930305
plot_id,batch_id 0 52 miss% 0.03684912216234329
plot_id,batch_id 0 53 miss% 0.0491321756019511
plot_id,batch_id 0 54 miss% 0.0711332562532244
plot_id,batch_id 0 55 miss% 0.07274994757756446
plot_id,batch_id 0 56 miss% 0.09498861781294016
plot_id,batch_id 0 57 miss% 0.0708913164049145
plot_id,batch_id 0 58 miss% 0.0649937008323136
plot_id,batch_id 0 59 miss% 0.04496335578740748
plot_id,batch_id 0 60 miss% 0.04261394926928765
plot_id,batch_id 0 61 miss% 0.03800262531327256
plot_id,batch_id 0 62 miss% 0.06816026545012156
plot_id,batch_id 0 63 miss% 0.06510170477430662
plot_id,batch_id 0 64 miss% 0.06454764944584883
plot_id,batch_id 0 65 miss% plot_id,batch_id 0 67 miss% 0.03641834013826727
plot_id,batch_id 0 68 miss% 0.03227756922905151
plot_id,batch_id 0 69 miss% 0.07564924785246491
plot_id,batch_id 0 70 miss% 0.06675871581495119
plot_id,batch_id 0 71 miss% 0.03153811330410407
plot_id,batch_id 0 72 miss% 0.10574006627758441
plot_id,batch_id 0 73 miss% 0.03710545992563825
plot_id,batch_id 0 74 miss% 0.06216051410815853
plot_id,batch_id 0 75 miss% 0.04525436747614878
plot_id,batch_id 0 76 miss% 0.07821894143243673
plot_id,batch_id 0 77 miss% 0.024050384110259344
plot_id,batch_id 0 78 miss% 0.03882932159846387
plot_id,batch_id 0 79 miss% 0.07045058183587453
plot_id,batch_id 0 80 miss% 0.07648954635170888
plot_id,batch_id 0 81 miss% 0.08375362869727604
plot_id,batch_id 0 82 miss% 0.053911672294157977
plot_id,batch_id 0 83 miss% 0.06845338640628457
plot_id,batch_id 0 84 miss% 0.04014156389615163
plot_id,batch_id 0 85 miss% 0.04003861798425502
plot_id,batch_id 0 86 miss% 0.03592903921573179
plot_id,batch_id 0 87 miss% 0.05495741640999107
plot_id,batch_id 0 88 miss% 0.07671226542461602
plot_id,batch_id 0 89 miss% 0.056264541993224754
plot_id,batch_id 0 90 miss% 0.04948144784784226
plot_id,batch_id 0 91 miss% 0.06462502158070299
plot_id,batch_id 0 92 miss% 0.03384971767658531
plot_id,batch_id 0 93 miss% 0.05116538939697502
plot_id,batch_id 0 94 miss% 0.08788312842089069
plot_id,batch_id 0 95 miss% 0.04479117729268634
plot_id,batch_id 0 96 miss% 0.05197454513109476
plot_id,batch_id 0 97 miss% 0.03103154716189658
plot_id,batch_id 0 98 miss% 0.041229261359402775
plot_id,batch_id 0 99 miss% 0.0885949270331437
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04227573 0.02622304 0.10204153 0.03486259 0.05531705 0.05952165
 0.05474516 0.09730416 0.06621124 0.03539064 0.03259747 0.08300001
 0.07718234 0.0653013  0.08206749 0.04432497 0.15981508 0.0526004
 0.06516853 0.07899547 0.07240142 0.02561441 0.05085179 0.03875288
 0.05625506 0.04331231 0.0328165  0.03268405 0.04738622 0.03210978
 0.03840281 0.08705907 0.11602957 0.04531292 0.03812165 0.02719376
 0.0932109  0.08151738 0.04980629 0.03014325 0.07332177 0.06035281
 0.02569279 0.06728171 0.03511081 0.11345037 0.03514864 0.03188941
 0.04018589 0.04862772 0.12079217 0.02489247 0.02987768 0.0391653
 0.04227502 0.04429434 0.07127631 0.0301296  0.04482344 0.04342943
 0.03838476 0.02484992 0.05496791 0.04963823 0.06410489 0.06028273
 0.05222636 0.03641834 0.03227757 0.07564925 0.06675872 0.03153811
 0.10574007 0.03710546 0.06216051 0.04525437 0.07821894 0.02405038
 0.03882932 0.07045058 0.07648955 0.08375363 0.05391167 0.06845339
 0.04014156 0.04003862 0.03592904 0.05495742 0.07671227 0.05626454
 0.04948145 0.06462502 0.03384972 0.05116539 0.08788313 0.04479118
 0.05197455 0.03103155 0.04122926 0.08859493]
for model  96 the mean error 0.05556129807055519
all id 96 hidden_dim 32 learning_rate 0.0025 num_layers 4 frames 25 out win 4 err 0.05556129807055519 time 15713.428469657898
Launcher: Job 97 completed in 15976 seconds.
Launcher: Task 247 done. Exiting.
plot_id,batch_id 0 68 miss% 0.051320606066784374
plot_id,batch_id 0 69 miss% 0.04808623166078801
plot_id,batch_id 0 70 miss% 0.06909264221686533
plot_id,batch_id 0 71 miss% 0.03213413958394337
plot_id,batch_id 0 72 miss% 0.06671507227064621
plot_id,batch_id 0 73 miss% 0.03732034659751466
plot_id,batch_id 0 74 miss% 0.17679418472603617
plot_id,batch_id 0 75 miss% 0.06865572356449356
plot_id,batch_id 0 76 miss% 0.059077341710325026
plot_id,batch_id 0 77 miss% 0.04929433822486905
plot_id,batch_id 0 78 miss% 0.04166392151227467
plot_id,batch_id 0 79 miss% 0.07794560530140827
plot_id,batch_id 0 80 miss% 0.0457016479923507
plot_id,batch_id 0 81 miss% 0.05537333267963399
plot_id,batch_id 0 82 miss% 0.04154506837745545
plot_id,batch_id 0 83 miss% 0.0767517873737606
plot_id,batch_id 0 84 miss% 0.07623774334009102
plot_id,batch_id 0 85 miss% 0.03577047474105372
plot_id,batch_id 0 86 miss% 0.04788883845003935
plot_id,batch_id 0 87 miss% 0.09914592072647017
plot_id,batch_id 0 88 miss% 0.08646813795802895
plot_id,batch_id 0 89 miss% 0.06637393871103096
plot_id,batch_id 0 90 miss% 0.024580034554351635
plot_id,batch_id 0 91 miss% 0.11296789120618274
plot_id,batch_id 0 92 miss% 0.03396158135577625
plot_id,batch_id 0 93 miss% 0.059252580986051
plot_id,batch_id 0 94 miss% 0.11757747512515862
plot_id,batch_id 0 95 miss% 0.042709553625561426
plot_id,batch_id 0 96 miss% 0.0561442499713805
plot_id,batch_id 0 97 miss% 0.058899446575668174
plot_id,batch_id 0 98 miss% 0.034202201078728234
plot_id,batch_id 0 99 miss% 0.06013136772064403
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08514892 0.04017245 0.09767984 0.09617168 0.06705601 0.07092946
 0.05992785 0.09594774 0.06089803 0.03751806 0.02553405 0.05986585
 0.07524026 0.05660901 0.10870493 0.03669275 0.09886577 0.06540899
 0.07625248 0.08000993 0.05870864 0.05871293 0.0662567  0.03401795
 0.07303312 0.03216972 0.04854431 0.05504608 0.03617462 0.02526751
 0.0443226  0.09909034 0.11513408 0.07353693 0.0592143  0.0440545
 0.08840941 0.11157817 0.07108512 0.05545236 0.08109685 0.03934001
 0.02930705 0.08073198 0.02874242 0.09647245 0.05558338 0.03602873
 0.04541551 0.02497552 0.15900149 0.0394925  0.05748627 0.02281658
 0.05673195 0.1352241  0.05486457 0.03574017 0.0330148  0.04978636
 0.04999636 0.07551836 0.06543885 0.05156178 0.03394461 0.06048549
 0.02911077 0.03368681 0.05132061 0.04808623 0.06909264 0.03213414
 0.06671507 0.03732035 0.17679418 0.06865572 0.05907734 0.04929434
 0.04166392 0.07794561 0.04570165 0.05537333 0.04154507 0.07675179
 0.07623774 0.03577047 0.04788884 0.09914592 0.08646814 0.06637394
 0.02458003 0.11296789 0.03396158 0.05925258 0.11757748 0.04270955
 0.05614425 0.05889945 0.0342022  0.06013137]
for model  88 the mean error 0.06215822579397396
all id 88 hidden_dim 32 learning_rate 0.0025 num_layers 3 frames 25 out win 5 err 0.06215822579397396 time 15753.252504348755
Launcher: Job 89 completed in 16018 seconds.
Launcher: Task 118 done. Exiting.
0.10567587824731761
plot_id,batch_id 0 66 miss% 0.04457221381128604
plot_id,batch_id 0 67 miss% 0.0649919637001511
plot_id,batch_id 0 68 miss% 0.07096494830919242
plot_id,batch_id 0 69 miss% 0.09692592588875465
plot_id,batch_id 0 70 miss% 0.043928925696591695
plot_id,batch_id 0 71 miss% 0.10358218053157192
plot_id,batch_id 0 72 miss% 0.06158701380599388
plot_id,batch_id 0 73 miss% 0.05649954587876976
plot_id,batch_id 0 74 miss% 0.09007698593115615
plot_id,batch_id 0 75 miss% 0.06307566644114962
plot_id,batch_id 0 76 miss% 0.0652148666241608
plot_id,batch_id 0 77 miss% 0.04287469754108764
plot_id,batch_id 0 78 miss% 0.05543014700154459
plot_id,batch_id 0 79 miss% 0.15530732859671675
plot_id,batch_id 0 80 miss% 0.03847233118957742
plot_id,batch_id 0 81 miss% 0.11898783939735308
plot_id,batch_id 0 82 miss% 0.10332722565381433
plot_id,batch_id 0 83 miss% 0.1175458549527139
plot_id,batch_id 0 84 miss% 0.07948968212715135
plot_id,batch_id 0 85 miss% 0.054962829425853385
plot_id,batch_id 0 86 miss% 0.05773676929206605
plot_id,batch_id 0 87 miss% 0.09057726692335827
plot_id,batch_id 0 88 miss% 0.0857203469457983
plot_id,batch_id 0 89 miss% 0.06551182045855027
plot_id,batch_id 0 90 miss% 0.02978813505451565
plot_id,batch_id 0 91 miss% 0.06293181006024164
plot_id,batch_id 0 92 miss% 0.03735834098363618
plot_id,batch_id 0 93 miss% 0.055428783367047305
plot_id,batch_id 0 94 miss% 0.0870969990874268
plot_id,batch_id 0 95 miss% 0.052317296846954395
plot_id,batch_id 0 96 miss% 0.0551063092406455
plot_id,batch_id 0 97 miss% 0.03442020479248657
plot_id,batch_id 0 98 miss% 0.050949036258529375
plot_id,batch_id 0 99 miss% 0.0961628233340171
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.077858   0.07530643 0.06306379 0.05663309 0.04701907 0.04792271
 0.04727886 0.13676688 0.11116274 0.07984228 0.02195715 0.04940739
 0.10270376 0.06559739 0.12405555 0.03993367 0.07372302 0.04949532
 0.10776088 0.07252263 0.05872319 0.08388615 0.11623875 0.03651518
 0.12222171 0.18385897 0.07258174 0.04457527 0.04991295 0.02619221
 0.0724173  0.13240477 0.06524442 0.07687228 0.02320138 0.09696177
 0.14296596 0.10030228 0.05650153 0.06091592 0.10297784 0.05249639
 0.04982186 0.09599109 0.02942509 0.06667369 0.05734772 0.05966732
 0.08000624 0.05211439 0.13654783 0.03450605 0.03684912 0.04913218
 0.07113326 0.07274995 0.09498862 0.07089132 0.0649937  0.04496336
 0.04261395 0.03800263 0.06816027 0.0651017  0.06454765 0.10567588
 0.04457221 0.06499196 0.07096495 0.09692593 0.04392893 0.10358218
 0.06158701 0.05649955 0.09007699 0.06307567 0.06521487 0.0428747
 0.05543015 0.15530733 0.03847233 0.11898784 0.10332723 0.11754585
 0.07948968 0.05496283 0.05773677 0.09057727 0.08572035 0.06551182
 0.02978814 0.06293181 0.03735834 0.05542878 0.087097   0.0523173
 0.05510631 0.0344202  0.05094904 0.09616282]
for model  234 the mean error 0.0716880748507416
all id 234 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 31 out win 4 err 0.0716880748507416 time 15760.740825414658
Launcher: Job 235 completed in 16021 seconds.
Launcher: Task 48 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  209681
Epoch:0, Train loss:0.615114, valid loss:0.611850
Epoch:1, Train loss:0.054616, valid loss:0.006980
Epoch:2, Train loss:0.014912, valid loss:0.005120
Epoch:3, Train loss:0.013117, valid loss:0.003851
Epoch:4, Train loss:0.011590, valid loss:0.003317
Epoch:5, Train loss:0.009286, valid loss:0.003207
Epoch:6, Train loss:0.007558, valid loss:0.002605
Epoch:7, Train loss:0.007023, valid loss:0.002394
Epoch:8, Train loss:0.005667, valid loss:0.002298
Epoch:9, Train loss:0.004885, valid loss:0.002201
Epoch:10, Train loss:0.004605, valid loss:0.001778
Epoch:11, Train loss:0.003706, valid loss:0.001572
Epoch:12, Train loss:0.003523, valid loss:0.001502
Epoch:13, Train loss:0.003456, valid loss:0.001494
Epoch:14, Train loss:0.003289, valid loss:0.001413
Epoch:15, Train loss:0.002598, valid loss:0.001264
Epoch:16, Train loss:0.002013, valid loss:0.001421
Epoch:17, Train loss:0.001969, valid loss:0.001223
Epoch:18, Train loss:0.001862, valid loss:0.001166
Epoch:19, Train loss:0.001789, valid loss:0.001290
Epoch:20, Train loss:0.001729, valid loss:0.001159
Epoch:21, Train loss:0.001305, valid loss:0.001070
Epoch:22, Train loss:0.001223, valid loss:0.001051
Epoch:23, Train loss:0.001208, valid loss:0.001036
Epoch:24, Train loss:0.001182, valid loss:0.001037
Epoch:25, Train loss:0.001158, valid loss:0.000937
Epoch:26, Train loss:0.001107, valid loss:0.000949
Epoch:27, Train loss:0.001104, valid loss:0.000966
Epoch:28, Train loss:0.001086, valid loss:0.001046
Epoch:29, Train loss:0.001055, valid loss:0.000951
Epoch:30, Train loss:0.001018, valid loss:0.001030
Epoch:31, Train loss:0.000814, valid loss:0.000953
Epoch:32, Train loss:0.000788, valid loss:0.000941
Epoch:33, Train loss:0.000778, valid loss:0.000950
Epoch:34, Train loss:0.000777, valid loss:0.000920
Epoch:35, Train loss:0.000752, valid loss:0.000974
Epoch:36, Train loss:0.000753, valid loss:0.000910
Epoch:37, Train loss:0.000747, valid loss:0.000892
Epoch:38, Train loss:0.000722, valid loss:0.001035
Epoch:39, Train loss:0.000721, valid loss:0.000924
Epoch:40, Train loss:0.000691, valid loss:0.000883
Epoch:41, Train loss:0.000613, valid loss:0.000877
Epoch:42, Train loss:0.000598, valid loss:0.000888
Epoch:43, Train loss:0.000592, valid loss:0.000893
Epoch:44, Train loss:0.000581, valid loss:0.000901
Epoch:45, Train loss:0.000573, valid loss:0.000881
Epoch:46, Train loss:0.000572, valid loss:0.000912
Epoch:47, Train loss:0.000567, valid loss:0.000858
Epoch:48, Train loss:0.000575, valid loss:0.000935
Epoch:49, Train loss:0.000557, valid loss:0.000867
Epoch:50, Train loss:0.000557, valid loss:0.000877
Epoch:51, Train loss:0.000525, valid loss:0.000853
Epoch:52, Train loss:0.000517, valid loss:0.000850
Epoch:53, Train loss:0.000514, valid loss:0.000851
Epoch:54, Train loss:0.000512, valid loss:0.000847
Epoch:55, Train loss:0.000510, valid loss:0.000849
Epoch:56, Train loss:0.000508, valid loss:0.000849
Epoch:57, Train loss:0.000507, valid loss:0.000850
Epoch:58, Train loss:0.000506, valid loss:0.000849
Epoch:59, Train loss:0.000505, valid loss:0.000855
Epoch:60, Train loss:0.000504, valid loss:0.000850
training time 15849.486867904663
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.10286922633062474
plot_id,batch_id 0 1 miss% 0.06296288067512912
plot_id,batch_id 0 2 miss% 0.06426119023949999
plot_id,batch_id 0 3 miss% 0.03966090726340147
plot_id,batch_id 0 4 miss% 0.06615243194075864
plot_id,batch_id 0 5 miss% 0.043089378656962225
plot_id,batch_id 0 6 miss% 0.05536619462896791
plot_id,batch_id 0 7 miss% 0.06540676572149996
plot_id,batch_id 0 8 miss% 0.0665303263379989
plot_id,batch_id 0 9 miss% 0.047913752665502185
plot_id,batch_id 0 10 miss% 0.047751700797598676
plot_id,batch_id 0 11 miss% 0.05917658174428613
plot_id,batch_id 0 12 miss% 0.04329459266910539
plot_id,batch_id 0 13 miss% 0.030074405738005832
plot_id,batch_id 0 14 miss% 0.03727534477295153
plot_id,batch_id 0 15 miss% 0.05012302916764192
plot_id,batch_id 0 16 miss% 0.1061944003904575
plot_id,batch_id 0 17 miss% 0.03509277637789853
plot_id,batch_id 0 18 miss% 0.1431095776253509
plot_id,batch_id 0 19 miss% 0.05325838125405413
plot_id,batch_id 0 20 miss% 0.05593556041423902
plot_id,batch_id 0 21 miss% 0.028880044791825585
plot_id,batch_id 0 22 miss% 0.05416822494186449
plot_id,batch_id 0 23 miss% 0.05060517430992781
plot_id,batch_id 0 24 miss% 0.04577640489557286
plot_id,batch_id 0 25 miss% 0.10897211910270373
plot_id,batch_id 0 26 miss% 0.02881457512060871
plot_id,batch_id 0 27 miss% 0.04465524648601369
plot_id,batch_id 0 28 miss% 0.07261668575687884
plot_id,batch_id 0 29 miss% 0.03964664969613536
plot_id,batch_id 0 30 miss% 0.05518071245097769
plot_id,batch_id 0 31 miss% 0.08701780650937932
plot_id,batch_id 0 32 miss% 0.06398587652844895
plot_id,batch_id 0 33 miss% 0.05262604130985588
plot_id,batch_id 0 34 miss% 0.05082521114349458
plot_id,batch_id 0 35 miss% 0.02811449819588337
plot_id,batch_id 0 36 miss% 0.06839309056384864
plot_id,batch_id 0 37 miss% 0.047660294178527414
plot_id,batch_id 0 38 miss% 0.07067743218308523
plot_id,batch_id 0 39 miss% 0.04928841474262116
plot_id,batch_id 0 40 miss% 0.026175883741600268
plot_id,batch_id 0 41 miss% 0.03717255398098838
plot_id,batch_id 0 42 miss% 0.021418997032585145
plot_id,batch_id 0 43 miss% 0.05460052789639992
plot_id,batch_id 0 44 miss% 0.04327395537360883
plot_id,batch_id 0 45 miss% 0.04182341692943755
plot_id,batch_id 0 46 miss% 0.043491328670617446
plot_id,batch_id 0 47 miss% 0.030446422556146623
plot_id,batch_id 0 48 miss% 0.030411996572453342
plot_id,batch_id 0 49 miss% 0.03575537066512374
plot_id,batch_id 0 50 miss% 0.1164102589779093
plot_id,batch_id 0 51 miss% 0.041965961334183614
plot_id,batch_id 0 52 miss% 0.04571639909867278
plot_id,batch_id 0 53 miss% 0.013158973122317015
plot_id,batch_id 0 54 miss% 0.03522027946572773
plot_id,batch_id 0 55 miss% 0.0343350208814046
plot_id,batch_id 0 56 miss% 0.044135141594994605
plot_id,batch_id 0 57 miss% 0.04986291565001196
plot_id,batch_id 0 58 miss% 0.03821122536266133
plot_id,batch_id 0 59 miss% 0.02519113897817458
plot_id,batch_id 0 60 miss% 0.03511316672931596
plot_id,batch_id 0 61 miss% 0.0319824330875276
plot_id,batch_id 0 62 miss% 0.08931695204264004
plot_id,batch_id 0 63 miss% 0.03943173835803365
plot_id,batch_id 0 64 miss% 0.06877332119351161
plot_id,batch_id 0 65 miss% 0.043233992610190446
plot_id,batch_id 0 66 miss% 0.14247663932781562
plot_id,batch_id 0 67 miss% 0.06345923517264485
plot_id,batch_id 0 68 miss% 0.0576186643984355
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  69649
Epoch:0, Train loss:0.458467, valid loss:0.438974
Epoch:1, Train loss:0.021495, valid loss:0.001931
Epoch:2, Train loss:0.004446, valid loss:0.001530
Epoch:3, Train loss:0.002860, valid loss:0.001211
Epoch:4, Train loss:0.002547, valid loss:0.001078
Epoch:5, Train loss:0.002280, valid loss:0.001210
Epoch:6, Train loss:0.002115, valid loss:0.001049
Epoch:7, Train loss:0.001958, valid loss:0.001101
Epoch:8, Train loss:0.001868, valid loss:0.000878
Epoch:9, Train loss:0.001708, valid loss:0.000756
Epoch:10, Train loss:0.001670, valid loss:0.000843
Epoch:11, Train loss:0.001268, valid loss:0.000635
Epoch:12, Train loss:0.001199, valid loss:0.000698
Epoch:13, Train loss:0.001192, valid loss:0.000612
Epoch:14, Train loss:0.001161, valid loss:0.000622
Epoch:15, Train loss:0.001148, valid loss:0.000650
Epoch:16, Train loss:0.001128, valid loss:0.000617
Epoch:17, Train loss:0.001105, valid loss:0.000692
Epoch:18, Train loss:0.001064, valid loss:0.000666
Epoch:19, Train loss:0.001024, valid loss:0.000623
Epoch:20, Train loss:0.001018, valid loss:0.000622
Epoch:21, Train loss:0.000806, valid loss:0.000547
Epoch:22, Train loss:0.000793, valid loss:0.000540
Epoch:23, Train loss:0.000780, valid loss:0.000556
Epoch:24, Train loss:0.000763, valid loss:0.000575
Epoch:25, Train loss:0.000773, valid loss:0.000508
Epoch:26, Train loss:0.000735, valid loss:0.000535
Epoch:27, Train loss:0.000735, valid loss:0.000519
Epoch:28, Train loss:0.000725, valid loss:0.000523
Epoch:29, Train loss:0.000713, valid loss:0.000569
Epoch:30, Train loss:0.000708, valid loss:0.000523
Epoch:31, Train loss:0.000599, valid loss:0.000461
Epoch:32, Train loss:0.000597, valid loss:0.000464
Epoch:33, Train loss:0.000579, valid loss:0.000427
Epoch:34, Train loss:0.000570, valid loss:0.000468
Epoch:35, Train loss:0.000584, valid loss:0.000541
Epoch:36, Train loss:0.000573, valid loss:0.000464
Epoch:37, Train loss:0.000563, valid loss:0.000443
Epoch:38, Train loss:0.000560, valid loss:0.000458
Epoch:39, Train loss:0.000546, valid loss:0.000484
Epoch:40, Train loss:0.000552, valid loss:0.000478
Epoch:41, Train loss:0.000499, valid loss:0.000444
Epoch:42, Train loss:0.000492, valid loss:0.000435
Epoch:43, Train loss:0.000490, valid loss:0.000445
Epoch:44, Train loss:0.000485, valid loss:0.000448
Epoch:45, Train loss:0.000484, valid loss:0.000444
Epoch:46, Train loss:0.000478, valid loss:0.000456
Epoch:47, Train loss:0.000477, valid loss:0.000452
Epoch:48, Train loss:0.000477, valid loss:0.000440
Epoch:49, Train loss:0.000476, valid loss:0.000435
Epoch:50, Train loss:0.000474, valid loss:0.000432
Epoch:51, Train loss:0.000438, valid loss:0.000411
Epoch:52, Train loss:0.000437, valid loss:0.000411
Epoch:53, Train loss:0.000436, valid loss:0.000410
Epoch:54, Train loss:0.000435, valid loss:0.000411
Epoch:55, Train loss:0.000435, valid loss:0.000412
Epoch:56, Train loss:0.000435, valid loss:0.000411
Epoch:57, Train loss:0.000434, valid loss:0.000411
Epoch:58, Train loss:0.000434, valid loss:0.000412
Epoch:59, Train loss:0.000434, valid loss:0.000410
Epoch:60, Train loss:0.000433, valid loss:0.000413
training time 15869.205983400345
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.09251454312003239
plot_id,batch_id 0 1 miss% 0.05118182452189235
plot_id,batch_id 0 2 miss% 0.11807209682025074
plot_id,batch_id 0 3 miss% 0.07193750322113839
plot_id,batch_id 0 4 miss% 0.04775341906442109
plot_id,batch_id 0 5 miss% 0.04738380425752543
plot_id,batch_id 0 6 miss% 0.06533229683610907
plot_id,batch_id 0 7 miss% 0.08513248433271409
plot_id,batch_id 0 8 miss% 0.06955326654472661
plot_id,batch_id 0 9 miss% 0.06095508250705568
plot_id,batch_id 0 10 miss% 0.05767580535726212
plot_id,batch_id 0 11 miss% 0.040331648584031814
plot_id,batch_id 0 12 miss% 0.06747594030397644
plot_id,batch_id 0 13 miss% 0.06623919233020141
plot_id,batch_id 0 14 miss% 0.057685924975890826
plot_id,batch_id 0 15 miss% 0.050554950658860044
plot_id,batch_id 0 16 miss% 0.0786663848047142
plot_id,batch_id 0 17 miss% 0.10911773122681737
plot_id,batch_id 0 18 miss% 0.06718329727882966
plot_id,batch_id 0 19 miss% 0.10256436380433605
plot_id,batch_id 0 20 miss% 0.02964050753368713
plot_id,batch_id 0 21 miss% 0.06746353434150013
plot_id,batch_id 0 22 miss% 0.057265748747540354
plot_id,batch_id 0 23 miss% 0.03558355818395142
plot_id,batch_id 0 24 miss% 0.04716444662746721
plot_id,batch_id 0 25 miss% 0.051114910109158467
plot_id,batch_id 0 26 miss% 0.06471386320992449
plot_id,batch_id 0 27 miss% 0.08225739734014284
plot_id,batch_id 0 28 miss% 0.04433335545937319
plot_id,batch_id 0 29 miss% 0.04128858644791323
plot_id,batch_id 0 30 miss% 0.06581649892380739
plot_id,batch_id 0 31 miss% 0.06597419715627252
plot_id,batch_id 0 32 miss% 0.08184198031976779
plot_id,batch_id 0 33 miss% 0.07512822718647919
plot_id,batch_id 0 34 miss% 0.08096170866337428
plot_id,batch_id 0 35 miss% 0.03511122196841457
plot_id,batch_id 0 36 miss% 0.11093562178665792
plot_id,batch_id 0 37 miss% 0.10011973785545097
plot_id,batch_id 0 38 miss% 0.056746795622430075
plot_id,batch_id 0 39 miss% 0.04594238814704539
plot_id,batch_id 0 40 miss% 0.13879685720856272
plot_id,batch_id 0 41 miss% 0.06155765447358279
plot_id,batch_id 0 42 miss% 0.06167287826019962
plot_id,batch_id 0 43 miss% 0.26105875577784693
plot_id,batch_id 0 44 miss% 0.026508075171577168
plot_id,batch_id 0 45 miss% 0.07304259481896579
plot_id,batch_id 0 46 miss% 0.06533337430974043
plot_id,batch_id 0 47 miss% 0.05742071237774032
plot_id,batch_id 0 48 miss% 0.036349519478512296
plot_id,batch_id 0 49 miss% 0.033152985867504856
plot_id,batch_id 0 50 miss% 0.15536946214711359
plot_id,batch_id 0 51 miss% 0.030828386481514165
plot_id,batch_id 0 52 miss% 0.055579216427860526
plot_id,batch_id 0 53 miss% 0.0369147957286779
plot_id,batch_id 0 54 miss% 0.06798746212974094
plot_id,batch_id 0 55 miss% 0.04298001846669076
plot_id,batch_id 0 56 miss% 0.11027398552239334
plot_id,batch_id 0 57 miss% 0.06283034633762448
plot_id,batch_id 0 58 miss% 0.034019329004736185
plot_id,batch_id 0 59 miss% 0.04498481634779662
plot_id,batch_id 0 60 miss% 0.04894595797056199
plot_id,batch_id 0 61 miss% 0.030370806622368025
plot_id,batch_id 0 62 miss% 0.061149757446999124
plot_id,batch_id 0 63 miss% 0.06975390847466534
plot_id,batch_id 0 64 miss% 0.06011196317597988
plot_id,batch_id 0 65 miss% 0.12072038367602078
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  120401
Epoch:0, Train loss:0.590279, valid loss:0.579846
Epoch:1, Train loss:0.105525, valid loss:0.004556
Epoch:2, Train loss:0.011283, valid loss:0.003400
Epoch:3, Train loss:0.008055, valid loss:0.002865
Epoch:4, Train loss:0.006401, valid loss:0.002476
Epoch:5, Train loss:0.005603, valid loss:0.001589
Epoch:6, Train loss:0.003089, valid loss:0.001301
Epoch:7, Train loss:0.002773, valid loss:0.001330
Epoch:8, Train loss:0.002611, valid loss:0.001484
Epoch:9, Train loss:0.002343, valid loss:0.001525
Epoch:10, Train loss:0.002173, valid loss:0.001075
Epoch:11, Train loss:0.001658, valid loss:0.000900
Epoch:12, Train loss:0.001560, valid loss:0.000900
Epoch:13, Train loss:0.001550, valid loss:0.000859
Epoch:14, Train loss:0.001469, valid loss:0.000900
Epoch:15, Train loss:0.001463, valid loss:0.000995
Epoch:16, Train loss:0.001434, valid loss:0.000779
Epoch:17, Train loss:0.001393, valid loss:0.000883
Epoch:18, Train loss:0.001293, valid loss:0.000803
Epoch:19, Train loss:0.001290, valid loss:0.000767
Epoch:20, Train loss:0.001245, valid loss:0.000824
Epoch:21, Train loss:0.000945, valid loss:0.000694
Epoch:22, Train loss:0.000926, valid loss:0.000738
Epoch:23, Train loss:0.000893, valid loss:0.000730
Epoch:24, Train loss:0.000886, valid loss:0.000830
Epoch:25, Train loss:0.000927, valid loss:0.000682
Epoch:26, Train loss:0.000867, valid loss:0.000625
Epoch:27, Train loss:0.000842, valid loss:0.000665
Epoch:28, Train loss:0.000853, valid loss:0.000666
Epoch:29, Train loss:0.000825, valid loss:0.000697
Epoch:30, Train loss:0.000805, valid loss:0.000653
Epoch:31, Train loss:0.000659, valid loss:0.000608
Epoch:32, Train loss:0.000655, valid loss:0.000616
Epoch:33, Train loss:0.000642, valid loss:0.000596
Epoch:34, Train loss:0.000627, valid loss:0.000572
Epoch:35, Train loss:0.000642, valid loss:0.000623
Epoch:36, Train loss:0.000629, valid loss:0.000571
Epoch:37, Train loss:0.000623, valid loss:0.000568
Epoch:38, Train loss:0.000609, valid loss:0.000690
Epoch:39, Train loss:0.000608, valid loss:0.000552
Epoch:40, Train loss:0.000584, valid loss:0.000609
Epoch:41, Train loss:0.000535, valid loss:0.000609
Epoch:42, Train loss:0.000529, valid loss:0.000554
Epoch:43, Train loss:0.000520, valid loss:0.000556
Epoch:44, Train loss:0.000518, valid loss:0.000567
Epoch:45, Train loss:0.000513, valid loss:0.000548
Epoch:46, Train loss:0.000513, valid loss:0.000561
Epoch:47, Train loss:0.000514, valid loss:0.000545
Epoch:48, Train loss:0.000501, valid loss:0.000546
Epoch:49, Train loss:0.000506, valid loss:0.000572
Epoch:50, Train loss:0.000498, valid loss:0.000546
Epoch:51, Train loss:0.000465, valid loss:0.000537
Epoch:52, Train loss:0.000462, valid loss:0.000536
Epoch:53, Train loss:0.000460, valid loss:0.000538
Epoch:54, Train loss:0.000459, valid loss:0.000533
Epoch:55, Train loss:0.000458, valid loss:0.000536
Epoch:56, Train loss:0.000457, valid loss:0.000534
Epoch:57, Train loss:0.000456, valid loss:0.000535
Epoch:58, Train loss:0.000456, valid loss:0.000537
Epoch:59, Train loss:0.000455, valid loss:0.000536
Epoch:60, Train loss:0.000454, valid loss:0.000534
training time 15881.459933042526
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.046166521913179874
plot_id,batch_id 0 1 miss% 0.035514649923085055
plot_id,batch_id 0 2 miss% 0.10034752210664467
plot_id,batch_id 0 3 miss% 0.06017759035252635
plot_id,batch_id 0 4 miss% 0.05233764931909132
plot_id,batch_id 0 5 miss% 0.10915997198846104
plot_id,batch_id 0 6 miss% 0.06410799223246155
plot_id,batch_id 0 7 miss% 0.09473962973050004
plot_id,batch_id 0 8 miss% 0.07049911915478292
plot_id,batch_id 0 9 miss% 0.02657368729198175
plot_id,batch_id 0 10 miss% 0.025109586213944896
plot_id,batch_id 0 11 miss% 0.04862976868486278
plot_id,batch_id 0 12 miss% 0.04568474761860803
plot_id,batch_id 0 13 miss% 0.08528917900373853
plot_id,batch_id 0 14 miss% 0.07408212531414254
plot_id,batch_id 0 15 miss% 0.05684138329058568
plot_id,batch_id 0 16 miss% 0.11562650319965816
plot_id,batch_id 0 17 miss% 0.04629434659807051
plot_id,batch_id 0 18 miss% 0.05526380782625165
plot_id,batch_id 0 19 miss% 0.08263169091631463
plot_id,batch_id 0 20 miss% 0.06615553170093383
plot_id,batch_id 0 21 miss% 0.04607897495072473
plot_id,batch_id 0 22 miss% 0.048157447475591024
plot_id,batch_id 0 23 miss% 0.0463446547073809
plot_id,batch_id 0 24 miss% 0.039327781330045206
plot_id,batch_id 0 25 miss% 0.09171512486474771
plot_id,batch_id 0 26 miss% 0.06714761543961212
plot_id,batch_id 0 27 miss% 0.03411425866522129
plot_id,batch_id 0 28 miss% 0.03065507977477599
plot_id,batch_id 0 29 miss% 0.0370269377909629
plot_id,batch_id 0 30 miss% 0.047217539468506105
plot_id,batch_id 0 31 miss% 0.10065852177904365
plot_id,batch_id 0 32 miss% 0.09587062157980185
plot_id,batch_id 0 33 miss% 0.044503712156289885
plot_id,batch_id 0 34 miss% 0.04101051697412714
plot_id,batch_id 0 35 miss% 0.049233104761970875
plot_id,batch_id 0 36 miss% 0.10312786922369088
plot_id,batch_id 0 37 miss% 0.05449284449587167
plot_id,batch_id 0 38 miss% 0.04305599418097135
plot_id,batch_id 0 39 miss% 0.023592663306948293
plot_id,batch_id 0 40 miss% 0.07500302857557353
plot_id,batch_id 0 41 miss% 0.04991330330102747
plot_id,batch_id 0 42 miss% 0.024646774147726754
plot_id,batch_id 0 43 miss% 0.04838075597548312
plot_id,batch_id 0 44 miss% 0.036482277427784905
plot_id,batch_id 0 45 miss% 0.04235803102187376
plot_id,batch_id 0 46 miss% 0.03792260538208442
plot_id,batch_id 0 47 miss% 0.030472400912080918
plot_id,batch_id 0 48 miss% 0.040453969246916396
plot_id,batch_id 0 49 miss% 0.019543106710483055
plot_id,batch_id 0 50 miss% 0.09443074961802465
plot_id,batch_id 0 51 miss% 0.04409886253348496
plot_id,batch_id 0 52 miss% 0.03937552750848372
plot_id,batch_id 0 53 miss% 0.02130753148411629
plot_id,batch_id 0 54 miss% 0.02500635269338538
plot_id,batch_id 0 55 miss% 0.05490477630349882
plot_id,batch_id 0 56 miss% 0.07763369848444696
plot_id,batch_id 0 57 miss% 0.07535929032401094
plot_id,batch_id 0 58 miss% 0.04596446460622377
plot_id,batch_id 0 59 miss% 0.03284528942143674
plot_id,batch_id 0 60 miss% 0.020281223655613197
plot_id,batch_id 0 61 miss% 0.03079587393082325
plot_id,batch_id 0 62 miss% 0.054463818172119594
plot_id,batch_id 0 63 miss% 0.03204896909850623
plot_id,batch_id 0 64 miss% 0.047190255059681455
plot_id,batch_id 0 65 miss% 0.11203733428269394
plot_id,batch_id 0 66 miss% 0.09601968606734841
plot_id,batch_id 0 67 miss% 0.038312831524155
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  55697
Epoch:0, Train loss:0.434513, valid loss:0.402725
Epoch:1, Train loss:0.090920, valid loss:0.002241
Epoch:2, Train loss:0.003964, valid loss:0.001473
Epoch:3, Train loss:0.003075, valid loss:0.001269
Epoch:4, Train loss:0.002674, valid loss:0.001364
Epoch:5, Train loss:0.002419, valid loss:0.001137
Epoch:6, Train loss:0.002207, valid loss:0.001081
Epoch:7, Train loss:0.002005, valid loss:0.001038
Epoch:8, Train loss:0.001897, valid loss:0.000907
Epoch:9, Train loss:0.001774, valid loss:0.000946
Epoch:10, Train loss:0.001682, valid loss:0.001051
Epoch:11, Train loss:0.001361, valid loss:0.000707
Epoch:12, Train loss:0.001319, valid loss:0.000759
Epoch:13, Train loss:0.001275, valid loss:0.000705
Epoch:14, Train loss:0.001262, valid loss:0.000701
Epoch:15, Train loss:0.001203, valid loss:0.000666
Epoch:16, Train loss:0.001172, valid loss:0.000693
Epoch:17, Train loss:0.001137, valid loss:0.000697
Epoch:18, Train loss:0.001130, valid loss:0.000683
Epoch:19, Train loss:0.001098, valid loss:0.000640
Epoch:20, Train loss:0.001072, valid loss:0.000739
Epoch:21, Train loss:0.000913, valid loss:0.000597
Epoch:22, Train loss:0.000904, valid loss:0.000609
Epoch:23, Train loss:0.000898, valid loss:0.000597
Epoch:24, Train loss:0.000884, valid loss:0.000604
Epoch:25, Train loss:0.000870, valid loss:0.000537
Epoch:26, Train loss:0.000863, valid loss:0.000547
Epoch:27, Train loss:0.000855, valid loss:0.000558
Epoch:28, Train loss:0.000833, valid loss:0.000596
Epoch:29, Train loss:0.000834, valid loss:0.000710
Epoch:30, Train loss:0.000828, valid loss:0.000535
Epoch:31, Train loss:0.000741, valid loss:0.000536
Epoch:32, Train loss:0.000739, valid loss:0.000540
Epoch:33, Train loss:0.000730, valid loss:0.000495
Epoch:34, Train loss:0.000722, valid loss:0.000530
Epoch:35, Train loss:0.000720, valid loss:0.000514
Epoch:36, Train loss:0.000719, valid loss:0.000510
Epoch:37, Train loss:0.000714, valid loss:0.000538
Epoch:38, Train loss:0.000706, valid loss:0.000518
Epoch:39, Train loss:0.000696, valid loss:0.000524
Epoch:40, Train loss:0.000697, valid loss:0.000515
Epoch:41, Train loss:0.000656, valid loss:0.000516
Epoch:42, Train loss:0.000653, valid loss:0.000492
Epoch:43, Train loss:0.000654, valid loss:0.000502
Epoch:44, Train loss:0.000647, valid loss:0.000489
Epoch:45, Train loss:0.000644, valid loss:0.000479
Epoch:46, Train loss:0.000644, valid loss:0.000498
Epoch:47, Train loss:0.000639, valid loss:0.000497
Epoch:48, Train loss:0.000643, valid loss:0.000500
Epoch:49, Train loss:0.000637, valid loss:0.000490
Epoch:50, Train loss:0.000638, valid loss:0.000491
Epoch:51, Train loss:0.000613, valid loss:0.000483
Epoch:52, Train loss:0.000608, valid loss:0.000483
Epoch:53, Train loss:0.000606, valid loss:0.000485
Epoch:54, Train loss:0.000606, valid loss:0.000481
Epoch:55, Train loss:0.000606, valid loss:0.000484
Epoch:56, Train loss:0.000604, valid loss:0.000482
Epoch:57, Train loss:0.000605, valid loss:0.000483
Epoch:58, Train loss:0.000604, valid loss:0.000484
Epoch:59, Train loss:0.000604, valid loss:0.000483
Epoch:60, Train loss:0.000603, valid loss:0.000486
training time 15895.463388681412
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.034831147338938835
plot_id,batch_id 0 1 miss% 0.0390858654616165
plot_id,batch_id 0 2 miss% 0.09209714900796064
plot_id,batch_id 0 3 miss% 0.06502262075821338
plot_id,batch_id 0 4 miss% 0.1001387786791604
plot_id,batch_id 0 5 miss% 0.07471546783952762
plot_id,batch_id 0 6 miss% 0.066597473081618
plot_id,batch_id 0 7 miss% 0.1227721952155838
plot_id,batch_id 0 8 miss% 0.09408865744223639
plot_id,batch_id 0 9 miss% 0.052980031799830817
plot_id,batch_id 0 10 miss% 0.0454064923402999
plot_id,batch_id 0 11 miss% 0.07987419375313211
plot_id,batch_id 0 12 miss% 0.06573470381599557
plot_id,batch_id 0 13 miss% 0.050262941238669104
plot_id,batch_id 0 14 miss% 0.12165086548190623
plot_id,batch_id 0 15 miss% 0.05909415493954161
plot_id,batch_id 0 16 miss% 0.1540432862808111
plot_id,batch_id 0 17 miss% 0.062200949772808725
plot_id,batch_id 0 18 miss% 0.08432281393715292
plot_id,batch_id 0 19 miss% 0.10343612606348092
plot_id,batch_id 0 20 miss% 0.06770479624709866
plot_id,batch_id 0 21 miss% 0.055471734397677976
plot_id,batch_id 0 22 miss% 0.07485187721667745
plot_id,batch_id 0 23 miss% 0.04876336818244338
plot_id,batch_id 0 24 miss% 0.09183451896756081
plot_id,batch_id 0 25 miss% 0.12653735370073443
plot_id,batch_id 0 26 miss% 0.06426338942694065
plot_id,batch_id 0 27 miss% 0.0644967250589571
plot_id,batch_id 0 28 miss% 0.03947322080833837
plot_id,batch_id 0 29 miss% 0.03015780005335347
plot_id,batch_id 0 30 miss% 0.018933266296694812
plot_id,batch_id 0 31 miss% 0.10817264855037746
plot_id,batch_id 0 32 miss% 0.13163490535511735
plot_id,batch_id 0 33 miss% 0.06705195564877897
plot_id,batch_id 0 34 miss% 0.05975318742823223
plot_id,batch_id 0 35 miss% 0.09520965843678138
plot_id,batch_id 0 36 miss% 0.11450793579553678
plot_id,batch_id 0 37 miss% 0.07317769296624058
plot_id,batch_id 0 38 miss% 0.0502051539728921
plot_id,batch_id 0 39 miss% 0.028305451920549764
plot_id,batch_id 0 40 miss% 0.10478353454682994
plot_id,batch_id 0 41 miss% 0.10913804393881757
plot_id,batch_id 0 42 miss% 0.03953487240224964
plot_id,batch_id 0 43 miss% 0.05476917100621599
plot_id,batch_id 0 44 miss% 0.06539006552339154
plot_id,batch_id 0 45 miss% 0.04753374954979097
plot_id,batch_id 0 46 miss% 0.054036813704678756
plot_id,batch_id 0 47 miss% 0.06619223085985858
plot_id,batch_id 0 48 miss% 0.05483757288064696
plot_id,batch_id 0 49 miss% 0.052191031041987065
plot_id,batch_id 0 50 miss% 0.1302584781853825
plot_id,batch_id 0 51 miss% 0.10632427401090018
plot_id,batch_id 0 52 miss% 0.11215599796001521
plot_id,batch_id 0 53 miss% 0.025947572546587132
plot_id,batch_id 0 54 miss% 0.055366412679169696
plot_id,batch_id 0 55 miss% 0.049254250703666444
plot_id,batch_id 0 56 miss% 0.09498346882671481
plot_id,batch_id 0 57 miss% 0.07256317702511947
plot_id,batch_id 0 58 miss% 0.05026957804374693
plot_id,batch_id 0 59 miss% 0.06852231922402494
plot_id,batch_id 0 60 miss% 0.051176570803939156
plot_id,batch_id 0 61 miss% 0.04699638721406358
plot_id,batch_id 0 62 miss% 0.06457213190831292
plot_id,batch_id 0 63 miss% 0.047286919363130904
plot_id,batch_id 0 64 miss% 0.07867058564635313
plot_id,batch_id 0 65 miss% 0.1239262898147517
plot_id,batch_id 0 69 miss% 0.0797445446255651
plot_id,batch_id 0 70 miss% 0.15457239645475696
plot_id,batch_id 0 71 miss% 0.05339694018877648
plot_id,batch_id 0 72 miss% 0.12427589584158123
plot_id,batch_id 0 73 miss% 0.03805821487810231
plot_id,batch_id 0 74 miss% 0.07738439326344677
plot_id,batch_id 0 75 miss% 0.04380879367442578
plot_id,batch_id 0 76 miss% 0.1022939230572939
plot_id,batch_id 0 77 miss% 0.024630930332808238
plot_id,batch_id 0 78 miss% 0.029125952127628457
plot_id,batch_id 0 79 miss% 0.06894995453401866
plot_id,batch_id 0 80 miss% 0.0311754990613907
plot_id,batch_id 0 81 miss% 0.0645384048739204
plot_id,batch_id 0 82 miss% 0.07976143535179472
plot_id,batch_id 0 83 miss% 0.03526064351731176
plot_id,batch_id 0 84 miss% 0.0347473069481399
plot_id,batch_id 0 85 miss% 0.07031776595194929
plot_id,batch_id 0 86 miss% 0.05272330123076671
plot_id,batch_id 0 87 miss% 0.10015580968993511
plot_id,batch_id 0 88 miss% 0.08654508566684345
plot_id,batch_id 0 89 miss% 0.0816567789882024
plot_id,batch_id 0 90 miss% 0.029648656564194385
plot_id,batch_id 0 91 miss% 0.05903599586529725
plot_id,batch_id 0 92 miss% 0.049545343383577974
plot_id,batch_id 0 93 miss% 0.06970468498316308
plot_id,batch_id 0 94 miss% 0.08826493442539642
plot_id,batch_id 0 95 miss% 0.052541270215994015
plot_id,batch_id 0 96 miss% 0.04721157358514046
plot_id,batch_id 0 97 miss% 0.09028409452485163
plot_id,batch_id 0 98 miss% 0.0304328696783185
plot_id,batch_id 0 99 miss% 0.05612054516321757
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10286923 0.06296288 0.06426119 0.03966091 0.06615243 0.04308938
 0.05536619 0.06540677 0.06653033 0.04791375 0.0477517  0.05917658
 0.04329459 0.03007441 0.03727534 0.05012303 0.1061944  0.03509278
 0.14310958 0.05325838 0.05593556 0.02888004 0.05416822 0.05060517
 0.0457764  0.10897212 0.02881458 0.04465525 0.07261669 0.03964665
 0.05518071 0.08701781 0.06398588 0.05262604 0.05082521 0.0281145
 0.06839309 0.04766029 0.07067743 0.04928841 0.02617588 0.03717255
 0.021419   0.05460053 0.04327396 0.04182342 0.04349133 0.03044642
 0.030412   0.03575537 0.11641026 0.04196596 0.0457164  0.01315897
 0.03522028 0.03433502 0.04413514 0.04986292 0.03821123 0.02519114
 0.03511317 0.03198243 0.08931695 0.03943174 0.06877332 0.04323399
 0.14247664 0.06345924 0.05761866 0.07974454 0.1545724  0.05339694
 0.1242759  0.03805821 0.07738439 0.04380879 0.10229392 0.02463093
 0.02912595 0.06894995 0.0311755  0.0645384  0.07976144 0.03526064
 0.03474731 0.07031777 0.0527233  0.10015581 0.08654509 0.08165678
 0.02964866 0.059036   0.04954534 0.06970468 0.08826493 0.05254127
 0.04721157 0.09028409 0.03043287 0.05612055]
for model  44 the mean error 0.05709501757772531
all id 44 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 21 out win 6 err 0.05709501757772531 time 15849.486867904663
Launcher: Job 45 completed in 16115 seconds.
Launcher: Task 28 done. Exiting.
plot_id,batch_id 0 66 miss% 0.037225026727946024
plot_id,batch_id 0 67 miss% 0.05004999788156031
plot_id,batch_id 0 68 miss% 0.04486189263855242
plot_id,batch_id 0 69 miss% 0.05349453274701016
plot_id,batch_id 0 70 miss% 0.06955750536455249
plot_id,batch_id 0 71 miss% 0.07041216231405899
plot_id,batch_id 0 72 miss% 0.12638425179106091
plot_id,batch_id 0 73 miss% 0.06633605009199298
plot_id,batch_id 0 74 miss% 0.07828042151678899
plot_id,batch_id 0 75 miss% 0.09492779865766536
plot_id,batch_id 0 76 miss% 0.05000693022482569
plot_id,batch_id 0 77 miss% 0.05198851198352583
plot_id,batch_id 0 78 miss% 0.04927456349665007
plot_id,batch_id 0 79 miss% 0.08665839743986073
plot_id,batch_id 0 80 miss% 0.04423473412446453
plot_id,batch_id 0 81 miss% 0.07591467674365761
plot_id,batch_id 0 82 miss% 0.06950018374387093
plot_id,batch_id 0 83 miss% 0.07098123054639427
plot_id,batch_id 0 84 miss% 0.07491209783582463
plot_id,batch_id 0 85 miss% 0.027933918963923866
plot_id,batch_id 0 86 miss% 0.07035954570198164
plot_id,batch_id 0 87 miss% 0.07023704638329697
plot_id,batch_id 0 88 miss% 0.09968474615542244
plot_id,batch_id 0 89 miss% 0.06836010077482575
plot_id,batch_id 0 90 miss% 0.051955984314732584
plot_id,batch_id 0 91 miss% 0.06150944849748068
plot_id,batch_id 0 92 miss% 0.04632297928822838
plot_id,batch_id 0 93 miss% 0.06652999972849509
plot_id,batch_id 0 94 miss% 0.058950258906873804
plot_id,batch_id 0 95 miss% 0.06877340734012852
plot_id,batch_id 0 96 miss% 0.05358072194321465
plot_id,batch_id 0 97 miss% 0.038779510311337334
plot_id,batch_id 0 98 miss% 0.06242698001094285
plot_id,batch_id 0 99 miss% 0.039079768493285795
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09251454 0.05118182 0.1180721  0.0719375  0.04775342 0.0473838
 0.0653323  0.08513248 0.06955327 0.06095508 0.05767581 0.04033165
 0.06747594 0.06623919 0.05768592 0.05055495 0.07866638 0.10911773
 0.0671833  0.10256436 0.02964051 0.06746353 0.05726575 0.03558356
 0.04716445 0.05111491 0.06471386 0.0822574  0.04433336 0.04128859
 0.0658165  0.0659742  0.08184198 0.07512823 0.08096171 0.03511122
 0.11093562 0.10011974 0.0567468  0.04594239 0.13879686 0.06155765
 0.06167288 0.26105876 0.02650808 0.07304259 0.06533337 0.05742071
 0.03634952 0.03315299 0.15536946 0.03082839 0.05557922 0.0369148
 0.06798746 0.04298002 0.11027399 0.06283035 0.03401933 0.04498482
 0.04894596 0.03037081 0.06114976 0.06975391 0.06011196 0.12072038
 0.03722503 0.05005    0.04486189 0.05349453 0.06955751 0.07041216
 0.12638425 0.06633605 0.07828042 0.0949278  0.05000693 0.05198851
 0.04927456 0.0866584  0.04423473 0.07591468 0.06950018 0.07098123
 0.0749121  0.02793392 0.07035955 0.07023705 0.09968475 0.0683601
 0.05195598 0.06150945 0.04632298 0.06653    0.05895026 0.06877341
 0.05358072 0.03877951 0.06242698 0.03907977]
for model  235 the mean error 0.06643915232570555
all id 235 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 31 out win 5 err 0.06643915232570555 time 15869.205983400345
Launcher: Job 236 completed in 16128 seconds.
Launcher: Task 208 done. Exiting.
plot_id,batch_id 0 68 miss% 0.044663802250862944
plot_id,batch_id 0 69 miss% 0.05655942024196804
plot_id,batch_id 0 70 miss% 0.09212275799975166
plot_id,batch_id 0 71 miss% 0.03548615047929438
plot_id,batch_id 0 72 miss% 0.03959343365805873
plot_id,batch_id 0 73 miss% 0.021216000756795275
plot_id,batch_id 0 74 miss% 0.04522578421731906
plot_id,batch_id 0 75 miss% 0.08628108985393158
plot_id,batch_id 0 76 miss% 0.06106225484247238
plot_id,batch_id 0 77 miss% 0.04195110939165194
plot_id,batch_id 0 78 miss% 0.05225784919423893
plot_id,batch_id 0 79 miss% 0.07731944692269196
plot_id,batch_id 0 80 miss% 0.08155401831626576
plot_id,batch_id 0 81 miss% 0.07053673059516824
plot_id,batch_id 0 82 miss% 0.03761273965026397
plot_id,batch_id 0 83 miss% 0.07392159355762125
plot_id,batch_id 0 84 miss% 0.0634173896272565
plot_id,batch_id 0 85 miss% 0.03184404067281483
plot_id,batch_id 0 86 miss% 0.05758820017694534
plot_id,batch_id 0 87 miss% 0.047088084192314875
plot_id,batch_id 0 88 miss% 0.07400530173451361
plot_id,batch_id 0 89 miss% 0.056648403254649225
plot_id,batch_id 0 90 miss% 0.03847616722696812
plot_id,batch_id 0 91 miss% 0.09037292647578378
plot_id,batch_id 0 92 miss% 0.04381656519706585
plot_id,batch_id 0 93 miss% 0.03828587971674933
plot_id,batch_id 0 94 miss% 0.08593267477634822
plot_id,batch_id 0 95 miss% 0.05388535189100038
plot_id,batch_id 0 96 miss% 0.039321705514288624
plot_id,batch_id 0 97 miss% 0.03254622598246999
plot_id,batch_id 0 98 miss% 0.06202358139622334
plot_id,batch_id 0 99 miss% 0.04848887110637786
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04616652 0.03551465 0.10034752 0.06017759 0.05233765 0.10915997
 0.06410799 0.09473963 0.07049912 0.02657369 0.02510959 0.04862977
 0.04568475 0.08528918 0.07408213 0.05684138 0.1156265  0.04629435
 0.05526381 0.08263169 0.06615553 0.04607897 0.04815745 0.04634465
 0.03932778 0.09171512 0.06714762 0.03411426 0.03065508 0.03702694
 0.04721754 0.10065852 0.09587062 0.04450371 0.04101052 0.0492331
 0.10312787 0.05449284 0.04305599 0.02359266 0.07500303 0.0499133
 0.02464677 0.04838076 0.03648228 0.04235803 0.03792261 0.0304724
 0.04045397 0.01954311 0.09443075 0.04409886 0.03937553 0.02130753
 0.02500635 0.05490478 0.0776337  0.07535929 0.04596446 0.03284529
 0.02028122 0.03079587 0.05446382 0.03204897 0.04719026 0.11203733
 0.09601969 0.03831283 0.0446638  0.05655942 0.09212276 0.03548615
 0.03959343 0.021216   0.04522578 0.08628109 0.06106225 0.04195111
 0.05225785 0.07731945 0.08155402 0.07053673 0.03761274 0.07392159
 0.06341739 0.03184404 0.0575882  0.04708808 0.0740053  0.0566484
 0.03847617 0.09037293 0.04381657 0.03828588 0.08593267 0.05388535
 0.03932171 0.03254623 0.06202358 0.04848887]
for model  121 the mean error 0.05532922605645324
all id 121 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 25 out win 5 err 0.05532922605645324 time 15881.459933042526
Launcher: Job 122 completed in 16148 seconds.
Launcher: Task 46 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  55697
Epoch:0, Train loss:0.408340, valid loss:0.378156
Epoch:1, Train loss:0.125330, valid loss:0.002228
Epoch:2, Train loss:0.004534, valid loss:0.002135
Epoch:3, Train loss:0.003317, valid loss:0.001558
Epoch:4, Train loss:0.002828, valid loss:0.001236
Epoch:5, Train loss:0.002498, valid loss:0.001236
Epoch:6, Train loss:0.002300, valid loss:0.001402
Epoch:7, Train loss:0.002183, valid loss:0.001016
Epoch:8, Train loss:0.001986, valid loss:0.000994
Epoch:9, Train loss:0.001946, valid loss:0.001066
Epoch:10, Train loss:0.001754, valid loss:0.001142
Epoch:11, Train loss:0.001448, valid loss:0.000796
Epoch:12, Train loss:0.001382, valid loss:0.000746
Epoch:13, Train loss:0.001315, valid loss:0.000722
Epoch:14, Train loss:0.001306, valid loss:0.000705
Epoch:15, Train loss:0.001238, valid loss:0.000723
Epoch:16, Train loss:0.001212, valid loss:0.000688
Epoch:17, Train loss:0.001184, valid loss:0.000664
Epoch:18, Train loss:0.001136, valid loss:0.000692
Epoch:19, Train loss:0.001129, valid loss:0.000749
Epoch:20, Train loss:0.001106, valid loss:0.000679
Epoch:21, Train loss:0.000926, valid loss:0.000617
Epoch:22, Train loss:0.000908, valid loss:0.000614
Epoch:23, Train loss:0.000903, valid loss:0.000576
Epoch:24, Train loss:0.000891, valid loss:0.000590
Epoch:25, Train loss:0.000878, valid loss:0.000590
Epoch:26, Train loss:0.000865, valid loss:0.000583
Epoch:27, Train loss:0.000861, valid loss:0.000726
Epoch:28, Train loss:0.000830, valid loss:0.000604
Epoch:29, Train loss:0.000838, valid loss:0.000564
Epoch:30, Train loss:0.000817, valid loss:0.000605
Epoch:31, Train loss:0.000735, valid loss:0.000567
Epoch:32, Train loss:0.000729, valid loss:0.000581
Epoch:33, Train loss:0.000721, valid loss:0.000595
Epoch:34, Train loss:0.000714, valid loss:0.000552
Epoch:35, Train loss:0.000715, valid loss:0.000570
Epoch:36, Train loss:0.000712, valid loss:0.000578
Epoch:37, Train loss:0.000701, valid loss:0.000558
Epoch:38, Train loss:0.000695, valid loss:0.000537
Epoch:39, Train loss:0.000689, valid loss:0.000572
Epoch:40, Train loss:0.000700, valid loss:0.000551
Epoch:41, Train loss:0.000644, valid loss:0.000538
Epoch:42, Train loss:0.000641, valid loss:0.000528
Epoch:43, Train loss:0.000636, valid loss:0.000537
Epoch:44, Train loss:0.000633, valid loss:0.000539
Epoch:45, Train loss:0.000632, valid loss:0.000522
Epoch:46, Train loss:0.000629, valid loss:0.000523
Epoch:47, Train loss:0.000626, valid loss:0.000525
Epoch:48, Train loss:0.000623, valid loss:0.000545
Epoch:49, Train loss:0.000624, valid loss:0.000538
Epoch:50, Train loss:0.000623, valid loss:0.000526
Epoch:51, Train loss:0.000592, valid loss:0.000520
Epoch:52, Train loss:0.000588, valid loss:0.000517
Epoch:53, Train loss:0.000586, valid loss:0.000516
Epoch:54, Train loss:0.000585, valid loss:0.000517
Epoch:55, Train loss:0.000584, valid loss:0.000514
Epoch:56, Train loss:0.000584, valid loss:0.000518
Epoch:57, Train loss:0.000583, valid loss:0.000516
Epoch:58, Train loss:0.000583, valid loss:0.000520
Epoch:59, Train loss:0.000582, valid loss:0.000517
Epoch:60, Train loss:0.000582, valid loss:0.000515
training time 15938.169939994812
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07220157414567588
plot_id,batch_id 0 1 miss% 0.046998456031278245
plot_id,batch_id 0 2 miss% 0.1355592665326706
plot_id,batch_id 0 3 miss% 0.058285691372282164
plot_id,batch_id 0 4 miss% 0.07421772028615567
plot_id,batch_id 0 5 miss% 0.07996308931463603
plot_id,batch_id 0 6 miss% 0.04478258932726128
plot_id,batch_id 0 7 miss% 0.10868340074487766
plot_id,batch_id 0 8 miss% 0.0823964099449053
plot_id,batch_id 0 9 miss% 0.04172353345982212
plot_id,batch_id 0 10 miss% 0.040192059757086666
plot_id,batch_id 0 11 miss% 0.05706217063556514
plot_id,batch_id 0 12 miss% 0.08574453690884026
plot_id,batch_id 0 13 miss% 0.08102617310038561
plot_id,batch_id 0 14 miss% 0.11412559313662872
plot_id,batch_id 0 15 miss% 0.03413890868737899
plot_id,batch_id 0 16 miss% 0.07499778127690086
plot_id,batch_id 0 17 miss% 0.08455904864623288
plot_id,batch_id 0 18 miss% 0.07899967183931104
plot_id,batch_id 0 19 miss% 0.07573983541757678
plot_id,batch_id 0 20 miss% 0.07514612203599494
plot_id,batch_id 0 21 miss% 0.03610941872931536
plot_id,batch_id 0 22 miss% 0.06137748167901255
plot_id,batch_id 0 23 miss% 0.0329099218866547
plot_id,batch_id 0 24 miss% 0.06361836147508355
plot_id,batch_id 0 25 miss% 0.056547718811067064
plot_id,batch_id 0 26 miss% 0.04756885572433422
plot_id,batch_id 0 27 miss% 0.043384295774920874
plot_id,batch_id 0 28 miss% 0.01742594254582898
plot_id,batch_id 0 29 miss% 0.02216904250467595
plot_id,batch_id 0 30 miss% 0.019931086021483614
plot_id,batch_id 0 31 miss% 0.08878791381612719
plot_id,batch_id 0 32 miss% 0.13663925267790553
plot_id,batch_id 0 33 miss% 0.049475433370382134
plot_id,batch_id 0 34 miss% 0.06427627445310632
plot_id,batch_id 0 35 miss% 0.030247235236845464
plot_id,batch_id 0 36 miss% 0.12087232618575079
plot_id,batch_id 0 37 miss% 0.07110955888136215
plot_id,batch_id 0 38 miss% 0.04145180419584101
plot_id,batch_id 0 39 miss% 0.04040156586247775
plot_id,batch_id 0 40 miss% 0.07548282821494011
plot_id,batch_id 0 41 miss% 0.07602702154497683
plot_id,batch_id 0 42 miss% 0.02588245675810012
plot_id,batch_id 0 43 miss% 0.041392079577284184
plot_id,batch_id 0 44 miss% 0.018798420169907296
plot_id,batch_id 0 45 miss% 0.04610998104360778
plot_id,batch_id 0 46 miss% 0.031072286812332296
plot_id,batch_id 0 47 miss% 0.0387716214371816
plot_id,batch_id 0 48 miss% 0.03319569152266942
plot_id,batch_id 0 49 miss% 0.03532534310014455
plot_id,batch_id 0 50 miss% 0.12791530604802567
plot_id,batch_id 0 51 miss% 0.04343276058824897
plot_id,batch_id 0 52 miss% 0.03399588098525157
plot_id,batch_id 0 53 miss% 0.02689717165995501
plot_id,batch_id 0 54 miss% 0.027595033139858626
plot_id,batch_id 0 55 miss% 0.06889443580672831
plot_id,batch_id 0 56 miss% 0.10447833978398047
plot_id,batch_id 0 57 miss% 0.04434751026271355
plot_id,batch_id 0 58 miss% 0.04290143514271098
plot_id,batch_id 0 59 miss% 0.043212029617135994
plot_id,batch_id 0 60 miss% 0.0347411426493273
plot_id,batch_id 0 61 miss% 0.0388758287084185
plot_id,batch_id 0 62 miss% 0.06000569853404535
plot_id,batch_id 0 63 miss% 0.06379679550278736
plot_id,batch_id 0 64 miss% 0.043903196087008135
plot_id,batch_id 0 65 miss% 0.08971552523815138
plot_id,batch_id 0 66 miss% 0.023653297722232045
plot_id,batch_id 0 67 miss% plot_id,batch_id 0 66 miss% 0.03319184328075152
plot_id,batch_id 0 67 miss% 0.021434248084858797
plot_id,batch_id 0 68 miss% 0.03725008376899788
plot_id,batch_id 0 69 miss% 0.08226029292332732
plot_id,batch_id 0 70 miss% 0.07704700973185255
plot_id,batch_id 0 71 miss% 0.053937808135257094
plot_id,batch_id 0 72 miss% 0.054852797510197455
plot_id,batch_id 0 73 miss% 0.11265919501777912
plot_id,batch_id 0 74 miss% 0.08852880809533618
plot_id,batch_id 0 75 miss% 0.023381323351141396
plot_id,batch_id 0 76 miss% 0.0590007240978305
plot_id,batch_id 0 77 miss% 0.04293390527554455
plot_id,batch_id 0 78 miss% 0.050369477688591686
plot_id,batch_id 0 79 miss% 0.049655300676944784
plot_id,batch_id 0 80 miss% 0.07090979070755732
plot_id,batch_id 0 81 miss% 0.11713737012989958
plot_id,batch_id 0 82 miss% 0.05313110702528558
plot_id,batch_id 0 83 miss% 0.08956664720269081
plot_id,batch_id 0 84 miss% 0.0981183065020657
plot_id,batch_id 0 85 miss% 0.08581313964965837
plot_id,batch_id 0 86 miss% 0.05254274949130301
plot_id,batch_id 0 87 miss% 0.1008555326271057
plot_id,batch_id 0 88 miss% 0.08881109810377757
plot_id,batch_id 0 89 miss% 0.10999785157550193
plot_id,batch_id 0 90 miss% 0.06877683755517211
plot_id,batch_id 0 91 miss% 0.07728218652754504
plot_id,batch_id 0 92 miss% 0.05113249569817029
plot_id,batch_id 0 93 miss% 0.04963699475263321
plot_id,batch_id 0 94 miss% 0.05311601367264992
plot_id,batch_id 0 95 miss% 0.07926137540791849
plot_id,batch_id 0 96 miss% 0.06369997147891894
plot_id,batch_id 0 97 miss% 0.050309061120883763
plot_id,batch_id 0 98 miss% 0.11461446968696988
plot_id,batch_id 0 99 miss% 0.06775623687451764
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03483115 0.03908587 0.09209715 0.06502262 0.10013878 0.07471547
 0.06659747 0.1227722  0.09408866 0.05298003 0.04540649 0.07987419
 0.0657347  0.05026294 0.12165087 0.05909415 0.15404329 0.06220095
 0.08432281 0.10343613 0.0677048  0.05547173 0.07485188 0.04876337
 0.09183452 0.12653735 0.06426339 0.06449673 0.03947322 0.0301578
 0.01893327 0.10817265 0.13163491 0.06705196 0.05975319 0.09520966
 0.11450794 0.07317769 0.05020515 0.02830545 0.10478353 0.10913804
 0.03953487 0.05476917 0.06539007 0.04753375 0.05403681 0.06619223
 0.05483757 0.05219103 0.13025848 0.10632427 0.112156   0.02594757
 0.05536641 0.04925425 0.09498347 0.07256318 0.05026958 0.06852232
 0.05117657 0.04699639 0.06457213 0.04728692 0.07867059 0.12392629
 0.03319184 0.02143425 0.03725008 0.08226029 0.07704701 0.05393781
 0.0548528  0.1126592  0.08852881 0.02338132 0.05900072 0.04293391
 0.05036948 0.0496553  0.07090979 0.11713737 0.05313111 0.08956665
 0.09811831 0.08581314 0.05254275 0.10085553 0.0888111  0.10999785
 0.06877684 0.07728219 0.0511325  0.04963699 0.05311601 0.07926138
 0.06369997 0.05030906 0.11461447 0.06775624]
for model  172 the mean error 0.0713451610751845
all id 172 hidden_dim 16 learning_rate 0.0025 num_layers 4 frames 31 out win 5 err 0.0713451610751845 time 15895.463388681412
Launcher: Job 173 completed in 16157 seconds.
Launcher: Task 19 done. Exiting.
0.039092185950861016
plot_id,batch_id 0 68 miss% 0.03992462427527743
plot_id,batch_id 0 69 miss% 0.06505390520938777
plot_id,batch_id 0 70 miss% 0.050832065972317465
plot_id,batch_id 0 71 miss% 0.03154663984437688
plot_id,batch_id 0 72 miss% 0.10912614996291327
plot_id,batch_id 0 73 miss% 0.04567614252205087
plot_id,batch_id 0 74 miss% 0.13571751847495642
plot_id,batch_id 0 75 miss% 0.09131535196084092
plot_id,batch_id 0 76 miss% 0.09001436859880634
plot_id,batch_id 0 77 miss% 0.06416073723237234
plot_id,batch_id 0 78 miss% 0.0368664705154869
plot_id,batch_id 0 79 miss% 0.08654193265505401
plot_id,batch_id 0 80 miss% 0.02789152478758914
plot_id,batch_id 0 81 miss% 0.09153547613053734
plot_id,batch_id 0 82 miss% 0.08814801210721725
plot_id,batch_id 0 83 miss% 0.0778992387401207
plot_id,batch_id 0 84 miss% 0.0845184360617667
plot_id,batch_id 0 85 miss% 0.040907961159461964
plot_id,batch_id 0 86 miss% 0.05496293603293626
plot_id,batch_id 0 87 miss% 0.09444869118833454
plot_id,batch_id 0 88 miss% 0.09465903396834842
plot_id,batch_id 0 89 miss% 0.05208902354409367
plot_id,batch_id 0 90 miss% 0.03892454249105408
plot_id,batch_id 0 91 miss% 0.03629787314077299
plot_id,batch_id 0 92 miss% 0.05957174875256841
plot_id,batch_id 0 93 miss% 0.03213771121775245
plot_id,batch_id 0 94 miss% 0.17246661344978853
plot_id,batch_id 0 95 miss% 0.084475813985993
plot_id,batch_id 0 96 miss% 0.060723766795482595
plot_id,batch_id 0 97 miss% 0.049327467873318265
plot_id,batch_id 0 98 miss% 0.04568137301175934
plot_id,batch_id 0 99 miss% 0.06847594995103348
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07220157 0.04699846 0.13555927 0.05828569 0.07421772 0.07996309
 0.04478259 0.1086834  0.08239641 0.04172353 0.04019206 0.05706217
 0.08574454 0.08102617 0.11412559 0.03413891 0.07499778 0.08455905
 0.07899967 0.07573984 0.07514612 0.03610942 0.06137748 0.03290992
 0.06361836 0.05654772 0.04756886 0.0433843  0.01742594 0.02216904
 0.01993109 0.08878791 0.13663925 0.04947543 0.06427627 0.03024724
 0.12087233 0.07110956 0.0414518  0.04040157 0.07548283 0.07602702
 0.02588246 0.04139208 0.01879842 0.04610998 0.03107229 0.03877162
 0.03319569 0.03532534 0.12791531 0.04343276 0.03399588 0.02689717
 0.02759503 0.06889444 0.10447834 0.04434751 0.04290144 0.04321203
 0.03474114 0.03887583 0.0600057  0.0637968  0.0439032  0.08971553
 0.0236533  0.03909219 0.03992462 0.06505391 0.05083207 0.03154664
 0.10912615 0.04567614 0.13571752 0.09131535 0.09001437 0.06416074
 0.03686647 0.08654193 0.02789152 0.09153548 0.08814801 0.07789924
 0.08451844 0.04090796 0.05496294 0.09444869 0.09465903 0.05208902
 0.03892454 0.03629787 0.05957175 0.03213771 0.17246661 0.08447581
 0.06072377 0.04932747 0.04568137 0.06847595]
for model  173 the mean error 0.06172277527655998
all id 173 hidden_dim 16 learning_rate 0.0025 num_layers 4 frames 31 out win 6 err 0.06172277527655998 time 15938.169939994812
Launcher: Job 174 completed in 16201 seconds.
Launcher: Task 30 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  154129
Epoch:0, Train loss:0.552158, valid loss:0.564355
Epoch:1, Train loss:0.079779, valid loss:0.004322
Epoch:2, Train loss:0.013549, valid loss:0.004265
Epoch:3, Train loss:0.012492, valid loss:0.003941
Epoch:4, Train loss:0.011074, valid loss:0.003503
Epoch:5, Train loss:0.005633, valid loss:0.001720
Epoch:6, Train loss:0.003219, valid loss:0.001628
Epoch:7, Train loss:0.002879, valid loss:0.001616
Epoch:8, Train loss:0.002660, valid loss:0.001210
Epoch:9, Train loss:0.002563, valid loss:0.001294
Epoch:10, Train loss:0.002510, valid loss:0.001340
Epoch:11, Train loss:0.001842, valid loss:0.000884
Epoch:12, Train loss:0.001615, valid loss:0.001155
Epoch:13, Train loss:0.001611, valid loss:0.001147
Epoch:14, Train loss:0.001527, valid loss:0.000945
Epoch:15, Train loss:0.001464, valid loss:0.000845
Epoch:16, Train loss:0.001444, valid loss:0.000836
Epoch:17, Train loss:0.001363, valid loss:0.000931
Epoch:18, Train loss:0.001315, valid loss:0.000781
Epoch:19, Train loss:0.001312, valid loss:0.000914
Epoch:20, Train loss:0.001291, valid loss:0.000706
Epoch:21, Train loss:0.000925, valid loss:0.000637
Epoch:22, Train loss:0.000876, valid loss:0.000710
Epoch:23, Train loss:0.000903, valid loss:0.000637
Epoch:24, Train loss:0.000835, valid loss:0.000645
Epoch:25, Train loss:0.000900, valid loss:0.000768
Epoch:26, Train loss:0.000840, valid loss:0.000614
Epoch:27, Train loss:0.000792, valid loss:0.000671
Epoch:28, Train loss:0.000821, valid loss:0.000784
Epoch:29, Train loss:0.000773, valid loss:0.000588
Epoch:30, Train loss:0.000788, valid loss:0.000618
Epoch:31, Train loss:0.000612, valid loss:0.000591
Epoch:32, Train loss:0.000611, valid loss:0.000635
Epoch:33, Train loss:0.000587, valid loss:0.000572
Epoch:34, Train loss:0.000573, valid loss:0.000591
Epoch:35, Train loss:0.000571, valid loss:0.000600
Epoch:36, Train loss:0.000580, valid loss:0.000558
Epoch:37, Train loss:0.000560, valid loss:0.000553
Epoch:38, Train loss:0.000575, valid loss:0.000553
Epoch:39, Train loss:0.000551, valid loss:0.000550
Epoch:40, Train loss:0.000544, valid loss:0.000601
Epoch:41, Train loss:0.000478, valid loss:0.000522
Epoch:42, Train loss:0.000462, valid loss:0.000517
Epoch:43, Train loss:0.000462, valid loss:0.000529
Epoch:44, Train loss:0.000459, valid loss:0.000524
Epoch:45, Train loss:0.000448, valid loss:0.000540
Epoch:46, Train loss:0.000461, valid loss:0.000536
Epoch:47, Train loss:0.000443, valid loss:0.000539
Epoch:48, Train loss:0.000438, valid loss:0.000565
Epoch:49, Train loss:0.000451, valid loss:0.000541
Epoch:50, Train loss:0.000448, valid loss:0.000520
Epoch:51, Train loss:0.000407, valid loss:0.000518
Epoch:52, Train loss:0.000405, valid loss:0.000517
Epoch:53, Train loss:0.000403, valid loss:0.000518
Epoch:54, Train loss:0.000403, valid loss:0.000518
Epoch:55, Train loss:0.000402, valid loss:0.000516
Epoch:56, Train loss:0.000401, valid loss:0.000517
Epoch:57, Train loss:0.000401, valid loss:0.000516
Epoch:58, Train loss:0.000400, valid loss:0.000517
Epoch:59, Train loss:0.000400, valid loss:0.000515
Epoch:60, Train loss:0.000400, valid loss:0.000513
training time 16027.346613407135
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.07344611762905932
plot_id,batch_id 0 1 miss% 0.06451102689972649
plot_id,batch_id 0 2 miss% 0.10532350976689832
plot_id,batch_id 0 3 miss% 0.05760396612775006
plot_id,batch_id 0 4 miss% 0.08470526020302852
plot_id,batch_id 0 5 miss% 0.05008808328399366
plot_id,batch_id 0 6 miss% 0.03849942366670898
plot_id,batch_id 0 7 miss% 0.08875991163216942
plot_id,batch_id 0 8 miss% 0.07569554422295381
plot_id,batch_id 0 9 miss% 0.052654598520400435
plot_id,batch_id 0 10 miss% 0.03228113874330759
plot_id,batch_id 0 11 miss% 0.07879438502357529
plot_id,batch_id 0 12 miss% 0.07515467643359934
plot_id,batch_id 0 13 miss% 0.03790059307833894
plot_id,batch_id 0 14 miss% 0.058315889487986085
plot_id,batch_id 0 15 miss% 0.06218124229513656
plot_id,batch_id 0 16 miss% 0.1470722586246591
plot_id,batch_id 0 17 miss% 0.02961405987760939
plot_id,batch_id 0 18 miss% 0.04818654186179316
plot_id,batch_id 0 19 miss% 0.0847849720446617
plot_id,batch_id 0 20 miss% 0.13457254737319474
plot_id,batch_id 0 21 miss% 0.04125239143549873
plot_id,batch_id 0 22 miss% 0.03675334674464629
plot_id,batch_id 0 23 miss% 0.05988616684263069
plot_id,batch_id 0 24 miss% 0.03908356858973742
plot_id,batch_id 0 25 miss% 0.10226231584361242
plot_id,batch_id 0 26 miss% 0.07377648765172916
plot_id,batch_id 0 27 miss% 0.04002650233905581
plot_id,batch_id 0 28 miss% 0.028603826430339555
plot_id,batch_id 0 29 miss% 0.029626857092854723
plot_id,batch_id 0 30 miss% 0.03790209095402583
plot_id,batch_id 0 31 miss% 0.07093409403410222
plot_id,batch_id 0 32 miss% 0.05971279670597949
plot_id,batch_id 0 33 miss% 0.05737193048901093
plot_id,batch_id 0 34 miss% 0.04266196491806279
plot_id,batch_id 0 35 miss% 0.03873092324890556
plot_id,batch_id 0 36 miss% 0.09022638417763659
plot_id,batch_id 0 37 miss% 0.04799224370672291
plot_id,batch_id 0 38 miss% 0.05632396405824629
plot_id,batch_id 0 39 miss% 0.030619052936558033
plot_id,batch_id 0 40 miss% 0.08175226169158728
plot_id,batch_id 0 41 miss% 0.041373258386111667
plot_id,batch_id 0 42 miss% 0.024158088625603286
plot_id,batch_id 0 43 miss% 0.06098274123421637
plot_id,batch_id 0 44 miss% 0.03372085634545201
plot_id,batch_id 0 45 miss% 0.03043854719907077
plot_id,batch_id 0 46 miss% 0.0335212529076411
plot_id,batch_id 0 47 miss% 0.03673892186357669
plot_id,batch_id 0 48 miss% 0.037877262743402036
plot_id,batch_id 0 49 miss% 0.03685675421395783
plot_id,batch_id 0 50 miss% 0.11773572471136264
plot_id,batch_id 0 51 miss% 0.07994809211019059
plot_id,batch_id 0 52 miss% 0.0493411771245017
plot_id,batch_id 0 53 miss% 0.013836686169780914
plot_id,batch_id 0 54 miss% 0.09211834049547286
plot_id,batch_id 0 55 miss% 0.11682039026992176
plot_id,batch_id 0 56 miss% 0.08631739434456812
plot_id,batch_id 0 57 miss% 0.0642482224833111
plot_id,batch_id 0 58 miss% 0.0347262874437599
plot_id,batch_id 0 59 miss% 0.04275692640137125
plot_id,batch_id 0 60 miss% 0.03437128844521847
plot_id,batch_id 0 61 miss% 0.021867397559873134
plot_id,batch_id 0 62 miss% 0.06492036445026204
plot_id,batch_id 0 63 miss% 0.04668942654535072
plot_id,batch_id 0 64 miss% 0.05178862303300461
plot_id,batch_id 0 65 miss% 0.06720102800568746
plot_id,batch_id 0 66 miss% 0.16036881986373658
plot_id,batch_id 0 67 miss% 0.03622775690980952
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  120401
Epoch:0, Train loss:0.567214, valid loss:0.551781
Epoch:1, Train loss:0.114823, valid loss:0.004219
Epoch:2, Train loss:0.012229, valid loss:0.003915
Epoch:3, Train loss:0.009602, valid loss:0.002496
Epoch:4, Train loss:0.006561, valid loss:0.002128
Epoch:5, Train loss:0.006134, valid loss:0.002108
Epoch:6, Train loss:0.005000, valid loss:0.001566
Epoch:7, Train loss:0.003247, valid loss:0.001587
Epoch:8, Train loss:0.002876, valid loss:0.001597
Epoch:9, Train loss:0.002748, valid loss:0.001441
Epoch:10, Train loss:0.002562, valid loss:0.001398
Epoch:11, Train loss:0.001899, valid loss:0.000986
Epoch:12, Train loss:0.001782, valid loss:0.001140
Epoch:13, Train loss:0.001710, valid loss:0.000992
Epoch:14, Train loss:0.001657, valid loss:0.000943
Epoch:15, Train loss:0.001589, valid loss:0.001043
Epoch:16, Train loss:0.001556, valid loss:0.000938
Epoch:17, Train loss:0.001522, valid loss:0.000926
Epoch:18, Train loss:0.001412, valid loss:0.000979
Epoch:19, Train loss:0.001429, valid loss:0.000961
Epoch:20, Train loss:0.001332, valid loss:0.000915
Epoch:21, Train loss:0.001048, valid loss:0.000799
Epoch:22, Train loss:0.001044, valid loss:0.000825
Epoch:23, Train loss:0.001018, valid loss:0.000967
Epoch:24, Train loss:0.000987, valid loss:0.000794
Epoch:25, Train loss:0.000977, valid loss:0.000738
Epoch:26, Train loss:0.000960, valid loss:0.000722
Epoch:27, Train loss:0.000937, valid loss:0.000735
Epoch:28, Train loss:0.000920, valid loss:0.000806
Epoch:29, Train loss:0.000927, valid loss:0.000813
Epoch:30, Train loss:0.000882, valid loss:0.000782
Epoch:31, Train loss:0.000729, valid loss:0.000654
Epoch:32, Train loss:0.000707, valid loss:0.000665
Epoch:33, Train loss:0.000708, valid loss:0.000704
Epoch:34, Train loss:0.000695, valid loss:0.000655
Epoch:35, Train loss:0.000707, valid loss:0.000770
Epoch:36, Train loss:0.000683, valid loss:0.000746
Epoch:37, Train loss:0.000683, valid loss:0.000665
Epoch:38, Train loss:0.000686, valid loss:0.000656
Epoch:39, Train loss:0.000652, valid loss:0.000703
Epoch:40, Train loss:0.000662, valid loss:0.000666
Epoch:41, Train loss:0.000576, valid loss:0.000646
Epoch:42, Train loss:0.000565, valid loss:0.000665
Epoch:43, Train loss:0.000564, valid loss:0.000648
Epoch:44, Train loss:0.000561, valid loss:0.000653
Epoch:45, Train loss:0.000555, valid loss:0.000648
Epoch:46, Train loss:0.000552, valid loss:0.000615
Epoch:47, Train loss:0.000554, valid loss:0.000644
Epoch:48, Train loss:0.000547, valid loss:0.000624
Epoch:49, Train loss:0.000544, valid loss:0.000619
Epoch:50, Train loss:0.000537, valid loss:0.000648
Epoch:51, Train loss:0.000503, valid loss:0.000632
Epoch:52, Train loss:0.000496, valid loss:0.000628
Epoch:53, Train loss:0.000494, valid loss:0.000624
Epoch:54, Train loss:0.000492, valid loss:0.000624
Epoch:55, Train loss:0.000491, valid loss:0.000623
Epoch:56, Train loss:0.000490, valid loss:0.000622
Epoch:57, Train loss:0.000489, valid loss:0.000623
Epoch:58, Train loss:0.000488, valid loss:0.000622
Epoch:59, Train loss:0.000488, valid loss:0.000624
Epoch:60, Train loss:0.000487, valid loss:0.000625
training time 16026.08884882927
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.11282734559247333
plot_id,batch_id 0 1 miss% 0.05579990473068091
plot_id,batch_id 0 2 miss% 0.08701797137593788
plot_id,batch_id 0 3 miss% 0.0384597056953432
plot_id,batch_id 0 4 miss% 0.019647896787798037
plot_id,batch_id 0 5 miss% 0.04469576514102233
plot_id,batch_id 0 6 miss% 0.04082000006608835
plot_id,batch_id 0 7 miss% 0.08548261891327995
plot_id,batch_id 0 8 miss% 0.05587249481471221
plot_id,batch_id 0 9 miss% 0.08761409894403142
plot_id,batch_id 0 10 miss% 0.04613246556901161
plot_id,batch_id 0 11 miss% 0.03847629394524583
plot_id,batch_id 0 12 miss% 0.054592153699613385
plot_id,batch_id 0 13 miss% 0.06758175300382095
plot_id,batch_id 0 14 miss% 0.10031357011917125
plot_id,batch_id 0 15 miss% 0.045307846457814605
plot_id,batch_id 0 16 miss% 0.05989669045754519
plot_id,batch_id 0 17 miss% 0.03605139228362522
plot_id,batch_id 0 18 miss% 0.08134757991720465
plot_id,batch_id 0 19 miss% 0.08265582684241216
plot_id,batch_id 0 20 miss% 0.18125631448889531
plot_id,batch_id 0 21 miss% 0.021135784053709847
plot_id,batch_id 0 22 miss% 0.04633501232213067
plot_id,batch_id 0 23 miss% 0.030589951844228417
plot_id,batch_id 0 24 miss% 0.03689922224371928
plot_id,batch_id 0 25 miss% 0.052581073920416915
plot_id,batch_id 0 26 miss% 0.050456898878736715
plot_id,batch_id 0 27 miss% 0.02897370980462861
plot_id,batch_id 0 28 miss% 0.026168180388983397
plot_id,batch_id 0 29 miss% 0.03089824782979829
plot_id,batch_id 0 30 miss% 0.0240995385408883
plot_id,batch_id 0 31 miss% 0.0608241642010566
plot_id,batch_id 0 32 miss% 0.09771624213424182
plot_id,batch_id 0 33 miss% 0.03173002671898804
plot_id,batch_id 0 34 miss% 0.029322492657061865
plot_id,batch_id 0 35 miss% 0.04994504263272131
plot_id,batch_id 0 36 miss% 0.07309879257840061
plot_id,batch_id 0 37 miss% 0.051826412360321344
plot_id,batch_id 0 38 miss% 0.03842724531922283
plot_id,batch_id 0 39 miss% 0.04316863810695288
plot_id,batch_id 0 40 miss% 0.081721973427108
plot_id,batch_id 0 41 miss% 0.04271594182850369
plot_id,batch_id 0 42 miss% 0.01703794054093809
plot_id,batch_id 0 43 miss% 0.045888833196071344
plot_id,batch_id 0 44 miss% 0.029302164024289673
plot_id,batch_id 0 45 miss% 0.055310125655071644
plot_id,batch_id 0 46 miss% 0.03140128575856443
plot_id,batch_id 0 47 miss% 0.018698484514322516
plot_id,batch_id 0 48 miss% 0.019516207468846646
plot_id,batch_id 0 49 miss% 0.03938679947312487
plot_id,batch_id 0 50 miss% 0.14647095893113038
plot_id,batch_id 0 51 miss% 0.03309214228018017
plot_id,batch_id 0 52 miss% 0.023712269441268932
plot_id,batch_id 0 53 miss% 0.02145228038356081
plot_id,batch_id 0 54 miss% 0.022628415474888763
plot_id,batch_id 0 55 miss% 0.07752447147144577
plot_id,batch_id 0 56 miss% 0.09274676037708246
plot_id,batch_id 0 57 miss% 0.03816708036698867
plot_id,batch_id 0 58 miss% 0.016347761435862505
plot_id,batch_id 0 59 miss% 0.02944467849684811
plot_id,batch_id 0 60 miss% 0.03154640119789064
plot_id,batch_id 0 61 miss% 0.05112636740882924
plot_id,batch_id 0 62 miss% 0.08500909515154316
plot_id,batch_id 0 63 miss% 0.07049591825725356
plot_id,batch_id 0 64 miss% 0.03961094975159666
plot_id,batch_id 0 65 miss% 0.06522981663941321
plot_id,batch_id 0 66 miss% 0.16554256903562958
plot_id,batch_id 0 67 miss% 0.03192712781083422
plot_id,batch_id 0 68 miss% 0.06458888697648268
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  209681
Epoch:0, Train loss:0.615114, valid loss:0.611850
Epoch:1, Train loss:0.050273, valid loss:0.006683
Epoch:2, Train loss:0.013719, valid loss:0.004060
Epoch:3, Train loss:0.009298, valid loss:0.003006
Epoch:4, Train loss:0.007397, valid loss:0.002618
Epoch:5, Train loss:0.006775, valid loss:0.002460
Epoch:6, Train loss:0.005443, valid loss:0.002204
Epoch:7, Train loss:0.004282, valid loss:0.001853
Epoch:8, Train loss:0.004289, valid loss:0.001799
Epoch:9, Train loss:0.003733, valid loss:0.001932
Epoch:10, Train loss:0.003748, valid loss:0.001762
Epoch:11, Train loss:0.002656, valid loss:0.001406
Epoch:12, Train loss:0.002438, valid loss:0.001235
Epoch:13, Train loss:0.002368, valid loss:0.001370
Epoch:14, Train loss:0.002244, valid loss:0.001286
Epoch:15, Train loss:0.002238, valid loss:0.001329
Epoch:16, Train loss:0.002072, valid loss:0.001356
Epoch:17, Train loss:0.002131, valid loss:0.001173
Epoch:18, Train loss:0.002135, valid loss:0.001171
Epoch:19, Train loss:0.001974, valid loss:0.001269
Epoch:20, Train loss:0.001868, valid loss:0.001300
Epoch:21, Train loss:0.001384, valid loss:0.001061
Epoch:22, Train loss:0.001288, valid loss:0.001011
Epoch:23, Train loss:0.001279, valid loss:0.001001
Epoch:24, Train loss:0.001218, valid loss:0.000909
Epoch:25, Train loss:0.001250, valid loss:0.000936
Epoch:26, Train loss:0.001159, valid loss:0.000951
Epoch:27, Train loss:0.001159, valid loss:0.000985
Epoch:28, Train loss:0.001121, valid loss:0.001030
Epoch:29, Train loss:0.001095, valid loss:0.000865
Epoch:30, Train loss:0.001094, valid loss:0.000965
Epoch:31, Train loss:0.000845, valid loss:0.000849
Epoch:32, Train loss:0.000784, valid loss:0.000856
Epoch:33, Train loss:0.000764, valid loss:0.000845
Epoch:34, Train loss:0.000785, valid loss:0.000824
Epoch:35, Train loss:0.000745, valid loss:0.000900
Epoch:36, Train loss:0.000740, valid loss:0.000817
Epoch:37, Train loss:0.000743, valid loss:0.000849
Epoch:38, Train loss:0.000741, valid loss:0.000882
Epoch:39, Train loss:0.000679, valid loss:0.000850
Epoch:40, Train loss:0.000693, valid loss:0.000896
Epoch:41, Train loss:0.000588, valid loss:0.000790
Epoch:42, Train loss:0.000562, valid loss:0.000823
Epoch:43, Train loss:0.000554, valid loss:0.000831
Epoch:44, Train loss:0.000568, valid loss:0.000848
Epoch:45, Train loss:0.000548, valid loss:0.000779
Epoch:46, Train loss:0.000548, valid loss:0.000818
Epoch:47, Train loss:0.000531, valid loss:0.000845
Epoch:48, Train loss:0.000546, valid loss:0.000821
Epoch:49, Train loss:0.000522, valid loss:0.000808
Epoch:50, Train loss:0.000517, valid loss:0.000791
Epoch:51, Train loss:0.000478, valid loss:0.000790
Epoch:52, Train loss:0.000473, valid loss:0.000790
Epoch:53, Train loss:0.000471, valid loss:0.000787
Epoch:54, Train loss:0.000469, valid loss:0.000784
Epoch:55, Train loss:0.000468, valid loss:0.000790
Epoch:56, Train loss:0.000467, valid loss:0.000786
Epoch:57, Train loss:0.000466, valid loss:0.000786
Epoch:58, Train loss:0.000466, valid loss:0.000787
Epoch:59, Train loss:0.000465, valid loss:0.000788
Epoch:60, Train loss:0.000464, valid loss:0.000787
training time 16066.8176279068
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.06895294080325728
plot_id,batch_id 0 1 miss% 0.09464589801328827
plot_id,batch_id 0 2 miss% 0.12157637234373161
plot_id,batch_id 0 3 miss% 0.085571570959734
plot_id,batch_id 0 4 miss% 0.05273847546859422
plot_id,batch_id 0 5 miss% 0.03523528292198641
plot_id,batch_id 0 6 miss% 0.05182326449754819
plot_id,batch_id 0 7 miss% 0.12335730387796148
plot_id,batch_id 0 8 miss% 0.12965126613694689
plot_id,batch_id 0 9 miss% 0.061419350541660395
plot_id,batch_id 0 10 miss% 0.036176928305062306
plot_id,batch_id 0 11 miss% 0.04976700469222204
plot_id,batch_id 0 12 miss% 0.09823386506962345
plot_id,batch_id 0 13 miss% 0.08608507669932614
plot_id,batch_id 0 14 miss% 0.09865076763648054
plot_id,batch_id 0 15 miss% 0.04214218043408467
plot_id,batch_id 0 16 miss% 0.07190341455124452
plot_id,batch_id 0 17 miss% 0.05002312339746668
plot_id,batch_id 0 18 miss% 0.10288376090886707
plot_id,batch_id 0 19 miss% 0.07771254672578007
plot_id,batch_id 0 20 miss% 0.07314983483245217
plot_id,batch_id 0 21 miss% 0.041795392088694074
plot_id,batch_id 0 22 miss% 0.050811193878151695
plot_id,batch_id 0 23 miss% 0.05457300850149046
plot_id,batch_id 0 24 miss% 0.04327737884489347
plot_id,batch_id 0 25 miss% 0.05929310543274075
plot_id,batch_id 0 26 miss% 0.04774030527817644
plot_id,batch_id 0 27 miss% 0.053919037227557934
plot_id,batch_id 0 28 miss% 0.058093608588197454
plot_id,batch_id 0 29 miss% 0.04967149348314466
plot_id,batch_id 0 30 miss% 0.06078749208920186
plot_id,batch_id 0 31 miss% 0.10955914773709413
plot_id,batch_id 0 32 miss% 0.09010308089428529
plot_id,batch_id 0 33 miss% 0.09419034154441812
plot_id,batch_id 0 34 miss% 0.07154124770796963
plot_id,batch_id 0 35 miss% 0.04186690099002198
plot_id,batch_id 0 36 miss% 0.06472372964442051
plot_id,batch_id 0 37 miss% 0.05622789440125647
plot_id,batch_id 0 38 miss% 0.07820744884342716
plot_id,batch_id 0 39 miss% 0.06829781426064116
plot_id,batch_id 0 40 miss% 0.061698004788891085
plot_id,batch_id 0 41 miss% 0.06744631279575967
plot_id,batch_id 0 42 miss% 0.06624457880033734
plot_id,batch_id 0 43 miss% 0.0246535405672392
plot_id,batch_id 0 44 miss% 0.03096782366352333
plot_id,batch_id 0 45 miss% 0.03002506932119954
plot_id,batch_id 0 46 miss% 0.052943713511313294
plot_id,batch_id 0 47 miss% 0.041691402960311395
plot_id,batch_id 0 48 miss% 0.04659380859796409
plot_id,batch_id 0 49 miss% 0.050380897259579234
plot_id,batch_id 0 50 miss% 0.09985504991371297
plot_id,batch_id 0 51 miss% 0.04613145649059183
plot_id,batch_id 0 52 miss% 0.049141158334522594
plot_id,batch_id 0 53 miss% 0.037258998855874005
plot_id,batch_id 0 54 miss% 0.023488786812019866
plot_id,batch_id 0 55 miss% 0.06757029646354892
plot_id,batch_id 0 56 miss% 0.07108596085524424
plot_id,batch_id 0 57 miss% 0.07475326458426101
plot_id,batch_id 0 58 miss% 0.045252321391228006
plot_id,batch_id 0 59 miss% 0.04892066444532098
plot_id,batch_id 0 60 miss% 0.02489245939283807
plot_id,batch_id 0 61 miss% 0.03251617800143819
plot_id,batch_id 0 62 miss% 0.07436232863932046
plot_id,batch_id 0 63 miss% 0.053933405945449486
plot_id,batch_id 0 64 miss% 0.06735002886658341
plot_id,batch_id 0 65 miss% 0.04558247852683819
plot_id,batch_id 0 66 miss% 0.04679308543212587
plot_id,batch_id 0 67 miss% 0.01705032223909027
plot_id,batch_id 0 68 miss% 0.05021402918892769
plot_id,batch_id 0 69 miss% 0.07770777582250527
plot_id,batch_id 0 70 miss% 0.06689551254931855
plot_id,batch_id 0 71 miss% 0.041839374118046176
plot_id,batch_id 0 72 miss% 0.07330342778141861
plot_id,batch_id 0 73 miss% 0.03179992961000905
plot_id,batch_id 0 74 miss% 0.07517617892902469
plot_id,batch_id 0 75 miss% 0.08528887912280242
plot_id,batch_id 0 76 miss% 0.05451326142870387
plot_id,batch_id 0 77 miss% 0.08861833286785555
plot_id,batch_id 0 78 miss% 0.04999515792562727
plot_id,batch_id 0 79 miss% 0.05335747504446713
plot_id,batch_id 0 80 miss% 0.03523286453338501
plot_id,batch_id 0 81 miss% 0.08033193440392099
plot_id,batch_id 0 82 miss% 0.05253732616375666
plot_id,batch_id 0 83 miss% 0.07197247788076222
plot_id,batch_id 0 84 miss% 0.05774520516111608
plot_id,batch_id 0 85 miss% 0.05540754534233694
plot_id,batch_id 0 86 miss% 0.05850059003432088
plot_id,batch_id 0 87 miss% 0.0680963900956686
plot_id,batch_id 0 88 miss% 0.07549897081292402
plot_id,batch_id 0 89 miss% 0.08598658095302843
plot_id,batch_id 0 90 miss% 0.026402581982260868
plot_id,batch_id 0 91 miss% 0.04121203787180953
plot_id,batch_id 0 92 miss% 0.05443196320016509
plot_id,batch_id 0 93 miss% 0.06697819649849375
plot_id,batch_id 0 94 miss% 0.07055254248474671
plot_id,batch_id 0 95 miss% 0.048471019937613144
plot_id,batch_id 0 96 miss% 0.080118327439547
plot_id,batch_id 0 97 miss% 0.06902086935584045
plot_id,batch_id 0 98 miss% 0.04024508206181796
plot_id,batch_id 0 99 miss% 0.07682382512090048
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.11282735 0.0557999  0.08701797 0.03845971 0.0196479  0.04469577
 0.04082    0.08548262 0.05587249 0.0876141  0.04613247 0.03847629
 0.05459215 0.06758175 0.10031357 0.04530785 0.05989669 0.03605139
 0.08134758 0.08265583 0.18125631 0.02113578 0.04633501 0.03058995
 0.03689922 0.05258107 0.0504569  0.02897371 0.02616818 0.03089825
 0.02409954 0.06082416 0.09771624 0.03173003 0.02932249 0.04994504
 0.07309879 0.05182641 0.03842725 0.04316864 0.08172197 0.04271594
 0.01703794 0.04588883 0.02930216 0.05531013 0.03140129 0.01869848
 0.01951621 0.0393868  0.14647096 0.03309214 0.02371227 0.02145228
 0.02262842 0.07752447 0.09274676 0.03816708 0.01634776 0.02944468
 0.0315464  0.05112637 0.0850091  0.07049592 0.03961095 0.06522982
 0.16554257 0.03192713 0.06458889 0.07770778 0.06689551 0.04183937
 0.07330343 0.03179993 0.07517618 0.08528888 0.05451326 0.08861833
 0.04999516 0.05335748 0.03523286 0.08033193 0.05253733 0.07197248
 0.05774521 0.05540755 0.05850059 0.06809639 0.07549897 0.08598658
 0.02640258 0.04121204 0.05443196 0.0669782  0.07055254 0.04847102
 0.08011833 0.06902087 0.04024508 0.07682383]
for model  122 the mean error 0.056477517086917006
all id 122 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 25 out win 6 err 0.056477517086917006 time 16026.08884882927
Launcher: Job 123 completed in 16289 seconds.
Launcher: Task 92 done. Exiting.
plot_id,batch_id 0 68 miss% 0.08071131292875085
plot_id,batch_id 0 69 miss% 0.05651520467321016
plot_id,batch_id 0 70 miss% 0.11300927700030908
plot_id,batch_id 0 71 miss% 0.027439530009811374
plot_id,batch_id 0 72 miss% 0.08647958940856938
plot_id,batch_id 0 73 miss% 0.04405333037620283
plot_id,batch_id 0 74 miss% 0.06916255654986067
plot_id,batch_id 0 75 miss% 0.041351433711548514
plot_id,batch_id 0 76 miss% 0.06272151227417448
plot_id,batch_id 0 77 miss% 0.03268025442655064
plot_id,batch_id 0 78 miss% 0.06450844616528001
plot_id,batch_id 0 79 miss% 0.05599813617705495
plot_id,batch_id 0 80 miss% 0.05911778651925163
plot_id,batch_id 0 81 miss% 0.07488937939614732
plot_id,batch_id 0 82 miss% 0.04433267510546139
plot_id,batch_id 0 83 miss% 0.06536899453106468
plot_id,batch_id 0 84 miss% 0.04514192029213351
plot_id,batch_id 0 85 miss% 0.03862792132732103
plot_id,batch_id 0 86 miss% 0.060637572246720194
plot_id,batch_id 0 87 miss% 0.07402319309206395
plot_id,batch_id 0 88 miss% 0.0428452777116753
plot_id,batch_id 0 89 miss% 0.07924472046331625
plot_id,batch_id 0 90 miss% 0.04596786784589072
plot_id,batch_id 0 91 miss% 0.05571733333481385
plot_id,batch_id 0 92 miss% 0.04058945292727298
plot_id,batch_id 0 93 miss% 0.04638560843989537
plot_id,batch_id 0 94 miss% 0.06104740744375539
plot_id,batch_id 0 95 miss% 0.017730739176921712
plot_id,batch_id 0 96 miss% 0.04882250474999649
plot_id,batch_id 0 97 miss% 0.03979454370719553
plot_id,batch_id 0 98 miss% 0.027043339105507776
plot_id,batch_id 0 99 miss% 0.11632707202285063
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07344612 0.06451103 0.10532351 0.05760397 0.08470526 0.05008808
 0.03849942 0.08875991 0.07569554 0.0526546  0.03228114 0.07879439
 0.07515468 0.03790059 0.05831589 0.06218124 0.14707226 0.02961406
 0.04818654 0.08478497 0.13457255 0.04125239 0.03675335 0.05988617
 0.03908357 0.10226232 0.07377649 0.0400265  0.02860383 0.02962686
 0.03790209 0.07093409 0.0597128  0.05737193 0.04266196 0.03873092
 0.09022638 0.04799224 0.05632396 0.03061905 0.08175226 0.04137326
 0.02415809 0.06098274 0.03372086 0.03043855 0.03352125 0.03673892
 0.03787726 0.03685675 0.11773572 0.07994809 0.04934118 0.01383669
 0.09211834 0.11682039 0.08631739 0.06424822 0.03472629 0.04275693
 0.03437129 0.0218674  0.06492036 0.04668943 0.05178862 0.06720103
 0.16036882 0.03622776 0.08071131 0.0565152  0.11300928 0.02743953
 0.08647959 0.04405333 0.06916256 0.04135143 0.06272151 0.03268025
 0.06450845 0.05599814 0.05911779 0.07488938 0.04433268 0.06536899
 0.04514192 0.03862792 0.06063757 0.07402319 0.04284528 0.07924472
 0.04596787 0.05571733 0.04058945 0.04638561 0.06104741 0.01773074
 0.0488225  0.03979454 0.02704334 0.11632707]
for model  142 the mean error 0.05880882439714288
all id 142 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 25 out win 5 err 0.05880882439714288 time 16027.346613407135
Launcher: Job 143 completed in 16289 seconds.
Launcher: Task 37 done. Exiting.
plot_id,batch_id 0 69 miss% 0.053930616606712924
plot_id,batch_id 0 70 miss% 0.062362000713228734
plot_id,batch_id 0 71 miss% 0.04476215196860137
plot_id,batch_id 0 72 miss% 0.06170532568550696
plot_id,batch_id 0 73 miss% 0.058837149741063445
plot_id,batch_id 0 74 miss% 0.07462344622059958
plot_id,batch_id 0 75 miss% 0.03131517764520682
plot_id,batch_id 0 76 miss% 0.0861336158374186
plot_id,batch_id 0 77 miss% 0.038088663278362245
plot_id,batch_id 0 78 miss% 0.038781097745011685
plot_id,batch_id 0 79 miss% 0.0502818557788079
plot_id,batch_id 0 80 miss% 0.05662612478295255
plot_id,batch_id 0 81 miss% 0.052085825441723486
plot_id,batch_id 0 82 miss% 0.03559093267810236
plot_id,batch_id 0 83 miss% 0.11264031352695424
plot_id,batch_id 0 84 miss% 0.0664703261112469
plot_id,batch_id 0 85 miss% 0.03336086418016444
plot_id,batch_id 0 86 miss% 0.04050803414057992
plot_id,batch_id 0 87 miss% 0.06026553099894997
plot_id,batch_id 0 88 miss% 0.08512615894949094
plot_id,batch_id 0 89 miss% 0.1261376875429637
plot_id,batch_id 0 90 miss% 0.038978162516567126
plot_id,batch_id 0 91 miss% 0.0362200065987514
plot_id,batch_id 0 92 miss% 0.050702521334019986
plot_id,batch_id 0 93 miss% 0.050121369391417744
plot_id,batch_id 0 94 miss% 0.07998850459265805
plot_id,batch_id 0 95 miss% 0.06551127052516698
plot_id,batch_id 0 96 miss% 0.05528719825046626
plot_id,batch_id 0 97 miss% 0.03345557252939799
plot_id,batch_id 0 98 miss% 0.056140853630672526
plot_id,batch_id 0 99 miss% 0.12811393852279576
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06895294 0.0946459  0.12157637 0.08557157 0.05273848 0.03523528
 0.05182326 0.1233573  0.12965127 0.06141935 0.03617693 0.049767
 0.09823387 0.08608508 0.09865077 0.04214218 0.07190341 0.05002312
 0.10288376 0.07771255 0.07314983 0.04179539 0.05081119 0.05457301
 0.04327738 0.05929311 0.04774031 0.05391904 0.05809361 0.04967149
 0.06078749 0.10955915 0.09010308 0.09419034 0.07154125 0.0418669
 0.06472373 0.05622789 0.07820745 0.06829781 0.061698   0.06744631
 0.06624458 0.02465354 0.03096782 0.03002507 0.05294371 0.0416914
 0.04659381 0.0503809  0.09985505 0.04613146 0.04914116 0.037259
 0.02348879 0.0675703  0.07108596 0.07475326 0.04525232 0.04892066
 0.02489246 0.03251618 0.07436233 0.05393341 0.06735003 0.04558248
 0.04679309 0.01705032 0.05021403 0.05393062 0.062362   0.04476215
 0.06170533 0.05883715 0.07462345 0.03131518 0.08613362 0.03808866
 0.0387811  0.05028186 0.05662612 0.05208583 0.03559093 0.11264031
 0.06647033 0.03336086 0.04050803 0.06026553 0.08512616 0.12613769
 0.03897816 0.03622001 0.05070252 0.05012137 0.0799885  0.06551127
 0.0552872  0.03345557 0.05614085 0.12811394]
for model  71 the mean error 0.06119329574363719
all id 71 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 21 out win 6 err 0.06119329574363719 time 16066.8176279068
Launcher: Job 72 completed in 16328 seconds.
Launcher: Task 21 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  55697
Epoch:0, Train loss:0.408340, valid loss:0.378156
Epoch:1, Train loss:0.066578, valid loss:0.003277
Epoch:2, Train loss:0.009222, valid loss:0.002786
Epoch:3, Train loss:0.008540, valid loss:0.002123
Epoch:4, Train loss:0.006812, valid loss:0.001616
Epoch:5, Train loss:0.005144, valid loss:0.001569
Epoch:6, Train loss:0.004109, valid loss:0.001141
Epoch:7, Train loss:0.003442, valid loss:0.001153
Epoch:8, Train loss:0.003275, valid loss:0.001076
Epoch:9, Train loss:0.002473, valid loss:0.000967
Epoch:10, Train loss:0.001640, valid loss:0.000962
Epoch:11, Train loss:0.001286, valid loss:0.000683
Epoch:12, Train loss:0.001235, valid loss:0.000629
Epoch:13, Train loss:0.001206, valid loss:0.000625
Epoch:14, Train loss:0.001159, valid loss:0.000629
Epoch:15, Train loss:0.001133, valid loss:0.000643
Epoch:16, Train loss:0.001087, valid loss:0.000633
Epoch:17, Train loss:0.001100, valid loss:0.000630
Epoch:18, Train loss:0.001036, valid loss:0.000607
Epoch:19, Train loss:0.001042, valid loss:0.000609
Epoch:20, Train loss:0.001029, valid loss:0.000696
Epoch:21, Train loss:0.000827, valid loss:0.000597
Epoch:22, Train loss:0.000799, valid loss:0.000546
Epoch:23, Train loss:0.000798, valid loss:0.000570
Epoch:24, Train loss:0.000795, valid loss:0.000556
Epoch:25, Train loss:0.000781, valid loss:0.000519
Epoch:26, Train loss:0.000765, valid loss:0.000556
Epoch:27, Train loss:0.000769, valid loss:0.000604
Epoch:28, Train loss:0.000752, valid loss:0.000555
Epoch:29, Train loss:0.000743, valid loss:0.000554
Epoch:30, Train loss:0.000732, valid loss:0.000569
Epoch:31, Train loss:0.000635, valid loss:0.000499
Epoch:32, Train loss:0.000626, valid loss:0.000501
Epoch:33, Train loss:0.000629, valid loss:0.000514
Epoch:34, Train loss:0.000621, valid loss:0.000517
Epoch:35, Train loss:0.000616, valid loss:0.000532
Epoch:36, Train loss:0.000613, valid loss:0.000562
Epoch:37, Train loss:0.000609, valid loss:0.000515
Epoch:38, Train loss:0.000596, valid loss:0.000483
Epoch:39, Train loss:0.000602, valid loss:0.000510
Epoch:40, Train loss:0.000598, valid loss:0.000561
Epoch:41, Train loss:0.000545, valid loss:0.000513
Epoch:42, Train loss:0.000542, valid loss:0.000499
Epoch:43, Train loss:0.000539, valid loss:0.000505
Epoch:44, Train loss:0.000539, valid loss:0.000506
Epoch:45, Train loss:0.000532, valid loss:0.000500
Epoch:46, Train loss:0.000533, valid loss:0.000506
Epoch:47, Train loss:0.000529, valid loss:0.000512
Epoch:48, Train loss:0.000525, valid loss:0.000521
Epoch:49, Train loss:0.000528, valid loss:0.000495
Epoch:50, Train loss:0.000524, valid loss:0.000497
Epoch:51, Train loss:0.000496, valid loss:0.000487
Epoch:52, Train loss:0.000492, valid loss:0.000487
Epoch:53, Train loss:0.000490, valid loss:0.000487
Epoch:54, Train loss:0.000489, valid loss:0.000490
Epoch:55, Train loss:0.000488, valid loss:0.000487
Epoch:56, Train loss:0.000488, valid loss:0.000487
Epoch:57, Train loss:0.000487, valid loss:0.000487
Epoch:58, Train loss:0.000486, valid loss:0.000488
Epoch:59, Train loss:0.000486, valid loss:0.000488
Epoch:60, Train loss:0.000486, valid loss:0.000488
training time 16128.783422708511
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07646432879175294
plot_id,batch_id 0 1 miss% 0.10002372285910274
plot_id,batch_id 0 2 miss% 0.1067998110454077
plot_id,batch_id 0 3 miss% 0.03531217490726218
plot_id,batch_id 0 4 miss% 0.03959363119152565
plot_id,batch_id 0 5 miss% 0.11818468706103877
plot_id,batch_id 0 6 miss% 0.04062889681301943
plot_id,batch_id 0 7 miss% 0.057844715943856824
plot_id,batch_id 0 8 miss% 0.06127380950338414
plot_id,batch_id 0 9 miss% 0.03160915972739418
plot_id,batch_id 0 10 miss% 0.03902498574771238
plot_id,batch_id 0 11 miss% 0.043947212671317494
plot_id,batch_id 0 12 miss% 0.0772590219054783
plot_id,batch_id 0 13 miss% 0.06001033818472028
plot_id,batch_id 0 14 miss% 0.09212226419064601
plot_id,batch_id 0 15 miss% 0.04500244254554358
plot_id,batch_id 0 16 miss% 0.09053834158988497
plot_id,batch_id 0 17 miss% 0.04970180602865371
plot_id,batch_id 0 18 miss% 0.07526749803370912
plot_id,batch_id 0 19 miss% 0.09796062776389731
plot_id,batch_id 0 20 miss% 0.1151053831092965
plot_id,batch_id 0 21 miss% 0.03815598934753643
plot_id,batch_id 0 22 miss% 0.04061498026142998
plot_id,batch_id 0 23 miss% 0.05803518356489417
plot_id,batch_id 0 24 miss% 0.05437375710258399
plot_id,batch_id 0 25 miss% 0.08817548534816601
plot_id,batch_id 0 26 miss% 0.07019971606166699
plot_id,batch_id 0 27 miss% 0.05592850052468385
plot_id,batch_id 0 28 miss% 0.036757439948780064
plot_id,batch_id 0 29 miss% 0.033252228551073515
plot_id,batch_id 0 30 miss% 0.04077877768770098
plot_id,batch_id 0 31 miss% 0.08398588726056941
plot_id,batch_id 0 32 miss% 0.10083473609763456
plot_id,batch_id 0 33 miss% 0.029738887702311062
plot_id,batch_id 0 34 miss% 0.037091644942196446
plot_id,batch_id 0 35 miss% 0.035983088851849795
plot_id,batch_id 0 36 miss% 0.1792548259992565
plot_id,batch_id 0 37 miss% 0.07219450664032549
plot_id,batch_id 0 38 miss% 0.03570111549391805
plot_id,batch_id 0 39 miss% 0.04762085313479288
plot_id,batch_id 0 40 miss% 0.09649045752486832
plot_id,batch_id 0 41 miss% 0.04111890391815205
plot_id,batch_id 0 42 miss% 0.030093864047377065
plot_id,batch_id 0 43 miss% 0.05739392518589322
plot_id,batch_id 0 44 miss% 0.041740048626851114
plot_id,batch_id 0 45 miss% 0.04568886826192479
plot_id,batch_id 0 46 miss% 0.04393575183947149
plot_id,batch_id 0 47 miss% 0.027403281740242365
plot_id,batch_id 0 48 miss% 0.04530400753528457
plot_id,batch_id 0 49 miss% 0.032635314417818986
plot_id,batch_id 0 50 miss% 0.13235499401701198
plot_id,batch_id 0 51 miss% 0.02629675414235262
plot_id,batch_id 0 52 miss% 0.02571939564515746
plot_id,batch_id 0 53 miss% 0.043274031355910104
plot_id,batch_id 0 54 miss% 0.051571675983060665
plot_id,batch_id 0 55 miss% 0.06546799926438071
plot_id,batch_id 0 56 miss% 0.08585304956925568
plot_id,batch_id 0 57 miss% 0.07101346403824509
plot_id,batch_id 0 58 miss% 0.03377369921337384
plot_id,batch_id 0 59 miss% 0.03275987688101742
plot_id,batch_id 0 60 miss% 0.03662512375713653
plot_id,batch_id 0 61 miss% 0.02485046014763711
plot_id,batch_id 0 62 miss% 0.06335139869131627
plot_id,batch_id 0 63 miss% 0.06700661157122985
plot_id,batch_id 0 64 miss% 0.07322170712950528
plot_id,batch_id 0 65 miss% 0.06168638832042098
plot_id,batch_id 0 66 miss% 0.017956466241559516
plot_id,batch_id 0 67 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  151697
Epoch:0, Train loss:0.500920, valid loss:0.461628
Epoch:1, Train loss:0.372935, valid loss:0.366390
Epoch:2, Train loss:0.362891, valid loss:0.365484
Epoch:3, Train loss:0.361951, valid loss:0.365262
Epoch:4, Train loss:0.361421, valid loss:0.365196
Epoch:5, Train loss:0.361111, valid loss:0.365002
Epoch:6, Train loss:0.360953, valid loss:0.364969
Epoch:7, Train loss:0.360659, valid loss:0.365201
Epoch:8, Train loss:0.360593, valid loss:0.364756
Epoch:9, Train loss:0.360666, valid loss:0.364716
Epoch:10, Train loss:0.360339, valid loss:0.364703
Epoch:11, Train loss:0.359779, valid loss:0.364506
Epoch:12, Train loss:0.359738, valid loss:0.364473
Epoch:13, Train loss:0.359638, valid loss:0.364493
Epoch:14, Train loss:0.359690, valid loss:0.364469
Epoch:15, Train loss:0.359605, valid loss:0.364473
Epoch:16, Train loss:0.359540, valid loss:0.364349
Epoch:17, Train loss:0.359547, valid loss:0.364487
Epoch:18, Train loss:0.359467, valid loss:0.364396
Epoch:19, Train loss:0.359450, valid loss:0.364434
Epoch:20, Train loss:0.359418, valid loss:0.364361
Epoch:21, Train loss:0.359128, valid loss:0.364353
Epoch:22, Train loss:0.359085, valid loss:0.364294
Epoch:23, Train loss:0.359094, valid loss:0.364299
Epoch:24, Train loss:0.359075, valid loss:0.364282
Epoch:25, Train loss:0.359052, valid loss:0.364275
Epoch:26, Train loss:0.359054, valid loss:0.364234
Epoch:27, Train loss:0.359023, valid loss:0.364300
Epoch:28, Train loss:0.359015, valid loss:0.364316
Epoch:29, Train loss:0.358995, valid loss:0.364290
Epoch:30, Train loss:0.358989, valid loss:0.364247
Epoch:31, Train loss:0.358855, valid loss:0.364178
Epoch:32, Train loss:0.358840, valid loss:0.364224
Epoch:33, Train loss:0.358848, valid loss:0.364202
Epoch:34, Train loss:0.358822, valid loss:0.364204
Epoch:35, Train loss:0.358834, valid loss:0.364197
Epoch:36, Train loss:0.358807, valid loss:0.364185
Epoch:37, Train loss:0.358809, valid loss:0.364185
Epoch:38, Train loss:0.358799, valid loss:0.364182
Epoch:39, Train loss:0.358788, valid loss:0.364182
Epoch:40, Train loss:0.358791, valid loss:0.364195
Epoch:41, Train loss:0.358742, valid loss:0.364167
Epoch:42, Train loss:0.358722, valid loss:0.364176
Epoch:43, Train loss:0.358716, valid loss:0.364183
Epoch:44, Train loss:0.358726, valid loss:0.364182
Epoch:45, Train loss:0.358714, valid loss:0.364164
Epoch:46, Train loss:0.358707, valid loss:0.364164
Epoch:47, Train loss:0.358706, valid loss:0.364154
Epoch:48, Train loss:0.358699, valid loss:0.364162
Epoch:49, Train loss:0.358700, valid loss:0.364187
Epoch:50, Train loss:0.358698, valid loss:0.364167
Epoch:51, Train loss:0.358670, valid loss:0.364164
Epoch:52, Train loss:0.358667, valid loss:0.364162
Epoch:53, Train loss:0.358666, valid loss:0.364159
Epoch:54, Train loss:0.358665, valid loss:0.364159
Epoch:55, Train loss:0.358664, valid loss:0.364158
Epoch:56, Train loss:0.358664, valid loss:0.364157
Epoch:57, Train loss:0.358663, valid loss:0.364158
Epoch:58, Train loss:0.358663, valid loss:0.364156
Epoch:59, Train loss:0.358662, valid loss:0.364159
Epoch:60, Train loss:0.358662, valid loss:0.364156
training time 16221.760625839233
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.8206422233941938
plot_id,batch_id 0 1 miss% 0.7851527312863095
plot_id,batch_id 0 2 miss% 0.7823726725770294
plot_id,batch_id 0 3 miss% 0.776281951100446
plot_id,batch_id 0 4 miss% 0.7816056317473572
plot_id,batch_id 0 5 miss% 0.8179658160531023
plot_id,batch_id 0 6 miss% 0.7865021332177715
plot_id,batch_id 0 7 miss% 0.7875751096306258
plot_id,batch_id 0 8 miss% 0.779365316134658
plot_id,batch_id 0 9 miss% 0.778671535945115
plot_id,batch_id 0 10 miss% 0.8190400179282742
plot_id,batch_id 0 11 miss% 0.7905132499341648
plot_id,batch_id 0 12 miss% 0.7818590653978041
plot_id,batch_id 0 13 miss% 0.7799166373677507
plot_id,batch_id 0 14 miss% 0.7829552912926375
plot_id,batch_id 0 15 miss% 0.8282507409380419
plot_id,batch_id 0 16 miss% 0.7827773713186125
plot_id,batch_id 0 17 miss% 0.7882888188248935
plot_id,batch_id 0 18 miss% 0.7873405132136283
plot_id,batch_id 0 19 miss% 0.7880061811839051
plot_id,batch_id 0 20 miss% 0.8093569157173485
plot_id,batch_id 0 21 miss% 0.7891159892340214
plot_id,batch_id 0 22 miss% 0.7785432776775983
plot_id,batch_id 0 23 miss% 0.778035018415604
plot_id,batch_id 0 24 miss% 0.7789261749241144
plot_id,batch_id 0 25 miss% 0.8059618428158141
plot_id,batch_id 0 26 miss% 0.7830570811893331
plot_id,batch_id 0 27 miss% 0.7796296529875573
plot_id,batch_id 0 28 miss% 0.7844933329321061
plot_id,batch_id 0 29 miss% 0.7772759574722463
plot_id,batch_id 0 30 miss% 0.8104907353578513
plot_id,batch_id 0 31 miss% 0.7883437416105387
plot_id,batch_id 0 32 miss% 0.7841568428823059
plot_id,batch_id 0 33 miss% 0.7806118786967926
plot_id,batch_id 0 34 miss% 0.7769607594120348
plot_id,batch_id 0 35 miss% 0.8087193351375533
plot_id,batch_id 0 36 miss% 0.7845535502843455
plot_id,batch_id 0 37 miss% 0.7811927861946536
plot_id,batch_id 0 38 miss% 0.7767968930951615
plot_id,batch_id 0 39 miss% 0.781157340609279
plot_id,batch_id 0 40 miss% 0.7917047957048676
plot_id,batch_id 0 41 miss% 0.7785219405930162
plot_id,batch_id 0 42 miss% 0.7766099026326875
plot_id,batch_id 0 43 miss% 0.7738495579637397
plot_id,batch_id 0 44 miss% 0.7740018310400443
plot_id,batch_id 0 45 miss% 0.7879625406632089
plot_id,batch_id 0 46 miss% 0.7814909330445879
plot_id,batch_id 0 47 miss% 0.7773734215181064
plot_id,batch_id 0 48 miss% 0.7954633333862884
plot_id,batch_id 0 49 miss% 0.811618377613364
plot_id,batch_id 0 50 miss% 0.790389696136721
plot_id,batch_id 0 51 miss% 0.7843728512853535
plot_id,batch_id 0 52 miss% 0.78201241925587
plot_id,batch_id 0 53 miss% 0.7787122102393019
plot_id,batch_id 0 54 miss% 0.7768041569024315
plot_id,batch_id 0 55 miss% 0.7879685101017287
plot_id,batch_id 0 56 miss% 0.7771524381316458
plot_id,batch_id 0 57 miss% 0.7819778538261231
plot_id,batch_id 0 58 miss% 0.7808447562935645
plot_id,batch_id 0 59 miss% 0.7734667103528485
plot_id,batch_id 0 60 miss% 0.8583949782998477
plot_id,batch_id 0 61 miss% 0.8068200031697118
plot_id,batch_id 0 62 miss% 0.790885480300048
plot_id,batch_id 0 63 miss% 0.7890662678400867
plot_id,batch_id 0 64 miss% 0.7938778264955484
plot_id,batch_id 0 65 miss% 0.8653684145692474
plot_id,batch_id 0 66 miss% 0.8136914302021454
plot_id,batch_id 0 67 miss% 0.8011145190708651
plot_id,batch_id 0 68 miss% 0.7864389591065106
plot_id,batch_id 0 69 miss% 0.7849932370246463
plot_id,batch_id 0 70 miss% 0.8469769143568412
plot_id,batch_id 0 71 miss% 0.8120207124677279
plot_id,batch_id 0 72 miss% 0.8047673044923742
plot_id,batch_id 0 73 miss% 0.797581416717582
plot_id,batch_id 0 74 miss% 0.7854568345704145
plot_id,batch_id 0 75 miss% 0.8427953005421611
plot_id,batch_id 0 76 miss% 0.8016120891076595
plot_id,batch_id 0 77 miss% 0.7968611430052995
plot_id,batch_id 0 78 miss% 0.7865063935090876
plot_id,batch_id 0 79 miss% 0.7862343262209233
plot_id,batch_id 0 80 miss% 0.8440363618329157
plot_id,batch_id 0 81 miss% 0.7917801106355352
plot_id,batch_id 0 82 miss% 0.7915121103652839
plot_id,batch_id 0 83 miss% 0.7811377264230385
plot_id,batch_id 0 84 miss% 0.7836702801053117
plot_id,batch_id 0 85 miss% 0.8334134956557105
plot_id,batch_id 0 86 miss% 0.7958819220241796
plot_id,batch_id 0 87 miss% 0.7903908634368222
plot_id,batch_id 0 88 miss% 0.7856034965119897
plot_id,batch_id 0 89 miss% 0.7849013559685928
plot_id,batch_id 0 90 miss% 0.8542132529898939
plot_id,batch_id 0 91 miss% 0.7968456498049439
plot_id,batch_id 0 92 miss% 0.7912284323308936
plot_id,batch_id 0 93 miss% 0.7872548172789645
plot_id,batch_id 0 94 miss% 0.7852849565655617
plot_id,batch_id 0 95 miss% 0.8636365295930735
plot_id,batch_id 0 96 miss% 0.7985135443890768
plot_id,batch_id 0 97 miss% 0.7867999026298607
plot_id,batch_id 0 98 miss% 0.780688485815385
plot_id,batch_id 0 99 miss% 0.7833573398075858
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.82064222 0.78515273 0.78237267 0.77628195 0.78160563 0.81796582
 0.78650213 0.78757511 0.77936532 0.77867154 0.81904002 0.79051325
 0.78185907 0.77991664 0.78295529 0.82825074 0.78277737 0.78828882
 0.78734051 0.78800618 0.80935692 0.78911599 0.77854328 0.77803502
 0.77892617 0.80596184 0.78305708 0.77962965 0.78449333 0.77727596
 0.81049074 0.78834374 0.78415684 0.78061188 0.77696076 0.80871934
 0.78455355 0.78119279 0.77679689 0.78115734 0.7917048  0.77852194
 0.7766099  0.77384956 0.77400183 0.78796254 0.78149093 0.77737342
 0.79546333 0.81161838 0.7903897  0.78437285 0.78201242 0.77871221
 0.77680416 0.78796851 0.77715244 0.78197785 0.78084476 0.77346671
 0.85839498 0.80682    0.79088548 0.78906627 0.79387783 0.86536841
 0.81369143 0.80111452 0.78643896 0.78499324 0.84697691 0.81202071
 0.8047673  0.79758142 0.78545683 0.8427953  0.80161209 0.79686114
 0.78650639 0.78623433 0.84403636 0.79178011 0.79151211 0.78113773
 0.78367028 0.8334135  0.79588192 0.79039086 0.7856035  0.78490136
 0.85421325 0.79684565 0.79122843 0.78725482 0.78528496 0.86363653
 0.79851354 0.7867999  0.78068849 0.78335734]
for model  158 the mean error 0.7938637453705144
all id 158 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 25 out win 6 err 0.7938637453705144 time 16221.760625839233
Launcher: Job 159 completed in 16358 seconds.
Launcher: Task 152 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  120401
Epoch:0, Train loss:0.567214, valid loss:0.551781
Epoch:1, Train loss:0.119272, valid loss:0.004695
Epoch:2, Train loss:0.012177, valid loss:0.003555
Epoch:3, Train loss:0.007464, valid loss:0.002529
Epoch:4, Train loss:0.004803, valid loss:0.002255
Epoch:5, Train loss:0.003775, valid loss:0.001663
Epoch:6, Train loss:0.003445, valid loss:0.001795
Epoch:7, Train loss:0.003272, valid loss:0.001779
Epoch:8, Train loss:0.002922, valid loss:0.001580
Epoch:9, Train loss:0.002811, valid loss:0.001248
Epoch:10, Train loss:0.002617, valid loss:0.001401
Epoch:11, Train loss:0.001904, valid loss:0.000957
Epoch:12, Train loss:0.001829, valid loss:0.001142
Epoch:13, Train loss:0.001781, valid loss:0.000985
Epoch:14, Train loss:0.001730, valid loss:0.000990
Epoch:15, Train loss:0.001661, valid loss:0.000941
Epoch:16, Train loss:0.001639, valid loss:0.000965
Epoch:17, Train loss:0.001560, valid loss:0.000857
Epoch:18, Train loss:0.001607, valid loss:0.001107
Epoch:19, Train loss:0.001507, valid loss:0.001086
Epoch:20, Train loss:0.001466, valid loss:0.000915
Epoch:21, Train loss:0.001111, valid loss:0.000720
Epoch:22, Train loss:0.001045, valid loss:0.000802
Epoch:23, Train loss:0.001052, valid loss:0.000815
Epoch:24, Train loss:0.001043, valid loss:0.000814
Epoch:25, Train loss:0.001019, valid loss:0.000869
Epoch:26, Train loss:0.000992, valid loss:0.000784
Epoch:27, Train loss:0.001008, valid loss:0.000752
Epoch:28, Train loss:0.000977, valid loss:0.000741
Epoch:29, Train loss:0.000982, valid loss:0.000719
Epoch:30, Train loss:0.000933, valid loss:0.000911
Epoch:31, Train loss:0.000756, valid loss:0.000714
Epoch:32, Train loss:0.000710, valid loss:0.000684
Epoch:33, Train loss:0.000738, valid loss:0.000695
Epoch:34, Train loss:0.000700, valid loss:0.000701
Epoch:35, Train loss:0.000709, valid loss:0.000669
Epoch:36, Train loss:0.000710, valid loss:0.000732
Epoch:37, Train loss:0.000697, valid loss:0.000666
Epoch:38, Train loss:0.000683, valid loss:0.000643
Epoch:39, Train loss:0.000668, valid loss:0.000638
Epoch:40, Train loss:0.000660, valid loss:0.000663
Epoch:41, Train loss:0.000570, valid loss:0.000621
Epoch:42, Train loss:0.000569, valid loss:0.000623
Epoch:43, Train loss:0.000558, valid loss:0.000615
Epoch:44, Train loss:0.000559, valid loss:0.000634
Epoch:45, Train loss:0.000562, valid loss:0.000628
Epoch:46, Train loss:0.000553, valid loss:0.000617
Epoch:47, Train loss:0.000543, valid loss:0.000653
Epoch:48, Train loss:0.000541, valid loss:0.000610
Epoch:49, Train loss:0.000537, valid loss:0.000604
Epoch:50, Train loss:0.000542, valid loss:0.000636
Epoch:51, Train loss:0.000498, valid loss:0.000613
Epoch:52, Train loss:0.000491, valid loss:0.000614
Epoch:53, Train loss:0.000488, valid loss:0.000615
Epoch:54, Train loss:0.000486, valid loss:0.000612
Epoch:55, Train loss:0.000485, valid loss:0.000614
Epoch:56, Train loss:0.000484, valid loss:0.000615
Epoch:57, Train loss:0.000484, valid loss:0.000614
Epoch:58, Train loss:0.000483, valid loss:0.000612
Epoch:59, Train loss:0.000482, valid loss:0.000612
Epoch:60, Train loss:0.000482, valid loss:0.000613
training time 16178.787504434586
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.35729284520637516
plot_id,batch_id 0 1 miss% 0.3935194526018358
plot_id,batch_id 0 2 miss% 0.4492440044415804
plot_id,batch_id 0 3 miss% 0.3212871152831844
plot_id,batch_id 0 4 miss% 0.3340759734336893
plot_id,batch_id 0 5 miss% 0.4107836716625252
plot_id,batch_id 0 6 miss% 0.39652421933097576
plot_id,batch_id 0 7 miss% 0.448327026807245
plot_id,batch_id 0 8 miss% 0.5682246080786865
plot_id,batch_id 0 9 miss% 0.4145956866570936
plot_id,batch_id 0 10 miss% 0.3687750522766558
plot_id,batch_id 0 11 miss% 0.37694601917897735
plot_id,batch_id 0 12 miss% 0.5066027762824894
plot_id,batch_id 0 13 miss% 0.4182000780808783
plot_id,batch_id 0 14 miss% 0.41276237293277257
plot_id,batch_id 0 15 miss% 0.3980938759971053
plot_id,batch_id 0 16 miss% 0.4441007215179756
plot_id,batch_id 0 17 miss% 0.572556189540928
plot_id,batch_id 0 18 miss% 0.4717452150592793
plot_id,batch_id 0 19 miss% 0.44851047137804195
plot_id,batch_id 0 20 miss% 0.3898739554601516
plot_id,batch_id 0 21 miss% 0.3760928357565869
plot_id,batch_id 0 22 miss% 0.33201697997343077
plot_id,batch_id 0 23 miss% 0.3363322937304746
plot_id,batch_id 0 24 miss% 0.3433981695056616
plot_id,batch_id 0 25 miss% 0.377075339142831
plot_id,batch_id 0 26 miss% 0.3734059332665822
plot_id,batch_id 0 27 miss% 0.35974927832842646
plot_id,batch_id 0 28 miss% 0.41309494182269235
plot_id,batch_id 0 29 miss% 0.40513736424728164
plot_id,batch_id 0 30 miss% 0.33107251461346077
plot_id,batch_id 0 31 miss% 0.5404894463540904
plot_id,batch_id 0 32 miss% 0.4329327525481714
plot_id,batch_id 0 33 miss% 0.3796833748451797
plot_id,batch_id 0 34 miss% 0.3737559124654387
plot_id,batch_id 0 35 miss% 0.37282098285998805
plot_id,batch_id 0 36 miss% 0.6182690193365448
plot_id,batch_id 0 37 miss% 0.4137925550091269
plot_id,batch_id 0 38 miss% 0.39337108512633384
plot_id,batch_id 0 39 miss% 0.36224214106242797
plot_id,batch_id 0 40 miss% 0.30466025497284926
plot_id,batch_id 0 41 miss% 0.357308974421305
plot_id,batch_id 0 42 miss% 0.3280337614542542
plot_id,batch_id 0 43 miss% 0.3574568813331736
plot_id,batch_id 0 44 miss% 0.26988038021488886
plot_id,batch_id 0 45 miss% 0.35072726719055586
plot_id,batch_id 0 46 miss% 0.3261168038861301
plot_id,batch_id 0 47 miss% 0.3567515639910774
plot_id,batch_id 0 48 miss% 0.37498292736530486
plot_id,batch_id 0 49 miss% 0.37780603443762767
plot_id,batch_id 0 50 miss% 0.48228492381785515
plot_id,batch_id 0 51 miss% 0.4470784549896552
plot_id,batch_id 0 52 miss% 0.4327522058508716
plot_id,batch_id 0 53 miss% 0.33630029871255035
plot_id,batch_id 0 54 miss% 0.43113856481937385
plot_id,batch_id 0 55 miss% 0.44958500253196226
plot_id,batch_id 0 56 miss% 0.5297865700490153
plot_id,batch_id 0 57 miss% 0.42664628396812276
plot_id,batch_id 0 58 miss% 0.3933191626194349
plot_id,batch_id 0 59 miss% 0.39903614487200917
plot_id,batch_id 0 60 miss% 0.26851810864448267
plot_id,batch_id 0 61 miss% 0.3414753929093313
plot_id,batch_id 0 62 miss% 0.387143674757165
plot_id,batch_id 0 63 miss% 0.36333449313910415
plot_id,batch_id 0 64 miss% 0.35520833963978404
plot_id,batch_id 0 65 miss% 0.4183709174420611
plot_id,batch_id 0 66 miss% 0.447093719701292
plot_id,batch_id 0 67 miss% 0.30475896393591
plot_id,batch_id 0 68 miss% 0.4816771988578153
plot_id,batch_id 0 69 miss% 0.3820852396337754
plot_id,batch_id 0 70the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  120401
Epoch:0, Train loss:0.590279, valid loss:0.579846
Epoch:1, Train loss:0.128735, valid loss:0.004439
Epoch:2, Train loss:0.010837, valid loss:0.003914
Epoch:3, Train loss:0.005526, valid loss:0.001945
Epoch:4, Train loss:0.004007, valid loss:0.001793
Epoch:5, Train loss:0.003594, valid loss:0.001688
Epoch:6, Train loss:0.003256, valid loss:0.001494
Epoch:7, Train loss:0.003064, valid loss:0.001783
Epoch:8, Train loss:0.002884, valid loss:0.001241
Epoch:9, Train loss:0.002635, valid loss:0.001373
Epoch:10, Train loss:0.002556, valid loss:0.001350
Epoch:11, Train loss:0.001883, valid loss:0.000987
Epoch:12, Train loss:0.001734, valid loss:0.001101
Epoch:13, Train loss:0.001746, valid loss:0.000936
Epoch:14, Train loss:0.001688, valid loss:0.000864
Epoch:15, Train loss:0.001571, valid loss:0.000911
Epoch:16, Train loss:0.001609, valid loss:0.000949
Epoch:17, Train loss:0.001527, valid loss:0.000871
Epoch:18, Train loss:0.001495, valid loss:0.000802
Epoch:19, Train loss:0.001385, valid loss:0.000828
Epoch:20, Train loss:0.001366, valid loss:0.000764
Epoch:21, Train loss:0.001032, valid loss:0.000682
Epoch:22, Train loss:0.001003, valid loss:0.000776
Epoch:23, Train loss:0.000972, valid loss:0.000760
Epoch:24, Train loss:0.000969, valid loss:0.000762
Epoch:25, Train loss:0.000959, valid loss:0.000694
Epoch:26, Train loss:0.000910, valid loss:0.000671
Epoch:27, Train loss:0.000901, valid loss:0.000703
Epoch:28, Train loss:0.000906, valid loss:0.000691
Epoch:29, Train loss:0.000878, valid loss:0.000712
Epoch:30, Train loss:0.000876, valid loss:0.000745
Epoch:31, Train loss:0.000699, valid loss:0.000622
Epoch:32, Train loss:0.000673, valid loss:0.000647
Epoch:33, Train loss:0.000676, valid loss:0.000614
Epoch:34, Train loss:0.000667, valid loss:0.000634
Epoch:35, Train loss:0.000671, valid loss:0.000616
Epoch:36, Train loss:0.000650, valid loss:0.000628
Epoch:37, Train loss:0.000650, valid loss:0.000628
Epoch:38, Train loss:0.000637, valid loss:0.000663
Epoch:39, Train loss:0.000631, valid loss:0.000602
Epoch:40, Train loss:0.000636, valid loss:0.000637
Epoch:41, Train loss:0.000553, valid loss:0.000602
Epoch:42, Train loss:0.000539, valid loss:0.000602
Epoch:43, Train loss:0.000540, valid loss:0.000597
Epoch:44, Train loss:0.000536, valid loss:0.000604
Epoch:45, Train loss:0.000538, valid loss:0.000608
Epoch:46, Train loss:0.000530, valid loss:0.000619
Epoch:47, Train loss:0.000521, valid loss:0.000599
Epoch:48, Train loss:0.000521, valid loss:0.000574
Epoch:49, Train loss:0.000522, valid loss:0.000593
Epoch:50, Train loss:0.000509, valid loss:0.000599
Epoch:51, Train loss:0.000485, valid loss:0.000594
Epoch:52, Train loss:0.000481, valid loss:0.000591
Epoch:53, Train loss:0.000479, valid loss:0.000591
Epoch:54, Train loss:0.000478, valid loss:0.000589
Epoch:55, Train loss:0.000477, valid loss:0.000591
Epoch:56, Train loss:0.000476, valid loss:0.000586
Epoch:57, Train loss:0.000475, valid loss:0.000588
Epoch:58, Train loss:0.000475, valid loss:0.000588
Epoch:59, Train loss:0.000475, valid loss:0.000585
Epoch:60, Train loss:0.000474, valid loss:0.000586
training time 16182.791972398758
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.3322915445056776
plot_id,batch_id 0 1 miss% 0.4049427299768546
plot_id,batch_id 0 2 miss% 0.36800693754671326
plot_id,batch_id 0 3 miss% 0.3260861334690309
plot_id,batch_id 0 4 miss% 0.2785177985879495
plot_id,batch_id 0 5 miss% 0.36237983061974777
plot_id,batch_id 0 6 miss% 0.3459460919554932
plot_id,batch_id 0 7 miss% 0.5314594008089811
plot_id,batch_id 0 8 miss% 0.4754293106610537
plot_id,batch_id 0 9 miss% 0.4336043919210986
plot_id,batch_id 0 10 miss% 0.28106419498144086
plot_id,batch_id 0 11 miss% 0.36290637634178863
plot_id,batch_id 0 12 miss% 0.4265535698488608
plot_id,batch_id 0 13 miss% 0.3473927349522759
plot_id,batch_id 0 14 miss% 0.515402204932121
plot_id,batch_id 0 15 miss% 0.4098600487205782
plot_id,batch_id 0 16 miss% 0.49325875284856807
plot_id,batch_id 0 17 miss% 0.4509222669975232
plot_id,batch_id 0 18 miss% 0.5446587385375539
plot_id,batch_id 0 19 miss% 0.4485437281985449
plot_id,batch_id 0 20 miss% 0.33534146144348487
plot_id,batch_id 0 21 miss% 0.5232897193283993
plot_id,batch_id 0 22 miss% 0.42384553284323895
plot_id,batch_id 0 23 miss% 0.3842586501297488
plot_id,batch_id 0 24 miss% 0.28675699087741513
plot_id,batch_id 0 25 miss% 0.3676576067838936
plot_id,batch_id 0 26 miss% 0.444592363767389
plot_id,batch_id 0 27 miss% 0.49481902884987194
plot_id,batch_id 0 28 miss% 0.4381077212197032
plot_id,batch_id 0 29 miss% 0.3945128443041774
plot_id,batch_id 0 30 miss% 0.2639198431398532
plot_id,batch_id 0 31 miss% 0.3663200275989149
plot_id,batch_id 0 32 miss% 0.4985474270064663
plot_id,batch_id 0 33 miss% 0.46669657945314785
plot_id,batch_id 0 34 miss% 0.4307899590642974
plot_id,batch_id 0 35 miss% 0.3117322316195318
plot_id,batch_id 0 36 miss% 0.5313260380891154
plot_id,batch_id 0 37 miss% 0.4379826592993618
plot_id,batch_id 0 38 miss% 0.5681763804111036
plot_id,batch_id 0 39 miss% 0.482264651729862
plot_id,batch_id 0 40 miss% 0.33095431502442996
plot_id,batch_id 0 41 miss% 0.4126929850402668
plot_id,batch_id 0 42 miss% 0.2351556466841867
plot_id,batch_id 0 43 miss% 0.3564389675781171
plot_id,batch_id 0 44 miss% 0.43461793146202954
plot_id,batch_id 0 45 miss% 0.34235892304339394
plot_id,batch_id 0 46 miss% 0.48145944150673653
plot_id,batch_id 0 47 miss% 0.3361073320750711
plot_id,batch_id 0 48 miss% 0.3252058002420299
plot_id,batch_id 0 49 miss% 0.3650941458038943
plot_id,batch_id 0 50 miss% 0.4660248054777942
plot_id,batch_id 0 51 miss% 0.5072818487253778
plot_id,batch_id 0 52 miss% 0.2886915790613307
plot_id,batch_id 0 53 miss% 0.47991319869812654
plot_id,batch_id 0 54 miss% 0.39484224671086054
plot_id,batch_id 0 55 miss% 0.5370168197729355
plot_id,batch_id 0 56 miss% 0.5187442893342712
plot_id,batch_id 0 57 miss% 0.42782906749802
plot_id,batch_id 0 58 miss% 0.3270038566604694
plot_id,batch_id 0 59 miss% 0.42609717929875357
plot_id,batch_id 0 60 miss% 0.3103173819738441
plot_id,batch_id 0 61 miss% 0.3079817623819868
plot_id,batch_id 0 62 miss% 0.42234772046680225
plot_id,batch_id 0 63 miss% 0.4504547076990742
plot_id,batch_id 0 64 miss% 0.37933534107176503
plot_id,batch_id 0 65 miss% 0.4101686545965131
plot_id,batch_id 0 66 miss% 0.4341240813190231
plot_id,batch_id 0 67 miss% 0.30156592699894286
plot_id,batch_id 0 68 miss% 0.3520374855771934
plot_id,batch_id 0 69 0.03125780694698656
plot_id,batch_id 0 68 miss% 0.06317768126505449
plot_id,batch_id 0 69 miss% 0.06377867477247201
plot_id,batch_id 0 70 miss% 0.048329374900998125
plot_id,batch_id 0 71 miss% 0.0440125616538985
plot_id,batch_id 0 72 miss% 0.11039923669053768
plot_id,batch_id 0 73 miss% 0.049537921231620416
plot_id,batch_id 0 74 miss% 0.07785099738317665
plot_id,batch_id 0 75 miss% 0.12000059656223186
plot_id,batch_id 0 76 miss% 0.04875314785000394
plot_id,batch_id 0 77 miss% 0.057764356977852836
plot_id,batch_id 0 78 miss% 0.041604506031897165
plot_id,batch_id 0 79 miss% 0.09306197402520278
plot_id,batch_id 0 80 miss% 0.03794140616310351
plot_id,batch_id 0 81 miss% 0.10838557957671748
plot_id,batch_id 0 82 miss% 0.06376093700216272
plot_id,batch_id 0 83 miss% 0.05547803506342151
plot_id,batch_id 0 84 miss% 0.05904508870145526
plot_id,batch_id 0 85 miss% 0.05070896056787779
plot_id,batch_id 0 86 miss% 0.06810986869459308
plot_id,batch_id 0 87 miss% 0.10705572955039232
plot_id,batch_id 0 88 miss% 0.08696861597913336
plot_id,batch_id 0 89 miss% 0.05527520924878705
plot_id,batch_id 0 90 miss% 0.04737088389962597
plot_id,batch_id 0 91 miss% 0.039558864831117514
plot_id,batch_id 0 92 miss% 0.07634333766753373
plot_id,batch_id 0 93 miss% 0.03892242008493759
plot_id,batch_id 0 94 miss% 0.11946978066065697
plot_id,batch_id 0 95 miss% 0.07266712808365046
plot_id,batch_id 0 96 miss% 0.06289005099960585
plot_id,batch_id 0 97 miss% 0.040733806680190686
plot_id,batch_id 0 98 miss% 0.048934123157498254
plot_id,batch_id 0 99 miss% 0.057333040832175525
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07646433 0.10002372 0.10679981 0.03531217 0.03959363 0.11818469
 0.0406289  0.05784472 0.06127381 0.03160916 0.03902499 0.04394721
 0.07725902 0.06001034 0.09212226 0.04500244 0.09053834 0.04970181
 0.0752675  0.09796063 0.11510538 0.03815599 0.04061498 0.05803518
 0.05437376 0.08817549 0.07019972 0.0559285  0.03675744 0.03325223
 0.04077878 0.08398589 0.10083474 0.02973889 0.03709164 0.03598309
 0.17925483 0.07219451 0.03570112 0.04762085 0.09649046 0.0411189
 0.03009386 0.05739393 0.04174005 0.04568887 0.04393575 0.02740328
 0.04530401 0.03263531 0.13235499 0.02629675 0.0257194  0.04327403
 0.05157168 0.065468   0.08585305 0.07101346 0.0337737  0.03275988
 0.03662512 0.02485046 0.0633514  0.06700661 0.07322171 0.06168639
 0.01795647 0.03125781 0.06317768 0.06377867 0.04832937 0.04401256
 0.11039924 0.04953792 0.077851   0.1200006  0.04875315 0.05776436
 0.04160451 0.09306197 0.03794141 0.10838558 0.06376094 0.05547804
 0.05904509 0.05070896 0.06810987 0.10705573 0.08696862 0.05527521
 0.04737088 0.03955886 0.07634334 0.03892242 0.11946978 0.07266713
 0.06289005 0.04073381 0.04893412 0.05733304]
for model  200 the mean error 0.06113421686938999
all id 200 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 31 out win 6 err 0.06113421686938999 time 16128.783422708511
Launcher: Job 201 completed in 16390 seconds.
Launcher: Task 18 done. Exiting.
 miss% 0.30284870212433807
plot_id,batch_id 0 71 miss% 0.44036516068940573
plot_id,batch_id 0 72 miss% 0.4354131193429595
plot_id,batch_id 0 73 miss% 0.4195666645332654
plot_id,batch_id 0 74 miss% 0.4917710519990292
plot_id,batch_id 0 75 miss% 0.3247083619278958
plot_id,batch_id 0 76 miss% 0.39584228372810665
plot_id,batch_id 0 77 miss% 0.36957961253725513
plot_id,batch_id 0 78 miss% 0.3828205024443826
plot_id,batch_id 0 79 miss% 0.394609254146575
plot_id,batch_id 0 80 miss% 0.3136238386069333
plot_id,batch_id 0 81 miss% 0.46168013870388586
plot_id,batch_id 0 82 miss% 0.36767329065353155
plot_id,batch_id 0 83 miss% 0.4302916331704385
plot_id,batch_id 0 84 miss% 0.4485472435908891
plot_id,batch_id 0 85 miss% 0.3107610432627478
plot_id,batch_id 0 86 miss% 0.4507568412137365
plot_id,batch_id 0 87 miss% 0.4330816735370634
plot_id,batch_id 0 88 miss% 0.4600690397022477
plot_id,batch_id 0 89 miss% 0.42819195330574095
plot_id,batch_id 0 90 miss% 0.2969626145163582
plot_id,batch_id 0 91 miss% 0.3806977419840431
plot_id,batch_id 0 92 miss% 0.37184452677398266
plot_id,batch_id 0 93 miss% 0.34106744172279424
plot_id,batch_id 0 94 miss% 0.4964981254766749
plot_id,batch_id 0 95 miss% 0.307440160204579
plot_id,batch_id 0 96 miss% 0.34671501041826475
plot_id,batch_id 0 97 miss% 0.5539110220048443
plot_id,batch_id 0 98 miss% 0.501030101877789
plot_id,batch_id 0 99 miss% 0.36087934070255806
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.35729285 0.39351945 0.449244   0.32128712 0.33407597 0.41078367
 0.39652422 0.44832703 0.56822461 0.41459569 0.36877505 0.37694602
 0.50660278 0.41820008 0.41276237 0.39809388 0.44410072 0.57255619
 0.47174522 0.44851047 0.38987396 0.37609284 0.33201698 0.33633229
 0.34339817 0.37707534 0.37340593 0.35974928 0.41309494 0.40513736
 0.33107251 0.54048945 0.43293275 0.37968337 0.37375591 0.37282098
 0.61826902 0.41379256 0.39337109 0.36224214 0.30466025 0.35730897
 0.32803376 0.35745688 0.26988038 0.35072727 0.3261168  0.35675156
 0.37498293 0.37780603 0.48228492 0.44707845 0.43275221 0.3363003
 0.43113856 0.449585   0.52978657 0.42664628 0.39331916 0.39903614
 0.26851811 0.34147539 0.38714367 0.36333449 0.35520834 0.41837092
 0.44709372 0.30475896 0.4816772  0.38208524 0.3028487  0.44036516
 0.43541312 0.41956666 0.49177105 0.32470836 0.39584228 0.36957961
 0.3828205  0.39460925 0.31362384 0.46168014 0.36767329 0.43029163
 0.44854724 0.31076104 0.45075684 0.43308167 0.46006904 0.42819195
 0.29696261 0.38069774 0.37184453 0.34106744 0.49649813 0.30744016
 0.34671501 0.55391102 0.5010301  0.36087934]
for model  149 the mean error 0.39937340256264237
all id 149 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 25 out win 6 err 0.39937340256264237 time 16178.787504434586
Launcher: Job 150 completed in 16407 seconds.
Launcher: Task 97 done. Exiting.
miss% 0.4707991773605679
plot_id,batch_id 0 70 miss% 0.3430867909294696
plot_id,batch_id 0 71 miss% 0.35101539787323977
plot_id,batch_id 0 72 miss% 0.375364281283333
plot_id,batch_id 0 73 miss% 0.37364518881506686
plot_id,batch_id 0 74 miss% 0.39703485952196416
plot_id,batch_id 0 75 miss% 0.33412756379978586
plot_id,batch_id 0 76 miss% 0.30563362249836784
plot_id,batch_id 0 77 miss% 0.4055780321310396
plot_id,batch_id 0 78 miss% 0.42642743607429073
plot_id,batch_id 0 79 miss% 0.3474926104042456
plot_id,batch_id 0 80 miss% 0.41996637375456686
plot_id,batch_id 0 81 miss% 0.42252651812361774
plot_id,batch_id 0 82 miss% 0.36853581702524896
plot_id,batch_id 0 83 miss% 0.35781086852887317
plot_id,batch_id 0 84 miss% 0.3241107500363328
plot_id,batch_id 0 85 miss% 0.3182804430282961
plot_id,batch_id 0 86 miss% 0.31920122391256345
plot_id,batch_id 0 87 miss% 0.3375452095115899
plot_id,batch_id 0 88 miss% 0.3498012692842955
plot_id,batch_id 0 89 miss% 0.3745492501001635
plot_id,batch_id 0 90 miss% 0.28495144254762994
plot_id,batch_id 0 91 miss% 0.4109414848353379
plot_id,batch_id 0 92 miss% 0.4125693857791647
plot_id,batch_id 0 93 miss% 0.27869478591587393
plot_id,batch_id 0 94 miss% 0.4683961002183769
plot_id,batch_id 0 95 miss% 0.3149776373922784
plot_id,batch_id 0 96 miss% 0.40243321457918196
plot_id,batch_id 0 97 miss% 0.4799454691462783
plot_id,batch_id 0 98 miss% 0.40034606749854645
plot_id,batch_id 0 99 miss% 0.37631967590963256
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.33229154 0.40494273 0.36800694 0.32608613 0.2785178  0.36237983
 0.34594609 0.5314594  0.47542931 0.43360439 0.28106419 0.36290638
 0.42655357 0.34739273 0.5154022  0.40986005 0.49325875 0.45092227
 0.54465874 0.44854373 0.33534146 0.52328972 0.42384553 0.38425865
 0.28675699 0.36765761 0.44459236 0.49481903 0.43810772 0.39451284
 0.26391984 0.36632003 0.49854743 0.46669658 0.43078996 0.31173223
 0.53132604 0.43798266 0.56817638 0.48226465 0.33095432 0.41269299
 0.23515565 0.35643897 0.43461793 0.34235892 0.48145944 0.33610733
 0.3252058  0.36509415 0.46602481 0.50728185 0.28869158 0.4799132
 0.39484225 0.53701682 0.51874429 0.42782907 0.32700386 0.42609718
 0.31031738 0.30798176 0.42234772 0.45045471 0.37933534 0.41016865
 0.43412408 0.30156593 0.35203749 0.47079918 0.34308679 0.3510154
 0.37536428 0.37364519 0.39703486 0.33412756 0.30563362 0.40557803
 0.42642744 0.34749261 0.41996637 0.42252652 0.36853582 0.35781087
 0.32411075 0.31828044 0.31920122 0.33754521 0.34980127 0.37454925
 0.28495144 0.41094148 0.41256939 0.27869479 0.4683961  0.31497764
 0.40243321 0.47994547 0.40034607 0.37631968]
for model  148 the mean error 0.395041358929733
all id 148 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 25 out win 5 err 0.395041358929733 time 16182.791972398758
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  69649
Epoch:0, Train loss:0.562909, valid loss:0.538390
Epoch:1, Train loss:0.043247, valid loss:0.009187
Epoch:2, Train loss:0.014797, valid loss:0.004049
Epoch:3, Train loss:0.010780, valid loss:0.003227
Epoch:4, Train loss:0.009168, valid loss:0.002882
Epoch:5, Train loss:0.008619, valid loss:0.002944
Epoch:6, Train loss:0.008388, valid loss:0.003101
Epoch:7, Train loss:0.008048, valid loss:0.002708
Epoch:8, Train loss:0.005900, valid loss:0.001746
Epoch:9, Train loss:0.004070, valid loss:0.001430
Epoch:10, Train loss:0.003108, valid loss:0.001840
Epoch:11, Train loss:0.002172, valid loss:0.001084
Epoch:12, Train loss:0.001987, valid loss:0.001111
Epoch:13, Train loss:0.001914, valid loss:0.001074
Epoch:14, Train loss:0.001895, valid loss:0.001005
Epoch:15, Train loss:0.001837, valid loss:0.001004
Epoch:16, Train loss:0.001719, valid loss:0.001038
Epoch:17, Train loss:0.001766, valid loss:0.000947
Epoch:18, Train loss:0.001662, valid loss:0.001008
Epoch:19, Train loss:0.001616, valid loss:0.001218
Epoch:20, Train loss:0.001595, valid loss:0.000983
Epoch:21, Train loss:0.001269, valid loss:0.000864
Epoch:22, Train loss:0.001263, valid loss:0.000866
Epoch:23, Train loss:0.001223, valid loss:0.000866
Epoch:24, Train loss:0.001211, valid loss:0.000796
Epoch:25, Train loss:0.001187, valid loss:0.000822
Epoch:26, Train loss:0.001171, valid loss:0.000798
Epoch:27, Train loss:0.001134, valid loss:0.000963
Epoch:28, Train loss:0.001137, valid loss:0.000783
Epoch:29, Train loss:0.001127, valid loss:0.000895
Epoch:30, Train loss:0.001095, valid loss:0.000760
Epoch:31, Train loss:0.000942, valid loss:0.000753
Epoch:32, Train loss:0.000934, valid loss:0.000732
Epoch:33, Train loss:0.000913, valid loss:0.000729
Epoch:34, Train loss:0.000919, valid loss:0.000728
Epoch:35, Train loss:0.000897, valid loss:0.000705
Epoch:36, Train loss:0.000900, valid loss:0.000760
Epoch:37, Train loss:0.000892, valid loss:0.000712
Epoch:38, Train loss:0.000878, valid loss:0.000701
Epoch:39, Train loss:0.000864, valid loss:0.000740
Epoch:40, Train loss:0.000866, valid loss:0.000720
Epoch:41, Train loss:0.000790, valid loss:0.000657
Epoch:42, Train loss:0.000781, valid loss:0.000680
Epoch:43, Train loss:0.000775, valid loss:0.000662
Epoch:44, Train loss:0.000775, valid loss:0.000665
Epoch:45, Train loss:0.000771, valid loss:0.000663
Epoch:46, Train loss:0.000762, valid loss:0.000695
Epoch:47, Train loss:0.000762, valid loss:0.000671
Epoch:48, Train loss:0.000759, valid loss:0.000668
Epoch:49, Train loss:0.000753, valid loss:0.000675
Epoch:50, Train loss:0.000749, valid loss:0.000659
Epoch:51, Train loss:0.000699, valid loss:0.000655
Epoch:52, Train loss:0.000695, valid loss:0.000653
Epoch:53, Train loss:0.000694, valid loss:0.000650
Epoch:54, Train loss:0.000693, valid loss:0.000650
Epoch:55, Train loss:0.000692, valid loss:0.000653
Epoch:56, Train loss:0.000691, valid loss:0.000649
Epoch:57, Train loss:0.000691, valid loss:0.000650
Epoch:58, Train loss:0.000690, valid loss:0.000654
Epoch:59, Train loss:0.000690, valid loss:0.000650
Epoch:60, Train loss:0.000690, valid loss:0.000648
training time 16202.070616006851
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.09205106793420267
plot_id,batch_id 0 1 miss% 0.047904930204454896
plot_id,batch_id 0 2 miss% 0.1027970509827475
plot_id,batch_id 0 3 miss% 0.053643906838785775
plot_id,batch_id 0 4 miss% 0.06431356439180716
plot_id,batch_id 0 5 miss% 0.05013501698711872
plot_id,batch_id 0 6 miss% 0.07133295433455873
plot_id,batch_id 0 7 miss% 0.06583939802073684
plot_id,batch_id 0 8 miss% 0.058402257372995314
plot_id,batch_id 0 9 miss% 0.03608802844402107
plot_id,batch_id 0 10 miss% 0.039302258475544534
plot_id,batch_id 0 11 miss% 0.08780659421264787
plot_id,batch_id 0 12 miss% 0.07096247670488029
plot_id,batch_id 0 13 miss% 0.032927687564082875
plot_id,batch_id 0 14 miss% 0.08319110973251595
plot_id,batch_id 0 15 miss% 0.06173418217095839
plot_id,batch_id 0 16 miss% 0.0744492004052307
plot_id,batch_id 0 17 miss% 0.05768649964139295
plot_id,batch_id 0 18 miss% 0.04287628622179015
plot_id,batch_id 0 19 miss% 0.10329934349758016
plot_id,batch_id 0 20 miss% 0.05273175765841328
plot_id,batch_id 0 21 miss% 0.048057560265240086
plot_id,batch_id 0 22 miss% 0.07141770192154107
plot_id,batch_id 0 23 miss% 0.05881508699979184
plot_id,batch_id 0 24 miss% 0.062017698784133615
plot_id,batch_id 0 25 miss% 0.0409294361654909
plot_id,batch_id 0 26 miss% 0.045604806263800535
plot_id,batch_id 0 27 miss% 0.03993482328873281
plot_id,batch_id 0 28 miss% 0.04041004398882143
plot_id,batch_id 0 29 miss% 0.03016658932339989
plot_id,batch_id 0 30 miss% 0.036306723235276096
plot_id,batch_id 0 31 miss% 0.08065357085336278
plot_id,batch_id 0 32 miss% 0.11803584546170905
plot_id,batch_id 0 33 miss% 0.062819591394851
plot_id,batch_id 0 34 miss% 0.06658297058997792
plot_id,batch_id 0 35 miss% 0.06293805234290906
plot_id,batch_id 0 36 miss% 0.053040364871657454
plot_id,batch_id 0 37 miss% 0.0658394695546088
plot_id,batch_id 0 38 miss% 0.0758065617791394
plot_id,batch_id 0 39 miss% 0.033008396911552745
plot_id,batch_id 0 40 miss% 0.05437209279400375
plot_id,batch_id 0 41 miss% 0.044507918221209086
plot_id,batch_id 0 42 miss% 0.021857050906867043
plot_id,batch_id 0 43 miss% 0.04561018296908006
plot_id,batch_id 0 44 miss% 0.03876016392396352
plot_id,batch_id 0 45 miss% 0.03750705410859723
plot_id,batch_id 0 46 miss% 0.020373176413404203
plot_id,batch_id 0 47 miss% 0.036781397769090445
plot_id,batch_id 0 48 miss% 0.03920339947821151
plot_id,batch_id 0 49 miss% 0.02779189531998745
plot_id,batch_id 0 50 miss% 0.1027461455854005
plot_id,batch_id 0 51 miss% 0.0442658996351899
plot_id,batch_id 0 52 miss% 0.02968441823386197
plot_id,batch_id 0 53 miss% 0.023354471132701775
plot_id,batch_id 0 54 miss% 0.035835039433159493
plot_id,batch_id 0 55 miss% 0.09291971414733698
plot_id,batch_id 0 56 miss% 0.05670658211001804
plot_id,batch_id 0 57 miss% 0.04442758842802695
plot_id,batch_id 0 58 miss% 0.041407203933008195
plot_id,batch_id 0 59 miss% 0.02364242726355191
plot_id,batch_id 0 60 miss% 0.03176394076627745
plot_id,batch_id 0 61 miss% 0.031224111616425652
plot_id,batch_id 0 62 miss% 0.05604222269468639
plot_id,batch_id 0 63 miss% 0.08105913551580127
plot_id,batch_id 0 64 miss% 0.044982926386827986
plot_id,batch_id 0 65 miss% 0.09444404528736153
plot_id,batch_id 0 66 miss% 0.10166323706263557
plot_id,batch_id 0 67 miss% 0.034700260114602755
plot_id,batch_id 0 68 miss% 0.043430879673844854
Launcher: Job 149 completed in 16415 seconds.
Launcher: Task 87 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  151697
Epoch:0, Train loss:0.538751, valid loss:0.495115
Epoch:1, Train loss:0.364026, valid loss:0.364665
Epoch:2, Train loss:0.356585, valid loss:0.363774
Epoch:3, Train loss:0.355620, valid loss:0.363511
Epoch:4, Train loss:0.355238, valid loss:0.363545
Epoch:5, Train loss:0.354950, valid loss:0.363596
Epoch:6, Train loss:0.354828, valid loss:0.363475
Epoch:7, Train loss:0.354629, valid loss:0.363308
Epoch:8, Train loss:0.354467, valid loss:0.363265
Epoch:9, Train loss:0.354448, valid loss:0.363528
Epoch:10, Train loss:0.354382, valid loss:0.363190
Epoch:11, Train loss:0.353831, valid loss:0.363017
Epoch:12, Train loss:0.353797, valid loss:0.362960
Epoch:13, Train loss:0.353772, valid loss:0.362997
Epoch:14, Train loss:0.353740, valid loss:0.363056
Epoch:15, Train loss:0.353761, valid loss:0.362951
Epoch:16, Train loss:0.353676, valid loss:0.362921
Epoch:17, Train loss:0.353669, valid loss:0.362995
Epoch:18, Train loss:0.353621, valid loss:0.363021
Epoch:19, Train loss:0.353605, valid loss:0.362929
Epoch:20, Train loss:0.353570, valid loss:0.362950
Epoch:21, Train loss:0.353324, valid loss:0.362898
Epoch:22, Train loss:0.353314, valid loss:0.362846
Epoch:23, Train loss:0.353308, valid loss:0.362838
Epoch:24, Train loss:0.353275, valid loss:0.362828
Epoch:25, Train loss:0.353285, valid loss:0.362775
Epoch:26, Train loss:0.353280, valid loss:0.362812
Epoch:27, Train loss:0.353268, valid loss:0.362831
Epoch:28, Train loss:0.353259, valid loss:0.362783
Epoch:29, Train loss:0.353245, valid loss:0.362839
Epoch:30, Train loss:0.353223, valid loss:0.362792
Epoch:31, Train loss:0.353110, valid loss:0.362739
Epoch:32, Train loss:0.353106, valid loss:0.362750
Epoch:33, Train loss:0.353097, valid loss:0.362778
Epoch:34, Train loss:0.353100, valid loss:0.362785
Epoch:35, Train loss:0.353088, valid loss:0.362748
Epoch:36, Train loss:0.353076, valid loss:0.362758
Epoch:37, Train loss:0.353084, valid loss:0.362744
Epoch:38, Train loss:0.353072, valid loss:0.362763
Epoch:39, Train loss:0.353064, valid loss:0.362824
Epoch:40, Train loss:0.353061, valid loss:0.362761
Epoch:41, Train loss:0.353015, valid loss:0.362745
Epoch:42, Train loss:0.353002, valid loss:0.362736
Epoch:43, Train loss:0.353004, valid loss:0.362748
Epoch:44, Train loss:0.353003, valid loss:0.362745
Epoch:45, Train loss:0.353000, valid loss:0.362777
Epoch:46, Train loss:0.352993, valid loss:0.362740
Epoch:47, Train loss:0.352995, valid loss:0.362725
Epoch:48, Train loss:0.352996, valid loss:0.362734
Epoch:49, Train loss:0.352981, valid loss:0.362771
Epoch:50, Train loss:0.352988, valid loss:0.362732
Epoch:51, Train loss:0.352978, valid loss:0.362723
Epoch:52, Train loss:0.352970, valid loss:0.362724
Epoch:53, Train loss:0.352967, valid loss:0.362720
Epoch:54, Train loss:0.352965, valid loss:0.362722
Epoch:55, Train loss:0.352964, valid loss:0.362724
Epoch:56, Train loss:0.352963, valid loss:0.362722
Epoch:57, Train loss:0.352962, valid loss:0.362723
Epoch:58, Train loss:0.352961, valid loss:0.362724
Epoch:59, Train loss:0.352961, valid loss:0.362722
Epoch:60, Train loss:0.352960, valid loss:0.362724
training time 16331.891967773438
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.8892384775625152
plot_id,batch_id 0 1 miss% 0.8610459676108009
plot_id,batch_id 0 2 miss% 0.8579076915207832
plot_id,batch_id 0 3 miss% 0.8530838526276419
plot_id,batch_id 0 4 miss% 0.8525460788416781
plot_id,batch_id 0 5 miss% 0.894422492332976
plot_id,batch_id 0 6 miss% 0.8622718503560494
plot_id,batch_id 0 7 miss% 0.8567171837712113
plot_id,batch_id 0 8 miss% 0.854638309764571
plot_id,batch_id 0 9 miss% 0.8525760829664821
plot_id,batch_id 0 10 miss% 0.8919907170150092
plot_id,batch_id 0 11 miss% 0.8620826768279986
plot_id,batch_id 0 12 miss% 0.8582521132780759
plot_id,batch_id 0 13 miss% 0.8523101280063191
plot_id,batch_id 0 14 miss% 0.8541036234429534
plot_id,batch_id 0 15 miss% 0.8798726121760103
plot_id,batch_id 0 16 miss% 0.8656033485394176
plot_id,batch_id 0 17 miss% 0.8587500023811072
plot_id,batch_id 0 18 miss% 0.8554630550201909
plot_id,batch_id 0 19 miss% 0.8531711955617507
plot_id,batch_id 0 20 miss% 0.8776690192229211
plot_id,batch_id 0 21 miss% 0.8559332849516718
plot_id,batch_id 0 22 miss% 0.8554595741248374
plot_id,batch_id 0 23 miss% 0.8512458121530408
plot_id,batch_id 0 24 miss% 0.8473039384815599
plot_id,batch_id 0 25 miss% 0.8755657473940399
plot_id,batch_id 0 26 miss% 0.8572681900952714
plot_id,batch_id 0 27 miss% 0.8540016556608747
plot_id,batch_id 0 28 miss% 0.8518944258752245
plot_id,batch_id 0 29 miss% 0.8490080025265189
plot_id,batch_id 0 30 miss% 0.8782232160471949
plot_id,batch_id 0 31 miss% 0.8567812172878232
plot_id,batch_id 0 32 miss% 0.8546427419597046
plot_id,batch_id 0 33 miss% 0.8527350477240127
plot_id,batch_id 0 34 miss% 0.8495487246754525
plot_id,batch_id 0 35 miss% 0.8778496323353697
plot_id,batch_id 0 36 miss% 0.8593655077245885
plot_id,batch_id 0 37 miss% 0.8545083128213639
plot_id,batch_id 0 38 miss% 0.8530941806415482
plot_id,batch_id 0 39 miss% 0.850584578141035
plot_id,batch_id 0 40 miss% 0.8718072712301789
plot_id,batch_id 0 41 miss% 0.8543666767784079
plot_id,batch_id 0 42 miss% 0.8484576239627137
plot_id,batch_id 0 43 miss% 0.8478093039074911
plot_id,batch_id 0 44 miss% 0.8463136496432517
plot_id,batch_id 0 45 miss% 0.8641459386761625
plot_id,batch_id 0 46 miss% 0.8532754715648517
plot_id,batch_id 0 47 miss% 0.852100938033054
plot_id,batch_id 0 48 miss% 0.8485312392341333
plot_id,batch_id 0 49 miss% 0.8467716872824548
plot_id,batch_id 0 50 miss% 0.8624610763586684
plot_id,batch_id 0 51 miss% 0.8538565909505342
plot_id,batch_id 0 52 miss% 0.8504169499687517
plot_id,batch_id 0 53 miss% 0.8500924071978879
plot_id,batch_id 0 54 miss% 0.847772030136759
plot_id,batch_id 0 55 miss% 0.8610722114612476
plot_id,batch_id 0 56 miss% 0.8531445220370119
plot_id,batch_id 0 57 miss% 0.853335436420417
plot_id,batch_id 0 58 miss% 0.849965011759522
plot_id,batch_id 0 59 miss% 0.8524128069596774
plot_id,batch_id 0 60 miss% 0.9149763193237185
plot_id,batch_id 0 61 miss% 0.88004917748338
plot_id,batch_id 0 62 miss% 0.8684701509066941
plot_id,batch_id 0 63 miss% 0.8607762455357353
plot_id,batch_id 0 64 miss% 0.8588006315530322
plot_id,batch_id 0 65 miss% 0.9171602026855602
plot_id,batch_id 0 66 miss% 0.8845709222346803
plot_id,batch_id 0 67 miss% 0.8702555655170733
plot_id,batch_id 0 68 miss% 0.8626730358939364
plot_id,batch_id 0 69 miss% 0.8573890737971052
plot_id,batch_id 0 70 miss% 0.9157069825811949
plot_id,batch_id 0 71 miss% 0.8866783525083641
plot_id,batch_id 0 72 miss% 0.8704121443501882
plot_id,batch_id 0 73 miss% 0.8634213507757231
plot_id,batch_id 0 74 miss% 0.8623474559749963
plot_id,batch_id 0 75 miss% 0.9198027134397482
plot_id,batch_id 0 76 miss% 0.8824233705622082
plot_id,batch_id 0 77 miss% 0.8703011918032628
plot_id,batch_id 0 78 miss% 0.8632685100264369
plot_id,batch_id 0 79 miss% 0.8625774785696522
plot_id,batch_id 0 80 miss% 0.9070064050877263
plot_id,batch_id 0 81 miss% 0.873633258154718
plot_id,batch_id 0 82 miss% 0.8625687167240645
plot_id,batch_id 0 83 miss% 0.8553388739780542
plot_id,batch_id 0 84 miss% 0.8546818965940748
plot_id,batch_id 0 85 miss% 0.9079269398146004
plot_id,batch_id 0 86 miss% 0.872297059570961
plot_id,batch_id 0 87 miss% 0.8635582343329781
plot_id,batch_id 0 88 miss% 0.8587212913459393
plot_id,batch_id 0 89 miss% 0.8548539720131965
plot_id,batch_id 0 90 miss% 0.9109509822366948
plot_id,batch_id 0 91 miss% 0.8742171527970363
plot_id,batch_id 0 92 miss% 0.8640070212839432
plot_id,batch_id 0 93 miss% 0.86079536173774
plot_id,batch_id 0 94 miss% 0.8555169382292171
plot_id,batch_id 0 95 miss% 0.9165801265875745
plot_id,batch_id 0 96 miss% 0.8774594806439381
plot_id,batch_id 0 97 miss% 0.8679170699311165
plot_id,batch_id 0 98 miss% 0.8583174214477338
plot_id,batch_id 0 99 miss% 0.8589113620865367
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.88923848 0.86104597 0.85790769 0.85308385 0.85254608 0.89442249
 0.86227185 0.85671718 0.85463831 0.85257608 0.89199072 0.86208268
 0.85825211 0.85231013 0.85410362 0.87987261 0.86560335 0.85875
 0.85546306 0.8531712  0.87766902 0.85593328 0.85545957 0.85124581
 0.84730394 0.87556575 0.85726819 0.85400166 0.85189443 0.849008
 0.87822322 0.85678122 0.85464274 0.85273505 0.84954872 0.87784963
 0.85936551 0.85450831 0.85309418 0.85058458 0.87180727 0.85436668
 0.84845762 0.8478093  0.84631365 0.86414594 0.85327547 0.85210094
 0.84853124 0.84677169 0.86246108 0.85385659 0.85041695 0.85009241
 0.84777203 0.86107221 0.85314452 0.85333544 0.84996501 0.85241281
 0.91497632 0.88004918 0.86847015 0.86077625 0.85880063 0.9171602
 0.88457092 0.87025557 0.86267304 0.85738907 0.91570698 0.88667835
 0.87041214 0.86342135 0.86234746 0.91980271 0.88242337 0.87030119
 0.86326851 0.86257748 0.90700641 0.87363326 0.86256872 0.85533887
 0.8546819  0.90792694 0.87229706 0.86355823 0.85872129 0.85485397
 0.91095098 0.87421715 0.86400702 0.86079536 0.85551694 0.91658013
 0.87745948 0.86791707 0.85831742 0.85891136]
for model  156 the mean error 0.865241595531313
all id 156 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 25 out win 4 err 0.865241595531313 time 16331.891967773438
Launcher: Job 157 completed in 16456 seconds.
Launcher: Task 45 done. Exiting.
plot_id,batch_id 0 69 miss% 0.023395604425280513
plot_id,batch_id 0 70 miss% 0.1054343566922517
plot_id,batch_id 0 71 miss% 0.0673636202076975
plot_id,batch_id 0 72 miss% 0.06758029889946054
plot_id,batch_id 0 73 miss% 0.04372385248719047
plot_id,batch_id 0 74 miss% 0.06252613335356598
plot_id,batch_id 0 75 miss% 0.06987179718240326
plot_id,batch_id 0 76 miss% 0.0688274393900555
plot_id,batch_id 0 77 miss% 0.08520699814269422
plot_id,batch_id 0 78 miss% 0.05663552860361807
plot_id,batch_id 0 79 miss% 0.07546590134940948
plot_id,batch_id 0 80 miss% 0.04522232107843762
plot_id,batch_id 0 81 miss% 0.0925106631977306
plot_id,batch_id 0 82 miss% 0.09647572500246046
plot_id,batch_id 0 83 miss% 0.07313179467145223
plot_id,batch_id 0 84 miss% 0.05992782670020287
plot_id,batch_id 0 85 miss% 0.04580170807038966
plot_id,batch_id 0 86 miss% 0.058735596868027165
plot_id,batch_id 0 87 miss% 0.08206929017202916
plot_id,batch_id 0 88 miss% 0.04600920354834858
plot_id,batch_id 0 89 miss% 0.06508130462105839
plot_id,batch_id 0 90 miss% 0.03764299817216591
plot_id,batch_id 0 91 miss% 0.041722136459156185
plot_id,batch_id 0 92 miss% 0.05400987393352921
plot_id,batch_id 0 93 miss% 0.03520993397920246
plot_id,batch_id 0 94 miss% 0.06280936759837707
plot_id,batch_id 0 95 miss% 0.044865028244127275
plot_id,batch_id 0 96 miss% 0.06807279661558932
plot_id,batch_id 0 97 miss% 0.04317517235638463
plot_id,batch_id 0 98 miss% 0.04609225279333194
plot_id,batch_id 0 99 miss% 0.07714954650669933
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09205107 0.04790493 0.10279705 0.05364391 0.06431356 0.05013502
 0.07133295 0.0658394  0.05840226 0.03608803 0.03930226 0.08780659
 0.07096248 0.03292769 0.08319111 0.06173418 0.0744492  0.0576865
 0.04287629 0.10329934 0.05273176 0.04805756 0.0714177  0.05881509
 0.0620177  0.04092944 0.04560481 0.03993482 0.04041004 0.03016659
 0.03630672 0.08065357 0.11803585 0.06281959 0.06658297 0.06293805
 0.05304036 0.06583947 0.07580656 0.0330084  0.05437209 0.04450792
 0.02185705 0.04561018 0.03876016 0.03750705 0.02037318 0.0367814
 0.0392034  0.0277919  0.10274615 0.0442659  0.02968442 0.02335447
 0.03583504 0.09291971 0.05670658 0.04442759 0.0414072  0.02364243
 0.03176394 0.03122411 0.05604222 0.08105914 0.04498293 0.09444405
 0.10166324 0.03470026 0.04343088 0.0233956  0.10543436 0.06736362
 0.0675803  0.04372385 0.06252613 0.0698718  0.06882744 0.085207
 0.05663553 0.0754659  0.04522232 0.09251066 0.09647573 0.07313179
 0.05992783 0.04580171 0.0587356  0.08206929 0.0460092  0.0650813
 0.037643   0.04172214 0.05400987 0.03520993 0.06280937 0.04486503
 0.0680728  0.04317517 0.04609225 0.07714955]
for model  128 the mean error 0.05724671518039923
all id 128 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 25 out win 6 err 0.05724671518039923 time 16202.070616006851
Launcher: Job 129 completed in 16463 seconds.
Launcher: Task 62 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  151697
Epoch:0, Train loss:0.500920, valid loss:0.461628
Epoch:1, Train loss:0.214473, valid loss:0.004958
Epoch:2, Train loss:0.014317, valid loss:0.004597
Epoch:3, Train loss:0.013499, valid loss:0.004024
Epoch:4, Train loss:0.013011, valid loss:0.003950
Epoch:5, Train loss:0.012212, valid loss:0.003579
Epoch:6, Train loss:0.011105, valid loss:0.003683
Epoch:7, Train loss:0.010266, valid loss:0.002103
Epoch:8, Train loss:0.004323, valid loss:0.001373
Epoch:9, Train loss:0.002787, valid loss:0.001291
Epoch:10, Train loss:0.002528, valid loss:0.001326
Epoch:11, Train loss:0.001888, valid loss:0.001008
Epoch:12, Train loss:0.001862, valid loss:0.001002
Epoch:13, Train loss:0.001694, valid loss:0.000971
Epoch:14, Train loss:0.001670, valid loss:0.000962
Epoch:15, Train loss:0.001589, valid loss:0.001079
Epoch:16, Train loss:0.001520, valid loss:0.000953
Epoch:17, Train loss:0.001492, valid loss:0.001071
Epoch:18, Train loss:0.001469, valid loss:0.000896
Epoch:19, Train loss:0.001391, valid loss:0.000843
Epoch:20, Train loss:0.001331, valid loss:0.000799
Epoch:21, Train loss:0.001056, valid loss:0.000718
Epoch:22, Train loss:0.001016, valid loss:0.000759
Epoch:23, Train loss:0.001005, valid loss:0.000740
Epoch:24, Train loss:0.000981, valid loss:0.000687
Epoch:25, Train loss:0.000979, valid loss:0.000728
Epoch:26, Train loss:0.000957, valid loss:0.000691
Epoch:27, Train loss:0.000922, valid loss:0.000685
Epoch:28, Train loss:0.000894, valid loss:0.000709
Epoch:29, Train loss:0.000888, valid loss:0.000749
Epoch:30, Train loss:0.000905, valid loss:0.000671
Epoch:31, Train loss:0.000719, valid loss:0.000596
Epoch:32, Train loss:0.000724, valid loss:0.000603
Epoch:33, Train loss:0.000711, valid loss:0.000615
Epoch:34, Train loss:0.000707, valid loss:0.000578
Epoch:35, Train loss:0.000683, valid loss:0.000597
Epoch:36, Train loss:0.000679, valid loss:0.000613
Epoch:37, Train loss:0.000691, valid loss:0.000594
Epoch:38, Train loss:0.000666, valid loss:0.000591
Epoch:39, Train loss:0.000656, valid loss:0.000629
Epoch:40, Train loss:0.000665, valid loss:0.000605
Epoch:41, Train loss:0.000583, valid loss:0.000541
Epoch:42, Train loss:0.000570, valid loss:0.000568
Epoch:43, Train loss:0.000565, valid loss:0.000582
Epoch:44, Train loss:0.000567, valid loss:0.000542
Epoch:45, Train loss:0.000568, valid loss:0.000566
Epoch:46, Train loss:0.000563, valid loss:0.000580
Epoch:47, Train loss:0.000554, valid loss:0.000546
Epoch:48, Train loss:0.000566, valid loss:0.000559
Epoch:49, Train loss:0.000543, valid loss:0.000560
Epoch:50, Train loss:0.000541, valid loss:0.000572
Epoch:51, Train loss:0.000506, valid loss:0.000539
Epoch:52, Train loss:0.000500, valid loss:0.000539
Epoch:53, Train loss:0.000498, valid loss:0.000539
Epoch:54, Train loss:0.000497, valid loss:0.000539
Epoch:55, Train loss:0.000496, valid loss:0.000538
Epoch:56, Train loss:0.000495, valid loss:0.000536
Epoch:57, Train loss:0.000495, valid loss:0.000534
Epoch:58, Train loss:0.000494, valid loss:0.000533
Epoch:59, Train loss:0.000494, valid loss:0.000538
Epoch:60, Train loss:0.000494, valid loss:0.000534
training time 16298.156789064407
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08728092818216494
plot_id,batch_id 0 1 miss% 0.05477557553858825
plot_id,batch_id 0 2 miss% 0.10394829271631048
plot_id,batch_id 0 3 miss% 0.03532621992587815
plot_id,batch_id 0 4 miss% 0.043144950017908704
plot_id,batch_id 0 5 miss% 0.04777066570526344
plot_id,batch_id 0 6 miss% 0.08242567603683895
plot_id,batch_id 0 7 miss% 0.0706174512440374
plot_id,batch_id 0 8 miss% 0.06754431025106701
plot_id,batch_id 0 9 miss% 0.052060016674302115
plot_id,batch_id 0 10 miss% 0.03068753350346887
plot_id,batch_id 0 11 miss% 0.06429894997939072
plot_id,batch_id 0 12 miss% 0.07659017733785928
plot_id,batch_id 0 13 miss% 0.0626954620438065
plot_id,batch_id 0 14 miss% 0.0583525862174745
plot_id,batch_id 0 15 miss% 0.027028780296468706
plot_id,batch_id 0 16 miss% 0.058053570151294566
plot_id,batch_id 0 17 miss% 0.06155713370471845
plot_id,batch_id 0 18 miss% 0.07839473651744831
plot_id,batch_id 0 19 miss% 0.08537432077835483
plot_id,batch_id 0 20 miss% 0.04321139088543813
plot_id,batch_id 0 21 miss% 0.027550234892972165
plot_id,batch_id 0 22 miss% 0.05716313235298255
plot_id,batch_id 0 23 miss% 0.031664184589048044
plot_id,batch_id 0 24 miss% 0.057415009756891865
plot_id,batch_id 0 25 miss% 0.04638794843947206
plot_id,batch_id 0 26 miss% 0.04475874391438077
plot_id,batch_id 0 27 miss% 0.05372698684035083
plot_id,batch_id 0 28 miss% 0.044266830034237044
plot_id,batch_id 0 29 miss% 0.041374241486634815
plot_id,batch_id 0 30 miss% 0.032481622109809476
plot_id,batch_id 0 31 miss% 0.07960038793365332
plot_id,batch_id 0 32 miss% 0.07033217976409831
plot_id,batch_id 0 33 miss% 0.07003109346730217
plot_id,batch_id 0 34 miss% 0.044039200651498356
plot_id,batch_id 0 35 miss% 0.04918056079504687
plot_id,batch_id 0 36 miss% 0.04084481842315756
plot_id,batch_id 0 37 miss% 0.0644577802172013
plot_id,batch_id 0 38 miss% 0.041009518433972734
plot_id,batch_id 0 39 miss% 0.03764393152289912
plot_id,batch_id 0 40 miss% 0.048305192933394935
plot_id,batch_id 0 41 miss% 0.06462819794975411
plot_id,batch_id 0 42 miss% 0.03736532383786028
plot_id,batch_id 0 43 miss% 0.06532559552427487
plot_id,batch_id 0 44 miss% 0.021403885079702742
plot_id,batch_id 0 45 miss% 0.04027468934583209
plot_id,batch_id 0 46 miss% 0.047686445251869505
plot_id,batch_id 0 47 miss% 0.04355853698030743
plot_id,batch_id 0 48 miss% 0.04240869805414234
plot_id,batch_id 0 49 miss% 0.029783303669660793
plot_id,batch_id 0 50 miss% 0.12701364147929145
plot_id,batch_id 0 51 miss% 0.028913461259142938
plot_id,batch_id 0 52 miss% 0.035446776831727214
plot_id,batch_id 0 53 miss% 0.029344729526038386
plot_id,batch_id 0 54 miss% 0.04015459037113377
plot_id,batch_id 0 55 miss% 0.08539807320293849
plot_id,batch_id 0 56 miss% 0.08083226825610418
plot_id,batch_id 0 57 miss% 0.029737574053220243
plot_id,batch_id 0 58 miss% 0.04157810911954854
plot_id,batch_id 0 59 miss% 0.04232741457976974
plot_id,batch_id 0 60 miss% 0.02687653787450639
plot_id,batch_id 0 61 miss% 0.04968238484716366
plot_id,batch_id 0 62 miss% 0.04695131865089205
plot_id,batch_id 0 63 miss% 0.041768341046522925
plot_id,batch_id 0 64 miss% 0.05255229777513707
plot_id,batch_id 0 65 miss% 0.033719554175030565
plot_id,batch_id 0 66 miss% 0.11500820117368975
plot_id,batch_id 0 67 miss% 0.0397289451695245
plot_id,batch_id 0 68 miss% 0.024400384414207825the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  55697
Epoch:0, Train loss:0.408340, valid loss:0.378156
Epoch:1, Train loss:0.069304, valid loss:0.001936
Epoch:2, Train loss:0.003704, valid loss:0.001533
Epoch:3, Train loss:0.002928, valid loss:0.001259
Epoch:4, Train loss:0.002578, valid loss:0.001162
Epoch:5, Train loss:0.002456, valid loss:0.001296
Epoch:6, Train loss:0.002254, valid loss:0.001521
Epoch:7, Train loss:0.002094, valid loss:0.001037
Epoch:8, Train loss:0.002011, valid loss:0.000926
Epoch:9, Train loss:0.001892, valid loss:0.000869
Epoch:10, Train loss:0.001848, valid loss:0.001187
Epoch:11, Train loss:0.001345, valid loss:0.000710
Epoch:12, Train loss:0.001286, valid loss:0.000699
Epoch:13, Train loss:0.001263, valid loss:0.000671
Epoch:14, Train loss:0.001240, valid loss:0.000697
Epoch:15, Train loss:0.001203, valid loss:0.000697
Epoch:16, Train loss:0.001178, valid loss:0.000784
Epoch:17, Train loss:0.001146, valid loss:0.000618
Epoch:18, Train loss:0.001111, valid loss:0.000729
Epoch:19, Train loss:0.001119, valid loss:0.000717
Epoch:20, Train loss:0.001103, valid loss:0.000738
Epoch:21, Train loss:0.000849, valid loss:0.000560
Epoch:22, Train loss:0.000831, valid loss:0.000568
Epoch:23, Train loss:0.000815, valid loss:0.000543
Epoch:24, Train loss:0.000804, valid loss:0.000578
Epoch:25, Train loss:0.000806, valid loss:0.000553
Epoch:26, Train loss:0.000788, valid loss:0.000534
Epoch:27, Train loss:0.000780, valid loss:0.000561
Epoch:28, Train loss:0.000754, valid loss:0.000623
Epoch:29, Train loss:0.000768, valid loss:0.000581
Epoch:30, Train loss:0.000750, valid loss:0.000563
Epoch:31, Train loss:0.000626, valid loss:0.000510
Epoch:32, Train loss:0.000623, valid loss:0.000535
Epoch:33, Train loss:0.000614, valid loss:0.000523
Epoch:34, Train loss:0.000615, valid loss:0.000491
Epoch:35, Train loss:0.000609, valid loss:0.000538
Epoch:36, Train loss:0.000607, valid loss:0.000540
Epoch:37, Train loss:0.000590, valid loss:0.000512
Epoch:38, Train loss:0.000587, valid loss:0.000502
Epoch:39, Train loss:0.000594, valid loss:0.000535
Epoch:40, Train loss:0.000589, valid loss:0.000530
Epoch:41, Train loss:0.000522, valid loss:0.000512
Epoch:42, Train loss:0.000522, valid loss:0.000481
Epoch:43, Train loss:0.000519, valid loss:0.000493
Epoch:44, Train loss:0.000515, valid loss:0.000506
Epoch:45, Train loss:0.000514, valid loss:0.000498
Epoch:46, Train loss:0.000509, valid loss:0.000497
Epoch:47, Train loss:0.000507, valid loss:0.000507
Epoch:48, Train loss:0.000501, valid loss:0.000521
Epoch:49, Train loss:0.000501, valid loss:0.000522
Epoch:50, Train loss:0.000507, valid loss:0.000501
Epoch:51, Train loss:0.000469, valid loss:0.000486
Epoch:52, Train loss:0.000466, valid loss:0.000485
Epoch:53, Train loss:0.000465, valid loss:0.000483
Epoch:54, Train loss:0.000464, valid loss:0.000484
Epoch:55, Train loss:0.000463, valid loss:0.000483
Epoch:56, Train loss:0.000463, valid loss:0.000483
Epoch:57, Train loss:0.000462, valid loss:0.000483
Epoch:58, Train loss:0.000462, valid loss:0.000483
Epoch:59, Train loss:0.000462, valid loss:0.000482
Epoch:60, Train loss:0.000461, valid loss:0.000482
training time 16335.674560070038
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.073928858350923
plot_id,batch_id 0 1 miss% 0.05639860607289948
plot_id,batch_id 0 2 miss% 0.09199985488769702
plot_id,batch_id 0 3 miss% 0.05774948323298685
plot_id,batch_id 0 4 miss% 0.05991985055222999
plot_id,batch_id 0 5 miss% 0.11456694498466394
plot_id,batch_id 0 6 miss% 0.07581518539583988
plot_id,batch_id 0 7 miss% 0.07965060736312
plot_id,batch_id 0 8 miss% 0.09340094131726852
plot_id,batch_id 0 9 miss% 0.07340515336150209
plot_id,batch_id 0 10 miss% 0.0675642511880291
plot_id,batch_id 0 11 miss% 0.0670986241363136
plot_id,batch_id 0 12 miss% 0.10840053901320382
plot_id,batch_id 0 13 miss% 0.07364306243844795
plot_id,batch_id 0 14 miss% 0.10124884357752875
plot_id,batch_id 0 15 miss% 0.04726976932370951
plot_id,batch_id 0 16 miss% 0.05980301962258731
plot_id,batch_id 0 17 miss% 0.08125432289491782
plot_id,batch_id 0 18 miss% 0.0628031250995006
plot_id,batch_id 0 19 miss% 0.06946571090906847
plot_id,batch_id 0 20 miss% 0.05464258189100431
plot_id,batch_id 0 21 miss% 0.07687784689862939
plot_id,batch_id 0 22 miss% 0.04597657751660346
plot_id,batch_id 0 23 miss% 0.05711265294366151
plot_id,batch_id 0 24 miss% 0.030426391459329027
plot_id,batch_id 0 25 miss% 0.07842243229531468
plot_id,batch_id 0 26 miss% 0.05826711583060617
plot_id,batch_id 0 27 miss% 0.05522124532266835
plot_id,batch_id 0 28 miss% 0.03046492013789674
plot_id,batch_id 0 29 miss% 0.04041372481117547
plot_id,batch_id 0 30 miss% 0.03429757237738683
plot_id,batch_id 0 31 miss% 0.10333888897871468
plot_id,batch_id 0 32 miss% 0.14503162269645892
plot_id,batch_id 0 33 miss% 0.06578327980033719
plot_id,batch_id 0 34 miss% 0.06897220477964076
plot_id,batch_id 0 35 miss% 0.047821949471058445
plot_id,batch_id 0 36 miss% 0.14893378933430215
plot_id,batch_id 0 37 miss% 0.07418838439378156
plot_id,batch_id 0 38 miss% 0.16449407771697158
plot_id,batch_id 0 39 miss% 0.04937636861358735
plot_id,batch_id 0 40 miss% 0.1455921196680661
plot_id,batch_id 0 41 miss% 0.042631977714133636
plot_id,batch_id 0 42 miss% 0.032057083685738924
plot_id,batch_id 0 43 miss% 0.09760584576716749
plot_id,batch_id 0 44 miss% 0.03837287473364909
plot_id,batch_id 0 45 miss% 0.0428464023616857
plot_id,batch_id 0 46 miss% 0.03841109014610152
plot_id,batch_id 0 47 miss% 0.03983926056112205
plot_id,batch_id 0 48 miss% 0.06355348459928817
plot_id,batch_id 0 49 miss% 0.0583965401056328
plot_id,batch_id 0 50 miss% 0.17451014318595862
plot_id,batch_id 0 51 miss% 0.05100859432486525
plot_id,batch_id 0 52 miss% 0.04349661921372195
plot_id,batch_id 0 53 miss% 0.08355604503322801
plot_id,batch_id 0 54 miss% 0.04483511066471134
plot_id,batch_id 0 55 miss% 0.07823526312311588
plot_id,batch_id 0 56 miss% 0.08286106559990541
plot_id,batch_id 0 57 miss% 0.07521321631178964
plot_id,batch_id 0 58 miss% 0.05131927437877645
plot_id,batch_id 0 59 miss% 0.05048907329116037
plot_id,batch_id 0 60 miss% 0.03890467254239852
plot_id,batch_id 0 61 miss% 0.04118875818604554
plot_id,batch_id 0 62 miss% 0.07832240801825643
plot_id,batch_id 0 63 miss% 0.06208689885242222
plot_id,batch_id 0 64 miss% 0.04680555582208025
plot_id,batch_id 0 65 miss% 0.09970928272895542
plot_id,batch_id 0 66 miss% 0.06116030026105857
plot_id,batch_id 0 67 miss% 0.04180693703768105

plot_id,batch_id 0 69 miss% 0.11952063617998374
plot_id,batch_id 0 70 miss% 0.05205129943517237
plot_id,batch_id 0 71 miss% 0.026514833410151224
plot_id,batch_id 0 72 miss% 0.08991376678466684
plot_id,batch_id 0 73 miss% 0.04529125803200546
plot_id,batch_id 0 74 miss% 0.11593912200022918
plot_id,batch_id 0 75 miss% 0.09155425528701139
plot_id,batch_id 0 76 miss% 0.055526048039544695
plot_id,batch_id 0 77 miss% 0.04893974591487966
plot_id,batch_id 0 78 miss% 0.01839216787367238
plot_id,batch_id 0 79 miss% 0.12267529908234931
plot_id,batch_id 0 80 miss% 0.04233647222684538
plot_id,batch_id 0 81 miss% 0.0646386485235318
plot_id,batch_id 0 82 miss% 0.04569906603404289
plot_id,batch_id 0 83 miss% 0.06670246089382159
plot_id,batch_id 0 84 miss% 0.07755893148298683
plot_id,batch_id 0 85 miss% 0.03729465086959291
plot_id,batch_id 0 86 miss% 0.03028177298648983
plot_id,batch_id 0 87 miss% 0.07793509182370845
plot_id,batch_id 0 88 miss% 0.050548138333225225
plot_id,batch_id 0 89 miss% 0.04766468910480664
plot_id,batch_id 0 90 miss% 0.04230443844812456
plot_id,batch_id 0 91 miss% 0.04660818008163341
plot_id,batch_id 0 92 miss% 0.05420158387045861
plot_id,batch_id 0 93 miss% 0.04324692813099181
plot_id,batch_id 0 94 miss% 0.060268779394806016
plot_id,batch_id 0 95 miss% 0.06920655204925041
plot_id,batch_id 0 96 miss% 0.04495807773309215
plot_id,batch_id 0 97 miss% 0.03414896308067886
plot_id,batch_id 0 98 miss% 0.03938809222956225
plot_id,batch_id 0 99 miss% 0.04764729337618389
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08728093 0.05477558 0.10394829 0.03532622 0.04314495 0.04777067
 0.08242568 0.07061745 0.06754431 0.05206002 0.03068753 0.06429895
 0.07659018 0.06269546 0.05835259 0.02702878 0.05805357 0.06155713
 0.07839474 0.08537432 0.04321139 0.02755023 0.05716313 0.03166418
 0.05741501 0.04638795 0.04475874 0.05372699 0.04426683 0.04137424
 0.03248162 0.07960039 0.07033218 0.07003109 0.0440392  0.04918056
 0.04084482 0.06445778 0.04100952 0.03764393 0.04830519 0.0646282
 0.03736532 0.0653256  0.02140389 0.04027469 0.04768645 0.04355854
 0.0424087  0.0297833  0.12701364 0.02891346 0.03544678 0.02934473
 0.04015459 0.08539807 0.08083227 0.02973757 0.04157811 0.04232741
 0.02687654 0.04968238 0.04695132 0.04176834 0.0525523  0.03371955
 0.1150082  0.03972895 0.02440038 0.11952064 0.0520513  0.02651483
 0.08991377 0.04529126 0.11593912 0.09155426 0.05552605 0.04893975
 0.01839217 0.1226753  0.04233647 0.06463865 0.04569907 0.06670246
 0.07755893 0.03729465 0.03028177 0.07793509 0.05054814 0.04766469
 0.04230444 0.04660818 0.05420158 0.04324693 0.06026878 0.06920655
 0.04495808 0.03414896 0.03938809 0.04764729]
for model  131 the mean error 0.054461988484795804
all id 131 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 25 out win 6 err 0.054461988484795804 time 16298.156789064407
Launcher: Job 132 completed in 16560 seconds.
Launcher: Task 137 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  151697
Epoch:0, Train loss:0.538751, valid loss:0.495115
Epoch:1, Train loss:0.364820, valid loss:0.364844
Epoch:2, Train loss:0.356677, valid loss:0.363946
Epoch:3, Train loss:0.355712, valid loss:0.363627
Epoch:4, Train loss:0.355277, valid loss:0.363629
Epoch:5, Train loss:0.354951, valid loss:0.363654
Epoch:6, Train loss:0.354776, valid loss:0.363319
Epoch:7, Train loss:0.354628, valid loss:0.363443
Epoch:8, Train loss:0.354449, valid loss:0.363173
Epoch:9, Train loss:0.354348, valid loss:0.363236
Epoch:10, Train loss:0.354293, valid loss:0.363306
Epoch:11, Train loss:0.353819, valid loss:0.362961
Epoch:12, Train loss:0.353774, valid loss:0.362952
Epoch:13, Train loss:0.353748, valid loss:0.362944
Epoch:14, Train loss:0.353692, valid loss:0.363004
Epoch:15, Train loss:0.353678, valid loss:0.362922
Epoch:16, Train loss:0.353640, valid loss:0.362981
Epoch:17, Train loss:0.353625, valid loss:0.362964
Epoch:18, Train loss:0.353581, valid loss:0.363038
Epoch:19, Train loss:0.353560, valid loss:0.362947
Epoch:20, Train loss:0.353539, valid loss:0.362884
Epoch:21, Train loss:0.353314, valid loss:0.362920
Epoch:22, Train loss:0.353296, valid loss:0.362831
Epoch:23, Train loss:0.353269, valid loss:0.362830
Epoch:24, Train loss:0.353278, valid loss:0.362817
Epoch:25, Train loss:0.353262, valid loss:0.362894
Epoch:26, Train loss:0.353266, valid loss:0.362826
Epoch:27, Train loss:0.353251, valid loss:0.362839
Epoch:28, Train loss:0.353224, valid loss:0.362870
Epoch:29, Train loss:0.353253, valid loss:0.362929
Epoch:30, Train loss:0.353224, valid loss:0.362797
Epoch:31, Train loss:0.353107, valid loss:0.362740
Epoch:32, Train loss:0.353095, valid loss:0.362758
Epoch:33, Train loss:0.353098, valid loss:0.362853
Epoch:34, Train loss:0.353095, valid loss:0.362786
Epoch:35, Train loss:0.353098, valid loss:0.362774
Epoch:36, Train loss:0.353084, valid loss:0.362809
Epoch:37, Train loss:0.353078, valid loss:0.362774
Epoch:38, Train loss:0.353078, valid loss:0.362771
Epoch:39, Train loss:0.353074, valid loss:0.362757
Epoch:40, Train loss:0.353067, valid loss:0.362761
Epoch:41, Train loss:0.353019, valid loss:0.362761
Epoch:42, Train loss:0.353018, valid loss:0.362766
Epoch:43, Train loss:0.353014, valid loss:0.362765
Epoch:44, Train loss:0.353012, valid loss:0.362762
Epoch:45, Train loss:0.353011, valid loss:0.362757
Epoch:46, Train loss:0.353005, valid loss:0.362744
Epoch:47, Train loss:0.353000, valid loss:0.362744
Epoch:48, Train loss:0.353002, valid loss:0.362747
Epoch:49, Train loss:0.352996, valid loss:0.362761
Epoch:50, Train loss:0.353004, valid loss:0.362745
Epoch:51, Train loss:0.352982, valid loss:0.362746
Epoch:52, Train loss:0.352978, valid loss:0.362745
Epoch:53, Train loss:0.352977, valid loss:0.362742
Epoch:54, Train loss:0.352976, valid loss:0.362741
Epoch:55, Train loss:0.352975, valid loss:0.362743
Epoch:56, Train loss:0.352974, valid loss:0.362742
Epoch:57, Train loss:0.352974, valid loss:0.362739
Epoch:58, Train loss:0.352973, valid loss:0.362739
Epoch:59, Train loss:0.352973, valid loss:0.362739
Epoch:60, Train loss:0.352973, valid loss:0.362740
training time 16447.366103887558
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.8963292154163509
plot_id,batch_id 0 1 miss% 0.8658670332396707
plot_id,batch_id 0 2 miss% 0.858331734075538
plot_id,batch_id 0 3 miss% 0.8551809816724846
plot_id,batch_id 0 4 miss% 0.855399367862435
plot_id,batch_id 0 5 miss% 0.8942233316514704
plot_id,batch_id 0 6 miss% 0.8673270377723504
plot_id,batch_id 0 7 miss% 0.8634847387057771
plot_id,batch_id 0 8 miss% 0.8550239349317099
plot_id,batch_id 0 9 miss% 0.8569214419199903
plot_id,batch_id 0 10 miss% 0.901023490829774
plot_id,batch_id 0 11 miss% 0.8718577880822649
plot_id,batch_id 0 12 miss% 0.8607454947259304
plot_id,batch_id 0 13 miss% 0.8540876769996536
plot_id,batch_id 0 14 miss% 0.8623830554334588
plot_id,batch_id 0 15 miss% 0.9055751522061886
plot_id,batch_id 0 16 miss% 0.8832747859273912
plot_id,batch_id 0 17 miss% 0.8631453187624933
plot_id,batch_id 0 18 miss% 0.8638721021359225
plot_id,batch_id 0 19 miss% 0.8621049624792607
plot_id,batch_id 0 20 miss% 0.8812910222013931
plot_id,batch_id 0 21 miss% 0.8603822681645676
plot_id,batch_id 0 22 miss% 0.8551178066004596
plot_id,batch_id 0 23 miss% 0.8522703676441695
plot_id,batch_id 0 24 miss% 0.8506892873753465
plot_id,batch_id 0 25 miss% 0.880599510796659
plot_id,batch_id 0 26 miss% 0.8628165128089229
plot_id,batch_id 0 27 miss% 0.8572166998964318
plot_id,batch_id 0 28 miss% 0.853791438203236
plot_id,batch_id 0 29 miss% 0.8528578177394376
plot_id,batch_id 0 30 miss% 0.8832618288844509
plot_id,batch_id 0 31 miss% 0.8664936341015672
plot_id,batch_id 0 32 miss% 0.8637257675861258
plot_id,batch_id 0 33 miss% 0.8581664280004418
plot_id,batch_id 0 34 miss% 0.8575884071324335
plot_id,batch_id 0 35 miss% 0.8877493797618119
plot_id,batch_id 0 36 miss% 0.8613693650931086
plot_id,batch_id 0 37 miss% 0.8606284357361426
plot_id,batch_id 0 38 miss% 0.857344002993424
plot_id,batch_id 0 39 miss% 0.8554287812905548
plot_id,batch_id 0 40 miss% 0.8747149972240915
plot_id,batch_id 0 41 miss% 0.8586311574197802
plot_id,batch_id 0 42 miss% 0.8512153697749163
plot_id,batch_id 0 43 miss% 0.8516564646398933
plot_id,batch_id 0 44 miss% 0.8493371798389187
plot_id,batch_id 0 45 miss% 0.8654597393888392
plot_id,batch_id 0 46 miss% 0.8567761507510471
plot_id,batch_id 0 47 miss% 0.8543460569164921
plot_id,batch_id 0 48 miss% 0.8512328620296397
plot_id,batch_id 0 49 miss% 0.8494166549404311
plot_id,batch_id 0 50 miss% 0.8712291842702775
plot_id,batch_id 0 51 miss% 0.8619247898101812
plot_id,batch_id 0 52 miss% 0.8556146588189328
plot_id,batch_id 0 53 miss% 0.851042153092169
plot_id,batch_id 0 54 miss% 0.8535151674307712
plot_id,batch_id 0 55 miss% 0.8745418224765497
plot_id,batch_id 0 56 miss% 0.8621555328754007
plot_id,batch_id 0 57 miss% 0.8563883115705117
plot_id,batch_id 0 58 miss% 0.8534414881692668
plot_id,batch_id 0 59 miss% 0.8539872498725335
plot_id,batch_id 0 60 miss% 0.9202501426435999
plot_id,batch_id 0 61 miss% 0.8818109409739975
plot_id,batch_id 0 62 miss% 0.8682834481343215
plot_id,batch_id 0 63 miss% 0.8639725732298094
plot_id,batch_id 0 64 miss% 0.861791769960798
plot_id,batch_id 0 65 miss% 0.9288833489987965
plot_id,batch_id 0 66 miss% 0.8833743756772626
plot_id,batch_id 0 67 miss% 0.8711691090017588
plot_id,batch_id 0 68 miss% 0.8681598283012487
plot_id,batch_id 0 69 miss% 0.8597475798668603
plot_id,batch_id 0 70 miss% 0.9268355102696423
plot_id,batch_id 0 71 miss% 0.9082622296615173
plot_id,batch_id 0 72 miss% 0.8736899122127676
plot_id,batch_id 0 73 miss% 0.8684390040336046
plot_id,batch_id 0 74 miss% 0.8688636218368768
plot_id,batch_id 0 75 miss% 0.9102083462007315
plot_id,batch_id 0 76 miss% 0.8847163144892581
plot_id,batch_id 0 77 miss% 0.8740772172034955
plot_id,batch_id 0 78 miss% 0.8675044324239036
plot_id,batch_id 0 79 miss% 0.8666327245774235
plot_id,batch_id 0 80 miss% 0.9130422683908446
plot_id,batch_id 0 81 miss% 0.8747348508957665
plot_id,batch_id 0 82 miss% 0.8657453617894363
plot_id,batch_id 0 83 miss% 0.8611900788062823
plot_id,batch_id 0 84 miss% 0.8579908061262737
plot_id,batch_id 0 85 miss% 0.9057782830092954
plot_id,batch_id 0 86 miss% 0.8760518409901427
plot_id,batch_id 0 87 miss% 0.8666972393603726
plot_id,batch_id 0 88 miss% 0.8590231946572983
plot_id,batch_id 0 89 miss% 0.8587759245544448
plot_id,batch_id 0 90 miss% 0.9233899430886708
plot_id,batch_id 0 91 miss% 0.8797335920087764
plot_id,batch_id 0 92 miss% 0.8703363065942039
plot_id,batch_id 0 93 miss% 0.8657543863842446
plot_id,batch_id 0 94 miss% 0.8650920675910275
plot_id,batch_id 0 95 miss% 0.9148168724783564
plot_id,batch_id 0 96 miss% 0.8975688462051573
plot_id,batch_id 0 97 miss% 0.8740934813364605
plot_id,batch_id 0 98 miss% 0.8707694456511194
plot_id,batch_id 0 99 miss% 0.8583903975684233
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.89632922 0.86586703 0.85833173 0.85518098 0.85539937 0.89422333
 0.86732704 0.86348474 0.85502393 0.85692144 0.90102349 0.87185779
 0.86074549 0.85408768 0.86238306 0.90557515 0.88327479 0.86314532
 0.8638721  0.86210496 0.88129102 0.86038227 0.85511781 0.85227037
 0.85068929 0.88059951 0.86281651 0.8572167  0.85379144 0.85285782
 0.88326183 0.86649363 0.86372577 0.85816643 0.85758841 0.88774938
 0.86136937 0.86062844 0.857344   0.85542878 0.874715   0.85863116
 0.85121537 0.85165646 0.84933718 0.86545974 0.85677615 0.85434606
 0.85123286 0.84941665 0.87122918 0.86192479 0.85561466 0.85104215
 0.85351517 0.87454182 0.86215553 0.85638831 0.85344149 0.85398725
 0.92025014 0.88181094 0.86828345 0.86397257 0.86179177 0.92888335
 0.88337438 0.87116911 0.86815983 0.85974758 0.92683551 0.90826223
 0.87368991 0.868439   0.86886362 0.91020835 0.88471631 0.87407722
 0.86750443 0.86663272 0.91304227 0.87473485 0.86574536 0.86119008
 0.85799081 0.90577828 0.87605184 0.86669724 0.85902319 0.85877592
 0.92338994 0.87973359 0.87033631 0.86575439 0.86509207 0.91481687
 0.89756885 0.87409348 0.87076945 0.8583904 ]
for model  129 the mean error 0.8702722201533571
all id 129 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 25 out win 4 err 0.8702722201533571 time 16447.366103887558
Launcher: Job 130 completed in 16573 seconds.
Launcher: Task 127 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  154129
Epoch:0, Train loss:0.542910, valid loss:0.547726
Epoch:1, Train loss:0.102368, valid loss:0.005143
Epoch:2, Train loss:0.014270, valid loss:0.004327
Epoch:3, Train loss:0.011439, valid loss:0.003519
Epoch:4, Train loss:0.006970, valid loss:0.002819
Epoch:5, Train loss:0.005089, valid loss:0.001678
Epoch:6, Train loss:0.003634, valid loss:0.001543
Epoch:7, Train loss:0.003223, valid loss:0.001669
Epoch:8, Train loss:0.002967, valid loss:0.001576
Epoch:9, Train loss:0.002742, valid loss:0.001578
Epoch:10, Train loss:0.002580, valid loss:0.001211
Epoch:11, Train loss:0.001977, valid loss:0.001053
Epoch:12, Train loss:0.001828, valid loss:0.001055
Epoch:13, Train loss:0.001768, valid loss:0.000943
Epoch:14, Train loss:0.001663, valid loss:0.000988
Epoch:15, Train loss:0.001646, valid loss:0.000954
Epoch:16, Train loss:0.001539, valid loss:0.000931
Epoch:17, Train loss:0.001490, valid loss:0.000864
Epoch:18, Train loss:0.001414, valid loss:0.000906
Epoch:19, Train loss:0.001439, valid loss:0.000840
Epoch:20, Train loss:0.001340, valid loss:0.000852
Epoch:21, Train loss:0.001100, valid loss:0.000769
Epoch:22, Train loss:0.001044, valid loss:0.000760
Epoch:23, Train loss:0.001027, valid loss:0.000759
Epoch:24, Train loss:0.001005, valid loss:0.000763
Epoch:25, Train loss:0.000990, valid loss:0.000724
Epoch:26, Train loss:0.000957, valid loss:0.000731
Epoch:27, Train loss:0.000963, valid loss:0.000678
Epoch:28, Train loss:0.000946, valid loss:0.000707
Epoch:29, Train loss:0.000915, valid loss:0.000713
Epoch:30, Train loss:0.000866, valid loss:0.000681
Epoch:31, Train loss:0.000750, valid loss:0.000702
Epoch:32, Train loss:0.000748, valid loss:0.000689
Epoch:33, Train loss:0.000738, valid loss:0.000660
Epoch:34, Train loss:0.000722, valid loss:0.000667
Epoch:35, Train loss:0.000728, valid loss:0.000670
Epoch:36, Train loss:0.000716, valid loss:0.000656
Epoch:37, Train loss:0.000719, valid loss:0.000670
Epoch:38, Train loss:0.000689, valid loss:0.000673
Epoch:39, Train loss:0.000688, valid loss:0.000628
Epoch:40, Train loss:0.000675, valid loss:0.000634
Epoch:41, Train loss:0.000619, valid loss:0.000621
Epoch:42, Train loss:0.000605, valid loss:0.000617
Epoch:43, Train loss:0.000605, valid loss:0.000601
Epoch:44, Train loss:0.000598, valid loss:0.000626
Epoch:45, Train loss:0.000600, valid loss:0.000616
Epoch:46, Train loss:0.000595, valid loss:0.000628
Epoch:47, Train loss:0.000585, valid loss:0.000658
Epoch:48, Train loss:0.000586, valid loss:0.000603
Epoch:49, Train loss:0.000582, valid loss:0.000625
Epoch:50, Train loss:0.000578, valid loss:0.000625
Epoch:51, Train loss:0.000538, valid loss:0.000609
Epoch:52, Train loss:0.000533, valid loss:0.000613
Epoch:53, Train loss:0.000531, valid loss:0.000614
Epoch:54, Train loss:0.000529, valid loss:0.000610
Epoch:55, Train loss:0.000529, valid loss:0.000605
Epoch:56, Train loss:0.000528, valid loss:0.000603
Epoch:57, Train loss:0.000527, valid loss:0.000606
Epoch:58, Train loss:0.000527, valid loss:0.000611
Epoch:59, Train loss:0.000526, valid loss:0.000608
Epoch:60, Train loss:0.000526, valid loss:0.000604
training time 16361.274114847183
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07565899982565065
plot_id,batch_id 0 1 miss% 0.06102272697639682
plot_id,batch_id 0 2 miss% 0.07228138626660763
plot_id,batch_id 0 3 miss% 0.07986220810992811
plot_id,batch_id 0 4 miss% 0.0438494195403534
plot_id,batch_id 0 5 miss% 0.0461261496967078
plot_id,batch_id 0 6 miss% 0.03389277034046248
plot_id,batch_id 0 7 miss% 0.08810772751779843
plot_id,batch_id 0 8 miss% 0.08205966304214218
plot_id,batch_id 0 9 miss% 0.0460273444391159
plot_id,batch_id 0 10 miss% 0.03040042886158092
plot_id,batch_id 0 11 miss% 0.08229715060630757
plot_id,batch_id 0 12 miss% 0.06955124598239937
plot_id,batch_id 0 13 miss% 0.11523355840284122
plot_id,batch_id 0 14 miss% 0.09669116089305536
plot_id,batch_id 0 15 miss% 0.036849378833412536
plot_id,batch_id 0 16 miss% 0.09950666836821605
plot_id,batch_id 0 17 miss% 0.0653620751961429
plot_id,batch_id 0 18 miss% 0.0939064118020494
plot_id,batch_id 0 19 miss% 0.11608288024410887
plot_id,batch_id 0 20 miss% 0.04481607933018621
plot_id,batch_id 0 21 miss% 0.09474008156928972
plot_id,batch_id 0 22 miss% 0.04084860322059286
plot_id,batch_id 0 23 miss% 0.032385122943291
plot_id,batch_id 0 24 miss% 0.038590536548502814
plot_id,batch_id 0 25 miss% 0.047149893115825424
plot_id,batch_id 0 26 miss% 0.05923978578581426
plot_id,batch_id 0 27 miss% 0.06579501166795719
plot_id,batch_id 0 28 miss% 0.04501354756586076
plot_id,batch_id 0 29 miss% 0.027783657208863312
plot_id,batch_id 0 30 miss% 0.03114280391775368
plot_id,batch_id 0 31 miss% 0.11852633650813002
plot_id,batch_id 0 32 miss% 0.12947947996297396
plot_id,batch_id 0 33 miss% 0.05493417838630442
plot_id,batch_id 0 34 miss% 0.027499311798754013
plot_id,batch_id 0 35 miss% 0.06499442462593714
plot_id,batch_id 0 36 miss% 0.09315769279028369
plot_id,batch_id 0 37 miss% 0.09204158712486137
plot_id,batch_id 0 38 miss% 0.03575379982505102
plot_id,batch_id 0 39 miss% 0.03332564314558219
plot_id,batch_id 0 40 miss% 0.06520203135497514
plot_id,batch_id 0 41 miss% 0.03648836577869208
plot_id,batch_id 0 42 miss% 0.06003501224176596
plot_id,batch_id 0 43 miss% 0.04905379962632957
plot_id,batch_id 0 44 miss% 0.047843278510772996
plot_id,batch_id 0 45 miss% 0.03641888810223854
plot_id,batch_id 0 46 miss% 0.020068769417176134
plot_id,batch_id 0 47 miss% 0.019993666774075367
plot_id,batch_id 0 48 miss% 0.02754731668788843
plot_id,batch_id 0 49 miss% 0.05915603378575126
plot_id,batch_id 0 50 miss% 0.1833858006085705
plot_id,batch_id 0 51 miss% 0.03331183466692204
plot_id,batch_id 0 52 miss% 0.027586468639648955
plot_id,batch_id 0 53 miss% 0.06193256796622443
plot_id,batch_id 0 54 miss% 0.03615570699307461
plot_id,batch_id 0 55 miss% 0.059153591026487456
plot_id,batch_id 0 56 miss% 0.06551663872052377
plot_id,batch_id 0 57 miss% 0.02953058695178169
plot_id,batch_id 0 58 miss% 0.048195036595400385
plot_id,batch_id 0 59 miss% 0.028098834063120103
plot_id,batch_id 0 60 miss% 0.06657139994639756
plot_id,batch_id 0 61 miss% 0.06113450014052405
plot_id,batch_id 0 62 miss% 0.06034457183348924
plot_id,batch_id 0 63 miss% 0.04240274255326931
plot_id,batch_id 0 64 miss% 0.052710004887284535
plot_id,batch_id 0 65 miss% 0.08428495618772945
plot_id,batch_id 0 66 miss% 0.11740200133800516
plot_id,batch_id 0 67 miss% 0.03041214008983802
plot_id,batch_id 0 68 miss% 0.029718325622542945
plot_id,batch_id 0 68 miss% 0.04912460410695519
plot_id,batch_id 0 69 miss% 0.07377407311042862
plot_id,batch_id 0 70 miss% 0.0449468439716105
plot_id,batch_id 0 71 miss% 0.03291206346512917
plot_id,batch_id 0 72 miss% 0.08651771642285819
plot_id,batch_id 0 73 miss% 0.0767508933559832
plot_id,batch_id 0 74 miss% 0.15439219901434817
plot_id,batch_id 0 75 miss% 0.10249688337005211
plot_id,batch_id 0 76 miss% 0.13594136819108887
plot_id,batch_id 0 77 miss% 0.033351233038641195
plot_id,batch_id 0 78 miss% 0.04305561129492479
plot_id,batch_id 0 79 miss% 0.07148920449440345
plot_id,batch_id 0 80 miss% 0.07724096965397667
plot_id,batch_id 0 81 miss% 0.07321497658433536
plot_id,batch_id 0 82 miss% 0.06461137694239684
plot_id,batch_id 0 83 miss% 0.07968096509235781
plot_id,batch_id 0 84 miss% 0.06783736388399451
plot_id,batch_id 0 85 miss% 0.04001753367357986
plot_id,batch_id 0 86 miss% 0.045629267586603024
plot_id,batch_id 0 87 miss% 0.05260982132910035
plot_id,batch_id 0 88 miss% 0.06665902668763204
plot_id,batch_id 0 89 miss% 0.08194970612239277
plot_id,batch_id 0 90 miss% 0.03523514641071346
plot_id,batch_id 0 91 miss% 0.07129865259884698
plot_id,batch_id 0 92 miss% 0.07993852967278484
plot_id,batch_id 0 93 miss% 0.09255454078766422
plot_id,batch_id 0 94 miss% 0.08079666959290802
plot_id,batch_id 0 95 miss% 0.15403623654157456
plot_id,batch_id 0 96 miss% 0.11993021754258891
plot_id,batch_id 0 97 miss% 0.028247293516437587
plot_id,batch_id 0 98 miss% 0.0468263287247651
plot_id,batch_id 0 99 miss% 0.1320670669954019
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07392886 0.05639861 0.09199985 0.05774948 0.05991985 0.11456694
 0.07581519 0.07965061 0.09340094 0.07340515 0.06756425 0.06709862
 0.10840054 0.07364306 0.10124884 0.04726977 0.05980302 0.08125432
 0.06280313 0.06946571 0.05464258 0.07687785 0.04597658 0.05711265
 0.03042639 0.07842243 0.05826712 0.05522125 0.03046492 0.04041372
 0.03429757 0.10333889 0.14503162 0.06578328 0.0689722  0.04782195
 0.14893379 0.07418838 0.16449408 0.04937637 0.14559212 0.04263198
 0.03205708 0.09760585 0.03837287 0.0428464  0.03841109 0.03983926
 0.06355348 0.05839654 0.17451014 0.05100859 0.04349662 0.08355605
 0.04483511 0.07823526 0.08286107 0.07521322 0.05131927 0.05048907
 0.03890467 0.04118876 0.07832241 0.0620869  0.04680556 0.09970928
 0.0611603  0.04180694 0.0491246  0.07377407 0.04494684 0.03291206
 0.08651772 0.07675089 0.1543922  0.10249688 0.13594137 0.03335123
 0.04305561 0.0714892  0.07724097 0.07321498 0.06461138 0.07968097
 0.06783736 0.04001753 0.04562927 0.05260982 0.06665903 0.08194971
 0.03523515 0.07129865 0.07993853 0.09255454 0.08079667 0.15403624
 0.11993022 0.02824729 0.04682633 0.13206707]
for model  227 the mean error 0.07145400662686761
all id 227 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 31 out win 6 err 0.07145400662686761 time 16335.674560070038
Launcher: Job 228 completed in 16593 seconds.
Launcher: Task 24 done. Exiting.
plot_id,batch_id 0 69 miss% 0.09036102235974851
plot_id,batch_id 0 70 miss% 0.09488779348935182
plot_id,batch_id 0 71 miss% 0.04386370776186828
plot_id,batch_id 0 72 miss% 0.10161788746775165
plot_id,batch_id 0 73 miss% 0.07474376657964442
plot_id,batch_id 0 74 miss% 0.06941404706738528
plot_id,batch_id 0 75 miss% 0.01950200479214631
plot_id,batch_id 0 76 miss% 0.06526034500016667
plot_id,batch_id 0 77 miss% 0.03948036333635202
plot_id,batch_id 0 78 miss% 0.03609217601912938
plot_id,batch_id 0 79 miss% 0.030462239068405454
plot_id,batch_id 0 80 miss% 0.09982981840940583
plot_id,batch_id 0 81 miss% 0.09749812227001212
plot_id,batch_id 0 82 miss% 0.04302055175890718
plot_id,batch_id 0 83 miss% 0.06769256442258766
plot_id,batch_id 0 84 miss% 0.08983888998576062
plot_id,batch_id 0 85 miss% 0.048858807186086844
plot_id,batch_id 0 86 miss% 0.049988922119371226
plot_id,batch_id 0 87 miss% 0.08379113344335762
plot_id,batch_id 0 88 miss% 0.10776942639894901
plot_id,batch_id 0 89 miss% 0.07600750562075773
plot_id,batch_id 0 90 miss% 0.020562065326229558
plot_id,batch_id 0 91 miss% 0.061170984806683606
plot_id,batch_id 0 92 miss% 0.043557830100551746
plot_id,batch_id 0 93 miss% 0.049892879200671814
plot_id,batch_id 0 94 miss% 0.06772781276244806
plot_id,batch_id 0 95 miss% 0.05669179587723257
plot_id,batch_id 0 96 miss% 0.07177909970336514
plot_id,batch_id 0 97 miss% 0.06102839410842248
plot_id,batch_id 0 98 miss% 0.04979242306458634
plot_id,batch_id 0 99 miss% 0.0569705911503929
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.075659   0.06102273 0.07228139 0.07986221 0.04384942 0.04612615
 0.03389277 0.08810773 0.08205966 0.04602734 0.03040043 0.08229715
 0.06955125 0.11523356 0.09669116 0.03684938 0.09950667 0.06536208
 0.09390641 0.11608288 0.04481608 0.09474008 0.0408486  0.03238512
 0.03859054 0.04714989 0.05923979 0.06579501 0.04501355 0.02778366
 0.0311428  0.11852634 0.12947948 0.05493418 0.02749931 0.06499442
 0.09315769 0.09204159 0.0357538  0.03332564 0.06520203 0.03648837
 0.06003501 0.0490538  0.04784328 0.03641889 0.02006877 0.01999367
 0.02754732 0.05915603 0.1833858  0.03331183 0.02758647 0.06193257
 0.03615571 0.05915359 0.06551664 0.02953059 0.04819504 0.02809883
 0.0665714  0.0611345  0.06034457 0.04240274 0.05271    0.08428496
 0.117402   0.03041214 0.02971833 0.09036102 0.09488779 0.04386371
 0.10161789 0.07474377 0.06941405 0.019502   0.06526035 0.03948036
 0.03609218 0.03046224 0.09982982 0.09749812 0.04302055 0.06769256
 0.08983889 0.04885881 0.04998892 0.08379113 0.10776943 0.07600751
 0.02056207 0.06117098 0.04355783 0.04989288 0.06772781 0.0566918
 0.0717791  0.06102839 0.04979242 0.05697059]
for model  89 the mean error 0.06120796773755322
all id 89 hidden_dim 32 learning_rate 0.0025 num_layers 3 frames 25 out win 6 err 0.06120796773755322 time 16361.274114847183
Launcher: Job 90 completed in 16625 seconds.
Launcher: Task 99 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  209681
Epoch:0, Train loss:0.455640, valid loss:0.459873
Epoch:1, Train loss:0.037023, valid loss:0.004991
Epoch:2, Train loss:0.011847, valid loss:0.003662
Epoch:3, Train loss:0.008686, valid loss:0.002983
Epoch:4, Train loss:0.005972, valid loss:0.001558
Epoch:5, Train loss:0.003212, valid loss:0.001362
Epoch:6, Train loss:0.002725, valid loss:0.001605
Epoch:7, Train loss:0.002465, valid loss:0.001179
Epoch:8, Train loss:0.002280, valid loss:0.001204
Epoch:9, Train loss:0.002052, valid loss:0.001140
Epoch:10, Train loss:0.001961, valid loss:0.001196
Epoch:11, Train loss:0.001421, valid loss:0.000871
Epoch:12, Train loss:0.001357, valid loss:0.000907
Epoch:13, Train loss:0.001338, valid loss:0.000892
Epoch:14, Train loss:0.001273, valid loss:0.000865
Epoch:15, Train loss:0.001233, valid loss:0.000802
Epoch:16, Train loss:0.001192, valid loss:0.000758
Epoch:17, Train loss:0.001178, valid loss:0.000812
Epoch:18, Train loss:0.001147, valid loss:0.000959
Epoch:19, Train loss:0.001099, valid loss:0.000760
Epoch:20, Train loss:0.001026, valid loss:0.000722
Epoch:21, Train loss:0.000827, valid loss:0.000670
Epoch:22, Train loss:0.000770, valid loss:0.000619
Epoch:23, Train loss:0.000786, valid loss:0.000670
Epoch:24, Train loss:0.000809, valid loss:0.000579
Epoch:25, Train loss:0.000750, valid loss:0.000707
Epoch:26, Train loss:0.000738, valid loss:0.000547
Epoch:27, Train loss:0.000705, valid loss:0.000638
Epoch:28, Train loss:0.000718, valid loss:0.000717
Epoch:29, Train loss:0.000713, valid loss:0.000636
Epoch:30, Train loss:0.000687, valid loss:0.000673
Epoch:31, Train loss:0.000576, valid loss:0.000596
Epoch:32, Train loss:0.000574, valid loss:0.000599
Epoch:33, Train loss:0.000561, valid loss:0.000532
Epoch:34, Train loss:0.000558, valid loss:0.000556
Epoch:35, Train loss:0.000546, valid loss:0.000526
Epoch:36, Train loss:0.000538, valid loss:0.000557
Epoch:37, Train loss:0.000545, valid loss:0.000569
Epoch:38, Train loss:0.000531, valid loss:0.000522
Epoch:39, Train loss:0.000527, valid loss:0.000553
Epoch:40, Train loss:0.000531, valid loss:0.000539
Epoch:41, Train loss:0.000472, valid loss:0.000515
Epoch:42, Train loss:0.000460, valid loss:0.000518
Epoch:43, Train loss:0.000460, valid loss:0.000557
Epoch:44, Train loss:0.000458, valid loss:0.000517
Epoch:45, Train loss:0.000452, valid loss:0.000554
Epoch:46, Train loss:0.000452, valid loss:0.000514
Epoch:47, Train loss:0.000456, valid loss:0.000514
Epoch:48, Train loss:0.000449, valid loss:0.000504
Epoch:49, Train loss:0.000439, valid loss:0.000508
Epoch:50, Train loss:0.000447, valid loss:0.000490
Epoch:51, Train loss:0.000430, valid loss:0.000489
Epoch:52, Train loss:0.000424, valid loss:0.000491
Epoch:53, Train loss:0.000423, valid loss:0.000499
Epoch:54, Train loss:0.000421, valid loss:0.000496
Epoch:55, Train loss:0.000421, valid loss:0.000491
Epoch:56, Train loss:0.000419, valid loss:0.000497
Epoch:57, Train loss:0.000417, valid loss:0.000497
Epoch:58, Train loss:0.000416, valid loss:0.000490
Epoch:59, Train loss:0.000418, valid loss:0.000504
Epoch:60, Train loss:0.000414, valid loss:0.000494
training time 16433.84038209915
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.05938108936742655
plot_id,batch_id 0 1 miss% 0.04098003318560028
plot_id,batch_id 0 2 miss% 0.0904469822253886
plot_id,batch_id 0 3 miss% 0.0590245071774477
plot_id,batch_id 0 4 miss% 0.03515689374631208
plot_id,batch_id 0 5 miss% 0.0350004502984464
plot_id,batch_id 0 6 miss% 0.07197350524035893
plot_id,batch_id 0 7 miss% 0.11258545865174198
plot_id,batch_id 0 8 miss% 0.06832981001161223
plot_id,batch_id 0 9 miss% 0.021600451366524106
plot_id,batch_id 0 10 miss% 0.02217659442621602
plot_id,batch_id 0 11 miss% 0.06464590925982773
plot_id,batch_id 0 12 miss% 0.05710523298115895
plot_id,batch_id 0 13 miss% 0.039240326953202294
plot_id,batch_id 0 14 miss% 0.08006979604358487
plot_id,batch_id 0 15 miss% 0.04792639281555845
plot_id,batch_id 0 16 miss% 0.07720404825092769
plot_id,batch_id 0 17 miss% 0.028153460874836733
plot_id,batch_id 0 18 miss% 0.06396494939983184
plot_id,batch_id 0 19 miss% 0.06880686611629952
plot_id,batch_id 0 20 miss% 0.052270217199497426
plot_id,batch_id 0 21 miss% 0.03150618500755307
plot_id,batch_id 0 22 miss% 0.049276494722559225
plot_id,batch_id 0 23 miss% 0.03496964126780819
plot_id,batch_id 0 24 miss% 0.04429771913043841
plot_id,batch_id 0 25 miss% 0.05965203674759052
plot_id,batch_id 0 26 miss% 0.041264953879740295
plot_id,batch_id 0 27 miss% 0.061920766089796554
plot_id,batch_id 0 28 miss% 0.033497507373852264
plot_id,batch_id 0 29 miss% 0.040430662212797806
plot_id,batch_id 0 30 miss% 0.0196519392497741
plot_id,batch_id 0 31 miss% 0.10144466406458388
plot_id,batch_id 0 32 miss% 0.07840184164185328
plot_id,batch_id 0 33 miss% 0.059203618476131814
plot_id,batch_id 0 34 miss% 0.05283995163984778
plot_id,batch_id 0 35 miss% 0.050871740094435666
plot_id,batch_id 0 36 miss% 0.058428218444135156
plot_id,batch_id 0 37 miss% 0.0530930613210854
plot_id,batch_id 0 38 miss% 0.03836620359883719
plot_id,batch_id 0 39 miss% 0.039589422738275704
plot_id,batch_id 0 40 miss% 0.12303525789066702
plot_id,batch_id 0 41 miss% 0.07385462206004775
plot_id,batch_id 0 42 miss% 0.02563183824356721
plot_id,batch_id 0 43 miss% 0.06328320657430628
plot_id,batch_id 0 44 miss% 0.016979358743776627
plot_id,batch_id 0 45 miss% 0.062179810415128774
plot_id,batch_id 0 46 miss% 0.04019426808453941
plot_id,batch_id 0 47 miss% 0.028725039629898308
plot_id,batch_id 0 48 miss% 0.022618684491803834
plot_id,batch_id 0 49 miss% 0.023316616040040553
plot_id,batch_id 0 50 miss% 0.14523962823315045
plot_id,batch_id 0 51 miss% 0.03518473256375725
plot_id,batch_id 0 52 miss% 0.042902134773011524
plot_id,batch_id 0 53 miss% 0.02388799665357758
plot_id,batch_id 0 54 miss% 0.044442487313117215
plot_id,batch_id 0 55 miss% 0.08963119076539013
plot_id,batch_id 0 56 miss% 0.08723256246880835
plot_id,batch_id 0 57 miss% 0.04232428819311062
plot_id,batch_id 0 58 miss% 0.03683667430953264
plot_id,batch_id 0 59 miss% 0.03534985218084254
plot_id,batch_id 0 60 miss% 0.03635959881825277
plot_id,batch_id 0 61 miss% 0.03367361290296099
plot_id,batch_id 0 62 miss% 0.07561150196367694
plot_id,batch_id 0 63 miss% 0.035037403128529654
plot_id,batch_id 0 64 miss% 0.07871324354667651
plot_id,batch_id 0 65 miss% 0.07325775396701101
plot_id,batch_id 0 66 miss% 0.06198385984053831
plot_id,batch_id 0 67 miss% 0.023135780877859145
plot_id,batch_id 0 68 miss% 0.03327949685740574
plot_id,batch_id 0 69 miss% 0.053762726312730824
plot_id,batch_id 0 70 miss% 0.048451774607106195
plot_id,batch_id 0 71 miss% 0.03522228233677326
plot_id,batch_id 0 72 miss% 0.09802468370053205
plot_id,batch_id 0 73 miss% 0.040520690700880324
plot_id,batch_id 0 74 miss% 0.10509565777595924
plot_id,batch_id 0 75 miss% 0.07629335686936091
plot_id,batch_id 0 76 miss% 0.06775344201078824
plot_id,batch_id 0 77 miss% 0.03177454804870027
plot_id,batch_id 0 78 miss% 0.03693065218165506
plot_id,batch_id 0 79 miss% 0.09681489985606792
plot_id,batch_id 0 80 miss% 0.09990611549443416
plot_id,batch_id 0 81 miss% 0.06274910706625798
plot_id,batch_id 0 82 miss% 0.04848484363428129
plot_id,batch_id 0 83 miss% 0.04519625991742307
plot_id,batch_id 0 84 miss% 0.0826642364325763
plot_id,batch_id 0 85 miss% 0.05264799079557363
plot_id,batch_id 0 86 miss% 0.06719656458713166
plot_id,batch_id 0 87 miss% 0.08140627454621979
plot_id,batch_id 0 88 miss% 0.1088818807512304
plot_id,batch_id 0 89 miss% 0.08243784908847483
plot_id,batch_id 0 90 miss% 0.04102548316631416
plot_id,batch_id 0 91 miss% 0.0564748904631463
plot_id,batch_id 0 92 miss% 0.04513502340050615
plot_id,batch_id 0 93 miss% 0.044753383834655165
plot_id,batch_id 0 94 miss% 0.06585845736498587
plot_id,batch_id 0 95 miss% 0.03040792584731288
plot_id,batch_id 0 96 miss% 0.05509078373012458
plot_id,batch_id 0 97 miss% 0.07071582985435446
plot_id,batch_id 0 98 miss% 0.028442801851356662
plot_id,batch_id 0 99 miss% 0.06499809147010828
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05938109 0.04098003 0.09044698 0.05902451 0.03515689 0.03500045
 0.07197351 0.11258546 0.06832981 0.02160045 0.02217659 0.06464591
 0.05710523 0.03924033 0.0800698  0.04792639 0.07720405 0.02815346
 0.06396495 0.06880687 0.05227022 0.03150619 0.04927649 0.03496964
 0.04429772 0.05965204 0.04126495 0.06192077 0.03349751 0.04043066
 0.01965194 0.10144466 0.07840184 0.05920362 0.05283995 0.05087174
 0.05842822 0.05309306 0.0383662  0.03958942 0.12303526 0.07385462
 0.02563184 0.06328321 0.01697936 0.06217981 0.04019427 0.02872504
 0.02261868 0.02331662 0.14523963 0.03518473 0.04290213 0.023888
 0.04444249 0.08963119 0.08723256 0.04232429 0.03683667 0.03534985
 0.0363596  0.03367361 0.0756115  0.0350374  0.07871324 0.07325775
 0.06198386 0.02313578 0.0332795  0.05376273 0.04845177 0.03522228
 0.09802468 0.04052069 0.10509566 0.07629336 0.06775344 0.03177455
 0.03693065 0.0968149  0.09990612 0.06274911 0.04848484 0.04519626
 0.08266424 0.05264799 0.06719656 0.08140627 0.10888188 0.08243785
 0.04102548 0.05647489 0.04513502 0.04475338 0.06585846 0.03040793
 0.05509078 0.07071583 0.0284428  0.06499809]
for model  97 the mean error 0.05583770612520904
all id 97 hidden_dim 32 learning_rate 0.0025 num_layers 4 frames 25 out win 5 err 0.05583770612520904 time 16433.84038209915
Launcher: Job 98 completed in 16695 seconds.
Launcher: Task 159 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  154129
Epoch:0, Train loss:0.416392, valid loss:0.430518
Epoch:1, Train loss:0.017922, valid loss:0.001510
Epoch:2, Train loss:0.003207, valid loss:0.001473
Epoch:3, Train loss:0.002661, valid loss:0.001271
Epoch:4, Train loss:0.002358, valid loss:0.001392
Epoch:5, Train loss:0.002150, valid loss:0.001162
Epoch:6, Train loss:0.001939, valid loss:0.001061
Epoch:7, Train loss:0.001816, valid loss:0.000962
Epoch:8, Train loss:0.001724, valid loss:0.000887
Epoch:9, Train loss:0.001616, valid loss:0.000766
Epoch:10, Train loss:0.001544, valid loss:0.000702
Epoch:11, Train loss:0.001148, valid loss:0.000727
Epoch:12, Train loss:0.001093, valid loss:0.000700
Epoch:13, Train loss:0.001055, valid loss:0.000629
Epoch:14, Train loss:0.001036, valid loss:0.000722
Epoch:15, Train loss:0.001015, valid loss:0.000571
Epoch:16, Train loss:0.000974, valid loss:0.000591
Epoch:17, Train loss:0.000952, valid loss:0.000600
Epoch:18, Train loss:0.000901, valid loss:0.000777
Epoch:19, Train loss:0.000902, valid loss:0.000606
Epoch:20, Train loss:0.000874, valid loss:0.000579
Epoch:21, Train loss:0.000673, valid loss:0.000483
Epoch:22, Train loss:0.000641, valid loss:0.000468
Epoch:23, Train loss:0.000631, valid loss:0.000555
Epoch:24, Train loss:0.000636, valid loss:0.000493
Epoch:25, Train loss:0.000617, valid loss:0.000444
Epoch:26, Train loss:0.000614, valid loss:0.000461
Epoch:27, Train loss:0.000601, valid loss:0.000468
Epoch:28, Train loss:0.000593, valid loss:0.000466
Epoch:29, Train loss:0.000576, valid loss:0.000524
Epoch:30, Train loss:0.000569, valid loss:0.000473
Epoch:31, Train loss:0.000460, valid loss:0.000412
Epoch:32, Train loss:0.000454, valid loss:0.000412
Epoch:33, Train loss:0.000452, valid loss:0.000429
Epoch:34, Train loss:0.000456, valid loss:0.000407
Epoch:35, Train loss:0.000442, valid loss:0.000448
Epoch:36, Train loss:0.000441, valid loss:0.000435
Epoch:37, Train loss:0.000430, valid loss:0.000436
Epoch:38, Train loss:0.000447, valid loss:0.000442
Epoch:39, Train loss:0.000460, valid loss:0.000436
Epoch:40, Train loss:0.000414, valid loss:0.000431
Epoch:41, Train loss:0.000372, valid loss:0.000397
Epoch:42, Train loss:0.000370, valid loss:0.000413
Epoch:43, Train loss:0.000373, valid loss:0.000399
Epoch:44, Train loss:0.000366, valid loss:0.000388
Epoch:45, Train loss:0.000367, valid loss:0.000404
Epoch:46, Train loss:0.000362, valid loss:0.000398
Epoch:47, Train loss:0.000359, valid loss:0.000406
Epoch:48, Train loss:0.000360, valid loss:0.000411
Epoch:49, Train loss:0.000354, valid loss:0.000401
Epoch:50, Train loss:0.000354, valid loss:0.000389
Epoch:51, Train loss:0.000331, valid loss:0.000387
Epoch:52, Train loss:0.000329, valid loss:0.000384
Epoch:53, Train loss:0.000328, valid loss:0.000387
Epoch:54, Train loss:0.000328, valid loss:0.000384
Epoch:55, Train loss:0.000327, valid loss:0.000385
Epoch:56, Train loss:0.000327, valid loss:0.000384
Epoch:57, Train loss:0.000327, valid loss:0.000387
Epoch:58, Train loss:0.000326, valid loss:0.000386
Epoch:59, Train loss:0.000326, valid loss:0.000387
Epoch:60, Train loss:0.000326, valid loss:0.000388
training time 16555.394847869873
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.0843920099712231
plot_id,batch_id 0 1 miss% 0.10067448590082609
plot_id,batch_id 0 2 miss% 0.09813613385212847
plot_id,batch_id 0 3 miss% 0.0409526695807489
plot_id,batch_id 0 4 miss% 0.06013821125919795
plot_id,batch_id 0 5 miss% 0.03695717387307592
plot_id,batch_id 0 6 miss% 0.05137467972658237
plot_id,batch_id 0 7 miss% 0.09751356396511943
plot_id,batch_id 0 8 miss% 0.0588455909826662
plot_id,batch_id 0 9 miss% 0.058102336945425714
plot_id,batch_id 0 10 miss% 0.025739958332245478
plot_id,batch_id 0 11 miss% 0.07012577354988538
plot_id,batch_id 0 12 miss% 0.05636946211376613
plot_id,batch_id 0 13 miss% 0.053574205362695276
plot_id,batch_id 0 14 miss% 0.06049215248428376
plot_id,batch_id 0 15 miss% 0.044303716769136116
plot_id,batch_id 0 16 miss% 0.11526832967101391
plot_id,batch_id 0 17 miss% 0.022168803347094337
plot_id,batch_id 0 18 miss% 0.06556052198739244
plot_id,batch_id 0 19 miss% 0.07401966692659179
plot_id,batch_id 0 20 miss% 0.1145429154078061
plot_id,batch_id 0 21 miss% 0.040184535308328315
plot_id,batch_id 0 22 miss% 0.03745439279786429
plot_id,batch_id 0 23 miss% 0.041739824116907374
plot_id,batch_id 0 24 miss% 0.05733279894035143
plot_id,batch_id 0 25 miss% 0.05251037308038741
plot_id,batch_id 0 26 miss% 0.08290217389728222
plot_id,batch_id 0 27 miss% 0.05789036760032109
plot_id,batch_id 0 28 miss% 0.030632031515390387
plot_id,batch_id 0 29 miss% 0.032478299506231836
plot_id,batch_id 0 30 miss% 0.02999335982250012
plot_id,batch_id 0 31 miss% 0.08241511947885125
plot_id,batch_id 0 32 miss% 0.07288951899472901
plot_id,batch_id 0 33 miss% 0.07869275682186459
plot_id,batch_id 0 34 miss% 0.05521476775719303
plot_id,batch_id 0 35 miss% 0.05600903640245105
plot_id,batch_id 0 36 miss% 0.07678342384159952
plot_id,batch_id 0 37 miss% 0.08994547459256388
plot_id,batch_id 0 38 miss% 0.05397048872239916
plot_id,batch_id 0 39 miss% 0.02322249940915165
plot_id,batch_id 0 40 miss% 0.05386636429594177
plot_id,batch_id 0 41 miss% 0.07333139435214593
plot_id,batch_id 0 42 miss% 0.04313954886190406
plot_id,batch_id 0 43 miss% 0.03923687295745135
plot_id,batch_id 0 44 miss% 0.016753801899793534
plot_id,batch_id 0 45 miss% 0.06138923788493524
plot_id,batch_id 0 46 miss% 0.033755510904232455
plot_id,batch_id 0 47 miss% 0.03243027604860449
plot_id,batch_id 0 48 miss% 0.029222853713002327
plot_id,batch_id 0 49 miss% 0.03144182624121137
plot_id,batch_id 0 50 miss% 0.11750743893100137
plot_id,batch_id 0 51 miss% 0.07349681772328319
plot_id,batch_id 0 52 miss% 0.04133898227003883
plot_id,batch_id 0 53 miss% 0.024350769396370365
plot_id,batch_id 0 54 miss% 0.03355839360709636
plot_id,batch_id 0 55 miss% 0.08602917170108569
plot_id,batch_id 0 56 miss% 0.07093374411418502
plot_id,batch_id 0 57 miss% 0.036012952061997644
plot_id,batch_id 0 58 miss% 0.03404355132809355
plot_id,batch_id 0 59 miss% 0.035308632579244804
plot_id,batch_id 0 60 miss% 0.027200795478684956
plot_id,batch_id 0 61 miss% 0.050532480204816495
plot_id,batch_id 0 62 miss% 0.06610104910164227
plot_id,batch_id 0 63 miss% 0.04890062596716637
plot_id,batch_id 0 64 miss% 0.06917301693709955
plot_id,batch_id 0 65 miss% 0.11673571701070884
plot_id,batch_id 0 66 miss% 0.03784015193374134
plot_id,batch_id 0 67 miss% 0.043905375354689684
plot_id,batch_id 0 68 miss% 0.04733023456703402
plot_id,batch_id 0 69 miss% 0.06470624644834404
plot_id,batch_id 0 70 miss% 0.07141028711258475
plot_id,batch_id 0 71 miss% 0.057050020072357266
plot_id,batch_id 0 72 miss% 0.08408426152742465
plot_id,batch_id 0 73 miss% 0.08576958102620233
plot_id,batch_id 0 74 miss% 0.1359842870532651
plot_id,batch_id 0 75 miss% 0.03792721873554922
plot_id,batch_id 0 76 miss% 0.0697422075990432
plot_id,batch_id 0 77 miss% 0.05677343554573845
plot_id,batch_id 0 78 miss% 0.03400477529724841
plot_id,batch_id 0 79 miss% 0.091350576061599
plot_id,batch_id 0 80 miss% 0.05159088645187938
plot_id,batch_id 0 81 miss% 0.11131518204074743
plot_id,batch_id 0 82 miss% 0.10465042822114183
plot_id,batch_id 0 83 miss% 0.08408579262813032
plot_id,batch_id 0 84 miss% 0.04946241746227074
plot_id,batch_id 0 85 miss% 0.04540074295604916
plot_id,batch_id 0 86 miss% 0.03860513200343388
plot_id,batch_id 0 87 miss% 0.07062960357077294
plot_id,batch_id 0 88 miss% 0.07047289372933971
plot_id,batch_id 0 89 miss% 0.09032300976594967
plot_id,batch_id 0 90 miss% 0.055006216570130634
plot_id,batch_id 0 91 miss% 0.06242438990328522
plot_id,batch_id 0 92 miss% 0.0394790337881891
plot_id,batch_id 0 93 miss% 0.05047660635634969
plot_id,batch_id 0 94 miss% 0.05070954937549613
plot_id,batch_id 0 95 miss% 0.039874915197691735
plot_id,batch_id 0 96 miss% 0.027015174117675175
plot_id,batch_id 0 97 miss% 0.035286641255071804
plot_id,batch_id 0 98 miss% 0.0287288068585142
plot_id,batch_id 0 99 miss% 0.05229162888354956
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08439201 0.10067449 0.09813613 0.04095267 0.06013821 0.03695717
 0.05137468 0.09751356 0.05884559 0.05810234 0.02573996 0.07012577
 0.05636946 0.05357421 0.06049215 0.04430372 0.11526833 0.0221688
 0.06556052 0.07401967 0.11454292 0.04018454 0.03745439 0.04173982
 0.0573328  0.05251037 0.08290217 0.05789037 0.03063203 0.0324783
 0.02999336 0.08241512 0.07288952 0.07869276 0.05521477 0.05600904
 0.07678342 0.08994547 0.05397049 0.0232225  0.05386636 0.07333139
 0.04313955 0.03923687 0.0167538  0.06138924 0.03375551 0.03243028
 0.02922285 0.03144183 0.11750744 0.07349682 0.04133898 0.02435077
 0.03355839 0.08602917 0.07093374 0.03601295 0.03404355 0.03530863
 0.0272008  0.05053248 0.06610105 0.04890063 0.06917302 0.11673572
 0.03784015 0.04390538 0.04733023 0.06470625 0.07141029 0.05705002
 0.08408426 0.08576958 0.13598429 0.03792722 0.06974221 0.05677344
 0.03400478 0.09135058 0.05159089 0.11131518 0.10465043 0.08408579
 0.04946242 0.04540074 0.03860513 0.0706296  0.07047289 0.09032301
 0.05500622 0.06242439 0.03947903 0.05047661 0.05070955 0.03987492
 0.02701517 0.03528664 0.02872881 0.05229163]
for model  223 the mean error 0.058630131396575
all id 223 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 31 out win 5 err 0.058630131396575 time 16555.394847869873
Launcher: Job 224 completed in 16815 seconds.
Launcher: Task 220 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  151697
Epoch:0, Train loss:0.500920, valid loss:0.461628
Epoch:1, Train loss:0.259119, valid loss:0.003717
Epoch:2, Train loss:0.008096, valid loss:0.002837
Epoch:3, Train loss:0.006798, valid loss:0.002504
Epoch:4, Train loss:0.006042, valid loss:0.002171
Epoch:5, Train loss:0.005556, valid loss:0.002230
Epoch:6, Train loss:0.004891, valid loss:0.001710
Epoch:7, Train loss:0.003845, valid loss:0.001653
Epoch:8, Train loss:0.003672, valid loss:0.001400
Epoch:9, Train loss:0.003400, valid loss:0.001480
Epoch:10, Train loss:0.003249, valid loss:0.001507
Epoch:11, Train loss:0.002758, valid loss:0.001181
Epoch:12, Train loss:0.002661, valid loss:0.001102
Epoch:13, Train loss:0.002609, valid loss:0.001119
Epoch:14, Train loss:0.002558, valid loss:0.001123
Epoch:15, Train loss:0.002191, valid loss:0.000889
Epoch:16, Train loss:0.001418, valid loss:0.000856
Epoch:17, Train loss:0.001382, valid loss:0.000866
Epoch:18, Train loss:0.001353, valid loss:0.000874
Epoch:19, Train loss:0.001288, valid loss:0.000894
Epoch:20, Train loss:0.001243, valid loss:0.000827
Epoch:21, Train loss:0.000982, valid loss:0.000759
Epoch:22, Train loss:0.000959, valid loss:0.000808
Epoch:23, Train loss:0.000933, valid loss:0.000706
Epoch:24, Train loss:0.000912, valid loss:0.000713
Epoch:25, Train loss:0.000901, valid loss:0.000695
Epoch:26, Train loss:0.000911, valid loss:0.000809
Epoch:27, Train loss:0.000879, valid loss:0.000751
Epoch:28, Train loss:0.000869, valid loss:0.000719
Epoch:29, Train loss:0.000851, valid loss:0.000708
Epoch:30, Train loss:0.000849, valid loss:0.000707
Epoch:31, Train loss:0.000706, valid loss:0.000633
Epoch:32, Train loss:0.000695, valid loss:0.000641
Epoch:33, Train loss:0.000681, valid loss:0.000645
Epoch:34, Train loss:0.000676, valid loss:0.000655
Epoch:35, Train loss:0.000670, valid loss:0.000613
Epoch:36, Train loss:0.000665, valid loss:0.000639
Epoch:37, Train loss:0.000661, valid loss:0.000625
Epoch:38, Train loss:0.000652, valid loss:0.000628
Epoch:39, Train loss:0.000639, valid loss:0.000638
Epoch:40, Train loss:0.000634, valid loss:0.000618
Epoch:41, Train loss:0.000574, valid loss:0.000602
Epoch:42, Train loss:0.000566, valid loss:0.000615
Epoch:43, Train loss:0.000567, valid loss:0.000601
Epoch:44, Train loss:0.000562, valid loss:0.000622
Epoch:45, Train loss:0.000558, valid loss:0.000613
Epoch:46, Train loss:0.000562, valid loss:0.000603
Epoch:47, Train loss:0.000552, valid loss:0.000582
Epoch:48, Train loss:0.000550, valid loss:0.000603
Epoch:49, Train loss:0.000551, valid loss:0.000610
Epoch:50, Train loss:0.000539, valid loss:0.000592
Epoch:51, Train loss:0.000512, valid loss:0.000587
Epoch:52, Train loss:0.000506, valid loss:0.000588
Epoch:53, Train loss:0.000506, valid loss:0.000585
Epoch:54, Train loss:0.000504, valid loss:0.000582
Epoch:55, Train loss:0.000503, valid loss:0.000579
Epoch:56, Train loss:0.000502, valid loss:0.000581
Epoch:57, Train loss:0.000502, valid loss:0.000580
Epoch:58, Train loss:0.000501, valid loss:0.000579
Epoch:59, Train loss:0.000501, valid loss:0.000582
Epoch:60, Train loss:0.000501, valid loss:0.000584
training time 16650.80476617813
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07593325346744303
plot_id,batch_id 0 1 miss% 0.047010992613353786
plot_id,batch_id 0 2 miss% 0.09139661155308366
plot_id,batch_id 0 3 miss% 0.062072361972446935
plot_id,batch_id 0 4 miss% 0.08888615730662648
plot_id,batch_id 0 5 miss% 0.04950668278214031
plot_id,batch_id 0 6 miss% 0.06115562535138245
plot_id,batch_id 0 7 miss% 0.08042901944613325
plot_id,batch_id 0 8 miss% 0.08243279832322792
plot_id,batch_id 0 9 miss% 0.031522784744413404
plot_id,batch_id 0 10 miss% 0.03585426699972407
plot_id,batch_id 0 11 miss% 0.04629670382231543
plot_id,batch_id 0 12 miss% 0.098531246490914
plot_id,batch_id 0 13 miss% 0.07170452796885972
plot_id,batch_id 0 14 miss% 0.08862855082802248
plot_id,batch_id 0 15 miss% 0.05290270488897237
plot_id,batch_id 0 16 miss% 0.06154524348006149
plot_id,batch_id 0 17 miss% 0.03330241835845228
plot_id,batch_id 0 18 miss% 0.05390697228130798
plot_id,batch_id 0 19 miss% 0.0616704225154189
plot_id,batch_id 0 20 miss% 0.04585283734551642
plot_id,batch_id 0 21 miss% 0.0600929887943608
plot_id,batch_id 0 22 miss% 0.05900604873383789
plot_id,batch_id 0 23 miss% 0.03884137652044328
plot_id,batch_id 0 24 miss% 0.07770580209168651
plot_id,batch_id 0 25 miss% 0.05647137165552274
plot_id,batch_id 0 26 miss% 0.07416542604379092
plot_id,batch_id 0 27 miss% 0.053160811994993167
plot_id,batch_id 0 28 miss% 0.024059942942810584
plot_id,batch_id 0 29 miss% 0.05053000567968653
plot_id,batch_id 0 30 miss% 0.05873003277502865
plot_id,batch_id 0 31 miss% 0.07904807950321485
plot_id,batch_id 0 32 miss% 0.11033175656004243
plot_id,batch_id 0 33 miss% 0.06139180877180687
plot_id,batch_id 0 34 miss% 0.06626971736879057
plot_id,batch_id 0 35 miss% 0.03445131162241526
plot_id,batch_id 0 36 miss% 0.06697379824672296
plot_id,batch_id 0 37 miss% 0.06823412426605008
plot_id,batch_id 0 38 miss% 0.04012524322668649
plot_id,batch_id 0 39 miss% 0.04025906643852514
plot_id,batch_id 0 40 miss% 0.0650389633001375
plot_id,batch_id 0 41 miss% 0.0374643111782529
plot_id,batch_id 0 42 miss% 0.0187172878354036
plot_id,batch_id 0 43 miss% 0.0556163428603627
plot_id,batch_id 0 44 miss% 0.02737460140023527
plot_id,batch_id 0 45 miss% 0.042063283215615484
plot_id,batch_id 0 46 miss% 0.03420210557093236
plot_id,batch_id 0 47 miss% 0.039170919149044636
plot_id,batch_id 0 48 miss% 0.02583357519325892
plot_id,batch_id 0 49 miss% 0.03711735290993552
plot_id,batch_id 0 50 miss% 0.08884559623687004
plot_id,batch_id 0 51 miss% 0.03262940195209875
plot_id,batch_id 0 52 miss% 0.03296472032171973
plot_id,batch_id 0 53 miss% 0.025641165059023398
plot_id,batch_id 0 54 miss% 0.03445593423792731
plot_id,batch_id 0 55 miss% 0.04428145510542761
plot_id,batch_id 0 56 miss% 0.047487934509138366
plot_id,batch_id 0 57 miss% 0.03823739798424842
plot_id,batch_id 0 58 miss% 0.03895841117114897
plot_id,batch_id 0 59 miss% 0.03531536833528317
plot_id,batch_id 0 60 miss% 0.0522280090165739
plot_id,batch_id 0 61 miss% 0.05413833734580462
plot_id,batch_id 0 62 miss% 0.21334516395049988
plot_id,batch_id 0 63 miss% 0.03754037870319375
plot_id,batch_id 0 64 miss% 0.0862008106630062
plot_id,batch_id 0 65 miss% 0.07875243982272884
plot_id,batch_id 0 66 miss% 0.10500851638287138
plot_id,batch_id 0 67 miss% 0.054026081888131926
plot_id,batch_id 0 68 miss% 0.046640615264323775
plot_id,batch_idthe mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  154129
Epoch:0, Train loss:0.542910, valid loss:0.547726
Epoch:1, Train loss:0.035275, valid loss:0.004312
Epoch:2, Train loss:0.009188, valid loss:0.003071
Epoch:3, Train loss:0.006826, valid loss:0.002694
Epoch:4, Train loss:0.004766, valid loss:0.001920
Epoch:5, Train loss:0.003628, valid loss:0.001896
Epoch:6, Train loss:0.003248, valid loss:0.001449
Epoch:7, Train loss:0.003025, valid loss:0.001483
Epoch:8, Train loss:0.002835, valid loss:0.001491
Epoch:9, Train loss:0.002537, valid loss:0.001474
Epoch:10, Train loss:0.002557, valid loss:0.001468
Epoch:11, Train loss:0.001813, valid loss:0.000957
Epoch:12, Train loss:0.001686, valid loss:0.001011
Epoch:13, Train loss:0.001676, valid loss:0.001000
Epoch:14, Train loss:0.001594, valid loss:0.000926
Epoch:15, Train loss:0.001590, valid loss:0.000983
Epoch:16, Train loss:0.001501, valid loss:0.000886
Epoch:17, Train loss:0.001458, valid loss:0.000880
Epoch:18, Train loss:0.001469, valid loss:0.000873
Epoch:19, Train loss:0.001410, valid loss:0.000864
Epoch:20, Train loss:0.001334, valid loss:0.000872
Epoch:21, Train loss:0.000979, valid loss:0.000719
Epoch:22, Train loss:0.000947, valid loss:0.000723
Epoch:23, Train loss:0.000920, valid loss:0.000688
Epoch:24, Train loss:0.000918, valid loss:0.000683
Epoch:25, Train loss:0.000890, valid loss:0.000680
Epoch:26, Train loss:0.000887, valid loss:0.000889
Epoch:27, Train loss:0.000866, valid loss:0.000715
Epoch:28, Train loss:0.000833, valid loss:0.000658
Epoch:29, Train loss:0.000833, valid loss:0.000703
Epoch:30, Train loss:0.000816, valid loss:0.000701
Epoch:31, Train loss:0.000643, valid loss:0.000604
Epoch:32, Train loss:0.000610, valid loss:0.000606
Epoch:33, Train loss:0.000600, valid loss:0.000650
Epoch:34, Train loss:0.000597, valid loss:0.000619
Epoch:35, Train loss:0.000598, valid loss:0.000643
Epoch:36, Train loss:0.000598, valid loss:0.000599
Epoch:37, Train loss:0.000582, valid loss:0.000620
Epoch:38, Train loss:0.000572, valid loss:0.000613
Epoch:39, Train loss:0.000566, valid loss:0.000597
Epoch:40, Train loss:0.000565, valid loss:0.000597
Epoch:41, Train loss:0.000479, valid loss:0.000572
Epoch:42, Train loss:0.000466, valid loss:0.000567
Epoch:43, Train loss:0.000466, valid loss:0.000584
Epoch:44, Train loss:0.000463, valid loss:0.000564
Epoch:45, Train loss:0.000461, valid loss:0.000576
Epoch:46, Train loss:0.000451, valid loss:0.000577
Epoch:47, Train loss:0.000451, valid loss:0.000584
Epoch:48, Train loss:0.000455, valid loss:0.000566
Epoch:49, Train loss:0.000445, valid loss:0.000582
Epoch:50, Train loss:0.000446, valid loss:0.000578
Epoch:51, Train loss:0.000405, valid loss:0.000564
Epoch:52, Train loss:0.000402, valid loss:0.000564
Epoch:53, Train loss:0.000400, valid loss:0.000564
Epoch:54, Train loss:0.000399, valid loss:0.000562
Epoch:55, Train loss:0.000398, valid loss:0.000563
Epoch:56, Train loss:0.000398, valid loss:0.000561
Epoch:57, Train loss:0.000397, valid loss:0.000561
Epoch:58, Train loss:0.000397, valid loss:0.000561
Epoch:59, Train loss:0.000397, valid loss:0.000560
Epoch:60, Train loss:0.000396, valid loss:0.000560
training time 16678.53924894333
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08814241581211155
plot_id,batch_id 0 1 miss% 0.03788001458721655
plot_id,batch_id 0 2 miss% 0.05847700731012432
plot_id,batch_id 0 3 miss% 0.051104814818761224
plot_id,batch_id 0 4 miss% 0.03928724142894156
plot_id,batch_id 0 5 miss% 0.07051643196331886
plot_id,batch_id 0 6 miss% 0.06946493483004126
plot_id,batch_id 0 7 miss% 0.06170474677347412
plot_id,batch_id 0 8 miss% 0.08231727851081534
plot_id,batch_id 0 9 miss% 0.04498533247144596
plot_id,batch_id 0 10 miss% 0.03213512780415755
plot_id,batch_id 0 11 miss% 0.05349380259764709
plot_id,batch_id 0 12 miss% 0.08433178540557712
plot_id,batch_id 0 13 miss% 0.08529816232741456
plot_id,batch_id 0 14 miss% 0.05413230576000864
plot_id,batch_id 0 15 miss% 0.035966138123346376
plot_id,batch_id 0 16 miss% 0.07260909817891172
plot_id,batch_id 0 17 miss% 0.05899238319808504
plot_id,batch_id 0 18 miss% 0.0873169793849828
plot_id,batch_id 0 19 miss% 0.05839985531178489
plot_id,batch_id 0 20 miss% 0.03117711784112987
plot_id,batch_id 0 21 miss% 0.02683734908491359
plot_id,batch_id 0 22 miss% 0.04728937699634818
plot_id,batch_id 0 23 miss% 0.03859511855959783
plot_id,batch_id 0 24 miss% 0.03966206614187661
plot_id,batch_id 0 25 miss% 0.028245679528154554
plot_id,batch_id 0 26 miss% 0.0562628278210365
plot_id,batch_id 0 27 miss% 0.06563276824973138
plot_id,batch_id 0 28 miss% 0.024526419810557486
plot_id,batch_id 0 29 miss% 0.03693430742163516
plot_id,batch_id 0 30 miss% 0.07373905221202542
plot_id,batch_id 0 31 miss% 0.09107787739241732
plot_id,batch_id 0 32 miss% 0.07148972710661447
plot_id,batch_id 0 33 miss% 0.047495162140346964
plot_id,batch_id 0 34 miss% 0.04888207201093416
plot_id,batch_id 0 35 miss% 0.02195005447862301
plot_id,batch_id 0 36 miss% 0.045784337403629645
plot_id,batch_id 0 37 miss% 0.07235839893382733
plot_id,batch_id 0 38 miss% 0.08831668829471385
plot_id,batch_id 0 39 miss% 0.0586005118984162
plot_id,batch_id 0 40 miss% 0.0485492613970982
plot_id,batch_id 0 41 miss% 0.0703810673615565
plot_id,batch_id 0 42 miss% 0.034058226509717286
plot_id,batch_id 0 43 miss% 0.037864267673667115
plot_id,batch_id 0 44 miss% 0.032171428279280434
plot_id,batch_id 0 45 miss% 0.044682486010486036
plot_id,batch_id 0 46 miss% 0.04272650097814522
plot_id,batch_id 0 47 miss% 0.031211950564843202
plot_id,batch_id 0 48 miss% 0.028252875853217733
plot_id,batch_id 0 49 miss% 0.031947622737300424
plot_id,batch_id 0 50 miss% 0.14603512614930375
plot_id,batch_id 0 51 miss% 0.05306565925041405
plot_id,batch_id 0 52 miss% 0.03726458827309934
plot_id,batch_id 0 53 miss% 0.02196749571053404
plot_id,batch_id 0 54 miss% 0.02700447592506691
plot_id,batch_id 0 55 miss% 0.056377861271022615
plot_id,batch_id 0 56 miss% 0.04584935260712825
plot_id,batch_id 0 57 miss% 0.05378827221081803
plot_id,batch_id 0 58 miss% 0.02298154793477768
plot_id,batch_id 0 59 miss% 0.024081803421147928
plot_id,batch_id 0 60 miss% 0.05084679682194927
plot_id,batch_id 0 61 miss% 0.039515517943156525
plot_id,batch_id 0 62 miss% 0.09541873503175514
plot_id,batch_id 0 63 miss% 0.05308312675116294
plot_id,batch_id 0 64 miss% 0.08426345060831154
plot_id,batch_id 0 65 miss% 0.07416544132127524
plot_id,batch_id 0 66 miss% 0.16069878493284295
plot_id,batch_id 0 67 miss% 0.023058063031100303
plot_id,batch_id 0 68 miss% 0.030698177178894268
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  120401
Epoch:0, Train loss:0.480405, valid loss:0.480868
Epoch:1, Train loss:0.021640, valid loss:0.002495
Epoch:2, Train loss:0.003501, valid loss:0.001733
Epoch:3, Train loss:0.002610, valid loss:0.001153
Epoch:4, Train loss:0.002251, valid loss:0.001177
Epoch:5, Train loss:0.002010, valid loss:0.001077
Epoch:6, Train loss:0.001809, valid loss:0.000892
Epoch:7, Train loss:0.001705, valid loss:0.000870
Epoch:8, Train loss:0.001618, valid loss:0.000896
Epoch:9, Train loss:0.001515, valid loss:0.000940
Epoch:10, Train loss:0.001446, valid loss:0.000752
Epoch:11, Train loss:0.001052, valid loss:0.000629
Epoch:12, Train loss:0.001033, valid loss:0.000572
Epoch:13, Train loss:0.001013, valid loss:0.000864
Epoch:14, Train loss:0.000992, valid loss:0.000724
Epoch:15, Train loss:0.000939, valid loss:0.000595
Epoch:16, Train loss:0.000946, valid loss:0.000560
Epoch:17, Train loss:0.000887, valid loss:0.000629
Epoch:18, Train loss:0.000878, valid loss:0.000574
Epoch:19, Train loss:0.000868, valid loss:0.000633
Epoch:20, Train loss:0.000853, valid loss:0.000570
Epoch:21, Train loss:0.000651, valid loss:0.000475
Epoch:22, Train loss:0.000634, valid loss:0.000523
Epoch:23, Train loss:0.000619, valid loss:0.000529
Epoch:24, Train loss:0.000621, valid loss:0.000445
Epoch:25, Train loss:0.000612, valid loss:0.000443
Epoch:26, Train loss:0.000597, valid loss:0.000441
Epoch:27, Train loss:0.000607, valid loss:0.000763
Epoch:28, Train loss:0.000581, valid loss:0.000444
Epoch:29, Train loss:0.000571, valid loss:0.000504
Epoch:30, Train loss:0.000561, valid loss:0.000477
Epoch:31, Train loss:0.000484, valid loss:0.000422
Epoch:32, Train loss:0.000467, valid loss:0.000471
Epoch:33, Train loss:0.000462, valid loss:0.000445
Epoch:34, Train loss:0.000465, valid loss:0.000445
Epoch:35, Train loss:0.000454, valid loss:0.000464
Epoch:36, Train loss:0.000446, valid loss:0.000435
Epoch:37, Train loss:0.000456, valid loss:0.000424
Epoch:38, Train loss:0.000452, valid loss:0.000454
Epoch:39, Train loss:0.000448, valid loss:0.000434
Epoch:40, Train loss:0.000439, valid loss:0.000442
Epoch:41, Train loss:0.000396, valid loss:0.000404
Epoch:42, Train loss:0.000395, valid loss:0.000400
Epoch:43, Train loss:0.000393, valid loss:0.000423
Epoch:44, Train loss:0.000388, valid loss:0.000410
Epoch:45, Train loss:0.000387, valid loss:0.000419
Epoch:46, Train loss:0.000387, valid loss:0.000402
Epoch:47, Train loss:0.000382, valid loss:0.000415
Epoch:48, Train loss:0.000380, valid loss:0.000424
Epoch:49, Train loss:0.000378, valid loss:0.000410
Epoch:50, Train loss:0.000378, valid loss:0.000409
Epoch:51, Train loss:0.000356, valid loss:0.000402
Epoch:52, Train loss:0.000354, valid loss:0.000400
Epoch:53, Train loss:0.000353, valid loss:0.000401
Epoch:54, Train loss:0.000353, valid loss:0.000401
Epoch:55, Train loss:0.000352, valid loss:0.000400
Epoch:56, Train loss:0.000352, valid loss:0.000398
Epoch:57, Train loss:0.000351, valid loss:0.000399
Epoch:58, Train loss:0.000351, valid loss:0.000400
Epoch:59, Train loss:0.000351, valid loss:0.000399
Epoch:60, Train loss:0.000351, valid loss:0.000400
training time 16697.225750684738
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.08235252490416756
plot_id,batch_id 0 1 miss% 0.04860374688621801
plot_id,batch_id 0 2 miss% 0.10458241057474002
plot_id,batch_id 0 3 miss% 0.07943756643656079
plot_id,batch_id 0 4 miss% 0.04808867206325535
plot_id,batch_id 0 5 miss% 0.05223521278608305
plot_id,batch_id 0 6 miss% 0.07071820415455442
plot_id,batch_id 0 7 miss% 0.07597658297394752
plot_id,batch_id 0 8 miss% 0.10054546977058537
plot_id,batch_id 0 9 miss% 0.1265713650356279
plot_id,batch_id 0 10 miss% 0.04125229463882731
plot_id,batch_id 0 11 miss% 0.06467891244899122
plot_id,batch_id 0 12 miss% 0.07077116608390577
plot_id,batch_id 0 13 miss% 0.05618615341925891
plot_id,batch_id 0 14 miss% 0.08325592792640372
plot_id,batch_id 0 15 miss% 0.04724632973363358
plot_id,batch_id 0 16 miss% 0.10540410138523441
plot_id,batch_id 0 17 miss% 0.04307201243687806
plot_id,batch_id 0 18 miss% 0.06638453517788952
plot_id,batch_id 0 19 miss% 0.08625939978321064
plot_id,batch_id 0 20 miss% 0.08092883410212369
plot_id,batch_id 0 21 miss% 0.08240306357257252
plot_id,batch_id 0 22 miss% 0.13600052073806976
plot_id,batch_id 0 23 miss% 0.058191323616945234
plot_id,batch_id 0 24 miss% 0.058418381063414494
plot_id,batch_id 0 25 miss% 0.06031451177365158
plot_id,batch_id 0 26 miss% 0.0708218831879743
plot_id,batch_id 0 27 miss% 0.07720354924188913
plot_id,batch_id 0 28 miss% 0.03769033285547884
plot_id,batch_id 0 29 miss% 0.03777381728065544
plot_id,batch_id 0 30 miss% 0.08525714999219794
plot_id,batch_id 0 31 miss% 0.1313523836537982
plot_id,batch_id 0 32 miss% 0.07875228201689374
plot_id,batch_id 0 33 miss% 0.07283047711064157
plot_id,batch_id 0 34 miss% 0.0778220251546973
plot_id,batch_id 0 35 miss% 0.037694040380771704
plot_id,batch_id 0 36 miss% 0.06559689194257647
plot_id,batch_id 0 37 miss% 0.08547622090015533
plot_id,batch_id 0 38 miss% 0.07082686767248654
plot_id,batch_id 0 39 miss% 0.03816941937316587
plot_id,batch_id 0 40 miss% 0.04553915081243676
plot_id,batch_id 0 41 miss% 0.05328334081013063
plot_id,batch_id 0 42 miss% 0.059932367556304
plot_id,batch_id 0 43 miss% 0.13767924841615858
plot_id,batch_id 0 44 miss% 0.11554426942961747
plot_id,batch_id 0 45 miss% 0.05211690821037619
plot_id,batch_id 0 46 miss% 0.019860339411454743
plot_id,batch_id 0 47 miss% 0.03929289885492855
plot_id,batch_id 0 48 miss% 0.07214684758715083
plot_id,batch_id 0 49 miss% 0.04680900577358346
plot_id,batch_id 0 50 miss% 0.11478942872250582
plot_id,batch_id 0 51 miss% 0.042226959647644065
plot_id,batch_id 0 52 miss% 0.045001537066904955
plot_id,batch_id 0 53 miss% 0.1008184442525122
plot_id,batch_id 0 54 miss% 0.09301475630164298
plot_id,batch_id 0 55 miss% 0.07596704921072883
plot_id,batch_id 0 56 miss% 0.10622100429023765
plot_id,batch_id 0 57 miss% 0.05331107630038876
plot_id,batch_id 0 58 miss% 0.04213815865026937
plot_id,batch_id 0 59 miss% 0.0881065286285406
plot_id,batch_id 0 60 miss% 0.02861308524747996
plot_id,batch_id 0 61 miss% 0.021307486424674824
plot_id,batch_id 0 62 miss% 0.04502706388037537
plot_id,batch_id 0 63 miss% 0.07200691498228858
plot_id,batch_id 0 64 miss% 0.0762850185036416
plot_id,batch_id 0 65 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  120401
Epoch:0, Train loss:0.480405, valid loss:0.480868
Epoch:1, Train loss:0.054770, valid loss:0.002341
Epoch:2, Train loss:0.006082, valid loss:0.002274
Epoch:3, Train loss:0.005404, valid loss:0.001595
Epoch:4, Train loss:0.003479, valid loss:0.001135
Epoch:5, Train loss:0.001960, valid loss:0.000949
Epoch:6, Train loss:0.001750, valid loss:0.000870
Epoch:7, Train loss:0.001617, valid loss:0.000962
Epoch:8, Train loss:0.001550, valid loss:0.000923
Epoch:9, Train loss:0.001409, valid loss:0.000778
Epoch:10, Train loss:0.001368, valid loss:0.000713
Epoch:11, Train loss:0.001019, valid loss:0.000604
Epoch:12, Train loss:0.000977, valid loss:0.000611
Epoch:13, Train loss:0.000949, valid loss:0.000638
Epoch:14, Train loss:0.000934, valid loss:0.000561
Epoch:15, Train loss:0.000877, valid loss:0.000557
Epoch:16, Train loss:0.000871, valid loss:0.000515
Epoch:17, Train loss:0.000861, valid loss:0.000603
Epoch:18, Train loss:0.000831, valid loss:0.000552
Epoch:19, Train loss:0.000801, valid loss:0.000521
Epoch:20, Train loss:0.000793, valid loss:0.000553
Epoch:21, Train loss:0.000629, valid loss:0.000489
Epoch:22, Train loss:0.000604, valid loss:0.000533
Epoch:23, Train loss:0.000599, valid loss:0.000475
Epoch:24, Train loss:0.000593, valid loss:0.000501
Epoch:25, Train loss:0.000578, valid loss:0.000480
Epoch:26, Train loss:0.000581, valid loss:0.000476
Epoch:27, Train loss:0.000569, valid loss:0.000476
Epoch:28, Train loss:0.000551, valid loss:0.000473
Epoch:29, Train loss:0.000562, valid loss:0.000470
Epoch:30, Train loss:0.000537, valid loss:0.000461
Epoch:31, Train loss:0.000459, valid loss:0.000454
Epoch:32, Train loss:0.000449, valid loss:0.000469
Epoch:33, Train loss:0.000454, valid loss:0.000427
Epoch:34, Train loss:0.000443, valid loss:0.000435
Epoch:35, Train loss:0.000439, valid loss:0.000447
Epoch:36, Train loss:0.000440, valid loss:0.000432
Epoch:37, Train loss:0.000435, valid loss:0.000425
Epoch:38, Train loss:0.000438, valid loss:0.000467
Epoch:39, Train loss:0.000428, valid loss:0.000463
Epoch:40, Train loss:0.000420, valid loss:0.000446
Epoch:41, Train loss:0.000386, valid loss:0.000411
Epoch:42, Train loss:0.000386, valid loss:0.000422
Epoch:43, Train loss:0.000379, valid loss:0.000434
Epoch:44, Train loss:0.000378, valid loss:0.000427
Epoch:45, Train loss:0.000377, valid loss:0.000422
Epoch:46, Train loss:0.000372, valid loss:0.000436
Epoch:47, Train loss:0.000373, valid loss:0.000415
Epoch:48, Train loss:0.000373, valid loss:0.000425
Epoch:49, Train loss:0.000367, valid loss:0.000429
Epoch:50, Train loss:0.000369, valid loss:0.000408
Epoch:51, Train loss:0.000349, valid loss:0.000414
Epoch:52, Train loss:0.000346, valid loss:0.000413
Epoch:53, Train loss:0.000345, valid loss:0.000412
Epoch:54, Train loss:0.000345, valid loss:0.000410
Epoch:55, Train loss:0.000344, valid loss:0.000414
Epoch:56, Train loss:0.000344, valid loss:0.000410
Epoch:57, Train loss:0.000343, valid loss:0.000410
Epoch:58, Train loss:0.000343, valid loss:0.000413
Epoch:59, Train loss:0.000342, valid loss:0.000409
Epoch:60, Train loss:0.000342, valid loss:0.000414
training time 16703.65442466736
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.05500430295515744
plot_id,batch_id 0 1 miss% 0.045811390057408335
plot_id,batch_id 0 2 miss% 0.10816994064956093
plot_id,batch_id 0 3 miss% 0.03234088387804149
plot_id,batch_id 0 4 miss% 0.04680129252383867
plot_id,batch_id 0 5 miss% 0.03160917229014827
plot_id,batch_id 0 6 miss% 0.016497124507235096
plot_id,batch_id 0 7 miss% 0.07825983578827356
plot_id,batch_id 0 8 miss% 0.05355844580189107
plot_id,batch_id 0 9 miss% 0.056868918667844465
plot_id,batch_id 0 10 miss% 0.03359588116998185
plot_id,batch_id 0 11 miss% 0.05306078307155788
plot_id,batch_id 0 12 miss% 0.052913364739302624
plot_id,batch_id 0 13 miss% 0.07917570072363504
plot_id,batch_id 0 14 miss% 0.06239619619205223
plot_id,batch_id 0 15 miss% 0.030807669941828972
plot_id,batch_id 0 16 miss% 0.08504524641078114
plot_id,batch_id 0 17 miss% 0.044007785928966314
plot_id,batch_id 0 18 miss% 0.07836003276937531
plot_id,batch_id 0 19 miss% 0.0780711248946981
plot_id,batch_id 0 20 miss% 0.041735196791947884
plot_id,batch_id 0 21 miss% 0.033668620460632424
plot_id,batch_id 0 22 miss% 0.06667651372495251
plot_id,batch_id 0 23 miss% 0.0452952742244465
plot_id,batch_id 0 24 miss% 0.05818630886546798
plot_id,batch_id 0 25 miss% 0.06564271181419208
plot_id,batch_id 0 26 miss% 0.05895496664869379
plot_id,batch_id 0 27 miss% 0.05567845515686507
plot_id,batch_id 0 28 miss% 0.0515630137262982
plot_id,batch_id 0 29 miss% 0.06244675232791991
plot_id,batch_id 0 30 miss% 0.019211311381465042
plot_id,batch_id 0 31 miss% 0.08545523600536949
plot_id,batch_id 0 32 miss% 0.10332076161279471
plot_id,batch_id 0 33 miss% 0.028813560839354838
plot_id,batch_id 0 34 miss% 0.037900205620321245
plot_id,batch_id 0 35 miss% 0.01896816797287103
plot_id,batch_id 0 36 miss% 0.04379429540650703
plot_id,batch_id 0 37 miss% 0.06490274230901527
plot_id,batch_id 0 38 miss% 0.06474853484997647
plot_id,batch_id 0 39 miss% 0.035465868598589426
plot_id,batch_id 0 40 miss% 0.07175388921726271
plot_id,batch_id 0 41 miss% 0.035450016465350564
plot_id,batch_id 0 42 miss% 0.019198434190132768
plot_id,batch_id 0 43 miss% 0.030808660785004983
plot_id,batch_id 0 44 miss% 0.023212824417270954
plot_id,batch_id 0 45 miss% 0.07172211894617539
plot_id,batch_id 0 46 miss% 0.04407930183823477
plot_id,batch_id 0 47 miss% 0.030352160262176416
plot_id,batch_id 0 48 miss% 0.040426208927779365
plot_id,batch_id 0 49 miss% 0.058244147841248185
plot_id,batch_id 0 50 miss% 0.11871948577125627
plot_id,batch_id 0 51 miss% 0.04437117722648986
plot_id,batch_id 0 52 miss% 0.026000374622987266
plot_id,batch_id 0 53 miss% 0.028918611065101765
plot_id,batch_id 0 54 miss% 0.04510158809399576
plot_id,batch_id 0 55 miss% 0.12187557988959062
plot_id,batch_id 0 56 miss% 0.06035307040366532
plot_id,batch_id 0 57 miss% 0.05634587125779389
plot_id,batch_id 0 58 miss% 0.06749585097812401
plot_id,batch_id 0 59 miss% 0.04557531064754569
plot_id,batch_id 0 60 miss% 0.030932310515599793
plot_id,batch_id 0 61 miss% 0.025817143931684508
plot_id,batch_id 0 62 miss% 0.05289195727043682
plot_id,batch_id 0 63 miss% 0.06906324964462461
plot_id,batch_id 0 64 miss% 0.06151775110137555
 0 69 miss% 0.10054002500270051
plot_id,batch_id 0 70 miss% 0.06868700508035712
plot_id,batch_id 0 71 miss% 0.0371326293973579
plot_id,batch_id 0 72 miss% 0.0817422138719862
plot_id,batch_id 0 73 miss% 0.06432729350626598
plot_id,batch_id 0 74 miss% 0.0986219542211225
plot_id,batch_id 0 75 miss% 0.05345277998927498
plot_id,batch_id 0 76 miss% 0.04250583254146955
plot_id,batch_id 0 77 miss% 0.03714099753485661
plot_id,batch_id 0 78 miss% 0.02886481543214472
plot_id,batch_id 0 79 miss% 0.10946412250990335
plot_id,batch_id 0 80 miss% 0.07683390088819929
plot_id,batch_id 0 81 miss% 0.08031862458066787
plot_id,batch_id 0 82 miss% 0.06762611058070449
plot_id,batch_id 0 83 miss% 0.07421958756591386
plot_id,batch_id 0 84 miss% 0.09176323097668135
plot_id,batch_id 0 85 miss% 0.08053442944054512
plot_id,batch_id 0 86 miss% 0.05910074655969593
plot_id,batch_id 0 87 miss% 0.0760889915861517
plot_id,batch_id 0 88 miss% 0.07105432435754788
plot_id,batch_id 0 89 miss% 0.06526142876744412
plot_id,batch_id 0 90 miss% 0.0339846244085305
plot_id,batch_id 0 91 miss% 0.08256052934488592
plot_id,batch_id 0 92 miss% 0.04438522265459973
plot_id,batch_id 0 93 miss% 0.032685584276040555
plot_id,batch_id 0 94 miss% 0.0946981614397378
plot_id,batch_id 0 95 miss% 0.041302706280068456
plot_id,batch_id 0 96 miss% 0.07646529983915226
plot_id,batch_id 0 97 miss% 0.052934139641852375
plot_id,batch_id 0 98 miss% 0.025670852923048975
plot_id,batch_id 0 99 miss% 0.062261348258684686
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07593325 0.04701099 0.09139661 0.06207236 0.08888616 0.04950668
 0.06115563 0.08042902 0.0824328  0.03152278 0.03585427 0.0462967
 0.09853125 0.07170453 0.08862855 0.0529027  0.06154524 0.03330242
 0.05390697 0.06167042 0.04585284 0.06009299 0.05900605 0.03884138
 0.0777058  0.05647137 0.07416543 0.05316081 0.02405994 0.05053001
 0.05873003 0.07904808 0.11033176 0.06139181 0.06626972 0.03445131
 0.0669738  0.06823412 0.04012524 0.04025907 0.06503896 0.03746431
 0.01871729 0.05561634 0.0273746  0.04206328 0.03420211 0.03917092
 0.02583358 0.03711735 0.0888456  0.0326294  0.03296472 0.02564117
 0.03445593 0.04428146 0.04748793 0.0382374  0.03895841 0.03531537
 0.05222801 0.05413834 0.21334516 0.03754038 0.08620081 0.07875244
 0.10500852 0.05402608 0.04664062 0.10054003 0.06868701 0.03713263
 0.08174221 0.06432729 0.09862195 0.05345278 0.04250583 0.037141
 0.02886482 0.10946412 0.0768339  0.08031862 0.06762611 0.07421959
 0.09176323 0.08053443 0.05910075 0.07608899 0.07105432 0.06526143
 0.03398462 0.08256053 0.04438522 0.03268558 0.09469816 0.04130271
 0.0764653  0.05293414 0.02567085 0.06226135]
for model  104 the mean error 0.05981916889797024
all id 104 hidden_dim 24 learning_rate 0.0025 num_layers 5 frames 25 out win 6 err 0.05981916889797024 time 16650.80476617813
Launcher: Job 105 completed in 16916 seconds.
Launcher: Task 164 done. Exiting.
plot_id,batch_id 0 69 miss% 0.06513720213842349
plot_id,batch_id 0 70 miss% 0.12863580327133875
plot_id,batch_id 0 71 miss% 0.05415557929452664
plot_id,batch_id 0 72 miss% 0.05253797617766604
plot_id,batch_id 0 73 miss% 0.040695141079153146
plot_id,batch_id 0 74 miss% 0.07818612973586858
plot_id,batch_id 0 75 miss% 0.12428160277024558
plot_id,batch_id 0 76 miss% 0.08203243261748801
plot_id,batch_id 0 77 miss% 0.0652294095358932
plot_id,batch_id 0 78 miss% 0.09644232905215627
plot_id,batch_id 0 79 miss% 0.04915813241630798
plot_id,batch_id 0 80 miss% 0.045440112005474806
plot_id,batch_id 0 81 miss% 0.0722135570335144
plot_id,batch_id 0 82 miss% 0.04158020677166272
plot_id,batch_id 0 83 miss% 0.08315274363942872
plot_id,batch_id 0 84 miss% 0.062303885615379584
plot_id,batch_id 0 85 miss% 0.04929349957505564
plot_id,batch_id 0 86 miss% 0.05923745532197211
plot_id,batch_id 0 87 miss% 0.07384560804983709
plot_id,batch_id 0 88 miss% 0.07364528215601582
plot_id,batch_id 0 89 miss% 0.06506178405371013
plot_id,batch_id 0 90 miss% 0.06672069406614134
plot_id,batch_id 0 91 miss% 0.08218989270088402
plot_id,batch_id 0 92 miss% 0.04106331825030068
plot_id,batch_id 0 93 miss% 0.027440459745962282
plot_id,batch_id 0 94 miss% 0.07791908448034027
plot_id,batch_id 0 95 miss% 0.05596412430135189
plot_id,batch_id 0 96 miss% 0.08151806475668587
plot_id,batch_id 0 97 miss% 0.025046127577900073
plot_id,batch_id 0 98 miss% 0.03342969854938965
plot_id,batch_id 0 99 miss% 0.04870068062557414
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08814242 0.03788001 0.05847701 0.05110481 0.03928724 0.07051643
 0.06946493 0.06170475 0.08231728 0.04498533 0.03213513 0.0534938
 0.08433179 0.08529816 0.05413231 0.03596614 0.0726091  0.05899238
 0.08731698 0.05839986 0.03117712 0.02683735 0.04728938 0.03859512
 0.03966207 0.02824568 0.05626283 0.06563277 0.02452642 0.03693431
 0.07373905 0.09107788 0.07148973 0.04749516 0.04888207 0.02195005
 0.04578434 0.0723584  0.08831669 0.05860051 0.04854926 0.07038107
 0.03405823 0.03786427 0.03217143 0.04468249 0.0427265  0.03121195
 0.02825288 0.03194762 0.14603513 0.05306566 0.03726459 0.0219675
 0.02700448 0.05637786 0.04584935 0.05378827 0.02298155 0.0240818
 0.0508468  0.03951552 0.09541874 0.05308313 0.08426345 0.07416544
 0.16069878 0.02305806 0.03069818 0.0651372  0.1286358  0.05415558
 0.05253798 0.04069514 0.07818613 0.1242816  0.08203243 0.06522941
 0.09644233 0.04915813 0.04544011 0.07221356 0.04158021 0.08315274
 0.06230389 0.0492935  0.05923746 0.07384561 0.07364528 0.06506178
 0.06672069 0.08218989 0.04106332 0.02744046 0.07791908 0.05596412
 0.08151806 0.02504613 0.0334297  0.04870068]
for model  143 the mean error 0.05745680751029418
all id 143 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 25 out win 6 err 0.05745680751029418 time 16678.53924894333
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  69649
Epoch:0, Train loss:0.428404, valid loss:0.407406
Epoch:1, Train loss:0.028118, valid loss:0.003654
Epoch:2, Train loss:0.010133, valid loss:0.003016
Epoch:3, Train loss:0.009108, valid loss:0.002768
Epoch:4, Train loss:0.007476, valid loss:0.001958
Epoch:5, Train loss:0.006353, valid loss:0.001824
Epoch:6, Train loss:0.006086, valid loss:0.001743
Epoch:7, Train loss:0.005896, valid loss:0.001728
Epoch:8, Train loss:0.004380, valid loss:0.001135
Epoch:9, Train loss:0.002622, valid loss:0.000874
Epoch:10, Train loss:0.001741, valid loss:0.000900
Epoch:11, Train loss:0.001386, valid loss:0.000751
Epoch:12, Train loss:0.001341, valid loss:0.000761
Epoch:13, Train loss:0.001297, valid loss:0.000760
Epoch:14, Train loss:0.001248, valid loss:0.000710
Epoch:15, Train loss:0.001215, valid loss:0.000738
Epoch:16, Train loss:0.001170, valid loss:0.000655
Epoch:17, Train loss:0.001160, valid loss:0.000637
Epoch:18, Train loss:0.001118, valid loss:0.000651
Epoch:19, Train loss:0.001078, valid loss:0.000628
Epoch:20, Train loss:0.001062, valid loss:0.000617
Epoch:21, Train loss:0.000903, valid loss:0.000602
Epoch:22, Train loss:0.000890, valid loss:0.000604
Epoch:23, Train loss:0.000874, valid loss:0.000572
Epoch:24, Train loss:0.000852, valid loss:0.000542
Epoch:25, Train loss:0.000866, valid loss:0.000574
Epoch:26, Train loss:0.000832, valid loss:0.000609
Epoch:27, Train loss:0.000830, valid loss:0.000580
Epoch:28, Train loss:0.000814, valid loss:0.000532
Epoch:29, Train loss:0.000818, valid loss:0.000565
Epoch:30, Train loss:0.000801, valid loss:0.000595
Epoch:31, Train loss:0.000710, valid loss:0.000506
Epoch:32, Train loss:0.000705, valid loss:0.000574
Epoch:33, Train loss:0.000702, valid loss:0.000521
Epoch:34, Train loss:0.000690, valid loss:0.000501
Epoch:35, Train loss:0.000691, valid loss:0.000502
Epoch:36, Train loss:0.000683, valid loss:0.000494
Epoch:37, Train loss:0.000680, valid loss:0.000541
Epoch:38, Train loss:0.000676, valid loss:0.000493
Epoch:39, Train loss:0.000669, valid loss:0.000502
Epoch:40, Train loss:0.000665, valid loss:0.000510
Epoch:41, Train loss:0.000618, valid loss:0.000487
Epoch:42, Train loss:0.000615, valid loss:0.000508
Epoch:43, Train loss:0.000616, valid loss:0.000489
Epoch:44, Train loss:0.000612, valid loss:0.000502
Epoch:45, Train loss:0.000610, valid loss:0.000486
Epoch:46, Train loss:0.000605, valid loss:0.000489
Epoch:47, Train loss:0.000602, valid loss:0.000482
Epoch:48, Train loss:0.000603, valid loss:0.000487
Epoch:49, Train loss:0.000598, valid loss:0.000476
Epoch:50, Train loss:0.000595, valid loss:0.000482
Epoch:51, Train loss:0.000572, valid loss:0.000473
Epoch:52, Train loss:0.000569, valid loss:0.000475
Epoch:53, Train loss:0.000568, valid loss:0.000470
Epoch:54, Train loss:0.000568, valid loss:0.000472
Epoch:55, Train loss:0.000566, valid loss:0.000468
Epoch:56, Train loss:0.000566, valid loss:0.000470
Epoch:57, Train loss:0.000566, valid loss:0.000465
Epoch:58, Train loss:0.000566, valid loss:0.000466
Epoch:59, Train loss:0.000566, valid loss:0.000469
Epoch:60, Train loss:0.000567, valid loss:0.000478
training time 16723.836084842682
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.06133652903653574
plot_id,batch_id 0 1 miss% 0.04386880663792661
plot_id,batch_id 0 2 miss% 0.12454907815824728
plot_id,batch_id 0 3 miss% 0.05339523176863042
plot_id,batch_id 0 4 miss% 0.06003290213654536
plot_id,batch_id 0 5 miss% 0.04586312681334658
plot_id,batch_id 0 6 miss% 0.06638211764404028
plot_id,batch_id 0 7 miss% 0.09515463083509239
plot_id,batch_id 0 8 miss% 0.08282908925678599
plot_id,batch_id 0 9 miss% 0.05294524617534009
plot_id,batch_id 0 10 miss% 0.0372517381328344
plot_id,batch_id 0 11 miss% 0.06633554427334412
plot_id,batch_id 0 12 miss% 0.08382662633651239
plot_id,batch_id 0 13 miss% 0.06806768840773578
plot_id,batch_id 0 14 miss% 0.10663240481578283
plot_id,batch_id 0 15 miss% 0.04195974612342521
plot_id,batch_id 0 16 miss% 0.1812732273472432
plot_id,batch_id 0 17 miss% 0.04224779288411792
plot_id,batch_id 0 18 miss% 0.061161108596911096
plot_id,batch_id 0 19 miss% 0.05755243629460864
plot_id,batch_id 0 20 miss% 0.07789306133301979
plot_id,batch_id 0 21 miss% 0.06088127747977208
plot_id,batch_id 0 22 miss% 0.05831728494311796
plot_id,batch_id 0 23 miss% 0.03829993336160813
plot_id,batch_id 0 24 miss% 0.06651261682636207
plot_id,batch_id 0 25 miss% 0.04336151518096984
plot_id,batch_id 0 26 miss% 0.06031353921220121
plot_id,batch_id 0 27 miss% 0.05214749010461326
plot_id,batch_id 0 28 miss% 0.03119132614576399
plot_id,batch_id 0 29 miss% 0.03783364223733531
plot_id,batch_id 0 30 miss% 0.06516381147327924
plot_id,batch_id 0 31 miss% 0.07882862450584381
plot_id,batch_id 0 32 miss% 0.11208354024729732
plot_id,batch_id 0 33 miss% 0.044602224497030135
plot_id,batch_id 0 34 miss% 0.03631133885401393
plot_id,batch_id 0 35 miss% 0.050600867185487244
plot_id,batch_id 0 36 miss% 0.15985700546758277
plot_id,batch_id 0 37 miss% 0.05437249084657584
plot_id,batch_id 0 38 miss% 0.052040506650151724
plot_id,batch_id 0 39 miss% 0.027863963117264738
plot_id,batch_id 0 40 miss% 0.09947658397463907
plot_id,batch_id 0 41 miss% 0.09499853199685611
plot_id,batch_id 0 42 miss% 0.02298440255265611
plot_id,batch_id 0 43 miss% 0.049540543580062595
plot_id,batch_id 0 44 miss% 0.06463571262620142
plot_id,batch_id 0 45 miss% 0.07437740731077408
plot_id,batch_id 0 46 miss% 0.03951130750260278
plot_id,batch_id 0 47 miss% 0.032748447460630895
plot_id,batch_id 0 48 miss% 0.02672137379634287
plot_id,batch_id 0 49 miss% 0.0326380973624731
plot_id,batch_id 0 50 miss% 0.12299501989156951
plot_id,batch_id 0 51 miss% 0.0669572127529407
plot_id,batch_id 0 52 miss% 0.030771440188842075
plot_id,batch_id 0 53 miss% 0.03291067275071724
plot_id,batch_id 0 54 miss% 0.03258284806910988
plot_id,batch_id 0 55 miss% 0.05179328762270195
plot_id,batch_id 0 56 miss% 0.06737423383664161
plot_id,batch_id 0 57 miss% 0.038884925144208
plot_id,batch_id 0 58 miss% 0.03440776806933215
plot_id,batch_id 0 59 miss% 0.03368829392753103
plot_id,batch_id 0 60 miss% 0.045875139550013794
plot_id,batch_id 0 61 miss% 0.022192529726733835
plot_id,batch_id 0 62 miss% 0.07653913470940484
plot_id,batch_id 0 63 miss% 0.045288412347303394
plot_id,batch_id 0 64 miss% 0.033125616152913075
plot_id,batch_id 0 65 miss% 0.06275425505320202
plot_id,batch_id 0 66 miss% 0.04218719862277381
plot_id,batch_id 0 67 miss% Launcher: Job 144 completed in 16938 seconds.
Launcher: Task 31 done. Exiting.
0.14212449053581477
plot_id,batch_id 0 66 miss% 0.04600561230912435
plot_id,batch_id 0 67 miss% 0.023657677075086984
plot_id,batch_id 0 68 miss% 0.04547390528190252
plot_id,batch_id 0 69 miss% 0.09249344084004282
plot_id,batch_id 0 70 miss% 0.1069643464250054
plot_id,batch_id 0 71 miss% 0.04752926861844247
plot_id,batch_id 0 72 miss% 0.049358585294147266
plot_id,batch_id 0 73 miss% 0.05506275664298288
plot_id,batch_id 0 74 miss% 0.0657678709243446
plot_id,batch_id 0 75 miss% 0.03333089996526886
plot_id,batch_id 0 76 miss% 0.09924324217994246
plot_id,batch_id 0 77 miss% 0.04184138500025265
plot_id,batch_id 0 78 miss% 0.0672513410548356
plot_id,batch_id 0 79 miss% 0.08070606200618782
plot_id,batch_id 0 80 miss% 0.08068805119593209
plot_id,batch_id 0 81 miss% 0.1112064221146218
plot_id,batch_id 0 82 miss% 0.06506014740199796
plot_id,batch_id 0 83 miss% 0.11608933825730139
plot_id,batch_id 0 84 miss% 0.06897556775532147
plot_id,batch_id 0 85 miss% 0.07016027334112765
plot_id,batch_id 0 86 miss% 0.0571004838594696
plot_id,batch_id 0 87 miss% 0.06526873819560314
plot_id,batch_id 0 88 miss% 0.08438652060143519
plot_id,batch_id 0 89 miss% 0.05243191334020701
plot_id,batch_id 0 90 miss% 0.0367457460040321
plot_id,batch_id 0 91 miss% 0.027177986699384146
plot_id,batch_id 0 92 miss% 0.08151902836726911
plot_id,batch_id 0 93 miss% 0.04089792693191592
plot_id,batch_id 0 94 miss% 0.06601437257805527
plot_id,batch_id 0 95 miss% 0.0519170962777247
plot_id,batch_id 0 96 miss% 0.04705980688020631
plot_id,batch_id 0 97 miss% 0.03182408763991539
plot_id,batch_id 0 98 miss% 0.03091215445469081
plot_id,batch_id 0 99 miss% 0.03538001650056542
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08235252 0.04860375 0.10458241 0.07943757 0.04808867 0.05223521
 0.0707182  0.07597658 0.10054547 0.12657137 0.04125229 0.06467891
 0.07077117 0.05618615 0.08325593 0.04724633 0.1054041  0.04307201
 0.06638454 0.0862594  0.08092883 0.08240306 0.13600052 0.05819132
 0.05841838 0.06031451 0.07082188 0.07720355 0.03769033 0.03777382
 0.08525715 0.13135238 0.07875228 0.07283048 0.07782203 0.03769404
 0.06559689 0.08547622 0.07082687 0.03816942 0.04553915 0.05328334
 0.05993237 0.13767925 0.11554427 0.05211691 0.01986034 0.0392929
 0.07214685 0.04680901 0.11478943 0.04222696 0.04500154 0.10081844
 0.09301476 0.07596705 0.106221   0.05331108 0.04213816 0.08810653
 0.02861309 0.02130749 0.04502706 0.07200691 0.07628502 0.14212449
 0.04600561 0.02365768 0.04547391 0.09249344 0.10696435 0.04752927
 0.04935859 0.05506276 0.06576787 0.0333309  0.09924324 0.04184139
 0.06725134 0.08070606 0.08068805 0.11120642 0.06506015 0.11608934
 0.06897557 0.07016027 0.05710048 0.06526874 0.08438652 0.05243191
 0.03674575 0.02717799 0.08151903 0.04089793 0.06601437 0.0519171
 0.04705981 0.03182409 0.03091215 0.03538002]
for model  228 the mean error 0.06761812015772266
all id 228 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 31 out win 4 err 0.06761812015772266 time 16697.225750684738
Launcher: Job 229 completed in 16955 seconds.
Launcher: Task 27 done. Exiting.
plot_id,batch_id 0 65 miss% 0.16920628467715637
plot_id,batch_id 0 66 miss% 0.0365013580515791
plot_id,batch_id 0 67 miss% 0.024835540252755674
plot_id,batch_id 0 68 miss% 0.04678528736814772
plot_id,batch_id 0 69 miss% 0.08856975809303755
plot_id,batch_id 0 70 miss% 0.0655356376995925
plot_id,batch_id 0 71 miss% 0.04180823665458283
plot_id,batch_id 0 72 miss% 0.07184131554239549
plot_id,batch_id 0 73 miss% 0.057814149900724074
plot_id,batch_id 0 74 miss% 0.10624192149853054
plot_id,batch_id 0 75 miss% 0.04592530673634396
plot_id,batch_id 0 76 miss% 0.052854795768452104
plot_id,batch_id 0 77 miss% 0.0361899595971153
plot_id,batch_id 0 78 miss% 0.03014056706465586
plot_id,batch_id 0 79 miss% 0.10966574213645662
plot_id,batch_id 0 80 miss% 0.04160096541405564
plot_id,batch_id 0 81 miss% 0.11751512981743012
plot_id,batch_id 0 82 miss% 0.04655527784273073
plot_id,batch_id 0 83 miss% 0.10549705015297926
plot_id,batch_id 0 84 miss% 0.05650612649162672
plot_id,batch_id 0 85 miss% 0.04588690630199918
plot_id,batch_id 0 86 miss% 0.0608166312765996
plot_id,batch_id 0 87 miss% 0.048862799016389714
plot_id,batch_id 0 88 miss% 0.07935906333517148
plot_id,batch_id 0 89 miss% 0.057268571568679726
plot_id,batch_id 0 90 miss% 0.017323451801803485
plot_id,batch_id 0 91 miss% 0.042130323711639675
plot_id,batch_id 0 92 miss% 0.04509466577433404
plot_id,batch_id 0 93 miss% 0.045996256486303136
plot_id,batch_id 0 94 miss% 0.052375816610676605
plot_id,batch_id 0 95 miss% 0.07815116595172658
plot_id,batch_id 0 96 miss% 0.09671454735425004
plot_id,batch_id 0 97 miss% 0.0347049133235672
plot_id,batch_id 0 98 miss% 0.028080452900129803
plot_id,batch_id 0 99 miss% 0.0677492827618083
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0550043  0.04581139 0.10816994 0.03234088 0.04680129 0.03160917
 0.01649712 0.07825984 0.05355845 0.05686892 0.03359588 0.05306078
 0.05291336 0.0791757  0.0623962  0.03080767 0.08504525 0.04400779
 0.07836003 0.07807112 0.0417352  0.03366862 0.06667651 0.04529527
 0.05818631 0.06564271 0.05895497 0.05567846 0.05156301 0.06244675
 0.01921131 0.08545524 0.10332076 0.02881356 0.03790021 0.01896817
 0.0437943  0.06490274 0.06474853 0.03546587 0.07175389 0.03545002
 0.01919843 0.03080866 0.02321282 0.07172212 0.0440793  0.03035216
 0.04042621 0.05824415 0.11871949 0.04437118 0.02600037 0.02891861
 0.04510159 0.12187558 0.06035307 0.05634587 0.06749585 0.04557531
 0.03093231 0.02581714 0.05289196 0.06906325 0.06151775 0.16920628
 0.03650136 0.02483554 0.04678529 0.08856976 0.06553564 0.04180824
 0.07184132 0.05781415 0.10624192 0.04592531 0.0528548  0.03618996
 0.03014057 0.10966574 0.04160097 0.11751513 0.04655528 0.10549705
 0.05650613 0.04588691 0.06081663 0.0488628  0.07935906 0.05726857
 0.01732345 0.04213032 0.04509467 0.04599626 0.05237582 0.07815117
 0.09671455 0.03470491 0.02808045 0.06774928]
for model  201 the mean error 0.05597115945545595
all id 201 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 31 out win 4 err 0.05597115945545595 time 16703.65442466736
Launcher: Job 202 completed in 16960 seconds.
Launcher: Task 1 done. Exiting.
0.015646029748989803
plot_id,batch_id 0 68 miss% 0.054420965110700305
plot_id,batch_id 0 69 miss% 0.08079804164091879
plot_id,batch_id 0 70 miss% 0.050752594090960994
plot_id,batch_id 0 71 miss% 0.04145976862091354
plot_id,batch_id 0 72 miss% 0.07635250312017698
plot_id,batch_id 0 73 miss% 0.06354203204080314
plot_id,batch_id 0 74 miss% 0.08943288102123284
plot_id,batch_id 0 75 miss% 0.07357048576952369
plot_id,batch_id 0 76 miss% 0.08886362268653118
plot_id,batch_id 0 77 miss% 0.04348694061679151
plot_id,batch_id 0 78 miss% 0.041693584317218105
plot_id,batch_id 0 79 miss% 0.09390713439278166
plot_id,batch_id 0 80 miss% 0.033976199943389004
plot_id,batch_id 0 81 miss% 0.0912822831401652
plot_id,batch_id 0 82 miss% 0.08968886291983116
plot_id,batch_id 0 83 miss% 0.051808012309225916
plot_id,batch_id 0 84 miss% 0.10039148640402387
plot_id,batch_id 0 85 miss% 0.03002251786868973
plot_id,batch_id 0 86 miss% 0.08422237880017236
plot_id,batch_id 0 87 miss% 0.06159397746183607
plot_id,batch_id 0 88 miss% 0.08779777594071683
plot_id,batch_id 0 89 miss% 0.05498685734626154
plot_id,batch_id 0 90 miss% 0.0610986342179356
plot_id,batch_id 0 91 miss% 0.06550798106459516
plot_id,batch_id 0 92 miss% 0.04161726443010327
plot_id,batch_id 0 93 miss% 0.039143165306129775
plot_id,batch_id 0 94 miss% 0.11017491061042814
plot_id,batch_id 0 95 miss% 0.08454318937477198
plot_id,batch_id 0 96 miss% 0.06889655849698695
plot_id,batch_id 0 97 miss% 0.10598381207877312
plot_id,batch_id 0 98 miss% 0.04051221832944559
plot_id,batch_id 0 99 miss% 0.060697374193598226
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06133653 0.04386881 0.12454908 0.05339523 0.0600329  0.04586313
 0.06638212 0.09515463 0.08282909 0.05294525 0.03725174 0.06633554
 0.08382663 0.06806769 0.1066324  0.04195975 0.18127323 0.04224779
 0.06116111 0.05755244 0.07789306 0.06088128 0.05831728 0.03829993
 0.06651262 0.04336152 0.06031354 0.05214749 0.03119133 0.03783364
 0.06516381 0.07882862 0.11208354 0.04460222 0.03631134 0.05060087
 0.15985701 0.05437249 0.05204051 0.02786396 0.09947658 0.09499853
 0.0229844  0.04954054 0.06463571 0.07437741 0.03951131 0.03274845
 0.02672137 0.0326381  0.12299502 0.06695721 0.03077144 0.03291067
 0.03258285 0.05179329 0.06737423 0.03888493 0.03440777 0.03368829
 0.04587514 0.02219253 0.07653913 0.04528841 0.03312562 0.06275426
 0.0421872  0.01564603 0.05442097 0.08079804 0.05075259 0.04145977
 0.0763525  0.06354203 0.08943288 0.07357049 0.08886362 0.04348694
 0.04169358 0.09390713 0.0339762  0.09128228 0.08968886 0.05180801
 0.10039149 0.03002252 0.08422238 0.06159398 0.08779778 0.05498686
 0.06109863 0.06550798 0.04161726 0.03914317 0.11017491 0.08454319
 0.06889656 0.10598381 0.04051222 0.06069737]
for model  182 the mean error 0.06196973571338096
all id 182 hidden_dim 16 learning_rate 0.0025 num_layers 5 frames 31 out win 6 err 0.06196973571338096 time 16723.836084842682
Launcher: Job 183 completed in 16987 seconds.
Launcher: Task 190 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  265233
Epoch:0, Train loss:0.669099, valid loss:0.640473
Epoch:1, Train loss:0.296235, valid loss:0.004720
Epoch:2, Train loss:0.008072, valid loss:0.003637
Epoch:3, Train loss:0.006114, valid loss:0.002298
Epoch:4, Train loss:0.005109, valid loss:0.002431
Epoch:5, Train loss:0.004408, valid loss:0.002185
Epoch:6, Train loss:0.003843, valid loss:0.001962
Epoch:7, Train loss:0.003489, valid loss:0.001741
Epoch:8, Train loss:0.002955, valid loss:0.001714
Epoch:9, Train loss:0.002809, valid loss:0.001526
Epoch:10, Train loss:0.002593, valid loss:0.001174
Epoch:11, Train loss:0.001865, valid loss:0.001044
Epoch:12, Train loss:0.001770, valid loss:0.001147
Epoch:13, Train loss:0.001746, valid loss:0.001073
Epoch:14, Train loss:0.001670, valid loss:0.001123
Epoch:15, Train loss:0.001587, valid loss:0.000940
Epoch:16, Train loss:0.001587, valid loss:0.000938
Epoch:17, Train loss:0.001538, valid loss:0.001075
Epoch:18, Train loss:0.001407, valid loss:0.000931
Epoch:19, Train loss:0.001370, valid loss:0.000915
Epoch:20, Train loss:0.001336, valid loss:0.000900
Epoch:21, Train loss:0.000987, valid loss:0.000695
Epoch:22, Train loss:0.000983, valid loss:0.000739
Epoch:23, Train loss:0.000953, valid loss:0.000774
Epoch:24, Train loss:0.000944, valid loss:0.000803
Epoch:25, Train loss:0.000934, valid loss:0.000868
Epoch:26, Train loss:0.000925, valid loss:0.000697
Epoch:27, Train loss:0.000885, valid loss:0.000754
Epoch:28, Train loss:0.000878, valid loss:0.000746
Epoch:29, Train loss:0.000882, valid loss:0.000741
Epoch:30, Train loss:0.000842, valid loss:0.000774
Epoch:31, Train loss:0.000686, valid loss:0.000607
Epoch:32, Train loss:0.000678, valid loss:0.000659
Epoch:33, Train loss:0.000666, valid loss:0.000651
Epoch:34, Train loss:0.000673, valid loss:0.000633
Epoch:35, Train loss:0.000654, valid loss:0.000626
Epoch:36, Train loss:0.000644, valid loss:0.000634
Epoch:37, Train loss:0.000642, valid loss:0.000624
Epoch:38, Train loss:0.000633, valid loss:0.000683
Epoch:39, Train loss:0.000629, valid loss:0.000636
Epoch:40, Train loss:0.000605, valid loss:0.000676
Epoch:41, Train loss:0.000549, valid loss:0.000614
Epoch:42, Train loss:0.000541, valid loss:0.000669
Epoch:43, Train loss:0.000536, valid loss:0.000607
Epoch:44, Train loss:0.000531, valid loss:0.000646
Epoch:45, Train loss:0.000532, valid loss:0.000608
Epoch:46, Train loss:0.000518, valid loss:0.000607
Epoch:47, Train loss:0.000523, valid loss:0.000606
Epoch:48, Train loss:0.000517, valid loss:0.000624
Epoch:49, Train loss:0.000522, valid loss:0.000616
Epoch:50, Train loss:0.000505, valid loss:0.000611
Epoch:51, Train loss:0.000474, valid loss:0.000599
Epoch:52, Train loss:0.000470, valid loss:0.000605
Epoch:53, Train loss:0.000468, valid loss:0.000597
Epoch:54, Train loss:0.000467, valid loss:0.000599
Epoch:55, Train loss:0.000466, valid loss:0.000599
Epoch:56, Train loss:0.000465, valid loss:0.000596
Epoch:57, Train loss:0.000465, valid loss:0.000595
Epoch:58, Train loss:0.000464, valid loss:0.000596
Epoch:59, Train loss:0.000464, valid loss:0.000592
Epoch:60, Train loss:0.000464, valid loss:0.000601
training time 16819.47791981697
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.2740222609648797
plot_id,batch_id 0 1 miss% 0.36950117704312585
plot_id,batch_id 0 2 miss% 0.4162091360103356
plot_id,batch_id 0 3 miss% 0.34440227570718956
plot_id,batch_id 0 4 miss% 0.3080344094585407
plot_id,batch_id 0 5 miss% 0.3004744837911977
plot_id,batch_id 0 6 miss% 0.2978568105982318
plot_id,batch_id 0 7 miss% 0.4991972552961733
plot_id,batch_id 0 8 miss% 0.62738996053781
plot_id,batch_id 0 9 miss% 0.5995058665303326
plot_id,batch_id 0 10 miss% 0.2708559719144057
plot_id,batch_id 0 11 miss% 0.30463950834998343
plot_id,batch_id 0 12 miss% 0.34667132391934874
plot_id,batch_id 0 13 miss% 0.33875095251369736
plot_id,batch_id 0 14 miss% 0.43123160101261804
plot_id,batch_id 0 15 miss% 0.3062566351415011
plot_id,batch_id 0 16 miss% 0.4754158785341972
plot_id,batch_id 0 17 miss% 0.4092938256477305
plot_id,batch_id 0 18 miss% 0.44530766957099155
plot_id,batch_id 0 19 miss% 0.37861493370314736
plot_id,batch_id 0 20 miss% 0.2995334240032375
plot_id,batch_id 0 21 miss% 0.4584503965997383
plot_id,batch_id 0 22 miss% 0.46811052562286876
plot_id,batch_id 0 23 miss% 0.4490976171606856
plot_id,batch_id 0 24 miss% 0.3947804998472362
plot_id,batch_id 0 25 miss% 0.35732206235665137
plot_id,batch_id 0 26 miss% 0.417195007921447
plot_id,batch_id 0 27 miss% 0.4103504078516508
plot_id,batch_id 0 28 miss% 0.4282087425059174
plot_id,batch_id 0 29 miss% 0.43371537668478677
plot_id,batch_id 0 30 miss% 0.3139265994998049
plot_id,batch_id 0 31 miss% 0.4573682523033748
plot_id,batch_id 0 32 miss% 0.4673802022929192
plot_id,batch_id 0 33 miss% 0.44621198850725985
plot_id,batch_id 0 34 miss% 0.3728791185670553
plot_id,batch_id 0 35 miss% 0.28761036603769924
plot_id,batch_id 0 36 miss% 0.4786636596844926
plot_id,batch_id 0 37 miss% 0.4164836324837053
plot_id,batch_id 0 38 miss% 0.4642754347831993
plot_id,batch_id 0 39 miss% 0.45589729053273054
plot_id,batch_id 0 40 miss% 0.3774905239836201
plot_id,batch_id 0 41 miss% 0.40791289456497426
plot_id,batch_id 0 42 miss% 0.4049717536723711
plot_id,batch_id 0 43 miss% 0.37168345698917465
plot_id,batch_id 0 44 miss% 0.3367917673152635
plot_id,batch_id 0 45 miss% 0.3980578371268066
plot_id,batch_id 0 46 miss% 0.39394899365627567
plot_id,batch_id 0 47 miss% 0.4767771152113094
plot_id,batch_id 0 48 miss% 0.48251946081921204
plot_id,batch_id 0 49 miss% 0.33372302056337605
plot_id,batch_id 0 50 miss% 0.5254446065539992
plot_id,batch_id 0 51 miss% 0.4330396447289757
plot_id,batch_id 0 52 miss% 0.4449331245997017
plot_id,batch_id 0 53 miss% 0.4103160269825177
plot_id,batch_id 0 54 miss% 0.37323740233593206
plot_id,batch_id 0 55 miss% 0.43858429352781114
plot_id,batch_id 0 56 miss% 0.5676669671937398
plot_id,batch_id 0 57 miss% 0.47556781491606037
plot_id,batch_id 0 58 miss% 0.44350205782729635
plot_id,batch_id 0 59 miss% 0.5032053011006231
plot_id,batch_id 0 60 miss% 0.23786692244181895
plot_id,batch_id 0 61 miss% 0.24314706015497956
plot_id,batch_id 0 62 miss% 0.35778628325206285
plot_id,batch_id 0 63 miss% 0.34241337401585636
plot_id,batch_id 0 64 miss% 0.36775350576701316
plot_id,batch_id 0 65 miss% 0.28875121305758294
plot_id,batch_id 0 66 miss% 0.3754785102191819
plot_id,batch_id 0 67 miss% 0.25217475119560917
plot_id,batch_id 0 68 miss% 0.3765437317339049
plot_id,batch_id 0 69 miss% 0.3761034138111113
plot_id,batch_id 0 70 miss% 0.2562867893017189
plot_id,batch_id 0 71 miss% 0.28662878214256
plot_id,batch_id 0 72 miss% 0.3968264336990593
plot_id,batch_id 0 73 miss% 0.3242553990545096
plot_id,batch_id 0 74 miss% 0.3229414704764574
plot_id,batch_id 0 75 miss% 0.25086017652464765
plot_id,batch_id 0 76 miss% 0.27503677689599526
plot_id,batch_id 0 77 miss% 0.3383387074319743
plot_id,batch_id 0 78 miss% 0.2794782168070684
plot_id,batch_id 0 79 miss% 0.319266837565884
plot_id,batch_id 0 80 miss% 0.3062090835700627
plot_id,batch_id 0 81 miss% 0.4594072450198918
plot_id,batch_id 0 82 miss% 0.36991709129653877
plot_id,batch_id 0 83 miss% 0.409746998050729
plot_id,batch_id 0 84 miss% 0.35886971844319776
plot_id,batch_id 0 85 miss% 0.2635757586130308
plot_id,batch_id 0 86 miss% 0.26405809838181243
plot_id,batch_id 0 87 miss% 0.3385962802914263
plot_id,batch_id 0 88 miss% 0.39026541647763763
plot_id,batch_id 0 89 miss% 0.3566509431202967
plot_id,batch_id 0 90 miss% 0.20757390299986359
plot_id,batch_id 0 91 miss% 0.2695742322632395
plot_id,batch_id 0 92 miss% 0.3325910245702088
plot_id,batch_id 0 93 miss% 0.28324690005324943
plot_id,batch_id 0 94 miss% 0.4313296436321162
plot_id,batch_id 0 95 miss% 0.22658331791156697
plot_id,batch_id 0 96 miss% 0.26668463457108077
plot_id,batch_id 0 97 miss% 0.3534696005389235
plot_id,batch_id 0 98 miss% 0.3539196369975337
plot_id,batch_id 0 99 miss% 0.3844912395870507
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.27402226 0.36950118 0.41620914 0.34440228 0.30803441 0.30047448
 0.29785681 0.49919726 0.62738996 0.59950587 0.27085597 0.30463951
 0.34667132 0.33875095 0.4312316  0.30625664 0.47541588 0.40929383
 0.44530767 0.37861493 0.29953342 0.4584504  0.46811053 0.44909762
 0.3947805  0.35732206 0.41719501 0.41035041 0.42820874 0.43371538
 0.3139266  0.45736825 0.4673802  0.44621199 0.37287912 0.28761037
 0.47866366 0.41648363 0.46427543 0.45589729 0.37749052 0.40791289
 0.40497175 0.37168346 0.33679177 0.39805784 0.39394899 0.47677712
 0.48251946 0.33372302 0.52544461 0.43303964 0.44493312 0.41031603
 0.3732374  0.43858429 0.56766697 0.47556781 0.44350206 0.5032053
 0.23786692 0.24314706 0.35778628 0.34241337 0.36775351 0.28875121
 0.37547851 0.25217475 0.37654373 0.37610341 0.25628679 0.28662878
 0.39682643 0.3242554  0.32294147 0.25086018 0.27503678 0.33833871
 0.27947822 0.31926684 0.30620908 0.45940725 0.36991709 0.409747
 0.35886972 0.26357576 0.2640581  0.33859628 0.39026542 0.35665094
 0.2075739  0.26957423 0.33259102 0.2832469  0.43132964 0.22658332
 0.26668463 0.3534696  0.35391964 0.38449124]
for model  24 the mean error 0.37515165727099387
all id 24 hidden_dim 32 learning_rate 0.0025 num_layers 5 frames 21 out win 4 err 0.37515165727099387 time 16819.47791981697
Launcher: Job 25 completed in 17058 seconds.
Launcher: Task 96 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  209681
Epoch:0, Train loss:0.451184, valid loss:0.450976
Epoch:1, Train loss:0.037645, valid loss:0.004757
Epoch:2, Train loss:0.010586, valid loss:0.003115
Epoch:3, Train loss:0.007424, valid loss:0.002364
Epoch:4, Train loss:0.006420, valid loss:0.001940
Epoch:5, Train loss:0.005425, valid loss:0.001915
Epoch:6, Train loss:0.003127, valid loss:0.001408
Epoch:7, Train loss:0.002706, valid loss:0.001493
Epoch:8, Train loss:0.002492, valid loss:0.001404
Epoch:9, Train loss:0.002390, valid loss:0.001169
Epoch:10, Train loss:0.002060, valid loss:0.001014
Epoch:11, Train loss:0.001598, valid loss:0.000961
Epoch:12, Train loss:0.001516, valid loss:0.000970
Epoch:13, Train loss:0.001457, valid loss:0.000890
Epoch:14, Train loss:0.001428, valid loss:0.000872
Epoch:15, Train loss:0.001355, valid loss:0.000930
Epoch:16, Train loss:0.001315, valid loss:0.000803
Epoch:17, Train loss:0.001310, valid loss:0.000788
Epoch:18, Train loss:0.001263, valid loss:0.000815
Epoch:19, Train loss:0.001168, valid loss:0.000866
Epoch:20, Train loss:0.001161, valid loss:0.001164
Epoch:21, Train loss:0.000954, valid loss:0.000723
Epoch:22, Train loss:0.000881, valid loss:0.000729
Epoch:23, Train loss:0.000884, valid loss:0.000681
Epoch:24, Train loss:0.000844, valid loss:0.000721
Epoch:25, Train loss:0.000842, valid loss:0.000686
Epoch:26, Train loss:0.000837, valid loss:0.000742
Epoch:27, Train loss:0.000830, valid loss:0.000728
Epoch:28, Train loss:0.000800, valid loss:0.000704
Epoch:29, Train loss:0.000800, valid loss:0.000677
Epoch:30, Train loss:0.000763, valid loss:0.000685
Epoch:31, Train loss:0.000655, valid loss:0.000637
Epoch:32, Train loss:0.000632, valid loss:0.000635
Epoch:33, Train loss:0.000646, valid loss:0.000637
Epoch:34, Train loss:0.000627, valid loss:0.000698
Epoch:35, Train loss:0.000609, valid loss:0.000629
Epoch:36, Train loss:0.000615, valid loss:0.000648
Epoch:37, Train loss:0.000596, valid loss:0.000700
Epoch:38, Train loss:0.000608, valid loss:0.000591
Epoch:39, Train loss:0.000582, valid loss:0.000632
Epoch:40, Train loss:0.000577, valid loss:0.000601
Epoch:41, Train loss:0.000518, valid loss:0.000624
Epoch:42, Train loss:0.000513, valid loss:0.000585
Epoch:43, Train loss:0.000510, valid loss:0.000598
Epoch:44, Train loss:0.000502, valid loss:0.000592
Epoch:45, Train loss:0.000507, valid loss:0.000568
Epoch:46, Train loss:0.000507, valid loss:0.000581
Epoch:47, Train loss:0.000497, valid loss:0.000596
Epoch:48, Train loss:0.000495, valid loss:0.000584
Epoch:49, Train loss:0.000489, valid loss:0.000594
Epoch:50, Train loss:0.000489, valid loss:0.000575
Epoch:51, Train loss:0.000464, valid loss:0.000571
Epoch:52, Train loss:0.000455, valid loss:0.000562
Epoch:53, Train loss:0.000452, valid loss:0.000568
Epoch:54, Train loss:0.000450, valid loss:0.000561
Epoch:55, Train loss:0.000449, valid loss:0.000573
Epoch:56, Train loss:0.000447, valid loss:0.000562
Epoch:57, Train loss:0.000447, valid loss:0.000560
Epoch:58, Train loss:0.000446, valid loss:0.000569
Epoch:59, Train loss:0.000445, valid loss:0.000564
Epoch:60, Train loss:0.000445, valid loss:0.000560
training time 16853.22337579727
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.05764884551481896
plot_id,batch_id 0 1 miss% 0.034482660942378535
plot_id,batch_id 0 2 miss% 0.0853499925526497
plot_id,batch_id 0 3 miss% 0.04084001176847033
plot_id,batch_id 0 4 miss% 0.06037648826603576
plot_id,batch_id 0 5 miss% 0.059612095063991656
plot_id,batch_id 0 6 miss% 0.05715071677467139
plot_id,batch_id 0 7 miss% 0.05762559123236445
plot_id,batch_id 0 8 miss% 0.0880964975784902
plot_id,batch_id 0 9 miss% 0.04515539662792153
plot_id,batch_id 0 10 miss% 0.04517679557154725
plot_id,batch_id 0 11 miss% 0.06919801645369944
plot_id,batch_id 0 12 miss% 0.06531248030125786
plot_id,batch_id 0 13 miss% 0.08917406981050875
plot_id,batch_id 0 14 miss% 0.0720336721473629
plot_id,batch_id 0 15 miss% 0.09249704680984157
plot_id,batch_id 0 16 miss% 0.09682313108836788
plot_id,batch_id 0 17 miss% 0.03489898676338768
plot_id,batch_id 0 18 miss% 0.03912138541824369
plot_id,batch_id 0 19 miss% 0.14033405573476682
plot_id,batch_id 0 20 miss% 0.042002743284440834
plot_id,batch_id 0 21 miss% 0.046489553182207856
plot_id,batch_id 0 22 miss% 0.06053052636102629
plot_id,batch_id 0 23 miss% 0.039752039256077815
plot_id,batch_id 0 24 miss% 0.05578332273045385
plot_id,batch_id 0 25 miss% 0.07280613296824498
plot_id,batch_id 0 26 miss% 0.03898583658238922
plot_id,batch_id 0 27 miss% 0.04784729722214755
plot_id,batch_id 0 28 miss% 0.03403790142778907
plot_id,batch_id 0 29 miss% 0.04345392352349322
plot_id,batch_id 0 30 miss% 0.03177366722460031
plot_id,batch_id 0 31 miss% 0.07628817153323947
plot_id,batch_id 0 32 miss% 0.10766972319871912
plot_id,batch_id 0 33 miss% 0.05802692625954369
plot_id,batch_id 0 34 miss% 0.07057122821644979
plot_id,batch_id 0 35 miss% 0.030540815655330417
plot_id,batch_id 0 36 miss% 0.10994785287111568
plot_id,batch_id 0 37 miss% 0.06139491827665562
plot_id,batch_id 0 38 miss% 0.04260246664487741
plot_id,batch_id 0 39 miss% 0.07449023298255074
plot_id,batch_id 0 40 miss% 0.043156462300341224
plot_id,batch_id 0 41 miss% 0.03819033847424668
plot_id,batch_id 0 42 miss% 0.07691616534721167
plot_id,batch_id 0 43 miss% 0.06292377783791003
plot_id,batch_id 0 44 miss% 0.035958674631840604
plot_id,batch_id 0 45 miss% 0.0890112023162979
plot_id,batch_id 0 46 miss% 0.028436748528568898
plot_id,batch_id 0 47 miss% 0.029862145255774356
plot_id,batch_id 0 48 miss% 0.024595928207483794
plot_id,batch_id 0 49 miss% 0.028355283429913367
plot_id,batch_id 0 50 miss% 0.12191362871485995
plot_id,batch_id 0 51 miss% 0.025099702377171115
plot_id,batch_id 0 52 miss% 0.03817969845651436
plot_id,batch_id 0 53 miss% 0.030903654218998693
plot_id,batch_id 0 54 miss% 0.05403017201481668
plot_id,batch_id 0 55 miss% 0.11869640888062782
plot_id,batch_id 0 56 miss% 0.0912088449210557
plot_id,batch_id 0 57 miss% 0.025414718794805714
plot_id,batch_id 0 58 miss% 0.03467666878487861
plot_id,batch_id 0 59 miss% 0.023890866621802245
plot_id,batch_id 0 60 miss% 0.037119625242870484
plot_id,batch_id 0 61 miss% 0.0402012781013081
plot_id,batch_id 0 62 miss% 0.06811443188926726
plot_id,batch_id 0 63 miss% 0.027633061716151437
plot_id,batch_id 0 64 miss% 0.05545132834488683
plot_id,batch_id 0 65 miss% 0.05143364796513112
plot_id,batch_id 0 66 miss% 0.0192052533267884
plot_id,batch_id 0 67 miss% 0.01840119553451874
plot_id,batch_id 0 68 miss% 0.036245676153112374
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  69649
Epoch:0, Train loss:0.458467, valid loss:0.438974
Epoch:1, Train loss:0.021215, valid loss:0.002737
Epoch:2, Train loss:0.005425, valid loss:0.002193
Epoch:3, Train loss:0.004585, valid loss:0.001731
Epoch:4, Train loss:0.003126, valid loss:0.001083
Epoch:5, Train loss:0.002162, valid loss:0.001066
Epoch:6, Train loss:0.001975, valid loss:0.000892
Epoch:7, Train loss:0.001787, valid loss:0.000833
Epoch:8, Train loss:0.001710, valid loss:0.000906
Epoch:9, Train loss:0.001591, valid loss:0.000749
Epoch:10, Train loss:0.001553, valid loss:0.000751
Epoch:11, Train loss:0.001159, valid loss:0.000603
Epoch:12, Train loss:0.001123, valid loss:0.000702
Epoch:13, Train loss:0.001115, valid loss:0.000657
Epoch:14, Train loss:0.001066, valid loss:0.000685
Epoch:15, Train loss:0.001058, valid loss:0.000640
Epoch:16, Train loss:0.001025, valid loss:0.000550
Epoch:17, Train loss:0.001014, valid loss:0.000637
Epoch:18, Train loss:0.000999, valid loss:0.000618
Epoch:19, Train loss:0.000982, valid loss:0.000539
Epoch:20, Train loss:0.000939, valid loss:0.000760
Epoch:21, Train loss:0.000776, valid loss:0.000532
Epoch:22, Train loss:0.000753, valid loss:0.000508
Epoch:23, Train loss:0.000752, valid loss:0.000499
Epoch:24, Train loss:0.000722, valid loss:0.000588
Epoch:25, Train loss:0.000752, valid loss:0.000502
Epoch:26, Train loss:0.000709, valid loss:0.000504
Epoch:27, Train loss:0.000710, valid loss:0.000502
Epoch:28, Train loss:0.000698, valid loss:0.000574
Epoch:29, Train loss:0.000700, valid loss:0.000563
Epoch:30, Train loss:0.000685, valid loss:0.000561
Epoch:31, Train loss:0.000596, valid loss:0.000467
Epoch:32, Train loss:0.000584, valid loss:0.000442
Epoch:33, Train loss:0.000577, valid loss:0.000465
Epoch:34, Train loss:0.000588, valid loss:0.000475
Epoch:35, Train loss:0.000576, valid loss:0.000463
Epoch:36, Train loss:0.000567, valid loss:0.000445
Epoch:37, Train loss:0.000564, valid loss:0.000428
Epoch:38, Train loss:0.000556, valid loss:0.000434
Epoch:39, Train loss:0.000550, valid loss:0.000451
Epoch:40, Train loss:0.000560, valid loss:0.000449
Epoch:41, Train loss:0.000503, valid loss:0.000432
Epoch:42, Train loss:0.000503, valid loss:0.000451
Epoch:43, Train loss:0.000501, valid loss:0.000426
Epoch:44, Train loss:0.000500, valid loss:0.000438
Epoch:45, Train loss:0.000501, valid loss:0.000434
Epoch:46, Train loss:0.000493, valid loss:0.000422
Epoch:47, Train loss:0.000496, valid loss:0.000424
Epoch:48, Train loss:0.000489, valid loss:0.000422
Epoch:49, Train loss:0.000492, valid loss:0.000410
Epoch:50, Train loss:0.000485, valid loss:0.000424
Epoch:51, Train loss:0.000464, valid loss:0.000414
Epoch:52, Train loss:0.000460, valid loss:0.000414
Epoch:53, Train loss:0.000458, valid loss:0.000411
Epoch:54, Train loss:0.000457, valid loss:0.000412
Epoch:55, Train loss:0.000457, valid loss:0.000412
Epoch:56, Train loss:0.000456, valid loss:0.000412
Epoch:57, Train loss:0.000455, valid loss:0.000410
Epoch:58, Train loss:0.000455, valid loss:0.000409
Epoch:59, Train loss:0.000454, valid loss:0.000409
Epoch:60, Train loss:0.000454, valid loss:0.000410
training time 16868.08417248726
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.11790306941277846
plot_id,batch_id 0 1 miss% 0.04909800756435829
plot_id,batch_id 0 2 miss% 0.09222231702222916
plot_id,batch_id 0 3 miss% 0.07753072912772521
plot_id,batch_id 0 4 miss% 0.06051492534494326
plot_id,batch_id 0 5 miss% 0.061582723232793596
plot_id,batch_id 0 6 miss% 0.036356224202253797
plot_id,batch_id 0 7 miss% 0.08957868316509404
plot_id,batch_id 0 8 miss% 0.056419952991001346
plot_id,batch_id 0 9 miss% 0.05621516004499165
plot_id,batch_id 0 10 miss% 0.019348947599462114
plot_id,batch_id 0 11 miss% 0.04120782155639675
plot_id,batch_id 0 12 miss% 0.07816830231941317
plot_id,batch_id 0 13 miss% 0.09251321385991992
plot_id,batch_id 0 14 miss% 0.08631101164305127
plot_id,batch_id 0 15 miss% 0.0255825512045265
plot_id,batch_id 0 16 miss% 0.19203431641029284
plot_id,batch_id 0 17 miss% 0.028524304573502423
plot_id,batch_id 0 18 miss% 0.0998395674268083
plot_id,batch_id 0 19 miss% 0.09019168930500596
plot_id,batch_id 0 20 miss% 0.05315064289235231
plot_id,batch_id 0 21 miss% 0.04936073086119739
plot_id,batch_id 0 22 miss% 0.04361078781786733
plot_id,batch_id 0 23 miss% 0.04307197690333354
plot_id,batch_id 0 24 miss% 0.032935979329657326
plot_id,batch_id 0 25 miss% 0.10234964562607687
plot_id,batch_id 0 26 miss% 0.06934483496934753
plot_id,batch_id 0 27 miss% 0.04480722984503131
plot_id,batch_id 0 28 miss% 0.037264447919011866
plot_id,batch_id 0 29 miss% 0.03547551770182029
plot_id,batch_id 0 30 miss% 0.034712131101918335
plot_id,batch_id 0 31 miss% 0.13846972075375513
plot_id,batch_id 0 32 miss% 0.08311587215990976
plot_id,batch_id 0 33 miss% 0.0729357526377914
plot_id,batch_id 0 34 miss% 0.037759708409194216
plot_id,batch_id 0 35 miss% 0.03263339234145669
plot_id,batch_id 0 36 miss% 0.13891842580073516
plot_id,batch_id 0 37 miss% 0.09701754556048606
plot_id,batch_id 0 38 miss% 0.040768410970332146
plot_id,batch_id 0 39 miss% 0.04271507939264372
plot_id,batch_id 0 40 miss% 0.046659115897718345
plot_id,batch_id 0 41 miss% 0.0524902893725632
plot_id,batch_id 0 42 miss% 0.023186496136346147
plot_id,batch_id 0 43 miss% 0.0404269355847295
plot_id,batch_id 0 44 miss% 0.03943982765538985
plot_id,batch_id 0 45 miss% 0.05135711755342595
plot_id,batch_id 0 46 miss% 0.045288387839169095
plot_id,batch_id 0 47 miss% 0.02874573907351302
plot_id,batch_id 0 48 miss% 0.02878399779433389
plot_id,batch_id 0 49 miss% 0.0386987322890121
plot_id,batch_id 0 50 miss% 0.09811929216442163
plot_id,batch_id 0 51 miss% 0.04799025147185204
plot_id,batch_id 0 52 miss% 0.031524882611965524
plot_id,batch_id 0 53 miss% 0.06888211052826619
plot_id,batch_id 0 54 miss% 0.03720952854713345
plot_id,batch_id 0 55 miss% 0.07178157995961963
plot_id,batch_id 0 56 miss% 0.08911530926057767
plot_id,batch_id 0 57 miss% 0.03817405261294733
plot_id,batch_id 0 58 miss% 0.030908683735689616
plot_id,batch_id 0 59 miss% 0.028606543886207498
plot_id,batch_id 0 60 miss% 0.046425506169763896
plot_id,batch_id 0 61 miss% 0.0378973612377099
plot_id,batch_id 0 62 miss% 0.10083921963900025
plot_id,batch_id 0 63 miss% 0.03661286440751051
plot_id,batch_id 0 64 miss% 0.05906247249008516
plot_id,batch_id 0 65 miss% 0.04003571819337497
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  120401
Epoch:0, Train loss:0.480405, valid loss:0.480868
Epoch:1, Train loss:0.058965, valid loss:0.002102
Epoch:2, Train loss:0.003522, valid loss:0.001661
Epoch:3, Train loss:0.002747, valid loss:0.001183
Epoch:4, Train loss:0.002355, valid loss:0.001066
Epoch:5, Train loss:0.002109, valid loss:0.000917
Epoch:6, Train loss:0.001903, valid loss:0.001134
Epoch:7, Train loss:0.001727, valid loss:0.000794
Epoch:8, Train loss:0.001574, valid loss:0.001033
Epoch:9, Train loss:0.001487, valid loss:0.000772
Epoch:10, Train loss:0.001350, valid loss:0.000853
Epoch:11, Train loss:0.001055, valid loss:0.000574
Epoch:12, Train loss:0.001017, valid loss:0.000584
Epoch:13, Train loss:0.000973, valid loss:0.000794
Epoch:14, Train loss:0.000948, valid loss:0.000539
Epoch:15, Train loss:0.000897, valid loss:0.000589
Epoch:16, Train loss:0.000898, valid loss:0.000567
Epoch:17, Train loss:0.000858, valid loss:0.000557
Epoch:18, Train loss:0.000838, valid loss:0.000546
Epoch:19, Train loss:0.000835, valid loss:0.000546
Epoch:20, Train loss:0.000808, valid loss:0.000527
Epoch:21, Train loss:0.000650, valid loss:0.000471
Epoch:22, Train loss:0.000630, valid loss:0.000510
Epoch:23, Train loss:0.000624, valid loss:0.000518
Epoch:24, Train loss:0.000615, valid loss:0.000476
Epoch:25, Train loss:0.000601, valid loss:0.000500
Epoch:26, Train loss:0.000602, valid loss:0.000456
Epoch:27, Train loss:0.000587, valid loss:0.000505
Epoch:28, Train loss:0.000577, valid loss:0.000459
Epoch:29, Train loss:0.000575, valid loss:0.000460
Epoch:30, Train loss:0.000571, valid loss:0.000457
Epoch:31, Train loss:0.000494, valid loss:0.000425
Epoch:32, Train loss:0.000483, valid loss:0.000441
Epoch:33, Train loss:0.000479, valid loss:0.000458
Epoch:34, Train loss:0.000478, valid loss:0.000450
Epoch:35, Train loss:0.000471, valid loss:0.000458
Epoch:36, Train loss:0.000470, valid loss:0.000420
Epoch:37, Train loss:0.000464, valid loss:0.000432
Epoch:38, Train loss:0.000462, valid loss:0.000459
Epoch:39, Train loss:0.000460, valid loss:0.000449
Epoch:40, Train loss:0.000449, valid loss:0.000455
Epoch:41, Train loss:0.000420, valid loss:0.000415
Epoch:42, Train loss:0.000416, valid loss:0.000422
Epoch:43, Train loss:0.000414, valid loss:0.000432
Epoch:44, Train loss:0.000415, valid loss:0.000430
Epoch:45, Train loss:0.000408, valid loss:0.000440
Epoch:46, Train loss:0.000409, valid loss:0.000432
Epoch:47, Train loss:0.000405, valid loss:0.000420
Epoch:48, Train loss:0.000405, valid loss:0.000444
Epoch:49, Train loss:0.000401, valid loss:0.000426
Epoch:50, Train loss:0.000401, valid loss:0.000425
Epoch:51, Train loss:0.000381, valid loss:0.000428
Epoch:52, Train loss:0.000378, valid loss:0.000436
Epoch:53, Train loss:0.000378, valid loss:0.000426
Epoch:54, Train loss:0.000377, valid loss:0.000424
Epoch:55, Train loss:0.000376, valid loss:0.000441
Epoch:56, Train loss:0.000376, valid loss:0.000424
Epoch:57, Train loss:0.000376, valid loss:0.000423
Epoch:58, Train loss:0.000375, valid loss:0.000423
Epoch:59, Train loss:0.000375, valid loss:0.000422
Epoch:60, Train loss:0.000375, valid loss:0.000425
training time 16871.148280620575
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.060468802069096876
plot_id,batch_id 0 1 miss% 0.06371249298393501
plot_id,batch_id 0 2 miss% 0.08511991545595089
plot_id,batch_id 0 3 miss% 0.07099163283500069
plot_id,batch_id 0 4 miss% 0.04593473102080574
plot_id,batch_id 0 5 miss% 0.03238507758003235
plot_id,batch_id 0 6 miss% 0.06390621559448774
plot_id,batch_id 0 7 miss% 0.09593970711932656
plot_id,batch_id 0 8 miss% 0.07711857243488517
plot_id,batch_id 0 9 miss% 0.09584739490070135
plot_id,batch_id 0 10 miss% 0.02214671446991382
plot_id,batch_id 0 11 miss% 0.06312699439429525
plot_id,batch_id 0 12 miss% 0.07418887861819004
plot_id,batch_id 0 13 miss% 0.054984977631816
plot_id,batch_id 0 14 miss% 0.07761180921062713
plot_id,batch_id 0 15 miss% 0.028488060997105442
plot_id,batch_id 0 16 miss% 0.09248687137558373
plot_id,batch_id 0 17 miss% 0.05263769674118726
plot_id,batch_id 0 18 miss% 0.06973152248135508
plot_id,batch_id 0 19 miss% 0.05287139877624932
plot_id,batch_id 0 20 miss% 0.07691782232390518
plot_id,batch_id 0 21 miss% 0.02996789427701852
plot_id,batch_id 0 22 miss% 0.06610001686879413
plot_id,batch_id 0 23 miss% 0.057758901691465706
plot_id,batch_id 0 24 miss% 0.0768408205159981
plot_id,batch_id 0 25 miss% 0.05569201926557521
plot_id,batch_id 0 26 miss% 0.06455490193548036
plot_id,batch_id 0 27 miss% 0.06432242056700244
plot_id,batch_id 0 28 miss% 0.024584761019513703
plot_id,batch_id 0 29 miss% 0.04982701702690153
plot_id,batch_id 0 30 miss% 0.03641182127590462
plot_id,batch_id 0 31 miss% 0.08578259131520219
plot_id,batch_id 0 32 miss% 0.10494815824035306
plot_id,batch_id 0 33 miss% 0.06407542388111022
plot_id,batch_id 0 34 miss% 0.046801614538923064
plot_id,batch_id 0 35 miss% 0.035319253574964175
plot_id,batch_id 0 36 miss% 0.10240910036595924
plot_id,batch_id 0 37 miss% 0.11323865614284721
plot_id,batch_id 0 38 miss% 0.025940593687984095
plot_id,batch_id 0 39 miss% 0.03890279136529317
plot_id,batch_id 0 40 miss% 0.05302574718358001
plot_id,batch_id 0 41 miss% 0.05130145937332134
plot_id,batch_id 0 42 miss% 0.025787825798167924
plot_id,batch_id 0 43 miss% 0.06697316319094569
plot_id,batch_id 0 44 miss% 0.034533980572806294
plot_id,batch_id 0 45 miss% 0.08308723656752999
plot_id,batch_id 0 46 miss% 0.02706985212841848
plot_id,batch_id 0 47 miss% 0.03898053245199746
plot_id,batch_id 0 48 miss% 0.0388235128674817
plot_id,batch_id 0 49 miss% 0.045451657030343144
plot_id,batch_id 0 50 miss% 0.1326700138066822
plot_id,batch_id 0 51 miss% 0.04969870265104291
plot_id,batch_id 0 52 miss% 0.02372990276187364
plot_id,batch_id 0 53 miss% 0.033540357814548755
plot_id,batch_id 0 54 miss% 0.037588583475198244
plot_id,batch_id 0 55 miss% 0.0788678811809788
plot_id,batch_id 0 56 miss% 0.07671326235446457
plot_id,batch_id 0 57 miss% 0.05575782075300204
plot_id,batch_id 0 58 miss% 0.05330286714364952
plot_id,batch_id 0 59 miss% 0.03452744477556809
plot_id,batch_id 0 60 miss% 0.02902245262234738
plot_id,batch_id 0 61 miss% 0.02413461639657784
plot_id,batch_id 0 62 miss% 0.05690950201631486
plot_id,batch_id 0 63 miss% 0.039836595743684304
plot_id,batch_id 0 64 miss% 0.06276982690171179
plot_id,batch_id 0plot_id,batch_id 0 69 miss% 0.0782619750408757
plot_id,batch_id 0 70 miss% 0.10268531022091128
plot_id,batch_id 0 71 miss% 0.034284957534252945
plot_id,batch_id 0 72 miss% 0.062483183813005556
plot_id,batch_id 0 73 miss% 0.05923031649153926
plot_id,batch_id 0 74 miss% 0.10472235229222888
plot_id,batch_id 0 75 miss% 0.10146178093941928
plot_id,batch_id 0 76 miss% 0.07560701240585409
plot_id,batch_id 0 77 miss% 0.04253963447399195
plot_id,batch_id 0 78 miss% 0.05260138022025806
plot_id,batch_id 0 79 miss% 0.061204652252607475
plot_id,batch_id 0 80 miss% 0.0393520551487114
plot_id,batch_id 0 81 miss% 0.0755854292069397
plot_id,batch_id 0 82 miss% 0.04696445364287567
plot_id,batch_id 0 83 miss% 0.05085311556652265
plot_id,batch_id 0 84 miss% 0.12072239353497503
plot_id,batch_id 0 85 miss% 0.06054887568429994
plot_id,batch_id 0 86 miss% 0.04175017775580135
plot_id,batch_id 0 87 miss% 0.07337643268975812
plot_id,batch_id 0 88 miss% 0.05317926567828352
plot_id,batch_id 0 89 miss% 0.04438337033688587
plot_id,batch_id 0 90 miss% 0.026489024942326078
plot_id,batch_id 0 91 miss% 0.056707450381536316
plot_id,batch_id 0 92 miss% 0.06469567421833584
plot_id,batch_id 0 93 miss% 0.05777824251759464
plot_id,batch_id 0 94 miss% 0.07316874823989294
plot_id,batch_id 0 95 miss% 0.024313322698931578
plot_id,batch_id 0 96 miss% 0.05145611697769597
plot_id,batch_id 0 97 miss% 0.033959943553491294
plot_id,batch_id 0 98 miss% 0.0270009467321781
plot_id,batch_id 0 99 miss% 0.05746394041376563
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05764885 0.03448266 0.08534999 0.04084001 0.06037649 0.0596121
 0.05715072 0.05762559 0.0880965  0.0451554  0.0451768  0.06919802
 0.06531248 0.08917407 0.07203367 0.09249705 0.09682313 0.03489899
 0.03912139 0.14033406 0.04200274 0.04648955 0.06053053 0.03975204
 0.05578332 0.07280613 0.03898584 0.0478473  0.0340379  0.04345392
 0.03177367 0.07628817 0.10766972 0.05802693 0.07057123 0.03054082
 0.10994785 0.06139492 0.04260247 0.07449023 0.04315646 0.03819034
 0.07691617 0.06292378 0.03595867 0.0890112  0.02843675 0.02986215
 0.02459593 0.02835528 0.12191363 0.0250997  0.0381797  0.03090365
 0.05403017 0.11869641 0.09120884 0.02541472 0.03467667 0.02389087
 0.03711963 0.04020128 0.06811443 0.02763306 0.05545133 0.05143365
 0.01920525 0.0184012  0.03624568 0.07826198 0.10268531 0.03428496
 0.06248318 0.05923032 0.10472235 0.10146178 0.07560701 0.04253963
 0.05260138 0.06120465 0.03935206 0.07558543 0.04696445 0.05085312
 0.12072239 0.06054888 0.04175018 0.07337643 0.05317927 0.04438337
 0.02648902 0.05670745 0.06469567 0.05777824 0.07316875 0.02431332
 0.05145612 0.03395994 0.02700095 0.05746394]
for model  98 the mean error 0.05705961339817031
all id 98 hidden_dim 32 learning_rate 0.0025 num_layers 4 frames 25 out win 6 err 0.05705961339817031 time 16853.22337579727
Launcher: Job 99 completed in 17116 seconds.
Launcher: Task 244 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  265233
Epoch:0, Train loss:0.669099, valid loss:0.640473
Epoch:1, Train loss:0.172510, valid loss:0.005644
Epoch:2, Train loss:0.011602, valid loss:0.004059
Epoch:3, Train loss:0.006429, valid loss:0.002780
Epoch:4, Train loss:0.005205, valid loss:0.002171
Epoch:5, Train loss:0.004394, valid loss:0.002114
Epoch:6, Train loss:0.003818, valid loss:0.001762
Epoch:7, Train loss:0.003503, valid loss:0.001801
Epoch:8, Train loss:0.003051, valid loss:0.001605
Epoch:9, Train loss:0.003063, valid loss:0.001799
Epoch:10, Train loss:0.002752, valid loss:0.001369
Epoch:11, Train loss:0.001939, valid loss:0.001042
Epoch:12, Train loss:0.001852, valid loss:0.001126
Epoch:13, Train loss:0.001762, valid loss:0.001035
Epoch:14, Train loss:0.001793, valid loss:0.001203
Epoch:15, Train loss:0.001641, valid loss:0.001019
Epoch:16, Train loss:0.001605, valid loss:0.001024
Epoch:17, Train loss:0.001601, valid loss:0.000935
Epoch:18, Train loss:0.001452, valid loss:0.000970
Epoch:19, Train loss:0.001478, valid loss:0.001068
Epoch:20, Train loss:0.001414, valid loss:0.000874
Epoch:21, Train loss:0.001031, valid loss:0.000819
Epoch:22, Train loss:0.001002, valid loss:0.000790
Epoch:23, Train loss:0.000953, valid loss:0.000756
Epoch:24, Train loss:0.000933, valid loss:0.000931
Epoch:25, Train loss:0.000934, valid loss:0.000819
Epoch:26, Train loss:0.000915, valid loss:0.000796
Epoch:27, Train loss:0.000866, valid loss:0.000730
Epoch:28, Train loss:0.000923, valid loss:0.000745
Epoch:29, Train loss:0.000885, valid loss:0.000692
Epoch:30, Train loss:0.000880, valid loss:0.000693
Epoch:31, Train loss:0.000670, valid loss:0.000631
Epoch:32, Train loss:0.000654, valid loss:0.000664
Epoch:33, Train loss:0.000632, valid loss:0.000640
Epoch:34, Train loss:0.000630, valid loss:0.000667
Epoch:35, Train loss:0.000639, valid loss:0.000629
Epoch:36, Train loss:0.000608, valid loss:0.000609
Epoch:37, Train loss:0.000625, valid loss:0.000619
Epoch:38, Train loss:0.000603, valid loss:0.000641
Epoch:39, Train loss:0.000587, valid loss:0.000630
Epoch:40, Train loss:0.000589, valid loss:0.000625
Epoch:41, Train loss:0.000512, valid loss:0.000648
Epoch:42, Train loss:0.000497, valid loss:0.000598
Epoch:43, Train loss:0.000491, valid loss:0.000598
Epoch:44, Train loss:0.000489, valid loss:0.000655
Epoch:45, Train loss:0.000484, valid loss:0.000590
Epoch:46, Train loss:0.000491, valid loss:0.000601
Epoch:47, Train loss:0.000474, valid loss:0.000594
Epoch:48, Train loss:0.000472, valid loss:0.000601
Epoch:49, Train loss:0.000469, valid loss:0.000598
Epoch:50, Train loss:0.000471, valid loss:0.000604
Epoch:51, Train loss:0.000427, valid loss:0.000584
Epoch:52, Train loss:0.000424, valid loss:0.000586
Epoch:53, Train loss:0.000423, valid loss:0.000581
Epoch:54, Train loss:0.000422, valid loss:0.000584
Epoch:55, Train loss:0.000421, valid loss:0.000586
Epoch:56, Train loss:0.000421, valid loss:0.000582
Epoch:57, Train loss:0.000420, valid loss:0.000586
Epoch:58, Train loss:0.000420, valid loss:0.000586
Epoch:59, Train loss:0.000420, valid loss:0.000580
Epoch:60, Train loss:0.000419, valid loss:0.000585
training time 16931.999676704407
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6000000000000001
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3304893504290982
plot_id,batch_id 0 1 miss% 0.37921069298764765
plot_id,batch_id 0 2 miss% 0.4094732051481494
plot_id,batch_id 0 3 miss% 0.33311547253753204
plot_id,batch_id 0 4 miss% 0.31873700144573885
plot_id,batch_id 0 5 miss% 0.39966827227928714
plot_id,batch_id 0 6 miss% 0.3223217440114097
plot_id,batch_id 0 7 miss% 0.4798722284020054
plot_id,batch_id 0 8 miss% 0.5538445507696603
plot_id,batch_id 0 9 miss% 0.44228187242879197
plot_id,batch_id 0 10 miss% 0.3189045097405178
plot_id,batch_id 0 11 miss% 0.3400627744396802
plot_id,batch_id 0 12 miss% 0.39011118697767694
plot_id,batch_id 0 13 miss% 0.3287926810429285
plot_id,batch_id 0 14 miss% 0.44842053933086157
plot_id,batch_id 0 15 miss% 0.3697112194153653
plot_id,batch_id 0 16 miss% 0.4560714030927564
plot_id,batch_id 0 17 miss% 0.392478444866045
plot_id,batch_id 0 18 miss% 0.47044563378953974
plot_id,batch_id 0 19 miss% 0.34475982434020763
plot_id,batch_id 0 20 miss% 0.3373774541708521
plot_id,batch_id 0 21 miss% 0.30343492171872155
plot_id,batch_id 0 22 miss% 0.3734061386961124
plot_id,batch_id 0 23 miss% 0.37816594151664035
plot_id,batch_id 0 24 miss% 0.31752717385167245
plot_id,batch_id 0 25 miss% 0.3714358997726881
plot_id,batch_id 0 26 miss% 0.4251017275769685
plot_id,batch_id 0 27 miss% 0.39071913202622643
plot_id,batch_id 0 28 miss% 0.3604873820773036
plot_id,batch_id 0 29 miss% 0.3774328031014406
plot_id,batch_id 0 30 miss% 0.2848495994240124
plot_id,batch_id 0 31 miss% 0.45341349315143387
plot_id,batch_id 0 32 miss% 0.41392368137324165
plot_id,batch_id 0 33 miss% 0.3766074510895345
plot_id,batch_id 0 34 miss% 0.3833183079061089
plot_id,batch_id 0 35 miss% 0.3425262166387614
plot_id,batch_id 0 36 miss% 0.4997324624274809
plot_id,batch_id 0 37 miss% 0.4253682047988359
plot_id,batch_id 0 38 miss% 0.3863122473992759
plot_id,batch_id 0 39 miss% 0.39172630183545615
plot_id,batch_id 0 40 miss% 0.38985952529849405
plot_id,batch_id 0 41 miss% 0.4400706018619842
plot_id,batch_id 0 42 miss% 0.36655420242662345
plot_id,batch_id 0 43 miss% 0.3213477496382357
plot_id,batch_id 0 44 miss% 0.25308058056096083
plot_id,batch_id 0 45 miss% 0.27280446188518365
plot_id,batch_id 0 46 miss% 0.346740953907692
plot_id,batch_id 0 47 miss% 0.3748020360962138
plot_id,batch_id 0 48 miss% 0.41681256822121054
plot_id,batch_id 0 49 miss% 0.3651958966026076
plot_id,batch_id 0 50 miss% 0.4580316058315673
plot_id,batch_id 0 51 miss% 0.4974201415809597
plot_id,batch_id 0 52 miss% 0.4189648648328713
plot_id,batch_id 0 53 miss% 0.3989955511856516
plot_id,batch_id 0 54 miss% 0.3104132424830158
plot_id,batch_id 0 55 miss% 0.47676666750699126
plot_id,batch_id 0 56 miss% 0.46174290572826193
plot_id,batch_id 0 57 miss% 0.4205957148913173
plot_id,batch_id 0 58 miss% 0.4272352031091753
plot_id,batch_id 0 59 miss% 0.4619389531910934
plot_id,batch_id 0 60 miss% 0.2858762444189732
plot_id,batch_id 0 61 miss% 0.24472614897479172
plot_id,batch_id 0 62 miss% 0.46542700313045576
plot_id,batch_id 0 63 miss% 0.402265326919449
plot_id,batch_id 0 64 miss% 0.3963857760349337
plot_id,batch_id 0 65 miss% 0.3534681105522405
plot_id,batch_id 0 66 miss% 0.3858209430823624
plot_id,batch_id 0 67 miss% 0.2606996737523448
plot_id,batch_id 0 68 miss% 0.41915146464989755
plot_id,batch_id 0 69 miss% plot_id,batch_id 0 66 miss% 0.017737680952300054
plot_id,batch_id 0 67 miss% 0.03260305970115371
plot_id,batch_id 0 68 miss% 0.03856076600416423
plot_id,batch_id 0 69 miss% 0.08674043100748767
plot_id,batch_id 0 70 miss% 0.08600618017382496
plot_id,batch_id 0 71 miss% 0.03301781611127046
plot_id,batch_id 0 72 miss% 0.07307266696586966
plot_id,batch_id 0 73 miss% 0.0590633128011794
plot_id,batch_id 0 74 miss% 0.1288080446823314
plot_id,batch_id 0 75 miss% 0.13421471263715729
plot_id,batch_id 0 76 miss% 0.051135277853219446
plot_id,batch_id 0 77 miss% 0.048843446180112694
plot_id,batch_id 0 78 miss% 0.06988853734699463
plot_id,batch_id 0 79 miss% 0.05258537811573606
plot_id,batch_id 0 80 miss% 0.08242357241047368
plot_id,batch_id 0 81 miss% 0.08779363867190094
plot_id,batch_id 0 82 miss% 0.05734452289441046
plot_id,batch_id 0 83 miss% 0.08652009474477736
plot_id,batch_id 0 84 miss% 0.07634775252982749
plot_id,batch_id 0 85 miss% 0.03750586752977916
plot_id,batch_id 0 86 miss% 0.060857362519306554
plot_id,batch_id 0 87 miss% 0.07225072868571258
plot_id,batch_id 0 88 miss% 0.08285170972435008
plot_id,batch_id 0 89 miss% 0.07276183139348275
plot_id,batch_id 0 90 miss% 0.030552680063991734
plot_id,batch_id 0 91 miss% 0.0449000456661963
plot_id,batch_id 0 92 miss% 0.0428441113566939
plot_id,batch_id 0 93 miss% 0.03901903217221881
plot_id,batch_id 0 94 miss% 0.09237167760078473
plot_id,batch_id 0 95 miss% 0.07443052522982502
plot_id,batch_id 0 96 miss% 0.05694823270014461
plot_id,batch_id 0 97 miss% 0.030975023985511628
plot_id,batch_id 0 98 miss% 0.06801495397636605
plot_id,batch_id 0 99 miss% 0.04613309930126649
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.11790307 0.04909801 0.09222232 0.07753073 0.06051493 0.06158272
 0.03635622 0.08957868 0.05641995 0.05621516 0.01934895 0.04120782
 0.0781683  0.09251321 0.08631101 0.02558255 0.19203432 0.0285243
 0.09983957 0.09019169 0.05315064 0.04936073 0.04361079 0.04307198
 0.03293598 0.10234965 0.06934483 0.04480723 0.03726445 0.03547552
 0.03471213 0.13846972 0.08311587 0.07293575 0.03775971 0.03263339
 0.13891843 0.09701755 0.04076841 0.04271508 0.04665912 0.05249029
 0.0231865  0.04042694 0.03943983 0.05135712 0.04528839 0.02874574
 0.028784   0.03869873 0.09811929 0.04799025 0.03152488 0.06888211
 0.03720953 0.07178158 0.08911531 0.03817405 0.03090868 0.02860654
 0.04642551 0.03789736 0.10083922 0.03661286 0.05906247 0.04003572
 0.01773768 0.03260306 0.03856077 0.08674043 0.08600618 0.03301782
 0.07307267 0.05906331 0.12880804 0.13421471 0.05113528 0.04884345
 0.06988854 0.05258538 0.08242357 0.08779364 0.05734452 0.08652009
 0.07634775 0.03750587 0.06085736 0.07225073 0.08285171 0.07276183
 0.03055268 0.04490005 0.04284411 0.03901903 0.09237168 0.07443053
 0.05694823 0.03097502 0.06801495 0.0461331 ]
for model  208 the mean error 0.06094947140802615
all id 208 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 31 out win 5 err 0.06094947140802615 time 16868.08417248726
Launcher: Job 209 completed in 17125 seconds.
Launcher: Task 85 done. Exiting.
 65 miss% 0.04667510965875831
plot_id,batch_id 0 66 miss% 0.02116285668771911
plot_id,batch_id 0 67 miss% 0.030986818821926294
plot_id,batch_id 0 68 miss% 0.044053620337458035
plot_id,batch_id 0 69 miss% 0.06074603198289755
plot_id,batch_id 0 70 miss% 0.045041878907426516
plot_id,batch_id 0 71 miss% 0.05878004657220639
plot_id,batch_id 0 72 miss% 0.05122753388022179
plot_id,batch_id 0 73 miss% 0.04446159204254743
plot_id,batch_id 0 74 miss% 0.1305220898893531
plot_id,batch_id 0 75 miss% 0.046290011642300205
plot_id,batch_id 0 76 miss% 0.04126090788861895
plot_id,batch_id 0 77 miss% 0.03706200149682496
plot_id,batch_id 0 78 miss% 0.03327957183058463
plot_id,batch_id 0 79 miss% 0.08569267942965543
plot_id,batch_id 0 80 miss% 0.04689000956090738
plot_id,batch_id 0 81 miss% 0.08607055051994351
plot_id,batch_id 0 82 miss% 0.05703587540809746
plot_id,batch_id 0 83 miss% 0.11950048225549197
plot_id,batch_id 0 84 miss% 0.06577464886342166
plot_id,batch_id 0 85 miss% 0.057771963107916574
plot_id,batch_id 0 86 miss% 0.05553930308089074
plot_id,batch_id 0 87 miss% 0.057041591006062084
plot_id,batch_id 0 88 miss% 0.10896707520762784
plot_id,batch_id 0 89 miss% 0.04266539020992024
plot_id,batch_id 0 90 miss% 0.023814873061624074
plot_id,batch_id 0 91 miss% 0.05368306577037852
plot_id,batch_id 0 92 miss% 0.04324799819974429
plot_id,batch_id 0 93 miss% 0.045265783050192784
plot_id,batch_id 0 94 miss% 0.08038326780444412
plot_id,batch_id 0 95 miss% 0.054921034627026806
plot_id,batch_id 0 96 miss% 0.03498278194358572
plot_id,batch_id 0 97 miss% 0.07087847425127522
plot_id,batch_id 0 98 miss% 0.029014146599538118
plot_id,batch_id 0 99 miss% 0.05250351955284073
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0604688  0.06371249 0.08511992 0.07099163 0.04593473 0.03238508
 0.06390622 0.09593971 0.07711857 0.09584739 0.02214671 0.06312699
 0.07418888 0.05498498 0.07761181 0.02848806 0.09248687 0.0526377
 0.06973152 0.0528714  0.07691782 0.02996789 0.06610002 0.0577589
 0.07684082 0.05569202 0.0645549  0.06432242 0.02458476 0.04982702
 0.03641182 0.08578259 0.10494816 0.06407542 0.04680161 0.03531925
 0.1024091  0.11323866 0.02594059 0.03890279 0.05302575 0.05130146
 0.02578783 0.06697316 0.03453398 0.08308724 0.02706985 0.03898053
 0.03882351 0.04545166 0.13267001 0.0496987  0.0237299  0.03354036
 0.03758858 0.07886788 0.07671326 0.05575782 0.05330287 0.03452744
 0.02902245 0.02413462 0.0569095  0.0398366  0.06276983 0.04667511
 0.02116286 0.03098682 0.04405362 0.06074603 0.04504188 0.05878005
 0.05122753 0.04446159 0.13052209 0.04629001 0.04126091 0.037062
 0.03327957 0.08569268 0.04689001 0.08607055 0.05703588 0.11950048
 0.06577465 0.05777196 0.0555393  0.05704159 0.10896708 0.04266539
 0.02381487 0.05368307 0.043248   0.04526578 0.08038327 0.05492103
 0.03498278 0.07087847 0.02901415 0.05250352]
for model  174 the mean error 0.05717395429282407
all id 174 hidden_dim 24 learning_rate 0.0025 num_layers 4 frames 31 out win 4 err 0.05717395429282407 time 16871.148280620575
Launcher: Job 175 completed in 17131 seconds.
Launcher: Task 83 done. Exiting.
0.43029874904861976
plot_id,batch_id 0 70 miss% 0.29990859717544954
plot_id,batch_id 0 71 miss% 0.44700324509020856
plot_id,batch_id 0 72 miss% 0.4412098939659193
plot_id,batch_id 0 73 miss% 0.3261768614434193
plot_id,batch_id 0 74 miss% 0.4077325801093521
plot_id,batch_id 0 75 miss% 0.24851987996666544
plot_id,batch_id 0 76 miss% 0.3404689322737248
plot_id,batch_id 0 77 miss% 0.31288553606229735
plot_id,batch_id 0 78 miss% 0.3461688962624458
plot_id,batch_id 0 79 miss% 0.35344403201332253
plot_id,batch_id 0 80 miss% 0.29875904436396544
plot_id,batch_id 0 81 miss% 0.40859932478015665
plot_id,batch_id 0 82 miss% 0.36278741865654196
plot_id,batch_id 0 83 miss% 0.41825115487212683
plot_id,batch_id 0 84 miss% 0.38223852410830317
plot_id,batch_id 0 85 miss% 0.2873458566175203
plot_id,batch_id 0 86 miss% 0.3417891802629794
plot_id,batch_id 0 87 miss% 0.36894792632531476
plot_id,batch_id 0 88 miss% 0.42101759863545934
plot_id,batch_id 0 89 miss% 0.36761025437811623
plot_id,batch_id 0 90 miss% 0.2347645330505076
plot_id,batch_id 0 91 miss% 0.34096661174140963
plot_id,batch_id 0 92 miss% 0.30672544998225054
plot_id,batch_id 0 93 miss% 0.32957275861641133
plot_id,batch_id 0 94 miss% 0.4118232741417651
plot_id,batch_id 0 95 miss% 0.2711077338398334
plot_id,batch_id 0 96 miss% 0.29180432631842973
plot_id,batch_id 0 97 miss% 0.4139984630326331
plot_id,batch_id 0 98 miss% 0.41101168077052347
plot_id,batch_id 0 99 miss% 0.3504800850419657
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.33048935 0.37921069 0.40947321 0.33311547 0.318737   0.39966827
 0.32232174 0.47987223 0.55384455 0.44228187 0.31890451 0.34006277
 0.39011119 0.32879268 0.44842054 0.36971122 0.4560714  0.39247844
 0.47044563 0.34475982 0.33737745 0.30343492 0.37340614 0.37816594
 0.31752717 0.3714359  0.42510173 0.39071913 0.36048738 0.3774328
 0.2848496  0.45341349 0.41392368 0.37660745 0.38331831 0.34252622
 0.49973246 0.4253682  0.38631225 0.3917263  0.38985953 0.4400706
 0.3665542  0.32134775 0.25308058 0.27280446 0.34674095 0.37480204
 0.41681257 0.3651959  0.45803161 0.49742014 0.41896486 0.39899555
 0.31041324 0.47676667 0.46174291 0.42059571 0.4272352  0.46193895
 0.28587624 0.24472615 0.465427   0.40226533 0.39638578 0.35346811
 0.38582094 0.26069967 0.41915146 0.43029875 0.2999086  0.44700325
 0.44120989 0.32617686 0.40773258 0.24851988 0.34046893 0.31288554
 0.3461689  0.35344403 0.29875904 0.40859932 0.36278742 0.41825115
 0.38223852 0.28734586 0.34178918 0.36894793 0.4210176  0.36761025
 0.23476453 0.34096661 0.30672545 0.32957276 0.41182327 0.27110773
 0.29180433 0.41399846 0.41101168 0.35048009]
for model  51 the mean error 0.3748825166933086
all id 51 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 21 out win 4 err 0.3748825166933086 time 16931.999676704407
Launcher: Job 52 completed in 17167 seconds.
Launcher: Task 67 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  154129
Epoch:0, Train loss:0.416392, valid loss:0.430518
Epoch:1, Train loss:0.031638, valid loss:0.001689
Epoch:2, Train loss:0.003216, valid loss:0.001391
Epoch:3, Train loss:0.002571, valid loss:0.001157
Epoch:4, Train loss:0.002174, valid loss:0.001331
Epoch:5, Train loss:0.001990, valid loss:0.000902
Epoch:6, Train loss:0.001740, valid loss:0.000819
Epoch:7, Train loss:0.001602, valid loss:0.000852
Epoch:8, Train loss:0.001489, valid loss:0.000709
Epoch:9, Train loss:0.001400, valid loss:0.000811
Epoch:10, Train loss:0.001399, valid loss:0.000852
Epoch:11, Train loss:0.000992, valid loss:0.000591
Epoch:12, Train loss:0.000970, valid loss:0.000599
Epoch:13, Train loss:0.000928, valid loss:0.000557
Epoch:14, Train loss:0.000896, valid loss:0.000620
Epoch:15, Train loss:0.000877, valid loss:0.000566
Epoch:16, Train loss:0.000837, valid loss:0.000536
Epoch:17, Train loss:0.000832, valid loss:0.000556
Epoch:18, Train loss:0.000799, valid loss:0.000570
Epoch:19, Train loss:0.000787, valid loss:0.000498
Epoch:20, Train loss:0.000779, valid loss:0.000560
Epoch:21, Train loss:0.000593, valid loss:0.000474
Epoch:22, Train loss:0.000566, valid loss:0.000523
Epoch:23, Train loss:0.000567, valid loss:0.000479
Epoch:24, Train loss:0.000576, valid loss:0.000497
Epoch:25, Train loss:0.000555, valid loss:0.000476
Epoch:26, Train loss:0.000550, valid loss:0.000473
Epoch:27, Train loss:0.000529, valid loss:0.000472
Epoch:28, Train loss:0.000520, valid loss:0.000530
Epoch:29, Train loss:0.000526, valid loss:0.000490
Epoch:30, Train loss:0.000519, valid loss:0.000461
Epoch:31, Train loss:0.000424, valid loss:0.000412
Epoch:32, Train loss:0.000418, valid loss:0.000523
Epoch:33, Train loss:0.000411, valid loss:0.000411
Epoch:34, Train loss:0.000411, valid loss:0.000431
Epoch:35, Train loss:0.000414, valid loss:0.000429
Epoch:36, Train loss:0.000398, valid loss:0.000433
Epoch:37, Train loss:0.000407, valid loss:0.000438
Epoch:38, Train loss:0.000389, valid loss:0.000460
Epoch:39, Train loss:0.000400, valid loss:0.000456
Epoch:40, Train loss:0.000390, valid loss:0.000446
Epoch:41, Train loss:0.000350, valid loss:0.000420
Epoch:42, Train loss:0.000345, valid loss:0.000411
Epoch:43, Train loss:0.000346, valid loss:0.000419
Epoch:44, Train loss:0.000341, valid loss:0.000422
Epoch:45, Train loss:0.000340, valid loss:0.000415
Epoch:46, Train loss:0.000338, valid loss:0.000413
Epoch:47, Train loss:0.000337, valid loss:0.000420
Epoch:48, Train loss:0.000340, valid loss:0.000439
Epoch:49, Train loss:0.000334, valid loss:0.000421
Epoch:50, Train loss:0.000330, valid loss:0.000415
Epoch:51, Train loss:0.000317, valid loss:0.000404
Epoch:52, Train loss:0.000313, valid loss:0.000404
Epoch:53, Train loss:0.000311, valid loss:0.000404
Epoch:54, Train loss:0.000310, valid loss:0.000403
Epoch:55, Train loss:0.000310, valid loss:0.000404
Epoch:56, Train loss:0.000309, valid loss:0.000404
Epoch:57, Train loss:0.000308, valid loss:0.000401
Epoch:58, Train loss:0.000308, valid loss:0.000404
Epoch:59, Train loss:0.000308, valid loss:0.000404
Epoch:60, Train loss:0.000307, valid loss:0.000406
training time 16965.697912931442
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.10089710869927256
plot_id,batch_id 0 1 miss% 0.03978450361594676
plot_id,batch_id 0 2 miss% 0.08685450536727062
plot_id,batch_id 0 3 miss% 0.043875984167127874
plot_id,batch_id 0 4 miss% 0.0530161200097998
plot_id,batch_id 0 5 miss% 0.04827742056378422
plot_id,batch_id 0 6 miss% 0.05442616959473463
plot_id,batch_id 0 7 miss% 0.09155718043692934
plot_id,batch_id 0 8 miss% 0.04419435658272911
plot_id,batch_id 0 9 miss% 0.05749823655768876
plot_id,batch_id 0 10 miss% 0.022547968607809023
plot_id,batch_id 0 11 miss% 0.07512473065168226
plot_id,batch_id 0 12 miss% 0.04011092205412308
plot_id,batch_id 0 13 miss% 0.062358895665575655
plot_id,batch_id 0 14 miss% 0.059744455825550165
plot_id,batch_id 0 15 miss% 0.03541064816485934
plot_id,batch_id 0 16 miss% 0.06959568476494872
plot_id,batch_id 0 17 miss% 0.03762873759197238
plot_id,batch_id 0 18 miss% 0.06304992967186888
plot_id,batch_id 0 19 miss% 0.03741010762548551
plot_id,batch_id 0 20 miss% 0.10694150747477081
plot_id,batch_id 0 21 miss% 0.036643616149182295
plot_id,batch_id 0 22 miss% 0.06154195587647156
plot_id,batch_id 0 23 miss% 0.044965689288564245
plot_id,batch_id 0 24 miss% 0.04595172611411181
plot_id,batch_id 0 25 miss% 0.041119697064953484
plot_id,batch_id 0 26 miss% 0.04870195966602191
plot_id,batch_id 0 27 miss% 0.0467012186301704
plot_id,batch_id 0 28 miss% 0.03285949493593018
plot_id,batch_id 0 29 miss% 0.03106101290126128
plot_id,batch_id 0 30 miss% 0.03270507283165858
plot_id,batch_id 0 31 miss% 0.08951446463575831
plot_id,batch_id 0 32 miss% 0.0704028355865975
plot_id,batch_id 0 33 miss% 0.07758461200531357
plot_id,batch_id 0 34 miss% 0.031416734503813375
plot_id,batch_id 0 35 miss% 0.06246590735460287
plot_id,batch_id 0 36 miss% 0.11458378665849063
plot_id,batch_id 0 37 miss% 0.08092423925078629
plot_id,batch_id 0 38 miss% 0.026079151494496376
plot_id,batch_id 0 39 miss% 0.04824830410793233
plot_id,batch_id 0 40 miss% 0.07344999773898052
plot_id,batch_id 0 41 miss% 0.040355007245612716
plot_id,batch_id 0 42 miss% 0.02728314333114452
plot_id,batch_id 0 43 miss% 0.05762957696312334
plot_id,batch_id 0 44 miss% 0.04463811030564056
plot_id,batch_id 0 45 miss% 0.05448706607925291
plot_id,batch_id 0 46 miss% 0.025504435163246908
plot_id,batch_id 0 47 miss% 0.023482049550797152
plot_id,batch_id 0 48 miss% 0.03794072201265524
plot_id,batch_id 0 49 miss% 0.055557604713652585
plot_id,batch_id 0 50 miss% 0.07470379245071451
plot_id,batch_id 0 51 miss% 0.03529203844288417
plot_id,batch_id 0 52 miss% 0.03553656379455017
plot_id,batch_id 0 53 miss% 0.034954693626461024
plot_id,batch_id 0 54 miss% 0.07822461116136617
plot_id,batch_id 0 55 miss% 0.04807046766726089
plot_id,batch_id 0 56 miss% 0.11042237643146348
plot_id,batch_id 0 57 miss% 0.04251916150299493
plot_id,batch_id 0 58 miss% 0.035773129719173555
plot_id,batch_id 0 59 miss% 0.03612803046235775
plot_id,batch_id 0 60 miss% 0.042890477479556335
plot_id,batch_id 0 61 miss% 0.030744464513779053
plot_id,batch_id 0 62 miss% 0.07084532391705813
plot_id,batch_id 0 63 miss% 0.046317393785577636
plot_id,batch_id 0 64 miss% 0.03530778627033451
plot_id,batch_id 0 65 miss% 0.08523982581188634
plot_id,batch_id 0 66 miss% 0.0163968461290123
plot_id,batch_id 0 67 miss% 0.017867403160230565
plot_id,batch_id 0 68 miss% 0.028935251230607986
plot_id,batch_id 0 69 miss% 0.06513736281198279
plot_id,batch_id 0 70 miss% 0.04660128760980261
plot_id,batch_id 0 71 miss% 0.06210011038776786
plot_id,batch_id 0 72 miss% 0.09422837546822965
plot_id,batch_id 0 73 miss% 0.032830781551673294
plot_id,batch_id 0 74 miss% 0.1159134716621816
plot_id,batch_id 0 75 miss% 0.03887346758054889
plot_id,batch_id 0 76 miss% 0.04703307080859074
plot_id,batch_id 0 77 miss% 0.05098355152805432
plot_id,batch_id 0 78 miss% 0.036619210800918275
plot_id,batch_id 0 79 miss% 0.08801608483911473
plot_id,batch_id 0 80 miss% 0.047030501330617536
plot_id,batch_id 0 81 miss% 0.07163387792179884
plot_id,batch_id 0 82 miss% 0.0560317989615443
plot_id,batch_id 0 83 miss% 0.04539127829220903
plot_id,batch_id 0 84 miss% 0.04769126944772281
plot_id,batch_id 0 85 miss% 0.03125227682181764
plot_id,batch_id 0 86 miss% 0.042905273779055904
plot_id,batch_id 0 87 miss% 0.07581059807138292
plot_id,batch_id 0 88 miss% 0.07633172841157332
plot_id,batch_id 0 89 miss% 0.04629480622941798
plot_id,batch_id 0 90 miss% 0.05314632634564675
plot_id,batch_id 0 91 miss% 0.04817367905360538
plot_id,batch_id 0 92 miss% 0.02030816525229505
plot_id,batch_id 0 93 miss% 0.025753166217756446
plot_id,batch_id 0 94 miss% 0.09243361777420082
plot_id,batch_id 0 95 miss% 0.049410347348198026
plot_id,batch_id 0 96 miss% 0.05652591309180947
plot_id,batch_id 0 97 miss% 0.03226847749171745
plot_id,batch_id 0 98 miss% 0.03383199564500948
plot_id,batch_id 0 99 miss% 0.05141187112337641
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10089711 0.0397845  0.08685451 0.04387598 0.05301612 0.04827742
 0.05442617 0.09155718 0.04419436 0.05749824 0.02254797 0.07512473
 0.04011092 0.0623589  0.05974446 0.03541065 0.06959568 0.03762874
 0.06304993 0.03741011 0.10694151 0.03664362 0.06154196 0.04496569
 0.04595173 0.0411197  0.04870196 0.04670122 0.03285949 0.03106101
 0.03270507 0.08951446 0.07040284 0.07758461 0.03141673 0.06246591
 0.11458379 0.08092424 0.02607915 0.0482483  0.07345    0.04035501
 0.02728314 0.05762958 0.04463811 0.05448707 0.02550444 0.02348205
 0.03794072 0.0555576  0.07470379 0.03529204 0.03553656 0.03495469
 0.07822461 0.04807047 0.11042238 0.04251916 0.03577313 0.03612803
 0.04289048 0.03074446 0.07084532 0.04631739 0.03530779 0.08523983
 0.01639685 0.0178674  0.02893525 0.06513736 0.04660129 0.06210011
 0.09422838 0.03283078 0.11591347 0.03887347 0.04703307 0.05098355
 0.03661921 0.08801608 0.0470305  0.07163388 0.0560318  0.04539128
 0.04769127 0.03125228 0.04290527 0.0758106  0.07633173 0.04629481
 0.05314633 0.04817368 0.02030817 0.02575317 0.09243362 0.04941035
 0.05652591 0.03226848 0.033832   0.05141187]
for model  196 the mean error 0.05282247747071042
all id 196 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 31 out win 5 err 0.05282247747071042 time 16965.697912931442
Launcher: Job 197 completed in 17224 seconds.
Launcher: Task 148 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  151697
Epoch:0, Train loss:0.517722, valid loss:0.476178
Epoch:1, Train loss:0.368563, valid loss:0.365457
Epoch:2, Train loss:0.360036, valid loss:0.365043
Epoch:3, Train loss:0.359351, valid loss:0.364712
Epoch:4, Train loss:0.358915, valid loss:0.364868
Epoch:5, Train loss:0.358574, valid loss:0.364346
Epoch:6, Train loss:0.358452, valid loss:0.364283
Epoch:7, Train loss:0.358222, valid loss:0.364690
Epoch:8, Train loss:0.358003, valid loss:0.364170
Epoch:9, Train loss:0.358001, valid loss:0.364096
Epoch:10, Train loss:0.357844, valid loss:0.364326
Epoch:11, Train loss:0.357296, valid loss:0.363915
Epoch:12, Train loss:0.357153, valid loss:0.363982
Epoch:13, Train loss:0.357140, valid loss:0.363870
Epoch:14, Train loss:0.357117, valid loss:0.363838
Epoch:15, Train loss:0.357126, valid loss:0.363951
Epoch:16, Train loss:0.357123, valid loss:0.363890
Epoch:17, Train loss:0.356997, valid loss:0.363838
Epoch:18, Train loss:0.356993, valid loss:0.363785
Epoch:19, Train loss:0.356991, valid loss:0.363934
Epoch:20, Train loss:0.356932, valid loss:0.363754
Epoch:21, Train loss:0.356669, valid loss:0.363672
Epoch:22, Train loss:0.356634, valid loss:0.363674
Epoch:23, Train loss:0.356624, valid loss:0.363704
Epoch:24, Train loss:0.356624, valid loss:0.363691
Epoch:25, Train loss:0.356598, valid loss:0.363775
Epoch:26, Train loss:0.356599, valid loss:0.363706
Epoch:27, Train loss:0.356575, valid loss:0.363711
Epoch:28, Train loss:0.356557, valid loss:0.363662
Epoch:29, Train loss:0.356548, valid loss:0.363639
Epoch:30, Train loss:0.356543, valid loss:0.363764
Epoch:31, Train loss:0.356413, valid loss:0.363678
Epoch:32, Train loss:0.356396, valid loss:0.363626
Epoch:33, Train loss:0.356383, valid loss:0.363616
Epoch:34, Train loss:0.356383, valid loss:0.363627
Epoch:35, Train loss:0.356392, valid loss:0.363642
Epoch:36, Train loss:0.356360, valid loss:0.363697
Epoch:37, Train loss:0.356373, valid loss:0.363613
Epoch:38, Train loss:0.356364, valid loss:0.363618
Epoch:39, Train loss:0.356361, valid loss:0.363654
Epoch:40, Train loss:0.356364, valid loss:0.363607
Epoch:41, Train loss:0.356293, valid loss:0.363590
Epoch:42, Train loss:0.356281, valid loss:0.363579
Epoch:43, Train loss:0.356283, valid loss:0.363630
Epoch:44, Train loss:0.356280, valid loss:0.363608
Epoch:45, Train loss:0.356281, valid loss:0.363573
Epoch:46, Train loss:0.356271, valid loss:0.363575
Epoch:47, Train loss:0.356267, valid loss:0.363588
Epoch:48, Train loss:0.356263, valid loss:0.363586
Epoch:49, Train loss:0.356265, valid loss:0.363608
Epoch:50, Train loss:0.356260, valid loss:0.363633
Epoch:51, Train loss:0.356243, valid loss:0.363587
Epoch:52, Train loss:0.356234, valid loss:0.363582
Epoch:53, Train loss:0.356231, valid loss:0.363580
Epoch:54, Train loss:0.356229, valid loss:0.363579
Epoch:55, Train loss:0.356229, valid loss:0.363577
Epoch:56, Train loss:0.356229, valid loss:0.363576
Epoch:57, Train loss:0.356228, valid loss:0.363575
Epoch:58, Train loss:0.356227, valid loss:0.363575
Epoch:59, Train loss:0.356227, valid loss:0.363576
Epoch:60, Train loss:0.356227, valid loss:0.363572
training time 17139.589416742325
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.8505622015351366
plot_id,batch_id 0 1 miss% 0.8223787034828746
plot_id,batch_id 0 2 miss% 0.8180378610534145
plot_id,batch_id 0 3 miss% 0.8133267777437949
plot_id,batch_id 0 4 miss% 0.8125497954552299
plot_id,batch_id 0 5 miss% 0.8553588166655182
plot_id,batch_id 0 6 miss% 0.8245793781898787
plot_id,batch_id 0 7 miss% 0.8198050128465578
plot_id,batch_id 0 8 miss% 0.8174514485181658
plot_id,batch_id 0 9 miss% 0.815693134421727
plot_id,batch_id 0 10 miss% 0.8545887602674076
plot_id,batch_id 0 11 miss% 0.8267461370236832
plot_id,batch_id 0 12 miss% 0.8211242464769459
plot_id,batch_id 0 13 miss% 0.8137967145528114
plot_id,batch_id 0 14 miss% 0.818286207901217
plot_id,batch_id 0 15 miss% 0.8513876000290596
plot_id,batch_id 0 16 miss% 0.8270831724988801
plot_id,batch_id 0 17 miss% 0.8214139874551141
plot_id,batch_id 0 18 miss% 0.8234584283557482
plot_id,batch_id 0 19 miss% 0.8234308847113214
plot_id,batch_id 0 20 miss% 0.8394279694269845
plot_id,batch_id 0 21 miss% 0.818092280994331
plot_id,batch_id 0 22 miss% 0.8153906068855211
plot_id,batch_id 0 23 miss% 0.8140461757048056
plot_id,batch_id 0 24 miss% 0.8097442488918783
plot_id,batch_id 0 25 miss% 0.8383175490357857
plot_id,batch_id 0 26 miss% 0.8214244855011653
plot_id,batch_id 0 27 miss% 0.8160571303917888
plot_id,batch_id 0 28 miss% 0.813274877403771
plot_id,batch_id 0 29 miss% 0.8112336471896219
plot_id,batch_id 0 30 miss% 0.8398765924556378
plot_id,batch_id 0 31 miss% 0.8225293317742881
plot_id,batch_id 0 32 miss% 0.8217219203385406
plot_id,batch_id 0 33 miss% 0.8172348007567348
plot_id,batch_id 0 34 miss% 0.8151311322919782
plot_id,batch_id 0 35 miss% 0.8413353337132033
plot_id,batch_id 0 36 miss% 0.8221864321969309
plot_id,batch_id 0 37 miss% 0.8197615672520855
plot_id,batch_id 0 38 miss% 0.8187665190024978
plot_id,batch_id 0 39 miss% 0.8140252089502306
plot_id,batch_id 0 40 miss% 0.8272119154666198
plot_id,batch_id 0 41 miss% 0.8127154389665782
plot_id,batch_id 0 42 miss% 0.8097739184719515
plot_id,batch_id 0 43 miss% 0.810067580763695
plot_id,batch_id 0 44 miss% 0.8096991568813385
plot_id,batch_id 0 45 miss% 0.822694069990776
plot_id,batch_id 0 46 miss% 0.8171185440371442
plot_id,batch_id 0 47 miss% 0.8141905256253935
plot_id,batch_id 0 48 miss% 0.81414520334077
plot_id,batch_id 0 49 miss% 0.811041511080448
plot_id,batch_id 0 50 miss% 0.8289383943815677
plot_id,batch_id 0 51 miss% 0.8181334399609416
plot_id,batch_id 0 52 miss% 0.8161324380247518
plot_id,batch_id 0 53 miss% 0.8124866019901179
plot_id,batch_id 0 54 miss% 0.8146784123893432
plot_id,batch_id 0 55 miss% 0.8308459818630765
plot_id,batch_id 0 56 miss% 0.8188755205319287
plot_id,batch_id 0 57 miss% 0.8162862576521639
plot_id,batch_id 0 58 miss% 0.8148988167237944
plot_id,batch_id 0 59 miss% 0.8164393508621278
plot_id,batch_id 0 60 miss% 0.8837637827478896
plot_id,batch_id 0 61 miss% 0.8409157603078914
plot_id,batch_id 0 62 miss% 0.8292979769275486
plot_id,batch_id 0 63 miss% 0.8209547017151804
plot_id,batch_id 0 64 miss% 0.8184400014654793
plot_id,batch_id 0 65 miss% 0.8844263778508457
plot_id,batch_id 0 66 miss% 0.8496657010868688
plot_id,batch_id 0 67 miss% 0.8303509395567442
plot_id,batch_id 0 68 miss% 0.822496050764128
plot_id,batch_id 0 69 miss% 0.8209442601037653
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  154129
Epoch:0, Train loss:0.416392, valid loss:0.430518
Epoch:1, Train loss:0.047826, valid loss:0.003638
Epoch:2, Train loss:0.009097, valid loss:0.002627
Epoch:3, Train loss:0.004922, valid loss:0.001178
Epoch:4, Train loss:0.002443, valid loss:0.001289
Epoch:5, Train loss:0.002128, valid loss:0.001045
Epoch:6, Train loss:0.001892, valid loss:0.000905
Epoch:7, Train loss:0.001656, valid loss:0.000891
Epoch:8, Train loss:0.001563, valid loss:0.000964
Epoch:9, Train loss:0.001448, valid loss:0.000720
Epoch:10, Train loss:0.001372, valid loss:0.000820
Epoch:11, Train loss:0.001042, valid loss:0.000595
Epoch:12, Train loss:0.000988, valid loss:0.000638
Epoch:13, Train loss:0.000962, valid loss:0.000626
Epoch:14, Train loss:0.000923, valid loss:0.000641
Epoch:15, Train loss:0.000894, valid loss:0.000656
Epoch:16, Train loss:0.000864, valid loss:0.000577
Epoch:17, Train loss:0.000843, valid loss:0.000576
Epoch:18, Train loss:0.000845, valid loss:0.000527
Epoch:19, Train loss:0.000789, valid loss:0.000520
Epoch:20, Train loss:0.000795, valid loss:0.000589
Epoch:21, Train loss:0.000631, valid loss:0.000485
Epoch:22, Train loss:0.000602, valid loss:0.000498
Epoch:23, Train loss:0.000604, valid loss:0.000481
Epoch:24, Train loss:0.000602, valid loss:0.000530
Epoch:25, Train loss:0.000575, valid loss:0.000441
Epoch:26, Train loss:0.000590, valid loss:0.000466
Epoch:27, Train loss:0.000573, valid loss:0.000466
Epoch:28, Train loss:0.000557, valid loss:0.000477
Epoch:29, Train loss:0.000549, valid loss:0.000481
Epoch:30, Train loss:0.000539, valid loss:0.000464
Epoch:31, Train loss:0.000465, valid loss:0.000415
Epoch:32, Train loss:0.000457, valid loss:0.000477
Epoch:33, Train loss:0.000454, valid loss:0.000429
Epoch:34, Train loss:0.000454, valid loss:0.000443
Epoch:35, Train loss:0.000449, valid loss:0.000468
Epoch:36, Train loss:0.000440, valid loss:0.000443
Epoch:37, Train loss:0.000444, valid loss:0.000504
Epoch:38, Train loss:0.000439, valid loss:0.000469
Epoch:39, Train loss:0.000437, valid loss:0.000441
Epoch:40, Train loss:0.000431, valid loss:0.000441
Epoch:41, Train loss:0.000390, valid loss:0.000425
Epoch:42, Train loss:0.000386, valid loss:0.000435
Epoch:43, Train loss:0.000385, valid loss:0.000411
Epoch:44, Train loss:0.000382, valid loss:0.000422
Epoch:45, Train loss:0.000385, valid loss:0.000419
Epoch:46, Train loss:0.000379, valid loss:0.000423
Epoch:47, Train loss:0.000375, valid loss:0.000407
Epoch:48, Train loss:0.000377, valid loss:0.000418
Epoch:49, Train loss:0.000377, valid loss:0.000423
Epoch:50, Train loss:0.000371, valid loss:0.000414
Epoch:51, Train loss:0.000352, valid loss:0.000409
Epoch:52, Train loss:0.000350, valid loss:0.000408
Epoch:53, Train loss:0.000348, valid loss:0.000407
Epoch:54, Train loss:0.000348, valid loss:0.000413
Epoch:55, Train loss:0.000347, valid loss:0.000421
Epoch:56, Train loss:0.000347, valid loss:0.000410
Epoch:57, Train loss:0.000347, valid loss:0.000410
Epoch:58, Train loss:0.000346, valid loss:0.000409
Epoch:59, Train loss:0.000346, valid loss:0.000411
Epoch:60, Train loss:0.000347, valid loss:0.000410
training time 17053.440948724747
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07936360798970904
plot_id,batch_id 0 1 miss% 0.032571036107899
plot_id,batch_id 0 2 miss% 0.10317794468422085
plot_id,batch_id 0 3 miss% 0.053139716418751974
plot_id,batch_id 0 4 miss% 0.1188858820727323
plot_id,batch_id 0 5 miss% 0.027985141714933506
plot_id,batch_id 0 6 miss% 0.05278872745007332
plot_id,batch_id 0 7 miss% 0.08076451721844753
plot_id,batch_id 0 8 miss% 0.06221004778944986
plot_id,batch_id 0 9 miss% 0.04096063909843295
plot_id,batch_id 0 10 miss% 0.030309344614390336
plot_id,batch_id 0 11 miss% 0.05412248734035735
plot_id,batch_id 0 12 miss% 0.07219448662942901
plot_id,batch_id 0 13 miss% 0.06596321094601848
plot_id,batch_id 0 14 miss% 0.08192165136144298
plot_id,batch_id 0 15 miss% 0.03472685262592702
plot_id,batch_id 0 16 miss% 0.16381007129437858
plot_id,batch_id 0 17 miss% 0.04776661403427857
plot_id,batch_id 0 18 miss% 0.08620564441804107
plot_id,batch_id 0 19 miss% 0.05701230547921972
plot_id,batch_id 0 20 miss% 0.028337349511664212
plot_id,batch_id 0 21 miss% 0.03473884190270277
plot_id,batch_id 0 22 miss% 0.0767779570179649
plot_id,batch_id 0 23 miss% 0.029355315400182783
plot_id,batch_id 0 24 miss% 0.07714438977331843
plot_id,batch_id 0 25 miss% 0.05751831306847212
plot_id,batch_id 0 26 miss% 0.04347574363249315
plot_id,batch_id 0 27 miss% 0.06638297272921684
plot_id,batch_id 0 28 miss% 0.056706005679808005
plot_id,batch_id 0 29 miss% 0.06761487800259687
plot_id,batch_id 0 30 miss% 0.03514644027105491
plot_id,batch_id 0 31 miss% 0.11777449083606772
plot_id,batch_id 0 32 miss% 0.0951006043484415
plot_id,batch_id 0 33 miss% 0.06361549292794304
plot_id,batch_id 0 34 miss% 0.1044809414822405
plot_id,batch_id 0 35 miss% 0.06618929623919141
plot_id,batch_id 0 36 miss% 0.10704207593276592
plot_id,batch_id 0 37 miss% 0.13935580871560813
plot_id,batch_id 0 38 miss% 0.07480101408184651
plot_id,batch_id 0 39 miss% 0.04210750744302839
plot_id,batch_id 0 40 miss% 0.13010754837503966
plot_id,batch_id 0 41 miss% 0.0694933120975309
plot_id,batch_id 0 42 miss% 0.032071169897154785
plot_id,batch_id 0 43 miss% 0.03327244797200595
plot_id,batch_id 0 44 miss% 0.024098227855086952
plot_id,batch_id 0 45 miss% 0.039860222396720865
plot_id,batch_id 0 46 miss% 0.03304861594260269
plot_id,batch_id 0 47 miss% 0.04101505828572869
plot_id,batch_id 0 48 miss% 0.030544657728964492
plot_id,batch_id 0 49 miss% 0.0379055922830431
plot_id,batch_id 0 50 miss% 0.09205187548640749
plot_id,batch_id 0 51 miss% 0.07469848244803895
plot_id,batch_id 0 52 miss% 0.05239670915407477
plot_id,batch_id 0 53 miss% 0.055742643557325264
plot_id,batch_id 0 54 miss% 0.027632444961960385
plot_id,batch_id 0 55 miss% 0.06787104015203749
plot_id,batch_id 0 56 miss% 0.0693927275027254
plot_id,batch_id 0 57 miss% 0.03131847764287586
plot_id,batch_id 0 58 miss% 0.03138314142417403
plot_id,batch_id 0 59 miss% 0.07679118699017794
plot_id,batch_id 0 60 miss% 0.09795345072045494
plot_id,batch_id 0 61 miss% 0.03730677133126448
plot_id,batch_id 0 62 miss% 0.07727735766220871
plot_id,batch_id 0 63 miss% 0.04281008166881482
plot_id,batch_id 0 64 miss% 0.043249589710913056
plot_id,batch_id 0 65 miss% 0.06996216689039787
plot_id,batch_id 0 70 miss% 0.882665001889513
plot_id,batch_id 0 71 miss% 0.848747614495613
plot_id,batch_id 0 72 miss% 0.8312220293748115
plot_id,batch_id 0 73 miss% 0.8272023318082671
plot_id,batch_id 0 74 miss% 0.8247926382372435
plot_id,batch_id 0 75 miss% 0.8815152686404468
plot_id,batch_id 0 76 miss% 0.8439094154225666
plot_id,batch_id 0 77 miss% 0.8311537501873058
plot_id,batch_id 0 78 miss% 0.824302658757992
plot_id,batch_id 0 79 miss% 0.8258512786503043
plot_id,batch_id 0 80 miss% 0.8745465264998606
plot_id,batch_id 0 81 miss% 0.8315579574720665
plot_id,batch_id 0 82 miss% 0.822433169783387
plot_id,batch_id 0 83 miss% 0.81738237248629
plot_id,batch_id 0 84 miss% 0.8141761420952535
plot_id,batch_id 0 85 miss% 0.871395428754998
plot_id,batch_id 0 86 miss% 0.8320282318524064
plot_id,batch_id 0 87 miss% 0.8218215977904928
plot_id,batch_id 0 88 miss% 0.8175207840905834
plot_id,batch_id 0 89 miss% 0.8169122101440878
plot_id,batch_id 0 90 miss% 0.8782232566668644
plot_id,batch_id 0 91 miss% 0.8369499422928698
plot_id,batch_id 0 92 miss% 0.8261958736051267
plot_id,batch_id 0 93 miss% 0.822662507055852
plot_id,batch_id 0 94 miss% 0.821565729931475
plot_id,batch_id 0 95 miss% 0.8877129476422145
plot_id,batch_id 0 96 miss% 0.8407280150691018
plot_id,batch_id 0 97 miss% 0.8235605548629917
plot_id,batch_id 0 98 miss% 0.8217524196180881
plot_id,batch_id 0 99 miss% 0.8214402652675674
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.8505622  0.8223787  0.81803786 0.81332678 0.8125498  0.85535882
 0.82457938 0.81980501 0.81745145 0.81569313 0.85458876 0.82674614
 0.82112425 0.81379671 0.81828621 0.8513876  0.82708317 0.82141399
 0.82345843 0.82343088 0.83942797 0.81809228 0.81539061 0.81404618
 0.80974425 0.83831755 0.82142449 0.81605713 0.81327488 0.81123365
 0.83987659 0.82252933 0.82172192 0.8172348  0.81513113 0.84133533
 0.82218643 0.81976157 0.81876652 0.81402521 0.82721192 0.81271544
 0.80977392 0.81006758 0.80969916 0.82269407 0.81711854 0.81419053
 0.8141452  0.81104151 0.82893839 0.81813344 0.81613244 0.8124866
 0.81467841 0.83084598 0.81887552 0.81628626 0.81489882 0.81643935
 0.88376378 0.84091576 0.82929798 0.8209547  0.81844    0.88442638
 0.8496657  0.83035094 0.82249605 0.82094426 0.882665   0.84874761
 0.83122203 0.82720233 0.82479264 0.88151527 0.84390942 0.83115375
 0.82430266 0.82585128 0.87454653 0.83155796 0.82243317 0.81738237
 0.81417614 0.87139543 0.83202823 0.8218216  0.81752078 0.81691221
 0.87822326 0.83694994 0.82619587 0.82266251 0.82156573 0.88771295
 0.84072802 0.82356055 0.82175242 0.82144027]
for model  157 the mean error 0.8282019363131878
all id 157 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 25 out win 5 err 0.8282019363131878 time 17139.589416742325
Launcher: Job 158 completed in 17270 seconds.
Launcher: Task 82 done. Exiting.
plot_id,batch_id 0 66 miss% 0.02255177618230672
plot_id,batch_id 0 67 miss% 0.04055278533372499
plot_id,batch_id 0 68 miss% 0.061216924893642735
plot_id,batch_id 0 69 miss% 0.09631084920150026
plot_id,batch_id 0 70 miss% 0.039914817966820365
plot_id,batch_id 0 71 miss% 0.039215859859603
plot_id,batch_id 0 72 miss% 0.07727340135587593
plot_id,batch_id 0 73 miss% 0.05249095047063944
plot_id,batch_id 0 74 miss% 0.12515781499812412
plot_id,batch_id 0 75 miss% 0.06336944198455273
plot_id,batch_id 0 76 miss% 0.04817292338916067
plot_id,batch_id 0 77 miss% 0.03996886945978424
plot_id,batch_id 0 78 miss% 0.04695255642713464
plot_id,batch_id 0 79 miss% 0.07721665159098491
plot_id,batch_id 0 80 miss% 0.08363322513293889
plot_id,batch_id 0 81 miss% 0.09461730696322164
plot_id,batch_id 0 82 miss% 0.060175749054904006
plot_id,batch_id 0 83 miss% 0.052607935842108496
plot_id,batch_id 0 84 miss% 0.10620233049219611
plot_id,batch_id 0 85 miss% 0.028210627857698154
plot_id,batch_id 0 86 miss% 0.11239575585438628
plot_id,batch_id 0 87 miss% 0.05737740990630722
plot_id,batch_id 0 88 miss% 0.09789847663738718
plot_id,batch_id 0 89 miss% 0.06800989355255627
plot_id,batch_id 0 90 miss% 0.027052337758505365
plot_id,batch_id 0 91 miss% 0.06096530366051774
plot_id,batch_id 0 92 miss% 0.060875154285022604
plot_id,batch_id 0 93 miss% 0.032214251475155664
plot_id,batch_id 0 94 miss% 0.11994339134183948
plot_id,batch_id 0 95 miss% 0.04841938492043902
plot_id,batch_id 0 96 miss% 0.059767298250120185
plot_id,batch_id 0 97 miss% 0.04794711486024671
plot_id,batch_id 0 98 miss% 0.03592860652194665
plot_id,batch_id 0 99 miss% 0.06291070950627352
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07936361 0.03257104 0.10317794 0.05313972 0.11888588 0.02798514
 0.05278873 0.08076452 0.06221005 0.04096064 0.03030934 0.05412249
 0.07219449 0.06596321 0.08192165 0.03472685 0.16381007 0.04776661
 0.08620564 0.05701231 0.02833735 0.03473884 0.07677796 0.02935532
 0.07714439 0.05751831 0.04347574 0.06638297 0.05670601 0.06761488
 0.03514644 0.11777449 0.0951006  0.06361549 0.10448094 0.0661893
 0.10704208 0.13935581 0.07480101 0.04210751 0.13010755 0.06949331
 0.03207117 0.03327245 0.02409823 0.03986022 0.03304862 0.04101506
 0.03054466 0.03790559 0.09205188 0.07469848 0.05239671 0.05574264
 0.02763244 0.06787104 0.06939273 0.03131848 0.03138314 0.07679119
 0.09795345 0.03730677 0.07727736 0.04281008 0.04324959 0.06996217
 0.02255178 0.04055279 0.06121692 0.09631085 0.03991482 0.03921586
 0.0772734  0.05249095 0.12515781 0.06336944 0.04817292 0.03996887
 0.04695256 0.07721665 0.08363323 0.09461731 0.06017575 0.05260794
 0.10620233 0.02821063 0.11239576 0.05737741 0.09789848 0.06800989
 0.02705234 0.0609653  0.06087515 0.03221425 0.11994339 0.04841938
 0.0597673  0.04794711 0.03592861 0.06291071]
for model  169 the mean error 0.06296318255410098
all id 169 hidden_dim 32 learning_rate 0.0025 num_layers 3 frames 31 out win 5 err 0.06296318255410098 time 17053.440948724747
Launcher: Job 170 completed in 17315 seconds.
Launcher: Task 210 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  69649
Epoch:0, Train loss:0.458467, valid loss:0.438974
Epoch:1, Train loss:0.028617, valid loss:0.003235
Epoch:2, Train loss:0.008735, valid loss:0.002205
Epoch:3, Train loss:0.005964, valid loss:0.001693
Epoch:4, Train loss:0.004483, valid loss:0.001489
Epoch:5, Train loss:0.002566, valid loss:0.001218
Epoch:6, Train loss:0.002278, valid loss:0.001227
Epoch:7, Train loss:0.002057, valid loss:0.001009
Epoch:8, Train loss:0.001875, valid loss:0.000983
Epoch:9, Train loss:0.001769, valid loss:0.000890
Epoch:10, Train loss:0.001652, valid loss:0.000830
Epoch:11, Train loss:0.001317, valid loss:0.000703
Epoch:12, Train loss:0.001258, valid loss:0.000706
Epoch:13, Train loss:0.001234, valid loss:0.000665
Epoch:14, Train loss:0.001180, valid loss:0.000756
Epoch:15, Train loss:0.001153, valid loss:0.000624
Epoch:16, Train loss:0.001120, valid loss:0.000651
Epoch:17, Train loss:0.001087, valid loss:0.000798
Epoch:18, Train loss:0.001070, valid loss:0.000703
Epoch:19, Train loss:0.001038, valid loss:0.000610
Epoch:20, Train loss:0.001000, valid loss:0.000699
Epoch:21, Train loss:0.000858, valid loss:0.000563
Epoch:22, Train loss:0.000830, valid loss:0.000517
Epoch:23, Train loss:0.000823, valid loss:0.000558
Epoch:24, Train loss:0.000803, valid loss:0.000532
Epoch:25, Train loss:0.000807, valid loss:0.000590
Epoch:26, Train loss:0.000779, valid loss:0.000547
Epoch:27, Train loss:0.000777, valid loss:0.000520
Epoch:28, Train loss:0.000768, valid loss:0.000567
Epoch:29, Train loss:0.000764, valid loss:0.000522
Epoch:30, Train loss:0.000751, valid loss:0.000553
Epoch:31, Train loss:0.000660, valid loss:0.000473
Epoch:32, Train loss:0.000664, valid loss:0.000481
Epoch:33, Train loss:0.000653, valid loss:0.000483
Epoch:34, Train loss:0.000652, valid loss:0.000488
Epoch:35, Train loss:0.000644, valid loss:0.000492
Epoch:36, Train loss:0.000638, valid loss:0.000489
Epoch:37, Train loss:0.000628, valid loss:0.000512
Epoch:38, Train loss:0.000629, valid loss:0.000492
Epoch:39, Train loss:0.000623, valid loss:0.000546
Epoch:40, Train loss:0.000615, valid loss:0.000486
Epoch:41, Train loss:0.000577, valid loss:0.000479
Epoch:42, Train loss:0.000572, valid loss:0.000469
Epoch:43, Train loss:0.000571, valid loss:0.000464
Epoch:44, Train loss:0.000573, valid loss:0.000505
Epoch:45, Train loss:0.000570, valid loss:0.000492
Epoch:46, Train loss:0.000560, valid loss:0.000478
Epoch:47, Train loss:0.000565, valid loss:0.000470
Epoch:48, Train loss:0.000558, valid loss:0.000480
Epoch:49, Train loss:0.000556, valid loss:0.000455
Epoch:50, Train loss:0.000555, valid loss:0.000478
Epoch:51, Train loss:0.000528, valid loss:0.000458
Epoch:52, Train loss:0.000525, valid loss:0.000454
Epoch:53, Train loss:0.000524, valid loss:0.000456
Epoch:54, Train loss:0.000524, valid loss:0.000459
Epoch:55, Train loss:0.000523, valid loss:0.000460
Epoch:56, Train loss:0.000523, valid loss:0.000462
Epoch:57, Train loss:0.000522, valid loss:0.000459
Epoch:58, Train loss:0.000523, valid loss:0.000454
Epoch:59, Train loss:0.000522, valid loss:0.000457
Epoch:60, Train loss:0.000522, valid loss:0.000458
training time 17381.734410762787
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07682329127218421
plot_id,batch_id 0 1 miss% 0.05384151666380835
plot_id,batch_id 0 2 miss% 0.12266283999810286
plot_id,batch_id 0 3 miss% 0.07034240707160372
plot_id,batch_id 0 4 miss% 0.057242175684273415
plot_id,batch_id 0 5 miss% 0.0785557491218637
plot_id,batch_id 0 6 miss% 0.05516373580375517
plot_id,batch_id 0 7 miss% 0.1149165434874276
plot_id,batch_id 0 8 miss% 0.08267945114941203
plot_id,batch_id 0 9 miss% 0.055762477585082036
plot_id,batch_id 0 10 miss% 0.06363112852403961
plot_id,batch_id 0 11 miss% 0.054830952214056004
plot_id,batch_id 0 12 miss% 0.07712236733758371
plot_id,batch_id 0 13 miss% 0.10769617887042228
plot_id,batch_id 0 14 miss% 0.11997313644610627
plot_id,batch_id 0 15 miss% 0.20399332656294006
plot_id,batch_id 0 16 miss% 0.0916856313439098
plot_id,batch_id 0 17 miss% 0.046431598767400085
plot_id,batch_id 0 18 miss% 0.09182154202151292
plot_id,batch_id 0 19 miss% 0.1138676851599982
plot_id,batch_id 0 20 miss% 0.07602388722752175
plot_id,batch_id 0 21 miss% 0.056569974700941175
plot_id,batch_id 0 22 miss% 0.04462561014164097
plot_id,batch_id 0 23 miss% 0.06702507198726376
plot_id,batch_id 0 24 miss% 0.04908033913753716
plot_id,batch_id 0 25 miss% 0.05202982335716936
plot_id,batch_id 0 26 miss% 0.072649380867017
plot_id,batch_id 0 27 miss% 0.035603111838637465
plot_id,batch_id 0 28 miss% 0.046718025351135284
plot_id,batch_id 0 29 miss% 0.044653051622361106
plot_id,batch_id 0 30 miss% 0.05065932204303627
plot_id,batch_id 0 31 miss% 0.13711449402795864
plot_id,batch_id 0 32 miss% 0.10692748814903466
plot_id,batch_id 0 33 miss% 0.07510702686173487
plot_id,batch_id 0 34 miss% 0.027309049107680033
plot_id,batch_id 0 35 miss% 0.048754645705980475
plot_id,batch_id 0 36 miss% 0.0735438559992825
plot_id,batch_id 0 37 miss% 0.0722162274546888
plot_id,batch_id 0 38 miss% 0.05659718401323823
plot_id,batch_id 0 39 miss% 0.058828607316168874
plot_id,batch_id 0 40 miss% 0.09599663625285428
plot_id,batch_id 0 41 miss% 0.05865809514450121
plot_id,batch_id 0 42 miss% 0.05774198892472788
plot_id,batch_id 0 43 miss% 0.06425842461879631
plot_id,batch_id 0 44 miss% 0.04253818965403204
plot_id,batch_id 0 45 miss% 0.036676612611781484
plot_id,batch_id 0 46 miss% 0.02248658948344804
plot_id,batch_id 0 47 miss% 0.0473080273119784
plot_id,batch_id 0 48 miss% 0.029150846591093155
plot_id,batch_id 0 49 miss% 0.039112960787530175
plot_id,batch_id 0 50 miss% 0.1301772445365495
plot_id,batch_id 0 51 miss% 0.030273069230767103
plot_id,batch_id 0 52 miss% 0.056300319158682145
plot_id,batch_id 0 53 miss% 0.06492760753166726
plot_id,batch_id 0 54 miss% 0.07475402385177038
plot_id,batch_id 0 55 miss% 0.11034288820771015
plot_id,batch_id 0 56 miss% 0.14126991540864947
plot_id,batch_id 0 57 miss% 0.08078669366894094
plot_id,batch_id 0 58 miss% 0.08160133670774725
plot_id,batch_id 0 59 miss% 0.03242172256687783
plot_id,batch_id 0 60 miss% 0.058306057290283375
plot_id,batch_id 0 61 miss% 0.035629161156722024
plot_id,batch_id 0 62 miss% 0.06182574546050464
plot_id,batch_id 0 63 miss% 0.06023292800065975
plot_id,batch_id 0 64 miss% 0.08175828041406441
plot_id,batch_id 0 65 miss% 0.05063287252399319
plot_id,batch_id 0 66 miss% 0.12256730795652628
plot_id,batch_id 0 67 miss% 0.029093964356572473
plot_id,batch_id 0 68 miss% 0.025849955782657485
plot_id,batch_id 0 69 miss% 0.0652391622608474
plot_id,batch_id 0 70 miss% 0.03255309648888692
plot_id,batch_id 0 71 miss% 0.046372268215467805
plot_id,batch_id 0 72 miss% 0.12329313239936328
plot_id,batch_id 0 73 miss% 0.0706403347427794
plot_id,batch_id 0 74 miss% 0.19210654209016387
plot_id,batch_id 0 75 miss% 0.08057613973417992
plot_id,batch_id 0 76 miss% 0.09895255599881488
plot_id,batch_id 0 77 miss% 0.0590763839480674
plot_id,batch_id 0 78 miss% 0.06341150541681967
plot_id,batch_id 0 79 miss% 0.04961968978125005
plot_id,batch_id 0 80 miss% 0.06063669248056251
plot_id,batch_id 0 81 miss% 0.08032141860280907
plot_id,batch_id 0 82 miss% 0.06162027034950929
plot_id,batch_id 0 83 miss% 0.11756295674571818
plot_id,batch_id 0 84 miss% 0.08749915624197237
plot_id,batch_id 0 85 miss% 0.033127029321131936
plot_id,batch_id 0 86 miss% 0.04126779151191738
plot_id,batch_id 0 87 miss% 0.10417240464180146
plot_id,batch_id 0 88 miss% 0.11167450353600483
plot_id,batch_id 0 89 miss% 0.07879891004301871
plot_id,batch_id 0 90 miss% 0.03984033067720033
plot_id,batch_id 0 91 miss% 0.043426515904043644
plot_id,batch_id 0 92 miss% 0.05037682777284765
plot_id,batch_id 0 93 miss% 0.04667251936758067
plot_id,batch_id 0 94 miss% 0.12877551117397595
plot_id,batch_id 0 95 miss% 0.07293482589740613
plot_id,batch_id 0 96 miss% 0.09746463099029166
plot_id,batch_id 0 97 miss% 0.026689720349154573
plot_id,batch_id 0 98 miss% 0.08710923374427196
plot_id,batch_id 0 99 miss% 0.04166542404933108
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07682329 0.05384152 0.12266284 0.07034241 0.05724218 0.07855575
 0.05516374 0.11491654 0.08267945 0.05576248 0.06363113 0.05483095
 0.07712237 0.10769618 0.11997314 0.20399333 0.09168563 0.0464316
 0.09182154 0.11386769 0.07602389 0.05656997 0.04462561 0.06702507
 0.04908034 0.05202982 0.07264938 0.03560311 0.04671803 0.04465305
 0.05065932 0.13711449 0.10692749 0.07510703 0.02730905 0.04875465
 0.07354386 0.07221623 0.05659718 0.05882861 0.09599664 0.0586581
 0.05774199 0.06425842 0.04253819 0.03667661 0.02248659 0.04730803
 0.02915085 0.03911296 0.13017724 0.03027307 0.05630032 0.06492761
 0.07475402 0.11034289 0.14126992 0.08078669 0.08160134 0.03242172
 0.05830606 0.03562916 0.06182575 0.06023293 0.08175828 0.05063287
 0.12256731 0.02909396 0.02584996 0.06523916 0.0325531  0.04637227
 0.12329313 0.07064033 0.19210654 0.08057614 0.09895256 0.05907638
 0.06341151 0.04961969 0.06063669 0.08032142 0.06162027 0.11756296
 0.08749916 0.03312703 0.04126779 0.1041724  0.1116745  0.07879891
 0.03984033 0.04342652 0.05037683 0.04667252 0.12877551 0.07293483
 0.09746463 0.02668972 0.08710923 0.04166542]
for model  181 the mean error 0.07107236861664788
all id 181 hidden_dim 16 learning_rate 0.0025 num_layers 5 frames 31 out win 5 err 0.07107236861664788 time 17381.734410762787
Launcher: Job 182 completed in 17643 seconds.
Launcher: Task 113 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  154129
Epoch:0, Train loss:0.406906, valid loss:0.413894
Epoch:1, Train loss:0.019890, valid loss:0.003297
Epoch:2, Train loss:0.004602, valid loss:0.001593
Epoch:3, Train loss:0.002913, valid loss:0.001317
Epoch:4, Train loss:0.002592, valid loss:0.001061
Epoch:5, Train loss:0.002394, valid loss:0.001133
Epoch:6, Train loss:0.002141, valid loss:0.001050
Epoch:7, Train loss:0.002039, valid loss:0.001044
Epoch:8, Train loss:0.001963, valid loss:0.000964
Epoch:9, Train loss:0.001755, valid loss:0.000958
Epoch:10, Train loss:0.001775, valid loss:0.001018
Epoch:11, Train loss:0.001247, valid loss:0.000678
Epoch:12, Train loss:0.001170, valid loss:0.000800
Epoch:13, Train loss:0.001094, valid loss:0.000661
Epoch:14, Train loss:0.001072, valid loss:0.000738
Epoch:15, Train loss:0.001023, valid loss:0.000665
Epoch:16, Train loss:0.000998, valid loss:0.000635
Epoch:17, Train loss:0.001014, valid loss:0.000746
Epoch:18, Train loss:0.000927, valid loss:0.000634
Epoch:19, Train loss:0.000912, valid loss:0.000647
Epoch:20, Train loss:0.000911, valid loss:0.000909
Epoch:21, Train loss:0.000680, valid loss:0.000546
Epoch:22, Train loss:0.000632, valid loss:0.000597
Epoch:23, Train loss:0.000640, valid loss:0.000489
Epoch:24, Train loss:0.000606, valid loss:0.000508
Epoch:25, Train loss:0.000606, valid loss:0.000546
Epoch:26, Train loss:0.000596, valid loss:0.000515
Epoch:27, Train loss:0.000589, valid loss:0.000486
Epoch:28, Train loss:0.000582, valid loss:0.000508
Epoch:29, Train loss:0.000577, valid loss:0.000585
Epoch:30, Train loss:0.000575, valid loss:0.000595
Epoch:31, Train loss:0.000449, valid loss:0.000460
Epoch:32, Train loss:0.000430, valid loss:0.000519
Epoch:33, Train loss:0.000433, valid loss:0.000485
Epoch:34, Train loss:0.000425, valid loss:0.000476
Epoch:35, Train loss:0.000427, valid loss:0.000464
Epoch:36, Train loss:0.000422, valid loss:0.000479
Epoch:37, Train loss:0.000407, valid loss:0.000556
Epoch:38, Train loss:0.000413, valid loss:0.000461
Epoch:39, Train loss:0.000409, valid loss:0.000484
Epoch:40, Train loss:0.000405, valid loss:0.000480
Epoch:41, Train loss:0.000351, valid loss:0.000459
Epoch:42, Train loss:0.000345, valid loss:0.000464
Epoch:43, Train loss:0.000344, valid loss:0.000446
Epoch:44, Train loss:0.000341, valid loss:0.000459
Epoch:45, Train loss:0.000339, valid loss:0.000461
Epoch:46, Train loss:0.000339, valid loss:0.000480
Epoch:47, Train loss:0.000334, valid loss:0.000463
Epoch:48, Train loss:0.000333, valid loss:0.000481
Epoch:49, Train loss:0.000330, valid loss:0.000486
Epoch:50, Train loss:0.000328, valid loss:0.000457
Epoch:51, Train loss:0.000310, valid loss:0.000456
Epoch:52, Train loss:0.000308, valid loss:0.000454
Epoch:53, Train loss:0.000307, valid loss:0.000452
Epoch:54, Train loss:0.000306, valid loss:0.000453
Epoch:55, Train loss:0.000305, valid loss:0.000452
Epoch:56, Train loss:0.000305, valid loss:0.000451
Epoch:57, Train loss:0.000304, valid loss:0.000451
Epoch:58, Train loss:0.000304, valid loss:0.000451
Epoch:59, Train loss:0.000304, valid loss:0.000450
Epoch:60, Train loss:0.000304, valid loss:0.000451
training time 17529.69333934784
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07833705074219198
plot_id,batch_id 0 1 miss% 0.09134147001747542
plot_id,batch_id 0 2 miss% 0.1056099989551256
plot_id,batch_id 0 3 miss% 0.03567886947710759
plot_id,batch_id 0 4 miss% 0.041799366970810915
plot_id,batch_id 0 5 miss% 0.10969428738928802
plot_id,batch_id 0 6 miss% 0.055676975097435646
plot_id,batch_id 0 7 miss% 0.09011572443706571
plot_id,batch_id 0 8 miss% 0.05805645948715837
plot_id,batch_id 0 9 miss% 0.07975334552061446
plot_id,batch_id 0 10 miss% 0.03747185784992204
plot_id,batch_id 0 11 miss% 0.06941750048180255
plot_id,batch_id 0 12 miss% 0.049624583825114754
plot_id,batch_id 0 13 miss% 0.043421205305767534
plot_id,batch_id 0 14 miss% 0.07744202331702123
plot_id,batch_id 0 15 miss% 0.029473944811836017
plot_id,batch_id 0 16 miss% 0.06251519904886185
plot_id,batch_id 0 17 miss% 0.036199428460928655
plot_id,batch_id 0 18 miss% 0.05623904570708461
plot_id,batch_id 0 19 miss% 0.054491015296866514
plot_id,batch_id 0 20 miss% 0.04836133786156026
plot_id,batch_id 0 21 miss% 0.022314414830300323
plot_id,batch_id 0 22 miss% 0.038854153621497266
plot_id,batch_id 0 23 miss% 0.038174393229881594
plot_id,batch_id 0 24 miss% 0.04609858271597216
plot_id,batch_id 0 25 miss% 0.05467906241474016
plot_id,batch_id 0 26 miss% 0.06169613409803291
plot_id,batch_id 0 27 miss% 0.0398495825818111
plot_id,batch_id 0 28 miss% 0.0278428762643918
plot_id,batch_id 0 29 miss% 0.041766183560897785
plot_id,batch_id 0 30 miss% 0.03467448682284428
plot_id,batch_id 0 31 miss% 0.09565798206553469
plot_id,batch_id 0 32 miss% 0.09079209900682178
plot_id,batch_id 0 33 miss% 0.036998191089224246
plot_id,batch_id 0 34 miss% 0.03535177873582033
plot_id,batch_id 0 35 miss% 0.05878969505139892
plot_id,batch_id 0 36 miss% 0.1102299744271831
plot_id,batch_id 0 37 miss% 0.059045951531054755
plot_id,batch_id 0 38 miss% 0.028482633126674817
plot_id,batch_id 0 39 miss% 0.033457358314720596
plot_id,batch_id 0 40 miss% 0.06546072861821761
plot_id,batch_id 0 41 miss% 0.03267945581559865
plot_id,batch_id 0 42 miss% 0.024158602681195088
plot_id,batch_id 0 43 miss% 0.03197675160568043
plot_id,batch_id 0 44 miss% 0.013964612111849488
plot_id,batch_id 0 45 miss% 0.053667906632185555
plot_id,batch_id 0 46 miss% 0.057512647936528376
plot_id,batch_id 0 47 miss% 0.025062729927427134
plot_id,batch_id 0 48 miss% 0.04072449430535176
plot_id,batch_id 0 49 miss% 0.03648399112762283
plot_id,batch_id 0 50 miss% 0.2008054982862203
plot_id,batch_id 0 51 miss% 0.036290043704155096
plot_id,batch_id 0 52 miss% 0.028144686845881388
plot_id,batch_id 0 53 miss% 0.0235161652537494
plot_id,batch_id 0 54 miss% 0.03904324390540088
plot_id,batch_id 0 55 miss% 0.035560312765806665
plot_id,batch_id 0 56 miss% 0.10424735439175738
plot_id,batch_id 0 57 miss% 0.032909134274010904
plot_id,batch_id 0 58 miss% 0.030720376687607215
plot_id,batch_id 0 59 miss% 0.027003316153465382
plot_id,batch_id 0 60 miss% 0.0335123931335665
plot_id,batch_id 0 61 miss% 0.04850225057807607
plot_id,batch_id 0 62 miss% 0.06287700839696866
plot_id,batch_id 0 63 miss% 0.06541954316070989
plot_id,batch_id 0 64 miss% 0.06305989832369327
plot_id,batch_id 0 65 miss% 0.06517021469053122
plot_id,batch_id 0 66 miss% 0.03813673408859833
plot_id,batch_id 0 67 miss% 0.02877125322249785
plot_id,batch_id 0 68 miss% 0.04177766382478166
plot_id,batch_id 0 69 miss% 0.08239515432273144
plot_id,batch_id 0 70 miss% 0.04195543470775037
plot_id,batch_id 0 71 miss% 0.028879797332267962
plot_id,batch_id 0 72 miss% 0.056299658750657226
plot_id,batch_id 0 73 miss% 0.027260549426797887
plot_id,batch_id 0 74 miss% 0.08342502291670048
plot_id,batch_id 0 75 miss% 0.10590191778997568
plot_id,batch_id 0 76 miss% 0.04267932992331434
plot_id,batch_id 0 77 miss% 0.04516869634897818
plot_id,batch_id 0 78 miss% 0.04654680812351369
plot_id,batch_id 0 79 miss% 0.06598240808137248
plot_id,batch_id 0 80 miss% 0.1653479280848431
plot_id,batch_id 0 81 miss% 0.09436522386633531
plot_id,batch_id 0 82 miss% 0.06687711352627124
plot_id,batch_id 0 83 miss% 0.061361511342551606
plot_id,batch_id 0 84 miss% 0.05555193398246322
plot_id,batch_id 0 85 miss% 0.055693583055274255
plot_id,batch_id 0 86 miss% 0.049644114959063845
plot_id,batch_id 0 87 miss% 0.06348721877536878
plot_id,batch_id 0 88 miss% 0.057033205586677675
plot_id,batch_id 0 89 miss% 0.0446548285463841
plot_id,batch_id 0 90 miss% 0.032513562974193166
plot_id,batch_id 0 91 miss% 0.06935499000766822
plot_id,batch_id 0 92 miss% 0.06281556552983521
plot_id,batch_id 0 93 miss% 0.0460066431192279
plot_id,batch_id 0 94 miss% 0.06761518445391201
plot_id,batch_id 0 95 miss% 0.10777373237097426
plot_id,batch_id 0 96 miss% 0.042508018610020105
plot_id,batch_id 0 97 miss% 0.037589986172036294
plot_id,batch_id 0 98 miss% 0.046609777866997326
plot_id,batch_id 0 99 miss% 0.040187209278204995
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07833705 0.09134147 0.10561    0.03567887 0.04179937 0.10969429
 0.05567698 0.09011572 0.05805646 0.07975335 0.03747186 0.0694175
 0.04962458 0.04342121 0.07744202 0.02947394 0.0625152  0.03619943
 0.05623905 0.05449102 0.04836134 0.02231441 0.03885415 0.03817439
 0.04609858 0.05467906 0.06169613 0.03984958 0.02784288 0.04176618
 0.03467449 0.09565798 0.0907921  0.03699819 0.03535178 0.0587897
 0.11022997 0.05904595 0.02848263 0.03345736 0.06546073 0.03267946
 0.0241586  0.03197675 0.01396461 0.05366791 0.05751265 0.02506273
 0.04072449 0.03648399 0.2008055  0.03629004 0.02814469 0.02351617
 0.03904324 0.03556031 0.10424735 0.03290913 0.03072038 0.02700332
 0.03351239 0.04850225 0.06287701 0.06541954 0.0630599  0.06517021
 0.03813673 0.02877125 0.04177766 0.08239515 0.04195543 0.0288798
 0.05629966 0.02726055 0.08342502 0.10590192 0.04267933 0.0451687
 0.04654681 0.06598241 0.16534793 0.09436522 0.06687711 0.06136151
 0.05555193 0.05569358 0.04964411 0.06348722 0.05703321 0.04465483
 0.03251356 0.06935499 0.06281557 0.04600664 0.06761518 0.10777373
 0.04250802 0.03758999 0.04660978 0.04018721]
for model  224 the mean error 0.055461213458313396
all id 224 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 31 out win 6 err 0.055461213458313396 time 17529.69333934784
Launcher: Job 225 completed in 17786 seconds.
Launcher: Task 134 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  209681
Epoch:0, Train loss:0.459966, valid loss:0.469450
Epoch:1, Train loss:0.025658, valid loss:0.003513
Epoch:2, Train loss:0.007820, valid loss:0.002490
Epoch:3, Train loss:0.004163, valid loss:0.001721
Epoch:4, Train loss:0.003067, valid loss:0.001454
Epoch:5, Train loss:0.002613, valid loss:0.001193
Epoch:6, Train loss:0.002365, valid loss:0.001320
Epoch:7, Train loss:0.002169, valid loss:0.001160
Epoch:8, Train loss:0.002017, valid loss:0.001307
Epoch:9, Train loss:0.001922, valid loss:0.001007
Epoch:10, Train loss:0.001797, valid loss:0.000958
Epoch:11, Train loss:0.001270, valid loss:0.000817
Epoch:12, Train loss:0.001241, valid loss:0.000791
Epoch:13, Train loss:0.001168, valid loss:0.000752
Epoch:14, Train loss:0.001179, valid loss:0.000700
Epoch:15, Train loss:0.001154, valid loss:0.000673
Epoch:16, Train loss:0.001062, valid loss:0.000705
Epoch:17, Train loss:0.001043, valid loss:0.000721
Epoch:18, Train loss:0.000995, valid loss:0.000722
Epoch:19, Train loss:0.000993, valid loss:0.000649
Epoch:20, Train loss:0.000967, valid loss:0.000826
Epoch:21, Train loss:0.000702, valid loss:0.000588
Epoch:22, Train loss:0.000681, valid loss:0.000582
Epoch:23, Train loss:0.000656, valid loss:0.000617
Epoch:24, Train loss:0.000665, valid loss:0.000564
Epoch:25, Train loss:0.000641, valid loss:0.000546
Epoch:26, Train loss:0.000623, valid loss:0.000577
Epoch:27, Train loss:0.000627, valid loss:0.000572
Epoch:28, Train loss:0.000635, valid loss:0.000563
Epoch:29, Train loss:0.000585, valid loss:0.000550
Epoch:30, Train loss:0.000590, valid loss:0.000589
Epoch:31, Train loss:0.000479, valid loss:0.000529
Epoch:32, Train loss:0.000474, valid loss:0.000518
Epoch:33, Train loss:0.000467, valid loss:0.000510
Epoch:34, Train loss:0.000454, valid loss:0.000548
Epoch:35, Train loss:0.000465, valid loss:0.000523
Epoch:36, Train loss:0.000454, valid loss:0.000503
Epoch:37, Train loss:0.000444, valid loss:0.000513
Epoch:38, Train loss:0.000451, valid loss:0.000549
Epoch:39, Train loss:0.000439, valid loss:0.000519
Epoch:40, Train loss:0.000451, valid loss:0.000496
Epoch:41, Train loss:0.000382, valid loss:0.000488
Epoch:42, Train loss:0.000379, valid loss:0.000504
Epoch:43, Train loss:0.000375, valid loss:0.000478
Epoch:44, Train loss:0.000373, valid loss:0.000504
Epoch:45, Train loss:0.000373, valid loss:0.000491
Epoch:46, Train loss:0.000369, valid loss:0.000487
Epoch:47, Train loss:0.000368, valid loss:0.000495
Epoch:48, Train loss:0.000365, valid loss:0.000484
Epoch:49, Train loss:0.000361, valid loss:0.000507
Epoch:50, Train loss:0.000359, valid loss:0.000492
Epoch:51, Train loss:0.000338, valid loss:0.000482
Epoch:52, Train loss:0.000334, valid loss:0.000481
Epoch:53, Train loss:0.000333, valid loss:0.000480
Epoch:54, Train loss:0.000333, valid loss:0.000479
Epoch:55, Train loss:0.000332, valid loss:0.000481
Epoch:56, Train loss:0.000332, valid loss:0.000480
Epoch:57, Train loss:0.000331, valid loss:0.000481
Epoch:58, Train loss:0.000331, valid loss:0.000481
Epoch:59, Train loss:0.000331, valid loss:0.000480
Epoch:60, Train loss:0.000331, valid loss:0.000481
training time 17580.928480386734
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08551281758188012
plot_id,batch_id 0 1 miss% 0.03800716020789347
plot_id,batch_id 0 2 miss% 0.1035815833057123
plot_id,batch_id 0 3 miss% 0.07161866575686525
plot_id,batch_id 0 4 miss% 0.0389658566931914
plot_id,batch_id 0 5 miss% 0.03508015599032319
plot_id,batch_id 0 6 miss% 0.045543270095299876
plot_id,batch_id 0 7 miss% 0.0781519887451162
plot_id,batch_id 0 8 miss% 0.0837838544436654
plot_id,batch_id 0 9 miss% 0.09648771799862078
plot_id,batch_id 0 10 miss% 0.031235669209154333
plot_id,batch_id 0 11 miss% 0.0814220052296079
plot_id,batch_id 0 12 miss% 0.045372687707357244
plot_id,batch_id 0 13 miss% 0.05888817779176154
plot_id,batch_id 0 14 miss% 0.08789016133895866
plot_id,batch_id 0 15 miss% 0.06738768518999128
plot_id,batch_id 0 16 miss% 0.13260125702965744
plot_id,batch_id 0 17 miss% 0.022727936621832184
plot_id,batch_id 0 18 miss% 0.07626595284889917
plot_id,batch_id 0 19 miss% 0.05801102410438364
plot_id,batch_id 0 20 miss% 0.04723752436059851
plot_id,batch_id 0 21 miss% 0.04809697700963466
plot_id,batch_id 0 22 miss% 0.048807032850042904
plot_id,batch_id 0 23 miss% 0.040755962966947765
plot_id,batch_id 0 24 miss% 0.05877057692874201
plot_id,batch_id 0 25 miss% 0.05110658598181136
plot_id,batch_id 0 26 miss% 0.03495907661293538
plot_id,batch_id 0 27 miss% 0.04191095217453132
plot_id,batch_id 0 28 miss% 0.02500029426532532
plot_id,batch_id 0 29 miss% 0.03485574342785073
plot_id,batch_id 0 30 miss% 0.01370403230858655
plot_id,batch_id 0 31 miss% 0.08808336935001485
plot_id,batch_id 0 32 miss% 0.054951585340218906
plot_id,batch_id 0 33 miss% 0.048814738929455405
plot_id,batch_id 0 34 miss% 0.032027285101195783
plot_id,batch_id 0 35 miss% 0.04948785609987449
plot_id,batch_id 0 36 miss% 0.07089221669398318
plot_id,batch_id 0 37 miss% 0.06503404548144356
plot_id,batch_id 0 38 miss% 0.05376085594357722
plot_id,batch_id 0 39 miss% 0.02864172394387081
plot_id,batch_id 0 40 miss% 0.06713202260689381
plot_id,batch_id 0 41 miss% 0.05630937337232564
plot_id,batch_id 0 42 miss% 0.018699739484866258
plot_id,batch_id 0 43 miss% 0.06491948379951401
plot_id,batch_id 0 44 miss% 0.01761142831748865
plot_id,batch_id 0 45 miss% 0.06156037885337444
plot_id,batch_id 0 46 miss% 0.031196054756494316
plot_id,batch_id 0 47 miss% 0.025729005654226245
plot_id,batch_id 0 48 miss% 0.04422045694104612
plot_id,batch_id 0 49 miss% 0.022959412745570746
plot_id,batch_id 0 50 miss% 0.14097875714768923
plot_id,batch_id 0 51 miss% 0.03244168573079044
plot_id,batch_id 0 52 miss% 0.02574331842498649
plot_id,batch_id 0 53 miss% 0.037929002159168064
plot_id,batch_id 0 54 miss% 0.025445627772084353
plot_id,batch_id 0 55 miss% 0.050609477479282815
plot_id,batch_id 0 56 miss% 0.05730374699925548
plot_id,batch_id 0 57 miss% 0.028985950537768444
plot_id,batch_id 0 58 miss% 0.05811837527744192
plot_id,batch_id 0 59 miss% 0.042738411711666445
plot_id,batch_id 0 60 miss% 0.03341231817439879
plot_id,batch_id 0 61 miss% 0.02638367356775859
plot_id,batch_id 0 62 miss% 0.04416962484906068
plot_id,batch_id 0 63 miss% 0.03157765591979274
plot_id,batch_id 0 64 miss% 0.05600323584809487
plot_id,batch_id 0 65 miss% 0.06973095529274302
plot_id,batch_id 0 66 miss% 0.0254017232780183
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  151697
Epoch:0, Train loss:0.399691, valid loss:0.359399
Epoch:1, Train loss:0.182435, valid loss:0.002041
Epoch:2, Train loss:0.003499, valid loss:0.001463
Epoch:3, Train loss:0.002687, valid loss:0.001257
Epoch:4, Train loss:0.002210, valid loss:0.001028
Epoch:5, Train loss:0.001911, valid loss:0.001036
Epoch:6, Train loss:0.001740, valid loss:0.000901
Epoch:7, Train loss:0.001559, valid loss:0.000893
Epoch:8, Train loss:0.001455, valid loss:0.000758
Epoch:9, Train loss:0.001371, valid loss:0.000861
Epoch:10, Train loss:0.001304, valid loss:0.000779
Epoch:11, Train loss:0.000967, valid loss:0.000581
Epoch:12, Train loss:0.000944, valid loss:0.000592
Epoch:13, Train loss:0.000912, valid loss:0.000546
Epoch:14, Train loss:0.000888, valid loss:0.000558
Epoch:15, Train loss:0.000859, valid loss:0.000541
Epoch:16, Train loss:0.000834, valid loss:0.000582
Epoch:17, Train loss:0.000809, valid loss:0.000584
Epoch:18, Train loss:0.000790, valid loss:0.000481
Epoch:19, Train loss:0.000785, valid loss:0.000594
Epoch:20, Train loss:0.000749, valid loss:0.000511
Epoch:21, Train loss:0.000601, valid loss:0.000450
Epoch:22, Train loss:0.000595, valid loss:0.000466
Epoch:23, Train loss:0.000587, valid loss:0.000501
Epoch:24, Train loss:0.000575, valid loss:0.000468
Epoch:25, Train loss:0.000562, valid loss:0.000459
Epoch:26, Train loss:0.000563, valid loss:0.000445
Epoch:27, Train loss:0.000552, valid loss:0.000492
Epoch:28, Train loss:0.000546, valid loss:0.000433
Epoch:29, Train loss:0.000529, valid loss:0.000444
Epoch:30, Train loss:0.000538, valid loss:0.000431
Epoch:31, Train loss:0.000459, valid loss:0.000405
Epoch:32, Train loss:0.000455, valid loss:0.000414
Epoch:33, Train loss:0.000449, valid loss:0.000432
Epoch:34, Train loss:0.000443, valid loss:0.000400
Epoch:35, Train loss:0.000442, valid loss:0.000406
Epoch:36, Train loss:0.000437, valid loss:0.000434
Epoch:37, Train loss:0.000437, valid loss:0.000402
Epoch:38, Train loss:0.000429, valid loss:0.000416
Epoch:39, Train loss:0.000434, valid loss:0.000411
Epoch:40, Train loss:0.000420, valid loss:0.000402
Epoch:41, Train loss:0.000393, valid loss:0.000385
Epoch:42, Train loss:0.000387, valid loss:0.000392
Epoch:43, Train loss:0.000384, valid loss:0.000462
Epoch:44, Train loss:0.000385, valid loss:0.000417
Epoch:45, Train loss:0.000384, valid loss:0.000392
Epoch:46, Train loss:0.000380, valid loss:0.000385
Epoch:47, Train loss:0.000378, valid loss:0.000386
Epoch:48, Train loss:0.000376, valid loss:0.000402
Epoch:49, Train loss:0.000374, valid loss:0.000386
Epoch:50, Train loss:0.000371, valid loss:0.000395
Epoch:51, Train loss:0.000355, valid loss:0.000405
Epoch:52, Train loss:0.000352, valid loss:0.000390
Epoch:53, Train loss:0.000351, valid loss:0.000395
Epoch:54, Train loss:0.000350, valid loss:0.000395
Epoch:55, Train loss:0.000350, valid loss:0.000397
Epoch:56, Train loss:0.000349, valid loss:0.000390
Epoch:57, Train loss:0.000349, valid loss:0.000388
Epoch:58, Train loss:0.000349, valid loss:0.000401
Epoch:59, Train loss:0.000348, valid loss:0.000399
Epoch:60, Train loss:0.000348, valid loss:0.000389
training time 17607.821801662445
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.06188688907928687
plot_id,batch_id 0 1 miss% 0.03698902334090616
plot_id,batch_id 0 2 miss% 0.09821415679946623
plot_id,batch_id 0 3 miss% 0.04939288476383781
plot_id,batch_id 0 4 miss% 0.03908778923484237
plot_id,batch_id 0 5 miss% 0.043872014543885965
plot_id,batch_id 0 6 miss% 0.04260010330143427
plot_id,batch_id 0 7 miss% 0.09286600868644684
plot_id,batch_id 0 8 miss% 0.07516861951241952
plot_id,batch_id 0 9 miss% 0.031575127939806845
plot_id,batch_id 0 10 miss% 0.045529250033561085
plot_id,batch_id 0 11 miss% 0.04905997227868869
plot_id,batch_id 0 12 miss% 0.05590045047698466
plot_id,batch_id 0 13 miss% 0.03527162120839188
plot_id,batch_id 0 14 miss% 0.06531856441149878
plot_id,batch_id 0 15 miss% 0.03227634168130298
plot_id,batch_id 0 16 miss% 0.07831626315108696
plot_id,batch_id 0 17 miss% 0.023009562551161905
plot_id,batch_id 0 18 miss% 0.07403216943052324
plot_id,batch_id 0 19 miss% 0.07117032209982777
plot_id,batch_id 0 20 miss% 0.10830833718128255
plot_id,batch_id 0 21 miss% 0.027498823663278948
plot_id,batch_id 0 22 miss% 0.029775293174718838
plot_id,batch_id 0 23 miss% 0.03499564916579142
plot_id,batch_id 0 24 miss% 0.032899134289121806
plot_id,batch_id 0 25 miss% 0.04314649203324939
plot_id,batch_id 0 26 miss% 0.04783867653774472
plot_id,batch_id 0 27 miss% 0.07325042960652171
plot_id,batch_id 0 28 miss% 0.03380634367591554
plot_id,batch_id 0 29 miss% 0.02897129174220791
plot_id,batch_id 0 30 miss% 0.031017001261617397
plot_id,batch_id 0 31 miss% 0.10234782061221329
plot_id,batch_id 0 32 miss% 0.11928659536959722
plot_id,batch_id 0 33 miss% 0.05997181132352807
plot_id,batch_id 0 34 miss% 0.06405315188626104
plot_id,batch_id 0 35 miss% 0.022399195271246276
plot_id,batch_id 0 36 miss% 0.0856237987461676
plot_id,batch_id 0 37 miss% 0.08991247783874624
plot_id,batch_id 0 38 miss% 0.0249933618673743
plot_id,batch_id 0 39 miss% 0.03825298457550892
plot_id,batch_id 0 40 miss% 0.10976232289743017
plot_id,batch_id 0 41 miss% 0.028691430172811516
plot_id,batch_id 0 42 miss% 0.11938325673733165
plot_id,batch_id 0 43 miss% 0.06408578292381854
plot_id,batch_id 0 44 miss% 0.026190529113516513
plot_id,batch_id 0 45 miss% 0.09777844078009815
plot_id,batch_id 0 46 miss% 0.03865164958172539
plot_id,batch_id 0 47 miss% 0.02323934840858469
plot_id,batch_id 0 48 miss% 0.023907106438427868
plot_id,batch_id 0 49 miss% 0.031200428672924094
plot_id,batch_id 0 50 miss% 0.1575223155463158
plot_id,batch_id 0 51 miss% 0.0591950792643802
plot_id,batch_id 0 52 miss% 0.025577065739661228
plot_id,batch_id 0 53 miss% 0.031021449241875644
plot_id,batch_id 0 54 miss% 0.05878274311912917
plot_id,batch_id 0 55 miss% 0.06567415471033877
plot_id,batch_id 0 56 miss% 0.0711503624694545
plot_id,batch_id 0 57 miss% 0.019642462951874187
plot_id,batch_id 0 58 miss% 0.026969350493095855
plot_id,batch_id 0 59 miss% 0.033137028541078056
plot_id,batch_id 0 60 miss% 0.04021251840936893
plot_id,batch_id 0 61 miss% 0.06656793872649648
plot_id,batch_id 0 62 miss% 0.04488001710526401
plot_id,batch_id 0 63 miss% 0.04041594559573383
plot_id,batch_id 0 64 miss% 0.07150715144374899
plot_id,batch_id 0 67 miss% 0.02073871696660609
plot_id,batch_id 0 68 miss% 0.02409950235115687
plot_id,batch_id 0 69 miss% 0.06910289628081927
plot_id,batch_id 0 70 miss% 0.09405959250137612
plot_id,batch_id 0 71 miss% 0.0291631159348242
plot_id,batch_id 0 72 miss% 0.08140025795353859
plot_id,batch_id 0 73 miss% 0.036834591984993476
plot_id,batch_id 0 74 miss% 0.08987014046733731
plot_id,batch_id 0 75 miss% 0.053337984427359576
plot_id,batch_id 0 76 miss% 0.06753769436563005
plot_id,batch_id 0 77 miss% 0.05429450950888314
plot_id,batch_id 0 78 miss% 0.04134745468952379
plot_id,batch_id 0 79 miss% 0.06020119823900418
plot_id,batch_id 0 80 miss% 0.03927391842515935
plot_id,batch_id 0 81 miss% 0.08080928799038359
plot_id,batch_id 0 82 miss% 0.04013368582830117
plot_id,batch_id 0 83 miss% 0.0812617756058793
plot_id,batch_id 0 84 miss% 0.05713393377076601
plot_id,batch_id 0 85 miss% 0.04736289368643936
plot_id,batch_id 0 86 miss% 0.052973088907914655
plot_id,batch_id 0 87 miss% 0.06632850886831301
plot_id,batch_id 0 88 miss% 0.07136766162859577
plot_id,batch_id 0 89 miss% 0.07417161388435588
plot_id,batch_id 0 90 miss% 0.03281047485512661
plot_id,batch_id 0 91 miss% 0.06504363248763312
plot_id,batch_id 0 92 miss% 0.025646669915816996
plot_id,batch_id 0 93 miss% 0.037392638233461326
plot_id,batch_id 0 94 miss% 0.05319617466485971
plot_id,batch_id 0 95 miss% 0.04596540427068634
plot_id,batch_id 0 96 miss% 0.04566023163620195
plot_id,batch_id 0 97 miss% 0.03318673246164663
plot_id,batch_id 0 98 miss% 0.036879276338655384
plot_id,batch_id 0 99 miss% 0.07398136356071458
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08551282 0.03800716 0.10358158 0.07161867 0.03896586 0.03508016
 0.04554327 0.07815199 0.08378385 0.09648772 0.03123567 0.08142201
 0.04537269 0.05888818 0.08789016 0.06738769 0.13260126 0.02272794
 0.07626595 0.05801102 0.04723752 0.04809698 0.04880703 0.04075596
 0.05877058 0.05110659 0.03495908 0.04191095 0.02500029 0.03485574
 0.01370403 0.08808337 0.05495159 0.04881474 0.03202729 0.04948786
 0.07089222 0.06503405 0.05376086 0.02864172 0.06713202 0.05630937
 0.01869974 0.06491948 0.01761143 0.06156038 0.03119605 0.02572901
 0.04422046 0.02295941 0.14097876 0.03244169 0.02574332 0.037929
 0.02544563 0.05060948 0.05730375 0.02898595 0.05811838 0.04273841
 0.03341232 0.02638367 0.04416962 0.03157766 0.05600324 0.06973096
 0.02540172 0.02073872 0.0240995  0.0691029  0.09405959 0.02916312
 0.08140026 0.03683459 0.08987014 0.05333798 0.06753769 0.05429451
 0.04134745 0.0602012  0.03927392 0.08080929 0.04013369 0.08126178
 0.05713393 0.04736289 0.05297309 0.06632851 0.07136766 0.07417161
 0.03281047 0.06504363 0.02564667 0.03739264 0.05319617 0.0459654
 0.04566023 0.03318673 0.03687928 0.07398136]
for model  123 the mean error 0.052553115830545755
all id 123 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 25 out win 4 err 0.052553115830545755 time 17580.928480386734
Launcher: Job 124 completed in 17841 seconds.
Launcher: Task 89 done. Exiting.
plot_id,batch_id 0 65 miss% 0.02015213070781524
plot_id,batch_id 0 66 miss% 0.03435445322130991
plot_id,batch_id 0 67 miss% 0.033136659079364784
plot_id,batch_id 0 68 miss% 0.04460185896484031
plot_id,batch_id 0 69 miss% 0.06081887246191023
plot_id,batch_id 0 70 miss% 0.02016228226581675
plot_id,batch_id 0 71 miss% 0.04059901252998028
plot_id,batch_id 0 72 miss% 0.11894719024430334
plot_id,batch_id 0 73 miss% 0.0609994850222742
plot_id,batch_id 0 74 miss% 0.11454662228506234
plot_id,batch_id 0 75 miss% 0.03492695074550137
plot_id,batch_id 0 76 miss% 0.047737153566419076
plot_id,batch_id 0 77 miss% 0.01661800153784228
plot_id,batch_id 0 78 miss% 0.03053955627241823
plot_id,batch_id 0 79 miss% 0.10740165443911158
plot_id,batch_id 0 80 miss% 0.0453510265709172
plot_id,batch_id 0 81 miss% 0.09759539594385716
plot_id,batch_id 0 82 miss% 0.07886339914137531
plot_id,batch_id 0 83 miss% 0.09044921536681605
plot_id,batch_id 0 84 miss% 0.12236321378115271
plot_id,batch_id 0 85 miss% 0.01739076169561383
plot_id,batch_id 0 86 miss% 0.059388556418930004
plot_id,batch_id 0 87 miss% 0.0676541296314243
plot_id,batch_id 0 88 miss% 0.0688779373453489
plot_id,batch_id 0 89 miss% 0.05033274484363689
plot_id,batch_id 0 90 miss% 0.022112867711411303
plot_id,batch_id 0 91 miss% 0.05403071581686509
plot_id,batch_id 0 92 miss% 0.04547694367901584
plot_id,batch_id 0 93 miss% 0.052790464922123
plot_id,batch_id 0 94 miss% 0.08019348365198471
plot_id,batch_id 0 95 miss% 0.047251785363175855
plot_id,batch_id 0 96 miss% 0.031605004048370244
plot_id,batch_id 0 97 miss% 0.052929021160266564
plot_id,batch_id 0 98 miss% 0.03162172713348918
plot_id,batch_id 0 99 miss% 0.07331683936884603
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06188689 0.03698902 0.09821416 0.04939288 0.03908779 0.04387201
 0.0426001  0.09286601 0.07516862 0.03157513 0.04552925 0.04905997
 0.05590045 0.03527162 0.06531856 0.03227634 0.07831626 0.02300956
 0.07403217 0.07117032 0.10830834 0.02749882 0.02977529 0.03499565
 0.03289913 0.04314649 0.04783868 0.07325043 0.03380634 0.02897129
 0.031017   0.10234782 0.1192866  0.05997181 0.06405315 0.0223992
 0.0856238  0.08991248 0.02499336 0.03825298 0.10976232 0.02869143
 0.11938326 0.06408578 0.02619053 0.09777844 0.03865165 0.02323935
 0.02390711 0.03120043 0.15752232 0.05919508 0.02557707 0.03102145
 0.05878274 0.06567415 0.07115036 0.01964246 0.02696935 0.03313703
 0.04021252 0.06656794 0.04488002 0.04041595 0.07150715 0.02015213
 0.03435445 0.03313666 0.04460186 0.06081887 0.02016228 0.04059901
 0.11894719 0.06099949 0.11454662 0.03492695 0.04773715 0.016618
 0.03053956 0.10740165 0.04535103 0.0975954  0.0788634  0.09044922
 0.12236321 0.01739076 0.05938856 0.06765413 0.06887794 0.05033274
 0.02211287 0.05403072 0.04547694 0.05279046 0.08019348 0.04725179
 0.031605   0.05292902 0.03162173 0.07331684]
for model  183 the mean error 0.05550168800370527
all id 183 hidden_dim 24 learning_rate 0.0025 num_layers 5 frames 31 out win 4 err 0.05550168800370527 time 17607.821801662445
Launcher: Job 184 completed in 17865 seconds.
Launcher: Task 206 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  265233
Epoch:0, Train loss:0.646980, valid loss:0.615764
Epoch:1, Train loss:0.307935, valid loss:0.004490
Epoch:2, Train loss:0.009047, valid loss:0.003541
Epoch:3, Train loss:0.007039, valid loss:0.002680
Epoch:4, Train loss:0.005814, valid loss:0.002556
Epoch:5, Train loss:0.004968, valid loss:0.002318
Epoch:6, Train loss:0.004481, valid loss:0.001964
Epoch:7, Train loss:0.003949, valid loss:0.002012
Epoch:8, Train loss:0.003566, valid loss:0.001732
Epoch:9, Train loss:0.003296, valid loss:0.001695
Epoch:10, Train loss:0.003075, valid loss:0.001887
Epoch:11, Train loss:0.002327, valid loss:0.001200
Epoch:12, Train loss:0.002118, valid loss:0.001224
Epoch:13, Train loss:0.002037, valid loss:0.001246
Epoch:14, Train loss:0.002007, valid loss:0.001284
Epoch:15, Train loss:0.001865, valid loss:0.001149
Epoch:16, Train loss:0.001832, valid loss:0.001257
Epoch:17, Train loss:0.001725, valid loss:0.001274
Epoch:18, Train loss:0.001731, valid loss:0.001178
Epoch:19, Train loss:0.001609, valid loss:0.001110
Epoch:20, Train loss:0.001628, valid loss:0.001076
Epoch:21, Train loss:0.001255, valid loss:0.000929
Epoch:22, Train loss:0.001151, valid loss:0.000951
Epoch:23, Train loss:0.001127, valid loss:0.001033
Epoch:24, Train loss:0.001120, valid loss:0.001003
Epoch:25, Train loss:0.001122, valid loss:0.000912
Epoch:26, Train loss:0.001080, valid loss:0.000924
Epoch:27, Train loss:0.001094, valid loss:0.000912
Epoch:28, Train loss:0.001019, valid loss:0.000960
Epoch:29, Train loss:0.001006, valid loss:0.001010
Epoch:30, Train loss:0.000992, valid loss:0.000970
Epoch:31, Train loss:0.000812, valid loss:0.000906
Epoch:32, Train loss:0.000781, valid loss:0.000831
Epoch:33, Train loss:0.000784, valid loss:0.000898
Epoch:34, Train loss:0.000763, valid loss:0.000873
Epoch:35, Train loss:0.000764, valid loss:0.000872
Epoch:36, Train loss:0.000740, valid loss:0.000904
Epoch:37, Train loss:0.000740, valid loss:0.000826
Epoch:38, Train loss:0.000719, valid loss:0.000864
Epoch:39, Train loss:0.000718, valid loss:0.000905
Epoch:40, Train loss:0.000703, valid loss:0.000897
Epoch:41, Train loss:0.000627, valid loss:0.000857
Epoch:42, Train loss:0.000611, valid loss:0.000837
Epoch:43, Train loss:0.000607, valid loss:0.000825
Epoch:44, Train loss:0.000619, valid loss:0.000844
Epoch:45, Train loss:0.000592, valid loss:0.000838
Epoch:46, Train loss:0.000597, valid loss:0.000944
Epoch:47, Train loss:0.000589, valid loss:0.000816
Epoch:48, Train loss:0.000582, valid loss:0.000857
Epoch:49, Train loss:0.000569, valid loss:0.000828
Epoch:50, Train loss:0.000572, valid loss:0.000815
Epoch:51, Train loss:0.000538, valid loss:0.000806
Epoch:52, Train loss:0.000527, valid loss:0.000813
Epoch:53, Train loss:0.000526, valid loss:0.000814
Epoch:54, Train loss:0.000523, valid loss:0.000812
Epoch:55, Train loss:0.000521, valid loss:0.000813
Epoch:56, Train loss:0.000521, valid loss:0.000820
Epoch:57, Train loss:0.000519, valid loss:0.000817
Epoch:58, Train loss:0.000518, valid loss:0.000821
Epoch:59, Train loss:0.000518, valid loss:0.000811
Epoch:60, Train loss:0.000518, valid loss:0.000814
training time 17703.920220136642
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.42982219036393243
plot_id,batch_id 0 1 miss% 0.47471811221312105
plot_id,batch_id 0 2 miss% 0.5469960955003249
plot_id,batch_id 0 3 miss% 0.3701114544338256
plot_id,batch_id 0 4 miss% 0.40533682228609247
plot_id,batch_id 0 5 miss% 0.4916690078843119
plot_id,batch_id 0 6 miss% 0.43053112173907
plot_id,batch_id 0 7 miss% 0.5589027893112227
plot_id,batch_id 0 8 miss% 0.6363954662871918
plot_id,batch_id 0 9 miss% 0.671393083242302
plot_id,batch_id 0 10 miss% 0.43075892724089176
plot_id,batch_id 0 11 miss% 0.45240882727132087
plot_id,batch_id 0 12 miss% 0.4628475182744139
plot_id,batch_id 0 13 miss% 0.4303651746884081
plot_id,batch_id 0 14 miss% 0.5133227466914047
plot_id,batch_id 0 15 miss% 0.5527926886144764
plot_id,batch_id 0 16 miss% 0.59160212305885
plot_id,batch_id 0 17 miss% 0.5512459719485788
plot_id,batch_id 0 18 miss% 0.5821949727444475
plot_id,batch_id 0 19 miss% 0.4798855953368845
plot_id,batch_id 0 20 miss% 0.4037564345256993
plot_id,batch_id 0 21 miss% 0.49335636149871753
plot_id,batch_id 0 22 miss% 0.5132321692833122
plot_id,batch_id 0 23 miss% 0.4939947069459625
plot_id,batch_id 0 24 miss% 0.49338330255379775
plot_id,batch_id 0 25 miss% 0.5714863374159226
plot_id,batch_id 0 26 miss% 0.5253684122740304
plot_id,batch_id 0 27 miss% 0.4538991766308298
plot_id,batch_id 0 28 miss% 0.5467374565066152
plot_id,batch_id 0 29 miss% 0.49227236741706115
plot_id,batch_id 0 30 miss% 0.4855155097176732
plot_id,batch_id 0 31 miss% 0.5609137081572789
plot_id,batch_id 0 32 miss% 0.5187296242600159
plot_id,batch_id 0 33 miss% 0.5254274211042714
plot_id,batch_id 0 34 miss% 0.4355008722979465
plot_id,batch_id 0 35 miss% 0.4721168682463073
plot_id,batch_id 0 36 miss% 0.6106217835328951
plot_id,batch_id 0 37 miss% 0.48934892596800317
plot_id,batch_id 0 38 miss% 0.5493030302123558
plot_id,batch_id 0 39 miss% 0.6108988914800919
plot_id,batch_id 0 40 miss% 0.4403293580202686
plot_id,batch_id 0 41 miss% 0.47684480517531647
plot_id,batch_id 0 42 miss% 0.44144677692854756
plot_id,batch_id 0 43 miss% 0.4502477630639943
plot_id,batch_id 0 44 miss% 0.45852655125089387
plot_id,batch_id 0 45 miss% 0.4272461122978402
plot_id,batch_id 0 46 miss% 0.4437441609976263
plot_id,batch_id 0 47 miss% 0.507462667004296
plot_id,batch_id 0 48 miss% 0.5518846602497968
plot_id,batch_id 0 49 miss% 0.4609908204558098
plot_id,batch_id 0 50 miss% 0.5784711564790429
plot_id,batch_id 0 51 miss% 0.5265316714994285
plot_id,batch_id 0 52 miss% 0.5094699404219146
plot_id,batch_id 0 53 miss% 0.4750885135337608
plot_id,batch_id 0 54 miss% 0.5378654116222813
plot_id,batch_id 0 55 miss% 0.5078088906961951
plot_id,batch_id 0 56 miss% 0.6151673073038844
plot_id,batch_id 0 57 miss% 0.5604769157367385
plot_id,batch_id 0 58 miss% 0.5279680191164271
plot_id,batch_id 0 59 miss% 0.54811367913966
plot_id,batch_id 0 60 miss% 0.4167820673561903
plot_id,batch_id 0 61 miss% 0.3972086129575533
plot_id,batch_id 0 62 miss% 0.5609535191640682
plot_id,batch_id 0 63 miss% 0.4821608913065751
plot_id,batch_id 0 64 miss% 0.4722025439182952
plot_id,batch_id 0 65 miss% 0.4285486171320019
plot_id,batch_id 0 66 miss% 0.5101002578393571
plot_id,batch_id 0 67 miss% 0.4443684120382402
plot_id,batch_id 0 68 miss% 0.543777551565692
plot_id,batch_id 0 69 miss% 0.5209972302753108
plot_id,batch_id 0 70 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  154129
Epoch:0, Train loss:0.406906, valid loss:0.413894
Epoch:1, Train loss:0.053416, valid loss:0.003928
Epoch:2, Train loss:0.008996, valid loss:0.001997
Epoch:3, Train loss:0.005560, valid loss:0.001894
Epoch:4, Train loss:0.005031, valid loss:0.001615
Epoch:5, Train loss:0.004698, valid loss:0.001290
Epoch:6, Train loss:0.003505, valid loss:0.001124
Epoch:7, Train loss:0.003271, valid loss:0.000942
Epoch:8, Train loss:0.001778, valid loss:0.001055
Epoch:9, Train loss:0.001634, valid loss:0.000862
Epoch:10, Train loss:0.001512, valid loss:0.000778
Epoch:11, Train loss:0.001170, valid loss:0.000751
Epoch:12, Train loss:0.001133, valid loss:0.000828
Epoch:13, Train loss:0.001087, valid loss:0.000681
Epoch:14, Train loss:0.001050, valid loss:0.000628
Epoch:15, Train loss:0.001010, valid loss:0.000602
Epoch:16, Train loss:0.000975, valid loss:0.000651
Epoch:17, Train loss:0.000958, valid loss:0.000662
Epoch:18, Train loss:0.000922, valid loss:0.000656
Epoch:19, Train loss:0.000904, valid loss:0.000586
Epoch:20, Train loss:0.000855, valid loss:0.000620
Epoch:21, Train loss:0.000700, valid loss:0.000582
Epoch:22, Train loss:0.000674, valid loss:0.000514
Epoch:23, Train loss:0.000669, valid loss:0.000510
Epoch:24, Train loss:0.000664, valid loss:0.000501
Epoch:25, Train loss:0.000645, valid loss:0.000517
Epoch:26, Train loss:0.000635, valid loss:0.000523
Epoch:27, Train loss:0.000619, valid loss:0.000529
Epoch:28, Train loss:0.000610, valid loss:0.000585
Epoch:29, Train loss:0.000606, valid loss:0.000533
Epoch:30, Train loss:0.000604, valid loss:0.000533
Epoch:31, Train loss:0.000507, valid loss:0.000492
Epoch:32, Train loss:0.000495, valid loss:0.000476
Epoch:33, Train loss:0.000501, valid loss:0.000489
Epoch:34, Train loss:0.000485, valid loss:0.000474
Epoch:35, Train loss:0.000489, valid loss:0.000466
Epoch:36, Train loss:0.000481, valid loss:0.000483
Epoch:37, Train loss:0.000478, valid loss:0.000489
Epoch:38, Train loss:0.000471, valid loss:0.000457
Epoch:39, Train loss:0.000469, valid loss:0.000478
Epoch:40, Train loss:0.000470, valid loss:0.000467
Epoch:41, Train loss:0.000420, valid loss:0.000458
Epoch:42, Train loss:0.000414, valid loss:0.000459
Epoch:43, Train loss:0.000413, valid loss:0.000494
Epoch:44, Train loss:0.000413, valid loss:0.000465
Epoch:45, Train loss:0.000410, valid loss:0.000477
Epoch:46, Train loss:0.000406, valid loss:0.000479
Epoch:47, Train loss:0.000409, valid loss:0.000454
Epoch:48, Train loss:0.000399, valid loss:0.000458
Epoch:49, Train loss:0.000399, valid loss:0.000449
Epoch:50, Train loss:0.000399, valid loss:0.000480
Epoch:51, Train loss:0.000380, valid loss:0.000450
Epoch:52, Train loss:0.000373, valid loss:0.000450
Epoch:53, Train loss:0.000372, valid loss:0.000446
Epoch:54, Train loss:0.000371, valid loss:0.000446
Epoch:55, Train loss:0.000370, valid loss:0.000446
Epoch:56, Train loss:0.000370, valid loss:0.000445
Epoch:57, Train loss:0.000369, valid loss:0.000445
Epoch:58, Train loss:0.000369, valid loss:0.000444
Epoch:59, Train loss:0.000368, valid loss:0.000444
Epoch:60, Train loss:0.000368, valid loss:0.000446
training time 17700.390602588654
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.09486034520269145
plot_id,batch_id 0 1 miss% 0.05989112416642078
plot_id,batch_id 0 2 miss% 0.09107964290414168
plot_id,batch_id 0 3 miss% 0.04650666030682319
plot_id,batch_id 0 4 miss% 0.05167381452581937
plot_id,batch_id 0 5 miss% 0.04119958903340182
plot_id,batch_id 0 6 miss% 0.035717809301500494
plot_id,batch_id 0 7 miss% 0.08974841420613558
plot_id,batch_id 0 8 miss% 0.04961373824423934
plot_id,batch_id 0 9 miss% 0.06614130612833874
plot_id,batch_id 0 10 miss% 0.06173635954690949
plot_id,batch_id 0 11 miss% 0.05775740268045081
plot_id,batch_id 0 12 miss% 0.08061710541596455
plot_id,batch_id 0 13 miss% 0.03964581859757963
plot_id,batch_id 0 14 miss% 0.10618115940860229
plot_id,batch_id 0 15 miss% 0.056031949017087926
plot_id,batch_id 0 16 miss% 0.13497424509396105
plot_id,batch_id 0 17 miss% 0.05223673912777356
plot_id,batch_id 0 18 miss% 0.0829935482395839
plot_id,batch_id 0 19 miss% 0.08416837324591273
plot_id,batch_id 0 20 miss% 0.054448128392534316
plot_id,batch_id 0 21 miss% 0.04432974468492168
plot_id,batch_id 0 22 miss% 0.026622861516134244
plot_id,batch_id 0 23 miss% 0.05728222882938505
plot_id,batch_id 0 24 miss% 0.06414275974809817
plot_id,batch_id 0 25 miss% 0.030977503222109386
plot_id,batch_id 0 26 miss% 0.039264377680397304
plot_id,batch_id 0 27 miss% 0.048864429301071335
plot_id,batch_id 0 28 miss% 0.026202796884711936
plot_id,batch_id 0 29 miss% 0.0706991119657316
plot_id,batch_id 0 30 miss% 0.049838097649730795
plot_id,batch_id 0 31 miss% 0.12678063625698702
plot_id,batch_id 0 32 miss% 0.093296729331707
plot_id,batch_id 0 33 miss% 0.09164800804946718
plot_id,batch_id 0 34 miss% 0.0668272746977107
plot_id,batch_id 0 35 miss% 0.032183090530985996
plot_id,batch_id 0 36 miss% 0.11593919116828208
plot_id,batch_id 0 37 miss% 0.08052141001627658
plot_id,batch_id 0 38 miss% 0.09968659468246323
plot_id,batch_id 0 39 miss% 0.10672774737291582
plot_id,batch_id 0 40 miss% 0.07388419651839077
plot_id,batch_id 0 41 miss% 0.04566168534878723
plot_id,batch_id 0 42 miss% 0.019111178651021826
plot_id,batch_id 0 43 miss% 0.06096306299539599
plot_id,batch_id 0 44 miss% 0.02908176028322438
plot_id,batch_id 0 45 miss% 0.07730406559762915
plot_id,batch_id 0 46 miss% 0.06223883902100248
plot_id,batch_id 0 47 miss% 0.03165462629693266
plot_id,batch_id 0 48 miss% 0.04483533713664549
plot_id,batch_id 0 49 miss% 0.05440803720426807
plot_id,batch_id 0 50 miss% 0.09470593746818189
plot_id,batch_id 0 51 miss% 0.023710217021753632
plot_id,batch_id 0 52 miss% 0.04735230574414367
plot_id,batch_id 0 53 miss% 0.021297381626966715
plot_id,batch_id 0 54 miss% 0.034428017009330354
plot_id,batch_id 0 55 miss% 0.07396927488367656
plot_id,batch_id 0 56 miss% 0.114682627018538
plot_id,batch_id 0 57 miss% 0.060007530326705784
plot_id,batch_id 0 58 miss% 0.0603616930317784
plot_id,batch_id 0 59 miss% 0.05127061043344377
plot_id,batch_id 0 60 miss% 0.023982784609425163
plot_id,batch_id 0 61 miss% 0.02734139892236943
plot_id,batch_id 0 62 miss% 0.04675674811020877
plot_id,batch_id 0 63 miss% 0.06458072027975365
plot_id,batch_id 0 64 miss% 0.032650988817756764
plot_id,batch_id 0 65 miss% 0.058737757981333134
plot_id,batch_id 0 66 miss% 0.14210831807985772
plot_id,batch_id 0 67 miss% 0.3235777150987869
plot_id,batch_id 0 71 miss% 0.534495370458297
plot_id,batch_id 0 72 miss% 0.5038325316838485
plot_id,batch_id 0 73 miss% 0.5160467534731825
plot_id,batch_id 0 74 miss% 0.47796783025607636
plot_id,batch_id 0 75 miss% 0.39219646164708033
plot_id,batch_id 0 76 miss% 0.4961406800520366
plot_id,batch_id 0 77 miss% 0.45371378171427373
plot_id,batch_id 0 78 miss% 0.44038326829940144
plot_id,batch_id 0 79 miss% 0.4815571236282916
plot_id,batch_id 0 80 miss% 0.44213522959924656
plot_id,batch_id 0 81 miss% 0.5668052021305157
plot_id,batch_id 0 82 miss% 0.48962854469457207
plot_id,batch_id 0 83 miss% 0.4985126230554681
plot_id,batch_id 0 84 miss% 0.4969262201930576
plot_id,batch_id 0 85 miss% 0.43500688618074074
plot_id,batch_id 0 86 miss% 0.43511927099906283
plot_id,batch_id 0 87 miss% 0.5455687738094736
plot_id,batch_id 0 88 miss% 0.5329559632440988
plot_id,batch_id 0 89 miss% 0.4799219284579361
plot_id,batch_id 0 90 miss% 0.35268982400687254
plot_id,batch_id 0 91 miss% 0.4469018567235475
plot_id,batch_id 0 92 miss% 0.4509068319621918
plot_id,batch_id 0 93 miss% 0.3741760602322711
plot_id,batch_id 0 94 miss% 0.5958576352085964
plot_id,batch_id 0 95 miss% 0.39023762578978866
plot_id,batch_id 0 96 miss% 0.46499939093992143
plot_id,batch_id 0 97 miss% 0.5516132625151726
plot_id,batch_id 0 98 miss% 0.518013743544212
plot_id,batch_id 0 99 miss% 0.4374421636983079
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.42982219 0.47471811 0.5469961  0.37011145 0.40533682 0.49166901
 0.43053112 0.55890279 0.63639547 0.67139308 0.43075893 0.45240883
 0.46284752 0.43036517 0.51332275 0.55279269 0.59160212 0.55124597
 0.58219497 0.4798856  0.40375643 0.49335636 0.51323217 0.49399471
 0.4933833  0.57148634 0.52536841 0.45389918 0.54673746 0.49227237
 0.48551551 0.56091371 0.51872962 0.52542742 0.43550087 0.47211687
 0.61062178 0.48934893 0.54930303 0.61089889 0.44032936 0.47684481
 0.44144678 0.45024776 0.45852655 0.42724611 0.44374416 0.50746267
 0.55188466 0.46099082 0.57847116 0.52653167 0.50946994 0.47508851
 0.53786541 0.50780889 0.61516731 0.56047692 0.52796802 0.54811368
 0.41678207 0.39720861 0.56095352 0.48216089 0.47220254 0.42854862
 0.51010026 0.44436841 0.54377755 0.52099723 0.32357772 0.53449537
 0.50383253 0.51604675 0.47796783 0.39219646 0.49614068 0.45371378
 0.44038327 0.48155712 0.44213523 0.5668052  0.48962854 0.49851262
 0.49692622 0.43500689 0.43511927 0.54556877 0.53295596 0.47992193
 0.35268982 0.44690186 0.45090683 0.37417606 0.59585764 0.39023763
 0.46499939 0.55161326 0.51801374 0.43744216]
for model  26 the mean error 0.49257279487003197
all id 26 hidden_dim 32 learning_rate 0.0025 num_layers 5 frames 21 out win 6 err 0.49257279487003197 time 17703.920220136642
Launcher: Job 27 completed in 17925 seconds.
Launcher: Task 69 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  265233
Epoch:0, Train loss:0.501245, valid loss:0.476277
Epoch:1, Train loss:0.257487, valid loss:0.013385
Epoch:2, Train loss:0.031390, valid loss:0.013933
Epoch:3, Train loss:0.031370, valid loss:0.015027
Epoch:4, Train loss:0.030608, valid loss:0.011169
Epoch:5, Train loss:0.026137, valid loss:0.006169
Epoch:6, Train loss:0.014734, valid loss:0.004631
Epoch:7, Train loss:0.007887, valid loss:0.003142
Epoch:8, Train loss:0.006311, valid loss:0.002702
Epoch:9, Train loss:0.005786, valid loss:0.002270
Epoch:10, Train loss:0.005375, valid loss:0.002256
Epoch:11, Train loss:0.004452, valid loss:0.001882
Epoch:12, Train loss:0.004250, valid loss:0.001804
Epoch:13, Train loss:0.004087, valid loss:0.001781
Epoch:14, Train loss:0.003825, valid loss:0.001576
Epoch:15, Train loss:0.003568, valid loss:0.001624
Epoch:16, Train loss:0.003367, valid loss:0.001643
Epoch:17, Train loss:0.003111, valid loss:0.001633
Epoch:18, Train loss:0.002833, valid loss:0.001246
Epoch:19, Train loss:0.002674, valid loss:0.001225
Epoch:20, Train loss:0.002529, valid loss:0.001282
Epoch:21, Train loss:0.002119, valid loss:0.001124
Epoch:22, Train loss:0.002068, valid loss:0.001033
Epoch:23, Train loss:0.001982, valid loss:0.000904
Epoch:24, Train loss:0.001949, valid loss:0.001000
Epoch:25, Train loss:0.001901, valid loss:0.001112
Epoch:26, Train loss:0.001801, valid loss:0.000915
Epoch:27, Train loss:0.001762, valid loss:0.000851
Epoch:28, Train loss:0.001708, valid loss:0.000930
Epoch:29, Train loss:0.001621, valid loss:0.000853
Epoch:30, Train loss:0.001590, valid loss:0.000820
Epoch:31, Train loss:0.001325, valid loss:0.000796
Epoch:32, Train loss:0.001296, valid loss:0.000787
Epoch:33, Train loss:0.001279, valid loss:0.000777
Epoch:34, Train loss:0.001259, valid loss:0.000762
Epoch:35, Train loss:0.001237, valid loss:0.000751
Epoch:36, Train loss:0.001211, valid loss:0.000752
Epoch:37, Train loss:0.001196, valid loss:0.000714
Epoch:38, Train loss:0.001171, valid loss:0.000736
Epoch:39, Train loss:0.001154, valid loss:0.000759
Epoch:40, Train loss:0.001133, valid loss:0.000738
Epoch:41, Train loss:0.001012, valid loss:0.000715
Epoch:42, Train loss:0.000994, valid loss:0.000703
Epoch:43, Train loss:0.000991, valid loss:0.000708
Epoch:44, Train loss:0.000972, valid loss:0.000692
Epoch:45, Train loss:0.000967, valid loss:0.000689
Epoch:46, Train loss:0.000958, valid loss:0.000681
Epoch:47, Train loss:0.000947, valid loss:0.000711
Epoch:48, Train loss:0.000938, valid loss:0.000692
Epoch:49, Train loss:0.000928, valid loss:0.000694
Epoch:50, Train loss:0.000914, valid loss:0.000692
Epoch:51, Train loss:0.000857, valid loss:0.000661
Epoch:52, Train loss:0.000849, valid loss:0.000663
Epoch:53, Train loss:0.000847, valid loss:0.000659
Epoch:54, Train loss:0.000844, valid loss:0.000655
Epoch:55, Train loss:0.000842, valid loss:0.000655
Epoch:56, Train loss:0.000841, valid loss:0.000656
Epoch:57, Train loss:0.000840, valid loss:0.000656
Epoch:58, Train loss:0.000839, valid loss:0.000653
Epoch:59, Train loss:0.000838, valid loss:0.000653
Epoch:60, Train loss:0.000837, valid loss:0.000652
training time 17736.39457631111
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.33745532071609496
plot_id,batch_id 0 1 miss% 0.6570734546867864
plot_id,batch_id 0 2 miss% 0.5216475961180053
plot_id,batch_id 0 3 miss% 0.6960343908592174
plot_id,batch_id 0 4 miss% 0.4333701926868312
plot_id,batch_id 0 5 miss% 0.5300766657303204
plot_id,batch_id 0 6 miss% 0.5926439254013985
plot_id,batch_id 0 7 miss% 0.42229804796119297
plot_id,batch_id 0 8 miss% 0.46724299712929623
plot_id,batch_id 0 9 miss% 0.7482002151663094
plot_id,batch_id 0 10 miss% 0.7490739038582421
plot_id,batch_id 0 11 miss% 0.7435350485560487
plot_id,batch_id 0 12 miss% 0.38628648133492816
plot_id,batch_id 0 13 miss% 0.8653006565902324
plot_id,batch_id 0 14 miss% 0.5765609152442507
plot_id,batch_id 0 15 miss% 0.2904072725119266
plot_id,batch_id 0 16 miss% 0.5295605652311061
plot_id,batch_id 0 17 miss% 0.5270886097268929
plot_id,batch_id 0 18 miss% 0.48507790848718935
plot_id,batch_id 0 19 miss% 0.5067606715351867
plot_id,batch_id 0 20 miss% 0.5552564146582115
plot_id,batch_id 0 21 miss% 0.7397937635402586
plot_id,batch_id 0 22 miss% 0.6635846324672494
plot_id,batch_id 0 23 miss% 0.6608187716541833
plot_id,batch_id 0 24 miss% 0.5332070436146937
plot_id,batch_id 0 25 miss% 0.5597327345713614
plot_id,batch_id 0 26 miss% 0.5482086491686629
plot_id,batch_id 0 27 miss% 0.6531733433477641
plot_id,batch_id 0 28 miss% 0.7306528876875908
plot_id,batch_id 0 29 miss% 0.5166378009168198
plot_id,batch_id 0 30 miss% 0.37056196873902314
plot_id,batch_id 0 31 miss% 0.571118972701685
plot_id,batch_id 0 32 miss% 0.6317141064216387
plot_id,batch_id 0 33 miss% 0.6337364096925516
plot_id,batch_id 0 34 miss% 0.5449737615215333
plot_id,batch_id 0 35 miss% 0.33764103375792576
plot_id,batch_id 0 36 miss% 0.4632721644043508
plot_id,batch_id 0 37 miss% 0.46283032660636303
plot_id,batch_id 0 38 miss% 0.6859941441500749
plot_id,batch_id 0 39 miss% 0.8412939258685185
plot_id,batch_id 0 40 miss% 0.5264951660608191
plot_id,batch_id 0 41 miss% 0.5536666270553101
plot_id,batch_id 0 42 miss% 0.7954752706184346
plot_id,batch_id 0 43 miss% 0.4633204669494154
plot_id,batch_id 0 44 miss% 0.6402605572055797
plot_id,batch_id 0 45 miss% 0.85062062322062
plot_id,batch_id 0 46 miss% 0.4304083196141077
plot_id,batch_id 0 47 miss% 0.6659268082262289
plot_id,batch_id 0 48 miss% 0.7486256429945958
plot_id,batch_id 0 49 miss% 0.8202525050404633
plot_id,batch_id 0 50 miss% 0.4941394768863674
plot_id,batch_id 0 51 miss% 0.6570205568854898
plot_id,batch_id 0 52 miss% 0.5213725521431373
plot_id,batch_id 0 53 miss% 0.8924189202129243
plot_id,batch_id 0 54 miss% 0.7370233507554097
plot_id,batch_id 0 55 miss% 0.7385601633766001
plot_id,batch_id 0 56 miss% 0.6004340804073549
plot_id,batch_id 0 57 miss% 0.7776859083600702
plot_id,batch_id 0 58 miss% 0.5169977321481445
plot_id,batch_id 0 59 miss% 0.4513837129617732
plot_id,batch_id 0 60 miss% 0.3820498232919479
plot_id,batch_id 0 61 miss% 0.3281569472517032
plot_id,batch_id 0 62 miss% 0.4014526237656479
plot_id,batch_id 0 63 miss% 0.6922846849546191
plot_id,batch_id 0 64 miss% 0.4548564966296251
plot_id,batch_id 0 65 miss% 0.45989576294829215
plot_id,batch_id 0 66 miss% 0.7585377682948237
plot_id,batch_id 0 67 miss% 0.7500258089023776
plot_id,batch_id 0 68 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  209681
Epoch:0, Train loss:0.459966, valid loss:0.469450
Epoch:1, Train loss:0.031372, valid loss:0.004341
Epoch:2, Train loss:0.009210, valid loss:0.002816
Epoch:3, Train loss:0.005646, valid loss:0.002366
Epoch:4, Train loss:0.003461, valid loss:0.001577
Epoch:5, Train loss:0.003001, valid loss:0.001537
Epoch:6, Train loss:0.002730, valid loss:0.001090
Epoch:7, Train loss:0.002449, valid loss:0.001270
Epoch:8, Train loss:0.002305, valid loss:0.001568
Epoch:9, Train loss:0.002234, valid loss:0.001391
Epoch:10, Train loss:0.002061, valid loss:0.001166
Epoch:11, Train loss:0.001442, valid loss:0.000826
Epoch:12, Train loss:0.001349, valid loss:0.000756
Epoch:13, Train loss:0.001333, valid loss:0.000795
Epoch:14, Train loss:0.001290, valid loss:0.000714
Epoch:15, Train loss:0.001253, valid loss:0.000861
Epoch:16, Train loss:0.001222, valid loss:0.000838
Epoch:17, Train loss:0.001179, valid loss:0.000807
Epoch:18, Train loss:0.001151, valid loss:0.000961
Epoch:19, Train loss:0.001083, valid loss:0.000636
Epoch:20, Train loss:0.001071, valid loss:0.000712
Epoch:21, Train loss:0.000768, valid loss:0.000541
Epoch:22, Train loss:0.000736, valid loss:0.000560
Epoch:23, Train loss:0.000733, valid loss:0.000599
Epoch:24, Train loss:0.000690, valid loss:0.000542
Epoch:25, Train loss:0.000715, valid loss:0.000550
Epoch:26, Train loss:0.000707, valid loss:0.000536
Epoch:27, Train loss:0.000658, valid loss:0.000594
Epoch:28, Train loss:0.000623, valid loss:0.000530
Epoch:29, Train loss:0.000647, valid loss:0.000598
Epoch:30, Train loss:0.000644, valid loss:0.000612
Epoch:31, Train loss:0.000490, valid loss:0.000516
Epoch:32, Train loss:0.000478, valid loss:0.000484
Epoch:33, Train loss:0.000489, valid loss:0.000499
Epoch:34, Train loss:0.000471, valid loss:0.000509
Epoch:35, Train loss:0.000466, valid loss:0.000494
Epoch:36, Train loss:0.000459, valid loss:0.000479
Epoch:37, Train loss:0.000456, valid loss:0.000525
Epoch:38, Train loss:0.000464, valid loss:0.000506
Epoch:39, Train loss:0.000454, valid loss:0.000489
Epoch:40, Train loss:0.000442, valid loss:0.000520
Epoch:41, Train loss:0.000384, valid loss:0.000475
Epoch:42, Train loss:0.000374, valid loss:0.000490
Epoch:43, Train loss:0.000378, valid loss:0.000483
Epoch:44, Train loss:0.000369, valid loss:0.000492
Epoch:45, Train loss:0.000373, valid loss:0.000479
Epoch:46, Train loss:0.000365, valid loss:0.000477
Epoch:47, Train loss:0.000367, valid loss:0.000501
Epoch:48, Train loss:0.000361, valid loss:0.000496
Epoch:49, Train loss:0.000354, valid loss:0.000496
Epoch:50, Train loss:0.000361, valid loss:0.000472
Epoch:51, Train loss:0.000335, valid loss:0.000466
Epoch:52, Train loss:0.000332, valid loss:0.000468
Epoch:53, Train loss:0.000331, valid loss:0.000469
Epoch:54, Train loss:0.000331, valid loss:0.000468
Epoch:55, Train loss:0.000330, valid loss:0.000468
Epoch:56, Train loss:0.000330, valid loss:0.000468
Epoch:57, Train loss:0.000329, valid loss:0.000468
Epoch:58, Train loss:0.000329, valid loss:0.000468
Epoch:59, Train loss:0.000329, valid loss:0.000467
Epoch:60, Train loss:0.000329, valid loss:0.000468
training time 17730.4966442585
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.06480915497590811
plot_id,batch_id 0 1 miss% 0.03622115838521167
plot_id,batch_id 0 2 miss% 0.07758404654794306
plot_id,batch_id 0 3 miss% 0.03469595266362453
plot_id,batch_id 0 4 miss% 0.04081648168553125
plot_id,batch_id 0 5 miss% 0.0485609931771813
plot_id,batch_id 0 6 miss% 0.06538676485552078
plot_id,batch_id 0 7 miss% 0.06580644074756932
plot_id,batch_id 0 8 miss% 0.03887416893603931
plot_id,batch_id 0 9 miss% 0.02041353539129527
plot_id,batch_id 0 10 miss% 0.04823436268893824
plot_id,batch_id 0 11 miss% 0.0748575238926764
plot_id,batch_id 0 12 miss% 0.05241189490430884
plot_id,batch_id 0 13 miss% 0.08114392595470447
plot_id,batch_id 0 14 miss% 0.06937270059028758
plot_id,batch_id 0 15 miss% 0.08398779039699628
plot_id,batch_id 0 16 miss% 0.12283170664458044
plot_id,batch_id 0 17 miss% 0.05645021760449316
plot_id,batch_id 0 18 miss% 0.04770574922189365
plot_id,batch_id 0 19 miss% 0.05143594449508946
plot_id,batch_id 0 20 miss% 0.2756713696437165
plot_id,batch_id 0 21 miss% 0.03998823673660539
plot_id,batch_id 0 22 miss% 0.03105579243735575
plot_id,batch_id 0 23 miss% 0.03337578992314949
plot_id,batch_id 0 24 miss% 0.05035862345661002
plot_id,batch_id 0 25 miss% 0.0457153759737706
plot_id,batch_id 0 26 miss% 0.035050186336359763
plot_id,batch_id 0 27 miss% 0.052575203286777214
plot_id,batch_id 0 28 miss% 0.03787240636263646
plot_id,batch_id 0 29 miss% 0.024649265977000824
plot_id,batch_id 0 30 miss% 0.055484059552681084
plot_id,batch_id 0 31 miss% 0.06973754126125424
plot_id,batch_id 0 32 miss% 0.06418322877952985
plot_id,batch_id 0 33 miss% 0.04042688027549388
plot_id,batch_id 0 34 miss% 0.030246819552946256
plot_id,batch_id 0 35 miss% 0.03554620661718566
plot_id,batch_id 0 36 miss% 0.06798509505335833
plot_id,batch_id 0 37 miss% 0.07448326474599297
plot_id,batch_id 0 38 miss% 0.07299599482684253
plot_id,batch_id 0 39 miss% 0.04128215387970096
plot_id,batch_id 0 40 miss% 0.05601681433419119
plot_id,batch_id 0 41 miss% 0.05391784619763842
plot_id,batch_id 0 42 miss% 0.02873288566098214
plot_id,batch_id 0 43 miss% 0.027793088380007595
plot_id,batch_id 0 44 miss% 0.02779413705723086
plot_id,batch_id 0 45 miss% 0.04686797175792467
plot_id,batch_id 0 46 miss% 0.028770554205180935
plot_id,batch_id 0 47 miss% 0.028283502654990533
plot_id,batch_id 0 48 miss% 0.03786531974924887
plot_id,batch_id 0 49 miss% 0.017123171180784106
plot_id,batch_id 0 50 miss% 0.12417958699577226
plot_id,batch_id 0 51 miss% 0.030891324732732428
plot_id,batch_id 0 52 miss% 0.018078442787591444
plot_id,batch_id 0 53 miss% 0.022923770827668985
plot_id,batch_id 0 54 miss% 0.021554853197484306
plot_id,batch_id 0 55 miss% 0.10466441188724833
plot_id,batch_id 0 56 miss% 0.05337471067013772
plot_id,batch_id 0 57 miss% 0.0426324491881654
plot_id,batch_id 0 58 miss% 0.0376773141419037
plot_id,batch_id 0 59 miss% 0.03101024098105905
plot_id,batch_id 0 60 miss% 0.04396193102129589
plot_id,batch_id 0 61 miss% 0.045276557152770715
plot_id,batch_id 0 62 miss% 0.0480932299978702
plot_id,batch_id 0 63 miss% 0.05787666848620131
plot_id,batch_id 0 64 miss% 0.0803111228950622
plot_id,batch_id 0 65 miss% 0.04680610857431367
plot_id,batch_id 0 66 miss% 0.12351425627432205
plot_id,batch_id 00.04766503110148035
plot_id,batch_id 0 68 miss% 0.036342386803720574
plot_id,batch_id 0 69 miss% 0.08583286529615913
plot_id,batch_id 0 70 miss% 0.052736996899724366
plot_id,batch_id 0 71 miss% 0.020911011127837817
plot_id,batch_id 0 72 miss% 0.09724429259900733
plot_id,batch_id 0 73 miss% 0.04184433488592498
plot_id,batch_id 0 74 miss% 0.1011817768859268
plot_id,batch_id 0 75 miss% 0.09021282999990343
plot_id,batch_id 0 76 miss% 0.1251542609086084
plot_id,batch_id 0 77 miss% 0.06252084758719482
plot_id,batch_id 0 78 miss% 0.0512650286320727
plot_id,batch_id 0 79 miss% 0.11464324812277055
plot_id,batch_id 0 80 miss% 0.04224667544418947
plot_id,batch_id 0 81 miss% 0.09522153239132201
plot_id,batch_id 0 82 miss% 0.07566263435302109
plot_id,batch_id 0 83 miss% 0.07586323554106028
plot_id,batch_id 0 84 miss% 0.06305815275414388
plot_id,batch_id 0 85 miss% 0.022788359563067677
plot_id,batch_id 0 86 miss% 0.06903772663028152
plot_id,batch_id 0 87 miss% 0.06349954657255152
plot_id,batch_id 0 88 miss% 0.0750190665612881
plot_id,batch_id 0 89 miss% 0.06726008089008263
plot_id,batch_id 0 90 miss% 0.06170894476182926
plot_id,batch_id 0 91 miss% 0.05325412412274803
plot_id,batch_id 0 92 miss% 0.028421040620022764
plot_id,batch_id 0 93 miss% 0.06836876929550122
plot_id,batch_id 0 94 miss% 0.07058621679775129
plot_id,batch_id 0 95 miss% 0.04662692253941545
plot_id,batch_id 0 96 miss% 0.03772183888008089
plot_id,batch_id 0 97 miss% 0.07395741965036005
plot_id,batch_id 0 98 miss% 0.07647421504307712
plot_id,batch_id 0 99 miss% 0.032074844214106016
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09486035 0.05989112 0.09107964 0.04650666 0.05167381 0.04119959
 0.03571781 0.08974841 0.04961374 0.06614131 0.06173636 0.0577574
 0.08061711 0.03964582 0.10618116 0.05603195 0.13497425 0.05223674
 0.08299355 0.08416837 0.05444813 0.04432974 0.02662286 0.05728223
 0.06414276 0.0309775  0.03926438 0.04886443 0.0262028  0.07069911
 0.0498381  0.12678064 0.09329673 0.09164801 0.06682727 0.03218309
 0.11593919 0.08052141 0.09968659 0.10672775 0.0738842  0.04566169
 0.01911118 0.06096306 0.02908176 0.07730407 0.06223884 0.03165463
 0.04483534 0.05440804 0.09470594 0.02371022 0.04735231 0.02129738
 0.03442802 0.07396927 0.11468263 0.06000753 0.06036169 0.05127061
 0.02398278 0.0273414  0.04675675 0.06458072 0.03265099 0.05873776
 0.14210832 0.04766503 0.03634239 0.08583287 0.052737   0.02091101
 0.09724429 0.04184433 0.10118178 0.09021283 0.12515426 0.06252085
 0.05126503 0.11464325 0.04224668 0.09522153 0.07566263 0.07586324
 0.06305815 0.02278836 0.06903773 0.06349955 0.07501907 0.06726008
 0.06170894 0.05325412 0.02842104 0.06836877 0.07058622 0.04662692
 0.03772184 0.07395742 0.07647422 0.03207484]
for model  170 the mean error 0.06312551224269712
all id 170 hidden_dim 32 learning_rate 0.0025 num_layers 3 frames 31 out win 6 err 0.06312551224269712 time 17700.390602588654
Launcher: Job 171 completed in 17959 seconds.
Launcher: Task 246 done. Exiting.
0.3342735499059728
plot_id,batch_id 0 69 miss% 0.6293953767529188
plot_id,batch_id 0 70 miss% 0.5753730265657763
plot_id,batch_id 0 71 miss% 0.40271758385276396
plot_id,batch_id 0 72 miss% 0.6446094544392876
plot_id,batch_id 0 73 miss% 0.3821187070340467
plot_id,batch_id 0 74 miss% 0.7258730341801889
plot_id,batch_id 0 75 miss% 0.27892923729090824
plot_id,batch_id 0 76 miss% 0.3970961877271932
plot_id,batch_id 0 77 miss% 0.41932613808982905
plot_id,batch_id 0 78 miss% 0.7011611778128237
plot_id,batch_id 0 79 miss% 0.32006841260448854
plot_id,batch_id 0 80 miss% 0.2957564764232678
plot_id,batch_id 0 81 miss% 0.5579047250422281
plot_id,batch_id 0 82 miss% 0.7491935858001325
plot_id,batch_id 0 83 miss% 0.6631257070618023
plot_id,batch_id 0 84 miss% 0.4201602415766079
plot_id,batch_id 0 85 miss% 0.3220535573605138
plot_id,batch_id 0 86 miss% 0.40660392910352466
plot_id,batch_id 0 87 miss% 0.7385393408719296
plot_id,batch_id 0 88 miss% 0.5628920207403627
plot_id,batch_id 0 89 miss% 0.7686144383555928
plot_id,batch_id 0 90 miss% 0.30453346381848595
plot_id,batch_id 0 91 miss% 0.6057455202977331
plot_id,batch_id 0 92 miss% 0.5814673321272074
plot_id,batch_id 0 93 miss% 0.5043426630845986
plot_id,batch_id 0 94 miss% 0.5412326942564705
plot_id,batch_id 0 95 miss% 0.30706317738824757
plot_id,batch_id 0 96 miss% 0.4705464865784673
plot_id,batch_id 0 97 miss% 0.5776653408037228
plot_id,batch_id 0 98 miss% 0.7450430817387924
plot_id,batch_id 0 99 miss% 0.5517582275397073
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.33745532 0.65707345 0.5216476  0.69603439 0.43337019 0.53007667
 0.59264393 0.42229805 0.467243   0.74820022 0.7490739  0.74353505
 0.38628648 0.86530066 0.57656092 0.29040727 0.52956057 0.52708861
 0.48507791 0.50676067 0.55525641 0.73979376 0.66358463 0.66081877
 0.53320704 0.55973273 0.54820865 0.65317334 0.73065289 0.5166378
 0.37056197 0.57111897 0.63171411 0.63373641 0.54497376 0.33764103
 0.46327216 0.46283033 0.68599414 0.84129393 0.52649517 0.55366663
 0.79547527 0.46332047 0.64026056 0.85062062 0.43040832 0.66592681
 0.74862564 0.82025251 0.49413948 0.65702056 0.52137255 0.89241892
 0.73702335 0.73856016 0.60043408 0.77768591 0.51699773 0.45138371
 0.38204982 0.32815695 0.40145262 0.69228468 0.4548565  0.45989576
 0.75853777 0.75002581 0.33427355 0.62939538 0.57537303 0.40271758
 0.64460945 0.38211871 0.72587303 0.27892924 0.39709619 0.41932614
 0.70116118 0.32006841 0.29575648 0.55790473 0.74919359 0.66312571
 0.42016024 0.32205356 0.40660393 0.73853934 0.56289202 0.76861444
 0.30453346 0.60574552 0.58146733 0.50434266 0.54123269 0.30706318
 0.47054649 0.57766534 0.74504308 0.55175823]
for model  159 the mean error 0.563644299484094
all id 159 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 25 out win 4 err 0.563644299484094 time 17736.39457631111
Launcher: Job 160 completed in 17973 seconds.
Launcher: Task 184 done. Exiting.
 67 miss% 0.031768435892391235
plot_id,batch_id 0 68 miss% 0.03773168150792451
plot_id,batch_id 0 69 miss% 0.025059842061352904
plot_id,batch_id 0 70 miss% 0.041318034030030315
plot_id,batch_id 0 71 miss% 0.04388674992836563
plot_id,batch_id 0 72 miss% 0.06756692707874391
plot_id,batch_id 0 73 miss% 0.02668785489972998
plot_id,batch_id 0 74 miss% 0.05025315816850218
plot_id,batch_id 0 75 miss% 0.07779429311377115
plot_id,batch_id 0 76 miss% 0.1197889984230366
plot_id,batch_id 0 77 miss% 0.06707260113299725
plot_id,batch_id 0 78 miss% 0.04690754522625299
plot_id,batch_id 0 79 miss% 0.040325258360284826
plot_id,batch_id 0 80 miss% 0.03529162339662997
plot_id,batch_id 0 81 miss% 0.08875601789010051
plot_id,batch_id 0 82 miss% 0.04639624262298799
plot_id,batch_id 0 83 miss% 0.04706855128823887
plot_id,batch_id 0 84 miss% 0.0404214211096726
plot_id,batch_id 0 85 miss% 0.06316632702465245
plot_id,batch_id 0 86 miss% 0.043634337430079245
plot_id,batch_id 0 87 miss% 0.03732803857262983
plot_id,batch_id 0 88 miss% 0.039915913020449854
plot_id,batch_id 0 89 miss% 0.06055094770114849
plot_id,batch_id 0 90 miss% 0.03779470183824911
plot_id,batch_id 0 91 miss% 0.042923635580217986
plot_id,batch_id 0 92 miss% 0.03105690834671563
plot_id,batch_id 0 93 miss% 0.04845947210327463
plot_id,batch_id 0 94 miss% 0.052649854419830645
plot_id,batch_id 0 95 miss% 0.031356966193119826
plot_id,batch_id 0 96 miss% 0.08087174445642366
plot_id,batch_id 0 97 miss% 0.03646870098680889
plot_id,batch_id 0 98 miss% 0.06265999259364999
plot_id,batch_id 0 99 miss% 0.04000758561213566
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06480915 0.03622116 0.07758405 0.03469595 0.04081648 0.04856099
 0.06538676 0.06580644 0.03887417 0.02041354 0.04823436 0.07485752
 0.05241189 0.08114393 0.0693727  0.08398779 0.12283171 0.05645022
 0.04770575 0.05143594 0.27567137 0.03998824 0.03105579 0.03337579
 0.05035862 0.04571538 0.03505019 0.0525752  0.03787241 0.02464927
 0.05548406 0.06973754 0.06418323 0.04042688 0.03024682 0.03554621
 0.0679851  0.07448326 0.07299599 0.04128215 0.05601681 0.05391785
 0.02873289 0.02779309 0.02779414 0.04686797 0.02877055 0.0282835
 0.03786532 0.01712317 0.12417959 0.03089132 0.01807844 0.02292377
 0.02155485 0.10466441 0.05337471 0.04263245 0.03767731 0.03101024
 0.04396193 0.04527656 0.04809323 0.05787667 0.08031112 0.04680611
 0.12351426 0.03176844 0.03773168 0.02505984 0.04131803 0.04388675
 0.06756693 0.02668785 0.05025316 0.07779429 0.119789   0.0670726
 0.04690755 0.04032526 0.03529162 0.08875602 0.04639624 0.04706855
 0.04042142 0.06316633 0.04363434 0.03732804 0.03991591 0.06055095
 0.0377947  0.04292364 0.03105691 0.04845947 0.05264985 0.03135697
 0.08087174 0.0364687  0.06265999 0.04000759]
for model  150 the mean error 0.05289216641438939
all id 150 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 25 out win 4 err 0.05289216641438939 time 17730.4966442585
Launcher: Job 151 completed in 17988 seconds.
Launcher: Task 109 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  265233
Epoch:0, Train loss:0.501245, valid loss:0.476277
Epoch:1, Train loss:0.098055, valid loss:0.002858
Epoch:2, Train loss:0.005646, valid loss:0.002287
Epoch:3, Train loss:0.004484, valid loss:0.001879
Epoch:4, Train loss:0.003760, valid loss:0.001636
Epoch:5, Train loss:0.003183, valid loss:0.001541
Epoch:6, Train loss:0.002817, valid loss:0.001315
Epoch:7, Train loss:0.002480, valid loss:0.001124
Epoch:8, Train loss:0.002208, valid loss:0.001134
Epoch:9, Train loss:0.002112, valid loss:0.001023
Epoch:10, Train loss:0.001951, valid loss:0.001090
Epoch:11, Train loss:0.001368, valid loss:0.000720
Epoch:12, Train loss:0.001262, valid loss:0.000706
Epoch:13, Train loss:0.001253, valid loss:0.000723
Epoch:14, Train loss:0.001217, valid loss:0.000656
Epoch:15, Train loss:0.001143, valid loss:0.000876
Epoch:16, Train loss:0.001149, valid loss:0.000840
Epoch:17, Train loss:0.001098, valid loss:0.000731
Epoch:18, Train loss:0.001074, valid loss:0.000676
Epoch:19, Train loss:0.001047, valid loss:0.000609
Epoch:20, Train loss:0.000983, valid loss:0.000700
Epoch:21, Train loss:0.000729, valid loss:0.000617
Epoch:22, Train loss:0.000706, valid loss:0.000543
Epoch:23, Train loss:0.000708, valid loss:0.000515
Epoch:24, Train loss:0.000666, valid loss:0.000604
Epoch:25, Train loss:0.000685, valid loss:0.000530
Epoch:26, Train loss:0.000665, valid loss:0.000661
Epoch:27, Train loss:0.000639, valid loss:0.000659
Epoch:28, Train loss:0.000639, valid loss:0.000538
Epoch:29, Train loss:0.000629, valid loss:0.000536
Epoch:30, Train loss:0.000626, valid loss:0.000602
Epoch:31, Train loss:0.000501, valid loss:0.000488
Epoch:32, Train loss:0.000471, valid loss:0.000476
Epoch:33, Train loss:0.000467, valid loss:0.000493
Epoch:34, Train loss:0.000480, valid loss:0.000489
Epoch:35, Train loss:0.000466, valid loss:0.000500
Epoch:36, Train loss:0.000455, valid loss:0.000480
Epoch:37, Train loss:0.000444, valid loss:0.000472
Epoch:38, Train loss:0.000455, valid loss:0.000484
Epoch:39, Train loss:0.000435, valid loss:0.000472
Epoch:40, Train loss:0.000428, valid loss:0.000527
Epoch:41, Train loss:0.000388, valid loss:0.000466
Epoch:42, Train loss:0.000383, valid loss:0.000448
Epoch:43, Train loss:0.000375, valid loss:0.000481
Epoch:44, Train loss:0.000378, valid loss:0.000451
Epoch:45, Train loss:0.000370, valid loss:0.000459
Epoch:46, Train loss:0.000373, valid loss:0.000451
Epoch:47, Train loss:0.000368, valid loss:0.000458
Epoch:48, Train loss:0.000371, valid loss:0.000440
Epoch:49, Train loss:0.000356, valid loss:0.000457
Epoch:50, Train loss:0.000360, valid loss:0.000465
Epoch:51, Train loss:0.000338, valid loss:0.000447
Epoch:52, Train loss:0.000335, valid loss:0.000450
Epoch:53, Train loss:0.000333, valid loss:0.000446
Epoch:54, Train loss:0.000332, valid loss:0.000445
Epoch:55, Train loss:0.000332, valid loss:0.000444
Epoch:56, Train loss:0.000331, valid loss:0.000446
Epoch:57, Train loss:0.000331, valid loss:0.000443
Epoch:58, Train loss:0.000330, valid loss:0.000444
Epoch:59, Train loss:0.000330, valid loss:0.000444
Epoch:60, Train loss:0.000330, valid loss:0.000444
training time 17798.00908446312
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.2552082503074135
plot_id,batch_id 0 1 miss% 0.3231776511004816
plot_id,batch_id 0 2 miss% 0.4206683606306628
plot_id,batch_id 0 3 miss% 0.31770939841885565
plot_id,batch_id 0 4 miss% 0.3605379736317719
plot_id,batch_id 0 5 miss% 0.2580428979440938
plot_id,batch_id 0 6 miss% 0.27655821838558675
plot_id,batch_id 0 7 miss% 0.43827566668372064
plot_id,batch_id 0 8 miss% 0.6007668009229673
plot_id,batch_id 0 9 miss% 0.4130081019761442
plot_id,batch_id 0 10 miss% 0.2216255142798864
plot_id,batch_id 0 11 miss% 0.24463524002270487
plot_id,batch_id 0 12 miss% 0.3191787147093423
plot_id,batch_id 0 13 miss% 0.2959016340696137
plot_id,batch_id 0 14 miss% 0.3860520992735718
plot_id,batch_id 0 15 miss% 0.3183100624790128
plot_id,batch_id 0 16 miss% 0.43789476372525826
plot_id,batch_id 0 17 miss% 0.3598928846095685
plot_id,batch_id 0 18 miss% 0.44818429670621135
plot_id,batch_id 0 19 miss% 0.38058102932019244
plot_id,batch_id 0 20 miss% 0.28347712922115476
plot_id,batch_id 0 21 miss% 0.302727796279132
plot_id,batch_id 0 22 miss% 0.33719194704762673
plot_id,batch_id 0 23 miss% 0.3759336694223561
plot_id,batch_id 0 24 miss% 0.3540706435924055
plot_id,batch_id 0 25 miss% 0.23977388985185852
plot_id,batch_id 0 26 miss% 0.31283852533004186
plot_id,batch_id 0 27 miss% 0.31298801758892253
plot_id,batch_id 0 28 miss% 0.3917925559983344
plot_id,batch_id 0 29 miss% 0.3755682743960507
plot_id,batch_id 0 30 miss% 0.2300686348871601
plot_id,batch_id 0 31 miss% 0.4327406858553972
plot_id,batch_id 0 32 miss% 0.4558245228212383
plot_id,batch_id 0 33 miss% 0.3827028216880497
plot_id,batch_id 0 34 miss% 0.28089603160166177
plot_id,batch_id 0 35 miss% 0.22613815015002964
plot_id,batch_id 0 36 miss% 0.44154605094351945
plot_id,batch_id 0 37 miss% 0.3825358534932629
plot_id,batch_id 0 38 miss% 0.46742337230366593
plot_id,batch_id 0 39 miss% 0.42651228247890843
plot_id,batch_id 0 40 miss% 0.340887798324639
plot_id,batch_id 0 41 miss% 0.3877640706864653
plot_id,batch_id 0 42 miss% 0.22060971862992432
plot_id,batch_id 0 43 miss% 0.23396768792855194
plot_id,batch_id 0 44 miss% 0.23363945929765736
plot_id,batch_id 0 45 miss% 0.31004296964583145
plot_id,batch_id 0 46 miss% 0.3716620207981785
plot_id,batch_id 0 47 miss% 0.3229259826547748
plot_id,batch_id 0 48 miss% 0.2288984423972903
plot_id,batch_id 0 49 miss% 0.19865282346470128
plot_id,batch_id 0 50 miss% 0.38823336662772
plot_id,batch_id 0 51 miss% 0.4626315989690039
plot_id,batch_id 0 52 miss% 0.27881306850298243
plot_id,batch_id 0 53 miss% 0.19862995176779463
plot_id,batch_id 0 54 miss% 0.24255479237050992
plot_id,batch_id 0 55 miss% 0.5011259356002995
plot_id,batch_id 0 56 miss% 0.5052865092810426
plot_id,batch_id 0 57 miss% 0.3151728573704339
plot_id,batch_id 0 58 miss% 0.24742902107348386
plot_id,batch_id 0 59 miss% 0.2997760100729258
plot_id,batch_id 0 60 miss% 0.17539072475663406
plot_id,batch_id 0 61 miss% 0.21157335636653718
plot_id,batch_id 0 62 miss% 0.30533249650006833
plot_id,batch_id 0 63 miss% 0.3435589936009373
plot_id,batch_id 0 64 miss% 0.317418095109219
plot_id,batch_id 0 65 miss% 0.3228354894193023
plot_id,batch_id 0 66 miss% 0.31125214120920947
plot_id,batch_id 0 67 miss% 0.2718620854165171
plot_id,batch_id the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  120401
Epoch:0, Train loss:0.452848, valid loss:0.445072
Epoch:1, Train loss:0.067332, valid loss:0.002308
Epoch:2, Train loss:0.004273, valid loss:0.001709
Epoch:3, Train loss:0.002996, valid loss:0.001308
Epoch:4, Train loss:0.002680, valid loss:0.001157
Epoch:5, Train loss:0.002394, valid loss:0.001196
Epoch:6, Train loss:0.002200, valid loss:0.001011
Epoch:7, Train loss:0.002012, valid loss:0.000935
Epoch:8, Train loss:0.001930, valid loss:0.001233
Epoch:9, Train loss:0.001812, valid loss:0.000908
Epoch:10, Train loss:0.001654, valid loss:0.000868
Epoch:11, Train loss:0.001249, valid loss:0.000691
Epoch:12, Train loss:0.001234, valid loss:0.000629
Epoch:13, Train loss:0.001166, valid loss:0.000692
Epoch:14, Train loss:0.001135, valid loss:0.000651
Epoch:15, Train loss:0.001091, valid loss:0.000651
Epoch:16, Train loss:0.001048, valid loss:0.000615
Epoch:17, Train loss:0.001032, valid loss:0.000665
Epoch:18, Train loss:0.001003, valid loss:0.000659
Epoch:19, Train loss:0.000990, valid loss:0.000590
Epoch:20, Train loss:0.000957, valid loss:0.000580
Epoch:21, Train loss:0.000744, valid loss:0.000546
Epoch:22, Train loss:0.000708, valid loss:0.000466
Epoch:23, Train loss:0.000711, valid loss:0.000501
Epoch:24, Train loss:0.000694, valid loss:0.000499
Epoch:25, Train loss:0.000677, valid loss:0.000537
Epoch:26, Train loss:0.000671, valid loss:0.000480
Epoch:27, Train loss:0.000657, valid loss:0.000458
Epoch:28, Train loss:0.000648, valid loss:0.000522
Epoch:29, Train loss:0.000645, valid loss:0.000497
Epoch:30, Train loss:0.000640, valid loss:0.000500
Epoch:31, Train loss:0.000523, valid loss:0.000463
Epoch:32, Train loss:0.000521, valid loss:0.000443
Epoch:33, Train loss:0.000503, valid loss:0.000464
Epoch:34, Train loss:0.000499, valid loss:0.000435
Epoch:35, Train loss:0.000504, valid loss:0.000442
Epoch:36, Train loss:0.000497, valid loss:0.000447
Epoch:37, Train loss:0.000487, valid loss:0.000456
Epoch:38, Train loss:0.000483, valid loss:0.000431
Epoch:39, Train loss:0.000476, valid loss:0.000476
Epoch:40, Train loss:0.000473, valid loss:0.000441
Epoch:41, Train loss:0.000426, valid loss:0.000429
Epoch:42, Train loss:0.000418, valid loss:0.000421
Epoch:43, Train loss:0.000418, valid loss:0.000446
Epoch:44, Train loss:0.000418, valid loss:0.000440
Epoch:45, Train loss:0.000413, valid loss:0.000465
Epoch:46, Train loss:0.000407, valid loss:0.000459
Epoch:47, Train loss:0.000406, valid loss:0.000453
Epoch:48, Train loss:0.000404, valid loss:0.000417
Epoch:49, Train loss:0.000400, valid loss:0.000444
Epoch:50, Train loss:0.000402, valid loss:0.000418
Epoch:51, Train loss:0.000383, valid loss:0.000418
Epoch:52, Train loss:0.000376, valid loss:0.000419
Epoch:53, Train loss:0.000373, valid loss:0.000419
Epoch:54, Train loss:0.000372, valid loss:0.000417
Epoch:55, Train loss:0.000372, valid loss:0.000416
Epoch:56, Train loss:0.000371, valid loss:0.000417
Epoch:57, Train loss:0.000370, valid loss:0.000416
Epoch:58, Train loss:0.000370, valid loss:0.000413
Epoch:59, Train loss:0.000370, valid loss:0.000415
Epoch:60, Train loss:0.000369, valid loss:0.000414
training time 17803.50999736786
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.2779600395851693
plot_id,batch_id 0 1 miss% 0.36102283655965156
plot_id,batch_id 0 2 miss% 0.34321187060436736
plot_id,batch_id 0 3 miss% 0.332253064622974
plot_id,batch_id 0 4 miss% 0.26974664023557554
plot_id,batch_id 0 5 miss% 0.2813712089845084
plot_id,batch_id 0 6 miss% 0.27284100462993516
plot_id,batch_id 0 7 miss% 0.4544343172011016
plot_id,batch_id 0 8 miss% 0.4391125265247317
plot_id,batch_id 0 9 miss% 0.3063182116038879
plot_id,batch_id 0 10 miss% 0.2270595516641007
plot_id,batch_id 0 11 miss% 0.309187828942423
plot_id,batch_id 0 12 miss% 0.3410501616861158
plot_id,batch_id 0 13 miss% 0.302884834230962
plot_id,batch_id 0 14 miss% 0.40398785974019097
plot_id,batch_id 0 15 miss% 0.29090010287186174
plot_id,batch_id 0 16 miss% 0.45599495683360036
plot_id,batch_id 0 17 miss% 0.429088229795564
plot_id,batch_id 0 18 miss% 0.46206668477298685
plot_id,batch_id 0 19 miss% 0.3652010109993992
plot_id,batch_id 0 20 miss% 0.31038556751482904
plot_id,batch_id 0 21 miss% 0.3167222009261202
plot_id,batch_id 0 22 miss% 0.2682573649644973
plot_id,batch_id 0 23 miss% 0.32484192910701254
plot_id,batch_id 0 24 miss% 0.24215001632396163
plot_id,batch_id 0 25 miss% 0.31721636644957657
plot_id,batch_id 0 26 miss% 0.36148204601558764
plot_id,batch_id 0 27 miss% 0.37640894669935643
plot_id,batch_id 0 28 miss% 0.288090862571274
plot_id,batch_id 0 29 miss% 0.35170349494190706
plot_id,batch_id 0 30 miss% 0.2615465463876258
plot_id,batch_id 0 31 miss% 0.4096583643783655
plot_id,batch_id 0 32 miss% 0.4320417891097406
plot_id,batch_id 0 33 miss% 0.31266440433950576
plot_id,batch_id 0 34 miss% 0.34735632805565236
plot_id,batch_id 0 35 miss% 0.28145193485132797
plot_id,batch_id 0 36 miss% 0.45902746891689306
plot_id,batch_id 0 37 miss% 0.415467994867682
plot_id,batch_id 0 38 miss% 0.3733898688023526
plot_id,batch_id 0 39 miss% 0.34695835713845996
plot_id,batch_id 0 40 miss% 0.43500411401796396
plot_id,batch_id 0 41 miss% 0.3583557339128853
plot_id,batch_id 0 42 miss% 0.2849798756814505
plot_id,batch_id 0 43 miss% 0.29636322150103794
plot_id,batch_id 0 44 miss% 0.23411280599423456
plot_id,batch_id 0 45 miss% 0.19196801484539194
plot_id,batch_id 0 46 miss% 0.24241639178227317
plot_id,batch_id 0 47 miss% 0.3879943319767842
plot_id,batch_id 0 48 miss% 0.38407196054378606
plot_id,batch_id 0 49 miss% 0.28045183360219916
plot_id,batch_id 0 50 miss% 0.2946207927707293
plot_id,batch_id 0 51 miss% 0.38446696586939855
plot_id,batch_id 0 52 miss% 0.3397170639504573
plot_id,batch_id 0 53 miss% 0.28987841817717697
plot_id,batch_id 0 54 miss% 0.40881815790737824
plot_id,batch_id 0 55 miss% 0.4335836892332216
plot_id,batch_id 0 56 miss% 0.44519466160998394
plot_id,batch_id 0 57 miss% 0.34243446772722996
plot_id,batch_id 0 58 miss% 0.2954313433455749
plot_id,batch_id 0 59 miss% 0.33669058727843226
plot_id,batch_id 0 60 miss% 0.2178044717164952
plot_id,batch_id 0 61 miss% 0.2342810287390456
plot_id,batch_id 0 62 miss% 0.2730529765374914
plot_id,batch_id 0 63 miss% 0.3726713697790091
plot_id,batch_id 0 64 miss% 0.35486319490651147
plot_id,batch_id 0 65 miss% 0.24740623508680062
plot_id,batch_id 0 66 miss% 0.3171396633674245
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  265233
Epoch:0, Train loss:0.657663, valid loss:0.627175
Epoch:1, Train loss:0.539427, valid loss:0.526949
Epoch:2, Train loss:0.521847, valid loss:0.526274
Epoch:3, Train loss:0.520453, valid loss:0.525653
Epoch:4, Train loss:0.519811, valid loss:0.525405
Epoch:5, Train loss:0.519451, valid loss:0.525278
Epoch:6, Train loss:0.519059, valid loss:0.525223
Epoch:7, Train loss:0.518937, valid loss:0.525338
Epoch:8, Train loss:0.518500, valid loss:0.525162
Epoch:9, Train loss:0.518151, valid loss:0.525243
Epoch:10, Train loss:0.518088, valid loss:0.524886
Epoch:11, Train loss:0.517203, valid loss:0.524598
Epoch:12, Train loss:0.517063, valid loss:0.524380
Epoch:13, Train loss:0.516940, valid loss:0.524478
Epoch:14, Train loss:0.516949, valid loss:0.524468
Epoch:15, Train loss:0.516842, valid loss:0.524490
Epoch:16, Train loss:0.516898, valid loss:0.524354
Epoch:17, Train loss:0.516766, valid loss:0.524504
Epoch:18, Train loss:0.516701, valid loss:0.524402
Epoch:19, Train loss:0.516714, valid loss:0.524381
Epoch:20, Train loss:0.516629, valid loss:0.524335
Epoch:21, Train loss:0.516202, valid loss:0.524249
Epoch:22, Train loss:0.516144, valid loss:0.524146
Epoch:23, Train loss:0.516146, valid loss:0.524131
Epoch:24, Train loss:0.516101, valid loss:0.524258
Epoch:25, Train loss:0.516126, valid loss:0.524280
Epoch:26, Train loss:0.516094, valid loss:0.524228
Epoch:27, Train loss:0.516084, valid loss:0.524151
Epoch:28, Train loss:0.516005, valid loss:0.524317
Epoch:29, Train loss:0.516012, valid loss:0.524187
Epoch:30, Train loss:0.516015, valid loss:0.524142
Epoch:31, Train loss:0.515805, valid loss:0.524135
Epoch:32, Train loss:0.515776, valid loss:0.524106
Epoch:33, Train loss:0.515767, valid loss:0.524099
Epoch:34, Train loss:0.515769, valid loss:0.524056
Epoch:35, Train loss:0.515760, valid loss:0.524076
Epoch:36, Train loss:0.515760, valid loss:0.524113
Epoch:37, Train loss:0.515744, valid loss:0.524102
Epoch:38, Train loss:0.515731, valid loss:0.524101
Epoch:39, Train loss:0.515744, valid loss:0.524067
Epoch:40, Train loss:0.515733, valid loss:0.524113
Epoch:41, Train loss:0.515637, valid loss:0.524066
Epoch:42, Train loss:0.515627, valid loss:0.524121
Epoch:43, Train loss:0.515621, valid loss:0.524088
Epoch:44, Train loss:0.515610, valid loss:0.524088
Epoch:45, Train loss:0.515605, valid loss:0.524076
Epoch:46, Train loss:0.515609, valid loss:0.524069
Epoch:47, Train loss:0.515613, valid loss:0.524084
Epoch:48, Train loss:0.515598, valid loss:0.524084
Epoch:49, Train loss:0.515595, valid loss:0.524081
Epoch:50, Train loss:0.515590, valid loss:0.524090
Epoch:51, Train loss:0.515559, valid loss:0.524055
Epoch:52, Train loss:0.515555, valid loss:0.524049
Epoch:53, Train loss:0.515554, valid loss:0.524052
Epoch:54, Train loss:0.515553, valid loss:0.524046
Epoch:55, Train loss:0.515552, valid loss:0.524049
Epoch:56, Train loss:0.515551, valid loss:0.524045
Epoch:57, Train loss:0.515551, valid loss:0.524051
Epoch:58, Train loss:0.515551, valid loss:0.524051
Epoch:59, Train loss:0.515550, valid loss:0.524049
Epoch:60, Train loss:0.515550, valid loss:0.524053
training time 17893.863214731216
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.9589620191798153
plot_id,batch_id 0 1 miss% 0.9626282254170234
plot_id,batch_id 0 2 miss% 0.9642334047162483
plot_id,batch_id 0 3 miss% 0.9651881273505505
plot_id,batch_id 0 4 miss% 0.964431982084305
plot_id,batch_id 0 5 miss% 0.9609515966640283
plot_id,batch_id 0 6 miss% 0.9639441926417784
plot_id,batch_id 0 7 miss% 0.9633774569474649
plot_id,batch_id 0 8 miss% 0.9637013720363636
plot_id,batch_id 0 9 miss% 0.9665383195788793
plot_id,batch_id 0 10 miss% 0.9618465521495134
plot_id,batch_id 0 11 miss% 0.9670414071417459
plot_id,batch_id 0 12 miss% 0.9640997858035844
plot_id,batch_id 0 13 miss% 0.9677421578033196
plot_id,batch_id 0 14 miss% 0.9696346829146857
plot_id,batch_id 0 15 miss% 0.9731625332342559
plot_id,batch_id 0 16 miss% 0.977437424289384
plot_id,batch_id 0 17 miss% 0.9657915034483207
plot_id,batch_id 0 18 miss% 0.9744279626883665
plot_id,batch_id 0 19 miss% 0.9695676383000966
plot_id,batch_id 0 20 miss% 0.9632455811573648
plot_id,batch_id 0 21 miss% 0.9648583101878162
plot_id,batch_id 0 22 miss% 0.9654475373892465
plot_id,batch_id 0 23 miss% 0.9652335999247661
plot_id,batch_id 0 24 miss% 0.9641615163632449
plot_id,batch_id 0 25 miss% 0.9623286327547275
plot_id,batch_id 0 26 miss% 0.9659209377881363
plot_id,batch_id 0 27 miss% 0.9648379165617936
plot_id,batch_id 0 28 miss% 0.9659372073772271
plot_id,batch_id 0 29 miss% 0.965447431920903
plot_id,batch_id 0 30 miss% 0.9599035159727246
plot_id,batch_id 0 31 miss% 0.9629874920627433
plot_id,batch_id 0 32 miss% 0.9718753214854791
plot_id,batch_id 0 33 miss% 0.9688968749398559
plot_id,batch_id 0 34 miss% 0.9674743840529418
plot_id,batch_id 0 35 miss% 0.9701914220600761
plot_id,batch_id 0 36 miss% 0.9759349871901348
plot_id,batch_id 0 37 miss% 0.9697255287573966
plot_id,batch_id 0 38 miss% 0.9703483747469048
plot_id,batch_id 0 39 miss% 0.9692884953127113
plot_id,batch_id 0 40 miss% 0.962063193194207
plot_id,batch_id 0 41 miss% 0.9657762337860721
plot_id,batch_id 0 42 miss% 0.9641177746022184
plot_id,batch_id 0 43 miss% 0.9643322514979149
plot_id,batch_id 0 44 miss% 0.964080328176476
plot_id,batch_id 0 45 miss% 0.9637181525126425
plot_id,batch_id 0 46 miss% 0.9649342511200473
plot_id,batch_id 0 47 miss% 0.964325749125933
plot_id,batch_id 0 48 miss% 0.9665013273055102
plot_id,batch_id 0 49 miss% 0.9643931097131446
plot_id,batch_id 0 50 miss% 0.9667818384907008
plot_id,batch_id 0 51 miss% 0.9698640786219259
plot_id,batch_id 0 52 miss% 0.9665552615130133
plot_id,batch_id 0 53 miss% 0.9693338998694968
plot_id,batch_id 0 54 miss% 0.9661821132391304
plot_id,batch_id 0 55 miss% 0.9629558827935252
plot_id,batch_id 0 56 miss% 0.9746151657677037
plot_id,batch_id 0 57 miss% 0.970598189865979
plot_id,batch_id 0 58 miss% 0.9676875389268504
plot_id,batch_id 0 59 miss% 0.9686945201703125
plot_id,batch_id 0 60 miss% 0.9705622261684163
plot_id,batch_id 0 61 miss% 0.9632374433407286
plot_id,batch_id 0 62 miss% 0.9620479673922683
plot_id,batch_id 0 63 miss% 0.9639867570674168
plot_id,batch_id 0 64 miss% 0.9644095831534742
plot_id,batch_id 0 65 miss% 0.9534572636790136
plot_id,batch_id 0 66 miss% 0.9616809252135045
plot_id,batch_id 0 67 miss% 0.963915452237181
plot_id,batch_id 0 68 miss% 0.9656498938199555
plot_id,batch_id 0 69 miss% 0.9641294763524945
plot_id,batch_id 0 70 miss% 0.9589991400318146
plot_id,batch_id 0 71 miss% 0.9722032004550754
plot_id,batch_id 0 72 miss% 0.970262212759389
plot_id,batch_id 0 73 miss% 0.9655583424568259
plot_id,batch_id 0 74 miss% 0.968975951662373
plot_id,batch_id 0 75 miss% 0.9476101217699667
plot_id,batch_id 0 76 miss% 0.9671401513688871
plot_id,batch_id 0 77 miss% 0.9646771027088167
plot_id,batch_id 0 78 miss% 0.960714047116544
plot_id,batch_id 0 79 miss% 0.9678390882578206
plot_id,batch_id 0 80 miss% 0.9621087184556105
plot_id,batch_id 0 81 miss% 0.962585393413374
plot_id,batch_id 0 82 miss% 0.9639287413569136
plot_id,batch_id 0 83 miss% 0.9642286644103273
plot_id,batch_id 0 84 miss% 0.9637753568817534
plot_id,batch_id 0 85 miss% 0.9515016983918486
plot_id,batch_id 0 86 miss% 0.9631289740556531
plot_id,batch_id 0 87 miss% 0.962656303123856
plot_id,batch_id 0 88 miss% 0.9630556946139234
plot_id,batch_id 0 89 miss% 0.9628264990071469
plot_id,batch_id 0 90 miss% 0.9613660079687617
plot_id,batch_id 0 91 miss% 0.9689938647222872
plot_id,batch_id 0 92 miss% 0.9794050980154738
plot_id,batch_id 0 93 miss% 0.9634696177682291
plot_id,batch_id 0 94 miss% 0.9731512951216671
plot_id,batch_id 0 95 miss% 0.9874952601344239
plot_id,batch_id 0 96 miss% 0.9768031514269087
plot_id,batch_id 0 97 miss% 0.9704235617407367
plot_id,batch_id 0 98 miss% 0.9820399488504528
plot_id,batch_id 0 99 miss% 0.9622289556524769
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.95896202 0.96262823 0.9642334  0.96518813 0.96443198 0.9609516
 0.96394419 0.96337746 0.96370137 0.96653832 0.96184655 0.96704141
 0.96409979 0.96774216 0.96963468 0.97316253 0.97743742 0.9657915
 0.97442796 0.96956764 0.96324558 0.96485831 0.96544754 0.9652336
 0.96416152 0.96232863 0.96592094 0.96483792 0.96593721 0.96544743
 0.95990352 0.96298749 0.97187532 0.96889687 0.96747438 0.97019142
 0.97593499 0.96972553 0.97034837 0.9692885  0.96206319 0.96577623
 0.96411777 0.96433225 0.96408033 0.96371815 0.96493425 0.96432575
 0.96650133 0.96439311 0.96678184 0.96986408 0.96655526 0.9693339
 0.96618211 0.96295588 0.97461517 0.97059819 0.96768754 0.96869452
 0.97056223 0.96323744 0.96204797 0.96398676 0.96440958 0.95345726
 0.96168093 0.96391545 0.96564989 0.96412948 0.95899914 0.9722032
 0.97026221 0.96555834 0.96897595 0.94761012 0.96714015 0.9646771
 0.96071405 0.96783909 0.96210872 0.96258539 0.96392874 0.96422866
 0.96377536 0.9515017  0.96312897 0.9626563  0.96305569 0.9628265
 0.96136601 0.96899386 0.9794051  0.96346962 0.9731513  0.98749526
 0.97680315 0.97042356 0.98203995 0.96222896]
for model  79 the mean error 0.9660849342481252
all id 79 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 21 out win 5 err 0.9660849342481252 time 17893.863214731216
Launcher: Job 80 completed in 18005 seconds.
Launcher: Task 52 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  265233
Epoch:0, Train loss:0.657663, valid loss:0.627175
Epoch:1, Train loss:0.533021, valid loss:0.526956
Epoch:2, Train loss:0.521944, valid loss:0.526314
Epoch:3, Train loss:0.520314, valid loss:0.525464
Epoch:4, Train loss:0.519527, valid loss:0.525539
Epoch:5, Train loss:0.518956, valid loss:0.525076
Epoch:6, Train loss:0.518472, valid loss:0.525018
Epoch:7, Train loss:0.518132, valid loss:0.524816
Epoch:8, Train loss:0.517969, valid loss:0.524766
Epoch:9, Train loss:0.517590, valid loss:0.524542
Epoch:10, Train loss:0.517603, valid loss:0.524599
Epoch:11, Train loss:0.516812, valid loss:0.524287
Epoch:12, Train loss:0.516682, valid loss:0.524366
Epoch:13, Train loss:0.516736, valid loss:0.524351
Epoch:14, Train loss:0.516675, valid loss:0.524260
Epoch:15, Train loss:0.516579, valid loss:0.524320
Epoch:16, Train loss:0.516619, valid loss:0.524338
Epoch:17, Train loss:0.516478, valid loss:0.524308
Epoch:18, Train loss:0.516449, valid loss:0.524208
Epoch:19, Train loss:0.516438, valid loss:0.524380
Epoch:20, Train loss:0.516439, valid loss:0.524238
Epoch:21, Train loss:0.516063, valid loss:0.524237
Epoch:22, Train loss:0.516051, valid loss:0.524119
Epoch:23, Train loss:0.516036, valid loss:0.524134
Epoch:24, Train loss:0.515980, valid loss:0.524260
Epoch:25, Train loss:0.515993, valid loss:0.524180
Epoch:26, Train loss:0.515969, valid loss:0.524132
Epoch:27, Train loss:0.515989, valid loss:0.524113
Epoch:28, Train loss:0.515990, valid loss:0.524190
Epoch:29, Train loss:0.515952, valid loss:0.524125
Epoch:30, Train loss:0.515913, valid loss:0.524071
Epoch:31, Train loss:0.515767, valid loss:0.524101
Epoch:32, Train loss:0.515741, valid loss:0.524043
Epoch:33, Train loss:0.515741, valid loss:0.524078
Epoch:34, Train loss:0.515750, valid loss:0.524078
Epoch:35, Train loss:0.515736, valid loss:0.524050
Epoch:36, Train loss:0.515740, valid loss:0.524103
Epoch:37, Train loss:0.515718, valid loss:0.524062
Epoch:38, Train loss:0.515706, valid loss:0.524074
Epoch:39, Train loss:0.515716, valid loss:0.524100
Epoch:40, Train loss:0.515713, valid loss:0.524066
Epoch:41, Train loss:0.515642, valid loss:0.524047
Epoch:42, Train loss:0.515627, valid loss:0.524059
Epoch:43, Train loss:0.515627, valid loss:0.524070
Epoch:44, Train loss:0.515624, valid loss:0.524056
Epoch:45, Train loss:0.515620, valid loss:0.524067
Epoch:46, Train loss:0.515615, valid loss:0.524030
Epoch:47, Train loss:0.515613, valid loss:0.524053
Epoch:48, Train loss:0.515616, valid loss:0.524054
Epoch:49, Train loss:0.515606, valid loss:0.524044
Epoch:50, Train loss:0.515609, valid loss:0.524070
Epoch:51, Train loss:0.515576, valid loss:0.524036
Epoch:52, Train loss:0.515573, valid loss:0.524028
Epoch:53, Train loss:0.515572, valid loss:0.524041
Epoch:54, Train loss:0.515571, valid loss:0.524035
Epoch:55, Train loss:0.515571, valid loss:0.524034
Epoch:56, Train loss:0.515571, valid loss:0.524027
Epoch:57, Train loss:0.515570, valid loss:0.524040
Epoch:58, Train loss:0.515569, valid loss:0.524035
Epoch:59, Train loss:0.515569, valid loss:0.524040
Epoch:60, Train loss:0.515569, valid loss:0.524038
training time 17922.32038140297
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.9693096959242045
plot_id,batch_id 0 1 miss% 0.9691144036698491
plot_id,batch_id 0 2 miss% 0.9724468009369971
plot_id,batch_id 0 3 miss% 0.9696438142852835
plot_id,batch_id 0 4 miss% 0.9714241814079556
plot_id,batch_id 0 5 miss% 0.961412113779634
plot_id,batch_id 0 6 miss% 0.9703159089827498
plot_id,batch_id 0 7 miss% 0.976444986546789
plot_id,batch_id 0 8 miss% 0.9729578297563137
plot_id,batch_id 0 9 miss% 0.9720026684488072
plot_id,batch_id 0 10 miss% 0.9655418418249502
plot_id,batch_id 0 11 miss% 0.9705462784944413
plot_id,batch_id 0 12 miss% 0.9709287875564337
plot_id,batch_id 0 13 miss% 0.9747750993727116
plot_id,batch_id 0 14 miss% 0.9800874283567351
plot_id,batch_id 0 15 miss% 0.9696017235940686
plot_id,batch_id 0 16 miss% 0.9765104463095603
plot_id,batch_id 0 17 miss% 0.9854785771791398
plot_id,batch_id 0 18 miss% 0.9801429786366174
plot_id,batch_id 0 19 miss% 0.9879297767758815
plot_id,batch_id 0 20 miss% 0.977790797741831
plot_id,batch_id 0 21 miss% 0.9757107211564287
plot_id,batch_id 0 22 miss% 0.972730525211765
plot_id,batch_id 0 23 miss% 0.9699035485513405
plot_id,batch_id 0 24 miss% 0.9696307547240743
plot_id,batch_id 0 25 miss% 0.9798195867472014
plot_id,batch_id 0 26 miss% 0.9771421856155855
plot_id,batch_id 0 27 miss% 0.977434255662087
plot_id,batch_id 0 28 miss% 0.9740434248606368
plot_id,batch_id 0 29 miss% 0.9788322556596603
plot_id,batch_id 0 30 miss% 0.9653349112659054
plot_id,batch_id 0 31 miss% 0.9733723935588059
plot_id,batch_id 0 32 miss% 0.9773231836035655
plot_id,batch_id 0 33 miss% 0.9800606175519623
plot_id,batch_id 0 34 miss% 0.9753790176076824
plot_id,batch_id 0 35 miss% 0.9971374920613039
plot_id,batch_id 0 36 miss% 0.9814998307467551
plot_id,batch_id 0 37 miss% 0.974234986971061
plot_id,batch_id 0 38 miss% 0.9793014530985351
plot_id,batch_id 0 39 miss% 0.9753090109641798
plot_id,batch_id 0 40 miss% 0.9680817278421111
plot_id,batch_id 0 41 miss% 0.9688000610808005
plot_id,batch_id 0 42 miss% 0.9695170443709324
plot_id,batch_id 0 43 miss% 0.9693682461407126
plot_id,batch_id 0 44 miss% 0.9728674774846303
plot_id,batch_id 0 45 miss% 0.9788864165398489
plot_id,batch_id 0 46 miss% 0.969955340392098
plot_id,batch_id 0 47 miss% 0.9763615321510285
plot_id,batch_id 0 48 miss% 0.9737209237431729
plot_id,batch_id 0 49 miss% 0.9742889228470859
plot_id,batch_id 0 50 miss% 0.9912066737029017
plot_id,batch_id 0 51 miss% 0.9739149508355732
plot_id,batch_id 0 52 miss% 0.9787253728530797
plot_id,batch_id 0 53 miss% 0.9751215348886825
plot_id,batch_id 0 54 miss% 0.9790348000089171
plot_id,batch_id 0 55 miss% 0.9814525128841716
plot_id,batch_id 0 56 miss% 0.9802418192109743
plot_id,batch_id 0 57 miss% 0.9768955842618137
plot_id,batch_id 0 58 miss% 0.975925307570518
plot_id,batch_id 0 59 miss% 0.9752692576555052
plot_id,batch_id 0 60 miss% 0.9643099610227229
plot_id,batch_id 0 61 miss% 0.9649087929645513
plot_id,batch_id 0 62 miss% 0.9805079014199345
plot_id,batch_id 0 63 miss% 0.9642471496353048
plot_id,batch_id 0 64 miss% 0.9754245857859429
plot_id,batch_id 0 65 miss% 0.9628658240620493
plot_id,batch_id 0 66 miss% 0.9731525377988381
plot_id,batch_id 0 67 miss% 0.9808534741711412
plot_id,batch_id 0 68 miss% 0.9700695421804308
plot_id,batch_id 0 69 miss% 0.9706910451348516
plot_id,batch_id 0 70 miss% 0.9556258879858915
0 68 miss% 0.37601348024314
plot_id,batch_id 0 69 miss% 0.4011188905938048
plot_id,batch_id 0 70 miss% 0.2503701797269443
plot_id,batch_id 0 71 miss% 0.3138682968052088
plot_id,batch_id 0 72 miss% 0.3732822941973062
plot_id,batch_id 0 73 miss% 0.25607648999588645
plot_id,batch_id 0 74 miss% 0.2828199481926527
plot_id,batch_id 0 75 miss% 0.22982551630223377
plot_id,batch_id 0 76 miss% 0.2608604643891443
plot_id,batch_id 0 77 miss% 0.2596677780325829
plot_id,batch_id 0 78 miss% 0.25552371234467786
plot_id,batch_id 0 79 miss% 0.3021575650691353
plot_id,batch_id 0 80 miss% 0.18458574678606457
plot_id,batch_id 0 81 miss% 0.33557315241460545
plot_id,batch_id 0 82 miss% 0.329665917140634
plot_id,batch_id 0 83 miss% 0.3765124142050145
plot_id,batch_id 0 84 miss% 0.286497209303624
plot_id,batch_id 0 85 miss% 0.18467881343833514
plot_id,batch_id 0 86 miss% 0.31235913630546114
plot_id,batch_id 0 87 miss% 0.32519915281307166
plot_id,batch_id 0 88 miss% 0.4264205258833543
plot_id,batch_id 0 89 miss% 0.32197387427599605
plot_id,batch_id 0 90 miss% 0.20197417510077825
plot_id,batch_id 0 91 miss% 0.24584659610431314
plot_id,batch_id 0 92 miss% 0.2954077915553344
plot_id,batch_id 0 93 miss% 0.30654812268671494
plot_id,batch_id 0 94 miss% 0.38632120866828834
plot_id,batch_id 0 95 miss% 0.18295698485613066
plot_id,batch_id 0 96 miss% 0.28730727589878546
plot_id,batch_id 0 97 miss% 0.3710574891089221
plot_id,batch_id 0 98 miss% 0.37365211964835193
plot_id,batch_id 0 99 miss% 0.3038371020747293
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.25520825 0.32317765 0.42066836 0.3177094  0.36053797 0.2580429
 0.27655822 0.43827567 0.6007668  0.4130081  0.22162551 0.24463524
 0.31917871 0.29590163 0.3860521  0.31831006 0.43789476 0.35989288
 0.4481843  0.38058103 0.28347713 0.3027278  0.33719195 0.37593367
 0.35407064 0.23977389 0.31283853 0.31298802 0.39179256 0.37556827
 0.23006863 0.43274069 0.45582452 0.38270282 0.28089603 0.22613815
 0.44154605 0.38253585 0.46742337 0.42651228 0.3408878  0.38776407
 0.22060972 0.23396769 0.23363946 0.31004297 0.37166202 0.32292598
 0.22889844 0.19865282 0.38823337 0.4626316  0.27881307 0.19862995
 0.24255479 0.50112594 0.50528651 0.31517286 0.24742902 0.29977601
 0.17539072 0.21157336 0.3053325  0.34355899 0.3174181  0.32283549
 0.31125214 0.27186209 0.37601348 0.40111889 0.25037018 0.3138683
 0.37328229 0.25607649 0.28281995 0.22982552 0.26086046 0.25966778
 0.25552371 0.30215757 0.18458575 0.33557315 0.32966592 0.37651241
 0.28649721 0.18467881 0.31235914 0.32519915 0.42642053 0.32197387
 0.20197418 0.2458466  0.29540779 0.30654812 0.38632121 0.18295698
 0.28730728 0.37105749 0.37365212 0.3038371 ]
for model  132 the mean error 0.32236847306151695
all id 132 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 25 out win 4 err 0.32236847306151695 time 17798.00908446312
Launcher: Job 133 completed in 18032 seconds.
Launcher: Task 214 done. Exiting.
plot_id,batch_id 0 71 miss% 0.9787709869116059
plot_id,batch_id 0 72 miss% 0.9677740177019944
plot_id,batch_id 0 73 miss% 0.9864078790946064
plot_id,batch_id 0 74 miss% 0.9802838164943466
plot_id,batch_id 0 75 miss% 0.951427792139484
plot_id,batch_id 0 76 miss% 0.9584757608231194
plot_id,batch_id 0 77 miss% 0.9833568173581453
plot_id,batch_id 0 78 miss% 0.9691105250184441
plot_id,batch_id 0 79 miss% 0.9761267747067405
plot_id,batch_id 0 80 miss% 0.9698441718709071
plot_id,batch_id 0 81 miss% 0.9728782444230385
plot_id,batch_id 0 82 miss% 0.9714798665807263
plot_id,batch_id 0 83 miss% 0.9693517203108712
plot_id,batch_id 0 84 miss% 0.9674006077057543
plot_id,batch_id 0 85 miss% 0.9580073531241682
plot_id,batch_id 0 86 miss% 0.9684150399216498
plot_id,batch_id 0 87 miss% 0.9703931018687435
plot_id,batch_id 0 88 miss% 0.9700887209346248
plot_id,batch_id 0 89 miss% 0.9722340437478832
plot_id,batch_id 0 90 miss% 0.9840668729482284
plot_id,batch_id 0 91 miss% 0.974705035477852
plot_id,batch_id 0 92 miss% 0.9845764015728263
plot_id,batch_id 0 93 miss% 0.9752511813542976
plot_id,batch_id 0 94 miss% 0.9751575012455528
plot_id,batch_id 0 95 miss% 0.9643524832538469
plot_id,batch_id 0 96 miss% 0.9788899007550003
plot_id,batch_id 0 97 miss% 0.9656876932022886
plot_id,batch_id 0 98 miss% 0.9815741164346344
plot_id,batch_id 0 99 miss% 0.9713413950852086
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.9693097  0.9691144  0.9724468  0.96964381 0.97142418 0.96141211
 0.97031591 0.97644499 0.97295783 0.97200267 0.96554184 0.97054628
 0.97092879 0.9747751  0.98008743 0.96960172 0.97651045 0.98547858
 0.98014298 0.98792978 0.9777908  0.97571072 0.97273053 0.96990355
 0.96963075 0.97981959 0.97714219 0.97743426 0.97404342 0.97883226
 0.96533491 0.97337239 0.97732318 0.98006062 0.97537902 0.99713749
 0.98149983 0.97423499 0.97930145 0.97530901 0.96808173 0.96880006
 0.96951704 0.96936825 0.97286748 0.97888642 0.96995534 0.97636153
 0.97372092 0.97428892 0.99120667 0.97391495 0.97872537 0.97512153
 0.9790348  0.98145251 0.98024182 0.97689558 0.97592531 0.97526926
 0.96430996 0.96490879 0.9805079  0.96424715 0.97542459 0.96286582
 0.97315254 0.98085347 0.97006954 0.97069105 0.95562589 0.97877099
 0.96777402 0.98640788 0.98028382 0.95142779 0.95847576 0.98335682
 0.96911053 0.97612677 0.96984417 0.97287824 0.97147987 0.96935172
 0.96740061 0.95800735 0.96841504 0.9703931  0.97008872 0.97223404
 0.98406687 0.97470504 0.9845764  0.97525118 0.9751575  0.96435248
 0.9788899  0.96568769 0.98157412 0.9713414 ]
for model  52 the mean error 0.973683283258923
all id 52 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 21 out win 5 err 0.973683283258923 time 17922.32038140297
Launcher: Job 53 completed in 18034 seconds.
Launcher: Task 88 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  265233
Epoch:0, Train loss:0.501245, valid loss:0.476277
Epoch:1, Train loss:0.124969, valid loss:0.002748
Epoch:2, Train loss:0.005358, valid loss:0.002099
Epoch:3, Train loss:0.004045, valid loss:0.001674
Epoch:4, Train loss:0.003283, valid loss:0.001550
Epoch:5, Train loss:0.002868, valid loss:0.001384
Epoch:6, Train loss:0.002480, valid loss:0.001230
Epoch:7, Train loss:0.002201, valid loss:0.001124
Epoch:8, Train loss:0.002043, valid loss:0.000969
Epoch:9, Train loss:0.001936, valid loss:0.001115
Epoch:10, Train loss:0.001813, valid loss:0.000986
Epoch:11, Train loss:0.001292, valid loss:0.000698
Epoch:12, Train loss:0.001222, valid loss:0.000733
Epoch:13, Train loss:0.001216, valid loss:0.000779
Epoch:14, Train loss:0.001164, valid loss:0.000650
Epoch:15, Train loss:0.001096, valid loss:0.000723
Epoch:16, Train loss:0.001079, valid loss:0.000751
Epoch:17, Train loss:0.001045, valid loss:0.000597
Epoch:18, Train loss:0.001020, valid loss:0.000654
Epoch:19, Train loss:0.001006, valid loss:0.000668
Epoch:20, Train loss:0.000971, valid loss:0.000697
Epoch:21, Train loss:0.000732, valid loss:0.000566
Epoch:22, Train loss:0.000719, valid loss:0.000542
Epoch:23, Train loss:0.000695, valid loss:0.000512
Epoch:24, Train loss:0.000702, valid loss:0.000567
Epoch:25, Train loss:0.000664, valid loss:0.000521
Epoch:26, Train loss:0.000683, valid loss:0.000561
Epoch:27, Train loss:0.000654, valid loss:0.000544
Epoch:28, Train loss:0.000644, valid loss:0.000543
Epoch:29, Train loss:0.000650, valid loss:0.000569
Epoch:30, Train loss:0.000620, valid loss:0.000566
Epoch:31, Train loss:0.000514, valid loss:0.000537
Epoch:32, Train loss:0.000507, valid loss:0.000490
Epoch:33, Train loss:0.000503, valid loss:0.000519
Epoch:34, Train loss:0.000496, valid loss:0.000493
Epoch:35, Train loss:0.000491, valid loss:0.000482
Epoch:36, Train loss:0.000483, valid loss:0.000535
Epoch:37, Train loss:0.000500, valid loss:0.000473
Epoch:38, Train loss:0.000476, valid loss:0.000562
Epoch:39, Train loss:0.000481, valid loss:0.000507
Epoch:40, Train loss:0.000457, valid loss:0.000514
Epoch:41, Train loss:0.000420, valid loss:0.000472
Epoch:42, Train loss:0.000416, valid loss:0.000463
Epoch:43, Train loss:0.000415, valid loss:0.000491
Epoch:44, Train loss:0.000416, valid loss:0.000463
Epoch:45, Train loss:0.000405, valid loss:0.000478
Epoch:46, Train loss:0.000406, valid loss:0.000467
Epoch:47, Train loss:0.000413, valid loss:0.000470
Epoch:48, Train loss:0.000407, valid loss:0.000469
Epoch:49, Train loss:0.000395, valid loss:0.000467
Epoch:50, Train loss:0.000392, valid loss:0.000479
Epoch:51, Train loss:0.000374, valid loss:0.000458
Epoch:52, Train loss:0.000371, valid loss:0.000457
Epoch:53, Train loss:0.000369, valid loss:0.000456
Epoch:54, Train loss:0.000368, valid loss:0.000450
Epoch:55, Train loss:0.000368, valid loss:0.000466
Epoch:56, Train loss:0.000368, valid loss:0.000454
Epoch:57, Train loss:0.000367, valid loss:0.000453
Epoch:58, Train loss:0.000367, valid loss:0.000450
Epoch:59, Train loss:0.000366, valid loss:0.000455
Epoch:60, Train loss:0.000366, valid loss:0.000450
training time 17848.74486875534
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333333
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.23405553046787894
plot_id,batch_id 0 1 miss% 0.35972289009648817
plot_id,batch_id 0 2 miss% 0.4487427737379009
plot_id,batch_id 0 3 miss% 0.3158581309204614
plot_id,batch_id 0 4 miss% 0.3176209656112176
plot_id,batch_id 0 5 miss% 0.2992708313481192
plot_id,batch_id 0 6 miss% 0.34370922804090204
plot_id,batch_id 0 7 miss% 0.46900444037620964
plot_id,batch_id 0 8 miss% 0.6595243419409764
plot_id,batch_id 0 9 miss% 0.46599767212834975
plot_id,batch_id 0 10 miss% 0.19360535763334297
plot_id,batch_id 0 11 miss% 0.32058080195079186
plot_id,batch_id 0 12 miss% 0.3649008261474267
plot_id,batch_id 0 13 miss% 0.2979596411021898
plot_id,batch_id 0 14 miss% 0.42163920374162317
plot_id,batch_id 0 15 miss% 0.25867776623419914
plot_id,batch_id 0 16 miss% 0.43476171837042316
plot_id,batch_id 0 17 miss% 0.37146420688001214
plot_id,batch_id 0 18 miss% 0.4502192491852669
plot_id,batch_id 0 19 miss% 0.34902443920478343
plot_id,batch_id 0 20 miss% 0.3004345504796215
plot_id,batch_id 0 21 miss% 0.490346243125509
plot_id,batch_id 0 22 miss% 0.4651920606814596
plot_id,batch_id 0 23 miss% 0.42662004663680236
plot_id,batch_id 0 24 miss% 0.4018570203958409
plot_id,batch_id 0 25 miss% 0.34632980669985636
plot_id,batch_id 0 26 miss% 0.3992644319801271
plot_id,batch_id 0 27 miss% 0.34385785226604243
plot_id,batch_id 0 28 miss% 0.3975277464154551
plot_id,batch_id 0 29 miss% 0.42266267974115157
plot_id,batch_id 0 30 miss% 0.22978420332225102
plot_id,batch_id 0 31 miss% 0.5086179660885628
plot_id,batch_id 0 32 miss% 0.40873983821573684
plot_id,batch_id 0 33 miss% 0.44661010866718975
plot_id,batch_id 0 34 miss% 0.4129309679764253
plot_id,batch_id 0 35 miss% 0.2554984706939247
plot_id,batch_id 0 36 miss% 0.5424617666453261
plot_id,batch_id 0 37 miss% 0.35639012113463553
plot_id,batch_id 0 38 miss% 0.42009712265424637
plot_id,batch_id 0 39 miss% 0.3317872961950953
plot_id,batch_id 0 40 miss% 0.3388117279964254
plot_id,batch_id 0 41 miss% 0.3617130633634191
plot_id,batch_id 0 42 miss% 0.3538753893389004
plot_id,batch_id 0 43 miss% 0.3194348764132872
plot_id,batch_id 0 44 miss% 0.2631333286732055
plot_id,batch_id 0 45 miss% 0.33937091463885566
plot_id,batch_id 0 46 miss% 0.37160244513429536
plot_id,batch_id 0 47 miss% 0.39883940560317815
plot_id,batch_id 0 48 miss% 0.33433001761289255
plot_id,batch_id 0 49 miss% 0.32501879221141694
plot_id,batch_id 0 50 miss% 0.42894526259764604
plot_id,batch_id 0 51 miss% 0.4584409757689109
plot_id,batch_id 0 52 miss% 0.43647440325343606
plot_id,batch_id 0 53 miss% 0.2863400837606238
plot_id,batch_id 0 54 miss% 0.2383786769515651
plot_id,batch_id 0 55 miss% 0.4172865327506854
plot_id,batch_id 0 56 miss% 0.4619819762874742
plot_id,batch_id 0 57 miss% 0.3543596963091804
plot_id,batch_id 0 58 miss% 0.3510086541484745
plot_id,batch_id 0 59 miss% 0.3378277021089577
plot_id,batch_id 0 60 miss% 0.1905797526385589
plot_id,batch_id 0 61 miss% 0.22048443158573056
plot_id,batch_id 0 62 miss% 0.44008400888102656
plot_id,batch_id 0 63 miss% 0.3000839232751868
plot_id,batch_id 0 64 miss% 0.3149951564265079
plot_id,batch_id 0 65 miss% 0.22667393021169185
plot_id,batch_id 0 66 miss% 0.30087425352325964
plot_id,batch_id 0 67 miss% 0.2199543580767956
plot_id,batch_id 0 67 miss% 0.23964043243198072
plot_id,batch_id 0 68 miss% 0.3934589605256964
plot_id,batch_id 0 69 miss% 0.3982172406868395
plot_id,batch_id 0 70 miss% 0.2471865232362193
plot_id,batch_id 0 71 miss% 0.29377809413029116
plot_id,batch_id 0 72 miss% 0.33933947925129104
plot_id,batch_id 0 73 miss% 0.38008090139171363
plot_id,batch_id 0 74 miss% 0.278999107062268
plot_id,batch_id 0 75 miss% 0.19816527057229819
plot_id,batch_id 0 76 miss% 0.30496850503440975
plot_id,batch_id 0 77 miss% 0.3177376638844614
plot_id,batch_id 0 78 miss% 0.28618305564379737
plot_id,batch_id 0 79 miss% 0.2684845419354339
plot_id,batch_id 0 80 miss% 0.24693579470258764
plot_id,batch_id 0 81 miss% 0.4193811382122762
plot_id,batch_id 0 82 miss% 0.3251546494394358
plot_id,batch_id 0 83 miss% 0.4306469059266423
plot_id,batch_id 0 84 miss% 0.3316971203330827
plot_id,batch_id 0 85 miss% 0.1997364784814867
plot_id,batch_id 0 86 miss% 0.3738129041645576
plot_id,batch_id 0 87 miss% 0.32367872111069407
plot_id,batch_id 0 88 miss% 0.4365580955016245
plot_id,batch_id 0 89 miss% 0.4011682579496121
plot_id,batch_id 0 90 miss% 0.17195076267620507
plot_id,batch_id 0 91 miss% 0.30594429121044725
plot_id,batch_id 0 92 miss% 0.31804491043024985
plot_id,batch_id 0 93 miss% 0.24327382698292632
plot_id,batch_id 0 94 miss% 0.5068795321776266
plot_id,batch_id 0 95 miss% 0.19450417162198697
plot_id,batch_id 0 96 miss% 0.30179459566762906
plot_id,batch_id 0 97 miss% 0.4262062011963991
plot_id,batch_id 0 98 miss% 0.37851678623793866
plot_id,batch_id 0 99 miss% 0.3269668618493099
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.27796004 0.36102284 0.34321187 0.33225306 0.26974664 0.28137121
 0.272841   0.45443432 0.43911253 0.30631821 0.22705955 0.30918783
 0.34105016 0.30288483 0.40398786 0.2909001  0.45599496 0.42908823
 0.46206668 0.36520101 0.31038557 0.3167222  0.26825736 0.32484193
 0.24215002 0.31721637 0.36148205 0.37640895 0.28809086 0.35170349
 0.26154655 0.40965836 0.43204179 0.3126644  0.34735633 0.28145193
 0.45902747 0.41546799 0.37338987 0.34695836 0.43500411 0.35835573
 0.28497988 0.29636322 0.23411281 0.19196801 0.24241639 0.38799433
 0.38407196 0.28045183 0.29462079 0.38446697 0.33971706 0.28987842
 0.40881816 0.43358369 0.44519466 0.34243447 0.29543134 0.33669059
 0.21780447 0.23428103 0.27305298 0.37267137 0.35486319 0.24740624
 0.31713966 0.23964043 0.39345896 0.39821724 0.24718652 0.29377809
 0.33933948 0.3800809  0.27899911 0.19816527 0.30496851 0.31773766
 0.28618306 0.26848454 0.24693579 0.41938114 0.32515465 0.43064691
 0.33169712 0.19973648 0.3738129  0.32367872 0.4365581  0.40116826
 0.17195076 0.30594429 0.31804491 0.24327383 0.50687953 0.19450417
 0.3017946  0.4262062  0.37851679 0.32696686]
for model  229 the mean error 0.33015379947970613
all id 229 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 31 out win 5 err 0.33015379947970613 time 17803.50999736786
Launcher: Job 230 completed in 18038 seconds.
Launcher: Task 2 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  154129
Epoch:0, Train loss:0.406906, valid loss:0.413894
Epoch:1, Train loss:0.047904, valid loss:0.003994
Epoch:2, Train loss:0.010080, valid loss:0.002021
Epoch:3, Train loss:0.004987, valid loss:0.001314
Epoch:4, Train loss:0.002594, valid loss:0.001156
Epoch:5, Train loss:0.002295, valid loss:0.001098
Epoch:6, Train loss:0.002109, valid loss:0.000996
Epoch:7, Train loss:0.001958, valid loss:0.000899
Epoch:8, Train loss:0.001810, valid loss:0.001025
Epoch:9, Train loss:0.001711, valid loss:0.000833
Epoch:10, Train loss:0.001607, valid loss:0.000857
Epoch:11, Train loss:0.001237, valid loss:0.000719
Epoch:12, Train loss:0.001145, valid loss:0.000689
Epoch:13, Train loss:0.001100, valid loss:0.000614
Epoch:14, Train loss:0.001092, valid loss:0.000628
Epoch:15, Train loss:0.001050, valid loss:0.000645
Epoch:16, Train loss:0.001029, valid loss:0.000748
Epoch:17, Train loss:0.000962, valid loss:0.001030
Epoch:18, Train loss:0.000967, valid loss:0.000633
Epoch:19, Train loss:0.000908, valid loss:0.000647
Epoch:20, Train loss:0.000896, valid loss:0.000602
Epoch:21, Train loss:0.000689, valid loss:0.000570
Epoch:22, Train loss:0.000659, valid loss:0.000491
Epoch:23, Train loss:0.000671, valid loss:0.000499
Epoch:24, Train loss:0.000641, valid loss:0.000567
Epoch:25, Train loss:0.000629, valid loss:0.000453
Epoch:26, Train loss:0.000616, valid loss:0.000468
Epoch:27, Train loss:0.000605, valid loss:0.000472
Epoch:28, Train loss:0.000589, valid loss:0.000474
Epoch:29, Train loss:0.000607, valid loss:0.000514
Epoch:30, Train loss:0.000581, valid loss:0.000496
Epoch:31, Train loss:0.000487, valid loss:0.000424
Epoch:32, Train loss:0.000466, valid loss:0.000496
Epoch:33, Train loss:0.000467, valid loss:0.000447
Epoch:34, Train loss:0.000453, valid loss:0.000433
Epoch:35, Train loss:0.000459, valid loss:0.000431
Epoch:36, Train loss:0.000446, valid loss:0.000441
Epoch:37, Train loss:0.000441, valid loss:0.000496
Epoch:38, Train loss:0.000440, valid loss:0.000443
Epoch:39, Train loss:0.000434, valid loss:0.000453
Epoch:40, Train loss:0.000433, valid loss:0.000408
Epoch:41, Train loss:0.000382, valid loss:0.000400
Epoch:42, Train loss:0.000379, valid loss:0.000433
Epoch:43, Train loss:0.000377, valid loss:0.000398
Epoch:44, Train loss:0.000374, valid loss:0.000414
Epoch:45, Train loss:0.000375, valid loss:0.000406
Epoch:46, Train loss:0.000367, valid loss:0.000429
Epoch:47, Train loss:0.000366, valid loss:0.000413
Epoch:48, Train loss:0.000363, valid loss:0.000422
Epoch:49, Train loss:0.000362, valid loss:0.000399
Epoch:50, Train loss:0.000360, valid loss:0.000395
Epoch:51, Train loss:0.000340, valid loss:0.000395
Epoch:52, Train loss:0.000337, valid loss:0.000400
Epoch:53, Train loss:0.000335, valid loss:0.000393
Epoch:54, Train loss:0.000334, valid loss:0.000397
Epoch:55, Train loss:0.000334, valid loss:0.000391
Epoch:56, Train loss:0.000333, valid loss:0.000391
Epoch:57, Train loss:0.000333, valid loss:0.000393
Epoch:58, Train loss:0.000333, valid loss:0.000391
Epoch:59, Train loss:0.000332, valid loss:0.000392
Epoch:60, Train loss:0.000332, valid loss:0.000392
training time 17867.972073316574
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08227203014275318
plot_id,batch_id 0 1 miss% 0.031548231852974076
plot_id,batch_id 0 2 miss% 0.09034119802413988
plot_id,batch_id 0 3 miss% 0.03656247176231519
plot_id,batch_id 0 4 miss% 0.0435264623416756
plot_id,batch_id 0 5 miss% 0.047920700374720876
plot_id,batch_id 0 6 miss% 0.035720245912816764
plot_id,batch_id 0 7 miss% 0.07274695961010086
plot_id,batch_id 0 8 miss% 0.05688075062722301
plot_id,batch_id 0 9 miss% 0.08831722054710238
plot_id,batch_id 0 10 miss% 0.02634090622879827
plot_id,batch_id 0 11 miss% 0.06639863801323885
plot_id,batch_id 0 12 miss% 0.06522254433368706
plot_id,batch_id 0 13 miss% 0.04727595208135637
plot_id,batch_id 0 14 miss% 0.08395299476374635
plot_id,batch_id 0 15 miss% 0.0414826663887323
plot_id,batch_id 0 16 miss% 0.09252510483147162
plot_id,batch_id 0 17 miss% 0.06344557217575895
plot_id,batch_id 0 18 miss% 0.07209470153928292
plot_id,batch_id 0 19 miss% 0.09789866279846873
plot_id,batch_id 0 20 miss% 0.06812089906647284
plot_id,batch_id 0 21 miss% 0.05074576162388666
plot_id,batch_id 0 22 miss% 0.051941033165301735
plot_id,batch_id 0 23 miss% 0.029767378236520627
plot_id,batch_id 0 24 miss% 0.06834621463885807
plot_id,batch_id 0 25 miss% 0.03668662683715643
plot_id,batch_id 0 26 miss% 0.040026761205509666
plot_id,batch_id 0 27 miss% 0.05192686287028691
plot_id,batch_id 0 28 miss% 0.04886894371415504
plot_id,batch_id 0 29 miss% 0.023850261271043168
plot_id,batch_id 0 30 miss% 0.047661449882501726
plot_id,batch_id 0 31 miss% 0.09018724347079561
plot_id,batch_id 0 32 miss% 0.07458063042751295
plot_id,batch_id 0 33 miss% 0.06347120416119124
plot_id,batch_id 0 34 miss% 0.037777817239930817
plot_id,batch_id 0 35 miss% 0.036914772493028075
plot_id,batch_id 0 36 miss% 0.07680730896906324
plot_id,batch_id 0 37 miss% 0.05467985387166343
plot_id,batch_id 0 38 miss% 0.02427602075308044
plot_id,batch_id 0 39 miss% 0.04984111730772426
plot_id,batch_id 0 40 miss% 0.04582849841705108
plot_id,batch_id 0 41 miss% 0.08181611330034351
plot_id,batch_id 0 42 miss% 0.02827434909311984
plot_id,batch_id 0 43 miss% 0.04678472998184248
plot_id,batch_id 0 44 miss% 0.017953131664006288
plot_id,batch_id 0 45 miss% 0.04825879715363942
plot_id,batch_id 0 46 miss% 0.016701760471047194
plot_id,batch_id 0 47 miss% 0.021230320180432964
plot_id,batch_id 0 48 miss% 0.037565179176115494
plot_id,batch_id 0 49 miss% 0.021186447437119404
plot_id,batch_id 0 50 miss% 0.15084029000180146
plot_id,batch_id 0 51 miss% 0.02881933850383637
plot_id,batch_id 0 52 miss% 0.04815442026495632
plot_id,batch_id 0 53 miss% 0.023851882351876998
plot_id,batch_id 0 54 miss% 0.018660340746775327
plot_id,batch_id 0 55 miss% 0.03933329983857502
plot_id,batch_id 0 56 miss% 0.10402449064851774
plot_id,batch_id 0 57 miss% 0.037292586392033235
plot_id,batch_id 0 58 miss% 0.03856693748747924
plot_id,batch_id 0 59 miss% 0.04107709538322741
plot_id,batch_id 0 60 miss% 0.039839394716072375
plot_id,batch_id 0 61 miss% 0.028790721002553095
plot_id,batch_id 0 62 miss% 0.0686093217959351
plot_id,batch_id 0 63 miss% 0.045661676434747285
plot_id,batch_id 0 64 miss% 0.05538842706093075
plot_id,batch_id 0 65 miss% 0.04459083903680965
plot_id,batch_id 0 66 miss% 0.0227799336645605
plot_id,batch_id 0 67 plot_id,batch_id 0 68 miss% 0.3730580421081301
plot_id,batch_id 0 69 miss% 0.39457457677658564
plot_id,batch_id 0 70 miss% 0.2175777620040405
plot_id,batch_id 0 71 miss% 0.30090671665831953
plot_id,batch_id 0 72 miss% 0.38077735775968446
plot_id,batch_id 0 73 miss% 0.21953666749918815
plot_id,batch_id 0 74 miss% 0.3145841744803097
plot_id,batch_id 0 75 miss% 0.15502666648620372
plot_id,batch_id 0 76 miss% 0.2181736238104513
plot_id,batch_id 0 77 miss% 0.2195985888439788
plot_id,batch_id 0 78 miss% 0.23864471175458207
plot_id,batch_id 0 79 miss% 0.36382178308884966
plot_id,batch_id 0 80 miss% 0.21835318251515007
plot_id,batch_id 0 81 miss% 0.43888783632143624
plot_id,batch_id 0 82 miss% 0.3134011521281192
plot_id,batch_id 0 83 miss% 0.37557761876377077
plot_id,batch_id 0 84 miss% 0.3723129654516051
plot_id,batch_id 0 85 miss% 0.2050149433175425
plot_id,batch_id 0 86 miss% 0.31587706140270666
plot_id,batch_id 0 87 miss% 0.3699766150831199
plot_id,batch_id 0 88 miss% 0.3769558850245963
plot_id,batch_id 0 89 miss% 0.3430356336165923
plot_id,batch_id 0 90 miss% 0.15866354106342312
plot_id,batch_id 0 91 miss% 0.2422200564134414
plot_id,batch_id 0 92 miss% 0.24183468840604674
plot_id,batch_id 0 93 miss% 0.2555105913888239
plot_id,batch_id 0 94 miss% 0.402245743952798
plot_id,batch_id 0 95 miss% 0.21318510932472978
plot_id,batch_id 0 96 miss% 0.26194681802663533
plot_id,batch_id 0 97 miss% 0.31992529464527814
plot_id,batch_id 0 98 miss% 0.39989850092734536
plot_id,batch_id 0 99 miss% 0.36791267126983546
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.23405553 0.35972289 0.44874277 0.31585813 0.31762097 0.29927083
 0.34370923 0.46900444 0.65952434 0.46599767 0.19360536 0.3205808
 0.36490083 0.29795964 0.4216392  0.25867777 0.43476172 0.37146421
 0.45021925 0.34902444 0.30043455 0.49034624 0.46519206 0.42662005
 0.40185702 0.34632981 0.39926443 0.34385785 0.39752775 0.42266268
 0.2297842  0.50861797 0.40873984 0.44661011 0.41293097 0.25549847
 0.54246177 0.35639012 0.42009712 0.3317873  0.33881173 0.36171306
 0.35387539 0.31943488 0.26313333 0.33937091 0.37160245 0.39883941
 0.33433002 0.32501879 0.42894526 0.45844098 0.4364744  0.28634008
 0.23837868 0.41728653 0.46198198 0.3543597  0.35100865 0.3378277
 0.19057975 0.22048443 0.44008401 0.30008392 0.31499516 0.22667393
 0.30087425 0.21995436 0.37305804 0.39457458 0.21757776 0.30090672
 0.38077736 0.21953667 0.31458417 0.15502667 0.21817362 0.21959859
 0.23864471 0.36382178 0.21835318 0.43888784 0.31340115 0.37557762
 0.37231297 0.20501494 0.31587706 0.36997662 0.37695589 0.34303563
 0.15866354 0.24222006 0.24183469 0.25551059 0.40224574 0.21318511
 0.26194682 0.31992529 0.3998985  0.36791267]
for model  105 the mean error 0.34263270634988735
all id 105 hidden_dim 32 learning_rate 0.0025 num_layers 5 frames 25 out win 4 err 0.34263270634988735 time 17848.74486875534
Launcher: Job 106 completed in 18079 seconds.
Launcher: Task 139 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 28800
total number of trained parameters  265233
Epoch:0, Train loss:0.657663, valid loss:0.627175
Epoch:1, Train loss:0.265583, valid loss:0.004990
Epoch:2, Train loss:0.013553, valid loss:0.004454
Epoch:3, Train loss:0.009455, valid loss:0.003316
Epoch:4, Train loss:0.006525, valid loss:0.002504
Epoch:5, Train loss:0.004882, valid loss:0.002195
Epoch:6, Train loss:0.004159, valid loss:0.001878
Epoch:7, Train loss:0.003823, valid loss:0.002257
Epoch:8, Train loss:0.003488, valid loss:0.001707
Epoch:9, Train loss:0.003077, valid loss:0.001691
Epoch:10, Train loss:0.002926, valid loss:0.001739
Epoch:11, Train loss:0.002149, valid loss:0.001223
Epoch:12, Train loss:0.001957, valid loss:0.001192
Epoch:13, Train loss:0.001969, valid loss:0.001159
Epoch:14, Train loss:0.001787, valid loss:0.001123
Epoch:15, Train loss:0.001812, valid loss:0.001069
Epoch:16, Train loss:0.001688, valid loss:0.001175
Epoch:17, Train loss:0.001627, valid loss:0.001003
Epoch:18, Train loss:0.001538, valid loss:0.001046
Epoch:19, Train loss:0.001491, valid loss:0.000986
Epoch:20, Train loss:0.001443, valid loss:0.001048
Epoch:21, Train loss:0.001092, valid loss:0.001006
Epoch:22, Train loss:0.001083, valid loss:0.000848
Epoch:23, Train loss:0.001023, valid loss:0.000925
Epoch:24, Train loss:0.001016, valid loss:0.000919
Epoch:25, Train loss:0.000992, valid loss:0.000912
Epoch:26, Train loss:0.000966, valid loss:0.000828
Epoch:27, Train loss:0.000960, valid loss:0.000895
Epoch:28, Train loss:0.000922, valid loss:0.000947
Epoch:29, Train loss:0.000922, valid loss:0.000831
Epoch:30, Train loss:0.000881, valid loss:0.000849
Epoch:31, Train loss:0.000720, valid loss:0.000820
Epoch:32, Train loss:0.000708, valid loss:0.000785
Epoch:33, Train loss:0.000698, valid loss:0.000847
Epoch:34, Train loss:0.000693, valid loss:0.000790
Epoch:35, Train loss:0.000701, valid loss:0.000777
Epoch:36, Train loss:0.000689, valid loss:0.000756
Epoch:37, Train loss:0.000670, valid loss:0.000772
Epoch:38, Train loss:0.000658, valid loss:0.000828
Epoch:39, Train loss:0.000643, valid loss:0.000818
Epoch:40, Train loss:0.000644, valid loss:0.000841
Epoch:41, Train loss:0.000566, valid loss:0.000758
Epoch:42, Train loss:0.000555, valid loss:0.000772
Epoch:43, Train loss:0.000554, valid loss:0.000775
Epoch:44, Train loss:0.000555, valid loss:0.000774
Epoch:45, Train loss:0.000551, valid loss:0.000763
Epoch:46, Train loss:0.000544, valid loss:0.000770
Epoch:47, Train loss:0.000537, valid loss:0.000760
Epoch:48, Train loss:0.000532, valid loss:0.000746
Epoch:49, Train loss:0.000529, valid loss:0.000775
Epoch:50, Train loss:0.000524, valid loss:0.000764
Epoch:51, Train loss:0.000486, valid loss:0.000740
Epoch:52, Train loss:0.000483, valid loss:0.000736
Epoch:53, Train loss:0.000481, valid loss:0.000740
Epoch:54, Train loss:0.000480, valid loss:0.000737
Epoch:55, Train loss:0.000480, valid loss:0.000734
Epoch:56, Train loss:0.000478, valid loss:0.000748
Epoch:57, Train loss:0.000478, valid loss:0.000731
Epoch:58, Train loss:0.000478, valid loss:0.000740
Epoch:59, Train loss:0.000478, valid loss:0.000733
Epoch:60, Train loss:0.000477, valid loss:0.000736
training time 17892.889323472977
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3901477246943328
plot_id,batch_id 0 1 miss% 0.42731644869566004
plot_id,batch_id 0 2 miss% 0.49346709236488484
plot_id,batch_id 0 3 miss% 0.3691558592939476
plot_id,batch_id 0 4 miss% 0.37417891915141577
plot_id,batch_id 0 5 miss% 0.4220441758870302
plot_id,batch_id 0 6 miss% 0.4143337096444407
plot_id,batch_id 0 7 miss% 0.5265335395989715
plot_id,batch_id 0 8 miss% 0.6591062285920256
plot_id,batch_id 0 9 miss% 0.6094612330654954
plot_id,batch_id 0 10 miss% 0.34783836710297333
plot_id,batch_id 0 11 miss% 0.38554253083182133
plot_id,batch_id 0 12 miss% 0.4485289609270812
plot_id,batch_id 0 13 miss% 0.4128169252658598
plot_id,batch_id 0 14 miss% 0.5212588145688933
plot_id,batch_id 0 15 miss% 0.3952461451332738
plot_id,batch_id 0 16 miss% 0.5474756762344575
plot_id,batch_id 0 17 miss% 0.5037144077026001
plot_id,batch_id 0 18 miss% 0.5401398325331132
plot_id,batch_id 0 19 miss% 0.43074616286436496
plot_id,batch_id 0 20 miss% 0.41583993845662287
plot_id,batch_id 0 21 miss% 0.4861047769464402
plot_id,batch_id 0 22 miss% 0.5024848398487515
plot_id,batch_id 0 23 miss% 0.48765460350768336
plot_id,batch_id 0 24 miss% 0.42559604770050813
plot_id,batch_id 0 25 miss% 0.43462495518466043
plot_id,batch_id 0 26 miss% 0.49778135464537326
plot_id,batch_id 0 27 miss% 0.3892176943719771
plot_id,batch_id 0 28 miss% 0.4709703839394711
plot_id,batch_id 0 29 miss% 0.5192805405524751
plot_id,batch_id 0 30 miss% 0.42447592210453206
plot_id,batch_id 0 31 miss% 0.559495676508246
plot_id,batch_id 0 32 miss% 0.4664080380367337
plot_id,batch_id 0 33 miss% 0.5330216316140272
plot_id,batch_id 0 34 miss% 0.4045883317153294
plot_id,batch_id 0 35 miss% 0.404743269680179
plot_id,batch_id 0 36 miss% 0.5803561857784126
plot_id,batch_id 0 37 miss% 0.4542070630808238
plot_id,batch_id 0 38 miss% 0.5175112623805211
plot_id,batch_id 0 39 miss% 0.47880058415473903
plot_id,batch_id 0 40 miss% 0.42964794822835695
plot_id,batch_id 0 41 miss% 0.5074217088886769
plot_id,batch_id 0 42 miss% 0.40686761352836487
plot_id,batch_id 0 43 miss% 0.4672584639228808
plot_id,batch_id 0 44 miss% 0.3902693156111912
plot_id,batch_id 0 45 miss% 0.45574227757417574
plot_id,batch_id 0 46 miss% 0.4354917430758559
plot_id,batch_id 0 47 miss% 0.5008167334667387
plot_id,batch_id 0 48 miss% 0.46845133282915374
plot_id,batch_id 0 49 miss% 0.3894764851567982
plot_id,batch_id 0 50 miss% 0.5630593926554204
plot_id,batch_id 0 51 miss% 0.5383143431308306
plot_id,batch_id 0 52 miss% 0.5205645184710117
plot_id,batch_id 0 53 miss% 0.4307120233450512
plot_id,batch_id 0 54 miss% 0.4003203641793525
plot_id,batch_id 0 55 miss% 0.4378180999125508
plot_id,batch_id 0 56 miss% 0.5471017044902974
plot_id,batch_id 0 57 miss% 0.538602956344832
plot_id,batch_id 0 58 miss% 0.459161012070877
plot_id,batch_id 0 59 miss% 0.48508563507813623
plot_id,batch_id 0 60 miss% 0.3381483648571925
plot_id,batch_id 0 61 miss% 0.35700212914778245
plot_id,batch_id 0 62 miss% 0.5084576537571167
plot_id,batch_id 0 63 miss% 0.42997163543016875
plot_id,batch_id 0 64 miss% 0.41829950468965943
plot_id,batch_id 0 65 miss% 0.3741344571880188
plot_id,batch_id 0 66 miss% 0.46355354504811364
plot_id,batch_id 0 67 miss% 0.3627536563183623
plot_id,batch_id 0 68 miss% 0.4842838874965432
plot_id,batch_id 0 69 miss% 0.47392660878189685
plot_id,batch_id 0 70 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  265233
Epoch:0, Train loss:0.646980, valid loss:0.615764
Epoch:1, Train loss:0.360261, valid loss:0.005398
Epoch:2, Train loss:0.013473, valid loss:0.004735
Epoch:3, Train loss:0.011127, valid loss:0.004122
Epoch:4, Train loss:0.010309, valid loss:0.003359
Epoch:5, Train loss:0.007660, valid loss:0.002948
Epoch:6, Train loss:0.005614, valid loss:0.002762
Epoch:7, Train loss:0.005125, valid loss:0.002795
Epoch:8, Train loss:0.004757, valid loss:0.002314
Epoch:9, Train loss:0.004456, valid loss:0.001960
Epoch:10, Train loss:0.004275, valid loss:0.001937
Epoch:11, Train loss:0.003181, valid loss:0.001633
Epoch:12, Train loss:0.002936, valid loss:0.001499
Epoch:13, Train loss:0.002879, valid loss:0.001620
Epoch:14, Train loss:0.002747, valid loss:0.001671
Epoch:15, Train loss:0.002602, valid loss:0.001435
Epoch:16, Train loss:0.002510, valid loss:0.001578
Epoch:17, Train loss:0.002608, valid loss:0.001382
Epoch:18, Train loss:0.002420, valid loss:0.001581
Epoch:19, Train loss:0.002284, valid loss:0.001346
Epoch:20, Train loss:0.002193, valid loss:0.001260
Epoch:21, Train loss:0.001695, valid loss:0.001216
Epoch:22, Train loss:0.001546, valid loss:0.001092
Epoch:23, Train loss:0.001501, valid loss:0.001247
Epoch:24, Train loss:0.001496, valid loss:0.001105
Epoch:25, Train loss:0.001452, valid loss:0.001095
Epoch:26, Train loss:0.001414, valid loss:0.001100
Epoch:27, Train loss:0.001417, valid loss:0.001122
Epoch:28, Train loss:0.001347, valid loss:0.001233
Epoch:29, Train loss:0.001358, valid loss:0.001232
Epoch:30, Train loss:0.001290, valid loss:0.001058
Epoch:31, Train loss:0.001027, valid loss:0.000938
Epoch:32, Train loss:0.000959, valid loss:0.000975
Epoch:33, Train loss:0.000940, valid loss:0.000902
Epoch:34, Train loss:0.000918, valid loss:0.000967
Epoch:35, Train loss:0.000917, valid loss:0.000965
Epoch:36, Train loss:0.000875, valid loss:0.000990
Epoch:37, Train loss:0.000873, valid loss:0.000892
Epoch:38, Train loss:0.000845, valid loss:0.000943
Epoch:39, Train loss:0.000849, valid loss:0.001010
Epoch:40, Train loss:0.000813, valid loss:0.001038
Epoch:41, Train loss:0.000706, valid loss:0.000952
Epoch:42, Train loss:0.000670, valid loss:0.000933
Epoch:43, Train loss:0.000664, valid loss:0.000880
Epoch:44, Train loss:0.000647, valid loss:0.000941
Epoch:45, Train loss:0.000664, valid loss:0.000936
Epoch:46, Train loss:0.000641, valid loss:0.000923
Epoch:47, Train loss:0.000633, valid loss:0.000979
Epoch:48, Train loss:0.000608, valid loss:0.000942
Epoch:49, Train loss:0.000613, valid loss:0.000911
Epoch:50, Train loss:0.000614, valid loss:0.001034
Epoch:51, Train loss:0.000562, valid loss:0.000905
Epoch:52, Train loss:0.000550, valid loss:0.000900
Epoch:53, Train loss:0.000547, valid loss:0.000896
Epoch:54, Train loss:0.000544, valid loss:0.000898
Epoch:55, Train loss:0.000542, valid loss:0.000893
Epoch:56, Train loss:0.000541, valid loss:0.000890
Epoch:57, Train loss:0.000539, valid loss:0.000889
Epoch:58, Train loss:0.000538, valid loss:0.000890
Epoch:59, Train loss:0.000538, valid loss:0.000890
Epoch:60, Train loss:0.000537, valid loss:0.000890
training time 17895.70427417755
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.4357627631009801
plot_id,batch_id 0 1 miss% 0.44799852260557166
plot_id,batch_id 0 2 miss% 0.4604724637288468
plot_id,batch_id 0 3 miss% 0.3555480016099034
plot_id,batch_id 0 4 miss% 0.401259675425475
plot_id,batch_id 0 5 miss% 0.48445362155837823
plot_id,batch_id 0 6 miss% 0.47606923403674706
plot_id,batch_id 0 7 miss% 0.5474186942220347
plot_id,batch_id 0 8 miss% 0.5322916421595666
plot_id,batch_id 0 9 miss% 0.5447473928041836
plot_id,batch_id 0 10 miss% 0.4334796297391815
plot_id,batch_id 0 11 miss% 0.4075358980519975
plot_id,batch_id 0 12 miss% 0.5177414515606731
plot_id,batch_id 0 13 miss% 0.40203088002301807
plot_id,batch_id 0 14 miss% 0.496493936592208
plot_id,batch_id 0 15 miss% 0.5011119632713198
plot_id,batch_id 0 16 miss% 0.5506181611473407
plot_id,batch_id 0 17 miss% 0.6048012746522511
plot_id,batch_id 0 18 miss% 0.5328613842022295
plot_id,batch_id 0 19 miss% 0.521889514121633
plot_id,batch_id 0 20 miss% 0.44385073691217425
plot_id,batch_id 0 21 miss% 0.4570221160100495
plot_id,batch_id 0 22 miss% 0.45366547419430675
plot_id,batch_id 0 23 miss% 0.3979423215231141
plot_id,batch_id 0 24 miss% 0.3669751237317399
plot_id,batch_id 0 25 miss% 0.4588849598081579
plot_id,batch_id 0 26 miss% 0.4329697402495
plot_id,batch_id 0 27 miss% 0.42119923836369905
plot_id,batch_id 0 28 miss% 0.3991462508895055
plot_id,batch_id 0 29 miss% 0.4690652030029799
plot_id,batch_id 0 30 miss% 0.4564675629833193
plot_id,batch_id 0 31 miss% 0.6029820545267479
plot_id,batch_id 0 32 miss% 0.4826556295726169
plot_id,batch_id 0 33 miss% 0.504836785947391
plot_id,batch_id 0 34 miss% 0.425668980965889
plot_id,batch_id 0 35 miss% 0.5013668511117173
plot_id,batch_id 0 36 miss% 0.661764132331588
plot_id,batch_id 0 37 miss% 0.49194485929520104
plot_id,batch_id 0 38 miss% 0.5543724014300927
plot_id,batch_id 0 39 miss% 0.4536775044699656
plot_id,batch_id 0 40 miss% 0.3720389450257429
plot_id,batch_id 0 41 miss% 0.4620658214313716
plot_id,batch_id 0 42 miss% 0.31725734688101437
plot_id,batch_id 0 43 miss% 0.3836087827089516
plot_id,batch_id 0 44 miss% 0.3604839717204716
plot_id,batch_id 0 45 miss% 0.43905530662148035
plot_id,batch_id 0 46 miss% 0.47979709306002405
plot_id,batch_id 0 47 miss% 0.411058872850449
plot_id,batch_id 0 48 miss% 0.38512224250350807
plot_id,batch_id 0 49 miss% 0.3872049627595905
plot_id,batch_id 0 50 miss% 0.5328940419962241
plot_id,batch_id 0 51 miss% 0.4828483442989065
plot_id,batch_id 0 52 miss% 0.4987680833988288
plot_id,batch_id 0 53 miss% 0.3546511462132233
plot_id,batch_id 0 54 miss% 0.2866722710661008
plot_id,batch_id 0 55 miss% 0.5004994624553374
plot_id,batch_id 0 56 miss% 0.44647877265932956
plot_id,batch_id 0 57 miss% 0.48483481548916835
plot_id,batch_id 0 58 miss% 0.4405194056328322
plot_id,batch_id 0 59 miss% 0.4478777529925624
plot_id,batch_id 0 60 miss% 0.39776232620209195
plot_id,batch_id 0 61 miss% 0.3682030417042455
plot_id,batch_id 0 62 miss% 0.4372209111717343
plot_id,batch_id 0 63 miss% 0.4180659397516004
plot_id,batch_id 0 64 miss% 0.4498768356879649
plot_id,batch_id 0 65 miss% 0.44742181789313124
plot_id,batch_id 0 66 miss% 0.5332840331025333
plot_id,batch_id 0 67 miss% 0.381773267723159
plot_id,batch_id 0 68 miss% 0.4784306308809695
plot_id,batch_id 0 69 miss% 0.5226566040478963
plot_id,batch_id 0 70 miss% 0.3658749954222347
plot_id,batch_id 0 71 miss% 0.49813221682627407
plot_id,batch_id 0 72 miss% 0.47039963310364563
plot_id,batch_id 0 73 miss% 0.47599377673236226
plot_id,batch_id 0 74 miss% 0.5398765069991985
plot_id,batch_id 0 75 miss% 0.3382826862857302
plot_id,batch_id 0 76 miss% 0.5101271632524245
plot_id,batch_id 0 77 miss% 0.42008350683029094
plot_id,batch_id 0 78 miss% 0.41586753846698943
plot_id,batch_id 0 79 miss% 0.36999440193314737
plot_id,batch_id 0 80 miss% 0.39418431350914485
plot_id,batch_id 0 81 miss% 0.5020669451819162
plot_id,batch_id 0 82 miss% 0.39721793278536466
plot_id,batch_id 0 83 miss% 0.48080939783159465
plot_id,batch_id 0 84 miss% 0.4923525734498195
plot_id,batch_id 0 85 miss% 0.34868611247375125
plot_id,batch_id 0 86 miss% 0.46567176008757305
plot_id,batch_id 0 87 miss% 0.5055280974258459
plot_id,batch_id 0 88 miss% 0.5043267712323632
plot_id,batch_id 0 89 miss% 0.4539077099978382
plot_id,batch_id 0 90 miss% 0.3473697196170606
plot_id,batch_id 0 91 miss% 0.39856012131488266
plot_id,batch_id 0 92 miss% 0.3898071850034465
plot_id,batch_id 0 93 miss% 0.3531714481994649
plot_id,batch_id 0 94 miss% 0.4818149605055814
plot_id,batch_id 0 95 miss% 0.3635899264378139
plot_id,batch_id 0 96 miss% 0.4473574045994549
plot_id,batch_id 0 97 miss% 0.5517960040196155
plot_id,batch_id 0 98 miss% 0.5179760660614057
plot_id,batch_id 0 99 miss% 0.42375205383082004
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.43576276 0.44799852 0.46047246 0.355548   0.40125968 0.48445362
 0.47606923 0.54741869 0.53229164 0.54474739 0.43347963 0.4075359
 0.51774145 0.40203088 0.49649394 0.50111196 0.55061816 0.60480127
 0.53286138 0.52188951 0.44385074 0.45702212 0.45366547 0.39794232
 0.36697512 0.45888496 0.43296974 0.42119924 0.39914625 0.4690652
 0.45646756 0.60298205 0.48265563 0.50483679 0.42566898 0.50136685
 0.66176413 0.49194486 0.5543724  0.4536775  0.37203895 0.46206582
 0.31725735 0.38360878 0.36048397 0.43905531 0.47979709 0.41105887
 0.38512224 0.38720496 0.53289404 0.48284834 0.49876808 0.35465115
 0.28667227 0.50049946 0.44647877 0.48483482 0.44051941 0.44787775
 0.39776233 0.36820304 0.43722091 0.41806594 0.44987684 0.44742182
 0.53328403 0.38177327 0.47843063 0.5226566  0.365875   0.49813222
 0.47039963 0.47599378 0.53987651 0.33828269 0.51012716 0.42008351
 0.41586754 0.3699944  0.39418431 0.50206695 0.39721793 0.4808094
 0.49235257 0.34868611 0.46567176 0.5055281  0.50432677 0.45390771
 0.34736972 0.39856012 0.38980719 0.35317145 0.48181496 0.36358993
 0.4473574  0.551796   0.51797607 0.42375205]
for model  80 the mean error 0.452240558112808
all id 80 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 21 out win 6 err 0.452240558112808 time 17895.70427417755
miss% 0.03426882018197197
plot_id,batch_id 0 68 miss% 0.053622042519977806
plot_id,batch_id 0 69 miss% 0.0626070058549813
plot_id,batch_id 0 70 miss% 0.05687566383382853
plot_id,batch_id 0 71 miss% 0.04365449618697794
plot_id,batch_id 0 72 miss% 0.0780447100490047
plot_id,batch_id 0 73 miss% 0.07384810865862788
plot_id,batch_id 0 74 miss% 0.05782869195202111
plot_id,batch_id 0 75 miss% 0.02506159149811712
plot_id,batch_id 0 76 miss% 0.11713630804030337
plot_id,batch_id 0 77 miss% 0.04098358910983636
plot_id,batch_id 0 78 miss% 0.034333902118886095
plot_id,batch_id 0 79 miss% 0.05158988388976808
plot_id,batch_id 0 80 miss% 0.05027273579728661
plot_id,batch_id 0 81 miss% 0.07284412292064478
plot_id,batch_id 0 82 miss% 0.041034927765173555
plot_id,batch_id 0 83 miss% 0.046132609036841396
plot_id,batch_id 0 84 miss% 0.0638989653599174
plot_id,batch_id 0 85 miss% 0.050288431317364886
plot_id,batch_id 0 86 miss% 0.04984217098793481
plot_id,batch_id 0 87 miss% 0.049929787720122104
plot_id,batch_id 0 88 miss% 0.07760489820823535
plot_id,batch_id 0 89 miss% 0.052180008248325485
plot_id,batch_id 0 90 miss% 0.045556057798035454
plot_id,batch_id 0 91 miss% 0.0491593392850804
plot_id,batch_id 0 92 miss% 0.03946160763542574
plot_id,batch_id 0 93 miss% 0.04489242757434652
plot_id,batch_id 0 94 miss% 0.05773155895177713
plot_id,batch_id 0 95 miss% 0.047434386039017
plot_id,batch_id 0 96 miss% 0.04633783946209288
plot_id,batch_id 0 97 miss% 0.03224643187915117
plot_id,batch_id 0 98 miss% 0.05259051132938703
plot_id,batch_id 0 99 miss% 0.06076742697738226
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08227203 0.03154823 0.0903412  0.03656247 0.04352646 0.0479207
 0.03572025 0.07274696 0.05688075 0.08831722 0.02634091 0.06639864
 0.06522254 0.04727595 0.08395299 0.04148267 0.0925251  0.06344557
 0.0720947  0.09789866 0.0681209  0.05074576 0.05194103 0.02976738
 0.06834621 0.03668663 0.04002676 0.05192686 0.04886894 0.02385026
 0.04766145 0.09018724 0.07458063 0.0634712  0.03777782 0.03691477
 0.07680731 0.05467985 0.02427602 0.04984112 0.0458285  0.08181611
 0.02827435 0.04678473 0.01795313 0.0482588  0.01670176 0.02123032
 0.03756518 0.02118645 0.15084029 0.02881934 0.04815442 0.02385188
 0.01866034 0.0393333  0.10402449 0.03729259 0.03856694 0.0410771
 0.03983939 0.02879072 0.06860932 0.04566168 0.05538843 0.04459084
 0.02277993 0.03426882 0.05362204 0.06260701 0.05687566 0.0436545
 0.07804471 0.07384811 0.05782869 0.02506159 0.11713631 0.04098359
 0.0343339  0.05158988 0.05027274 0.07284412 0.04103493 0.04613261
 0.06389897 0.05028843 0.04984217 0.04992979 0.0776049  0.05218001
 0.04555606 0.04915934 0.03946161 0.04489243 0.05773156 0.04743439
 0.04633784 0.03224643 0.05259051 0.06076743]
for model  197 the mean error 0.05230893555949297
all id 197 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 31 out win 6 err 0.05230893555949297 time 17867.972073316574
0.34599067134110884
plot_id,batch_id 0 71 miss% 0.3959291874610011
plot_id,batch_id 0 72 miss% 0.46517124278482963
plot_id,batch_id 0 73 miss% 0.38010966868663326
plot_id,batch_id 0 74 miss% 0.5268488084952876
plot_id,batch_id 0 75 miss% 0.32205306759513036
plot_id,batch_id 0 76 miss% 0.4097421454325721
plot_id,batch_id 0 77 miss% 0.4289725356335013
plot_id,batch_id 0 78 miss% 0.3722783223234116
plot_id,batch_id 0 79 miss% 0.4387825481555919
plot_id,batch_id 0 80 miss% 0.4071505503454027
plot_id,batch_id 0 81 miss% 0.5132198841449932
plot_id,batch_id 0 82 miss% 0.3941371290799128
plot_id,batch_id 0 83 miss% 0.4626800161689156
plot_id,batch_id 0 84 miss% 0.42275751631710334
plot_id,batch_id 0 85 miss% 0.36057445091511653
plot_id,batch_id 0 86 miss% 0.43430480516814074
plot_id,batch_id 0 87 miss% 0.4378839583440245
plot_id,batch_id 0 88 miss% 0.4802927039742704
plot_id,batch_id 0 89 miss% 0.4188176409249558
plot_id,batch_id 0 90 miss% 0.2807861371487125
plot_id,batch_id 0 91 miss% 0.4167804518347241
plot_id,batch_id 0 92 miss% 0.34610175452202135
plot_id,batch_id 0 93 miss% 0.3534633894249256
plot_id,batch_id 0 94 miss% 0.526640817569898
plot_id,batch_id 0 95 miss% 0.3123614566187201
plot_id,batch_id 0 96 miss% 0.37768090093284284
plot_id,batch_id 0 97 miss% 0.44569103316762304
plot_id,batch_id 0 98 miss% 0.5394815959648711
plot_id,batch_id 0 99 miss% 0.4327715516454472
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.39014772 0.42731645 0.49346709 0.36915586 0.37417892 0.42204418
 0.41433371 0.52653354 0.65910623 0.60946123 0.34783837 0.38554253
 0.44852896 0.41281693 0.52125881 0.39524615 0.54747568 0.50371441
 0.54013983 0.43074616 0.41583994 0.48610478 0.50248484 0.4876546
 0.42559605 0.43462496 0.49778135 0.38921769 0.47097038 0.51928054
 0.42447592 0.55949568 0.46640804 0.53302163 0.40458833 0.40474327
 0.58035619 0.45420706 0.51751126 0.47880058 0.42964795 0.50742171
 0.40686761 0.46725846 0.39026932 0.45574228 0.43549174 0.50081673
 0.46845133 0.38947649 0.56305939 0.53831434 0.52056452 0.43071202
 0.40032036 0.4378181  0.5471017  0.53860296 0.45916101 0.48508564
 0.33814836 0.35700213 0.50845765 0.42997164 0.4182995  0.37413446
 0.46355355 0.36275366 0.48428389 0.47392661 0.34599067 0.39592919
 0.46517124 0.38010967 0.52684881 0.32205307 0.40974215 0.42897254
 0.37227832 0.43878255 0.40715055 0.51321988 0.39413713 0.46268002
 0.42275752 0.36057445 0.43430481 0.43788396 0.4802927  0.41881764
 0.28078614 0.41678045 0.34610175 0.35346339 0.52664082 0.31236146
 0.3776809  0.44569103 0.5394816  0.43277155]
for model  25 the mean error 0.44704386911157246
all id 25 hidden_dim 32 learning_rate 0.0025 num_layers 5 frames 21 out win 5 err 0.44704386911157246 time 17892.889323472977
Launcher: Job 81 completed in 18118 seconds.
Launcher: Task 116 done. Exiting.
Launcher: Job 198 completed in 18116 seconds.
Launcher: Task 211 done. Exiting.
Launcher: Job 26 completed in 18121 seconds.
Launcher: Task 90 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  120401
Epoch:0, Train loss:0.452848, valid loss:0.445072
Epoch:1, Train loss:0.061194, valid loss:0.003943
Epoch:2, Train loss:0.008072, valid loss:0.002278
Epoch:3, Train loss:0.005077, valid loss:0.001947
Epoch:4, Train loss:0.002762, valid loss:0.001150
Epoch:5, Train loss:0.002197, valid loss:0.001156
Epoch:6, Train loss:0.001975, valid loss:0.000962
Epoch:7, Train loss:0.001805, valid loss:0.000820
Epoch:8, Train loss:0.001656, valid loss:0.001002
Epoch:9, Train loss:0.001547, valid loss:0.000740
Epoch:10, Train loss:0.001466, valid loss:0.000760
Epoch:11, Train loss:0.001135, valid loss:0.000652
Epoch:12, Train loss:0.001066, valid loss:0.000587
Epoch:13, Train loss:0.001058, valid loss:0.000580
Epoch:14, Train loss:0.001018, valid loss:0.000640
Epoch:15, Train loss:0.000979, valid loss:0.000583
Epoch:16, Train loss:0.000970, valid loss:0.000535
Epoch:17, Train loss:0.000916, valid loss:0.000564
Epoch:18, Train loss:0.000889, valid loss:0.000551
Epoch:19, Train loss:0.000881, valid loss:0.000550
Epoch:20, Train loss:0.000881, valid loss:0.000549
Epoch:21, Train loss:0.000693, valid loss:0.000491
Epoch:22, Train loss:0.000678, valid loss:0.000484
Epoch:23, Train loss:0.000670, valid loss:0.000468
Epoch:24, Train loss:0.000654, valid loss:0.000501
Epoch:25, Train loss:0.000655, valid loss:0.000494
Epoch:26, Train loss:0.000643, valid loss:0.000457
Epoch:27, Train loss:0.000629, valid loss:0.000488
Epoch:28, Train loss:0.000619, valid loss:0.000499
Epoch:29, Train loss:0.000625, valid loss:0.000506
Epoch:30, Train loss:0.000597, valid loss:0.000450
Epoch:31, Train loss:0.000522, valid loss:0.000478
Epoch:32, Train loss:0.000516, valid loss:0.000451
Epoch:33, Train loss:0.000515, valid loss:0.000456
Epoch:34, Train loss:0.000506, valid loss:0.000454
Epoch:35, Train loss:0.000502, valid loss:0.000432
Epoch:36, Train loss:0.000501, valid loss:0.000449
Epoch:37, Train loss:0.000492, valid loss:0.000471
Epoch:38, Train loss:0.000491, valid loss:0.000448
Epoch:39, Train loss:0.000479, valid loss:0.000439
Epoch:40, Train loss:0.000483, valid loss:0.000458
Epoch:41, Train loss:0.000440, valid loss:0.000415
Epoch:42, Train loss:0.000438, valid loss:0.000421
Epoch:43, Train loss:0.000435, valid loss:0.000428
Epoch:44, Train loss:0.000435, valid loss:0.000412
Epoch:45, Train loss:0.000431, valid loss:0.000423
Epoch:46, Train loss:0.000430, valid loss:0.000425
Epoch:47, Train loss:0.000426, valid loss:0.000426
Epoch:48, Train loss:0.000422, valid loss:0.000420
Epoch:49, Train loss:0.000427, valid loss:0.000413
Epoch:50, Train loss:0.000422, valid loss:0.000430
Epoch:51, Train loss:0.000397, valid loss:0.000420
Epoch:52, Train loss:0.000395, valid loss:0.000412
Epoch:53, Train loss:0.000393, valid loss:0.000416
Epoch:54, Train loss:0.000393, valid loss:0.000412
Epoch:55, Train loss:0.000393, valid loss:0.000411
Epoch:56, Train loss:0.000392, valid loss:0.000412
Epoch:57, Train loss:0.000392, valid loss:0.000416
Epoch:58, Train loss:0.000392, valid loss:0.000411
Epoch:59, Train loss:0.000391, valid loss:0.000415
Epoch:60, Train loss:0.000391, valid loss:0.000411
training time 18106.034157037735
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.09546790962139946
plot_id,batch_id 0 1 miss% 0.06796903244347435
plot_id,batch_id 0 2 miss% 0.09595467455054447
plot_id,batch_id 0 3 miss% 0.06128436057389605
plot_id,batch_id 0 4 miss% 0.0684658049503965
plot_id,batch_id 0 5 miss% 0.095613712474138
plot_id,batch_id 0 6 miss% 0.04825999290214518
plot_id,batch_id 0 7 miss% 0.09672707982749468
plot_id,batch_id 0 8 miss% 0.08634262013802443
plot_id,batch_id 0 9 miss% 0.058126625812648755
plot_id,batch_id 0 10 miss% 0.038890211738899776
plot_id,batch_id 0 11 miss% 0.06862571207410842
plot_id,batch_id 0 12 miss% 0.04045939834665325
plot_id,batch_id 0 13 miss% 0.04489333415928397
plot_id,batch_id 0 14 miss% 0.055421476528921015
plot_id,batch_id 0 15 miss% 0.05508219130289018
plot_id,batch_id 0 16 miss% 0.13600268071907362
plot_id,batch_id 0 17 miss% 0.03175002921045252
plot_id,batch_id 0 18 miss% 0.057971551496696895
plot_id,batch_id 0 19 miss% 0.09725053333547971
plot_id,batch_id 0 20 miss% 0.05000560233662394
plot_id,batch_id 0 21 miss% 0.04251167193102918
plot_id,batch_id 0 22 miss% 0.04672407222202588
plot_id,batch_id 0 23 miss% 0.05090446095032969
plot_id,batch_id 0 24 miss% 0.052614674704221494
plot_id,batch_id 0 25 miss% 0.04958237582416371
plot_id,batch_id 0 26 miss% 0.0529914746806944
plot_id,batch_id 0 27 miss% 0.05363912796641964
plot_id,batch_id 0 28 miss% 0.03818815491127717
plot_id,batch_id 0 29 miss% 0.04641573844696043
plot_id,batch_id 0 30 miss% 0.018900526059571144
plot_id,batch_id 0 31 miss% 0.11015891220996947
plot_id,batch_id 0 32 miss% 0.10947431011567971
plot_id,batch_id 0 33 miss% 0.053847028079034985
plot_id,batch_id 0 34 miss% 0.056463850824658185
plot_id,batch_id 0 35 miss% 0.035151843286400176
plot_id,batch_id 0 36 miss% 0.09153672931858992
plot_id,batch_id 0 37 miss% 0.11520692618431695
plot_id,batch_id 0 38 miss% 0.07674165745931079
plot_id,batch_id 0 39 miss% 0.0466994080731302
plot_id,batch_id 0 40 miss% 0.0782304953008347
plot_id,batch_id 0 41 miss% 0.0651353482410526
plot_id,batch_id 0 42 miss% 0.029992388919091402
plot_id,batch_id 0 43 miss% 0.0682908221089427
plot_id,batch_id 0 44 miss% 0.05108238251541053
plot_id,batch_id 0 45 miss% 0.034410507956045215
plot_id,batch_id 0 46 miss% 0.045926814709671436
plot_id,batch_id 0 47 miss% 0.03640954556050017
plot_id,batch_id 0 48 miss% 0.020504430707573372
plot_id,batch_id 0 49 miss% 0.04622185294180192
plot_id,batch_id 0 50 miss% 0.10478499090603029
plot_id,batch_id 0 51 miss% 0.03789849344537821
plot_id,batch_id 0 52 miss% 0.043400470896195316
plot_id,batch_id 0 53 miss% 0.03157169906172069
plot_id,batch_id 0 54 miss% 0.04675618014341039
plot_id,batch_id 0 55 miss% 0.048123097934192186
plot_id,batch_id 0 56 miss% 0.08809890766715467
plot_id,batch_id 0 57 miss% 0.10107453726342704
plot_id,batch_id 0 58 miss% 0.04441796047141265
plot_id,batch_id 0 59 miss% 0.03811714251903238
plot_id,batch_id 0 60 miss% 0.05801032615458049
plot_id,batch_id 0 61 miss% 0.038999232661440855
plot_id,batch_id 0 62 miss% 0.05141579312055768
plot_id,batch_id 0 63 miss% 0.024330231208260607
plot_id,batch_id 0 64 miss% 0.05894495529210713
plot_id,batch_id 0 65 miss% 0.050775048431493365the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 24000
total number of trained parameters  265233
Epoch:0, Train loss:0.646980, valid loss:0.615764
Epoch:1, Train loss:0.536240, valid loss:0.527005
Epoch:2, Train loss:0.525072, valid loss:0.526509
Epoch:3, Train loss:0.523649, valid loss:0.526195
Epoch:4, Train loss:0.522805, valid loss:0.525811
Epoch:5, Train loss:0.522313, valid loss:0.526166
Epoch:6, Train loss:0.521795, valid loss:0.525787
Epoch:7, Train loss:0.521488, valid loss:0.525388
Epoch:8, Train loss:0.521243, valid loss:0.525529
Epoch:9, Train loss:0.520974, valid loss:0.525302
Epoch:10, Train loss:0.520854, valid loss:0.525372
Epoch:11, Train loss:0.520212, valid loss:0.525090
Epoch:12, Train loss:0.520048, valid loss:0.525080
Epoch:13, Train loss:0.519934, valid loss:0.524987
Epoch:14, Train loss:0.519920, valid loss:0.525017
Epoch:15, Train loss:0.519874, valid loss:0.524962
Epoch:16, Train loss:0.519812, valid loss:0.524976
Epoch:17, Train loss:0.519783, valid loss:0.524991
Epoch:18, Train loss:0.519709, valid loss:0.524974
Epoch:19, Train loss:0.519744, valid loss:0.525096
Epoch:20, Train loss:0.519660, valid loss:0.525150
Epoch:21, Train loss:0.519384, valid loss:0.524814
Epoch:22, Train loss:0.519296, valid loss:0.524818
Epoch:23, Train loss:0.519276, valid loss:0.524813
Epoch:24, Train loss:0.519272, valid loss:0.524819
Epoch:25, Train loss:0.519257, valid loss:0.524788
Epoch:26, Train loss:0.519239, valid loss:0.524890
Epoch:27, Train loss:0.519261, valid loss:0.524827
Epoch:28, Train loss:0.519229, valid loss:0.524899
Epoch:29, Train loss:0.519176, valid loss:0.524801
Epoch:30, Train loss:0.519200, valid loss:0.524826
Epoch:31, Train loss:0.519041, valid loss:0.524746
Epoch:32, Train loss:0.519001, valid loss:0.524743
Epoch:33, Train loss:0.519013, valid loss:0.524781
Epoch:34, Train loss:0.519015, valid loss:0.524803
Epoch:35, Train loss:0.518993, valid loss:0.524807
Epoch:36, Train loss:0.518982, valid loss:0.524823
Epoch:37, Train loss:0.518982, valid loss:0.524776
Epoch:38, Train loss:0.518981, valid loss:0.524789
Epoch:39, Train loss:0.518962, valid loss:0.524771
Epoch:40, Train loss:0.518969, valid loss:0.524776
Epoch:41, Train loss:0.518891, valid loss:0.524787
Epoch:42, Train loss:0.518884, valid loss:0.524740
Epoch:43, Train loss:0.518878, valid loss:0.524774
Epoch:44, Train loss:0.518878, valid loss:0.524788
Epoch:45, Train loss:0.518879, valid loss:0.524777
Epoch:46, Train loss:0.518868, valid loss:0.524749
Epoch:47, Train loss:0.518862, valid loss:0.524778
Epoch:48, Train loss:0.518855, valid loss:0.524757
Epoch:49, Train loss:0.518864, valid loss:0.524772
Epoch:50, Train loss:0.518857, valid loss:0.524777
Epoch:51, Train loss:0.518833, valid loss:0.524756
Epoch:52, Train loss:0.518828, valid loss:0.524760
Epoch:53, Train loss:0.518826, valid loss:0.524755
Epoch:54, Train loss:0.518825, valid loss:0.524758
Epoch:55, Train loss:0.518824, valid loss:0.524757
Epoch:56, Train loss:0.518823, valid loss:0.524765
Epoch:57, Train loss:0.518822, valid loss:0.524760
Epoch:58, Train loss:0.518822, valid loss:0.524762
Epoch:59, Train loss:0.518821, valid loss:0.524763
Epoch:60, Train loss:0.518821, valid loss:0.524763
training time 18217.402760267258
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.30000000000000004
nondim time 0.6000000000000001
nondim time 0.9
plot_id,batch_id 0 0 miss% 0.9815951038502072
plot_id,batch_id 0 1 miss% 0.9709010563069129
plot_id,batch_id 0 2 miss% 0.9728814029347471
plot_id,batch_id 0 3 miss% 0.9758026253842256
plot_id,batch_id 0 4 miss% 0.9709658998787564
plot_id,batch_id 0 5 miss% 0.964341874746314
plot_id,batch_id 0 6 miss% 0.9752893856780097
plot_id,batch_id 0 7 miss% 0.9706382853048044
plot_id,batch_id 0 8 miss% 0.9767855419382127
plot_id,batch_id 0 9 miss% 0.9811543926895593
plot_id,batch_id 0 10 miss% 0.974879812269032
plot_id,batch_id 0 11 miss% 0.9692948647455464
plot_id,batch_id 0 12 miss% 0.9704728430561588
plot_id,batch_id 0 13 miss% 0.969181345965275
plot_id,batch_id 0 14 miss% 0.9818400545422226
plot_id,batch_id 0 15 miss% 0.9655432733754292
plot_id,batch_id 0 16 miss% 0.9645410333278781
plot_id,batch_id 0 17 miss% 0.9712044684116824
plot_id,batch_id 0 18 miss% 0.9841899861553078
plot_id,batch_id 0 19 miss% 0.9780503748559112
plot_id,batch_id 0 20 miss% 0.9708716837539688
plot_id,batch_id 0 21 miss% 0.9755360732007005
plot_id,batch_id 0 22 miss% 0.973839571251625
plot_id,batch_id 0 23 miss% 0.9701452799710386
plot_id,batch_id 0 24 miss% 0.9716513701585636
plot_id,batch_id 0 25 miss% 0.9763787763687217
plot_id,batch_id 0 26 miss% 0.976739791662732
plot_id,batch_id 0 27 miss% 0.9785790932709626
plot_id,batch_id 0 28 miss% 0.9724117461783938
plot_id,batch_id 0 29 miss% 0.9765648622465387
plot_id,batch_id 0 30 miss% 0.9641773263257253
plot_id,batch_id 0 31 miss% 0.9754237635857548
plot_id,batch_id 0 32 miss% 0.9779840191538164
plot_id,batch_id 0 33 miss% 0.9791654935016002
plot_id,batch_id 0 34 miss% 0.9782431444725205
plot_id,batch_id 0 35 miss% 0.9806532600760282
plot_id,batch_id 0 36 miss% 0.9850029653369188
plot_id,batch_id 0 37 miss% 0.9740768343943366
plot_id,batch_id 0 38 miss% 0.9777205167036519
plot_id,batch_id 0 39 miss% 0.9831389914621972
plot_id,batch_id 0 40 miss% 0.9705331118607016
plot_id,batch_id 0 41 miss% 0.9750599425918257
plot_id,batch_id 0 42 miss% 0.974157765301789
plot_id,batch_id 0 43 miss% 0.9696801826237962
plot_id,batch_id 0 44 miss% 0.9747753579520863
plot_id,batch_id 0 45 miss% 0.9760251685635398
plot_id,batch_id 0 46 miss% 0.9733569500056336
plot_id,batch_id 0 47 miss% 0.9739480192587475
plot_id,batch_id 0 48 miss% 0.9798500163073819
plot_id,batch_id 0 49 miss% 0.974566352128469
plot_id,batch_id 0 50 miss% 0.978325174914364
plot_id,batch_id 0 51 miss% 0.9799017067034766
plot_id,batch_id 0 52 miss% 0.9763018997313998
plot_id,batch_id 0 53 miss% 0.9743271654271857
plot_id,batch_id 0 54 miss% 0.98008900710508
plot_id,batch_id 0 55 miss% 0.9731352560238492
plot_id,batch_id 0 56 miss% 0.9855018390434302
plot_id,batch_id 0 57 miss% 0.9825041420692487
plot_id,batch_id 0 58 miss% 0.9777296121057657
plot_id,batch_id 0 59 miss% 0.9776971901006936
plot_id,batch_id 0 60 miss% 0.969998671730813
plot_id,batch_id 0 61 miss% 0.968903669608473
plot_id,batch_id 0 62 miss% 0.9874548011154044
plot_id,batch_id 0 63 miss% 0.9679806559620232
plot_id,batch_id 0 64 miss% 0.9774504228348636
plot_id,batch_id 0 65 miss% 0.9651740609378521
plot_id,batch_id 0 66 miss% 0.968512956671377
plot_id,batch_id 0 67 miss% 0.967009909301961
plot_id,batch_id 0 68 miss% 0.9852233443992372
plot_id,batch_id 0 69 miss% 0.969487274146647
plot_id,batch_id 0 70 miss% 0.9455133078729325
plot_id,batch_id 0 71 miss% 0.9816486612192264
plot_id,batch_id 0 72 miss% 0.9646566796773044
plot_id,batch_id 0 73 miss% 0.9738992675379186
plot_id,batch_id 0 74 miss% 0.9832635725061798
plot_id,batch_id 0 75 miss% 0.9478657920030626
plot_id,batch_id 0 76 miss% 0.9628371218545564
plot_id,batch_id 0 77 miss% 0.9763632692207914
plot_id,batch_id 0 78 miss% 0.9691848496510722
plot_id,batch_id 0 79 miss% 0.9783820700600379
plot_id,batch_id 0 80 miss% 0.9922614435663761
plot_id,batch_id 0 81 miss% 0.9744348826816255
plot_id,batch_id 0 82 miss% 0.9764289348501393
plot_id,batch_id 0 83 miss% 0.9693913262442354
plot_id,batch_id 0 84 miss% 0.9692821114846841
plot_id,batch_id 0 85 miss% 0.9550153952142674
plot_id,batch_id 0 86 miss% 0.978047557087906
plot_id,batch_id 0 87 miss% 0.9674604621321631
plot_id,batch_id 0 88 miss% 0.9679233078917526
plot_id,batch_id 0 89 miss% 0.9704462011225845
plot_id,batch_id 0 90 miss% 0.973587209806547
plot_id,batch_id 0 91 miss% 0.9708707679106461
plot_id,batch_id 0 92 miss% 0.9887473472494123
plot_id,batch_id 0 93 miss% 0.975152479247499
plot_id,batch_id 0 94 miss% 0.9786380117380754
plot_id,batch_id 0 95 miss% 0.9739305681429261
plot_id,batch_id 0 96 miss% 0.9766313942192398
plot_id,batch_id 0 97 miss% 0.971993839472661
plot_id,batch_id 0 98 miss% 0.9812209780929275
plot_id,batch_id 0 99 miss% 0.9701883043416483
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.9815951  0.97090106 0.9728814  0.97580263 0.9709659  0.96434187
 0.97528939 0.97063829 0.97678554 0.98115439 0.97487981 0.96929486
 0.97047284 0.96918135 0.98184005 0.96554327 0.96454103 0.97120447
 0.98418999 0.97805037 0.97087168 0.97553607 0.97383957 0.97014528
 0.97165137 0.97637878 0.97673979 0.97857909 0.97241175 0.97656486
 0.96417733 0.97542376 0.97798402 0.97916549 0.97824314 0.98065326
 0.98500297 0.97407683 0.97772052 0.98313899 0.97053311 0.97505994
 0.97415777 0.96968018 0.97477536 0.97602517 0.97335695 0.97394802
 0.97985002 0.97456635 0.97832517 0.97990171 0.9763019  0.97432717
 0.98008901 0.97313526 0.98550184 0.98250414 0.97772961 0.97769719
 0.96999867 0.96890367 0.9874548  0.96798066 0.97745042 0.96517406
 0.96851296 0.96700991 0.98522334 0.96948727 0.94551331 0.98164866
 0.96465668 0.97389927 0.98326357 0.94786579 0.96283712 0.97636327
 0.96918485 0.97838207 0.99226144 0.97443488 0.97642893 0.96939133
 0.96928211 0.9550154  0.97804756 0.96746046 0.96792331 0.9704462
 0.97358721 0.97087077 0.98874735 0.97515248 0.97863801 0.97393057
 0.97663139 0.97199384 0.98122098 0.9701883 ]
for model  53 the mean error 0.974077869291195
all id 53 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 21 out win 6 err 0.974077869291195 time 18217.402760267258
Launcher: Job 54 completed in 18329 seconds.
Launcher: Task 42 done. Exiting.

plot_id,batch_id 0 66 miss% 0.03419461624082937
plot_id,batch_id 0 67 miss% 0.01844105902076852
plot_id,batch_id 0 68 miss% 0.039889612525437775
plot_id,batch_id 0 69 miss% 0.07937852794098502
plot_id,batch_id 0 70 miss% 0.05938558319922443
plot_id,batch_id 0 71 miss% 0.03223365321208355
plot_id,batch_id 0 72 miss% 0.08530200455090382
plot_id,batch_id 0 73 miss% 0.04228376109790637
plot_id,batch_id 0 74 miss% 0.1435466654629973
plot_id,batch_id 0 75 miss% 0.03301050357722613
plot_id,batch_id 0 76 miss% 0.04735824006951588
plot_id,batch_id 0 77 miss% 0.06870807346676862
plot_id,batch_id 0 78 miss% 0.03787867069438053
plot_id,batch_id 0 79 miss% 0.03963872108402767
plot_id,batch_id 0 80 miss% 0.04198523997320165
plot_id,batch_id 0 81 miss% 0.11369281426351072
plot_id,batch_id 0 82 miss% 0.05717977367335515
plot_id,batch_id 0 83 miss% 0.05139017994658887
plot_id,batch_id 0 84 miss% 0.04814709876062582
plot_id,batch_id 0 85 miss% 0.049239433889517736
plot_id,batch_id 0 86 miss% 0.05841943952950252
plot_id,batch_id 0 87 miss% 0.08172321292139825
plot_id,batch_id 0 88 miss% 0.07951648028444912
plot_id,batch_id 0 89 miss% 0.1026012596990652
plot_id,batch_id 0 90 miss% 0.03553092729864696
plot_id,batch_id 0 91 miss% 0.023423588733917602
plot_id,batch_id 0 92 miss% 0.03637435126913112
plot_id,batch_id 0 93 miss% 0.0583553953125292
plot_id,batch_id 0 94 miss% 0.08053443983475263
plot_id,batch_id 0 95 miss% 0.050348865647979066
plot_id,batch_id 0 96 miss% 0.04150345487119802
plot_id,batch_id 0 97 miss% 0.034924462499754165
plot_id,batch_id 0 98 miss% 0.05432746019892914
plot_id,batch_id 0 99 miss% 0.0722752838935414
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.09546791 0.06796903 0.09595467 0.06128436 0.0684658  0.09561371
 0.04825999 0.09672708 0.08634262 0.05812663 0.03889021 0.06862571
 0.0404594  0.04489333 0.05542148 0.05508219 0.13600268 0.03175003
 0.05797155 0.09725053 0.0500056  0.04251167 0.04672407 0.05090446
 0.05261467 0.04958238 0.05299147 0.05363913 0.03818815 0.04641574
 0.01890053 0.11015891 0.10947431 0.05384703 0.05646385 0.03515184
 0.09153673 0.11520693 0.07674166 0.04669941 0.0782305  0.06513535
 0.02999239 0.06829082 0.05108238 0.03441051 0.04592681 0.03640955
 0.02050443 0.04622185 0.10478499 0.03789849 0.04340047 0.0315717
 0.04675618 0.0481231  0.08809891 0.10107454 0.04441796 0.03811714
 0.05801033 0.03899923 0.05141579 0.02433023 0.05894496 0.05077505
 0.03419462 0.01844106 0.03988961 0.07937853 0.05938558 0.03223365
 0.085302   0.04228376 0.14354667 0.0330105  0.04735824 0.06870807
 0.03787867 0.03963872 0.04198524 0.11369281 0.05717977 0.05139018
 0.0481471  0.04923943 0.05841944 0.08172321 0.07951648 0.10260126
 0.03553093 0.02342359 0.03637435 0.0583554  0.08053444 0.05034887
 0.04150345 0.03492446 0.05432746 0.07227528]
for model  175 the mean error 0.05873983988572995
all id 175 hidden_dim 24 learning_rate 0.0025 num_layers 4 frames 31 out win 5 err 0.05873983988572995 time 18106.034157037735
Launcher: Job 176 completed in 18363 seconds.
Launcher: Task 72 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  120401
Epoch:0, Train loss:0.452848, valid loss:0.445072
Epoch:1, Train loss:0.051802, valid loss:0.002268
Epoch:2, Train loss:0.005328, valid loss:0.001798
Epoch:3, Train loss:0.004427, valid loss:0.001367
Epoch:4, Train loss:0.003317, valid loss:0.001237
Epoch:5, Train loss:0.003001, valid loss:0.001305
Epoch:6, Train loss:0.002813, valid loss:0.001214
Epoch:7, Train loss:0.002484, valid loss:0.000946
Epoch:8, Train loss:0.001569, valid loss:0.000925
Epoch:9, Train loss:0.001506, valid loss:0.000805
Epoch:10, Train loss:0.001376, valid loss:0.000800
Epoch:11, Train loss:0.001038, valid loss:0.000570
Epoch:12, Train loss:0.001005, valid loss:0.000526
Epoch:13, Train loss:0.000979, valid loss:0.000548
Epoch:14, Train loss:0.000935, valid loss:0.000607
Epoch:15, Train loss:0.000918, valid loss:0.000558
Epoch:16, Train loss:0.000901, valid loss:0.000555
Epoch:17, Train loss:0.000865, valid loss:0.000591
Epoch:18, Train loss:0.000845, valid loss:0.000538
Epoch:19, Train loss:0.000821, valid loss:0.000516
Epoch:20, Train loss:0.000816, valid loss:0.000555
Epoch:21, Train loss:0.000628, valid loss:0.000549
Epoch:22, Train loss:0.000627, valid loss:0.000491
Epoch:23, Train loss:0.000605, valid loss:0.000481
Epoch:24, Train loss:0.000603, valid loss:0.000477
Epoch:25, Train loss:0.000587, valid loss:0.000515
Epoch:26, Train loss:0.000592, valid loss:0.000502
Epoch:27, Train loss:0.000575, valid loss:0.000460
Epoch:28, Train loss:0.000556, valid loss:0.000503
Epoch:29, Train loss:0.000557, valid loss:0.000505
Epoch:30, Train loss:0.000554, valid loss:0.000505
Epoch:31, Train loss:0.000465, valid loss:0.000510
Epoch:32, Train loss:0.000450, valid loss:0.000482
Epoch:33, Train loss:0.000448, valid loss:0.000490
Epoch:34, Train loss:0.000439, valid loss:0.000461
Epoch:35, Train loss:0.000446, valid loss:0.000453
Epoch:36, Train loss:0.000431, valid loss:0.000452
Epoch:37, Train loss:0.000432, valid loss:0.000464
Epoch:38, Train loss:0.000434, valid loss:0.000478
Epoch:39, Train loss:0.000421, valid loss:0.000468
Epoch:40, Train loss:0.000425, valid loss:0.000435
Epoch:41, Train loss:0.000375, valid loss:0.000447
Epoch:42, Train loss:0.000375, valid loss:0.000453
Epoch:43, Train loss:0.000373, valid loss:0.000464
Epoch:44, Train loss:0.000372, valid loss:0.000446
Epoch:45, Train loss:0.000369, valid loss:0.000440
Epoch:46, Train loss:0.000366, valid loss:0.000451
Epoch:47, Train loss:0.000364, valid loss:0.000441
Epoch:48, Train loss:0.000363, valid loss:0.000446
Epoch:49, Train loss:0.000361, valid loss:0.000442
Epoch:50, Train loss:0.000361, valid loss:0.000439
Epoch:51, Train loss:0.000337, valid loss:0.000435
Epoch:52, Train loss:0.000335, valid loss:0.000438
Epoch:53, Train loss:0.000334, valid loss:0.000435
Epoch:54, Train loss:0.000334, valid loss:0.000434
Epoch:55, Train loss:0.000333, valid loss:0.000435
Epoch:56, Train loss:0.000333, valid loss:0.000440
Epoch:57, Train loss:0.000333, valid loss:0.000438
Epoch:58, Train loss:0.000332, valid loss:0.000434
Epoch:59, Train loss:0.000332, valid loss:0.000434
Epoch:60, Train loss:0.000332, valid loss:0.000436
training time 18373.714636802673
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.25873414871458694
plot_id,batch_id 0 1 miss% 0.31328630881340874
plot_id,batch_id 0 2 miss% 0.4086076309015625
plot_id,batch_id 0 3 miss% 0.3401442073284982
plot_id,batch_id 0 4 miss% 0.24840892759037717
plot_id,batch_id 0 5 miss% 0.23920667146627775
plot_id,batch_id 0 6 miss% 0.26554322387275203
plot_id,batch_id 0 7 miss% 0.428778531138812
plot_id,batch_id 0 8 miss% 0.49333999045999205
plot_id,batch_id 0 9 miss% 0.47047879103906265
plot_id,batch_id 0 10 miss% 0.22020992300229042
plot_id,batch_id 0 11 miss% 0.2498140181118372
plot_id,batch_id 0 12 miss% 0.31383449757219056
plot_id,batch_id 0 13 miss% 0.3201670632354523
plot_id,batch_id 0 14 miss% 0.40391208882472646
plot_id,batch_id 0 15 miss% 0.28048086671979655
plot_id,batch_id 0 16 miss% 0.41248554976204554
plot_id,batch_id 0 17 miss% 0.42087892241766167
plot_id,batch_id 0 18 miss% 0.4076894564386588
plot_id,batch_id 0 19 miss% 0.36616533471729823
plot_id,batch_id 0 20 miss% 0.27909648688414285
plot_id,batch_id 0 21 miss% 0.380813097449374
plot_id,batch_id 0 22 miss% 0.3548452647574473
plot_id,batch_id 0 23 miss% 0.3572814976453807
plot_id,batch_id 0 24 miss% 0.2308749599930437
plot_id,batch_id 0 25 miss% 0.28856146907017727
plot_id,batch_id 0 26 miss% 0.34759676018434044
plot_id,batch_id 0 27 miss% 0.32338521247965685
plot_id,batch_id 0 28 miss% 0.4687680415218998
plot_id,batch_id 0 29 miss% 0.354326925536419
plot_id,batch_id 0 30 miss% 0.2596385286793495
plot_id,batch_id 0 31 miss% 0.4115752324925899
plot_id,batch_id 0 32 miss% 0.3980642724179202
plot_id,batch_id 0 33 miss% 0.41667292619316043
plot_id,batch_id 0 34 miss% 0.3266739436059551
plot_id,batch_id 0 35 miss% 0.24735853493941387
plot_id,batch_id 0 36 miss% 0.4659872374096508
plot_id,batch_id 0 37 miss% 0.29819089518975767
plot_id,batch_id 0 38 miss% 0.41915420348420684
plot_id,batch_id 0 39 miss% 0.38440606723142484
plot_id,batch_id 0 40 miss% 0.357825699403837
plot_id,batch_id 0 41 miss% 0.36295458437073436
plot_id,batch_id 0 42 miss% 0.22018993184666616
plot_id,batch_id 0 43 miss% 0.263917349392237
plot_id,batch_id 0 44 miss% 0.2885385561791165
plot_id,batch_id 0 45 miss% 0.18552339395272494
plot_id,batch_id 0 46 miss% 0.3539486866355527
plot_id,batch_id 0 47 miss% 0.35892017721632613
plot_id,batch_id 0 48 miss% 0.33174031457250897
plot_id,batch_id 0 49 miss% 0.290869300704815
plot_id,batch_id 0 50 miss% 0.3370095141960196
plot_id,batch_id 0 51 miss% 0.3880764351291337
plot_id,batch_id 0 52 miss% 0.41769200489952285
plot_id,batch_id 0 53 miss% 0.27326484070846707
plot_id,batch_id 0 54 miss% 0.3259390041773628
plot_id,batch_id 0 55 miss% 0.398855087012306
plot_id,batch_id 0 56 miss% 0.48202154041010364
plot_id,batch_id 0 57 miss% 0.35279273984808346
plot_id,batch_id 0 58 miss% 0.37607350522425076
plot_id,batch_id 0 59 miss% 0.41752547589537353
plot_id,batch_id 0 60 miss% 0.19960395411958584
plot_id,batch_id 0 61 miss% 0.21968959787898473
plot_id,batch_id 0 62 miss% 0.3218820738711439
plot_id,batch_id 0 63 miss% 0.27351145447756064
plot_id,batch_id 0 64 miss% 0.31174599999321856
plot_id,batch_id 0 65 miss% 0.22456520232607677
plot_id,batch_id 0 66 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  209681
Epoch:0, Train loss:0.455640, valid loss:0.459873
Epoch:1, Train loss:0.030038, valid loss:0.003973
Epoch:2, Train loss:0.007244, valid loss:0.002230
Epoch:3, Train loss:0.004530, valid loss:0.002089
Epoch:4, Train loss:0.003851, valid loss:0.001709
Epoch:5, Train loss:0.003402, valid loss:0.001749
Epoch:6, Train loss:0.003116, valid loss:0.001605
Epoch:7, Train loss:0.002891, valid loss:0.001289
Epoch:8, Train loss:0.002554, valid loss:0.001660
Epoch:9, Train loss:0.002399, valid loss:0.001932
Epoch:10, Train loss:0.002358, valid loss:0.001375
Epoch:11, Train loss:0.001623, valid loss:0.001009
Epoch:12, Train loss:0.001462, valid loss:0.000897
Epoch:13, Train loss:0.001421, valid loss:0.000818
Epoch:14, Train loss:0.001389, valid loss:0.000949
Epoch:15, Train loss:0.001379, valid loss:0.000954
Epoch:16, Train loss:0.001302, valid loss:0.000773
Epoch:17, Train loss:0.001336, valid loss:0.000846
Epoch:18, Train loss:0.001184, valid loss:0.000905
Epoch:19, Train loss:0.001192, valid loss:0.000919
Epoch:20, Train loss:0.001151, valid loss:0.000892
Epoch:21, Train loss:0.000834, valid loss:0.000701
Epoch:22, Train loss:0.000785, valid loss:0.000681
Epoch:23, Train loss:0.000789, valid loss:0.000712
Epoch:24, Train loss:0.000774, valid loss:0.000650
Epoch:25, Train loss:0.000725, valid loss:0.000638
Epoch:26, Train loss:0.000734, valid loss:0.000635
Epoch:27, Train loss:0.000708, valid loss:0.000861
Epoch:28, Train loss:0.000773, valid loss:0.000601
Epoch:29, Train loss:0.000689, valid loss:0.000645
Epoch:30, Train loss:0.000687, valid loss:0.000652
Epoch:31, Train loss:0.000524, valid loss:0.000608
Epoch:32, Train loss:0.000505, valid loss:0.000618
Epoch:33, Train loss:0.000507, valid loss:0.000555
Epoch:34, Train loss:0.000498, valid loss:0.000617
Epoch:35, Train loss:0.000503, valid loss:0.000559
Epoch:36, Train loss:0.000477, valid loss:0.000565
Epoch:37, Train loss:0.000486, valid loss:0.000604
Epoch:38, Train loss:0.000465, valid loss:0.000582
Epoch:39, Train loss:0.000465, valid loss:0.000558
Epoch:40, Train loss:0.000504, valid loss:0.000565
Epoch:41, Train loss:0.000401, valid loss:0.000541
Epoch:42, Train loss:0.000388, valid loss:0.000551
Epoch:43, Train loss:0.000386, valid loss:0.000531
Epoch:44, Train loss:0.000389, valid loss:0.000535
Epoch:45, Train loss:0.000381, valid loss:0.000564
Epoch:46, Train loss:0.000384, valid loss:0.000577
Epoch:47, Train loss:0.000382, valid loss:0.000535
Epoch:48, Train loss:0.000369, valid loss:0.000551
Epoch:49, Train loss:0.000367, valid loss:0.000539
Epoch:50, Train loss:0.000367, valid loss:0.000537
Epoch:51, Train loss:0.000346, valid loss:0.000531
Epoch:52, Train loss:0.000344, valid loss:0.000528
Epoch:53, Train loss:0.000342, valid loss:0.000530
Epoch:54, Train loss:0.000341, valid loss:0.000530
Epoch:55, Train loss:0.000341, valid loss:0.000529
Epoch:56, Train loss:0.000340, valid loss:0.000530
Epoch:57, Train loss:0.000340, valid loss:0.000531
Epoch:58, Train loss:0.000339, valid loss:0.000530
Epoch:59, Train loss:0.000339, valid loss:0.000529
Epoch:60, Train loss:0.000338, valid loss:0.000528
training time 18363.237239837646
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.08772248813428926
plot_id,batch_id 0 1 miss% 0.06834116300748588
plot_id,batch_id 0 2 miss% 0.053697118754982974
plot_id,batch_id 0 3 miss% 0.046825472985609774
plot_id,batch_id 0 4 miss% 0.04223824475072817
plot_id,batch_id 0 5 miss% 0.09984939221494388
plot_id,batch_id 0 6 miss% 0.05188583110889329
plot_id,batch_id 0 7 miss% 0.07613521951235724
plot_id,batch_id 0 8 miss% 0.10450683337838333
plot_id,batch_id 0 9 miss% 0.04759153055169153
plot_id,batch_id 0 10 miss% 0.016172342270340073
plot_id,batch_id 0 11 miss% 0.08397738382786234
plot_id,batch_id 0 12 miss% 0.04904564780113875
plot_id,batch_id 0 13 miss% 0.08161518209486655
plot_id,batch_id 0 14 miss% 0.08968782066109451
plot_id,batch_id 0 15 miss% 0.08491808586522341
plot_id,batch_id 0 16 miss% 0.06531405591063044
plot_id,batch_id 0 17 miss% 0.03546329021775019
plot_id,batch_id 0 18 miss% 0.0693219589388767
plot_id,batch_id 0 19 miss% 0.07792555500157182
plot_id,batch_id 0 20 miss% 0.14709843887135837
plot_id,batch_id 0 21 miss% 0.05261457395662471
plot_id,batch_id 0 22 miss% 0.04652285077612498
plot_id,batch_id 0 23 miss% 0.02613208283537877
plot_id,batch_id 0 24 miss% 0.03703944967540691
plot_id,batch_id 0 25 miss% 0.07073935377645545
plot_id,batch_id 0 26 miss% 0.0413901490283843
plot_id,batch_id 0 27 miss% 0.039715221566949656
plot_id,batch_id 0 28 miss% 0.026381980345625163
plot_id,batch_id 0 29 miss% 0.022207709207096245
plot_id,batch_id 0 30 miss% 0.025678891756175385
plot_id,batch_id 0 31 miss% 0.08645095263665091
plot_id,batch_id 0 32 miss% 0.0677176060712104
plot_id,batch_id 0 33 miss% 0.036632250857585175
plot_id,batch_id 0 34 miss% 0.026104466801225554
plot_id,batch_id 0 35 miss% 0.06289078755969395
plot_id,batch_id 0 36 miss% 0.05554565863374643
plot_id,batch_id 0 37 miss% 0.06995991103699106
plot_id,batch_id 0 38 miss% 0.08099639763803373
plot_id,batch_id 0 39 miss% 0.03109325263120511
plot_id,batch_id 0 40 miss% 0.056186557162245875
plot_id,batch_id 0 41 miss% 0.029214615236335454
plot_id,batch_id 0 42 miss% 0.02263624175124504
plot_id,batch_id 0 43 miss% 0.03873063155575949
plot_id,batch_id 0 44 miss% 0.03841158879830987
plot_id,batch_id 0 45 miss% 0.06436460908255501
plot_id,batch_id 0 46 miss% 0.050701434326980835
plot_id,batch_id 0 47 miss% 0.03528064957032741
plot_id,batch_id 0 48 miss% 0.026226880407582597
plot_id,batch_id 0 49 miss% 0.023424582991322666
plot_id,batch_id 0 50 miss% 0.1313562586984613
plot_id,batch_id 0 51 miss% 0.04834467925037872
plot_id,batch_id 0 52 miss% 0.03457612478886398
plot_id,batch_id 0 53 miss% 0.012391333477464551
plot_id,batch_id 0 54 miss% 0.04045762498105813
plot_id,batch_id 0 55 miss% 0.0388638876291068
plot_id,batch_id 0 56 miss% 0.0447442032118036
plot_id,batch_id 0 57 miss% 0.041588253045411486
plot_id,batch_id 0 58 miss% 0.043917472713720575
plot_id,batch_id 0 59 miss% 0.033281240770774684
plot_id,batch_id 0 60 miss% 0.031828989511084226
plot_id,batch_id 0 61 miss% 0.026813084920828702
plot_id,batch_id 0 62 miss% 0.06058470174840742
plot_id,batch_id 0 63 miss% 0.05177390092235373
plot_id,batch_id 0 64 miss% 0.05949069835414921
plot_id,batch_id 0 65 miss% 0.14260406483569424
plot_id,batch_id 0 66 miss% 0.046880977797098836
plot_id,batch_id 0 67 miss% 0.056581326662410544
0.30587303903871216
plot_id,batch_id 0 67 miss% 0.24645196536064298
plot_id,batch_id 0 68 miss% 0.34110840120486985
plot_id,batch_id 0 69 miss% 0.39238332639504997
plot_id,batch_id 0 70 miss% 0.23616573761915255
plot_id,batch_id 0 71 miss% 0.2726412982161591
plot_id,batch_id 0 72 miss% 0.34625987277937925
plot_id,batch_id 0 73 miss% 0.2862572167113269
plot_id,batch_id 0 74 miss% 0.32261592507456405
plot_id,batch_id 0 75 miss% 0.2033917604578415
plot_id,batch_id 0 76 miss% 0.2733554080484463
plot_id,batch_id 0 77 miss% 0.2696987322613708
plot_id,batch_id 0 78 miss% 0.2439221750797849
plot_id,batch_id 0 79 miss% 0.3140200397745078
plot_id,batch_id 0 80 miss% 0.18818655053917951
plot_id,batch_id 0 81 miss% 0.3873866806328859
plot_id,batch_id 0 82 miss% 0.3108387509480828
plot_id,batch_id 0 83 miss% 0.33322423322980804
plot_id,batch_id 0 84 miss% 0.3433476185154521
plot_id,batch_id 0 85 miss% 0.19720462697698046
plot_id,batch_id 0 86 miss% 0.2412712747042319
plot_id,batch_id 0 87 miss% 0.3180263835005126
plot_id,batch_id 0 88 miss% 0.3179685883048369
plot_id,batch_id 0 89 miss% 0.3660209091813898
plot_id,batch_id 0 90 miss% 0.16857354745139824
plot_id,batch_id 0 91 miss% 0.21428934550310064
plot_id,batch_id 0 92 miss% 0.2384877802813914
plot_id,batch_id 0 93 miss% 0.22279908119771924
plot_id,batch_id 0 94 miss% 0.3679188568788382
plot_id,batch_id 0 95 miss% 0.20945101184892242
plot_id,batch_id 0 96 miss% 0.21452293194941338
plot_id,batch_id 0 97 miss% 0.33309245176455105
plot_id,batch_id 0 98 miss% 0.34220524805045127
plot_id,batch_id 0 99 miss% 0.3065965060734401
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.25873415 0.31328631 0.40860763 0.34014421 0.24840893 0.23920667
 0.26554322 0.42877853 0.49333999 0.47047879 0.22020992 0.24981402
 0.3138345  0.32016706 0.40391209 0.28048087 0.41248555 0.42087892
 0.40768946 0.36616533 0.27909649 0.3808131  0.35484526 0.3572815
 0.23087496 0.28856147 0.34759676 0.32338521 0.46876804 0.35432693
 0.25963853 0.41157523 0.39806427 0.41667293 0.32667394 0.24735853
 0.46598724 0.2981909  0.4191542  0.38440607 0.3578257  0.36295458
 0.22018993 0.26391735 0.28853856 0.18552339 0.35394869 0.35892018
 0.33174031 0.2908693  0.33700951 0.38807644 0.417692   0.27326484
 0.325939   0.39885509 0.48202154 0.35279274 0.37607351 0.41752548
 0.19960395 0.2196896  0.32188207 0.27351145 0.311746   0.2245652
 0.30587304 0.24645197 0.3411084  0.39238333 0.23616574 0.2726413
 0.34625987 0.28625722 0.32261593 0.20339176 0.27335541 0.26969873
 0.24392218 0.31402004 0.18818655 0.38738668 0.31083875 0.33322423
 0.34334762 0.19720463 0.24127127 0.31802638 0.31796859 0.36602091
 0.16857355 0.21428935 0.23848778 0.22279908 0.36791886 0.20945101
 0.21452293 0.33309245 0.34220525 0.30659651]
for model  202 the mean error 0.3188567140928871
all id 202 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 31 out win 5 err 0.3188567140928871 time 18373.714636802673
Launcher: Job 203 completed in 18609 seconds.
Launcher: Task 35 done. Exiting.
plot_id,batch_id 0 68 miss% 0.05762682981022316
plot_id,batch_id 0 69 miss% 0.08417051594732868
plot_id,batch_id 0 70 miss% 0.06427785531214346
plot_id,batch_id 0 71 miss% 0.027248534171879085
plot_id,batch_id 0 72 miss% 0.08126586079153587
plot_id,batch_id 0 73 miss% 0.053744192628216204
plot_id,batch_id 0 74 miss% 0.08377618561642357
plot_id,batch_id 0 75 miss% 0.057486942608842906
plot_id,batch_id 0 76 miss% 0.08489632403790857
plot_id,batch_id 0 77 miss% 0.029646671927716636
plot_id,batch_id 0 78 miss% 0.03027051919313304
plot_id,batch_id 0 79 miss% 0.10227740873055809
plot_id,batch_id 0 80 miss% 0.06679752609061833
plot_id,batch_id 0 81 miss% 0.06561061047060585
plot_id,batch_id 0 82 miss% 0.053145548538591846
plot_id,batch_id 0 83 miss% 0.05644431383762756
plot_id,batch_id 0 84 miss% 0.08251909181681985
plot_id,batch_id 0 85 miss% 0.04587598168391566
plot_id,batch_id 0 86 miss% 0.08404593020532078
plot_id,batch_id 0 87 miss% 0.07539063877167913
plot_id,batch_id 0 88 miss% 0.05774102820284793
plot_id,batch_id 0 89 miss% 0.047600753533155835
plot_id,batch_id 0 90 miss% 0.03305747353200013
plot_id,batch_id 0 91 miss% 0.09815206211419075
plot_id,batch_id 0 92 miss% 0.03471250512975863
plot_id,batch_id 0 93 miss% 0.07260812484358674
plot_id,batch_id 0 94 miss% 0.05329944103810443
plot_id,batch_id 0 95 miss% 0.0447235647631467
plot_id,batch_id 0 96 miss% 0.05130526754614629
plot_id,batch_id 0 97 miss% 0.03163630728455541
plot_id,batch_id 0 98 miss% 0.03648369019252489
plot_id,batch_id 0 99 miss% 0.13887713326581883
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08772249 0.06834116 0.05369712 0.04682547 0.04223824 0.09984939
 0.05188583 0.07613522 0.10450683 0.04759153 0.01617234 0.08397738
 0.04904565 0.08161518 0.08968782 0.08491809 0.06531406 0.03546329
 0.06932196 0.07792556 0.14709844 0.05261457 0.04652285 0.02613208
 0.03703945 0.07073935 0.04139015 0.03971522 0.02638198 0.02220771
 0.02567889 0.08645095 0.06771761 0.03663225 0.02610447 0.06289079
 0.05554566 0.06995991 0.0809964  0.03109325 0.05618656 0.02921462
 0.02263624 0.03873063 0.03841159 0.06436461 0.05070143 0.03528065
 0.02622688 0.02342458 0.13135626 0.04834468 0.03457612 0.01239133
 0.04045762 0.03886389 0.0447442  0.04158825 0.04391747 0.03328124
 0.03182899 0.02681308 0.0605847  0.0517739  0.0594907  0.14260406
 0.04688098 0.05658133 0.05762683 0.08417052 0.06427786 0.02724853
 0.08126586 0.05374419 0.08377619 0.05748694 0.08489632 0.02964667
 0.03027052 0.10227741 0.06679753 0.06561061 0.05314555 0.05644431
 0.08251909 0.04587598 0.08404593 0.07539064 0.05774103 0.04760075
 0.03305747 0.09815206 0.03471251 0.07260812 0.05329944 0.04472356
 0.05130527 0.03163631 0.03648369 0.13887713]
for model  151 the mean error 0.057031180504893024
all id 151 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 25 out win 5 err 0.057031180504893024 time 18363.237239837646
Launcher: Job 152 completed in 18621 seconds.
Launcher: Task 114 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  69649
Epoch:0, Train loss:0.428404, valid loss:0.407406
Epoch:1, Train loss:0.019400, valid loss:0.002464
Epoch:2, Train loss:0.005881, valid loss:0.001723
Epoch:3, Train loss:0.003770, valid loss:0.001302
Epoch:4, Train loss:0.002645, valid loss:0.001142
Epoch:5, Train loss:0.002376, valid loss:0.001145
Epoch:6, Train loss:0.002282, valid loss:0.001020
Epoch:7, Train loss:0.002062, valid loss:0.001023
Epoch:8, Train loss:0.001946, valid loss:0.000991
Epoch:9, Train loss:0.001854, valid loss:0.000859
Epoch:10, Train loss:0.001785, valid loss:0.000815
Epoch:11, Train loss:0.001324, valid loss:0.000679
Epoch:12, Train loss:0.001305, valid loss:0.000862
Epoch:13, Train loss:0.001232, valid loss:0.000760
Epoch:14, Train loss:0.001189, valid loss:0.000669
Epoch:15, Train loss:0.001188, valid loss:0.000709
Epoch:16, Train loss:0.001135, valid loss:0.000686
Epoch:17, Train loss:0.001129, valid loss:0.000642
Epoch:18, Train loss:0.001076, valid loss:0.000657
Epoch:19, Train loss:0.001080, valid loss:0.000699
Epoch:20, Train loss:0.001052, valid loss:0.000661
Epoch:21, Train loss:0.000847, valid loss:0.000543
Epoch:22, Train loss:0.000816, valid loss:0.000660
Epoch:23, Train loss:0.000791, valid loss:0.000540
Epoch:24, Train loss:0.000791, valid loss:0.000547
Epoch:25, Train loss:0.000782, valid loss:0.000579
Epoch:26, Train loss:0.000769, valid loss:0.000579
Epoch:27, Train loss:0.000745, valid loss:0.000536
Epoch:28, Train loss:0.000751, valid loss:0.000573
Epoch:29, Train loss:0.000745, valid loss:0.000530
Epoch:30, Train loss:0.000734, valid loss:0.000669
Epoch:31, Train loss:0.000610, valid loss:0.000510
Epoch:32, Train loss:0.000601, valid loss:0.000501
Epoch:33, Train loss:0.000593, valid loss:0.000492
Epoch:34, Train loss:0.000593, valid loss:0.000497
Epoch:35, Train loss:0.000583, valid loss:0.000513
Epoch:36, Train loss:0.000577, valid loss:0.000517
Epoch:37, Train loss:0.000583, valid loss:0.000514
Epoch:38, Train loss:0.000572, valid loss:0.000509
Epoch:39, Train loss:0.000558, valid loss:0.000532
Epoch:40, Train loss:0.000563, valid loss:0.000517
Epoch:41, Train loss:0.000503, valid loss:0.000464
Epoch:42, Train loss:0.000496, valid loss:0.000491
Epoch:43, Train loss:0.000494, valid loss:0.000478
Epoch:44, Train loss:0.000496, valid loss:0.000484
Epoch:45, Train loss:0.000487, valid loss:0.000481
Epoch:46, Train loss:0.000490, valid loss:0.000474
Epoch:47, Train loss:0.000487, valid loss:0.000482
Epoch:48, Train loss:0.000480, valid loss:0.000476
Epoch:49, Train loss:0.000479, valid loss:0.000473
Epoch:50, Train loss:0.000480, valid loss:0.000469
Epoch:51, Train loss:0.000452, valid loss:0.000465
Epoch:52, Train loss:0.000449, valid loss:0.000466
Epoch:53, Train loss:0.000448, valid loss:0.000461
Epoch:54, Train loss:0.000447, valid loss:0.000457
Epoch:55, Train loss:0.000446, valid loss:0.000458
Epoch:56, Train loss:0.000445, valid loss:0.000459
Epoch:57, Train loss:0.000445, valid loss:0.000458
Epoch:58, Train loss:0.000444, valid loss:0.000458
Epoch:59, Train loss:0.000444, valid loss:0.000457
Epoch:60, Train loss:0.000443, valid loss:0.000457
training time 18453.557562589645
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07752582717036463
plot_id,batch_id 0 1 miss% 0.08076252005226271
plot_id,batch_id 0 2 miss% 0.13157632547035383
plot_id,batch_id 0 3 miss% 0.06237749206993747
plot_id,batch_id 0 4 miss% 0.03744951770051925
plot_id,batch_id 0 5 miss% 0.13092997144027432
plot_id,batch_id 0 6 miss% 0.05053423838175575
plot_id,batch_id 0 7 miss% 0.11199981876453996
plot_id,batch_id 0 8 miss% 0.07701092838604935
plot_id,batch_id 0 9 miss% 0.07357279744682707
plot_id,batch_id 0 10 miss% 0.05400816576369007
plot_id,batch_id 0 11 miss% 0.06680820596669261
plot_id,batch_id 0 12 miss% 0.07824591489994585
plot_id,batch_id 0 13 miss% 0.05890158970676282
plot_id,batch_id 0 14 miss% 0.12657440422963465
plot_id,batch_id 0 15 miss% 0.04058123362776836
plot_id,batch_id 0 16 miss% 0.13835325032831347
plot_id,batch_id 0 17 miss% 0.0750714952947614
plot_id,batch_id 0 18 miss% 0.10271815348886323
plot_id,batch_id 0 19 miss% 0.08807522223539216
plot_id,batch_id 0 20 miss% 0.08572700192404908
plot_id,batch_id 0 21 miss% 0.05817521123177774
plot_id,batch_id 0 22 miss% 0.056382381622436896
plot_id,batch_id 0 23 miss% 0.05852920701482611
plot_id,batch_id 0 24 miss% 0.07949897224356615
plot_id,batch_id 0 25 miss% 0.051611366600571494
plot_id,batch_id 0 26 miss% 0.059772393444808204
plot_id,batch_id 0 27 miss% 0.05806008364512317
plot_id,batch_id 0 28 miss% 0.04730507176852043
plot_id,batch_id 0 29 miss% 0.05891827433636672
plot_id,batch_id 0 30 miss% 0.034227408204504456
plot_id,batch_id 0 31 miss% 0.1552847538973186
plot_id,batch_id 0 32 miss% 0.10119967331275165
plot_id,batch_id 0 33 miss% 0.07333949364148938
plot_id,batch_id 0 34 miss% 0.06350561633925998
plot_id,batch_id 0 35 miss% 0.046915377766269584
plot_id,batch_id 0 36 miss% 0.17572185326257966
plot_id,batch_id 0 37 miss% 0.0633479887294472
plot_id,batch_id 0 38 miss% 0.06722966799108358
plot_id,batch_id 0 39 miss% 0.04410720114294272
plot_id,batch_id 0 40 miss% 0.12231340684992825
plot_id,batch_id 0 41 miss% 0.06654266779193962
plot_id,batch_id 0 42 miss% 0.042430606779481586
plot_id,batch_id 0 43 miss% 0.0905254998788746
plot_id,batch_id 0 44 miss% 0.044791322637301005
plot_id,batch_id 0 45 miss% 0.05821221264524288
plot_id,batch_id 0 46 miss% 0.03294673299991237
plot_id,batch_id 0 47 miss% 0.03644831868078181
plot_id,batch_id 0 48 miss% 0.04303044385764914
plot_id,batch_id 0 49 miss% 0.04675490943920833
plot_id,batch_id 0 50 miss% 0.13602093152906858
plot_id,batch_id 0 51 miss% 0.0608758272014865
plot_id,batch_id 0 52 miss% 0.05731605930172733
plot_id,batch_id 0 53 miss% 0.031183989787183217
plot_id,batch_id 0 54 miss% 0.08346966057868713
plot_id,batch_id 0 55 miss% 0.07897017396353247
plot_id,batch_id 0 56 miss% 0.11374789928472714
plot_id,batch_id 0 57 miss% 0.09084227347765052
plot_id,batch_id 0 58 miss% 0.07062962631947259
plot_id,batch_id 0 59 miss% 0.05759503173791789
plot_id,batch_id 0 60 miss% 0.04731952032075721
plot_id,batch_id 0 61 miss% 0.022619400273631816
plot_id,batch_id 0 62 miss% 0.04358337603548426
plot_id,batch_id 0 63 miss% 0.05514821427583
plot_id,batch_id 0 64 miss% 0.044222386426405726
plot_id,batch_id 0 65 miss% 0.04768751788976889
plot_id,batch_id 0 66 miss% 0.13508722628893027
plot_id,batch_id 0 67 miss% 0.05409209519742067
plot_id,batch_id 0 68 miss% 0.05625330235771554
plot_id,batch_id 0 69 miss% 0.06871640634967587
plot_id,batch_id 0 70 miss% 0.08060693309877638
plot_id,batch_id 0 71 miss% 0.025090801023838948
plot_id,batch_id 0 72 miss% 0.10532478990718114
plot_id,batch_id 0 73 miss% 0.04556360265836498
plot_id,batch_id 0 74 miss% 0.1736925538763428
plot_id,batch_id 0 75 miss% 0.08588312272727454
plot_id,batch_id 0 76 miss% 0.09348749740919732
plot_id,batch_id 0 77 miss% 0.03718767662920604
plot_id,batch_id 0 78 miss% 0.034871461114536664
plot_id,batch_id 0 79 miss% 0.08725534862800537
plot_id,batch_id 0 80 miss% 0.10339650860229592
plot_id,batch_id 0 81 miss% 0.081747105900408
plot_id,batch_id 0 82 miss% 0.09180528952181044
plot_id,batch_id 0 83 miss% 0.08964946472196504
plot_id,batch_id 0 84 miss% 0.048870388098603876
plot_id,batch_id 0 85 miss% 0.08329423429960088
plot_id,batch_id 0 86 miss% 0.08024005065017484
plot_id,batch_id 0 87 miss% 0.09773297370599701
plot_id,batch_id 0 88 miss% 0.07931182443241923
plot_id,batch_id 0 89 miss% 0.06738593262475615
plot_id,batch_id 0 90 miss% 0.05523928770421109
plot_id,batch_id 0 91 miss% 0.07641852234228741
plot_id,batch_id 0 92 miss% 0.07241963810115266
plot_id,batch_id 0 93 miss% 0.0588416798361723
plot_id,batch_id 0 94 miss% 0.08633597865649742
plot_id,batch_id 0 95 miss% 0.05835210716739597
plot_id,batch_id 0 96 miss% 0.05911542157354113
plot_id,batch_id 0 97 miss% 0.05186285241080806
plot_id,batch_id 0 98 miss% 0.08165126865892137
plot_id,batch_id 0 99 miss% 0.04736014959188882
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07752583 0.08076252 0.13157633 0.06237749 0.03744952 0.13092997
 0.05053424 0.11199982 0.07701093 0.0735728  0.05400817 0.06680821
 0.07824591 0.05890159 0.1265744  0.04058123 0.13835325 0.0750715
 0.10271815 0.08807522 0.085727   0.05817521 0.05638238 0.05852921
 0.07949897 0.05161137 0.05977239 0.05806008 0.04730507 0.05891827
 0.03422741 0.15528475 0.10119967 0.07333949 0.06350562 0.04691538
 0.17572185 0.06334799 0.06722967 0.0441072  0.12231341 0.06654267
 0.04243061 0.0905255  0.04479132 0.05821221 0.03294673 0.03644832
 0.04303044 0.04675491 0.13602093 0.06087583 0.05731606 0.03118399
 0.08346966 0.07897017 0.1137479  0.09084227 0.07062963 0.05759503
 0.04731952 0.0226194  0.04358338 0.05514821 0.04422239 0.04768752
 0.13508723 0.0540921  0.0562533  0.06871641 0.08060693 0.0250908
 0.10532479 0.0455636  0.17369255 0.08588312 0.0934875  0.03718768
 0.03487146 0.08725535 0.10339651 0.08174711 0.09180529 0.08964946
 0.04887039 0.08329423 0.08024005 0.09773297 0.07931182 0.06738593
 0.05523929 0.07641852 0.07241964 0.05884168 0.08633598 0.05835211
 0.05911542 0.05186285 0.08165127 0.04736015]
for model  236 the mean error 0.07281307576375427
all id 236 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 31 out win 6 err 0.07281307576375427 time 18453.557562589645
Launcher: Job 237 completed in 18708 seconds.
Launcher: Task 41 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  151697
Epoch:0, Train loss:0.378775, valid loss:0.340922
Epoch:1, Train loss:0.074706, valid loss:0.002406
Epoch:2, Train loss:0.006988, valid loss:0.001913
Epoch:3, Train loss:0.005778, valid loss:0.001454
Epoch:4, Train loss:0.003838, valid loss:0.001284
Epoch:5, Train loss:0.002241, valid loss:0.001097
Epoch:6, Train loss:0.001903, valid loss:0.001065
Epoch:7, Train loss:0.001699, valid loss:0.000987
Epoch:8, Train loss:0.001570, valid loss:0.000774
Epoch:9, Train loss:0.001446, valid loss:0.000727
Epoch:10, Train loss:0.001365, valid loss:0.000718
Epoch:11, Train loss:0.001015, valid loss:0.000570
Epoch:12, Train loss:0.000968, valid loss:0.000592
Epoch:13, Train loss:0.000944, valid loss:0.000602
Epoch:14, Train loss:0.000914, valid loss:0.000558
Epoch:15, Train loss:0.000880, valid loss:0.000525
Epoch:16, Train loss:0.000848, valid loss:0.000625
Epoch:17, Train loss:0.000830, valid loss:0.000523
Epoch:18, Train loss:0.000806, valid loss:0.000572
Epoch:19, Train loss:0.000773, valid loss:0.000545
Epoch:20, Train loss:0.000780, valid loss:0.000534
Epoch:21, Train loss:0.000621, valid loss:0.000461
Epoch:22, Train loss:0.000595, valid loss:0.000479
Epoch:23, Train loss:0.000593, valid loss:0.000511
Epoch:24, Train loss:0.000594, valid loss:0.000476
Epoch:25, Train loss:0.000575, valid loss:0.000441
Epoch:26, Train loss:0.000560, valid loss:0.000438
Epoch:27, Train loss:0.000557, valid loss:0.000453
Epoch:28, Train loss:0.000541, valid loss:0.000471
Epoch:29, Train loss:0.000552, valid loss:0.000445
Epoch:30, Train loss:0.000522, valid loss:0.000438
Epoch:31, Train loss:0.000457, valid loss:0.000439
Epoch:32, Train loss:0.000451, valid loss:0.000424
Epoch:33, Train loss:0.000446, valid loss:0.000446
Epoch:34, Train loss:0.000450, valid loss:0.000451
Epoch:35, Train loss:0.000440, valid loss:0.000421
Epoch:36, Train loss:0.000437, valid loss:0.000420
Epoch:37, Train loss:0.000432, valid loss:0.000409
Epoch:38, Train loss:0.000427, valid loss:0.000428
Epoch:39, Train loss:0.000425, valid loss:0.000418
Epoch:40, Train loss:0.000433, valid loss:0.000407
Epoch:41, Train loss:0.000383, valid loss:0.000391
Epoch:42, Train loss:0.000383, valid loss:0.000402
Epoch:43, Train loss:0.000380, valid loss:0.000402
Epoch:44, Train loss:0.000377, valid loss:0.000413
Epoch:45, Train loss:0.000377, valid loss:0.000408
Epoch:46, Train loss:0.000376, valid loss:0.000393
Epoch:47, Train loss:0.000372, valid loss:0.000400
Epoch:48, Train loss:0.000371, valid loss:0.000398
Epoch:49, Train loss:0.000369, valid loss:0.000418
Epoch:50, Train loss:0.000372, valid loss:0.000402
Epoch:51, Train loss:0.000349, valid loss:0.000395
Epoch:52, Train loss:0.000346, valid loss:0.000398
Epoch:53, Train loss:0.000345, valid loss:0.000391
Epoch:54, Train loss:0.000345, valid loss:0.000393
Epoch:55, Train loss:0.000344, valid loss:0.000389
Epoch:56, Train loss:0.000344, valid loss:0.000389
Epoch:57, Train loss:0.000343, valid loss:0.000390
Epoch:58, Train loss:0.000343, valid loss:0.000390
Epoch:59, Train loss:0.000343, valid loss:0.000389
Epoch:60, Train loss:0.000343, valid loss:0.000390
training time 18757.264048099518
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.050702465181589156
plot_id,batch_id 0 1 miss% 0.04275086836107036
plot_id,batch_id 0 2 miss% 0.08825325376943793
plot_id,batch_id 0 3 miss% 0.04412996757842223
plot_id,batch_id 0 4 miss% 0.07692373203640385
plot_id,batch_id 0 5 miss% 0.06485259371731834
plot_id,batch_id 0 6 miss% 0.04258795910881742
plot_id,batch_id 0 7 miss% 0.09416663671215812
plot_id,batch_id 0 8 miss% 0.07122927519029261
plot_id,batch_id 0 9 miss% 0.035319783722040705
plot_id,batch_id 0 10 miss% 0.024345630500179155
plot_id,batch_id 0 11 miss% 0.05783326302910822
plot_id,batch_id 0 12 miss% 0.05795917403369332
plot_id,batch_id 0 13 miss% 0.04050207302830142
plot_id,batch_id 0 14 miss% 0.0712460395959867
plot_id,batch_id 0 15 miss% 0.0206989520583825
plot_id,batch_id 0 16 miss% 0.08341236591410299
plot_id,batch_id 0 17 miss% 0.03643788868920508
plot_id,batch_id 0 18 miss% 0.08234431192753293
plot_id,batch_id 0 19 miss% 0.07146060525256875
plot_id,batch_id 0 20 miss% 0.04427160311734369
plot_id,batch_id 0 21 miss% 0.03254779312585589
plot_id,batch_id 0 22 miss% 0.06105817018244869
plot_id,batch_id 0 23 miss% 0.03190610285983043
plot_id,batch_id 0 24 miss% 0.04023052971767734
plot_id,batch_id 0 25 miss% 0.036693871511961296
plot_id,batch_id 0 26 miss% 0.04083227353517494
plot_id,batch_id 0 27 miss% 0.06686766526897533
plot_id,batch_id 0 28 miss% 0.04675895355435909
plot_id,batch_id 0 29 miss% 0.030019511973324017
plot_id,batch_id 0 30 miss% 0.07979960685354269
plot_id,batch_id 0 31 miss% 0.07800678155021071
plot_id,batch_id 0 32 miss% 0.10270566881632484
plot_id,batch_id 0 33 miss% 0.08362776425039048
plot_id,batch_id 0 34 miss% 0.04923241440310589
plot_id,batch_id 0 35 miss% 0.07002516495805661
plot_id,batch_id 0 36 miss% 0.0908783233386605
plot_id,batch_id 0 37 miss% 0.05106350262891014
plot_id,batch_id 0 38 miss% 0.04532883121647893
plot_id,batch_id 0 39 miss% 0.026874238839815434
plot_id,batch_id 0 40 miss% 0.04177314587383896
plot_id,batch_id 0 41 miss% 0.055295814717595405
plot_id,batch_id 0 42 miss% 0.02075657255194205
plot_id,batch_id 0 43 miss% 0.08133955893049767
plot_id,batch_id 0 44 miss% 0.016946859644609416
plot_id,batch_id 0 45 miss% 0.03690231146554583
plot_id,batch_id 0 46 miss% 0.030794175993300497
plot_id,batch_id 0 47 miss% 0.03819275833083225
plot_id,batch_id 0 48 miss% 0.03131083905339906
plot_id,batch_id 0 49 miss% 0.023860995773844138
plot_id,batch_id 0 50 miss% 0.09022423734753952
plot_id,batch_id 0 51 miss% 0.030266305103926836
plot_id,batch_id 0 52 miss% 0.037616407366263246
plot_id,batch_id 0 53 miss% 0.01952646633587689
plot_id,batch_id 0 54 miss% 0.06700713756359516
plot_id,batch_id 0 55 miss% 0.07560808972312236
plot_id,batch_id 0 56 miss% 0.06420492183295748
plot_id,batch_id 0 57 miss% 0.05415059246434073
plot_id,batch_id 0 58 miss% 0.06338871285679956
plot_id,batch_id 0 59 miss% 0.042272505118077675
plot_id,batch_id 0 60 miss% 0.023232455562663318
plot_id,batch_id 0 61 miss% 0.032237853365451506
plot_id,batch_id 0 62 miss% 0.031262111585520815
plot_id,batch_id 0 63 miss% 0.03205664505532419
plot_id,batch_id 0 64 miss% 0.06772941891713402
plot_id,batch_id 0 65 miss% 0.05977929355526976
plot_id,batch_id 0 66 miss% 0.02708437775286465
plot_id,batch_id 0 67 miss% 0.04568637324141461
plot_id,batch_id 0 68 miss% 0.029433666316103647
plot_id,batch_id 0 69 miss% 0.048946810573802284
plot_id,batch_id 0 70 miss% 0.03050876381325247
plot_id,batch_id 0 71 miss% 0.041066444986152056
plot_id,batch_id 0 72 miss% 0.11554409853211932
plot_id,batch_id 0 73 miss% 0.05654666453584839
plot_id,batch_id 0 74 miss% 0.06517105298297528
plot_id,batch_id 0 75 miss% 0.037534850835673546
plot_id,batch_id 0 76 miss% 0.054905124035827015
plot_id,batch_id 0 77 miss% 0.032321890860070764
plot_id,batch_id 0 78 miss% 0.04948070539068598
plot_id,batch_id 0 79 miss% 0.10440824335749661
plot_id,batch_id 0 80 miss% 0.04015015502284832
plot_id,batch_id 0 81 miss% 0.06481727752710612
plot_id,batch_id 0 82 miss% 0.05590368447239025
plot_id,batch_id 0 83 miss% 0.0659939356744209
plot_id,batch_id 0 84 miss% 0.03625640585019314
plot_id,batch_id 0 85 miss% 0.04250932981911829
plot_id,batch_id 0 86 miss% 0.043152428508364794
plot_id,batch_id 0 87 miss% 0.07477153975864573
plot_id,batch_id 0 88 miss% 0.09421371632153894
plot_id,batch_id 0 89 miss% 0.049210346615063544
plot_id,batch_id 0 90 miss% 0.05305624480291704
plot_id,batch_id 0 91 miss% 0.04949615196494816
plot_id,batch_id 0 92 miss% 0.04950396425446879
plot_id,batch_id 0 93 miss% 0.04040778497467774
plot_id,batch_id 0 94 miss% 0.09924815668429361
plot_id,batch_id 0 95 miss% 0.03695063079214787
plot_id,batch_id 0 96 miss% 0.03799519797873521
plot_id,batch_id 0 97 miss% 0.02900528799615572
plot_id,batch_id 0 98 miss% 0.049770662821790344
plot_id,batch_id 0 99 miss% 0.05175466779376526
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05070247 0.04275087 0.08825325 0.04412997 0.07692373 0.06485259
 0.04258796 0.09416664 0.07122928 0.03531978 0.02434563 0.05783326
 0.05795917 0.04050207 0.07124604 0.02069895 0.08341237 0.03643789
 0.08234431 0.07146061 0.0442716  0.03254779 0.06105817 0.0319061
 0.04023053 0.03669387 0.04083227 0.06686767 0.04675895 0.03001951
 0.07979961 0.07800678 0.10270567 0.08362776 0.04923241 0.07002516
 0.09087832 0.0510635  0.04532883 0.02687424 0.04177315 0.05529581
 0.02075657 0.08133956 0.01694686 0.03690231 0.03079418 0.03819276
 0.03131084 0.023861   0.09022424 0.03026631 0.03761641 0.01952647
 0.06700714 0.07560809 0.06420492 0.05415059 0.06338871 0.04227251
 0.02323246 0.03223785 0.03126211 0.03205665 0.06772942 0.05977929
 0.02708438 0.04568637 0.02943367 0.04894681 0.03050876 0.04106644
 0.1155441  0.05654666 0.06517105 0.03753485 0.05490512 0.03232189
 0.04948071 0.10440824 0.04015016 0.06481728 0.05590368 0.06599394
 0.03625641 0.04250933 0.04315243 0.07477154 0.09421372 0.04921035
 0.05305624 0.04949615 0.04950396 0.04040778 0.09924816 0.03695063
 0.0379952  0.02900529 0.04977066 0.05175467]
for model  184 the mean error 0.05236428434046204
all id 184 hidden_dim 24 learning_rate 0.0025 num_layers 5 frames 31 out win 5 err 0.05236428434046204 time 18757.264048099518
Launcher: Job 185 completed in 19002 seconds.
Launcher: Task 238 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  265233
Epoch:0, Train loss:0.489850, valid loss:0.463285
Epoch:1, Train loss:0.367139, valid loss:0.365021
Epoch:2, Train loss:0.359851, valid loss:0.364828
Epoch:3, Train loss:0.358986, valid loss:0.364510
Epoch:4, Train loss:0.358491, valid loss:0.364350
Epoch:5, Train loss:0.358227, valid loss:0.364512
Epoch:6, Train loss:0.357927, valid loss:0.364061
Epoch:7, Train loss:0.357788, valid loss:0.363970
Epoch:8, Train loss:0.357683, valid loss:0.363964
Epoch:9, Train loss:0.357549, valid loss:0.363929
Epoch:10, Train loss:0.357506, valid loss:0.364132
Epoch:11, Train loss:0.356999, valid loss:0.363782
Epoch:12, Train loss:0.356889, valid loss:0.363770
Epoch:13, Train loss:0.356911, valid loss:0.363759
Epoch:14, Train loss:0.356857, valid loss:0.363761
Epoch:15, Train loss:0.356815, valid loss:0.363751
Epoch:16, Train loss:0.356811, valid loss:0.363777
Epoch:17, Train loss:0.356801, valid loss:0.363754
Epoch:18, Train loss:0.356736, valid loss:0.363930
Epoch:19, Train loss:0.356722, valid loss:0.363708
Epoch:20, Train loss:0.356685, valid loss:0.363708
Epoch:21, Train loss:0.356471, valid loss:0.363595
Epoch:22, Train loss:0.356472, valid loss:0.363633
Epoch:23, Train loss:0.356431, valid loss:0.363656
Epoch:24, Train loss:0.356441, valid loss:0.363640
Epoch:25, Train loss:0.356428, valid loss:0.363623
Epoch:26, Train loss:0.356433, valid loss:0.363591
Epoch:27, Train loss:0.356412, valid loss:0.363690
Epoch:28, Train loss:0.356403, valid loss:0.363650
Epoch:29, Train loss:0.356384, valid loss:0.363778
Epoch:30, Train loss:0.356388, valid loss:0.363627
Epoch:31, Train loss:0.356280, valid loss:0.363542
Epoch:32, Train loss:0.356270, valid loss:0.363570
Epoch:33, Train loss:0.356262, valid loss:0.363575
Epoch:34, Train loss:0.356257, valid loss:0.363593
Epoch:35, Train loss:0.356265, valid loss:0.363610
Epoch:36, Train loss:0.356268, valid loss:0.363570
Epoch:37, Train loss:0.356255, valid loss:0.363564
Epoch:38, Train loss:0.356241, valid loss:0.363610
Epoch:39, Train loss:0.356242, valid loss:0.363567
Epoch:40, Train loss:0.356231, valid loss:0.363559
Epoch:41, Train loss:0.356193, valid loss:0.363564
Epoch:42, Train loss:0.356190, valid loss:0.363551
Epoch:43, Train loss:0.356183, valid loss:0.363559
Epoch:44, Train loss:0.356185, valid loss:0.363556
Epoch:45, Train loss:0.356179, valid loss:0.363545
Epoch:46, Train loss:0.356178, valid loss:0.363554
Epoch:47, Train loss:0.356177, valid loss:0.363549
Epoch:48, Train loss:0.356175, valid loss:0.363575
Epoch:49, Train loss:0.356176, valid loss:0.363559
Epoch:50, Train loss:0.356173, valid loss:0.363526
Epoch:51, Train loss:0.356156, valid loss:0.363532
Epoch:52, Train loss:0.356154, valid loss:0.363536
Epoch:53, Train loss:0.356153, valid loss:0.363533
Epoch:54, Train loss:0.356153, valid loss:0.363540
Epoch:55, Train loss:0.356152, valid loss:0.363537
Epoch:56, Train loss:0.356152, valid loss:0.363535
Epoch:57, Train loss:0.356151, valid loss:0.363542
Epoch:58, Train loss:0.356151, valid loss:0.363537
Epoch:59, Train loss:0.356151, valid loss:0.363538
Epoch:60, Train loss:0.356150, valid loss:0.363540
training time 18999.411832094193
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.9575296696904733
plot_id,batch_id 0 1 miss% 0.963013965014007
plot_id,batch_id 0 2 miss% 0.9658663602808463
plot_id,batch_id 0 3 miss% 0.9653018240398983
plot_id,batch_id 0 4 miss% 0.9648404855972491
plot_id,batch_id 0 5 miss% 0.9637245258138507
plot_id,batch_id 0 6 miss% 0.9637131790706212
plot_id,batch_id 0 7 miss% 0.9654835506208637
plot_id,batch_id 0 8 miss% 0.9662540097519597
plot_id,batch_id 0 9 miss% 0.9697349630882067
plot_id,batch_id 0 10 miss% 0.96367790351541
plot_id,batch_id 0 11 miss% 0.9603579930428815
plot_id,batch_id 0 12 miss% 0.9639264640099453
plot_id,batch_id 0 13 miss% 0.9651144252107967
plot_id,batch_id 0 14 miss% 0.9676039068231604
plot_id,batch_id 0 15 miss% 0.9457002616160556
plot_id,batch_id 0 16 miss% 0.9666491289269296
plot_id,batch_id 0 17 miss% 0.9676752273787278
plot_id,batch_id 0 18 miss% 0.967951285705885
plot_id,batch_id 0 19 miss% 0.9726057001509957
plot_id,batch_id 0 20 miss% 0.9647440879154949
plot_id,batch_id 0 21 miss% 0.9707753685486521
plot_id,batch_id 0 22 miss% 0.9662292245158153
plot_id,batch_id 0 23 miss% 0.9663060513973125
plot_id,batch_id 0 24 miss% 0.9661295257759865
plot_id,batch_id 0 25 miss% 0.9645205535722582
plot_id,batch_id 0 26 miss% 0.967908172192945
plot_id,batch_id 0 27 miss% 0.969168558964381
plot_id,batch_id 0 28 miss% 0.9682923638911908
plot_id,batch_id 0 29 miss% 0.9681027780812559
plot_id,batch_id 0 30 miss% 0.9547531671927155
plot_id,batch_id 0 31 miss% 0.9683747585455158
plot_id,batch_id 0 32 miss% 0.9726972153604685
plot_id,batch_id 0 33 miss% 0.9700823042176164
plot_id,batch_id 0 34 miss% 0.9670600307469813
plot_id,batch_id 0 35 miss% 0.9622370047513282
plot_id,batch_id 0 36 miss% 0.9722992504215352
plot_id,batch_id 0 37 miss% 0.9697090387361362
plot_id,batch_id 0 38 miss% 0.9692843861607086
plot_id,batch_id 0 39 miss% 0.9677286439987441
plot_id,batch_id 0 40 miss% 0.9693545728349013
plot_id,batch_id 0 41 miss% 0.9658256028916794
plot_id,batch_id 0 42 miss% 0.9642847038384951
plot_id,batch_id 0 43 miss% 0.9657699995753986
plot_id,batch_id 0 44 miss% 0.9665341629533524
plot_id,batch_id 0 45 miss% 0.9701944354039053
plot_id,batch_id 0 46 miss% 0.9671680445959386
plot_id,batch_id 0 47 miss% 0.9663736537818918
plot_id,batch_id 0 48 miss% 0.9678733277816682
plot_id,batch_id 0 49 miss% 0.9668209828875421
plot_id,batch_id 0 50 miss% 0.9705623743437668
plot_id,batch_id 0 51 miss% 0.968914887592581
plot_id,batch_id 0 52 miss% 0.9723309782658042
plot_id,batch_id 0 53 miss% 0.9675797580641543
plot_id,batch_id 0 54 miss% 0.9686739481695826
plot_id,batch_id 0 55 miss% 0.964296171936205
plot_id,batch_id 0 56 miss% 0.9741905479300327
plot_id,batch_id 0 57 miss% 0.9722248942699564
plot_id,batch_id 0 58 miss% 0.968915085138538
plot_id,batch_id 0 59 miss% 0.9698352243968598
plot_id,batch_id 0 60 miss% 0.9606340001486373
plot_id,batch_id 0 61 miss% 0.9615098101012851
plot_id,batch_id 0 62 miss% 0.9728362495298296
plot_id,batch_id 0 63 miss% 0.9633282497601254
plot_id,batch_id 0 64 miss% 0.9653832792211638
plot_id,batch_id 0 65 miss% 0.9603024536137537
plot_id,batch_id 0 66 miss% 0.9625527310606848
plot_id,batch_id 0 67 miss% 0.9677969117505337
plot_id,batch_id 0 68 miss% 0.9618877141398975
plot_id,batch_id 0 69 miss% 0.9634332773900516
plot_id,batch_id 0 70 miss% 0.9520096185762189
plot_id,batch_id 0 71 miss% 0.9741000254614193
plot_id,batch_id 0 72 miss% 0.9612652054321356
plot_id,batch_id 0 73 miss% 0.9697680805576295
plot_id,batch_id 0 74 miss% 0.967100779153067
plot_id,batch_id 0 75 miss% 0.95279658761835
plot_id,batch_id 0 76 miss% 0.9636884041449604
plot_id,batch_id 0 77 miss% 0.9611709799659213
plot_id,batch_id 0 78 miss% 0.9673876674739521
plot_id,batch_id 0 79 miss% 0.9635579172119093
plot_id,batch_id 0 80 miss% 0.9591680611183137
plot_id,batch_id 0 81 miss% 0.9668239050843344
plot_id,batch_id 0 82 miss% 0.9661950896318034
plot_id,batch_id 0 83 miss% 0.963545773148654
plot_id,batch_id 0 84 miss% 0.964743873286733
plot_id,batch_id 0 85 miss% 0.9565798805203751
plot_id,batch_id 0 86 miss% 0.9672435943401645
plot_id,batch_id 0 87 miss% 0.9644826397666424
plot_id,batch_id 0 88 miss% 0.9644592901188731
plot_id,batch_id 0 89 miss% 0.9650617835550248
plot_id,batch_id 0 90 miss% 0.9659207346227505
plot_id,batch_id 0 91 miss% 0.9636649858277272
plot_id,batch_id 0 92 miss% 0.9656364320010536
plot_id,batch_id 0 93 miss% 0.9652279933425931
plot_id,batch_id 0 94 miss% 0.9697196497015075
plot_id,batch_id 0 95 miss% 0.9571718081829051
plot_id,batch_id 0 96 miss% 0.9637193435266032
plot_id,batch_id 0 97 miss% 0.9599483420979826
plot_id,batch_id 0 98 miss% 0.9676296200687444
plot_id,batch_id 0 99 miss% 0.9651090790046108
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.95752967 0.96301397 0.96586636 0.96530182 0.96484049 0.96372453
 0.96371318 0.96548355 0.96625401 0.96973496 0.9636779  0.96035799
 0.96392646 0.96511443 0.96760391 0.94570026 0.96664913 0.96767523
 0.96795129 0.9726057  0.96474409 0.97077537 0.96622922 0.96630605
 0.96612953 0.96452055 0.96790817 0.96916856 0.96829236 0.96810278
 0.95475317 0.96837476 0.97269722 0.9700823  0.96706003 0.962237
 0.97229925 0.96970904 0.96928439 0.96772864 0.96935457 0.9658256
 0.9642847  0.96577    0.96653416 0.97019444 0.96716804 0.96637365
 0.96787333 0.96682098 0.97056237 0.96891489 0.97233098 0.96757976
 0.96867395 0.96429617 0.97419055 0.97222489 0.96891509 0.96983522
 0.960634   0.96150981 0.97283625 0.96332825 0.96538328 0.96030245
 0.96255273 0.96779691 0.96188771 0.96343328 0.95200962 0.97410003
 0.96126521 0.96976808 0.96710078 0.95279659 0.9636884  0.96117098
 0.96738767 0.96355792 0.95916806 0.96682391 0.96619509 0.96354577
 0.96474387 0.95657988 0.96724359 0.96448264 0.96445929 0.96506178
 0.96592073 0.96366499 0.96563643 0.96522799 0.96971965 0.95717181
 0.96371934 0.95994834 0.96762962 0.96510908]
for model  133 the mean error 0.9655541249224698
all id 133 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 25 out win 5 err 0.9655541249224698 time 18999.411832094193
Launcher: Job 134 completed in 19108 seconds.
Launcher: Task 228 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  120401
Epoch:0, Train loss:0.429770, valid loss:0.417277
Epoch:1, Train loss:0.068648, valid loss:0.003561
Epoch:2, Train loss:0.008034, valid loss:0.002406
Epoch:3, Train loss:0.006230, valid loss:0.002016
Epoch:4, Train loss:0.004912, valid loss:0.001845
Epoch:5, Train loss:0.002673, valid loss:0.001252
Epoch:6, Train loss:0.002034, valid loss:0.001027
Epoch:7, Train loss:0.001826, valid loss:0.000876
Epoch:8, Train loss:0.001643, valid loss:0.000849
Epoch:9, Train loss:0.001573, valid loss:0.000814
Epoch:10, Train loss:0.001447, valid loss:0.000722
Epoch:11, Train loss:0.001113, valid loss:0.000734
Epoch:12, Train loss:0.001071, valid loss:0.000626
Epoch:13, Train loss:0.001039, valid loss:0.000686
Epoch:14, Train loss:0.000991, valid loss:0.000685
Epoch:15, Train loss:0.000981, valid loss:0.000597
Epoch:16, Train loss:0.000967, valid loss:0.000604
Epoch:17, Train loss:0.000914, valid loss:0.000623
Epoch:18, Train loss:0.000891, valid loss:0.000624
Epoch:19, Train loss:0.000880, valid loss:0.000575
Epoch:20, Train loss:0.000857, valid loss:0.000629
Epoch:21, Train loss:0.000693, valid loss:0.000563
Epoch:22, Train loss:0.000678, valid loss:0.000582
Epoch:23, Train loss:0.000666, valid loss:0.000692
Epoch:24, Train loss:0.000668, valid loss:0.000511
Epoch:25, Train loss:0.000640, valid loss:0.000530
Epoch:26, Train loss:0.000634, valid loss:0.000516
Epoch:27, Train loss:0.000631, valid loss:0.000532
Epoch:28, Train loss:0.000626, valid loss:0.000497
Epoch:29, Train loss:0.000618, valid loss:0.000525
Epoch:30, Train loss:0.000597, valid loss:0.000507
Epoch:31, Train loss:0.000509, valid loss:0.000489
Epoch:32, Train loss:0.000514, valid loss:0.000528
Epoch:33, Train loss:0.000507, valid loss:0.000480
Epoch:34, Train loss:0.000500, valid loss:0.000484
Epoch:35, Train loss:0.000495, valid loss:0.000482
Epoch:36, Train loss:0.000495, valid loss:0.000467
Epoch:37, Train loss:0.000489, valid loss:0.000491
Epoch:38, Train loss:0.000479, valid loss:0.000473
Epoch:39, Train loss:0.000483, valid loss:0.000482
Epoch:40, Train loss:0.000486, valid loss:0.000478
Epoch:41, Train loss:0.000434, valid loss:0.000470
Epoch:42, Train loss:0.000429, valid loss:0.000470
Epoch:43, Train loss:0.000425, valid loss:0.000483
Epoch:44, Train loss:0.000427, valid loss:0.000475
Epoch:45, Train loss:0.000426, valid loss:0.000463
Epoch:46, Train loss:0.000421, valid loss:0.000477
Epoch:47, Train loss:0.000419, valid loss:0.000456
Epoch:48, Train loss:0.000421, valid loss:0.000473
Epoch:49, Train loss:0.000415, valid loss:0.000479
Epoch:50, Train loss:0.000411, valid loss:0.000460
Epoch:51, Train loss:0.000387, valid loss:0.000461
Epoch:52, Train loss:0.000384, valid loss:0.000458
Epoch:53, Train loss:0.000383, valid loss:0.000464
Epoch:54, Train loss:0.000382, valid loss:0.000469
Epoch:55, Train loss:0.000381, valid loss:0.000462
Epoch:56, Train loss:0.000381, valid loss:0.000463
Epoch:57, Train loss:0.000381, valid loss:0.000466
Epoch:58, Train loss:0.000381, valid loss:0.000464
Epoch:59, Train loss:0.000380, valid loss:0.000458
Epoch:60, Train loss:0.000380, valid loss:0.000461
training time 18898.879329442978
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.10523887546169539
plot_id,batch_id 0 1 miss% 0.05063744398639547
plot_id,batch_id 0 2 miss% 0.08255839040302405
plot_id,batch_id 0 3 miss% 0.0814349296118664
plot_id,batch_id 0 4 miss% 0.08073621688857135
plot_id,batch_id 0 5 miss% 0.0759872178399713
plot_id,batch_id 0 6 miss% 0.04335820376939942
plot_id,batch_id 0 7 miss% 0.11188972111565584
plot_id,batch_id 0 8 miss% 0.08672188174887775
plot_id,batch_id 0 9 miss% 0.04211946771087862
plot_id,batch_id 0 10 miss% 0.027292877263735988
plot_id,batch_id 0 11 miss% 0.06379087797163223
plot_id,batch_id 0 12 miss% 0.058878222952281754
plot_id,batch_id 0 13 miss% 0.056969425730415704
plot_id,batch_id 0 14 miss% 0.12078682649130675
plot_id,batch_id 0 15 miss% 0.05799351051428662
plot_id,batch_id 0 16 miss% 0.07145160329455431
plot_id,batch_id 0 17 miss% 0.07905670767665167
plot_id,batch_id 0 18 miss% 0.08446350648427518
plot_id,batch_id 0 19 miss% 0.09028082032778151
plot_id,batch_id 0 20 miss% 0.024895736135403208
plot_id,batch_id 0 21 miss% 0.048785344579033864
plot_id,batch_id 0 22 miss% 0.07635716690890218
plot_id,batch_id 0 23 miss% 0.024255791287020354
plot_id,batch_id 0 24 miss% 0.061578123989636815
plot_id,batch_id 0 25 miss% 0.10344231326209799
plot_id,batch_id 0 26 miss% 0.07186237350092421
plot_id,batch_id 0 27 miss% 0.04938325519382625
plot_id,batch_id 0 28 miss% 0.03674203354862558
plot_id,batch_id 0 29 miss% 0.040294004709402934
plot_id,batch_id 0 30 miss% 0.0415534693543026
plot_id,batch_id 0 31 miss% 0.09483288350569785
plot_id,batch_id 0 32 miss% 0.12226432579327039
plot_id,batch_id 0 33 miss% 0.06602737240022417
plot_id,batch_id 0 34 miss% 0.053999848080243565
plot_id,batch_id 0 35 miss% 0.05811694842450391
plot_id,batch_id 0 36 miss% 0.12720975440432752
plot_id,batch_id 0 37 miss% 0.08391367958444694
plot_id,batch_id 0 38 miss% 0.030440809426541572
plot_id,batch_id 0 39 miss% 0.051480502867285394
plot_id,batch_id 0 40 miss% 0.07122319715582348
plot_id,batch_id 0 41 miss% 0.10126495434669035
plot_id,batch_id 0 42 miss% 0.06412598602656785
plot_id,batch_id 0 43 miss% 0.05482411640396653
plot_id,batch_id 0 44 miss% 0.025209805940340724
plot_id,batch_id 0 45 miss% 0.06464669603056088
plot_id,batch_id 0 46 miss% 0.04818364444546946
plot_id,batch_id 0 47 miss% 0.041016903407824816
plot_id,batch_id 0 48 miss% 0.032576946489471806
plot_id,batch_id 0 49 miss% 0.02428918588089122
plot_id,batch_id 0 50 miss% 0.09713792198776651
plot_id,batch_id 0 51 miss% 0.04420191129677929
plot_id,batch_id 0 52 miss% 0.03158724990755521
plot_id,batch_id 0 53 miss% 0.033260361837471816
plot_id,batch_id 0 54 miss% 0.07334019858886098
plot_id,batch_id 0 55 miss% 0.04959075853947958
plot_id,batch_id 0 56 miss% 0.07400651348328922
plot_id,batch_id 0 57 miss% 0.034601870766692626
plot_id,batch_id 0 58 miss% 0.041245185261229375
plot_id,batch_id 0 59 miss% 0.038040171601151286
plot_id,batch_id 0 60 miss% 0.03762415229902903
plot_id,batch_id 0 61 miss% 0.04984605601339639
plot_id,batch_id 0 62 miss% 0.054554675984823615
plot_id,batch_id 0 63 miss% 0.06714847725378789
plot_id,batch_id 0 64 miss% 0.059315920751336945
plot_id,batch_id 0 65 miss% 0.058030379268700465
plot_id,batch_id 0 66 miss% 0.09151294032575885
plot_id,batch_id 0 the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  209681
Epoch:0, Train loss:0.455640, valid loss:0.459873
Epoch:1, Train loss:0.028946, valid loss:0.003926
Epoch:2, Train loss:0.006081, valid loss:0.002124
Epoch:3, Train loss:0.004070, valid loss:0.002046
Epoch:4, Train loss:0.003432, valid loss:0.001558
Epoch:5, Train loss:0.003062, valid loss:0.001501
Epoch:6, Train loss:0.002628, valid loss:0.001613
Epoch:7, Train loss:0.002412, valid loss:0.001177
Epoch:8, Train loss:0.002167, valid loss:0.001521
Epoch:9, Train loss:0.002097, valid loss:0.001170
Epoch:10, Train loss:0.001900, valid loss:0.001244
Epoch:11, Train loss:0.001382, valid loss:0.000929
Epoch:12, Train loss:0.001319, valid loss:0.000764
Epoch:13, Train loss:0.001273, valid loss:0.000841
Epoch:14, Train loss:0.001253, valid loss:0.000824
Epoch:15, Train loss:0.001236, valid loss:0.000718
Epoch:16, Train loss:0.001177, valid loss:0.000724
Epoch:17, Train loss:0.001131, valid loss:0.000760
Epoch:18, Train loss:0.001101, valid loss:0.000903
Epoch:19, Train loss:0.001057, valid loss:0.000835
Epoch:20, Train loss:0.001042, valid loss:0.000739
Epoch:21, Train loss:0.000778, valid loss:0.000599
Epoch:22, Train loss:0.000739, valid loss:0.000611
Epoch:23, Train loss:0.000732, valid loss:0.000596
Epoch:24, Train loss:0.000733, valid loss:0.000600
Epoch:25, Train loss:0.000709, valid loss:0.000588
Epoch:26, Train loss:0.000695, valid loss:0.000562
Epoch:27, Train loss:0.000701, valid loss:0.000612
Epoch:28, Train loss:0.000659, valid loss:0.000570
Epoch:29, Train loss:0.000669, valid loss:0.000642
Epoch:30, Train loss:0.000643, valid loss:0.000648
Epoch:31, Train loss:0.000529, valid loss:0.000582
Epoch:32, Train loss:0.000501, valid loss:0.000584
Epoch:33, Train loss:0.000509, valid loss:0.000566
Epoch:34, Train loss:0.000506, valid loss:0.000545
Epoch:35, Train loss:0.000501, valid loss:0.000560
Epoch:36, Train loss:0.000489, valid loss:0.000538
Epoch:37, Train loss:0.000501, valid loss:0.000582
Epoch:38, Train loss:0.000493, valid loss:0.000556
Epoch:39, Train loss:0.000470, valid loss:0.000552
Epoch:40, Train loss:0.000467, valid loss:0.000545
Epoch:41, Train loss:0.000419, valid loss:0.000545
Epoch:42, Train loss:0.000415, valid loss:0.000539
Epoch:43, Train loss:0.000402, valid loss:0.000512
Epoch:44, Train loss:0.000409, valid loss:0.000514
Epoch:45, Train loss:0.000404, valid loss:0.000523
Epoch:46, Train loss:0.000400, valid loss:0.000542
Epoch:47, Train loss:0.000396, valid loss:0.000495
Epoch:48, Train loss:0.000401, valid loss:0.000518
Epoch:49, Train loss:0.000385, valid loss:0.000517
Epoch:50, Train loss:0.000390, valid loss:0.000527
Epoch:51, Train loss:0.000365, valid loss:0.000514
Epoch:52, Train loss:0.000362, valid loss:0.000514
Epoch:53, Train loss:0.000360, valid loss:0.000513
Epoch:54, Train loss:0.000359, valid loss:0.000513
Epoch:55, Train loss:0.000358, valid loss:0.000513
Epoch:56, Train loss:0.000357, valid loss:0.000513
Epoch:57, Train loss:0.000357, valid loss:0.000512
Epoch:58, Train loss:0.000356, valid loss:0.000513
Epoch:59, Train loss:0.000356, valid loss:0.000514
Epoch:60, Train loss:0.000355, valid loss:0.000512
training time 18903.882771015167
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.06497121919725618
plot_id,batch_id 0 1 miss% 0.05609640297147261
plot_id,batch_id 0 2 miss% 0.07042160300808718
plot_id,batch_id 0 3 miss% 0.046698441971350944
plot_id,batch_id 0 4 miss% 0.028348172699121613
plot_id,batch_id 0 5 miss% 0.08680404319642819
plot_id,batch_id 0 6 miss% 0.05276988147523737
plot_id,batch_id 0 7 miss% 0.06860449513360832
plot_id,batch_id 0 8 miss% 0.07946647995919767
plot_id,batch_id 0 9 miss% 0.046296933881028554
plot_id,batch_id 0 10 miss% 0.054545861791198406
plot_id,batch_id 0 11 miss% 0.05111011873778494
plot_id,batch_id 0 12 miss% 0.04712240584596057
plot_id,batch_id 0 13 miss% 0.044499940256761024
plot_id,batch_id 0 14 miss% 0.05909999929186016
plot_id,batch_id 0 15 miss% 0.08749467009224852
plot_id,batch_id 0 16 miss% 0.09422793563019502
plot_id,batch_id 0 17 miss% 0.033213568913459876
plot_id,batch_id 0 18 miss% 0.048990406122047796
plot_id,batch_id 0 19 miss% 0.0695552398603054
plot_id,batch_id 0 20 miss% 0.1094690766399175
plot_id,batch_id 0 21 miss% 0.04780773228547104
plot_id,batch_id 0 22 miss% 0.09576103509210863
plot_id,batch_id 0 23 miss% 0.030328889616781672
plot_id,batch_id 0 24 miss% 0.05605927241621434
plot_id,batch_id 0 25 miss% 0.06488747804625969
plot_id,batch_id 0 26 miss% 0.05012718741612412
plot_id,batch_id 0 27 miss% 0.03961179060478081
plot_id,batch_id 0 28 miss% 0.030424780556359724
plot_id,batch_id 0 29 miss% 0.025695420523192174
plot_id,batch_id 0 30 miss% 0.03249714165534744
plot_id,batch_id 0 31 miss% 0.08179264307085567
plot_id,batch_id 0 32 miss% 0.07037886706671016
plot_id,batch_id 0 33 miss% 0.05307535368320565
plot_id,batch_id 0 34 miss% 0.02495517937878215
plot_id,batch_id 0 35 miss% 0.030375551863108453
plot_id,batch_id 0 36 miss% 0.07540637591648931
plot_id,batch_id 0 37 miss% 0.05052300800161429
plot_id,batch_id 0 38 miss% 0.026969063154897683
plot_id,batch_id 0 39 miss% 0.04433765005889673
plot_id,batch_id 0 40 miss% 0.05166760683335507
plot_id,batch_id 0 41 miss% 0.06719817164159529
plot_id,batch_id 0 42 miss% 0.0294300218278393
plot_id,batch_id 0 43 miss% 0.041447296910096236
plot_id,batch_id 0 44 miss% 0.024547212555837236
plot_id,batch_id 0 45 miss% 0.06899472351868116
plot_id,batch_id 0 46 miss% 0.03414913875261105
plot_id,batch_id 0 47 miss% 0.036688627106581065
plot_id,batch_id 0 48 miss% 0.04149917713630095
plot_id,batch_id 0 49 miss% 0.018374650785615093
plot_id,batch_id 0 50 miss% 0.13660357905573123
plot_id,batch_id 0 51 miss% 0.050960810112231913
plot_id,batch_id 0 52 miss% 0.027940025859910202
plot_id,batch_id 0 53 miss% 0.01703478500482483
plot_id,batch_id 0 54 miss% 0.049369565328754264
plot_id,batch_id 0 55 miss% 0.08440700344485788
plot_id,batch_id 0 56 miss% 0.05732729283973002
plot_id,batch_id 0 57 miss% 0.041034656121061636
plot_id,batch_id 0 58 miss% 0.030716562607313556
plot_id,batch_id 0 59 miss% 0.03765685412745483
plot_id,batch_id 0 60 miss% 0.027145365010362844
plot_id,batch_id 0 61 miss% 0.03310550964180408
plot_id,batch_id 0 62 miss% 0.0626930956067634
plot_id,batch_id 0 63 miss% 0.04719739498322221
plot_id,batch_id 0 64 miss% 0.07829774513989654
plot_id,batch_id 0 65 miss% 0.09163626939813685
plot_id,batch_id 0 66 miss% 0.050558580693133036
plot_id,batch_id 0 67 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  209681
Epoch:0, Train loss:0.451184, valid loss:0.450976
Epoch:1, Train loss:0.031664, valid loss:0.004061
Epoch:2, Train loss:0.008888, valid loss:0.002557
Epoch:3, Train loss:0.004727, valid loss:0.002018
Epoch:4, Train loss:0.003828, valid loss:0.001563
Epoch:5, Train loss:0.003460, valid loss:0.001789
Epoch:6, Train loss:0.003254, valid loss:0.001469
Epoch:7, Train loss:0.002924, valid loss:0.001734
Epoch:8, Train loss:0.002783, valid loss:0.001476
Epoch:9, Train loss:0.002589, valid loss:0.001332
Epoch:10, Train loss:0.002609, valid loss:0.001287
Epoch:11, Train loss:0.001712, valid loss:0.001034
Epoch:12, Train loss:0.001629, valid loss:0.000963
Epoch:13, Train loss:0.001590, valid loss:0.000968
Epoch:14, Train loss:0.001573, valid loss:0.000871
Epoch:15, Train loss:0.001510, valid loss:0.000982
Epoch:16, Train loss:0.001431, valid loss:0.000894
Epoch:17, Train loss:0.001382, valid loss:0.000836
Epoch:18, Train loss:0.001337, valid loss:0.000937
Epoch:19, Train loss:0.001316, valid loss:0.000950
Epoch:20, Train loss:0.001317, valid loss:0.000871
Epoch:21, Train loss:0.000920, valid loss:0.000699
Epoch:22, Train loss:0.000874, valid loss:0.000710
Epoch:23, Train loss:0.000857, valid loss:0.000691
Epoch:24, Train loss:0.000845, valid loss:0.000627
Epoch:25, Train loss:0.000833, valid loss:0.000665
Epoch:26, Train loss:0.000807, valid loss:0.000699
Epoch:27, Train loss:0.000774, valid loss:0.000776
Epoch:28, Train loss:0.000788, valid loss:0.000696
Epoch:29, Train loss:0.000772, valid loss:0.000789
Epoch:30, Train loss:0.000774, valid loss:0.000663
Epoch:31, Train loss:0.000583, valid loss:0.000627
Epoch:32, Train loss:0.000554, valid loss:0.000647
Epoch:33, Train loss:0.000547, valid loss:0.000652
Epoch:34, Train loss:0.000542, valid loss:0.000639
Epoch:35, Train loss:0.000545, valid loss:0.000670
Epoch:36, Train loss:0.000525, valid loss:0.000611
Epoch:37, Train loss:0.000520, valid loss:0.000622
Epoch:38, Train loss:0.000551, valid loss:0.000674
Epoch:39, Train loss:0.000509, valid loss:0.000689
Epoch:40, Train loss:0.000505, valid loss:0.000642
Epoch:41, Train loss:0.000428, valid loss:0.000636
Epoch:42, Train loss:0.000417, valid loss:0.000600
Epoch:43, Train loss:0.000412, valid loss:0.000613
Epoch:44, Train loss:0.000422, valid loss:0.000655
Epoch:45, Train loss:0.000408, valid loss:0.000597
Epoch:46, Train loss:0.000409, valid loss:0.000653
Epoch:47, Train loss:0.000404, valid loss:0.000648
Epoch:48, Train loss:0.000404, valid loss:0.000645
Epoch:49, Train loss:0.000410, valid loss:0.000595
Epoch:50, Train loss:0.000394, valid loss:0.000598
Epoch:51, Train loss:0.000373, valid loss:0.000600
Epoch:52, Train loss:0.000369, valid loss:0.000599
Epoch:53, Train loss:0.000367, valid loss:0.000597
Epoch:54, Train loss:0.000366, valid loss:0.000598
Epoch:55, Train loss:0.000365, valid loss:0.000598
Epoch:56, Train loss:0.000364, valid loss:0.000599
Epoch:57, Train loss:0.000363, valid loss:0.000597
Epoch:58, Train loss:0.000362, valid loss:0.000598
Epoch:59, Train loss:0.000362, valid loss:0.000599
Epoch:60, Train loss:0.000361, valid loss:0.000597
training time 18937.00044155121
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.05363782139148785
plot_id,batch_id 0 1 miss% 0.0584915719642705
plot_id,batch_id 0 2 miss% 0.05430756231878516
plot_id,batch_id 0 3 miss% 0.09077397878771948
plot_id,batch_id 0 4 miss% 0.04027861045094214
plot_id,batch_id 0 5 miss% 0.04742221989673191
plot_id,batch_id 0 6 miss% 0.036261699570596446
plot_id,batch_id 0 7 miss% 0.0691704775359546
plot_id,batch_id 0 8 miss% 0.11729179353200118
plot_id,batch_id 0 9 miss% 0.0385535297984753
plot_id,batch_id 0 10 miss% 0.027733099111946238
plot_id,batch_id 0 11 miss% 0.0385420449263125
plot_id,batch_id 0 12 miss% 0.05573794558741068
plot_id,batch_id 0 13 miss% 0.05293377673327347
plot_id,batch_id 0 14 miss% 0.09711094081844605
plot_id,batch_id 0 15 miss% 0.039422742794751726
plot_id,batch_id 0 16 miss% 0.06934206482286669
plot_id,batch_id 0 17 miss% 0.028577239708513037
plot_id,batch_id 0 18 miss% 0.08357339736914902
plot_id,batch_id 0 19 miss% 0.06483160205925159
plot_id,batch_id 0 20 miss% 0.04220541245867258
plot_id,batch_id 0 21 miss% 0.035136646957063866
plot_id,batch_id 0 22 miss% 0.01799256040070517
plot_id,batch_id 0 23 miss% 0.039500824457645654
plot_id,batch_id 0 24 miss% 0.028920710582639952
plot_id,batch_id 0 25 miss% 0.03911782154955115
plot_id,batch_id 0 26 miss% 0.06486334269850155
plot_id,batch_id 0 27 miss% 0.030657447547470806
plot_id,batch_id 0 28 miss% 0.0353663592463982
plot_id,batch_id 0 29 miss% 0.041550715562396826
plot_id,batch_id 0 30 miss% 0.06208196267516396
plot_id,batch_id 0 31 miss% 0.08854366479410174
plot_id,batch_id 0 32 miss% 0.06696536073419398
plot_id,batch_id 0 33 miss% 0.015485872390516941
plot_id,batch_id 0 34 miss% 0.031639525476942675
plot_id,batch_id 0 35 miss% 0.05042278881818874
plot_id,batch_id 0 36 miss% 0.0676531489348671
plot_id,batch_id 0 37 miss% 0.059595343243338435
plot_id,batch_id 0 38 miss% 0.04829185718502498
plot_id,batch_id 0 39 miss% 0.05509273422478797
plot_id,batch_id 0 40 miss% 0.07903626186257748
plot_id,batch_id 0 41 miss% 0.025271095620115917
plot_id,batch_id 0 42 miss% 0.03105723907765061
plot_id,batch_id 0 43 miss% 0.01569985011311723
plot_id,batch_id 0 44 miss% 0.047088052137056455
plot_id,batch_id 0 45 miss% 0.022405428895796612
plot_id,batch_id 0 46 miss% 0.05244399483435325
plot_id,batch_id 0 47 miss% 0.021449142817099948
plot_id,batch_id 0 48 miss% 0.020741571517422272
plot_id,batch_id 0 49 miss% 0.03771627376213483
plot_id,batch_id 0 50 miss% 0.10257391961782014
plot_id,batch_id 0 51 miss% 0.04249239501462555
plot_id,batch_id 0 52 miss% 0.051478055282878535
plot_id,batch_id 0 53 miss% 0.02800208753116844
plot_id,batch_id 0 54 miss% 0.06513246847920212
plot_id,batch_id 0 55 miss% 0.06427688007082707
plot_id,batch_id 0 56 miss% 0.045194311473043834
plot_id,batch_id 0 57 miss% 0.041486616426491964
plot_id,batch_id 0 58 miss% 0.04575365235336416
plot_id,batch_id 0 59 miss% 0.023934884546869446
plot_id,batch_id 0 60 miss% 0.04403130360483066
plot_id,batch_id 0 61 miss% 0.03290362041786469
plot_id,batch_id 0 62 miss% 0.047809945465931016
plot_id,batch_id 0 63 miss% 0.041875119231298746
plot_id,batch_id 0 64 miss% 0.056871970131902226
plot_id,batch_id 0 65 miss% 0.061925069991515404
plot_id,batch_id 0 66 miss% 0.13531770773435234
plot_id,batch_id 0 67 miss% 0.018685003548191515
plot_id,batch_id 0 68 miss% 67 miss% 0.02339518274633142
plot_id,batch_id 0 68 miss% 0.03148871259099464
plot_id,batch_id 0 69 miss% 0.10656937414357018
plot_id,batch_id 0 70 miss% 0.07042767601578556
plot_id,batch_id 0 71 miss% 0.043606287127766846
plot_id,batch_id 0 72 miss% 0.07204237104051214
plot_id,batch_id 0 73 miss% 0.048161867646379614
plot_id,batch_id 0 74 miss% 0.06057129062860028
plot_id,batch_id 0 75 miss% 0.04264292065634793
plot_id,batch_id 0 76 miss% 0.12155147270643575
plot_id,batch_id 0 77 miss% 0.028540988012308453
plot_id,batch_id 0 78 miss% 0.029292295454793296
plot_id,batch_id 0 79 miss% 0.13237990111629686
plot_id,batch_id 0 80 miss% 0.08494259681076066
plot_id,batch_id 0 81 miss% 0.10974850882214901
plot_id,batch_id 0 82 miss% 0.04177386079349056
plot_id,batch_id 0 83 miss% 0.11611906834336246
plot_id,batch_id 0 84 miss% 0.04625611692687546
plot_id,batch_id 0 85 miss% 0.050420757473458075
plot_id,batch_id 0 86 miss% 0.08043239703967076
plot_id,batch_id 0 87 miss% 0.06103468528589049
plot_id,batch_id 0 88 miss% 0.11860257402201747
plot_id,batch_id 0 89 miss% 0.05564698500237772
plot_id,batch_id 0 90 miss% 0.019249234577307362
plot_id,batch_id 0 91 miss% 0.07049855340570622
plot_id,batch_id 0 92 miss% 0.07681739789107896
plot_id,batch_id 0 93 miss% 0.076285927180019
plot_id,batch_id 0 94 miss% 0.12042782142256392
plot_id,batch_id 0 95 miss% 0.053467085618666296
plot_id,batch_id 0 96 miss% 0.07305855323405515
plot_id,batch_id 0 97 miss% 0.03404365889568836
plot_id,batch_id 0 98 miss% 0.02925969967721673
plot_id,batch_id 0 99 miss% 0.04401697812093948
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.10523888 0.05063744 0.08255839 0.08143493 0.08073622 0.07598722
 0.0433582  0.11188972 0.08672188 0.04211947 0.02729288 0.06379088
 0.05887822 0.05696943 0.12078683 0.05799351 0.0714516  0.07905671
 0.08446351 0.09028082 0.02489574 0.04878534 0.07635717 0.02425579
 0.06157812 0.10344231 0.07186237 0.04938326 0.03674203 0.040294
 0.04155347 0.09483288 0.12226433 0.06602737 0.05399985 0.05811695
 0.12720975 0.08391368 0.03044081 0.0514805  0.0712232  0.10126495
 0.06412599 0.05482412 0.02520981 0.0646467  0.04818364 0.0410169
 0.03257695 0.02428919 0.09713792 0.04420191 0.03158725 0.03326036
 0.0733402  0.04959076 0.07400651 0.03460187 0.04124519 0.03804017
 0.03762415 0.04984606 0.05455468 0.06714848 0.05931592 0.05803038
 0.09151294 0.02339518 0.03148871 0.10656937 0.07042768 0.04360629
 0.07204237 0.04816187 0.06057129 0.04264292 0.12155147 0.02854099
 0.0292923  0.1323799  0.0849426  0.10974851 0.04177386 0.11611907
 0.04625612 0.05042076 0.0804324  0.06103469 0.11860257 0.05564699
 0.01924923 0.07049855 0.0768174  0.07628593 0.12042782 0.05346709
 0.07305855 0.03404366 0.0292597  0.04401698]
for model  176 the mean error 0.06374261445923107
all id 176 hidden_dim 24 learning_rate 0.0025 num_layers 4 frames 31 out win 6 err 0.06374261445923107 time 18898.879329442978
Launcher: Job 177 completed in 19156 seconds.
Launcher: Task 104 done. Exiting.
0.06187439384659603
plot_id,batch_id 0 68 miss% 0.072294953374705
plot_id,batch_id 0 69 miss% 0.06346998963505397
plot_id,batch_id 0 70 miss% 0.041616638833959455
plot_id,batch_id 0 71 miss% 0.025259783793690822
plot_id,batch_id 0 72 miss% 0.07780633796748677
plot_id,batch_id 0 73 miss% 0.04052386273937031
plot_id,batch_id 0 74 miss% 0.10566330246442444
plot_id,batch_id 0 75 miss% 0.05109930435568464
plot_id,batch_id 0 76 miss% 0.054187419156345035
plot_id,batch_id 0 77 miss% 0.07114039120515372
plot_id,batch_id 0 78 miss% 0.030885068629827005
plot_id,batch_id 0 79 miss% 0.11642147188418514
plot_id,batch_id 0 80 miss% 0.043000267290826524
plot_id,batch_id 0 81 miss% 0.08289736017712128
plot_id,batch_id 0 82 miss% 0.06285976989952682
plot_id,batch_id 0 83 miss% 0.09495629858682057
plot_id,batch_id 0 84 miss% 0.0425077562901008
plot_id,batch_id 0 85 miss% 0.03289819113776132
plot_id,batch_id 0 86 miss% 0.06507985020985099
plot_id,batch_id 0 87 miss% 0.0640535417218226
plot_id,batch_id 0 88 miss% 0.07591609763902286
plot_id,batch_id 0 89 miss% 0.06623714928305269
plot_id,batch_id 0 90 miss% 0.041697953226174304
plot_id,batch_id 0 91 miss% 0.06631086934024648
plot_id,batch_id 0 92 miss% 0.03700031430842036
plot_id,batch_id 0 93 miss% 0.02979960000014115
plot_id,batch_id 0 94 miss% 0.0688780441239512
plot_id,batch_id 0 95 miss% 0.03193968544103727
plot_id,batch_id 0 96 miss% 0.03582861939993085
plot_id,batch_id 0 97 miss% 0.034692682913754394
plot_id,batch_id 0 98 miss% 0.026935573578272472
plot_id,batch_id 0 99 miss% 0.10838348900149157
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06497122 0.0560964  0.0704216  0.04669844 0.02834817 0.08680404
 0.05276988 0.0686045  0.07946648 0.04629693 0.05454586 0.05111012
 0.04712241 0.04449994 0.0591     0.08749467 0.09422794 0.03321357
 0.04899041 0.06955524 0.10946908 0.04780773 0.09576104 0.03032889
 0.05605927 0.06488748 0.05012719 0.03961179 0.03042478 0.02569542
 0.03249714 0.08179264 0.07037887 0.05307535 0.02495518 0.03037555
 0.07540638 0.05052301 0.02696906 0.04433765 0.05166761 0.06719817
 0.02943002 0.0414473  0.02454721 0.06899472 0.03414914 0.03668863
 0.04149918 0.01837465 0.13660358 0.05096081 0.02794003 0.01703479
 0.04936957 0.084407   0.05732729 0.04103466 0.03071656 0.03765685
 0.02714537 0.03310551 0.0626931  0.04719739 0.07829775 0.09163627
 0.05055858 0.06187439 0.07229495 0.06346999 0.04161664 0.02525978
 0.07780634 0.04052386 0.1056633  0.0510993  0.05418742 0.07114039
 0.03088507 0.11642147 0.04300027 0.08289736 0.06285977 0.0949563
 0.04250776 0.03289819 0.06507985 0.06405354 0.0759161  0.06623715
 0.04169795 0.06631087 0.03700031 0.0297996  0.06887804 0.03193969
 0.03582862 0.03469268 0.02693557 0.10838349]
for model  124 the mean error 0.05492619070581238
all id 124 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 25 out win 5 err 0.05492619070581238 time 18903.882771015167
Launcher: Job 125 completed in 19164 seconds.
Launcher: Task 86 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  69649
Epoch:0, Train loss:0.428404, valid loss:0.407406
Epoch:1, Train loss:0.023446, valid loss:0.002735
Epoch:2, Train loss:0.008825, valid loss:0.002354
Epoch:3, Train loss:0.006710, valid loss:0.001905
Epoch:4, Train loss:0.003479, valid loss:0.001254
Epoch:5, Train loss:0.002443, valid loss:0.001178
Epoch:6, Train loss:0.002226, valid loss:0.001105
Epoch:7, Train loss:0.002039, valid loss:0.001000
Epoch:8, Train loss:0.001861, valid loss:0.000802
Epoch:9, Train loss:0.001764, valid loss:0.000948
Epoch:10, Train loss:0.001680, valid loss:0.001105
Epoch:11, Train loss:0.001286, valid loss:0.000705
Epoch:12, Train loss:0.001254, valid loss:0.000770
Epoch:13, Train loss:0.001189, valid loss:0.000749
Epoch:14, Train loss:0.001144, valid loss:0.000752
Epoch:15, Train loss:0.001145, valid loss:0.000682
Epoch:16, Train loss:0.001094, valid loss:0.000658
Epoch:17, Train loss:0.001071, valid loss:0.000632
Epoch:18, Train loss:0.001036, valid loss:0.000638
Epoch:19, Train loss:0.001043, valid loss:0.000663
Epoch:20, Train loss:0.001011, valid loss:0.000645
Epoch:21, Train loss:0.000814, valid loss:0.000563
Epoch:22, Train loss:0.000791, valid loss:0.000624
Epoch:23, Train loss:0.000779, valid loss:0.000581
Epoch:24, Train loss:0.000769, valid loss:0.000554
Epoch:25, Train loss:0.000771, valid loss:0.000548
Epoch:26, Train loss:0.000740, valid loss:0.000585
Epoch:27, Train loss:0.000738, valid loss:0.000603
Epoch:28, Train loss:0.000739, valid loss:0.000589
Epoch:29, Train loss:0.000724, valid loss:0.000558
Epoch:30, Train loss:0.000715, valid loss:0.000581
Epoch:31, Train loss:0.000623, valid loss:0.000534
Epoch:32, Train loss:0.000622, valid loss:0.000505
Epoch:33, Train loss:0.000615, valid loss:0.000530
Epoch:34, Train loss:0.000609, valid loss:0.000529
Epoch:35, Train loss:0.000603, valid loss:0.000555
Epoch:36, Train loss:0.000600, valid loss:0.000537
Epoch:37, Train loss:0.000589, valid loss:0.000559
Epoch:38, Train loss:0.000586, valid loss:0.000535
Epoch:39, Train loss:0.000586, valid loss:0.000568
Epoch:40, Train loss:0.000590, valid loss:0.000518
Epoch:41, Train loss:0.000527, valid loss:0.000528
Epoch:42, Train loss:0.000524, valid loss:0.000519
Epoch:43, Train loss:0.000524, valid loss:0.000511
Epoch:44, Train loss:0.000524, valid loss:0.000516
Epoch:45, Train loss:0.000522, valid loss:0.000503
Epoch:46, Train loss:0.000516, valid loss:0.000514
Epoch:47, Train loss:0.000520, valid loss:0.000517
Epoch:48, Train loss:0.000513, valid loss:0.000496
Epoch:49, Train loss:0.000513, valid loss:0.000505
Epoch:50, Train loss:0.000506, valid loss:0.000498
Epoch:51, Train loss:0.000480, valid loss:0.000496
Epoch:52, Train loss:0.000476, valid loss:0.000496
Epoch:53, Train loss:0.000474, valid loss:0.000498
Epoch:54, Train loss:0.000474, valid loss:0.000498
Epoch:55, Train loss:0.000473, valid loss:0.000499
Epoch:56, Train loss:0.000472, valid loss:0.000497
Epoch:57, Train loss:0.000472, valid loss:0.000498
Epoch:58, Train loss:0.000471, valid loss:0.000497
Epoch:59, Train loss:0.000471, valid loss:0.000497
Epoch:60, Train loss:0.000470, valid loss:0.000496
training time 18981.967677354813
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.0871204898500225
plot_id,batch_id 0 1 miss% 0.07640411168103944
plot_id,batch_id 0 2 miss% 0.11566297559546022
plot_id,batch_id 0 3 miss% 0.05054885188847596
plot_id,batch_id 0 4 miss% 0.05669112577911318
plot_id,batch_id 0 5 miss% 0.0746101351385161
plot_id,batch_id 0 6 miss% 0.035862525093863645
plot_id,batch_id 0 7 miss% 0.09420910207074369
plot_id,batch_id 0 8 miss% 0.08678316246494942
plot_id,batch_id 0 9 miss% 0.03879179857716934
plot_id,batch_id 0 10 miss% 0.05865173174455521
plot_id,batch_id 0 11 miss% 0.09232466421579957
plot_id,batch_id 0 12 miss% 0.07926667409409953
plot_id,batch_id 0 13 miss% 0.06801274030340214
plot_id,batch_id 0 14 miss% 0.07555474758211791
plot_id,batch_id 0 15 miss% 0.07311045366460227
plot_id,batch_id 0 16 miss% 0.07935680601413304
plot_id,batch_id 0 17 miss% 0.05325568853402764
plot_id,batch_id 0 18 miss% 0.07690689074739628
plot_id,batch_id 0 19 miss% 0.06459075873026793
plot_id,batch_id 0 20 miss% 0.06045367405693972
plot_id,batch_id 0 21 miss% 0.09044779225541123
plot_id,batch_id 0 22 miss% 0.0743376560259831
plot_id,batch_id 0 23 miss% 0.048834124863600455
plot_id,batch_id 0 24 miss% 0.071484768740955
plot_id,batch_id 0 25 miss% 0.09316763563096515
plot_id,batch_id 0 26 miss% 0.07101486799234775
plot_id,batch_id 0 27 miss% 0.04869368761236271
plot_id,batch_id 0 28 miss% 0.026565962203533303
plot_id,batch_id 0 29 miss% 0.034118303493150474
plot_id,batch_id 0 30 miss% 0.024681266404542948
plot_id,batch_id 0 31 miss% 0.08497988602876587
plot_id,batch_id 0 32 miss% 0.09232456604243622
plot_id,batch_id 0 33 miss% 0.04998733478197866
plot_id,batch_id 0 34 miss% 0.05259969185150143
plot_id,batch_id 0 35 miss% 0.059687645133443615
plot_id,batch_id 0 36 miss% 0.09899499986870505
plot_id,batch_id 0 37 miss% 0.0824464649536724
plot_id,batch_id 0 38 miss% 0.04492120041508426
plot_id,batch_id 0 39 miss% 0.040149914433644324
plot_id,batch_id 0 40 miss% 0.04224506018643903
plot_id,batch_id 0 41 miss% 0.044984481823198134
plot_id,batch_id 0 42 miss% 0.052380344287841836
plot_id,batch_id 0 43 miss% 0.07483142847018232
plot_id,batch_id 0 44 miss% 0.03020245514396398
plot_id,batch_id 0 45 miss% 0.050206949142109276
plot_id,batch_id 0 46 miss% 0.050450889584516746
plot_id,batch_id 0 47 miss% 0.031769216069842246
plot_id,batch_id 0 48 miss% 0.021706395729543214
plot_id,batch_id 0 49 miss% 0.03648197309086811
plot_id,batch_id 0 50 miss% 0.14995865224195962
plot_id,batch_id 0 51 miss% 0.04008115810078274
plot_id,batch_id 0 52 miss% 0.03588132324616986
plot_id,batch_id 0 53 miss% 0.038220209896507144
plot_id,batch_id 0 54 miss% 0.06382528646125793
plot_id,batch_id 0 55 miss% 0.08447791701142099
plot_id,batch_id 0 56 miss% 0.07021466592492022
plot_id,batch_id 0 57 miss% 0.06214819741003873
plot_id,batch_id 0 58 miss% 0.07531995454024314
plot_id,batch_id 0 59 miss% 0.05290135856326506
plot_id,batch_id 0 60 miss% 0.03672211017581564
plot_id,batch_id 0 61 miss% 0.08655265610524711
plot_id,batch_id 0 62 miss% 0.08899889457859994
plot_id,batch_id 0 63 miss% 0.06096110127779842
plot_id,batch_id 0 64 miss% 0.06368707853580487
plot_id,batch_id 0 65 miss% 0.11724294843676977
plot_id,batch_id 0 66 miss% 0.07341069435775413
plot_id,batch_id 0 67 miss% 0.05156454171172407
plot_id,batch_id 0 69 miss% 0.08102812632463438
plot_id,batch_id 0 70 miss% 0.11223695120866316
plot_id,batch_id 0 71 miss% 0.04440429620150134
plot_id,batch_id 0 72 miss% 0.07795331258237631
plot_id,batch_id 0 73 miss% 0.03660986530793421
plot_id,batch_id 0 74 miss% 0.04632555125706003
plot_id,batch_id 0 75 miss% 0.034560193839372755
plot_id,batch_id 0 76 miss% 0.040479230641128926
plot_id,batch_id 0 77 miss% 0.02900612759484887
plot_id,batch_id 0 78 miss% 0.03146941090949667
plot_id,batch_id 0 79 miss% 0.07764842930656246
plot_id,batch_id 0 80 miss% 0.05064697056424019
plot_id,batch_id 0 81 miss% 0.0822278889937218
plot_id,batch_id 0 82 miss% 0.05968009826614569
plot_id,batch_id 0 83 miss% 0.09448451951468285
plot_id,batch_id 0 84 miss% 0.042732658318582584
plot_id,batch_id 0 85 miss% 0.04505347780870101
plot_id,batch_id 0 86 miss% 0.0904042406530516
plot_id,batch_id 0 87 miss% 0.07926081595359748
plot_id,batch_id 0 88 miss% 0.06927739505754257
plot_id,batch_id 0 89 miss% 0.048786979838563706
plot_id,batch_id 0 90 miss% 0.02378077107491429
plot_id,batch_id 0 91 miss% 0.0371451598907289
plot_id,batch_id 0 92 miss% 0.03248672180906586
plot_id,batch_id 0 93 miss% 0.022786977179094156
plot_id,batch_id 0 94 miss% 0.05677139085422449
plot_id,batch_id 0 95 miss% 0.03738348454867602
plot_id,batch_id 0 96 miss% 0.07497151067900587
plot_id,batch_id 0 97 miss% 0.03402520614628566
plot_id,batch_id 0 98 miss% 0.03518421503137288
plot_id,batch_id 0 99 miss% 0.12095646884708115
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05363782 0.05849157 0.05430756 0.09077398 0.04027861 0.04742222
 0.0362617  0.06917048 0.11729179 0.03855353 0.0277331  0.03854204
 0.05573795 0.05293378 0.09711094 0.03942274 0.06934206 0.02857724
 0.0835734  0.0648316  0.04220541 0.03513665 0.01799256 0.03950082
 0.02892071 0.03911782 0.06486334 0.03065745 0.03536636 0.04155072
 0.06208196 0.08854366 0.06696536 0.01548587 0.03163953 0.05042279
 0.06765315 0.05959534 0.04829186 0.05509273 0.07903626 0.0252711
 0.03105724 0.01569985 0.04708805 0.02240543 0.05244399 0.02144914
 0.02074157 0.03771627 0.10257392 0.0424924  0.05147806 0.02800209
 0.06513247 0.06427688 0.04519431 0.04148662 0.04575365 0.02393488
 0.0440313  0.03290362 0.04780995 0.04187512 0.05687197 0.06192507
 0.13531771 0.018685   0.05156454 0.08102813 0.11223695 0.0444043
 0.07795331 0.03660987 0.04632555 0.03456019 0.04047923 0.02900613
 0.03146941 0.07764843 0.05064697 0.08222789 0.0596801  0.09448452
 0.04273266 0.04505348 0.09040424 0.07926082 0.0692774  0.04878698
 0.02378077 0.03714516 0.03248672 0.02278698 0.05677139 0.03738348
 0.07497151 0.03402521 0.03518422 0.12095647]
for model  152 the mean error 0.051890651285911416
all id 152 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 25 out win 6 err 0.051890651285911416 time 18937.00044155121
Launcher: Job 153 completed in 19196 seconds.
Launcher: Task 110 done. Exiting.
0.045062721912456014
plot_id,batch_id 0 68 miss% 0.038457521102194546
plot_id,batch_id 0 69 miss% 0.06686665296157798
plot_id,batch_id 0 70 miss% 0.04781970174072155
plot_id,batch_id 0 71 miss% 0.03956662726187872
plot_id,batch_id 0 72 miss% 0.08963226682887294
plot_id,batch_id 0 73 miss% 0.043131468719837814
plot_id,batch_id 0 74 miss% 0.14976914752790912
plot_id,batch_id 0 75 miss% 0.035650190718300556
plot_id,batch_id 0 76 miss% 0.0465612470285934
plot_id,batch_id 0 77 miss% 0.041350337071894804
plot_id,batch_id 0 78 miss% 0.04337892036489208
plot_id,batch_id 0 79 miss% 0.06826532169691182
plot_id,batch_id 0 80 miss% 0.0395390026913948
plot_id,batch_id 0 81 miss% 0.10591571581035544
plot_id,batch_id 0 82 miss% 0.09304456741682857
plot_id,batch_id 0 83 miss% 0.07530231872526906
plot_id,batch_id 0 84 miss% 0.09703291735818052
plot_id,batch_id 0 85 miss% 0.05348911650034754
plot_id,batch_id 0 86 miss% 0.037393736308359744
plot_id,batch_id 0 87 miss% 0.05999967963866347
plot_id,batch_id 0 88 miss% 0.14734973200271445
plot_id,batch_id 0 89 miss% 0.05511090522759136
plot_id,batch_id 0 90 miss% 0.04540060768708833
plot_id,batch_id 0 91 miss% 0.02927749229951935
plot_id,batch_id 0 92 miss% 0.0677838646416324
plot_id,batch_id 0 93 miss% 0.0355295220747195
plot_id,batch_id 0 94 miss% 0.0337440086712549
plot_id,batch_id 0 95 miss% 0.051207619418551356
plot_id,batch_id 0 96 miss% 0.042867953609320865
plot_id,batch_id 0 97 miss% 0.04822335218095553
plot_id,batch_id 0 98 miss% 0.04100505808808342
plot_id,batch_id 0 99 miss% 0.037601317969281796
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08712049 0.07640411 0.11566298 0.05054885 0.05669113 0.07461014
 0.03586253 0.0942091  0.08678316 0.0387918  0.05865173 0.09232466
 0.07926667 0.06801274 0.07555475 0.07311045 0.07935681 0.05325569
 0.07690689 0.06459076 0.06045367 0.09044779 0.07433766 0.04883412
 0.07148477 0.09316764 0.07101487 0.04869369 0.02656596 0.0341183
 0.02468127 0.08497989 0.09232457 0.04998733 0.05259969 0.05968765
 0.098995   0.08244646 0.0449212  0.04014991 0.04224506 0.04498448
 0.05238034 0.07483143 0.03020246 0.05020695 0.05045089 0.03176922
 0.0217064  0.03648197 0.14995865 0.04008116 0.03588132 0.03822021
 0.06382529 0.08447792 0.07021467 0.0621482  0.07531995 0.05290136
 0.03672211 0.08655266 0.08899889 0.0609611  0.06368708 0.11724295
 0.07341069 0.04506272 0.03845752 0.06686665 0.0478197  0.03956663
 0.08963227 0.04313147 0.14976915 0.03565019 0.04656125 0.04135034
 0.04337892 0.06826532 0.039539   0.10591572 0.09304457 0.07530232
 0.09703292 0.05348912 0.03739374 0.05999968 0.14734973 0.05511091
 0.04540061 0.02927749 0.06778386 0.03552952 0.03374401 0.05120762
 0.04286795 0.04822335 0.04100506 0.03760132]
for model  209 the mean error 0.06275800890207793
all id 209 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 31 out win 6 err 0.06275800890207793 time 18981.967677354813
Launcher: Job 210 completed in 19239 seconds.
Launcher: Task 124 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  120401
Epoch:0, Train loss:0.429770, valid loss:0.417277
Epoch:1, Train loss:0.058076, valid loss:0.003848
Epoch:2, Train loss:0.006404, valid loss:0.001387
Epoch:3, Train loss:0.003059, valid loss:0.001305
Epoch:4, Train loss:0.002686, valid loss:0.001262
Epoch:5, Train loss:0.002396, valid loss:0.001076
Epoch:6, Train loss:0.002131, valid loss:0.001388
Epoch:7, Train loss:0.002033, valid loss:0.001047
Epoch:8, Train loss:0.001924, valid loss:0.000988
Epoch:9, Train loss:0.001835, valid loss:0.001027
Epoch:10, Train loss:0.001687, valid loss:0.000883
Epoch:11, Train loss:0.001259, valid loss:0.000739
Epoch:12, Train loss:0.001212, valid loss:0.000774
Epoch:13, Train loss:0.001206, valid loss:0.000750
Epoch:14, Train loss:0.001152, valid loss:0.000692
Epoch:15, Train loss:0.001148, valid loss:0.000619
Epoch:16, Train loss:0.001065, valid loss:0.000672
Epoch:17, Train loss:0.001051, valid loss:0.000657
Epoch:18, Train loss:0.001056, valid loss:0.000682
Epoch:19, Train loss:0.001013, valid loss:0.000709
Epoch:20, Train loss:0.000976, valid loss:0.000693
Epoch:21, Train loss:0.000747, valid loss:0.000587
Epoch:22, Train loss:0.000731, valid loss:0.000579
Epoch:23, Train loss:0.000743, valid loss:0.000547
Epoch:24, Train loss:0.000705, valid loss:0.000544
Epoch:25, Train loss:0.000687, valid loss:0.000550
Epoch:26, Train loss:0.000686, valid loss:0.000563
Epoch:27, Train loss:0.000663, valid loss:0.000554
Epoch:28, Train loss:0.000649, valid loss:0.000515
Epoch:29, Train loss:0.000636, valid loss:0.000523
Epoch:30, Train loss:0.000626, valid loss:0.000509
Epoch:31, Train loss:0.000526, valid loss:0.000527
Epoch:32, Train loss:0.000504, valid loss:0.000585
Epoch:33, Train loss:0.000501, valid loss:0.000507
Epoch:34, Train loss:0.000498, valid loss:0.000519
Epoch:35, Train loss:0.000486, valid loss:0.000503
Epoch:36, Train loss:0.000483, valid loss:0.000524
Epoch:37, Train loss:0.000495, valid loss:0.000513
Epoch:38, Train loss:0.000476, valid loss:0.000503
Epoch:39, Train loss:0.000475, valid loss:0.000526
Epoch:40, Train loss:0.000472, valid loss:0.000539
Epoch:41, Train loss:0.000412, valid loss:0.000497
Epoch:42, Train loss:0.000405, valid loss:0.000491
Epoch:43, Train loss:0.000408, valid loss:0.000545
Epoch:44, Train loss:0.000404, valid loss:0.000489
Epoch:45, Train loss:0.000402, valid loss:0.000487
Epoch:46, Train loss:0.000399, valid loss:0.000497
Epoch:47, Train loss:0.000394, valid loss:0.000489
Epoch:48, Train loss:0.000392, valid loss:0.000491
Epoch:49, Train loss:0.000392, valid loss:0.000497
Epoch:50, Train loss:0.000388, valid loss:0.000492
Epoch:51, Train loss:0.000374, valid loss:0.000482
Epoch:52, Train loss:0.000370, valid loss:0.000481
Epoch:53, Train loss:0.000369, valid loss:0.000479
Epoch:54, Train loss:0.000367, valid loss:0.000480
Epoch:55, Train loss:0.000366, valid loss:0.000479
Epoch:56, Train loss:0.000366, valid loss:0.000478
Epoch:57, Train loss:0.000365, valid loss:0.000478
Epoch:58, Train loss:0.000365, valid loss:0.000477
Epoch:59, Train loss:0.000364, valid loss:0.000476
Epoch:60, Train loss:0.000364, valid loss:0.000477
training time 19059.789984703064
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.29840479672541104
plot_id,batch_id 0 1 miss% 0.3677105761509882
plot_id,batch_id 0 2 miss% 0.3835551604102367
plot_id,batch_id 0 3 miss% 0.313384912928989
plot_id,batch_id 0 4 miss% 0.33496076856407914
plot_id,batch_id 0 5 miss% 0.3430585076689072
plot_id,batch_id 0 6 miss% 0.3977335141599091
plot_id,batch_id 0 7 miss% 0.47830593831795415
plot_id,batch_id 0 8 miss% 0.5879077606522541
plot_id,batch_id 0 9 miss% 0.3732708062875683
plot_id,batch_id 0 10 miss% 0.2607230143915224
plot_id,batch_id 0 11 miss% 0.3491450765009982
plot_id,batch_id 0 12 miss% 0.4764761974746953
plot_id,batch_id 0 13 miss% 0.3503204021996414
plot_id,batch_id 0 14 miss% 0.4559656958668427
plot_id,batch_id 0 15 miss% 0.39127925555093984
plot_id,batch_id 0 16 miss% 0.4534536938844198
plot_id,batch_id 0 17 miss% 0.4969060290220098
plot_id,batch_id 0 18 miss% 0.38091290214223694
plot_id,batch_id 0 19 miss% 0.419334424695429
plot_id,batch_id 0 20 miss% 0.6287802826882225
plot_id,batch_id 0 21 miss% 0.3310866805801759
plot_id,batch_id 0 22 miss% 0.279944952509619
plot_id,batch_id 0 23 miss% 0.30463923004616494
plot_id,batch_id 0 24 miss% 0.2397236003819696
plot_id,batch_id 0 25 miss% 0.30728983895708284
plot_id,batch_id 0 26 miss% 0.36738336607103533
plot_id,batch_id 0 27 miss% 0.34549583372333936
plot_id,batch_id 0 28 miss% 0.2431641287214637
plot_id,batch_id 0 29 miss% 0.2939504709709421
plot_id,batch_id 0 30 miss% 0.3632840776557081
plot_id,batch_id 0 31 miss% 0.4615533965037296
plot_id,batch_id 0 32 miss% 0.3856172509499904
plot_id,batch_id 0 33 miss% 0.3210147195298897
plot_id,batch_id 0 34 miss% 0.3211054856785596
plot_id,batch_id 0 35 miss% 0.28055052494629784
plot_id,batch_id 0 36 miss% 0.5448142670843966
plot_id,batch_id 0 37 miss% 0.3251184342204005
plot_id,batch_id 0 38 miss% 0.3893060095428057
plot_id,batch_id 0 39 miss% 0.30447005698559826
plot_id,batch_id 0 40 miss% 0.3712917883273238
plot_id,batch_id 0 41 miss% 0.3826546711119628
plot_id,batch_id 0 42 miss% 0.26092923342631263
plot_id,batch_id 0 43 miss% 0.3738552212903418
plot_id,batch_id 0 44 miss% 0.30740366013371423
plot_id,batch_id 0 45 miss% 0.46860440859500446
plot_id,batch_id 0 46 miss% 0.3553629937308763
plot_id,batch_id 0 47 miss% 0.2928292580299845
plot_id,batch_id 0 48 miss% 0.28335633238944224
plot_id,batch_id 0 49 miss% 0.24298854923266347
plot_id,batch_id 0 50 miss% 0.4628244515496831
plot_id,batch_id 0 51 miss% 0.39632312051134805
plot_id,batch_id 0 52 miss% 0.26813660154417274
plot_id,batch_id 0 53 miss% 0.2217193305089707
plot_id,batch_id 0 54 miss% 0.38243011137694427
plot_id,batch_id 0 55 miss% 0.47122338567091687
plot_id,batch_id 0 56 miss% 0.4362565085083847
plot_id,batch_id 0 57 miss% 0.37784608514739365
plot_id,batch_id 0 58 miss% 0.44705147523888444
plot_id,batch_id 0 59 miss% 0.2811622624538609
plot_id,batch_id 0 60 miss% 0.2590413848329789
plot_id,batch_id 0 61 miss% 0.26786599282798734
plot_id,batch_id 0 62 miss% 0.3321717939743097
plot_id,batch_id 0 63 miss% 0.39662731859881895
plot_id,batch_id 0 64 miss% 0.40499240806698644
plot_id,batch_id 0 65 miss% 0.5029756350616181
plot_id,batch_id 0 66 miss% 0.4146515494580057
plot_id,batch_id 0 67 miss% 0.3010507831791296
plot_id,batch_id 0 68 miss% 0.44140292996389674
plot_id,batch_id 0 69 miss% 0.386881317096657
plot_id,batch_id 0 70 miss% 0.27558284310324443
plot_id,batch_id 0 71 miss% 0.3691189055388292
plot_id,batch_id 0 72 miss% 0.3321303890340458
plot_id,batch_id 0 73 miss% 0.33362334703950314
plot_id,batch_id 0 74 miss% 0.42714617002611077
plot_id,batch_id 0 75 miss% 0.23647156426198165
plot_id,batch_id 0 76 miss% 0.3486534781658004
plot_id,batch_id 0 77 miss% 0.3408488374006792
plot_id,batch_id 0 78 miss% 0.36540006193059815
plot_id,batch_id 0 79 miss% 0.3541784291791448
plot_id,batch_id 0 80 miss% 0.2761018886476538
plot_id,batch_id 0 81 miss% 0.46891441820810476
plot_id,batch_id 0 82 miss% 0.3930620119249855
plot_id,batch_id 0 83 miss% 0.39930456417708904
plot_id,batch_id 0 84 miss% 0.3385654988727885
plot_id,batch_id 0 85 miss% 0.27340894776596797
plot_id,batch_id 0 86 miss% 0.3934022851594174
plot_id,batch_id 0 87 miss% 0.4207549492126618
plot_id,batch_id 0 88 miss% 0.4488960632148169
plot_id,batch_id 0 89 miss% 0.3891744922347422
plot_id,batch_id 0 90 miss% 0.2180626507050722
plot_id,batch_id 0 91 miss% 0.32083167398212276
plot_id,batch_id 0 92 miss% 0.34739610072044724
plot_id,batch_id 0 93 miss% 0.31205253219549445
plot_id,batch_id 0 94 miss% 0.33717962839266336
plot_id,batch_id 0 95 miss% 0.27727119624644786
plot_id,batch_id 0 96 miss% 0.35200744532907036
plot_id,batch_id 0 97 miss% 0.46598857570430857
plot_id,batch_id 0 98 miss% 0.4932735028740376
plot_id,batch_id 0 99 miss% 0.40440975785575745
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.2984048  0.36771058 0.38355516 0.31338491 0.33496077 0.34305851
 0.39773351 0.47830594 0.58790776 0.37327081 0.26072301 0.34914508
 0.4764762  0.3503204  0.4559657  0.39127926 0.45345369 0.49690603
 0.3809129  0.41933442 0.62878028 0.33108668 0.27994495 0.30463923
 0.2397236  0.30728984 0.36738337 0.34549583 0.24316413 0.29395047
 0.36328408 0.4615534  0.38561725 0.32101472 0.32110549 0.28055052
 0.54481427 0.32511843 0.38930601 0.30447006 0.37129179 0.38265467
 0.26092923 0.37385522 0.30740366 0.46860441 0.35536299 0.29282926
 0.28335633 0.24298855 0.46282445 0.39632312 0.2681366  0.22171933
 0.38243011 0.47122339 0.43625651 0.37784609 0.44705148 0.28116226
 0.25904138 0.26786599 0.33217179 0.39662732 0.40499241 0.50297564
 0.41465155 0.30105078 0.44140293 0.38688132 0.27558284 0.36911891
 0.33213039 0.33362335 0.42714617 0.23647156 0.34865348 0.34084884
 0.36540006 0.35417843 0.27610189 0.46891442 0.39306201 0.39930456
 0.3385655  0.27340895 0.39340229 0.42075495 0.44889606 0.38917449
 0.21806265 0.32083167 0.3473961  0.31205253 0.33717963 0.2772712
 0.35200745 0.46598858 0.4932735  0.40440976]
for model  230 the mean error 0.3645622481228459
all id 230 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 31 out win 6 err 0.3645622481228459 time 19059.789984703064
Launcher: Job 231 completed in 19285 seconds.
Launcher: Task 38 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  265233
Epoch:0, Train loss:0.489850, valid loss:0.463285
Epoch:1, Train loss:0.534316, valid loss:0.338035
Epoch:2, Train loss:0.694438, valid loss:0.600365
Epoch:3, Train loss:1.123439, valid loss:1.006373
Epoch:4, Train loss:1.385679, valid loss:1.488738
Epoch:5, Train loss:1.780099, valid loss:1.490329
Epoch:6, Train loss:1.783210, valid loss:1.488365
Epoch:7, Train loss:1.778802, valid loss:1.471031
Epoch:8, Train loss:1.782814, valid loss:1.488963
Epoch:9, Train loss:1.781979, valid loss:1.486000
Epoch:10, Train loss:1.775550, valid loss:1.484748
Epoch:11, Train loss:1.764948, valid loss:1.449460
Epoch:12, Train loss:1.734755, valid loss:1.449182
Epoch:13, Train loss:1.733936, valid loss:1.450910
Epoch:14, Train loss:1.733960, valid loss:1.450655
Epoch:15, Train loss:1.744114, valid loss:1.480070
Epoch:16, Train loss:1.755377, valid loss:1.449677
Epoch:17, Train loss:1.739602, valid loss:1.450294
Epoch:18, Train loss:1.467549, valid loss:0.999125
Epoch:19, Train loss:1.227754, valid loss:0.975657
Epoch:20, Train loss:1.215123, valid loss:0.981463
Epoch:21, Train loss:1.139613, valid loss:0.903607
Epoch:22, Train loss:1.102446, valid loss:0.906175
Epoch:23, Train loss:1.101627, valid loss:0.905623
Epoch:24, Train loss:1.100138, valid loss:0.902610
Epoch:25, Train loss:1.098937, valid loss:0.905043
Epoch:26, Train loss:1.097826, valid loss:0.903481
Epoch:27, Train loss:1.095944, valid loss:0.902509
Epoch:28, Train loss:1.094501, valid loss:0.900636
Epoch:29, Train loss:1.091528, valid loss:0.900523
Epoch:30, Train loss:1.090044, valid loss:0.898822
Epoch:31, Train loss:1.089062, valid loss:0.898554
Epoch:32, Train loss:1.088845, valid loss:0.898242
Epoch:33, Train loss:1.088659, valid loss:0.898199
Epoch:34, Train loss:1.088545, valid loss:0.898158
Epoch:35, Train loss:1.088416, valid loss:0.898379
Epoch:36, Train loss:1.088301, valid loss:0.898062
Epoch:37, Train loss:1.088172, valid loss:0.898100
Epoch:38, Train loss:1.088079, valid loss:0.898004
Epoch:39, Train loss:1.087939, valid loss:0.898537
Epoch:40, Train loss:1.087901, valid loss:0.898016
Epoch:41, Train loss:1.087651, valid loss:0.897819
Epoch:42, Train loss:1.087626, valid loss:0.897827
Epoch:43, Train loss:1.087603, valid loss:0.897789
Epoch:44, Train loss:1.087578, valid loss:0.897791
Epoch:45, Train loss:1.087557, valid loss:0.897786
Epoch:46, Train loss:1.087518, valid loss:0.897801
Epoch:47, Train loss:1.087494, valid loss:0.897781
Epoch:48, Train loss:1.087481, valid loss:0.897735
Epoch:49, Train loss:1.087433, valid loss:0.897742
Epoch:50, Train loss:1.087418, valid loss:0.897794
Epoch:51, Train loss:1.178728, valid loss:0.924250
Epoch:52, Train loss:1.138323, valid loss:0.958747
Epoch:53, Train loss:1.115513, valid loss:0.908347
Epoch:54, Train loss:1.107326, valid loss:0.903089
Epoch:55, Train loss:1.102349, valid loss:0.911732
Epoch:56, Train loss:1.099512, valid loss:0.906873
Epoch:57, Train loss:1.098392, valid loss:0.900593
Epoch:58, Train loss:1.096622, valid loss:0.901562
Epoch:59, Train loss:1.096297, valid loss:0.900195
Epoch:60, Train loss:1.095135, valid loss:0.899727
training time 19124.095629692078
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.6420773082816262
plot_id,batch_id 0 1 miss% 0.7541113932971505
plot_id,batch_id 0 2 miss% 0.7605860618098716
plot_id,batch_id 0 3 miss% 0.7897152804526911
plot_id,batch_id 0 4 miss% 0.6366741338048824
plot_id,batch_id 0 5 miss% 0.7418352655906646
plot_id,batch_id 0 6 miss% 0.7870957623258652
plot_id,batch_id 0 7 miss% 0.686136437903262
plot_id,batch_id 0 8 miss% 0.6488689670438196
plot_id,batch_id 0 9 miss% 0.7441623588301964
plot_id,batch_id 0 10 miss% 0.665706493835557
plot_id,batch_id 0 11 miss% 0.7045224127250864
plot_id,batch_id 0 12 miss% 0.6883410127440549
plot_id,batch_id 0 13 miss% 0.656714165790005
plot_id,batch_id 0 14 miss% 0.5985998410262732
plot_id,batch_id 0 15 miss% 0.662179250281623
plot_id,batch_id 0 16 miss% 0.756331475128645
plot_id,batch_id 0 17 miss% 0.8674030911927435
plot_id,batch_id 0 18 miss% 0.599797279456232
plot_id,batch_id 0 19 miss% 0.5271895747086376
plot_id,batch_id 0 20 miss% 0.7710692895058049
plot_id,batch_id 0 21 miss% 0.7553840310024583
plot_id,batch_id 0 22 miss% 0.6539641307651919
plot_id,batch_id 0 23 miss% 0.8680870382431123
plot_id,batch_id 0 24 miss% 0.6972059696430085
plot_id,batch_id 0 25 miss% 0.7748979036210183
plot_id,batch_id 0 26 miss% 0.7996906138647663
plot_id,batch_id 0 27 miss% 0.6783800387304408
plot_id,batch_id 0 28 miss% 0.7546742750471989
plot_id,batch_id 0 29 miss% 0.7527554881862221
plot_id,batch_id 0 30 miss% 0.5606377985606509
plot_id,batch_id 0 31 miss% 0.6809332799072432
plot_id,batch_id 0 32 miss% 0.7270800200709286
plot_id,batch_id 0 33 miss% 0.7192775531504164
plot_id,batch_id 0 34 miss% 0.6533064263123533
plot_id,batch_id 0 35 miss% 0.8195670655941404
plot_id,batch_id 0 36 miss% 0.8499380667003515
plot_id,batch_id 0 37 miss% 0.5421870583710013
plot_id,batch_id 0 38 miss% 0.734036460211532
plot_id,batch_id 0 39 miss% 0.9049433687137577
plot_id,batch_id 0 40 miss% 0.6555406611139809
plot_id,batch_id 0 41 miss% 0.710932000786338
plot_id,batch_id 0 42 miss% 0.7626055473269784
plot_id,batch_id 0 43 miss% 0.6747623875701666
plot_id,batch_id 0 44 miss% 0.8402423348435359
plot_id,batch_id 0 45 miss% 0.8266454635687375
plot_id,batch_id 0 46 miss% 0.7595600233346579
plot_id,batch_id 0 47 miss% 0.7184320759694012
plot_id,batch_id 0 48 miss% 0.7986707797150526
plot_id,batch_id 0 49 miss% 0.8067353505808771
plot_id,batch_id 0 50 miss% 0.49645576322790297
plot_id,batch_id 0 51 miss% 0.7180803274148643
plot_id,batch_id 0 52 miss% 0.7457046196575514
plot_id,batch_id 0 53 miss% 0.8267644169761765
plot_id,batch_id 0 54 miss% 0.8377183462477846
plot_id,batch_id 0 55 miss% 0.5724771503026013
plot_id,batch_id 0 56 miss% 0.7063371913547618
plot_id,batch_id 0 57 miss% 0.8439996792287985
plot_id,batch_id 0 58 miss% 0.8085010846124171
plot_id,batch_id 0 59 miss% 0.7367742071480518
plot_id,batch_id 0 60 miss% 0.7515368331306829
plot_id,batch_id 0 61 miss% 0.5169037811901885
plot_id,batch_id 0 62 miss% 0.6478386806844667
plot_id,batch_id 0 63 miss% 0.6103934202040117
plot_id,batch_id 0 64 miss% 0.5725855397104843
plot_id,batch_id 0 65 miss% 0.8932385453130143
plot_id,batch_id 0 66 miss% 0.7244268920638479
plot_id,batch_id 0 67 miss% 0.5833201349489602
plot_id,batch_id 0 68 miss% 0.6127365384864124
plot_id,batch_id 0 69 miss% 0.8080168504031876the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  151697
Epoch:0, Train loss:0.399691, valid loss:0.359399
Epoch:1, Train loss:0.086175, valid loss:0.002280
Epoch:2, Train loss:0.005521, valid loss:0.001958
Epoch:3, Train loss:0.004427, valid loss:0.001332
Epoch:4, Train loss:0.003326, valid loss:0.001019
Epoch:5, Train loss:0.001992, valid loss:0.001099
Epoch:6, Train loss:0.001724, valid loss:0.000797
Epoch:7, Train loss:0.001584, valid loss:0.000919
Epoch:8, Train loss:0.001483, valid loss:0.000701
Epoch:9, Train loss:0.001388, valid loss:0.000972
Epoch:10, Train loss:0.001317, valid loss:0.000650
Epoch:11, Train loss:0.000951, valid loss:0.000574
Epoch:12, Train loss:0.000933, valid loss:0.000560
Epoch:13, Train loss:0.000889, valid loss:0.000558
Epoch:14, Train loss:0.000876, valid loss:0.000540
Epoch:15, Train loss:0.000859, valid loss:0.000510
Epoch:16, Train loss:0.000819, valid loss:0.000526
Epoch:17, Train loss:0.000817, valid loss:0.000512
Epoch:18, Train loss:0.000781, valid loss:0.000508
Epoch:19, Train loss:0.000787, valid loss:0.000476
Epoch:20, Train loss:0.000774, valid loss:0.000602
Epoch:21, Train loss:0.000589, valid loss:0.000420
Epoch:22, Train loss:0.000581, valid loss:0.000430
Epoch:23, Train loss:0.000572, valid loss:0.000396
Epoch:24, Train loss:0.000560, valid loss:0.000420
Epoch:25, Train loss:0.000546, valid loss:0.000442
Epoch:26, Train loss:0.000541, valid loss:0.000423
Epoch:27, Train loss:0.000538, valid loss:0.000498
Epoch:28, Train loss:0.000519, valid loss:0.000405
Epoch:29, Train loss:0.000525, valid loss:0.000399
Epoch:30, Train loss:0.000514, valid loss:0.000419
Epoch:31, Train loss:0.000431, valid loss:0.000392
Epoch:32, Train loss:0.000423, valid loss:0.000386
Epoch:33, Train loss:0.000425, valid loss:0.000385
Epoch:34, Train loss:0.000419, valid loss:0.000387
Epoch:35, Train loss:0.000427, valid loss:0.000356
Epoch:36, Train loss:0.000417, valid loss:0.000384
Epoch:37, Train loss:0.000407, valid loss:0.000378
Epoch:38, Train loss:0.000402, valid loss:0.000352
Epoch:39, Train loss:0.000408, valid loss:0.000411
Epoch:40, Train loss:0.000405, valid loss:0.000364
Epoch:41, Train loss:0.000360, valid loss:0.000356
Epoch:42, Train loss:0.000357, valid loss:0.000352
Epoch:43, Train loss:0.000357, valid loss:0.000368
Epoch:44, Train loss:0.000355, valid loss:0.000352
Epoch:45, Train loss:0.000351, valid loss:0.000352
Epoch:46, Train loss:0.000354, valid loss:0.000353
Epoch:47, Train loss:0.000349, valid loss:0.000371
Epoch:48, Train loss:0.000346, valid loss:0.000354
Epoch:49, Train loss:0.000341, valid loss:0.000346
Epoch:50, Train loss:0.000351, valid loss:0.000354
Epoch:51, Train loss:0.000324, valid loss:0.000352
Epoch:52, Train loss:0.000322, valid loss:0.000347
Epoch:53, Train loss:0.000321, valid loss:0.000353
Epoch:54, Train loss:0.000321, valid loss:0.000352
Epoch:55, Train loss:0.000321, valid loss:0.000352
Epoch:56, Train loss:0.000320, valid loss:0.000355
Epoch:57, Train loss:0.000320, valid loss:0.000349
Epoch:58, Train loss:0.000320, valid loss:0.000356
Epoch:59, Train loss:0.000320, valid loss:0.000359
Epoch:60, Train loss:0.000320, valid loss:0.000348
training time 19111.20187807083
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.06376793705940136
plot_id,batch_id 0 1 miss% 0.0416168749974603
plot_id,batch_id 0 2 miss% 0.0961421376976728
plot_id,batch_id 0 3 miss% 0.03583781651933846
plot_id,batch_id 0 4 miss% 0.0402331768823705
plot_id,batch_id 0 5 miss% 0.029706416803854222
plot_id,batch_id 0 6 miss% 0.08187515162493528
plot_id,batch_id 0 7 miss% 0.10329570402677046
plot_id,batch_id 0 8 miss% 0.09391741506269848
plot_id,batch_id 0 9 miss% 0.04491893564439195
plot_id,batch_id 0 10 miss% 0.026060869515324284
plot_id,batch_id 0 11 miss% 0.05159160019349771
plot_id,batch_id 0 12 miss% 0.04361551666239548
plot_id,batch_id 0 13 miss% 0.04908636248928383
plot_id,batch_id 0 14 miss% 0.06668879223291002
plot_id,batch_id 0 15 miss% 0.046238341469244555
plot_id,batch_id 0 16 miss% 0.05006529784377892
plot_id,batch_id 0 17 miss% 0.04362489845258848
plot_id,batch_id 0 18 miss% 0.06384846014503925
plot_id,batch_id 0 19 miss% 0.07756882846529248
plot_id,batch_id 0 20 miss% 0.09301836495693463
plot_id,batch_id 0 21 miss% 0.0946319417794776
plot_id,batch_id 0 22 miss% 0.04015892223074849
plot_id,batch_id 0 23 miss% 0.03108445675168917
plot_id,batch_id 0 24 miss% 0.024855671435458474
plot_id,batch_id 0 25 miss% 0.07291739562335704
plot_id,batch_id 0 26 miss% 0.06463978543100561
plot_id,batch_id 0 27 miss% 0.042215698148756725
plot_id,batch_id 0 28 miss% 0.03808741439064704
plot_id,batch_id 0 29 miss% 0.04016276766941615
plot_id,batch_id 0 30 miss% 0.04716750704193449
plot_id,batch_id 0 31 miss% 0.08688096666089197
plot_id,batch_id 0 32 miss% 0.08896166279635305
plot_id,batch_id 0 33 miss% 0.04837316736875145
plot_id,batch_id 0 34 miss% 0.032121442054080906
plot_id,batch_id 0 35 miss% 0.03661367469115946
plot_id,batch_id 0 36 miss% 0.08090016133518527
plot_id,batch_id 0 37 miss% 0.07522678494282616
plot_id,batch_id 0 38 miss% 0.040660557719601524
plot_id,batch_id 0 39 miss% 0.03175336689345504
plot_id,batch_id 0 40 miss% 0.0670162141372644
plot_id,batch_id 0 41 miss% 0.03692055059592362
plot_id,batch_id 0 42 miss% 0.019840297397205026
plot_id,batch_id 0 43 miss% 0.03178703367000132
plot_id,batch_id 0 44 miss% 0.02179415109735673
plot_id,batch_id 0 45 miss% 0.07294935427728429
plot_id,batch_id 0 46 miss% 0.035004846568754584
plot_id,batch_id 0 47 miss% 0.030678049401704145
plot_id,batch_id 0 48 miss% 0.018036768192352455
plot_id,batch_id 0 49 miss% 0.026470641155276666
plot_id,batch_id 0 50 miss% 0.13126560660759876
plot_id,batch_id 0 51 miss% 0.03177341415158204
plot_id,batch_id 0 52 miss% 0.03628787308809639
plot_id,batch_id 0 53 miss% 0.021302730675587748
plot_id,batch_id 0 54 miss% 0.020476984217795015
plot_id,batch_id 0 55 miss% 0.10257060567410947
plot_id,batch_id 0 56 miss% 0.07103624762522441
plot_id,batch_id 0 57 miss% 0.03789487688108959
plot_id,batch_id 0 58 miss% 0.026990297014980357
plot_id,batch_id 0 59 miss% 0.03171825072646559
plot_id,batch_id 0 60 miss% 0.03345616457482471
plot_id,batch_id 0 61 miss% 0.02652088957255999
plot_id,batch_id 0 62 miss% 0.0735346932680205
plot_id,batch_id 0 63 miss% 0.03260825645581137
plot_id,batch_id 0 64 miss% 0.04444090921157406
plot_id,batch_id 0
plot_id,batch_id 0 70 miss% 0.8542771539508465
plot_id,batch_id 0 71 miss% 0.8942327977215991
plot_id,batch_id 0 72 miss% 0.9092614343184784
plot_id,batch_id 0 73 miss% 0.5849380099425332
plot_id,batch_id 0 74 miss% 0.5940028179012014
plot_id,batch_id 0 75 miss% 0.6979728237813002
plot_id,batch_id 0 76 miss% 0.7264224644932498
plot_id,batch_id 0 77 miss% 0.8840664690481858
plot_id,batch_id 0 78 miss% 0.8869643171938756
plot_id,batch_id 0 79 miss% 0.5149515222215777
plot_id,batch_id 0 80 miss% 0.49553036174036763
plot_id,batch_id 0 81 miss% 0.7096470185058259
plot_id,batch_id 0 82 miss% 0.6303697234604607
plot_id,batch_id 0 83 miss% 0.8231949481809488
plot_id,batch_id 0 84 miss% 0.8616533140677478
plot_id,batch_id 0 85 miss% 0.8221355580786889
plot_id,batch_id 0 86 miss% 0.788979566791434
plot_id,batch_id 0 87 miss% 0.8027107097489034
plot_id,batch_id 0 88 miss% 0.8136815599173554
plot_id,batch_id 0 89 miss% 0.563687465351045
plot_id,batch_id 0 90 miss% 0.567937090847314
plot_id,batch_id 0 91 miss% 0.6095726353896238
plot_id,batch_id 0 92 miss% 0.5033256013913164
plot_id,batch_id 0 93 miss% 0.5661098129083159
plot_id,batch_id 0 94 miss% 0.6878197796906037
plot_id,batch_id 0 95 miss% 0.6904425851712681
plot_id,batch_id 0 96 miss% 0.6493690319349325
plot_id,batch_id 0 97 miss% 0.8416849067274524
plot_id,batch_id 0 98 miss% 0.7164490064459533
plot_id,batch_id 0 99 miss% 0.6234338368528172
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.64207731 0.75411139 0.76058606 0.78971528 0.63667413 0.74183527
 0.78709576 0.68613644 0.64886897 0.74416236 0.66570649 0.70452241
 0.68834101 0.65671417 0.59859984 0.66217925 0.75633148 0.86740309
 0.59979728 0.52718957 0.77106929 0.75538403 0.65396413 0.86808704
 0.69720597 0.7748979  0.79969061 0.67838004 0.75467428 0.75275549
 0.5606378  0.68093328 0.72708002 0.71927755 0.65330643 0.81956707
 0.84993807 0.54218706 0.73403646 0.90494337 0.65554066 0.710932
 0.76260555 0.67476239 0.84024233 0.82664546 0.75956002 0.71843208
 0.79867078 0.80673535 0.49645576 0.71808033 0.74570462 0.82676442
 0.83771835 0.57247715 0.70633719 0.84399968 0.80850108 0.73677421
 0.75153683 0.51690378 0.64783868 0.61039342 0.57258554 0.89323855
 0.72442689 0.58332013 0.61273654 0.80801685 0.85427715 0.8942328
 0.90926143 0.58493801 0.59400282 0.69797282 0.72642246 0.88406647
 0.88696432 0.51495152 0.49553036 0.70964702 0.63036972 0.82319495
 0.86165331 0.82213556 0.78897957 0.80271071 0.81368156 0.56368747
 0.56793709 0.60957264 0.5033256  0.56610981 0.68781978 0.69044259
 0.64936903 0.84168491 0.71644901 0.62343384]
for model  160 the mean error 0.714988243933216
all id 160 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 25 out win 5 err 0.714988243933216 time 19124.095629692078
Launcher: Job 161 completed in 19340 seconds.
Launcher: Task 181 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  265233
Epoch:0, Train loss:0.489850, valid loss:0.463285
Epoch:1, Train loss:0.158307, valid loss:0.002694
Epoch:2, Train loss:0.005618, valid loss:0.002127
Epoch:3, Train loss:0.004211, valid loss:0.001932
Epoch:4, Train loss:0.003552, valid loss:0.001582
Epoch:5, Train loss:0.003018, valid loss:0.001459
Epoch:6, Train loss:0.002718, valid loss:0.001308
Epoch:7, Train loss:0.002417, valid loss:0.001284
Epoch:8, Train loss:0.002262, valid loss:0.001062
Epoch:9, Train loss:0.002116, valid loss:0.001115
Epoch:10, Train loss:0.001929, valid loss:0.001238
Epoch:11, Train loss:0.001429, valid loss:0.000793
Epoch:12, Train loss:0.001337, valid loss:0.000827
Epoch:13, Train loss:0.001283, valid loss:0.000777
Epoch:14, Train loss:0.001244, valid loss:0.000828
Epoch:15, Train loss:0.001256, valid loss:0.000834
Epoch:16, Train loss:0.001177, valid loss:0.000743
Epoch:17, Train loss:0.001194, valid loss:0.000811
Epoch:18, Train loss:0.001096, valid loss:0.000874
Epoch:19, Train loss:0.001057, valid loss:0.000735
Epoch:20, Train loss:0.001081, valid loss:0.000780
Epoch:21, Train loss:0.000792, valid loss:0.000629
Epoch:22, Train loss:0.000777, valid loss:0.000651
Epoch:23, Train loss:0.000763, valid loss:0.000656
Epoch:24, Train loss:0.000741, valid loss:0.000655
Epoch:25, Train loss:0.000750, valid loss:0.000624
Epoch:26, Train loss:0.000744, valid loss:0.000713
Epoch:27, Train loss:0.000717, valid loss:0.000628
Epoch:28, Train loss:0.000700, valid loss:0.000631
Epoch:29, Train loss:0.000683, valid loss:0.000620
Epoch:30, Train loss:0.000693, valid loss:0.000645
Epoch:31, Train loss:0.000556, valid loss:0.000587
Epoch:32, Train loss:0.000543, valid loss:0.000616
Epoch:33, Train loss:0.000537, valid loss:0.000612
Epoch:34, Train loss:0.000531, valid loss:0.000608
Epoch:35, Train loss:0.000528, valid loss:0.000606
Epoch:36, Train loss:0.000530, valid loss:0.000579
Epoch:37, Train loss:0.000533, valid loss:0.000600
Epoch:38, Train loss:0.000504, valid loss:0.000576
Epoch:39, Train loss:0.000522, valid loss:0.000580
Epoch:40, Train loss:0.000502, valid loss:0.000582
Epoch:41, Train loss:0.000440, valid loss:0.000547
Epoch:42, Train loss:0.000436, valid loss:0.000560
Epoch:43, Train loss:0.000433, valid loss:0.000553
Epoch:44, Train loss:0.000435, valid loss:0.000547
Epoch:45, Train loss:0.000432, valid loss:0.000548
Epoch:46, Train loss:0.000430, valid loss:0.000564
Epoch:47, Train loss:0.000425, valid loss:0.000559
Epoch:48, Train loss:0.000422, valid loss:0.000578
Epoch:49, Train loss:0.000417, valid loss:0.000544
Epoch:50, Train loss:0.000420, valid loss:0.000581
Epoch:51, Train loss:0.000400, valid loss:0.000540
Epoch:52, Train loss:0.000392, valid loss:0.000540
Epoch:53, Train loss:0.000388, valid loss:0.000541
Epoch:54, Train loss:0.000387, valid loss:0.000537
Epoch:55, Train loss:0.000386, valid loss:0.000533
Epoch:56, Train loss:0.000385, valid loss:0.000535
Epoch:57, Train loss:0.000384, valid loss:0.000539
Epoch:58, Train loss:0.000383, valid loss:0.000539
Epoch:59, Train loss:0.000383, valid loss:0.000536
Epoch:60, Train loss:0.000382, valid loss:0.000535
training time 19172.042642831802
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.20833333333333331
nondim time 0.41666666666666663
nondim time 0.625
nondim time 0.8333333333333333
plot_id,batch_id 0 0 miss% 0.3394331560348774
plot_id,batch_id 0 1 miss% 0.39398660626471327
plot_id,batch_id 0 2 miss% 0.48372401146521127
plot_id,batch_id 0 3 miss% 0.3458106533879184
plot_id,batch_id 0 4 miss% 0.3310362547721456
plot_id,batch_id 0 5 miss% 0.3451794836093116
plot_id,batch_id 0 6 miss% 0.37225232134351377
plot_id,batch_id 0 7 miss% 0.510372772682577
plot_id,batch_id 0 8 miss% 0.6390296178308115
plot_id,batch_id 0 9 miss% 0.6010002916192063
plot_id,batch_id 0 10 miss% 0.31030477322860106
plot_id,batch_id 0 11 miss% 0.39966543242608804
plot_id,batch_id 0 12 miss% 0.3937102073323315
plot_id,batch_id 0 13 miss% 0.34295047822744656
plot_id,batch_id 0 14 miss% 0.502113066793338
plot_id,batch_id 0 15 miss% 0.40895802686979316
plot_id,batch_id 0 16 miss% 0.4989038783151571
plot_id,batch_id 0 17 miss% 0.47921876627830107
plot_id,batch_id 0 18 miss% 0.4786929142168625
plot_id,batch_id 0 19 miss% 0.4472117143286329
plot_id,batch_id 0 20 miss% 0.363017429639502
plot_id,batch_id 0 21 miss% 0.4322767899948607
plot_id,batch_id 0 22 miss% 0.4766567182816401
plot_id,batch_id 0 23 miss% 0.4749982172032091
plot_id,batch_id 0 24 miss% 0.42895766011964986
plot_id,batch_id 0 25 miss% 0.4094510869062018
plot_id,batch_id 0 26 miss% 0.4658352545749618
plot_id,batch_id 0 27 miss% 0.4032204625920933
plot_id,batch_id 0 28 miss% 0.4893994791052043
plot_id,batch_id 0 29 miss% 0.4506512214222631
plot_id,batch_id 0 30 miss% 0.3420132518896611
plot_id,batch_id 0 31 miss% 0.5261061792591214
plot_id,batch_id 0 32 miss% 0.461505900196509
plot_id,batch_id 0 33 miss% 0.448973548721858
plot_id,batch_id 0 34 miss% 0.4309011704621308
plot_id,batch_id 0 35 miss% 0.35465196644137564
plot_id,batch_id 0 36 miss% 0.5393756303487525
plot_id,batch_id 0 37 miss% 0.38931401976507674
plot_id,batch_id 0 38 miss% 0.48909303417939015
plot_id,batch_id 0 39 miss% 0.48715636026846065
plot_id,batch_id 0 40 miss% 0.32621915642940663
plot_id,batch_id 0 41 miss% 0.42741123351177673
plot_id,batch_id 0 42 miss% 0.3474273350143474
plot_id,batch_id 0 43 miss% 0.32442066333212627
plot_id,batch_id 0 44 miss% 0.3725353980797324
plot_id,batch_id 0 45 miss% 0.40772877262496215
plot_id,batch_id 0 46 miss% 0.39553120929722535
plot_id,batch_id 0 47 miss% 0.45703227605113717
plot_id,batch_id 0 48 miss% 0.43625832087565203
plot_id,batch_id 0 49 miss% 0.31105984221597566
plot_id,batch_id 0 50 miss% 0.5339039189361547
plot_id,batch_id 0 51 miss% 0.48538215665283835
plot_id,batch_id 0 52 miss% 0.5079673444609065
plot_id,batch_id 0 53 miss% 0.34240054330776304
plot_id,batch_id 0 54 miss% 0.39261142639084595
plot_id,batch_id 0 55 miss% 0.39350541335844635
plot_id,batch_id 0 56 miss% 0.5075409648195724
plot_id,batch_id 0 57 miss% 0.4846202342545411
plot_id,batch_id 0 58 miss% 0.41668325899981146
plot_id,batch_id 0 59 miss% 0.49454074517012336
plot_id,batch_id 0 60 miss% 0.27595459652856674
plot_id,batch_id 0 61 miss% 0.2754903523418961
plot_id,batch_id 0 62 miss% 0.42158436371906227
plot_id,batch_id 0 63 miss% 0.3827222384198641
plot_id,batch_id 0 64 miss% 0.40465997360866823
plot_id,batch_id 0 65 miss% 0.319864520029227
plot_id,batch_id 0 66 miss% 0.4074043224304409
plot_id,batch_id 0 67 miss% 0.3095683274137407
plot_id,batch_id 0 68 miss% 0.43469649719478504
 65 miss% 0.048016010960446255
plot_id,batch_id 0 66 miss% 0.012565526856071178
plot_id,batch_id 0 67 miss% 0.02729429641235689
plot_id,batch_id 0 68 miss% 0.04299419334827646
plot_id,batch_id 0 69 miss% 0.1067856590120387
plot_id,batch_id 0 70 miss% 0.02380822797986299
plot_id,batch_id 0 71 miss% 0.03780437477879967
plot_id,batch_id 0 72 miss% 0.08806319001391688
plot_id,batch_id 0 73 miss% 0.04435592778683454
plot_id,batch_id 0 74 miss% 0.1211617037266721
plot_id,batch_id 0 75 miss% 0.056590185097357276
plot_id,batch_id 0 76 miss% 0.04374696967857095
plot_id,batch_id 0 77 miss% 0.038090328585709356
plot_id,batch_id 0 78 miss% 0.02889084683929904
plot_id,batch_id 0 79 miss% 0.05007797172141363
plot_id,batch_id 0 80 miss% 0.037114576608630344
plot_id,batch_id 0 81 miss% 0.08750879795537325
plot_id,batch_id 0 82 miss% 0.07238929023097723
plot_id,batch_id 0 83 miss% 0.09945524664783052
plot_id,batch_id 0 84 miss% 0.07198802809062926
plot_id,batch_id 0 85 miss% 0.05269524369857631
plot_id,batch_id 0 86 miss% 0.04276210150964019
plot_id,batch_id 0 87 miss% 0.05380387366493691
plot_id,batch_id 0 88 miss% 0.08696588871796426
plot_id,batch_id 0 89 miss% 0.07259745777320478
plot_id,batch_id 0 90 miss% 0.039897311560001696
plot_id,batch_id 0 91 miss% 0.02247843931610137
plot_id,batch_id 0 92 miss% 0.08686913192486616
plot_id,batch_id 0 93 miss% 0.03963678765731753
plot_id,batch_id 0 94 miss% 0.0490796519248414
plot_id,batch_id 0 95 miss% 0.051620720318536775
plot_id,batch_id 0 96 miss% 0.050752709955778366
plot_id,batch_id 0 97 miss% 0.03715661221505008
plot_id,batch_id 0 98 miss% 0.056134085370978234
plot_id,batch_id 0 99 miss% 0.08622296943970327
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06376794 0.04161687 0.09614214 0.03583782 0.04023318 0.02970642
 0.08187515 0.1032957  0.09391742 0.04491894 0.02606087 0.0515916
 0.04361552 0.04908636 0.06668879 0.04623834 0.0500653  0.0436249
 0.06384846 0.07756883 0.09301836 0.09463194 0.04015892 0.03108446
 0.02485567 0.0729174  0.06463979 0.0422157  0.03808741 0.04016277
 0.04716751 0.08688097 0.08896166 0.04837317 0.03212144 0.03661367
 0.08090016 0.07522678 0.04066056 0.03175337 0.06701621 0.03692055
 0.0198403  0.03178703 0.02179415 0.07294935 0.03500485 0.03067805
 0.01803677 0.02647064 0.13126561 0.03177341 0.03628787 0.02130273
 0.02047698 0.10257061 0.07103625 0.03789488 0.0269903  0.03171825
 0.03345616 0.02652089 0.07353469 0.03260826 0.04444091 0.04801601
 0.01256553 0.0272943  0.04299419 0.10678566 0.02380823 0.03780437
 0.08806319 0.04435593 0.1211617  0.05659019 0.04374697 0.03809033
 0.02889085 0.05007797 0.03711458 0.0875088  0.07238929 0.09945525
 0.07198803 0.05269524 0.0427621  0.05380387 0.08696589 0.07259746
 0.03989731 0.02247844 0.08686913 0.03963679 0.04907965 0.05162072
 0.05075271 0.03715661 0.05613409 0.08622297]
for model  210 the mean error 0.053098822873269856
all id 210 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 31 out win 4 err 0.053098822873269856 time 19111.20187807083
Launcher: Job 211 completed in 19371 seconds.
Launcher: Task 115 done. Exiting.
plot_id,batch_id 0 69 miss% 0.49190480399130154
plot_id,batch_id 0 70 miss% 0.2632338097125947
plot_id,batch_id 0 71 miss% 0.36352346063894553
plot_id,batch_id 0 72 miss% 0.40940342111256606
plot_id,batch_id 0 73 miss% 0.3576226710264016
plot_id,batch_id 0 74 miss% 0.43842526880269306
plot_id,batch_id 0 75 miss% 0.3004353676497734
plot_id,batch_id 0 76 miss% 0.27819315520723203
plot_id,batch_id 0 77 miss% 0.3256239176066259
plot_id,batch_id 0 78 miss% 0.3117642167153495
plot_id,batch_id 0 79 miss% 0.42436478980152603
plot_id,batch_id 0 80 miss% 0.28882793927744643
plot_id,batch_id 0 81 miss% 0.4772564457044042
plot_id,batch_id 0 82 miss% 0.36703914612573
plot_id,batch_id 0 83 miss% 0.41888997222294
plot_id,batch_id 0 84 miss% 0.3952759609192171
plot_id,batch_id 0 85 miss% 0.28914165096566086
plot_id,batch_id 0 86 miss% 0.38169938784056406
plot_id,batch_id 0 87 miss% 0.38449521776998224
plot_id,batch_id 0 88 miss% 0.39373232860231244
plot_id,batch_id 0 89 miss% 0.35681654303579013
plot_id,batch_id 0 90 miss% 0.25688946378182675
plot_id,batch_id 0 91 miss% 0.39511252414600556
plot_id,batch_id 0 92 miss% 0.36878458277890097
plot_id,batch_id 0 93 miss% 0.3105547263998307
plot_id,batch_id 0 94 miss% 0.4975702656036261
plot_id,batch_id 0 95 miss% 0.2844226011442045
plot_id,batch_id 0 96 miss% 0.3207555997711428
plot_id,batch_id 0 97 miss% 0.41153033350586593
plot_id,batch_id 0 98 miss% 0.4473556570238936
plot_id,batch_id 0 99 miss% 0.3538054593313798
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.33943316 0.39398661 0.48372401 0.34581065 0.33103625 0.34517948
 0.37225232 0.51037277 0.63902962 0.60100029 0.31030477 0.39966543
 0.39371021 0.34295048 0.50211307 0.40895803 0.49890388 0.47921877
 0.47869291 0.44721171 0.36301743 0.43227679 0.47665672 0.47499822
 0.42895766 0.40945109 0.46583525 0.40322046 0.48939948 0.45065122
 0.34201325 0.52610618 0.4615059  0.44897355 0.43090117 0.35465197
 0.53937563 0.38931402 0.48909303 0.48715636 0.32621916 0.42741123
 0.34742734 0.32442066 0.3725354  0.40772877 0.39553121 0.45703228
 0.43625832 0.31105984 0.53390392 0.48538216 0.50796734 0.34240054
 0.39261143 0.39350541 0.50754096 0.48462023 0.41668326 0.49454075
 0.2759546  0.27549035 0.42158436 0.38272224 0.40465997 0.31986452
 0.40740432 0.30956833 0.4346965  0.4919048  0.26323381 0.36352346
 0.40940342 0.35762267 0.43842527 0.30043537 0.27819316 0.32562392
 0.31176422 0.42436479 0.28882794 0.47725645 0.36703915 0.41888997
 0.39527596 0.28914165 0.38169939 0.38449522 0.39373233 0.35681654
 0.25688946 0.39511252 0.36878458 0.31055473 0.49757027 0.2844226
 0.3207556  0.41153033 0.44735566 0.35380546]
for model  106 the mean error 0.40448285902084086
all id 106 hidden_dim 32 learning_rate 0.0025 num_layers 5 frames 25 out win 5 err 0.40448285902084086 time 19172.042642831802
Launcher: Job 107 completed in 19393 seconds.
Launcher: Task 205 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  120401
Epoch:0, Train loss:0.429770, valid loss:0.417277
Epoch:1, Train loss:0.059088, valid loss:0.002962
Epoch:2, Train loss:0.007876, valid loss:0.002439
Epoch:3, Train loss:0.007141, valid loss:0.002376
Epoch:4, Train loss:0.005602, valid loss:0.001443
Epoch:5, Train loss:0.003574, valid loss:0.001231
Epoch:6, Train loss:0.003012, valid loss:0.000956
Epoch:7, Train loss:0.001821, valid loss:0.001017
Epoch:8, Train loss:0.001680, valid loss:0.000842
Epoch:9, Train loss:0.001645, valid loss:0.000955
Epoch:10, Train loss:0.001480, valid loss:0.000780
Epoch:11, Train loss:0.001119, valid loss:0.000697
Epoch:12, Train loss:0.001065, valid loss:0.000631
Epoch:13, Train loss:0.001042, valid loss:0.000734
Epoch:14, Train loss:0.001012, valid loss:0.000579
Epoch:15, Train loss:0.000988, valid loss:0.000652
Epoch:16, Train loss:0.000990, valid loss:0.000647
Epoch:17, Train loss:0.000916, valid loss:0.000643
Epoch:18, Train loss:0.000915, valid loss:0.000623
Epoch:19, Train loss:0.000897, valid loss:0.000529
Epoch:20, Train loss:0.000872, valid loss:0.000677
Epoch:21, Train loss:0.000676, valid loss:0.000525
Epoch:22, Train loss:0.000661, valid loss:0.000593
Epoch:23, Train loss:0.000653, valid loss:0.000521
Epoch:24, Train loss:0.000643, valid loss:0.000494
Epoch:25, Train loss:0.000633, valid loss:0.000509
Epoch:26, Train loss:0.000622, valid loss:0.000514
Epoch:27, Train loss:0.000624, valid loss:0.000546
Epoch:28, Train loss:0.000594, valid loss:0.000496
Epoch:29, Train loss:0.000586, valid loss:0.000510
Epoch:30, Train loss:0.000600, valid loss:0.000512
Epoch:31, Train loss:0.000494, valid loss:0.000467
Epoch:32, Train loss:0.000475, valid loss:0.000467
Epoch:33, Train loss:0.000478, valid loss:0.000447
Epoch:34, Train loss:0.000473, valid loss:0.000462
Epoch:35, Train loss:0.000474, valid loss:0.000448
Epoch:36, Train loss:0.000465, valid loss:0.000451
Epoch:37, Train loss:0.000464, valid loss:0.000462
Epoch:38, Train loss:0.000453, valid loss:0.000459
Epoch:39, Train loss:0.000442, valid loss:0.000445
Epoch:40, Train loss:0.000452, valid loss:0.000553
Epoch:41, Train loss:0.000404, valid loss:0.000462
Epoch:42, Train loss:0.000397, valid loss:0.000456
Epoch:43, Train loss:0.000396, valid loss:0.000457
Epoch:44, Train loss:0.000394, valid loss:0.000448
Epoch:45, Train loss:0.000394, valid loss:0.000471
Epoch:46, Train loss:0.000391, valid loss:0.000458
Epoch:47, Train loss:0.000385, valid loss:0.000452
Epoch:48, Train loss:0.000383, valid loss:0.000433
Epoch:49, Train loss:0.000384, valid loss:0.000459
Epoch:50, Train loss:0.000388, valid loss:0.000447
Epoch:51, Train loss:0.000360, valid loss:0.000434
Epoch:52, Train loss:0.000357, valid loss:0.000437
Epoch:53, Train loss:0.000356, valid loss:0.000433
Epoch:54, Train loss:0.000355, valid loss:0.000430
Epoch:55, Train loss:0.000354, valid loss:0.000436
Epoch:56, Train loss:0.000353, valid loss:0.000433
Epoch:57, Train loss:0.000353, valid loss:0.000435
Epoch:58, Train loss:0.000353, valid loss:0.000431
Epoch:59, Train loss:0.000352, valid loss:0.000440
Epoch:60, Train loss:0.000352, valid loss:0.000440
training time 19237.809027910233
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.08203605200997344
plot_id,batch_id 0 1 miss% 0.04090826716961628
plot_id,batch_id 0 2 miss% 0.1240649792687956
plot_id,batch_id 0 3 miss% 0.07217629353174376
plot_id,batch_id 0 4 miss% 0.09202885955130659
plot_id,batch_id 0 5 miss% 0.0784736932860815
plot_id,batch_id 0 6 miss% 0.03383949285659017
plot_id,batch_id 0 7 miss% 0.07804429915011984
plot_id,batch_id 0 8 miss% 0.06182303217906322
plot_id,batch_id 0 9 miss% 0.09719213625821514
plot_id,batch_id 0 10 miss% 0.04582698974692999
plot_id,batch_id 0 11 miss% 0.07084321379983212
plot_id,batch_id 0 12 miss% 0.0594389384401797
plot_id,batch_id 0 13 miss% 0.08337848504271288
plot_id,batch_id 0 14 miss% 0.08952363837500579
plot_id,batch_id 0 15 miss% 0.06606533443062666
plot_id,batch_id 0 16 miss% 0.09904791382997806
plot_id,batch_id 0 17 miss% 0.08453480489516892
plot_id,batch_id 0 18 miss% 0.07513026865534632
plot_id,batch_id 0 19 miss% 0.09176944407576201
plot_id,batch_id 0 20 miss% 0.05320746678336903
plot_id,batch_id 0 21 miss% 0.05254839475306418
plot_id,batch_id 0 22 miss% 0.06450555033675939
plot_id,batch_id 0 23 miss% 0.054941665942994024
plot_id,batch_id 0 24 miss% 0.04971123917422558
plot_id,batch_id 0 25 miss% 0.061402512980677226
plot_id,batch_id 0 26 miss% 0.04279412788436907
plot_id,batch_id 0 27 miss% 0.07360197031300876
plot_id,batch_id 0 28 miss% 0.04744060364211398
plot_id,batch_id 0 29 miss% 0.06838740824080983
plot_id,batch_id 0 30 miss% 0.050428680540146174
plot_id,batch_id 0 31 miss% 0.08647991185960896
plot_id,batch_id 0 32 miss% 0.10907445476867475
plot_id,batch_id 0 33 miss% 0.05758114109826623
plot_id,batch_id 0 34 miss% 0.040679205421962704
plot_id,batch_id 0 35 miss% 0.05262578907479206
plot_id,batch_id 0 36 miss% 0.11724499110464677
plot_id,batch_id 0 37 miss% 0.07306651329398837
plot_id,batch_id 0 38 miss% 0.06824842753955136
plot_id,batch_id 0 39 miss% 0.0615787274153772
plot_id,batch_id 0 40 miss% 0.06566879037213215
plot_id,batch_id 0 41 miss% 0.0354789688633123
plot_id,batch_id 0 42 miss% 0.041455171118441485
plot_id,batch_id 0 43 miss% 0.04196430076930654
plot_id,batch_id 0 44 miss% 0.0677334083582414
plot_id,batch_id 0 45 miss% 0.061737247783679595
plot_id,batch_id 0 46 miss% 0.022803652004805604
plot_id,batch_id 0 47 miss% 0.040265532503054
plot_id,batch_id 0 48 miss% 0.04940019635117249
plot_id,batch_id 0 49 miss% 0.037439453440051555
plot_id,batch_id 0 50 miss% 0.10809517869137582
plot_id,batch_id 0 51 miss% 0.05704816969526577
plot_id,batch_id 0 52 miss% 0.04909786599673277
plot_id,batch_id 0 53 miss% 0.026802328400766077
plot_id,batch_id 0 54 miss% 0.028926282872166142
plot_id,batch_id 0 55 miss% 0.06992826055711678
plot_id,batch_id 0 56 miss% 0.09897801750656512
plot_id,batch_id 0 57 miss% 0.03280874608972837
plot_id,batch_id 0 58 miss% 0.06470109827350805
plot_id,batch_id 0 59 miss% 0.07046994234457109
plot_id,batch_id 0 60 miss% 0.038512120592495695
plot_id,batch_id 0 61 miss% 0.027050965780009383
plot_id,batch_id 0 62 miss% 0.05600076078541503
plot_id,batch_id 0 63 miss% 0.04964594621846915
plot_id,batch_id 0 64 miss% 0.048852921483212026
plot_id,batch_id 0 65 miss% 0.16008946896089304
plot_id,batch_id 0 66 miss% 0.04815748269360896
plot_id,batch_id 0 67 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  151697
Epoch:0, Train loss:0.399691, valid loss:0.359399
Epoch:1, Train loss:0.232438, valid loss:0.232905
Epoch:2, Train loss:0.226734, valid loss:0.232572
Epoch:3, Train loss:0.226242, valid loss:0.232511
Epoch:4, Train loss:0.225984, valid loss:0.232441
Epoch:5, Train loss:0.225831, valid loss:0.232371
Epoch:6, Train loss:0.225684, valid loss:0.232289
Epoch:7, Train loss:0.225537, valid loss:0.232328
Epoch:8, Train loss:0.225510, valid loss:0.232138
Epoch:9, Train loss:0.225401, valid loss:0.232425
Epoch:10, Train loss:0.225370, valid loss:0.232087
Epoch:11, Train loss:0.225022, valid loss:0.231992
Epoch:12, Train loss:0.224993, valid loss:0.231946
Epoch:13, Train loss:0.224957, valid loss:0.232098
Epoch:14, Train loss:0.224943, valid loss:0.231993
Epoch:15, Train loss:0.224910, valid loss:0.231911
Epoch:16, Train loss:0.224904, valid loss:0.231968
Epoch:17, Train loss:0.224890, valid loss:0.231899
Epoch:18, Train loss:0.224867, valid loss:0.231940
Epoch:19, Train loss:0.224860, valid loss:0.231935
Epoch:20, Train loss:0.224850, valid loss:0.231993
Epoch:21, Train loss:0.224666, valid loss:0.231859
Epoch:22, Train loss:0.224658, valid loss:0.231859
Epoch:23, Train loss:0.224656, valid loss:0.231848
Epoch:24, Train loss:0.224641, valid loss:0.231853
Epoch:25, Train loss:0.224634, valid loss:0.231873
Epoch:26, Train loss:0.224635, valid loss:0.231903
Epoch:27, Train loss:0.224626, valid loss:0.232019
Epoch:28, Train loss:0.224607, valid loss:0.231857
Epoch:29, Train loss:0.224612, valid loss:0.231849
Epoch:30, Train loss:0.224591, valid loss:0.231882
Epoch:31, Train loss:0.224512, valid loss:0.231818
Epoch:32, Train loss:0.224507, valid loss:0.231819
Epoch:33, Train loss:0.224505, valid loss:0.231844
Epoch:34, Train loss:0.224504, valid loss:0.231829
Epoch:35, Train loss:0.224496, valid loss:0.231832
Epoch:36, Train loss:0.224498, valid loss:0.231817
Epoch:37, Train loss:0.224495, valid loss:0.231825
Epoch:38, Train loss:0.224491, valid loss:0.231798
Epoch:39, Train loss:0.224485, valid loss:0.231857
Epoch:40, Train loss:0.224474, valid loss:0.231833
Epoch:41, Train loss:0.224442, valid loss:0.231792
Epoch:42, Train loss:0.224438, valid loss:0.231801
Epoch:43, Train loss:0.224438, valid loss:0.231820
Epoch:44, Train loss:0.224435, valid loss:0.231810
Epoch:45, Train loss:0.224430, valid loss:0.231791
Epoch:46, Train loss:0.224432, valid loss:0.231780
Epoch:47, Train loss:0.224427, valid loss:0.231790
Epoch:48, Train loss:0.224425, valid loss:0.231827
Epoch:49, Train loss:0.224425, valid loss:0.231792
Epoch:50, Train loss:0.224422, valid loss:0.231799
Epoch:51, Train loss:0.224405, valid loss:0.231786
Epoch:52, Train loss:0.224402, valid loss:0.231784
Epoch:53, Train loss:0.224400, valid loss:0.231784
Epoch:54, Train loss:0.224400, valid loss:0.231784
Epoch:55, Train loss:0.224400, valid loss:0.231785
Epoch:56, Train loss:0.224399, valid loss:0.231785
Epoch:57, Train loss:0.224399, valid loss:0.231785
Epoch:58, Train loss:0.224399, valid loss:0.231784
Epoch:59, Train loss:0.224399, valid loss:0.231784
Epoch:60, Train loss:0.224399, valid loss:0.231783
training time 19355.454556703568
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.9140084635762474
plot_id,batch_id 0 1 miss% 0.8869263960997178
plot_id,batch_id 0 2 miss% 0.8843728315743544
plot_id,batch_id 0 3 miss% 0.8788602273964182
plot_id,batch_id 0 4 miss% 0.883434636063235
plot_id,batch_id 0 5 miss% 0.9169002422202425
plot_id,batch_id 0 6 miss% 0.8871720336864642
plot_id,batch_id 0 7 miss% 0.8825696063470305
plot_id,batch_id 0 8 miss% 0.8791370371503636
plot_id,batch_id 0 9 miss% 0.8790590897936424
plot_id,batch_id 0 10 miss% 0.910368084903206
plot_id,batch_id 0 11 miss% 0.8865282641116727
plot_id,batch_id 0 12 miss% 0.8813237319277848
plot_id,batch_id 0 13 miss% 0.8784879519086476
plot_id,batch_id 0 14 miss% 0.8787005832829908
plot_id,batch_id 0 15 miss% 0.9197646938982125
plot_id,batch_id 0 16 miss% 0.8875855666878182
plot_id,batch_id 0 17 miss% 0.8809916583210149
plot_id,batch_id 0 18 miss% 0.8816217457095846
plot_id,batch_id 0 19 miss% 0.8813787070645157
plot_id,batch_id 0 20 miss% 0.9070784677106292
plot_id,batch_id 0 21 miss% 0.8877362220076218
plot_id,batch_id 0 22 miss% 0.8801377843526117
plot_id,batch_id 0 23 miss% 0.8930762261547623
plot_id,batch_id 0 24 miss% 0.8746994384168257
plot_id,batch_id 0 25 miss% 0.9007415556888991
plot_id,batch_id 0 26 miss% 0.8823918628902752
plot_id,batch_id 0 27 miss% 0.879412092458221
plot_id,batch_id 0 28 miss% 0.8813533925158098
plot_id,batch_id 0 29 miss% 0.8783823353287247
plot_id,batch_id 0 30 miss% 0.8998606443466663
plot_id,batch_id 0 31 miss% 0.8822428532042175
plot_id,batch_id 0 32 miss% 0.8827235208420324
plot_id,batch_id 0 33 miss% 0.8801384148421381
plot_id,batch_id 0 34 miss% 0.8765357115141373
plot_id,batch_id 0 35 miss% 0.9012714992361829
plot_id,batch_id 0 36 miss% 0.8785961541635425
plot_id,batch_id 0 37 miss% 0.8789843458452243
plot_id,batch_id 0 38 miss% 0.8803063657751913
plot_id,batch_id 0 39 miss% 0.8799742198175533
plot_id,batch_id 0 40 miss% 0.8902033210424822
plot_id,batch_id 0 41 miss% 0.8770334284685357
plot_id,batch_id 0 42 miss% 0.8751849627984002
plot_id,batch_id 0 43 miss% 0.8774544584761065
plot_id,batch_id 0 44 miss% 0.8745882328472037
plot_id,batch_id 0 45 miss% 0.8882172176160086
plot_id,batch_id 0 46 miss% 0.8824652999079666
plot_id,batch_id 0 47 miss% 0.877871673837172
plot_id,batch_id 0 48 miss% 0.8774033771837332
plot_id,batch_id 0 49 miss% 0.8770993991384402
plot_id,batch_id 0 50 miss% 0.8867239720162317
plot_id,batch_id 0 51 miss% 0.8806065940215129
plot_id,batch_id 0 52 miss% 0.881478952593826
plot_id,batch_id 0 53 miss% 0.8932381714694005
plot_id,batch_id 0 54 miss% 0.8748543450451353
plot_id,batch_id 0 55 miss% 0.891430294696089
plot_id,batch_id 0 56 miss% 0.8785764507258108
plot_id,batch_id 0 57 miss% 0.8831798056047196
plot_id,batch_id 0 58 miss% 0.8755000657562921
plot_id,batch_id 0 59 miss% 0.8758201275293275
plot_id,batch_id 0 60 miss% 0.9298794185725544
plot_id,batch_id 0 61 miss% 0.902402885314639
plot_id,batch_id 0 62 miss% 0.894108812705516
plot_id,batch_id 0 63 miss% 0.8841112862107943
plot_id,batch_id 0 64 miss% 0.884029155998733
plot_id,batch_id 0 65 miss% 0.9273009278276887
plot_id,batch_id 0 66 miss% 0.9035812269850133
plot_id,batch_id 0 67 miss% 0.8952672027595625
plot_id,batch_id 0 68 miss% 0.8868385515390661
plot_id,batch_id 0 69 miss% 0.8863679785584837
plot_id,batch_id 0 70 miss% 0.9288332591905555
plot_id,batch_id 0 71 miss% 0.9096358011031233
plot_id,batch_id 0 72 miss% 0.8922373653470638
plot_id,batch_id 0 73 miss% 0.8936462271111844
plot_id,batch_id 0 74 miss% 0.887203908078597
plot_id,batch_id 0 75 miss% 0.9287702344897465
plot_id,batch_id 0 76 miss% 0.9081794991643217
plot_id,batch_id 0 77 miss% 0.8892641206622367
plot_id,batch_id 0 78 miss% 0.885175062841094
plot_id,batch_id 0 79 miss% 0.8851531949130342
plot_id,batch_id 0 80 miss% 0.9247667716133459
plot_id,batch_id 0 81 miss% 0.8956923709170191
plot_id,batch_id 0 82 miss% 0.8898725012772067
plot_id,batch_id 0 83 miss% 0.8820513926100648
plot_id,batch_id 0 84 miss% 0.8793325528836484
plot_id,batch_id 0 85 miss% 0.9254439229641758
plot_id,batch_id 0 86 miss% 0.8971270358958408
plot_id,batch_id 0 87 miss% 0.8874114673602659
plot_id,batch_id 0 88 miss% 0.8832094976302892
plot_id,batch_id 0 89 miss% 0.8816113891339489
plot_id,batch_id 0 90 miss% 0.931447153351417
plot_id,batch_id 0 91 miss% 0.8985030793286538
plot_id,batch_id 0 92 miss% 0.8864667237290207
plot_id,batch_id 0 93 miss% 0.8853025404360726
plot_id,batch_id 0 94 miss% 0.8829294871241821
plot_id,batch_id 0 95 miss% 0.9252713643858828
plot_id,batch_id 0 96 miss% 0.8997146852190845
plot_id,batch_id 0 97 miss% 0.8884283001764353
plot_id,batch_id 0 98 miss% 0.8849611277906884
plot_id,batch_id 0 99 miss% 0.8829552073021552
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.91400846 0.8869264  0.88437283 0.87886023 0.88343464 0.91690024
 0.88717203 0.88256961 0.87913704 0.87905909 0.91036808 0.88652826
 0.88132373 0.87848795 0.87870058 0.91976469 0.88758557 0.88099166
 0.88162175 0.88137871 0.90707847 0.88773622 0.88013778 0.89307623
 0.87469944 0.90074156 0.88239186 0.87941209 0.88135339 0.87838234
 0.89986064 0.88224285 0.88272352 0.88013841 0.87653571 0.9012715
 0.87859615 0.87898435 0.88030637 0.87997422 0.89020332 0.87703343
 0.87518496 0.87745446 0.87458823 0.88821722 0.8824653  0.87787167
 0.87740338 0.8770994  0.88672397 0.88060659 0.88147895 0.89323817
 0.87485435 0.89143029 0.87857645 0.88317981 0.87550007 0.87582013
 0.92987942 0.90240289 0.89410881 0.88411129 0.88402916 0.92730093
 0.90358123 0.8952672  0.88683855 0.88636798 0.92883326 0.9096358
 0.89223737 0.89364623 0.88720391 0.92877023 0.9081795  0.88926412
 0.88517506 0.88515319 0.92476677 0.89569237 0.8898725  0.88205139
 0.87933255 0.92544392 0.89712704 0.88741147 0.8832095  0.88161139
 0.93144715 0.89850308 0.88646672 0.88530254 0.88292949 0.92527136
 0.89971469 0.8884283  0.88496113 0.88295521]
for model  237 the mean error 0.8901824950011122
all id 237 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 31 out win 4 err 0.8901824950011122 time 19355.454556703568
Launcher: Job 238 completed in 19473 seconds.
Launcher: Task 54 done. Exiting.
0.023228452385381248
plot_id,batch_id 0 68 miss% 0.05519247994194484
plot_id,batch_id 0 69 miss% 0.07749625540939634
plot_id,batch_id 0 70 miss% 0.07796789069433822
plot_id,batch_id 0 71 miss% 0.0397364827181891
plot_id,batch_id 0 72 miss% 0.08220342806852982
plot_id,batch_id 0 73 miss% 0.03996361288782014
plot_id,batch_id 0 74 miss% 0.05889615659706497
plot_id,batch_id 0 75 miss% 0.11754123659094608
plot_id,batch_id 0 76 miss% 0.061258739000399826
plot_id,batch_id 0 77 miss% 0.03098712013450025
plot_id,batch_id 0 78 miss% 0.030957116176306237
plot_id,batch_id 0 79 miss% 0.04817483617468476
plot_id,batch_id 0 80 miss% 0.09772700495084354
plot_id,batch_id 0 81 miss% 0.08929279759079688
plot_id,batch_id 0 82 miss% 0.04948552456646365
plot_id,batch_id 0 83 miss% 0.0836209561132292
plot_id,batch_id 0 84 miss% 0.052521536396531565
plot_id,batch_id 0 85 miss% 0.060055232369538095
plot_id,batch_id 0 86 miss% 0.061520310000048636
plot_id,batch_id 0 87 miss% 0.08420089540657734
plot_id,batch_id 0 88 miss% 0.05768128029733171
plot_id,batch_id 0 89 miss% 0.07990962368981784
plot_id,batch_id 0 90 miss% 0.04077852903035692
plot_id,batch_id 0 91 miss% 0.059922990598029545
plot_id,batch_id 0 92 miss% 0.0370942084312111
plot_id,batch_id 0 93 miss% 0.04220912671772724
plot_id,batch_id 0 94 miss% 0.07286080166987544
plot_id,batch_id 0 95 miss% 0.045441442096836086
plot_id,batch_id 0 96 miss% 0.05838371209541466
plot_id,batch_id 0 97 miss% 0.020004804104401606
plot_id,batch_id 0 98 miss% 0.04585580500061294
plot_id,batch_id 0 99 miss% 0.08488041853557753
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08203605 0.04090827 0.12406498 0.07217629 0.09202886 0.07847369
 0.03383949 0.0780443  0.06182303 0.09719214 0.04582699 0.07084321
 0.05943894 0.08337849 0.08952364 0.06606533 0.09904791 0.0845348
 0.07513027 0.09176944 0.05320747 0.05254839 0.06450555 0.05494167
 0.04971124 0.06140251 0.04279413 0.07360197 0.0474406  0.06838741
 0.05042868 0.08647991 0.10907445 0.05758114 0.04067921 0.05262579
 0.11724499 0.07306651 0.06824843 0.06157873 0.06566879 0.03547897
 0.04145517 0.0419643  0.06773341 0.06173725 0.02280365 0.04026553
 0.0494002  0.03743945 0.10809518 0.05704817 0.04909787 0.02680233
 0.02892628 0.06992826 0.09897802 0.03280875 0.0647011  0.07046994
 0.03851212 0.02705097 0.05600076 0.04964595 0.04885292 0.16008947
 0.04815748 0.02322845 0.05519248 0.07749626 0.07796789 0.03973648
 0.08220343 0.03996361 0.05889616 0.11754124 0.06125874 0.03098712
 0.03095712 0.04817484 0.097727   0.0892928  0.04948552 0.08362096
 0.05252154 0.06005523 0.06152031 0.0842009  0.05768128 0.07990962
 0.04077853 0.05992299 0.03709421 0.04220913 0.0728608  0.04544144
 0.05838371 0.0200048  0.04585581 0.08488042]
for model  203 the mean error 0.06277858003668274
all id 203 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 31 out win 6 err 0.06277858003668274 time 19237.809027910233
Launcher: Job 204 completed in 19493 seconds.
Launcher: Task 49 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  209681
Epoch:0, Train loss:0.451184, valid loss:0.450976
Epoch:1, Train loss:0.032435, valid loss:0.005263
Epoch:2, Train loss:0.012189, valid loss:0.003633
Epoch:3, Train loss:0.008554, valid loss:0.003039
Epoch:4, Train loss:0.006430, valid loss:0.002302
Epoch:5, Train loss:0.005149, valid loss:0.002291
Epoch:6, Train loss:0.003878, valid loss:0.001424
Epoch:7, Train loss:0.002591, valid loss:0.001297
Epoch:8, Train loss:0.002400, valid loss:0.001320
Epoch:9, Train loss:0.002127, valid loss:0.001104
Epoch:10, Train loss:0.002011, valid loss:0.001097
Epoch:11, Train loss:0.001442, valid loss:0.000927
Epoch:12, Train loss:0.001410, valid loss:0.000832
Epoch:13, Train loss:0.001303, valid loss:0.000961
Epoch:14, Train loss:0.001288, valid loss:0.000786
Epoch:15, Train loss:0.001268, valid loss:0.000815
Epoch:16, Train loss:0.001261, valid loss:0.000823
Epoch:17, Train loss:0.001167, valid loss:0.000823
Epoch:18, Train loss:0.001138, valid loss:0.000790
Epoch:19, Train loss:0.001153, valid loss:0.000810
Epoch:20, Train loss:0.001093, valid loss:0.000883
Epoch:21, Train loss:0.000810, valid loss:0.000715
Epoch:22, Train loss:0.000766, valid loss:0.000704
Epoch:23, Train loss:0.000752, valid loss:0.000722
Epoch:24, Train loss:0.000749, valid loss:0.000648
Epoch:25, Train loss:0.000717, valid loss:0.000673
Epoch:26, Train loss:0.000717, valid loss:0.000717
Epoch:27, Train loss:0.000702, valid loss:0.000715
Epoch:28, Train loss:0.000692, valid loss:0.000651
Epoch:29, Train loss:0.000661, valid loss:0.000676
Epoch:30, Train loss:0.000651, valid loss:0.000667
Epoch:31, Train loss:0.000527, valid loss:0.000656
Epoch:32, Train loss:0.000513, valid loss:0.000660
Epoch:33, Train loss:0.000501, valid loss:0.000650
Epoch:34, Train loss:0.000510, valid loss:0.000616
Epoch:35, Train loss:0.000492, valid loss:0.000626
Epoch:36, Train loss:0.000490, valid loss:0.000604
Epoch:37, Train loss:0.000486, valid loss:0.000601
Epoch:38, Train loss:0.000489, valid loss:0.000636
Epoch:39, Train loss:0.000478, valid loss:0.000621
Epoch:40, Train loss:0.000466, valid loss:0.000635
Epoch:41, Train loss:0.000402, valid loss:0.000638
Epoch:42, Train loss:0.000394, valid loss:0.000588
Epoch:43, Train loss:0.000401, valid loss:0.000590
Epoch:44, Train loss:0.000392, valid loss:0.000608
Epoch:45, Train loss:0.000392, valid loss:0.000611
Epoch:46, Train loss:0.000386, valid loss:0.000604
Epoch:47, Train loss:0.000381, valid loss:0.000606
Epoch:48, Train loss:0.000386, valid loss:0.000602
Epoch:49, Train loss:0.000384, valid loss:0.000592
Epoch:50, Train loss:0.000383, valid loss:0.000596
Epoch:51, Train loss:0.000357, valid loss:0.000579
Epoch:52, Train loss:0.000353, valid loss:0.000577
Epoch:53, Train loss:0.000351, valid loss:0.000578
Epoch:54, Train loss:0.000350, valid loss:0.000577
Epoch:55, Train loss:0.000349, valid loss:0.000578
Epoch:56, Train loss:0.000348, valid loss:0.000579
Epoch:57, Train loss:0.000347, valid loss:0.000579
Epoch:58, Train loss:0.000347, valid loss:0.000580
Epoch:59, Train loss:0.000346, valid loss:0.000581
Epoch:60, Train loss:0.000346, valid loss:0.000578
training time 19359.862690925598
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07465093318801082
plot_id,batch_id 0 1 miss% 0.05011059735385457
plot_id,batch_id 0 2 miss% 0.09110900438551736
plot_id,batch_id 0 3 miss% 0.0530660530824909
plot_id,batch_id 0 4 miss% 0.033856105656199446
plot_id,batch_id 0 5 miss% 0.03947499550764763
plot_id,batch_id 0 6 miss% 0.040923345538567836
plot_id,batch_id 0 7 miss% 0.05277954720677359
plot_id,batch_id 0 8 miss% 0.07499136853880568
plot_id,batch_id 0 9 miss% 0.03773771791283419
plot_id,batch_id 0 10 miss% 0.03503773298716039
plot_id,batch_id 0 11 miss% 0.04005409066215565
plot_id,batch_id 0 12 miss% 0.05773322876295877
plot_id,batch_id 0 13 miss% 0.05914675376342811
plot_id,batch_id 0 14 miss% 0.05792110659679433
plot_id,batch_id 0 15 miss% 0.039575458318342804
plot_id,batch_id 0 16 miss% 0.10231678989539676
plot_id,batch_id 0 17 miss% 0.06516689129542509
plot_id,batch_id 0 18 miss% 0.04083166728147398
plot_id,batch_id 0 19 miss% 0.05546004232094346
plot_id,batch_id 0 20 miss% 0.028362976314647886
plot_id,batch_id 0 21 miss% 0.03336144523478145
plot_id,batch_id 0 22 miss% 0.028015975212266155
plot_id,batch_id 0 23 miss% 0.035633272661704594
plot_id,batch_id 0 24 miss% 0.035455990768389845
plot_id,batch_id 0 25 miss% 0.05356314058331497
plot_id,batch_id 0 26 miss% 0.03738225830959269
plot_id,batch_id 0 27 miss% 0.03315037856688111
plot_id,batch_id 0 28 miss% 0.025889307038686003
plot_id,batch_id 0 29 miss% 0.03344227597339744
plot_id,batch_id 0 30 miss% 0.03169555818985545
plot_id,batch_id 0 31 miss% 0.08412863794724895
plot_id,batch_id 0 32 miss% 0.08598746646219481
plot_id,batch_id 0 33 miss% 0.05929255142210782
plot_id,batch_id 0 34 miss% 0.04801361578987021
plot_id,batch_id 0 35 miss% 0.02478421014713468
plot_id,batch_id 0 36 miss% 0.07734775193059205
plot_id,batch_id 0 37 miss% 0.05173311754382149
plot_id,batch_id 0 38 miss% 0.05379011503343374
plot_id,batch_id 0 39 miss% 0.053035068121732275
plot_id,batch_id 0 40 miss% 0.060717985095939155
plot_id,batch_id 0 41 miss% 0.04110852900760013
plot_id,batch_id 0 42 miss% 0.02086781334102581
plot_id,batch_id 0 43 miss% 0.03624155291861272
plot_id,batch_id 0 44 miss% 0.02176928170830781
plot_id,batch_id 0 45 miss% 0.07676729079499331
plot_id,batch_id 0 46 miss% 0.033514986250554435
plot_id,batch_id 0 47 miss% 0.020160593911435213
plot_id,batch_id 0 48 miss% 0.02582320972362431
plot_id,batch_id 0 49 miss% 0.029898035162730027
plot_id,batch_id 0 50 miss% 0.11951557026430493
plot_id,batch_id 0 51 miss% 0.03329929168760214
plot_id,batch_id 0 52 miss% 0.04465085183813052
plot_id,batch_id 0 53 miss% 0.028422893457176104
plot_id,batch_id 0 54 miss% 0.023291925990719987
plot_id,batch_id 0 55 miss% 0.03477013748904349
plot_id,batch_id 0 56 miss% 0.04953045591649565
plot_id,batch_id 0 57 miss% 0.03698549452494661
plot_id,batch_id 0 58 miss% 0.042992532491504254
plot_id,batch_id 0 59 miss% 0.020910376908423435
plot_id,batch_id 0 60 miss% 0.029176113757338185
plot_id,batch_id 0 61 miss% 0.036692880471911785
plot_id,batch_id 0 62 miss% 0.0661218915913833
plot_id,batch_id 0 63 miss% 0.044529523885092834
plot_id,batch_id 0 64 miss% 0.030401323506917502
plot_id,batch_id 0 65 miss% 0.06671736892850849
plot_id,batch_id 0 66 miss% 0.04746648677530079
plot_id,batch_id 0 67 miss% 0.032528688193340365
plot_id,batch_id 0 68 miss% 0.04048944232292049
plot_id,batch_id 0 69 miss% 0.090713331834562
plot_id,batch_id 0 70 miss% 0.098023750723409
plot_id,batch_id 0 71 miss% 0.03358581516605937
plot_id,batch_id 0 72 miss% 0.10200239152137645
plot_id,batch_id 0 73 miss% 0.06975926452946862
plot_id,batch_id 0 74 miss% 0.10155024436398426
plot_id,batch_id 0 75 miss% 0.11379367554921632
plot_id,batch_id 0 76 miss% 0.07329379800376853
plot_id,batch_id 0 77 miss% 0.05281786816525701
plot_id,batch_id 0 78 miss% 0.031524638302330786
plot_id,batch_id 0 79 miss% 0.0540810286170054
plot_id,batch_id 0 80 miss% 0.028318374577568896
plot_id,batch_id 0 81 miss% 0.06545299158768951
plot_id,batch_id 0 82 miss% 0.07526719520189587
plot_id,batch_id 0 83 miss% 0.04820964831841694
plot_id,batch_id 0 84 miss% 0.07227036450399789
plot_id,batch_id 0 85 miss% 0.05592775269744915
plot_id,batch_id 0 86 miss% 0.048783101175766584
plot_id,batch_id 0 87 miss% 0.08666279225356695
plot_id,batch_id 0 88 miss% 0.06340241140754392
plot_id,batch_id 0 89 miss% 0.05195199354560947
plot_id,batch_id 0 90 miss% 0.02780626406831034
plot_id,batch_id 0 91 miss% 0.04408750786790648
plot_id,batch_id 0 92 miss% 0.0392260592633426
plot_id,batch_id 0 93 miss% 0.040600722605537994
plot_id,batch_id 0 94 miss% 0.07673989115039365
plot_id,batch_id 0 95 miss% 0.039042137673450446
plot_id,batch_id 0 96 miss% 0.03177268379970773
plot_id,batch_id 0 97 miss% 0.02134943036024247
plot_id,batch_id 0 98 miss% 0.043384560584829965
plot_id,batch_id 0 99 miss% 0.06345914033279187
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07465093 0.0501106  0.091109   0.05306605 0.03385611 0.039475
 0.04092335 0.05277955 0.07499137 0.03773772 0.03503773 0.04005409
 0.05773323 0.05914675 0.05792111 0.03957546 0.10231679 0.06516689
 0.04083167 0.05546004 0.02836298 0.03336145 0.02801598 0.03563327
 0.03545599 0.05356314 0.03738226 0.03315038 0.02588931 0.03344228
 0.03169556 0.08412864 0.08598747 0.05929255 0.04801362 0.02478421
 0.07734775 0.05173312 0.05379012 0.05303507 0.06071799 0.04110853
 0.02086781 0.03624155 0.02176928 0.07676729 0.03351499 0.02016059
 0.02582321 0.02989804 0.11951557 0.03329929 0.04465085 0.02842289
 0.02329193 0.03477014 0.04953046 0.03698549 0.04299253 0.02091038
 0.02917611 0.03669288 0.06612189 0.04452952 0.03040132 0.06671737
 0.04746649 0.03252869 0.04048944 0.09071333 0.09802375 0.03358582
 0.10200239 0.06975926 0.10155024 0.11379368 0.0732938  0.05281787
 0.03152464 0.05408103 0.02831837 0.06545299 0.0752672  0.04820965
 0.07227036 0.05592775 0.0487831  0.08666279 0.06340241 0.05195199
 0.02780626 0.04408751 0.03922606 0.04060072 0.07673989 0.03904214
 0.03177268 0.02134943 0.04338456 0.06345914]
for model  125 the mean error 0.05056231901174774
all id 125 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 25 out win 6 err 0.05056231901174774 time 19359.862690925598
Launcher: Job 126 completed in 19609 seconds.
Launcher: Task 95 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  265233
Epoch:0, Train loss:0.479342, valid loss:0.452080
Epoch:1, Train loss:0.369816, valid loss:0.365785
Epoch:2, Train loss:0.362495, valid loss:0.365388
Epoch:3, Train loss:0.361594, valid loss:0.365057
Epoch:4, Train loss:0.361171, valid loss:0.364886
Epoch:5, Train loss:0.360878, valid loss:0.364853
Epoch:6, Train loss:0.360595, valid loss:0.364689
Epoch:7, Train loss:0.360336, valid loss:0.364832
Epoch:8, Train loss:0.360196, valid loss:0.364573
Epoch:9, Train loss:0.360148, valid loss:0.364571
Epoch:10, Train loss:0.360014, valid loss:0.364607
Epoch:11, Train loss:0.359505, valid loss:0.364391
Epoch:12, Train loss:0.359446, valid loss:0.364447
Epoch:13, Train loss:0.359399, valid loss:0.364429
Epoch:14, Train loss:0.359378, valid loss:0.364431
Epoch:15, Train loss:0.359345, valid loss:0.364371
Epoch:16, Train loss:0.359319, valid loss:0.364320
Epoch:17, Train loss:0.359285, valid loss:0.364391
Epoch:18, Train loss:0.359235, valid loss:0.364344
Epoch:19, Train loss:0.359266, valid loss:0.364460
Epoch:20, Train loss:0.359208, valid loss:0.364352
Epoch:21, Train loss:0.358973, valid loss:0.364252
Epoch:22, Train loss:0.358927, valid loss:0.364290
Epoch:23, Train loss:0.358914, valid loss:0.364286
Epoch:24, Train loss:0.358932, valid loss:0.364282
Epoch:25, Train loss:0.358906, valid loss:0.364300
Epoch:26, Train loss:0.358892, valid loss:0.364266
Epoch:27, Train loss:0.358891, valid loss:0.364232
Epoch:28, Train loss:0.358927, valid loss:0.364251
Epoch:29, Train loss:0.358874, valid loss:0.364210
Epoch:30, Train loss:0.358846, valid loss:0.364236
Epoch:31, Train loss:0.358741, valid loss:0.364207
Epoch:32, Train loss:0.358728, valid loss:0.364181
Epoch:33, Train loss:0.358720, valid loss:0.364212
Epoch:34, Train loss:0.358733, valid loss:0.364201
Epoch:35, Train loss:0.358709, valid loss:0.364222
Epoch:36, Train loss:0.358703, valid loss:0.364166
Epoch:37, Train loss:0.358694, valid loss:0.364173
Epoch:38, Train loss:0.358698, valid loss:0.364171
Epoch:39, Train loss:0.358699, valid loss:0.364163
Epoch:40, Train loss:0.358699, valid loss:0.364191
Epoch:41, Train loss:0.358634, valid loss:0.364162
Epoch:42, Train loss:0.358623, valid loss:0.364154
Epoch:43, Train loss:0.358621, valid loss:0.364185
Epoch:44, Train loss:0.358624, valid loss:0.364158
Epoch:45, Train loss:0.358621, valid loss:0.364158
Epoch:46, Train loss:0.358622, valid loss:0.364144
Epoch:47, Train loss:0.358625, valid loss:0.364160
Epoch:48, Train loss:0.358614, valid loss:0.364180
Epoch:49, Train loss:0.358618, valid loss:0.364159
Epoch:50, Train loss:0.358606, valid loss:0.364166
Epoch:51, Train loss:0.358591, valid loss:0.364156
Epoch:52, Train loss:0.358587, valid loss:0.364152
Epoch:53, Train loss:0.358586, valid loss:0.364153
Epoch:54, Train loss:0.358585, valid loss:0.364156
Epoch:55, Train loss:0.358584, valid loss:0.364150
Epoch:56, Train loss:0.358584, valid loss:0.364150
Epoch:57, Train loss:0.358584, valid loss:0.364150
Epoch:58, Train loss:0.358583, valid loss:0.364148
Epoch:59, Train loss:0.358583, valid loss:0.364148
Epoch:60, Train loss:0.358582, valid loss:0.364152
training time 19569.236453056335
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.9617027203754966
plot_id,batch_id 0 1 miss% 0.965247775862549
plot_id,batch_id 0 2 miss% 0.9644183695050759
plot_id,batch_id 0 3 miss% 0.9673492158320388
plot_id,batch_id 0 4 miss% 0.9664790488986924
plot_id,batch_id 0 5 miss% 0.9658057188753879
plot_id,batch_id 0 6 miss% 0.964682721766363
plot_id,batch_id 0 7 miss% 0.9674103154566484
plot_id,batch_id 0 8 miss% 0.968391425057252
plot_id,batch_id 0 9 miss% 0.9694374731892629
plot_id,batch_id 0 10 miss% 0.9634874882062139
plot_id,batch_id 0 11 miss% 0.9643145382441493
plot_id,batch_id 0 12 miss% 0.9670066907610648
plot_id,batch_id 0 13 miss% 0.9645622082879084
plot_id,batch_id 0 14 miss% 0.9682370021464123
plot_id,batch_id 0 15 miss% 0.9434023108328867
plot_id,batch_id 0 16 miss% 0.9629922416303652
plot_id,batch_id 0 17 miss% 0.9667209422397492
plot_id,batch_id 0 18 miss% 0.9707791999301271
plot_id,batch_id 0 19 miss% 0.9722820698863328
plot_id,batch_id 0 20 miss% 0.9666036984845771
plot_id,batch_id 0 21 miss% 0.9669102108957555
plot_id,batch_id 0 22 miss% 0.9709859548290092
plot_id,batch_id 0 23 miss% 0.9674722792072373
plot_id,batch_id 0 24 miss% 0.9666236367725797
plot_id,batch_id 0 25 miss% 0.9632071970489875
plot_id,batch_id 0 26 miss% 0.9682957697351606
plot_id,batch_id 0 27 miss% 0.9684247583096235
plot_id,batch_id 0 28 miss% 0.9691136379990007
plot_id,batch_id 0 29 miss% 0.9698194961941495
plot_id,batch_id 0 30 miss% 0.9576298435528793
plot_id,batch_id 0 31 miss% 0.972225711227783
plot_id,batch_id 0 32 miss% 0.9701628875539063
plot_id,batch_id 0 33 miss% 0.9701959027616704
plot_id,batch_id 0 34 miss% 0.9680720788383685
plot_id,batch_id 0 35 miss% 0.9858780221888702
plot_id,batch_id 0 36 miss% 0.9672199988659238
plot_id,batch_id 0 37 miss% 0.9713786591017753
plot_id,batch_id 0 38 miss% 0.972522674353351
plot_id,batch_id 0 39 miss% 0.9687139931286581
plot_id,batch_id 0 40 miss% 0.9670174963309026
plot_id,batch_id 0 41 miss% 0.9678239339986777
plot_id,batch_id 0 42 miss% 0.9673643839258979
plot_id,batch_id 0 43 miss% 0.9659535241987608
plot_id,batch_id 0 44 miss% 0.9677646897152326
plot_id,batch_id 0 45 miss% 0.9655214212880581
plot_id,batch_id 0 46 miss% 0.9683838293404193
plot_id,batch_id 0 47 miss% 0.9691097649471171
plot_id,batch_id 0 48 miss% 0.9696842954803441
plot_id,batch_id 0 49 miss% 0.9725412997687464
plot_id,batch_id 0 50 miss% 0.9696827392592461
plot_id,batch_id 0 51 miss% 0.9735253683274501
plot_id,batch_id 0 52 miss% 0.973036284892199
plot_id,batch_id 0 53 miss% 0.9684661378274593
plot_id,batch_id 0 54 miss% 0.9682784179743505
plot_id,batch_id 0 55 miss% 0.9793458301548843
plot_id,batch_id 0 56 miss% 0.9713933821126307
plot_id,batch_id 0 57 miss% 0.9721875623821352
plot_id,batch_id 0 58 miss% 0.9734244541678312
plot_id,batch_id 0 59 miss% 0.9693740092746083
plot_id,batch_id 0 60 miss% 0.9645544503990464
plot_id,batch_id 0 61 miss% 0.9614132273143229
plot_id,batch_id 0 62 miss% 0.9664267449906454
plot_id,batch_id 0 63 miss% 0.963541086097611
plot_id,batch_id 0 64 miss% 0.9652128367059823
plot_id,batch_id 0 65 miss% 0.9660751826158093
plot_id,batch_id 0 66 miss% 0.9638557172878195
plot_id,batch_id 0 67 miss% 0.9642345875889239
plot_id,batch_id 0 68 miss% 0.9643786156113634
plot_id,batch_id 0 69 miss% 0.9655182077007202
plot_id,batch_id 0 70 miss% 0.9484618110979393
plot_id,batch_id 0 71 miss% 0.96955449318321
plot_id,batch_id 0 72 miss% 0.962223731820189
plot_id,batch_id 0 73 miss% 0.9669737050036359
plot_id,batch_id 0 74 miss% 0.9663380076588761
plot_id,batch_id 0 75 miss% 0.9554576072272248
plot_id,batch_id 0 76 miss% 0.9714419659608888
plot_id,batch_id 0 77 miss% 0.9608669458797601
plot_id,batch_id 0 78 miss% 0.9597602862917215
plot_id,batch_id 0 79 miss% 0.9636377830461141
plot_id,batch_id 0 80 miss% 0.9605567572884964
plot_id,batch_id 0 81 miss% 0.9651212510351399
plot_id,batch_id 0 82 miss% 0.9652592333713392
plot_id,batch_id 0 83 miss% 0.9665569373615516
plot_id,batch_id 0 84 miss% 0.9661649986759712
plot_id,batch_id 0 85 miss% 0.956133817520364
plot_id,batch_id 0 86 miss% 0.965253565631509
plot_id,batch_id 0 87 miss% 0.963050789959501
plot_id,batch_id 0 88 miss% 0.9650758520195162
plot_id,batch_id 0 89 miss% 0.9632104139362868
plot_id,batch_id 0 90 miss% 0.9724882224259765
plot_id,batch_id 0 91 miss% 0.9672339436174061
plot_id,batch_id 0 92 miss% 0.9653498201795806
plot_id,batch_id 0 93 miss% 0.9667922312593558
plot_id,batch_id 0 94 miss% 0.9732342909730942
plot_id,batch_id 0 95 miss% 0.9653208378003023
plot_id,batch_id 0 96 miss% 0.9648843395417563
plot_id,batch_id 0 97 miss% 0.964291042440272
plot_id,batch_id 0 98 miss% 0.9698048100925488
plot_id,batch_id 0 99 miss% 0.9690043979522717
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.96170272 0.96524778 0.96441837 0.96734922 0.96647905 0.96580572
 0.96468272 0.96741032 0.96839143 0.96943747 0.96348749 0.96431454
 0.96700669 0.96456221 0.968237   0.94340231 0.96299224 0.96672094
 0.9707792  0.97228207 0.9666037  0.96691021 0.97098595 0.96747228
 0.96662364 0.9632072  0.96829577 0.96842476 0.96911364 0.9698195
 0.95762984 0.97222571 0.97016289 0.9701959  0.96807208 0.98587802
 0.96722    0.97137866 0.97252267 0.96871399 0.9670175  0.96782393
 0.96736438 0.96595352 0.96776469 0.96552142 0.96838383 0.96910976
 0.9696843  0.9725413  0.96968274 0.97352537 0.97303628 0.96846614
 0.96827842 0.97934583 0.97139338 0.97218756 0.97342445 0.96937401
 0.96455445 0.96141323 0.96642674 0.96354109 0.96521284 0.96607518
 0.96385572 0.96423459 0.96437862 0.96551821 0.94846181 0.96955449
 0.96222373 0.96697371 0.96633801 0.95545761 0.97144197 0.96086695
 0.95976029 0.96363778 0.96055676 0.96512125 0.96525923 0.96655694
 0.966165   0.95613382 0.96525357 0.96305079 0.96507585 0.96321041
 0.97248822 0.96723394 0.96534982 0.96679223 0.97323429 0.96532084
 0.96488434 0.96429104 0.96980481 0.9690044 ]
for model  134 the mean error 0.9667075925996425
all id 134 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 25 out win 6 err 0.9667075925996425 time 19569.236453056335
Launcher: Job 135 completed in 19679 seconds.
Launcher: Task 163 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  265233
Epoch:0, Train loss:0.479342, valid loss:0.452080
Epoch:1, Train loss:0.175998, valid loss:0.004547
Epoch:2, Train loss:0.011806, valid loss:0.003706
Epoch:3, Train loss:0.005347, valid loss:0.001844
Epoch:4, Train loss:0.003841, valid loss:0.001724
Epoch:5, Train loss:0.003233, valid loss:0.001790
Epoch:6, Train loss:0.002790, valid loss:0.001306
Epoch:7, Train loss:0.002581, valid loss:0.001324
Epoch:8, Train loss:0.002331, valid loss:0.001376
Epoch:9, Train loss:0.002246, valid loss:0.001314
Epoch:10, Train loss:0.002028, valid loss:0.001414
Epoch:11, Train loss:0.001507, valid loss:0.000826
Epoch:12, Train loss:0.001450, valid loss:0.000862
Epoch:13, Train loss:0.001366, valid loss:0.000835
Epoch:14, Train loss:0.001342, valid loss:0.000876
Epoch:15, Train loss:0.001303, valid loss:0.000754
Epoch:16, Train loss:0.001258, valid loss:0.000847
Epoch:17, Train loss:0.001235, valid loss:0.000954
Epoch:18, Train loss:0.001178, valid loss:0.000751
Epoch:19, Train loss:0.001132, valid loss:0.000816
Epoch:20, Train loss:0.001151, valid loss:0.000718
Epoch:21, Train loss:0.000847, valid loss:0.000638
Epoch:22, Train loss:0.000826, valid loss:0.000684
Epoch:23, Train loss:0.000806, valid loss:0.000668
Epoch:24, Train loss:0.000794, valid loss:0.000688
Epoch:25, Train loss:0.000789, valid loss:0.000661
Epoch:26, Train loss:0.000772, valid loss:0.000685
Epoch:27, Train loss:0.000730, valid loss:0.000694
Epoch:28, Train loss:0.000746, valid loss:0.000635
Epoch:29, Train loss:0.000717, valid loss:0.000595
Epoch:30, Train loss:0.000711, valid loss:0.000695
Epoch:31, Train loss:0.000584, valid loss:0.000578
Epoch:32, Train loss:0.000566, valid loss:0.000547
Epoch:33, Train loss:0.000572, valid loss:0.000572
Epoch:34, Train loss:0.000548, valid loss:0.000673
Epoch:35, Train loss:0.000547, valid loss:0.000553
Epoch:36, Train loss:0.000545, valid loss:0.000575
Epoch:37, Train loss:0.000533, valid loss:0.000652
Epoch:38, Train loss:0.000533, valid loss:0.000583
Epoch:39, Train loss:0.000522, valid loss:0.000563
Epoch:40, Train loss:0.000533, valid loss:0.000540
Epoch:41, Train loss:0.000458, valid loss:0.000537
Epoch:42, Train loss:0.000447, valid loss:0.000536
Epoch:43, Train loss:0.000448, valid loss:0.000537
Epoch:44, Train loss:0.000447, valid loss:0.000542
Epoch:45, Train loss:0.000449, valid loss:0.000539
Epoch:46, Train loss:0.000438, valid loss:0.000527
Epoch:47, Train loss:0.000435, valid loss:0.000536
Epoch:48, Train loss:0.000429, valid loss:0.000561
Epoch:49, Train loss:0.000432, valid loss:0.000533
Epoch:50, Train loss:0.000416, valid loss:0.000548
Epoch:51, Train loss:0.000397, valid loss:0.000528
Epoch:52, Train loss:0.000393, valid loss:0.000521
Epoch:53, Train loss:0.000391, valid loss:0.000525
Epoch:54, Train loss:0.000389, valid loss:0.000603
Epoch:55, Train loss:0.000389, valid loss:0.000520
Epoch:56, Train loss:0.000388, valid loss:0.000520
Epoch:57, Train loss:0.000387, valid loss:0.000521
Epoch:58, Train loss:0.000387, valid loss:0.000521
Epoch:59, Train loss:0.000386, valid loss:0.000522
Epoch:60, Train loss:0.000386, valid loss:0.000521
training time 19759.35306286812
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.4111161783496489
plot_id,batch_id 0 1 miss% 0.43780274706487715
plot_id,batch_id 0 2 miss% 0.5213181456534455
plot_id,batch_id 0 3 miss% 0.39438567842671546
plot_id,batch_id 0 4 miss% 0.3399821354515114
plot_id,batch_id 0 5 miss% 0.36439179610697847
plot_id,batch_id 0 6 miss% 0.4456064579891636
plot_id,batch_id 0 7 miss% 0.5450982542997606
plot_id,batch_id 0 8 miss% 0.6723288047202127
plot_id,batch_id 0 9 miss% 0.5254580906962784
plot_id,batch_id 0 10 miss% 0.380399854935048
plot_id,batch_id 0 11 miss% 0.41132588918165724
plot_id,batch_id 0 12 miss% 0.4915875144252575
plot_id,batch_id 0 13 miss% 0.4044107015529209
plot_id,batch_id 0 14 miss% 0.5100558701952781
plot_id,batch_id 0 15 miss% 0.43503739207409775
plot_id,batch_id 0 16 miss% 0.5638040424465648
plot_id,batch_id 0 17 miss% 0.5041272917792683
plot_id,batch_id 0 18 miss% 0.5080150784812848
plot_id,batch_id 0 19 miss% 0.4597373628340787
plot_id,batch_id 0 20 miss% 0.44455085184081944
plot_id,batch_id 0 21 miss% 0.47819239828071103
plot_id,batch_id 0 22 miss% 0.48638334257366017
plot_id,batch_id 0 23 miss% 0.49591298822757846
plot_id,batch_id 0 24 miss% 0.46776314873582636
plot_id,batch_id 0 25 miss% 0.4995545210295605
plot_id,batch_id 0 26 miss% 0.467933497892301
plot_id,batch_id 0 27 miss% 0.4378921003422123
plot_id,batch_id 0 28 miss% 0.48022863393317106
plot_id,batch_id 0 29 miss% 0.4737240573583913
plot_id,batch_id 0 30 miss% 0.41833576458182653
plot_id,batch_id 0 31 miss% 0.57139090554653
plot_id,batch_id 0 32 miss% 0.53307748380852
plot_id,batch_id 0 33 miss% 0.5212840806594755
plot_id,batch_id 0 34 miss% 0.46369899427752176
plot_id,batch_id 0 35 miss% 0.43449655627986417
plot_id,batch_id 0 36 miss% 0.5662132791625736
plot_id,batch_id 0 37 miss% 0.5045448489760075
plot_id,batch_id 0 38 miss% 0.5267054056292494
plot_id,batch_id 0 39 miss% 0.4403612432141004
plot_id,batch_id 0 40 miss% 0.3851083358868977
plot_id,batch_id 0 41 miss% 0.5097856535026084
plot_id,batch_id 0 42 miss% 0.4209320102310454
plot_id,batch_id 0 43 miss% 0.37349807587929507
plot_id,batch_id 0 44 miss% 0.4304623345666037
plot_id,batch_id 0 45 miss% 0.44867578489005033
plot_id,batch_id 0 46 miss% 0.47025731995308295
plot_id,batch_id 0 47 miss% 0.5062445404187715
plot_id,batch_id 0 48 miss% 0.4848286911846773
plot_id,batch_id 0 49 miss% 0.42352134220000887
plot_id,batch_id 0 50 miss% 0.564494161345275
plot_id,batch_id 0 51 miss% 0.5051814211877961
plot_id,batch_id 0 52 miss% 0.5516298463973437
plot_id,batch_id 0 53 miss% 0.4407397057070555
plot_id,batch_id 0 54 miss% 0.3904324048152075
plot_id,batch_id 0 55 miss% 0.5024675311017112
plot_id,batch_id 0 56 miss% 0.6042487604814727
plot_id,batch_id 0 57 miss% 0.5806124985797844
plot_id,batch_id 0 58 miss% 0.5377475094501374
plot_id,batch_id 0 59 miss% 0.5576640030785158
plot_id,batch_id 0 60 miss% 0.33155178906407906
plot_id,batch_id 0 61 miss% 0.3670496249502344
plot_id,batch_id 0 62 miss% 0.4758862737537532
plot_id,batch_id 0 63 miss% 0.44451484119054907
plot_id,batch_id 0 64 miss% 0.4461258278066485
plot_id,batch_id 0 65 miss% 0.3710794862692423
plot_id,batch_id 0 66 miss% 0.4733477084164355
plot_id,batch_id 0 67 miss% 0.37490359957147873
plot_id,batch_id 0 68 miss% 0.5034485644547397
plot_id,batch_id 0 69 miss% 0.4903593624965684
plot_id,batch_id 0 70 miss% 0.2965116567323263
plot_id,batch_id 0 71 miss% 0.4610861598903061
plot_id,batch_id 0 72 miss% 0.46191581385733865
plot_id,batch_id 0 73 miss% 0.4019653542235764
plot_id,batch_id 0 74 miss% 0.4217151112726116
plot_id,batch_id 0 75 miss% 0.43706588112645944
plot_id,batch_id 0 76 miss% 0.4067269569247077
plot_id,batch_id 0 77 miss% 0.39518167114993613
plot_id,batch_id 0 78 miss% 0.39305687837276065
plot_id,batch_id 0 79 miss% 0.4365090360400716
plot_id,batch_id 0 80 miss% 0.3592378298570637
plot_id,batch_id 0 81 miss% 0.5019042269363372
plot_id,batch_id 0 82 miss% 0.44677941915271807
plot_id,batch_id 0 83 miss% 0.4802235739202691
plot_id,batch_id 0 84 miss% 0.42784676169671615
plot_id,batch_id 0 85 miss% 0.3757324277747727
plot_id,batch_id 0 86 miss% 0.4519005989897019
plot_id,batch_id 0 87 miss% 0.4365953474483086
plot_id,batch_id 0 88 miss% 0.47148459512893554
plot_id,batch_id 0 89 miss% 0.47904560960194476
plot_id,batch_id 0 90 miss% 0.291848433608736
plot_id,batch_id 0 91 miss% 0.35963390906316545
plot_id,batch_id 0 92 miss% 0.40738435704433584
plot_id,batch_id 0 93 miss% 0.3329085871240964
plot_id,batch_id 0 94 miss% 0.46836793043682345
plot_id,batch_id 0 95 miss% 0.3440832727978655
plot_id,batch_id 0 96 miss% 0.38098941060508945
plot_id,batch_id 0 97 miss% 0.5055995576528167
plot_id,batch_id 0 98 miss% 0.5172211488474722
plot_id,batch_id 0 99 miss% 0.4815328267139996
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.41111618 0.43780275 0.52131815 0.39438568 0.33998214 0.3643918
 0.44560646 0.54509825 0.6723288  0.52545809 0.38039985 0.41132589
 0.49158751 0.4044107  0.51005587 0.43503739 0.56380404 0.50412729
 0.50801508 0.45973736 0.44455085 0.4781924  0.48638334 0.49591299
 0.46776315 0.49955452 0.4679335  0.4378921  0.48022863 0.47372406
 0.41833576 0.57139091 0.53307748 0.52128408 0.46369899 0.43449656
 0.56621328 0.50454485 0.52670541 0.44036124 0.38510834 0.50978565
 0.42093201 0.37349808 0.43046233 0.44867578 0.47025732 0.50624454
 0.48482869 0.42352134 0.56449416 0.50518142 0.55162985 0.44073971
 0.3904324  0.50246753 0.60424876 0.5806125  0.53774751 0.557664
 0.33155179 0.36704962 0.47588627 0.44451484 0.44612583 0.37107949
 0.47334771 0.3749036  0.50344856 0.49035936 0.29651166 0.46108616
 0.46191581 0.40196535 0.42171511 0.43706588 0.40672696 0.39518167
 0.39305688 0.43650904 0.35923783 0.50190423 0.44677942 0.48022357
 0.42784676 0.37573243 0.4519006  0.43659535 0.4714846  0.47904561
 0.29184843 0.35963391 0.40738436 0.33290859 0.46836793 0.34408327
 0.38098941 0.50559956 0.51722115 0.48153283]
for model  107 the mean error 0.4563708674183628
all id 107 hidden_dim 32 learning_rate 0.0025 num_layers 5 frames 25 out win 6 err 0.4563708674183628 time 19759.35306286812
Launcher: Job 108 completed in 19973 seconds.
Launcher: Task 233 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  151697
Epoch:0, Train loss:0.362207, valid loss:0.326726
Epoch:1, Train loss:0.129298, valid loss:0.002066
Epoch:2, Train loss:0.004114, valid loss:0.001632
Epoch:3, Train loss:0.003200, valid loss:0.001305
Epoch:4, Train loss:0.002684, valid loss:0.001197
Epoch:5, Train loss:0.002404, valid loss:0.001089
Epoch:6, Train loss:0.002195, valid loss:0.001088
Epoch:7, Train loss:0.001993, valid loss:0.001076
Epoch:8, Train loss:0.001865, valid loss:0.000973
Epoch:9, Train loss:0.001721, valid loss:0.000821
Epoch:10, Train loss:0.001589, valid loss:0.000806
Epoch:11, Train loss:0.001238, valid loss:0.000741
Epoch:12, Train loss:0.001185, valid loss:0.000652
Epoch:13, Train loss:0.001135, valid loss:0.000653
Epoch:14, Train loss:0.001115, valid loss:0.000666
Epoch:15, Train loss:0.001044, valid loss:0.000620
Epoch:16, Train loss:0.001002, valid loss:0.000730
Epoch:17, Train loss:0.000977, valid loss:0.000640
Epoch:18, Train loss:0.000955, valid loss:0.000597
Epoch:19, Train loss:0.000918, valid loss:0.000647
Epoch:20, Train loss:0.000877, valid loss:0.000596
Epoch:21, Train loss:0.000723, valid loss:0.000538
Epoch:22, Train loss:0.000701, valid loss:0.000514
Epoch:23, Train loss:0.000685, valid loss:0.000510
Epoch:24, Train loss:0.000685, valid loss:0.000569
Epoch:25, Train loss:0.000658, valid loss:0.000565
Epoch:26, Train loss:0.000660, valid loss:0.000535
Epoch:27, Train loss:0.000654, valid loss:0.000585
Epoch:28, Train loss:0.000640, valid loss:0.000524
Epoch:29, Train loss:0.000631, valid loss:0.000521
Epoch:30, Train loss:0.000608, valid loss:0.000530
Epoch:31, Train loss:0.000522, valid loss:0.000489
Epoch:32, Train loss:0.000516, valid loss:0.000512
Epoch:33, Train loss:0.000512, valid loss:0.000485
Epoch:34, Train loss:0.000510, valid loss:0.000495
Epoch:35, Train loss:0.000506, valid loss:0.000478
Epoch:36, Train loss:0.000498, valid loss:0.000489
Epoch:37, Train loss:0.000491, valid loss:0.000497
Epoch:38, Train loss:0.000488, valid loss:0.000494
Epoch:39, Train loss:0.000491, valid loss:0.000467
Epoch:40, Train loss:0.000478, valid loss:0.000496
Epoch:41, Train loss:0.000435, valid loss:0.000481
Epoch:42, Train loss:0.000435, valid loss:0.000492
Epoch:43, Train loss:0.000430, valid loss:0.000482
Epoch:44, Train loss:0.000429, valid loss:0.000479
Epoch:45, Train loss:0.000427, valid loss:0.000473
Epoch:46, Train loss:0.000423, valid loss:0.000498
Epoch:47, Train loss:0.000423, valid loss:0.000478
Epoch:48, Train loss:0.000415, valid loss:0.000510
Epoch:49, Train loss:0.000417, valid loss:0.000461
Epoch:50, Train loss:0.000418, valid loss:0.000465
Epoch:51, Train loss:0.000390, valid loss:0.000467
Epoch:52, Train loss:0.000387, valid loss:0.000468
Epoch:53, Train loss:0.000386, valid loss:0.000472
Epoch:54, Train loss:0.000385, valid loss:0.000473
Epoch:55, Train loss:0.000385, valid loss:0.000475
Epoch:56, Train loss:0.000385, valid loss:0.000471
Epoch:57, Train loss:0.000384, valid loss:0.000477
Epoch:58, Train loss:0.000384, valid loss:0.000470
Epoch:59, Train loss:0.000384, valid loss:0.000469
Epoch:60, Train loss:0.000384, valid loss:0.000469
training time 20010.852744579315
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.0867600348548074
plot_id,batch_id 0 1 miss% 0.06731841588729075
plot_id,batch_id 0 2 miss% 0.11387528349291291
plot_id,batch_id 0 3 miss% 0.0578185613838307
plot_id,batch_id 0 4 miss% 0.04610305444447655
plot_id,batch_id 0 5 miss% 0.08917476364379866
plot_id,batch_id 0 6 miss% 0.050891875206451824
plot_id,batch_id 0 7 miss% 0.07909767462639711
plot_id,batch_id 0 8 miss% 0.053474998838691994
plot_id,batch_id 0 9 miss% 0.05075601528257445
plot_id,batch_id 0 10 miss% 0.03289640093555958
plot_id,batch_id 0 11 miss% 0.05824150730554124
plot_id,batch_id 0 12 miss% 0.05877469800159359
plot_id,batch_id 0 13 miss% 0.050420133830295776
plot_id,batch_id 0 14 miss% 0.06725526306784393
plot_id,batch_id 0 15 miss% 0.04280031443804394
plot_id,batch_id 0 16 miss% 0.04093610974351057
plot_id,batch_id 0 17 miss% 0.057437383525181046
plot_id,batch_id 0 18 miss% 0.051102174946156445
plot_id,batch_id 0 19 miss% 0.11609436701805978
plot_id,batch_id 0 20 miss% 0.08182188219843693
plot_id,batch_id 0 21 miss% 0.07221508040180712
plot_id,batch_id 0 22 miss% 0.033518650562352696
plot_id,batch_id 0 23 miss% 0.04327628663253008
plot_id,batch_id 0 24 miss% 0.05398666823263211
plot_id,batch_id 0 25 miss% 0.06119501635138842
plot_id,batch_id 0 26 miss% 0.05077768962974934
plot_id,batch_id 0 27 miss% 0.03951392177596047
plot_id,batch_id 0 28 miss% 0.029464687497518353
plot_id,batch_id 0 29 miss% 0.04153380574471762
plot_id,batch_id 0 30 miss% 0.05315142340006493
plot_id,batch_id 0 31 miss% 0.07075292857340887
plot_id,batch_id 0 32 miss% 0.08415029697077472
plot_id,batch_id 0 33 miss% 0.04718243082122343
plot_id,batch_id 0 34 miss% 0.02030467223765278
plot_id,batch_id 0 35 miss% 0.05148284534497933
plot_id,batch_id 0 36 miss% 0.058768538183417825
plot_id,batch_id 0 37 miss% 0.058127603971554975
plot_id,batch_id 0 38 miss% 0.034855803182321274
plot_id,batch_id 0 39 miss% 0.03597731201464206
plot_id,batch_id 0 40 miss% 0.06381265181884715
plot_id,batch_id 0 41 miss% 0.05546583099132696
plot_id,batch_id 0 42 miss% 0.02943816900359277
plot_id,batch_id 0 43 miss% 0.05062167923514952
plot_id,batch_id 0 44 miss% 0.024242630635680936
plot_id,batch_id 0 45 miss% 0.11631919877812083
plot_id,batch_id 0 46 miss% 0.025876388902356766
plot_id,batch_id 0 47 miss% 0.035794556188108104
plot_id,batch_id 0 48 miss% 0.04193418168503568
plot_id,batch_id 0 49 miss% 0.022765242477337227
plot_id,batch_id 0 50 miss% 0.12710534833456452
plot_id,batch_id 0 51 miss% 0.020918186114798454
plot_id,batch_id 0 52 miss% 0.027872919719754596
plot_id,batch_id 0 53 miss% 0.008506031802507127
plot_id,batch_id 0 54 miss% 0.058590171543072764
plot_id,batch_id 0 55 miss% 0.06836861947183925
plot_id,batch_id 0 56 miss% 0.09119961579564989
plot_id,batch_id 0 57 miss% 0.07781603281148632
plot_id,batch_id 0 58 miss% 0.035987632554438334
plot_id,batch_id 0 59 miss% 0.04456934327835002
plot_id,batch_id 0 60 miss% 0.03392853610524471
plot_id,batch_id 0 61 miss% 0.022107388240200667
plot_id,batch_id 0 62 miss% 0.05301901682574178
plot_id,batch_id 0 63 miss% 0.04311336772383665
plot_id,batch_id 0 64 miss% 0.0519632631741285
plot_id,batch_id 0 65 miss% 0.04139257759060905
plot_id,batch_id 0 66 miss% 0.07230013596394498
plot_id,batch_id 0 67the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  209681
Epoch:0, Train loss:0.326765, valid loss:0.337755
Epoch:1, Train loss:0.018330, valid loss:0.002084
Epoch:2, Train loss:0.003450, valid loss:0.001454
Epoch:3, Train loss:0.002448, valid loss:0.001126
Epoch:4, Train loss:0.002045, valid loss:0.001204
Epoch:5, Train loss:0.001829, valid loss:0.001093
Epoch:6, Train loss:0.001722, valid loss:0.001210
Epoch:7, Train loss:0.001632, valid loss:0.000841
Epoch:8, Train loss:0.001508, valid loss:0.000992
Epoch:9, Train loss:0.001428, valid loss:0.000862
Epoch:10, Train loss:0.001365, valid loss:0.000798
Epoch:11, Train loss:0.000965, valid loss:0.000534
Epoch:12, Train loss:0.000925, valid loss:0.000582
Epoch:13, Train loss:0.000919, valid loss:0.000487
Epoch:14, Train loss:0.000893, valid loss:0.000517
Epoch:15, Train loss:0.000856, valid loss:0.000641
Epoch:16, Train loss:0.000861, valid loss:0.000560
Epoch:17, Train loss:0.000815, valid loss:0.000518
Epoch:18, Train loss:0.000817, valid loss:0.000548
Epoch:19, Train loss:0.000782, valid loss:0.000563
Epoch:20, Train loss:0.000745, valid loss:0.000503
Epoch:21, Train loss:0.000575, valid loss:0.000437
Epoch:22, Train loss:0.000561, valid loss:0.000394
Epoch:23, Train loss:0.000540, valid loss:0.000418
Epoch:24, Train loss:0.000529, valid loss:0.000442
Epoch:25, Train loss:0.000528, valid loss:0.000400
Epoch:26, Train loss:0.000549, valid loss:0.000412
Epoch:27, Train loss:0.000490, valid loss:0.000399
Epoch:28, Train loss:0.000511, valid loss:0.000395
Epoch:29, Train loss:0.000506, valid loss:0.000405
Epoch:30, Train loss:0.000462, valid loss:0.000469
Epoch:31, Train loss:0.000389, valid loss:0.000373
Epoch:32, Train loss:0.000386, valid loss:0.000375
Epoch:33, Train loss:0.000380, valid loss:0.000382
Epoch:34, Train loss:0.000373, valid loss:0.000356
Epoch:35, Train loss:0.000367, valid loss:0.000370
Epoch:36, Train loss:0.000367, valid loss:0.000408
Epoch:37, Train loss:0.000358, valid loss:0.000364
Epoch:38, Train loss:0.000366, valid loss:0.000375
Epoch:39, Train loss:0.000366, valid loss:0.000383
Epoch:40, Train loss:0.000349, valid loss:0.000363
Epoch:41, Train loss:0.000307, valid loss:0.000350
Epoch:42, Train loss:0.000306, valid loss:0.000364
Epoch:43, Train loss:0.000305, valid loss:0.000354
Epoch:44, Train loss:0.000303, valid loss:0.000382
Epoch:45, Train loss:0.000303, valid loss:0.000349
Epoch:46, Train loss:0.000304, valid loss:0.000353
Epoch:47, Train loss:0.000298, valid loss:0.000365
Epoch:48, Train loss:0.000294, valid loss:0.000376
Epoch:49, Train loss:0.000295, valid loss:0.000355
Epoch:50, Train loss:0.000298, valid loss:0.000351
Epoch:51, Train loss:0.000281, valid loss:0.000341
Epoch:52, Train loss:0.000278, valid loss:0.000341
Epoch:53, Train loss:0.000277, valid loss:0.000340
Epoch:54, Train loss:0.000276, valid loss:0.000342
Epoch:55, Train loss:0.000276, valid loss:0.000339
Epoch:56, Train loss:0.000276, valid loss:0.000340
Epoch:57, Train loss:0.000275, valid loss:0.000341
Epoch:58, Train loss:0.000275, valid loss:0.000340
Epoch:59, Train loss:0.000275, valid loss:0.000339
Epoch:60, Train loss:0.000274, valid loss:0.000340
training time 20051.44768857956
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.060115990939725146
plot_id,batch_id 0 1 miss% 0.07923458832376734
plot_id,batch_id 0 2 miss% 0.09087551904803005
plot_id,batch_id 0 3 miss% 0.05530167343877132
plot_id,batch_id 0 4 miss% 0.04200361781468807
plot_id,batch_id 0 5 miss% 0.03939384517728727
plot_id,batch_id 0 6 miss% 0.04594768130304936
plot_id,batch_id 0 7 miss% 0.0639984241654103
plot_id,batch_id 0 8 miss% 0.06010913858915402
plot_id,batch_id 0 9 miss% 0.055365400428811835
plot_id,batch_id 0 10 miss% 0.02959169805505856
plot_id,batch_id 0 11 miss% 0.06264883673258015
plot_id,batch_id 0 12 miss% 0.06342943764978794
plot_id,batch_id 0 13 miss% 0.05483556653439916
plot_id,batch_id 0 14 miss% 0.07544788530877622
plot_id,batch_id 0 15 miss% 0.04637414636683773
plot_id,batch_id 0 16 miss% 0.0798702558212143
plot_id,batch_id 0 17 miss% 0.07246369306411382
plot_id,batch_id 0 18 miss% 0.0633886207506827
plot_id,batch_id 0 19 miss% 0.057762234583805354
plot_id,batch_id 0 20 miss% 0.07052536446097304
plot_id,batch_id 0 21 miss% 0.054883233148903615
plot_id,batch_id 0 22 miss% 0.03862499758594345
plot_id,batch_id 0 23 miss% 0.059757523732981994
plot_id,batch_id 0 24 miss% 0.033671071501438846
plot_id,batch_id 0 25 miss% 0.0400553551405258
plot_id,batch_id 0 26 miss% 0.06797348911806395
plot_id,batch_id 0 27 miss% 0.041936588267055916
plot_id,batch_id 0 28 miss% 0.02858800966551546
plot_id,batch_id 0 29 miss% 0.03454943634528428
plot_id,batch_id 0 30 miss% 0.031374103810975544
plot_id,batch_id 0 31 miss% 0.07253353339751353
plot_id,batch_id 0 32 miss% 0.07087607492139189
plot_id,batch_id 0 33 miss% 0.03638269387354482
plot_id,batch_id 0 34 miss% 0.03272519955950771
plot_id,batch_id 0 35 miss% 0.03563257719171742
plot_id,batch_id 0 36 miss% 0.1302042018441014
plot_id,batch_id 0 37 miss% 0.0742379068772132
plot_id,batch_id 0 38 miss% 0.07826737779223203
plot_id,batch_id 0 39 miss% 0.035995824751266364
plot_id,batch_id 0 40 miss% 0.11489749112714644
plot_id,batch_id 0 41 miss% 0.046929347881069894
plot_id,batch_id 0 42 miss% 0.02779636560095995
plot_id,batch_id 0 43 miss% 0.05434617785300647
plot_id,batch_id 0 44 miss% 0.05050274442010369
plot_id,batch_id 0 45 miss% 0.0668755884590731
plot_id,batch_id 0 46 miss% 0.030601737690638606
plot_id,batch_id 0 47 miss% 0.027752018309530578
plot_id,batch_id 0 48 miss% 0.0340927418548964
plot_id,batch_id 0 49 miss% 0.03988601490697258
plot_id,batch_id 0 50 miss% 0.14945421732474076
plot_id,batch_id 0 51 miss% 0.0319512702965275
plot_id,batch_id 0 52 miss% 0.030255638214516045
plot_id,batch_id 0 53 miss% 0.024630685671932665
plot_id,batch_id 0 54 miss% 0.0523482735604245
plot_id,batch_id 0 55 miss% 0.1094279148402143
plot_id,batch_id 0 56 miss% 0.04647237448419498
plot_id,batch_id 0 57 miss% 0.05589436621519034
plot_id,batch_id 0 58 miss% 0.06830134610474922
plot_id,batch_id 0 59 miss% 0.04167130785089511
plot_id,batch_id 0 60 miss% 0.035883672563281614
plot_id,batch_id 0 61 miss% 0.023464235769955485
plot_id,batch_id 0 62 miss% 0.03630455040844799
plot_id,batch_id 0 63 miss% 0.04172029134095511
plot_id,batch_id 0 64 miss% 0.04782178517873765
plot_id,batch_id 0 65  miss% 0.06818858556300174
plot_id,batch_id 0 68 miss% 0.05487836213510724
plot_id,batch_id 0 69 miss% 0.10622274998163975
plot_id,batch_id 0 70 miss% 0.06670984047866045
plot_id,batch_id 0 71 miss% 0.07423575559946109
plot_id,batch_id 0 72 miss% 0.10549382658108339
plot_id,batch_id 0 73 miss% 0.047969074960401384
plot_id,batch_id 0 74 miss% 0.0693055032281807
plot_id,batch_id 0 75 miss% 0.05491500564175862
plot_id,batch_id 0 76 miss% 0.07408999588504184
plot_id,batch_id 0 77 miss% 0.03494943595824824
plot_id,batch_id 0 78 miss% 0.052142305032184394
plot_id,batch_id 0 79 miss% 0.06157930267186264
plot_id,batch_id 0 80 miss% 0.06777051121894452
plot_id,batch_id 0 81 miss% 0.10348309369216799
plot_id,batch_id 0 82 miss% 0.05523883705993266
plot_id,batch_id 0 83 miss% 0.0606645783090523
plot_id,batch_id 0 84 miss% 0.11547155071857976
plot_id,batch_id 0 85 miss% 0.042810690124100194
plot_id,batch_id 0 86 miss% 0.0407163438007445
plot_id,batch_id 0 87 miss% 0.08950353008403612
plot_id,batch_id 0 88 miss% 0.07678394053941073
plot_id,batch_id 0 89 miss% 0.06403735506287209
plot_id,batch_id 0 90 miss% 0.02285961804003019
plot_id,batch_id 0 91 miss% 0.07567932509146141
plot_id,batch_id 0 92 miss% 0.05035155138136761
plot_id,batch_id 0 93 miss% 0.05685018247020541
plot_id,batch_id 0 94 miss% 0.0666150687295992
plot_id,batch_id 0 95 miss% 0.029257898264173954
plot_id,batch_id 0 96 miss% 0.04658216779611816
plot_id,batch_id 0 97 miss% 0.06125796199379784
plot_id,batch_id 0 98 miss% 0.04710118715144559
plot_id,batch_id 0 99 miss% 0.11324628765616984
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.08676003 0.06731842 0.11387528 0.05781856 0.04610305 0.08917476
 0.05089188 0.07909767 0.053475   0.05075602 0.0328964  0.05824151
 0.0587747  0.05042013 0.06725526 0.04280031 0.04093611 0.05743738
 0.05110217 0.11609437 0.08182188 0.07221508 0.03351865 0.04327629
 0.05398667 0.06119502 0.05077769 0.03951392 0.02946469 0.04153381
 0.05315142 0.07075293 0.0841503  0.04718243 0.02030467 0.05148285
 0.05876854 0.0581276  0.0348558  0.03597731 0.06381265 0.05546583
 0.02943817 0.05062168 0.02424263 0.1163192  0.02587639 0.03579456
 0.04193418 0.02276524 0.12710535 0.02091819 0.02787292 0.00850603
 0.05859017 0.06836862 0.09119962 0.07781603 0.03598763 0.04456934
 0.03392854 0.02210739 0.05301902 0.04311337 0.05196326 0.04139258
 0.07230014 0.06818859 0.05487836 0.10622275 0.06670984 0.07423576
 0.10549383 0.04796907 0.0693055  0.05491501 0.07409    0.03494944
 0.05214231 0.0615793  0.06777051 0.10348309 0.05523884 0.06066458
 0.11547155 0.04281069 0.04071634 0.08950353 0.07678394 0.06403736
 0.02285962 0.07567933 0.05035155 0.05685018 0.06661507 0.0292579
 0.04658217 0.06125796 0.04710119 0.11324629]
for model  185 the mean error 0.05795276713866718
all id 185 hidden_dim 24 learning_rate 0.0025 num_layers 5 frames 31 out win 6 err 0.05795276713866718 time 20010.852744579315
Launcher: Job 186 completed in 20259 seconds.
Launcher: Task 209 done. Exiting.
miss% 0.05450117190558901
plot_id,batch_id 0 66 miss% 0.03164526838620327
plot_id,batch_id 0 67 miss% 0.022079333663994397
plot_id,batch_id 0 68 miss% 0.03859286552247839
plot_id,batch_id 0 69 miss% 0.06758205422606249
plot_id,batch_id 0 70 miss% 0.046118644453048065
plot_id,batch_id 0 71 miss% 0.047120572054625595
plot_id,batch_id 0 72 miss% 0.0986431139307839
plot_id,batch_id 0 73 miss% 0.07723343396186923
plot_id,batch_id 0 74 miss% 0.11397632651346111
plot_id,batch_id 0 75 miss% 0.0512360065423161
plot_id,batch_id 0 76 miss% 0.06635511091092483
plot_id,batch_id 0 77 miss% 0.028290690853096708
plot_id,batch_id 0 78 miss% 0.04161330210111218
plot_id,batch_id 0 79 miss% 0.07319539784476091
plot_id,batch_id 0 80 miss% 0.040071345906733793
plot_id,batch_id 0 81 miss% 0.08604493074508061
plot_id,batch_id 0 82 miss% 0.05281530180086795
plot_id,batch_id 0 83 miss% 0.07167431466559351
plot_id,batch_id 0 84 miss% 0.040153795171230765
plot_id,batch_id 0 85 miss% 0.0334080825652765
plot_id,batch_id 0 86 miss% 0.04453655557334531
plot_id,batch_id 0 87 miss% 0.047137812690052225
plot_id,batch_id 0 88 miss% 0.07023295493994522
plot_id,batch_id 0 89 miss% 0.06686448905277602
plot_id,batch_id 0 90 miss% 0.04012331064922915
plot_id,batch_id 0 91 miss% 0.04194533746151473
plot_id,batch_id 0 92 miss% 0.025615990055007715
plot_id,batch_id 0 93 miss% 0.03259714564298176
plot_id,batch_id 0 94 miss% 0.07773038324715272
plot_id,batch_id 0 95 miss% 0.04061286994781836
plot_id,batch_id 0 96 miss% 0.0492508667874209
plot_id,batch_id 0 97 miss% 0.023541954687927864
plot_id,batch_id 0 98 miss% 0.024238380863588958
plot_id,batch_id 0 99 miss% 0.04900146057643355
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06011599 0.07923459 0.09087552 0.05530167 0.04200362 0.03939385
 0.04594768 0.06399842 0.06010914 0.0553654  0.0295917  0.06264884
 0.06342944 0.05483557 0.07544789 0.04637415 0.07987026 0.07246369
 0.06338862 0.05776223 0.07052536 0.05488323 0.038625   0.05975752
 0.03367107 0.04005536 0.06797349 0.04193659 0.02858801 0.03454944
 0.0313741  0.07253353 0.07087607 0.03638269 0.0327252  0.03563258
 0.1302042  0.07423791 0.07826738 0.03599582 0.11489749 0.04692935
 0.02779637 0.05434618 0.05050274 0.06687559 0.03060174 0.02775202
 0.03409274 0.03988601 0.14945422 0.03195127 0.03025564 0.02463069
 0.05234827 0.10942791 0.04647237 0.05589437 0.06830135 0.04167131
 0.03588367 0.02346424 0.03630455 0.04172029 0.04782179 0.05450117
 0.03164527 0.02207933 0.03859287 0.06758205 0.04611864 0.04712057
 0.09864311 0.07723343 0.11397633 0.05123601 0.06635511 0.02829069
 0.0416133  0.0731954  0.04007135 0.08604493 0.0528153  0.07167431
 0.0401538  0.03340808 0.04453656 0.04713781 0.07023295 0.06686449
 0.04012331 0.04194534 0.02561599 0.03259715 0.07773038 0.04061287
 0.04925087 0.02354195 0.02423838 0.04900146]
for model  231 the mean error 0.05372015550910565
all id 231 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 31 out win 4 err 0.05372015550910565 time 20051.44768857956
Launcher: Job 232 completed in 20301 seconds.
Launcher: Task 34 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  209681
Epoch:0, Train loss:0.326765, valid loss:0.337755
Epoch:1, Train loss:0.014195, valid loss:0.001689
Epoch:2, Train loss:0.003515, valid loss:0.001432
Epoch:3, Train loss:0.002213, valid loss:0.001072
Epoch:4, Train loss:0.001852, valid loss:0.000932
Epoch:5, Train loss:0.001676, valid loss:0.000723
Epoch:6, Train loss:0.001496, valid loss:0.000891
Epoch:7, Train loss:0.001409, valid loss:0.000784
Epoch:8, Train loss:0.001338, valid loss:0.000967
Epoch:9, Train loss:0.001239, valid loss:0.000901
Epoch:10, Train loss:0.001145, valid loss:0.000692
Epoch:11, Train loss:0.000842, valid loss:0.000573
Epoch:12, Train loss:0.000822, valid loss:0.000712
Epoch:13, Train loss:0.000817, valid loss:0.000522
Epoch:14, Train loss:0.000774, valid loss:0.000504
Epoch:15, Train loss:0.000738, valid loss:0.000547
Epoch:16, Train loss:0.000754, valid loss:0.000562
Epoch:17, Train loss:0.000712, valid loss:0.000490
Epoch:18, Train loss:0.000698, valid loss:0.000566
Epoch:19, Train loss:0.000680, valid loss:0.000498
Epoch:20, Train loss:0.000658, valid loss:0.000559
Epoch:21, Train loss:0.000511, valid loss:0.000424
Epoch:22, Train loss:0.000491, valid loss:0.000406
Epoch:23, Train loss:0.000494, valid loss:0.000434
Epoch:24, Train loss:0.000480, valid loss:0.000478
Epoch:25, Train loss:0.000467, valid loss:0.000414
Epoch:26, Train loss:0.000477, valid loss:0.000427
Epoch:27, Train loss:0.000456, valid loss:0.000395
Epoch:28, Train loss:0.000454, valid loss:0.000438
Epoch:29, Train loss:0.000455, valid loss:0.000426
Epoch:30, Train loss:0.000436, valid loss:0.000451
Epoch:31, Train loss:0.000364, valid loss:0.000357
Epoch:32, Train loss:0.000359, valid loss:0.000368
Epoch:33, Train loss:0.000356, valid loss:0.000397
Epoch:34, Train loss:0.000354, valid loss:0.000382
Epoch:35, Train loss:0.000353, valid loss:0.000390
Epoch:36, Train loss:0.000342, valid loss:0.000480
Epoch:37, Train loss:0.000345, valid loss:0.000383
Epoch:38, Train loss:0.000343, valid loss:0.000387
Epoch:39, Train loss:0.000338, valid loss:0.000406
Epoch:40, Train loss:0.000333, valid loss:0.000385
Epoch:41, Train loss:0.000300, valid loss:0.000366
Epoch:42, Train loss:0.000299, valid loss:0.000378
Epoch:43, Train loss:0.000298, valid loss:0.000357
Epoch:44, Train loss:0.000295, valid loss:0.000353
Epoch:45, Train loss:0.000293, valid loss:0.000373
Epoch:46, Train loss:0.000293, valid loss:0.000376
Epoch:47, Train loss:0.000294, valid loss:0.000368
Epoch:48, Train loss:0.000290, valid loss:0.000356
Epoch:49, Train loss:0.000286, valid loss:0.000377
Epoch:50, Train loss:0.000285, valid loss:0.000381
Epoch:51, Train loss:0.000274, valid loss:0.000360
Epoch:52, Train loss:0.000271, valid loss:0.000353
Epoch:53, Train loss:0.000269, valid loss:0.000352
Epoch:54, Train loss:0.000269, valid loss:0.000351
Epoch:55, Train loss:0.000268, valid loss:0.000351
Epoch:56, Train loss:0.000268, valid loss:0.000350
Epoch:57, Train loss:0.000267, valid loss:0.000354
Epoch:58, Train loss:0.000267, valid loss:0.000349
Epoch:59, Train loss:0.000267, valid loss:0.000351
Epoch:60, Train loss:0.000267, valid loss:0.000355
training time 20232.33307814598
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.06527058331764664
plot_id,batch_id 0 1 miss% 0.03717769968702569
plot_id,batch_id 0 2 miss% 0.09351179641588445
plot_id,batch_id 0 3 miss% 0.027716962585227045
plot_id,batch_id 0 4 miss% 0.030040912661763397
plot_id,batch_id 0 5 miss% 0.03804888938221072
plot_id,batch_id 0 6 miss% 0.06687760463486146
plot_id,batch_id 0 7 miss% 0.09549259203200512
plot_id,batch_id 0 8 miss% 0.09811716968278451
plot_id,batch_id 0 9 miss% 0.04332827626585687
plot_id,batch_id 0 10 miss% 0.05585230935505239
plot_id,batch_id 0 11 miss% 0.05254469962787186
plot_id,batch_id 0 12 miss% 0.04651771008738984
plot_id,batch_id 0 13 miss% 0.03786348887707061
plot_id,batch_id 0 14 miss% 0.03480459675998754
plot_id,batch_id 0 15 miss% 0.029907304545249317
plot_id,batch_id 0 16 miss% 0.09531927476723252
plot_id,batch_id 0 17 miss% 0.05893459548529403
plot_id,batch_id 0 18 miss% 0.03602282809953844
plot_id,batch_id 0 19 miss% 0.061776114173475144
plot_id,batch_id 0 20 miss% 0.07140382797700527
plot_id,batch_id 0 21 miss% 0.03435417585257846
plot_id,batch_id 0 22 miss% 0.03560394633828384
plot_id,batch_id 0 23 miss% 0.02982434941423361
plot_id,batch_id 0 24 miss% 0.029435654192263842
plot_id,batch_id 0 25 miss% 0.04794198037948488
plot_id,batch_id 0 26 miss% 0.033124707849937256
plot_id,batch_id 0 27 miss% 0.04034852532333789
plot_id,batch_id 0 28 miss% 0.03125752749666661
plot_id,batch_id 0 29 miss% 0.032584193874997336
plot_id,batch_id 0 30 miss% 0.02252513586553301
plot_id,batch_id 0 31 miss% 0.08212153079933263
plot_id,batch_id 0 32 miss% 0.0701551357536354
plot_id,batch_id 0 33 miss% 0.02864227591043326
plot_id,batch_id 0 34 miss% 0.028220699175269787
plot_id,batch_id 0 35 miss% 0.05890835677560683
plot_id,batch_id 0 36 miss% 0.05366958898258973
plot_id,batch_id 0 37 miss% 0.05338977702229479
plot_id,batch_id 0 38 miss% 0.04563204669230608
plot_id,batch_id 0 39 miss% 0.030758627901623536
plot_id,batch_id 0 40 miss% 0.09106997983339288
plot_id,batch_id 0 41 miss% 0.041288886891894715
plot_id,batch_id 0 42 miss% 0.021542942037726238
plot_id,batch_id 0 43 miss% 0.06032387205580566
plot_id,batch_id 0 44 miss% 0.029291237563016683
plot_id,batch_id 0 45 miss% 0.05747458658171714
plot_id,batch_id 0 46 miss% 0.043511900712842264
plot_id,batch_id 0 47 miss% 0.016884692063155553
plot_id,batch_id 0 48 miss% 0.025864988319252546
plot_id,batch_id 0 49 miss% 0.02388951435955018
plot_id,batch_id 0 50 miss% 0.12276557240612458
plot_id,batch_id 0 51 miss% 0.045104476936049596
plot_id,batch_id 0 52 miss% 0.029432772119540915
plot_id,batch_id 0 53 miss% 0.02203316780714743
plot_id,batch_id 0 54 miss% 0.03912213051025035
plot_id,batch_id 0 55 miss% 0.05150728906410877
plot_id,batch_id 0 56 miss% 0.06516895074246311
plot_id,batch_id 0 57 miss% 0.02146692397697474
plot_id,batch_id 0 58 miss% 0.02581931080236232
plot_id,batch_id 0 59 miss% 0.03208361759473818
plot_id,batch_id 0 60 miss% 0.029421662519743357
plot_id,batch_id 0 61 miss% 0.04231402574812276
plot_id,batch_id 0 62 miss% 0.07240824308489281
plot_id,batch_id 0 63 miss% 0.04110211621754643
plot_id,batch_id 0 64 miss% 0.03749411252962552
plot_id,batch_id 0 65 miss% 0.1295225882945451
plot_id,batch_id 0 66 miss% 0.02844850673876937
plot_id,batch_id 0 67 miss% 0.026555088102037386
plot_id,batch_id 0 68 miss% 0.02808944492186081
plot_id,batch_id 0 69 miss% 0.06984083665772761
plot_id,batch_id 0 70 miss% 0.07318040160607488
plot_id,batch_id 0 71 miss% 0.04038269555805914
plot_id,batch_id 0 72 miss% 0.06709218906852532
plot_id,batch_id 0 73 miss% 0.057196861698379634
plot_id,batch_id 0 74 miss% 0.08556803492574395
plot_id,batch_id 0 75 miss% 0.06059966733680662
plot_id,batch_id 0 76 miss% 0.03613766731710901
plot_id,batch_id 0 77 miss% 0.02605499004539014
plot_id,batch_id 0 78 miss% 0.03330731890421802
plot_id,batch_id 0 79 miss% 0.08388948718029171
plot_id,batch_id 0 80 miss% 0.05572573626330153
plot_id,batch_id 0 81 miss% 0.08784797085845608
plot_id,batch_id 0 82 miss% 0.05671112040431624
plot_id,batch_id 0 83 miss% 0.062373369881661686
plot_id,batch_id 0 84 miss% 0.059197225762931495
plot_id,batch_id 0 85 miss% 0.020877767394795292
plot_id,batch_id 0 86 miss% 0.044750325205770235
plot_id,batch_id 0 87 miss% 0.04433579772118454
plot_id,batch_id 0 88 miss% 0.05951223665959205
plot_id,batch_id 0 89 miss% 0.04769371228968176
plot_id,batch_id 0 90 miss% 0.045307019012479084
plot_id,batch_id 0 91 miss% 0.04804766751382305
plot_id,batch_id 0 92 miss% 0.038778386553590234
plot_id,batch_id 0 93 miss% 0.05490752468568307
plot_id,batch_id 0 94 miss% 0.04864138469941996
plot_id,batch_id 0 95 miss% 0.07163168122907032
plot_id,batch_id 0 96 miss% 0.05058442763006379
plot_id,batch_id 0 97 miss% 0.05134882974556484
plot_id,batch_id 0 98 miss% 0.035329730399226635
plot_id,batch_id 0 99 miss% 0.10485373748997111
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.06527058 0.0371777  0.0935118  0.02771696 0.03004091 0.03804889
 0.0668776  0.09549259 0.09811717 0.04332828 0.05585231 0.0525447
 0.04651771 0.03786349 0.0348046  0.0299073  0.09531927 0.0589346
 0.03602283 0.06177611 0.07140383 0.03435418 0.03560395 0.02982435
 0.02943565 0.04794198 0.03312471 0.04034853 0.03125753 0.03258419
 0.02252514 0.08212153 0.07015514 0.02864228 0.0282207  0.05890836
 0.05366959 0.05338978 0.04563205 0.03075863 0.09106998 0.04128889
 0.02154294 0.06032387 0.02929124 0.05747459 0.0435119  0.01688469
 0.02586499 0.02388951 0.12276557 0.04510448 0.02943277 0.02203317
 0.03912213 0.05150729 0.06516895 0.02146692 0.02581931 0.03208362
 0.02942166 0.04231403 0.07240824 0.04110212 0.03749411 0.12952259
 0.02844851 0.02655509 0.02808944 0.06984084 0.0731804  0.0403827
 0.06709219 0.05719686 0.08556803 0.06059967 0.03613767 0.02605499
 0.03330732 0.08388949 0.05572574 0.08784797 0.05671112 0.06237337
 0.05919723 0.02087777 0.04475033 0.0443358  0.05951224 0.04769371
 0.04530702 0.04804767 0.03877839 0.05490752 0.04864138 0.07163168
 0.05058443 0.05134883 0.03532973 0.10485374]
for model  204 the mean error 0.04987735874255007
all id 204 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 31 out win 4 err 0.04987735874255007 time 20232.33307814598
Launcher: Job 205 completed in 20479 seconds.
Launcher: Task 39 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  209681
Epoch:0, Train loss:0.326765, valid loss:0.337755
Epoch:1, Train loss:0.020329, valid loss:0.003736
Epoch:2, Train loss:0.007793, valid loss:0.001622
Epoch:3, Train loss:0.002919, valid loss:0.001298
Epoch:4, Train loss:0.002104, valid loss:0.000987
Epoch:5, Train loss:0.001791, valid loss:0.000924
Epoch:6, Train loss:0.001638, valid loss:0.000799
Epoch:7, Train loss:0.001500, valid loss:0.000724
Epoch:8, Train loss:0.001382, valid loss:0.000858
Epoch:9, Train loss:0.001285, valid loss:0.000719
Epoch:10, Train loss:0.001199, valid loss:0.000633
Epoch:11, Train loss:0.000905, valid loss:0.000556
Epoch:12, Train loss:0.000898, valid loss:0.000641
Epoch:13, Train loss:0.000871, valid loss:0.000540
Epoch:14, Train loss:0.000830, valid loss:0.000485
Epoch:15, Train loss:0.000806, valid loss:0.000578
Epoch:16, Train loss:0.000781, valid loss:0.000525
Epoch:17, Train loss:0.000751, valid loss:0.000590
Epoch:18, Train loss:0.000761, valid loss:0.000508
Epoch:19, Train loss:0.000732, valid loss:0.000464
Epoch:20, Train loss:0.000717, valid loss:0.000517
Epoch:21, Train loss:0.000557, valid loss:0.000450
Epoch:22, Train loss:0.000555, valid loss:0.000410
Epoch:23, Train loss:0.000526, valid loss:0.000458
Epoch:24, Train loss:0.000525, valid loss:0.000466
Epoch:25, Train loss:0.000516, valid loss:0.000439
Epoch:26, Train loss:0.000511, valid loss:0.000426
Epoch:27, Train loss:0.000509, valid loss:0.000409
Epoch:28, Train loss:0.000490, valid loss:0.000440
Epoch:29, Train loss:0.000491, valid loss:0.000403
Epoch:30, Train loss:0.000471, valid loss:0.000457
Epoch:31, Train loss:0.000406, valid loss:0.000371
Epoch:32, Train loss:0.000400, valid loss:0.000400
Epoch:33, Train loss:0.000400, valid loss:0.000384
Epoch:34, Train loss:0.000396, valid loss:0.000389
Epoch:35, Train loss:0.000386, valid loss:0.000375
Epoch:36, Train loss:0.000385, valid loss:0.000445
Epoch:37, Train loss:0.000388, valid loss:0.000403
Epoch:38, Train loss:0.000376, valid loss:0.000384
Epoch:39, Train loss:0.000384, valid loss:0.000385
Epoch:40, Train loss:0.000373, valid loss:0.000438
Epoch:41, Train loss:0.000340, valid loss:0.000370
Epoch:42, Train loss:0.000338, valid loss:0.000385
Epoch:43, Train loss:0.000339, valid loss:0.000368
Epoch:44, Train loss:0.000332, valid loss:0.000372
Epoch:45, Train loss:0.000332, valid loss:0.000376
Epoch:46, Train loss:0.000335, valid loss:0.000375
Epoch:47, Train loss:0.000331, valid loss:0.000367
Epoch:48, Train loss:0.000325, valid loss:0.000390
Epoch:49, Train loss:0.000323, valid loss:0.000376
Epoch:50, Train loss:0.000323, valid loss:0.000384
Epoch:51, Train loss:0.000306, valid loss:0.000379
Epoch:52, Train loss:0.000304, valid loss:0.000372
Epoch:53, Train loss:0.000303, valid loss:0.000371
Epoch:54, Train loss:0.000302, valid loss:0.000369
Epoch:55, Train loss:0.000302, valid loss:0.000373
Epoch:56, Train loss:0.000301, valid loss:0.000375
Epoch:57, Train loss:0.000301, valid loss:0.000374
Epoch:58, Train loss:0.000301, valid loss:0.000370
Epoch:59, Train loss:0.000301, valid loss:0.000372
Epoch:60, Train loss:0.000300, valid loss:0.000376
training time 20415.51780819893
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.04929414782812506
plot_id,batch_id 0 1 miss% 0.038414604400691055
plot_id,batch_id 0 2 miss% 0.11171757288847647
plot_id,batch_id 0 3 miss% 0.0680821111422849
plot_id,batch_id 0 4 miss% 0.03607930515529502
plot_id,batch_id 0 5 miss% 0.06338268194603316
plot_id,batch_id 0 6 miss% 0.04240016944426104
plot_id,batch_id 0 7 miss% 0.07335699684768154
plot_id,batch_id 0 8 miss% 0.09129037495609073
plot_id,batch_id 0 9 miss% 0.03374702125076553
plot_id,batch_id 0 10 miss% 0.02208929965841913
plot_id,batch_id 0 11 miss% 0.06503540448902612
plot_id,batch_id 0 12 miss% 0.04050494996710875
plot_id,batch_id 0 13 miss% 0.051461570157636485
plot_id,batch_id 0 14 miss% 0.07359810368670791
plot_id,batch_id 0 15 miss% 0.042571587862007196
plot_id,batch_id 0 16 miss% 0.09184358480365595
plot_id,batch_id 0 17 miss% 0.0363156667660552
plot_id,batch_id 0 18 miss% 0.051544306016538165
plot_id,batch_id 0 19 miss% 0.0609740254217883
plot_id,batch_id 0 20 miss% 0.02545342680847315
plot_id,batch_id 0 21 miss% 0.05037149520669421
plot_id,batch_id 0 22 miss% 0.0517790865510223
plot_id,batch_id 0 23 miss% 0.0721495725259395
plot_id,batch_id 0 24 miss% 0.03877426940543961
plot_id,batch_id 0 25 miss% 0.024468566129074745
plot_id,batch_id 0 26 miss% 0.07496089105467192
plot_id,batch_id 0 27 miss% 0.04777666618460378
plot_id,batch_id 0 28 miss% 0.03286623564682978
plot_id,batch_id 0 29 miss% 0.0294105650327749
plot_id,batch_id 0 30 miss% 0.06324378446203717
plot_id,batch_id 0 31 miss% 0.10876586498716541
plot_id,batch_id 0 32 miss% 0.09058092878280921
plot_id,batch_id 0 33 miss% 0.03528264698303121
plot_id,batch_id 0 34 miss% 0.02741899370235713
plot_id,batch_id 0 35 miss% 0.04445877363222804
plot_id,batch_id 0 36 miss% 0.09381820924255672
plot_id,batch_id 0 37 miss% 0.0409513147078113
plot_id,batch_id 0 38 miss% 0.05140439604436856
plot_id,batch_id 0 39 miss% 0.022975951588742757
plot_id,batch_id 0 40 miss% 0.058543475768645784
plot_id,batch_id 0 41 miss% 0.03586712278831566
plot_id,batch_id 0 42 miss% 0.032795417013985015
plot_id,batch_id 0 43 miss% 0.04357667107526848
plot_id,batch_id 0 44 miss% 0.04337520025558091
plot_id,batch_id 0 45 miss% 0.07607695662565457
plot_id,batch_id 0 46 miss% 0.041102296006596796
plot_id,batch_id 0 47 miss% 0.0307317956293291
plot_id,batch_id 0 48 miss% 0.0360354980658813
plot_id,batch_id 0 49 miss% 0.029607076130685722
plot_id,batch_id 0 50 miss% 0.1679041711577992
plot_id,batch_id 0 51 miss% 0.030394886303242545
plot_id,batch_id 0 52 miss% 0.02689251678581632
plot_id,batch_id 0 53 miss% 0.028582575064807676
plot_id,batch_id 0 54 miss% 0.06546730875571335
plot_id,batch_id 0 55 miss% 0.07209577936065166
plot_id,batch_id 0 56 miss% 0.05790734532137233
plot_id,batch_id 0 57 miss% 0.031183448783605707
plot_id,batch_id 0 58 miss% 0.04123966858794496
plot_id,batch_id 0 59 miss% 0.0499923804040243
plot_id,batch_id 0 60 miss% 0.040686243915553796
plot_id,batch_id 0 61 miss% 0.03699763529535983
plot_id,batch_id 0 62 miss% 0.049783622486830346
plot_id,batch_id 0 63 miss% 0.0652126881353057
plot_id,batch_id 0 64 miss% 0.03633419681335725
plot_id,batch_id 0 65 miss% 0.08448094943531002
plot_id,batch_id 0 66 miss% 0.01706440247824923
plot_id,batch_id 0 67 miss% 0.03277044543518821
plot_id,batch_id 0 68 miss% 0.035210460190465497
plot_id,batch_id 0 69 miss% 0.04921867978574346
plot_id,batch_id 0 70 miss% 0.08420530802704423
plot_id,batch_id 0 71 miss% 0.05649144080065536
plot_id,batch_id 0 72 miss% 0.05059910159196128
plot_id,batch_id 0 73 miss% 0.04281367501506031
plot_id,batch_id 0 74 miss% 0.08325324227621522
plot_id,batch_id 0 75 miss% 0.051327719989238414
plot_id,batch_id 0 76 miss% 0.04041862585731532
plot_id,batch_id 0 77 miss% 0.028923259036279132
plot_id,batch_id 0 78 miss% 0.03154038481349535
plot_id,batch_id 0 79 miss% 0.06501830545075846
plot_id,batch_id 0 80 miss% 0.08886134607086399
plot_id,batch_id 0 81 miss% 0.09495918689603498
plot_id,batch_id 0 82 miss% 0.05418488654507877
plot_id,batch_id 0 83 miss% 0.06043374111655768
plot_id,batch_id 0 84 miss% 0.08197143749551045
plot_id,batch_id 0 85 miss% 0.02614047545737705
plot_id,batch_id 0 86 miss% 0.052125878784078844
plot_id,batch_id 0 87 miss% 0.06368011790533733
plot_id,batch_id 0 88 miss% 0.09702533992058955
plot_id,batch_id 0 89 miss% 0.05977059920129276
plot_id,batch_id 0 90 miss% 0.03252908301266432
plot_id,batch_id 0 91 miss% 0.03412280629292718
plot_id,batch_id 0 92 miss% 0.030558637727271437
plot_id,batch_id 0 93 miss% 0.029527793172311016
plot_id,batch_id 0 94 miss% 0.05755256855117309
plot_id,batch_id 0 95 miss% 0.03674851034109775
plot_id,batch_id 0 96 miss% 0.04536941985284101
plot_id,batch_id 0 97 miss% 0.06556891413561233
plot_id,batch_id 0 98 miss% 0.028835857470706935
plot_id,batch_id 0 99 miss% 0.07438612397799503
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04929415 0.0384146  0.11171757 0.06808211 0.03607931 0.06338268
 0.04240017 0.073357   0.09129037 0.03374702 0.0220893  0.0650354
 0.04050495 0.05146157 0.0735981  0.04257159 0.09184358 0.03631567
 0.05154431 0.06097403 0.02545343 0.0503715  0.05177909 0.07214957
 0.03877427 0.02446857 0.07496089 0.04777667 0.03286624 0.02941057
 0.06324378 0.10876586 0.09058093 0.03528265 0.02741899 0.04445877
 0.09381821 0.04095131 0.0514044  0.02297595 0.05854348 0.03586712
 0.03279542 0.04357667 0.0433752  0.07607696 0.0411023  0.0307318
 0.0360355  0.02960708 0.16790417 0.03039489 0.02689252 0.02858258
 0.06546731 0.07209578 0.05790735 0.03118345 0.04123967 0.04999238
 0.04068624 0.03699764 0.04978362 0.06521269 0.0363342  0.08448095
 0.0170644  0.03277045 0.03521046 0.04921868 0.08420531 0.05649144
 0.0505991  0.04281368 0.08325324 0.05132772 0.04041863 0.02892326
 0.03154038 0.06501831 0.08886135 0.09495919 0.05418489 0.06043374
 0.08197144 0.02614048 0.05212588 0.06368012 0.09702534 0.0597706
 0.03252908 0.03412281 0.03055864 0.02952779 0.05755257 0.03674851
 0.04536942 0.06556891 0.02883586 0.07438612]
for model  177 the mean error 0.052666898240089084
all id 177 hidden_dim 32 learning_rate 0.0025 num_layers 4 frames 31 out win 4 err 0.052666898240089084 time 20415.51780819893
Launcher: Job 178 completed in 20659 seconds.
Launcher: Task 106 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  151697
Epoch:0, Train loss:0.378775, valid loss:0.340922
Epoch:1, Train loss:0.086765, valid loss:0.002197
Epoch:2, Train loss:0.005307, valid loss:0.001642
Epoch:3, Train loss:0.002863, valid loss:0.001188
Epoch:4, Train loss:0.002445, valid loss:0.001183
Epoch:5, Train loss:0.002148, valid loss:0.000958
Epoch:6, Train loss:0.001890, valid loss:0.000961
Epoch:7, Train loss:0.001722, valid loss:0.000869
Epoch:8, Train loss:0.001638, valid loss:0.000939
Epoch:9, Train loss:0.001488, valid loss:0.000821
Epoch:10, Train loss:0.001417, valid loss:0.000739
Epoch:11, Train loss:0.001050, valid loss:0.000575
Epoch:12, Train loss:0.001000, valid loss:0.000632
Epoch:13, Train loss:0.000960, valid loss:0.000504
Epoch:14, Train loss:0.000962, valid loss:0.000658
Epoch:15, Train loss:0.000920, valid loss:0.000513
Epoch:16, Train loss:0.000896, valid loss:0.000549
Epoch:17, Train loss:0.000862, valid loss:0.000581
Epoch:18, Train loss:0.000842, valid loss:0.000579
Epoch:19, Train loss:0.000818, valid loss:0.000561
Epoch:20, Train loss:0.000790, valid loss:0.000531
Epoch:21, Train loss:0.000625, valid loss:0.000469
Epoch:22, Train loss:0.000608, valid loss:0.000474
Epoch:23, Train loss:0.000596, valid loss:0.000550
Epoch:24, Train loss:0.000585, valid loss:0.000521
Epoch:25, Train loss:0.000565, valid loss:0.000491
Epoch:26, Train loss:0.000558, valid loss:0.000481
Epoch:27, Train loss:0.000556, valid loss:0.000476
Epoch:28, Train loss:0.000551, valid loss:0.000474
Epoch:29, Train loss:0.000549, valid loss:0.000459
Epoch:30, Train loss:0.000519, valid loss:0.000466
Epoch:31, Train loss:0.000443, valid loss:0.000439
Epoch:32, Train loss:0.000442, valid loss:0.000424
Epoch:33, Train loss:0.000431, valid loss:0.000443
Epoch:34, Train loss:0.000430, valid loss:0.000435
Epoch:35, Train loss:0.000428, valid loss:0.000477
Epoch:36, Train loss:0.000421, valid loss:0.000465
Epoch:37, Train loss:0.000418, valid loss:0.000432
Epoch:38, Train loss:0.000411, valid loss:0.000447
Epoch:39, Train loss:0.000409, valid loss:0.000432
Epoch:40, Train loss:0.000406, valid loss:0.000410
Epoch:41, Train loss:0.000365, valid loss:0.000419
Epoch:42, Train loss:0.000361, valid loss:0.000428
Epoch:43, Train loss:0.000359, valid loss:0.000408
Epoch:44, Train loss:0.000356, valid loss:0.000438
Epoch:45, Train loss:0.000356, valid loss:0.000423
Epoch:46, Train loss:0.000356, valid loss:0.000414
Epoch:47, Train loss:0.000350, valid loss:0.000423
Epoch:48, Train loss:0.000352, valid loss:0.000411
Epoch:49, Train loss:0.000349, valid loss:0.000412
Epoch:50, Train loss:0.000350, valid loss:0.000433
Epoch:51, Train loss:0.000328, valid loss:0.000415
Epoch:52, Train loss:0.000324, valid loss:0.000417
Epoch:53, Train loss:0.000323, valid loss:0.000414
Epoch:54, Train loss:0.000322, valid loss:0.000416
Epoch:55, Train loss:0.000322, valid loss:0.000413
Epoch:56, Train loss:0.000322, valid loss:0.000412
Epoch:57, Train loss:0.000321, valid loss:0.000414
Epoch:58, Train loss:0.000321, valid loss:0.000413
Epoch:59, Train loss:0.000321, valid loss:0.000413
Epoch:60, Train loss:0.000321, valid loss:0.000414
training time 20681.26298713684
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07211506863823298
plot_id,batch_id 0 1 miss% 0.030829543627723076
plot_id,batch_id 0 2 miss% 0.09936484514638451
plot_id,batch_id 0 3 miss% 0.04694548449350932
plot_id,batch_id 0 4 miss% 0.07121681402731381
plot_id,batch_id 0 5 miss% 0.04853166203407112
plot_id,batch_id 0 6 miss% 0.062239497854106776
plot_id,batch_id 0 7 miss% 0.07411828530214219
plot_id,batch_id 0 8 miss% 0.08618821505352885
plot_id,batch_id 0 9 miss% 0.026796594181010366
plot_id,batch_id 0 10 miss% 0.028473183547688564
plot_id,batch_id 0 11 miss% 0.04995715729382349
plot_id,batch_id 0 12 miss% 0.06746128827205622
plot_id,batch_id 0 13 miss% 0.05198981488572663
plot_id,batch_id 0 14 miss% 0.05844472528698816
plot_id,batch_id 0 15 miss% 0.057523420588308935
plot_id,batch_id 0 16 miss% 0.06718886654199019
plot_id,batch_id 0 17 miss% 0.03940680628421462
plot_id,batch_id 0 18 miss% 0.08201361455156572
plot_id,batch_id 0 19 miss% 0.07475501413027826
plot_id,batch_id 0 20 miss% 0.10030211243406999
plot_id,batch_id 0 21 miss% 0.06030730535164351
plot_id,batch_id 0 22 miss% 0.044764122475290526
plot_id,batch_id 0 23 miss% 0.04740052769514217
plot_id,batch_id 0 24 miss% 0.07710639360390074
plot_id,batch_id 0 25 miss% 0.07167923700253946
plot_id,batch_id 0 26 miss% 0.039230617579196346
plot_id,batch_id 0 27 miss% 0.06004474366694191
plot_id,batch_id 0 28 miss% 0.08003550893263367
plot_id,batch_id 0 29 miss% 0.03586421784016771
plot_id,batch_id 0 30 miss% 0.03799875860299932
plot_id,batch_id 0 31 miss% 0.09217410975376865
plot_id,batch_id 0 32 miss% 0.09634750628122911
plot_id,batch_id 0 33 miss% 0.061175737808568345
plot_id,batch_id 0 34 miss% 0.05343153768748385
plot_id,batch_id 0 35 miss% 0.0471131344180018
plot_id,batch_id 0 36 miss% 0.06302901574343062
plot_id,batch_id 0 37 miss% 0.06463953694417438
plot_id,batch_id 0 38 miss% 0.0537373535534959
plot_id,batch_id 0 39 miss% 0.031803208402899394
plot_id,batch_id 0 40 miss% 0.09732992085156761
plot_id,batch_id 0 41 miss% 0.04622052860673068
plot_id,batch_id 0 42 miss% 0.04774278631453929
plot_id,batch_id 0 43 miss% 0.024440490035073804
plot_id,batch_id 0 44 miss% 0.056319468460338765
plot_id,batch_id 0 45 miss% 0.04305159499575181
plot_id,batch_id 0 46 miss% 0.046567114869667206
plot_id,batch_id 0 47 miss% 0.025359993331639342
plot_id,batch_id 0 48 miss% 0.0372508641332738
plot_id,batch_id 0 49 miss% 0.0259804565075318
plot_id,batch_id 0 50 miss% 0.15905775557884766
plot_id,batch_id 0 51 miss% 0.03989749384159051
plot_id,batch_id 0 52 miss% 0.039427265179514205
plot_id,batch_id 0 53 miss% 0.023412929071605346
plot_id,batch_id 0 54 miss% 0.02847402432453653
plot_id,batch_id 0 55 miss% 0.044154974215175294
plot_id,batch_id 0 56 miss% 0.06643063444186591
plot_id,batch_id 0 57 miss% 0.05584038022458011
plot_id,batch_id 0 58 miss% 0.04896036081421766
plot_id,batch_id 0 59 miss% 0.03797778247651633
plot_id,batch_id 0 60 miss% 0.047964069246693476
plot_id,batch_id 0 61 miss% 0.047207250684579316
plot_id,batch_id 0 62 miss% 0.0901150899408085
plot_id,batch_id 0 63 miss% 0.03886655043846636
plot_id,batch_id 0 64 miss% 0.06443851265421581
plot_id,batch_id 0 65 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  151697
Epoch:0, Train loss:0.378775, valid loss:0.340922
Epoch:1, Train loss:0.234656, valid loss:0.233547
Epoch:2, Train loss:0.228948, valid loss:0.233128
Epoch:3, Train loss:0.228474, valid loss:0.232977
Epoch:4, Train loss:0.228221, valid loss:0.232853
Epoch:5, Train loss:0.228020, valid loss:0.232867
Epoch:6, Train loss:0.227898, valid loss:0.233192
Epoch:7, Train loss:0.227778, valid loss:0.232853
Epoch:8, Train loss:0.227726, valid loss:0.232805
Epoch:9, Train loss:0.227625, valid loss:0.232993
Epoch:10, Train loss:0.227644, valid loss:0.232750
Epoch:11, Train loss:0.227194, valid loss:0.232609
Epoch:12, Train loss:0.227169, valid loss:0.232667
Epoch:13, Train loss:0.227130, valid loss:0.232595
Epoch:14, Train loss:0.227116, valid loss:0.232721
Epoch:15, Train loss:0.227094, valid loss:0.232578
Epoch:16, Train loss:0.227070, valid loss:0.232684
Epoch:17, Train loss:0.227058, valid loss:0.232548
Epoch:18, Train loss:0.227023, valid loss:0.232524
Epoch:19, Train loss:0.226985, valid loss:0.232668
Epoch:20, Train loss:0.226983, valid loss:0.232506
Epoch:21, Train loss:0.226819, valid loss:0.232460
Epoch:22, Train loss:0.226797, valid loss:0.232451
Epoch:23, Train loss:0.226793, valid loss:0.232566
Epoch:24, Train loss:0.226765, valid loss:0.232481
Epoch:25, Train loss:0.226756, valid loss:0.232476
Epoch:26, Train loss:0.226764, valid loss:0.232480
Epoch:27, Train loss:0.226744, valid loss:0.232450
Epoch:28, Train loss:0.226751, valid loss:0.232469
Epoch:29, Train loss:0.226755, valid loss:0.232421
Epoch:30, Train loss:0.226719, valid loss:0.232424
Epoch:31, Train loss:0.226635, valid loss:0.232413
Epoch:32, Train loss:0.226621, valid loss:0.232401
Epoch:33, Train loss:0.226619, valid loss:0.232417
Epoch:34, Train loss:0.226616, valid loss:0.232424
Epoch:35, Train loss:0.226620, valid loss:0.232422
Epoch:36, Train loss:0.226611, valid loss:0.232433
Epoch:37, Train loss:0.226610, valid loss:0.232411
Epoch:38, Train loss:0.226605, valid loss:0.232441
Epoch:39, Train loss:0.226598, valid loss:0.232407
Epoch:40, Train loss:0.226602, valid loss:0.232399
Epoch:41, Train loss:0.226552, valid loss:0.232388
Epoch:42, Train loss:0.226551, valid loss:0.232392
Epoch:43, Train loss:0.226547, valid loss:0.232388
Epoch:44, Train loss:0.226543, valid loss:0.232476
Epoch:45, Train loss:0.226552, valid loss:0.232398
Epoch:46, Train loss:0.226542, valid loss:0.232394
Epoch:47, Train loss:0.226540, valid loss:0.232403
Epoch:48, Train loss:0.226539, valid loss:0.232393
Epoch:49, Train loss:0.226533, valid loss:0.232410
Epoch:50, Train loss:0.226539, valid loss:0.232409
Epoch:51, Train loss:0.226521, valid loss:0.232399
Epoch:52, Train loss:0.226517, valid loss:0.232396
Epoch:53, Train loss:0.226516, valid loss:0.232395
Epoch:54, Train loss:0.226515, valid loss:0.232394
Epoch:55, Train loss:0.226515, valid loss:0.232394
Epoch:56, Train loss:0.226514, valid loss:0.232393
Epoch:57, Train loss:0.226514, valid loss:0.232393
Epoch:58, Train loss:0.226513, valid loss:0.232393
Epoch:59, Train loss:0.226513, valid loss:0.232394
Epoch:60, Train loss:0.226513, valid loss:0.232393
training time 20765.98525452614
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.8855756727234547
plot_id,batch_id 0 1 miss% 0.8538620914282588
plot_id,batch_id 0 2 miss% 0.8488346867746093
plot_id,batch_id 0 3 miss% 0.8459954589583931
plot_id,batch_id 0 4 miss% 0.8446328066862733
plot_id,batch_id 0 5 miss% 0.8913375580982776
plot_id,batch_id 0 6 miss% 0.8568288288683203
plot_id,batch_id 0 7 miss% 0.8500950955040516
plot_id,batch_id 0 8 miss% 0.8463510988258551
plot_id,batch_id 0 9 miss% 0.8452578725078265
plot_id,batch_id 0 10 miss% 0.8892127069728256
plot_id,batch_id 0 11 miss% 0.8595376991782102
plot_id,batch_id 0 12 miss% 0.8502936074174487
plot_id,batch_id 0 13 miss% 0.8459907252814194
plot_id,batch_id 0 14 miss% 0.8466707721951131
plot_id,batch_id 0 15 miss% 0.8798545759894265
plot_id,batch_id 0 16 miss% 0.8571598825067067
plot_id,batch_id 0 17 miss% 0.8543222422855345
plot_id,batch_id 0 18 miss% 0.848028498763744
plot_id,batch_id 0 19 miss% 0.8486583433038428
plot_id,batch_id 0 20 miss% 0.8738525140194078
plot_id,batch_id 0 21 miss% 0.8495761117822735
plot_id,batch_id 0 22 miss% 0.8453925981238578
plot_id,batch_id 0 23 miss% 0.8441881766749381
plot_id,batch_id 0 24 miss% 0.8427970156660788
plot_id,batch_id 0 25 miss% 0.8686920039559036
plot_id,batch_id 0 26 miss% 0.8516859643994931
plot_id,batch_id 0 27 miss% 0.844639815437238
plot_id,batch_id 0 28 miss% 0.8436145824541607
plot_id,batch_id 0 29 miss% 0.8434414086563843
plot_id,batch_id 0 30 miss% 0.8745426150336054
plot_id,batch_id 0 31 miss% 0.8497654884935537
plot_id,batch_id 0 32 miss% 0.8465702360555707
plot_id,batch_id 0 33 miss% 0.8441198086886018
plot_id,batch_id 0 34 miss% 0.8425608334531988
plot_id,batch_id 0 35 miss% 0.8742737963065147
plot_id,batch_id 0 36 miss% 0.8502609427574185
plot_id,batch_id 0 37 miss% 0.8463910697607439
plot_id,batch_id 0 38 miss% 0.8450951927390687
plot_id,batch_id 0 39 miss% 0.8417536881426424
plot_id,batch_id 0 40 miss% 0.8644741356264029
plot_id,batch_id 0 41 miss% 0.8489801968119864
plot_id,batch_id 0 42 miss% 0.8424269730787483
plot_id,batch_id 0 43 miss% 0.8453479321461828
plot_id,batch_id 0 44 miss% 0.8407796405034091
plot_id,batch_id 0 45 miss% 0.8570480401957749
plot_id,batch_id 0 46 miss% 0.8478599539140832
plot_id,batch_id 0 47 miss% 0.844274400734026
plot_id,batch_id 0 48 miss% 0.8434426936202412
plot_id,batch_id 0 49 miss% 0.8419945235503672
plot_id,batch_id 0 50 miss% 0.8562823497067324
plot_id,batch_id 0 51 miss% 0.845630100643134
plot_id,batch_id 0 52 miss% 0.8455013983588942
plot_id,batch_id 0 53 miss% 0.8413412089143883
plot_id,batch_id 0 54 miss% 0.8418470968841923
plot_id,batch_id 0 55 miss% 0.8558346157876394
plot_id,batch_id 0 56 miss% 0.8467342876557724
plot_id,batch_id 0 57 miss% 0.8442588068612101
plot_id,batch_id 0 58 miss% 0.8433427455485025
plot_id,batch_id 0 59 miss% 0.8433084855885044
plot_id,batch_id 0 60 miss% 0.906640092085138
plot_id,batch_id 0 61 miss% 0.8729865962967658
plot_id,batch_id 0 62 miss% 0.8610243613671533
plot_id,batch_id 0 63 miss% 0.8540811990581305
plot_id,batch_id 0 64 miss% 0.8547433746254883
plot_id,batch_id 0 65 miss% 0.9071890671031625
plot_id,batch_id 0 66 miss% 0.8764386877735874
plot_id,batch_id 0 67 miss% 0.8653386450145525
plot_id,batch_id 0 68 miss% 0.8556157559243243
plot_id,batch_id 0 69 miss% 0.8513869649224928
plot_id,batch_id 0 70 miss% 0.9088131575102919
plot_id,batch_id 0 71 miss% 0.8773056282840213
plot_id,batch_id 0 72 miss% 0.8641750177783192
plot_id,batch_id 0 73 miss% 0.858918774187263
plot_id,batch_id 0 74 miss% 0.855058636040308
plot_id,batch_id 0 75 miss% 0.9110949361022291
plot_id,batch_id 0 76 miss% 0.8776821766282763
plot_id,batch_id 0 77 miss% 0.8624675800400716
plot_id,batch_id 0 78 miss% 0.8592435621315473
plot_id,batch_id 0 79 miss% 0.8539666685716131
plot_id,batch_id 0 80 miss% 0.9038600213760652
plot_id,batch_id 0 81 miss% 0.8659738995573418
plot_id,batch_id 0 82 miss% 0.858497999896952
plot_id,batch_id 0 83 miss% 0.8503015220351016
plot_id,batch_id 0 84 miss% 0.8474352209285448
plot_id,batch_id 0 85 miss% 0.9022668136266232
plot_id,batch_id 0 86 miss% 0.8653082052892458
plot_id,batch_id 0 87 miss% 0.857093964251274
plot_id,batch_id 0 88 miss% 0.8519615627458258
plot_id,batch_id 0 89 miss% 0.850928248397268
plot_id,batch_id 0 90 miss% 0.9028655672634962
plot_id,batch_id 0 91 miss% 0.8669383772221385
plot_id,batch_id 0 92 miss% 0.8580731818305678
plot_id,batch_id 0 93 miss% 0.8533409137849305
plot_id,batch_id 0 94 miss% 0.8504387065851694
plot_id,batch_id 0 95 miss% 0.9027426786177132
plot_id,batch_id 0 96 miss% 0.8692315766155024
plot_id,batch_id 0 97 miss% 0.8596642585151454
plot_id,batch_id 0 98 miss% 0.8532384062752967
plot_id,batch_id 0 99 miss% 0.8508066533001093
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.88557567 0.85386209 0.84883469 0.84599546 0.84463281 0.89133756
 0.85682883 0.8500951  0.8463511  0.84525787 0.88921271 0.8595377
 0.85029361 0.84599073 0.84667077 0.87985458 0.85715988 0.85432224
 0.8480285  0.84865834 0.87385251 0.84957611 0.8453926  0.84418818
 0.84279702 0.868692   0.85168596 0.84463982 0.84361458 0.84344141
 0.87454262 0.84976549 0.84657024 0.84411981 0.84256083 0.8742738
 0.85026094 0.84639107 0.84509519 0.84175369 0.86447414 0.8489802
 0.84242697 0.84534793 0.84077964 0.85704804 0.84785995 0.8442744
 0.84344269 0.84199452 0.85628235 0.8456301  0.8455014  0.84134121
 0.8418471  0.85583462 0.84673429 0.84425881 0.84334275 0.84330849
 0.90664009 0.8729866  0.86102436 0.8540812  0.85474337 0.90718907
 0.87643869 0.86533865 0.85561576 0.85138696 0.90881316 0.87730563
 0.86417502 0.85891877 0.85505864 0.91109494 0.87768218 0.86246758
 0.85924356 0.85396667 0.90386002 0.8659739  0.858498   0.85030152
 0.84743522 0.90226681 0.86530821 0.85709396 0.85196156 0.85092825
 0.90286557 0.86693838 0.85807318 0.85334091 0.85043871 0.90274268
 0.86923158 0.85966426 0.85323841 0.85080665]
for model  238 the mean error 0.8586756233255949
all id 238 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 31 out win 5 err 0.8586756233255949 time 20765.98525452614
Launcher: Job 239 completed in 20887 seconds.
Launcher: Task 56 done. Exiting.
0.07391417700687257
plot_id,batch_id 0 66 miss% 0.07650381926032086
plot_id,batch_id 0 67 miss% 0.018006823919173177
plot_id,batch_id 0 68 miss% 0.027833419193121205
plot_id,batch_id 0 69 miss% 0.07659305941249468
plot_id,batch_id 0 70 miss% 0.03398713296408249
plot_id,batch_id 0 71 miss% 0.08604426974550623
plot_id,batch_id 0 72 miss% 0.10330094946043741
plot_id,batch_id 0 73 miss% 0.047014388467493805
plot_id,batch_id 0 74 miss% 0.11034176581554242
plot_id,batch_id 0 75 miss% 0.021686344368444778
plot_id,batch_id 0 76 miss% 0.04132986362383082
plot_id,batch_id 0 77 miss% 0.04271595982437394
plot_id,batch_id 0 78 miss% 0.06143863612321366
plot_id,batch_id 0 79 miss% 0.07180400792347078
plot_id,batch_id 0 80 miss% 0.04838022686143652
plot_id,batch_id 0 81 miss% 0.08825096653081707
plot_id,batch_id 0 82 miss% 0.07032027337824541
plot_id,batch_id 0 83 miss% 0.05103522551899731
plot_id,batch_id 0 84 miss% 0.0490437121490958
plot_id,batch_id 0 85 miss% 0.019812008072413675
plot_id,batch_id 0 86 miss% 0.08029181674334251
plot_id,batch_id 0 87 miss% 0.07619496213731491
plot_id,batch_id 0 88 miss% 0.08193127276857044
plot_id,batch_id 0 89 miss% 0.04429220962047761
plot_id,batch_id 0 90 miss% 0.01942920750687007
plot_id,batch_id 0 91 miss% 0.05452605468317294
plot_id,batch_id 0 92 miss% 0.050858474487714946
plot_id,batch_id 0 93 miss% 0.040039597831366174
plot_id,batch_id 0 94 miss% 0.09362778401171282
plot_id,batch_id 0 95 miss% 0.05189024464106965
plot_id,batch_id 0 96 miss% 0.10053905176873618
plot_id,batch_id 0 97 miss% 0.03897112632648672
plot_id,batch_id 0 98 miss% 0.02832713971249335
plot_id,batch_id 0 99 miss% 0.09173095763387396
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07211507 0.03082954 0.09936485 0.04694548 0.07121681 0.04853166
 0.0622395  0.07411829 0.08618822 0.02679659 0.02847318 0.04995716
 0.06746129 0.05198981 0.05844473 0.05752342 0.06718887 0.03940681
 0.08201361 0.07475501 0.10030211 0.06030731 0.04476412 0.04740053
 0.07710639 0.07167924 0.03923062 0.06004474 0.08003551 0.03586422
 0.03799876 0.09217411 0.09634751 0.06117574 0.05343154 0.04711313
 0.06302902 0.06463954 0.05373735 0.03180321 0.09732992 0.04622053
 0.04774279 0.02444049 0.05631947 0.04305159 0.04656711 0.02535999
 0.03725086 0.02598046 0.15905776 0.03989749 0.03942727 0.02341293
 0.02847402 0.04415497 0.06643063 0.05584038 0.04896036 0.03797778
 0.04796407 0.04720725 0.09011509 0.03886655 0.06443851 0.07391418
 0.07650382 0.01800682 0.02783342 0.07659306 0.03398713 0.08604427
 0.10330095 0.04701439 0.11034177 0.02168634 0.04132986 0.04271596
 0.06143864 0.07180401 0.04838023 0.08825097 0.07032027 0.05103523
 0.04904371 0.01981201 0.08029182 0.07619496 0.08193127 0.04429221
 0.01942921 0.05452605 0.05085847 0.0400396  0.09362778 0.05189024
 0.10053905 0.03897113 0.02832714 0.09173096]
for model  211 the mean error 0.05768239808250155
all id 211 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 31 out win 5 err 0.05768239808250155 time 20681.26298713684
Launcher: Job 212 completed in 20925 seconds.
Launcher: Task 98 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  265233
Epoch:0, Train loss:0.363305, valid loss:0.341393
Epoch:1, Train loss:0.230494, valid loss:0.232730
Epoch:2, Train loss:0.226373, valid loss:0.232560
Epoch:3, Train loss:0.225927, valid loss:0.232323
Epoch:4, Train loss:0.225650, valid loss:0.232225
Epoch:5, Train loss:0.225468, valid loss:0.232233
Epoch:6, Train loss:0.225368, valid loss:0.232127
Epoch:7, Train loss:0.225314, valid loss:0.232108
Epoch:8, Train loss:0.225209, valid loss:0.232195
Epoch:9, Train loss:0.225159, valid loss:0.232215
Epoch:10, Train loss:0.225111, valid loss:0.232090
Epoch:11, Train loss:0.224829, valid loss:0.231896
Epoch:12, Train loss:0.224797, valid loss:0.231899
Epoch:13, Train loss:0.224796, valid loss:0.231902
Epoch:14, Train loss:0.224766, valid loss:0.232018
Epoch:15, Train loss:0.224762, valid loss:0.231925
Epoch:16, Train loss:0.224745, valid loss:0.231943
Epoch:17, Train loss:0.224713, valid loss:0.231863
Epoch:18, Train loss:0.224707, valid loss:0.231893
Epoch:19, Train loss:0.224705, valid loss:0.231878
Epoch:20, Train loss:0.224679, valid loss:0.231952
Epoch:21, Train loss:0.224550, valid loss:0.231816
Epoch:22, Train loss:0.224541, valid loss:0.231814
Epoch:23, Train loss:0.224539, valid loss:0.231843
Epoch:24, Train loss:0.224530, valid loss:0.231872
Epoch:25, Train loss:0.224528, valid loss:0.231881
Epoch:26, Train loss:0.224513, valid loss:0.231844
Epoch:27, Train loss:0.224494, valid loss:0.231830
Epoch:28, Train loss:0.224506, valid loss:0.231814
Epoch:29, Train loss:0.224500, valid loss:0.231860
Epoch:30, Train loss:0.224494, valid loss:0.231832
Epoch:31, Train loss:0.224429, valid loss:0.231811
Epoch:32, Train loss:0.224419, valid loss:0.231829
Epoch:33, Train loss:0.224415, valid loss:0.231793
Epoch:34, Train loss:0.224417, valid loss:0.231831
Epoch:35, Train loss:0.224414, valid loss:0.231796
Epoch:36, Train loss:0.224401, valid loss:0.231809
Epoch:37, Train loss:0.224410, valid loss:0.231794
Epoch:38, Train loss:0.224408, valid loss:0.231827
Epoch:39, Train loss:0.224402, valid loss:0.231811
Epoch:40, Train loss:0.224402, valid loss:0.231846
Epoch:41, Train loss:0.224370, valid loss:0.231795
Epoch:42, Train loss:0.224366, valid loss:0.231803
Epoch:43, Train loss:0.224367, valid loss:0.231800
Epoch:44, Train loss:0.224367, valid loss:0.231826
Epoch:45, Train loss:0.224363, valid loss:0.231792
Epoch:46, Train loss:0.224362, valid loss:0.231798
Epoch:47, Train loss:0.224365, valid loss:0.231796
Epoch:48, Train loss:0.224359, valid loss:0.231821
Epoch:49, Train loss:0.224356, valid loss:0.231801
Epoch:50, Train loss:0.224356, valid loss:0.231835
Epoch:51, Train loss:0.224351, valid loss:0.231812
Epoch:52, Train loss:0.224349, valid loss:0.231808
Epoch:53, Train loss:0.224348, valid loss:0.231807
Epoch:54, Train loss:0.224347, valid loss:0.231806
Epoch:55, Train loss:0.224347, valid loss:0.231809
Epoch:56, Train loss:0.224346, valid loss:0.231805
Epoch:57, Train loss:0.224346, valid loss:0.231804
Epoch:58, Train loss:0.224345, valid loss:0.231806
Epoch:59, Train loss:0.224345, valid loss:0.231803
Epoch:60, Train loss:0.224345, valid loss:0.231803
training time 20929.441180467606
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.958387000770816
plot_id,batch_id 0 1 miss% 0.9648191822939148
plot_id,batch_id 0 2 miss% 0.9664212072181008
plot_id,batch_id 0 3 miss% 0.9685115247920394
plot_id,batch_id 0 4 miss% 0.9663562371165729
plot_id,batch_id 0 5 miss% 0.9596021577171526
plot_id,batch_id 0 6 miss% 0.9631396694180544
plot_id,batch_id 0 7 miss% 0.966233288762209
plot_id,batch_id 0 8 miss% 0.9666369266287188
plot_id,batch_id 0 9 miss% 0.9691757337406416
plot_id,batch_id 0 10 miss% 0.9605946310397305
plot_id,batch_id 0 11 miss% 0.9600953733954624
plot_id,batch_id 0 12 miss% 0.961532454167709
plot_id,batch_id 0 13 miss% 0.9638517481373318
plot_id,batch_id 0 14 miss% 0.9673401494308349
plot_id,batch_id 0 15 miss% 0.9585112904263534
plot_id,batch_id 0 16 miss% 0.9742476447463515
plot_id,batch_id 0 17 miss% 0.962677330770429
plot_id,batch_id 0 18 miss% 0.9654102619337508
plot_id,batch_id 0 19 miss% 0.9703267445870416
plot_id,batch_id 0 20 miss% 0.9632525584760228
plot_id,batch_id 0 21 miss% 0.9668488620263103
plot_id,batch_id 0 22 miss% 0.9689020563150158
plot_id,batch_id 0 23 miss% 0.9675853189525808
plot_id,batch_id 0 24 miss% 0.9672185215794553
plot_id,batch_id 0 25 miss% 0.9712674829898799
plot_id,batch_id 0 26 miss% 0.9694565708243813
plot_id,batch_id 0 27 miss% 0.9679357596748575
plot_id,batch_id 0 28 miss% 0.9660011837172938
plot_id,batch_id 0 29 miss% 0.9680624309389967
plot_id,batch_id 0 30 miss% 0.9663398768239144
plot_id,batch_id 0 31 miss% 0.9680378775886437
plot_id,batch_id 0 32 miss% 0.9686615919595027
plot_id,batch_id 0 33 miss% 0.9698937965281554
plot_id,batch_id 0 34 miss% 0.9712092378564594
plot_id,batch_id 0 35 miss% 0.9563025371903655
plot_id,batch_id 0 36 miss% 0.9637449809738681
plot_id,batch_id 0 37 miss% 0.9701236867318049
plot_id,batch_id 0 38 miss% 0.9689168666665237
plot_id,batch_id 0 39 miss% 0.9705618670308003
plot_id,batch_id 0 40 miss% 0.96519579309062
plot_id,batch_id 0 41 miss% 0.9663646370450402
plot_id,batch_id 0 42 miss% 0.9662033492766025
plot_id,batch_id 0 43 miss% 0.967185742841575
plot_id,batch_id 0 44 miss% 0.9678293073926179
plot_id,batch_id 0 45 miss% 0.9686373061127288
plot_id,batch_id 0 46 miss% 0.9694848402260137
plot_id,batch_id 0 47 miss% 0.9683160866813839
plot_id,batch_id 0 48 miss% 0.9701866203105152
plot_id,batch_id 0 49 miss% 0.9699622025907974
plot_id,batch_id 0 50 miss% 0.9609202290389608
plot_id,batch_id 0 51 miss% 0.9647291503463594
plot_id,batch_id 0 52 miss% 0.9704755754485725
plot_id,batch_id 0 53 miss% 0.9668951553656144
plot_id,batch_id 0 54 miss% 0.9689824969798175
plot_id,batch_id 0 55 miss% 0.9654168196624363
plot_id,batch_id 0 56 miss% 0.9708510621110887
plot_id,batch_id 0 57 miss% 0.9679560182473801
plot_id,batch_id 0 58 miss% 0.9691895554378396
plot_id,batch_id 0 59 miss% 0.9685958345822334
plot_id,batch_id 0 60 miss% 0.9621507993075511
plot_id,batch_id 0 61 miss% 0.961752697067164
plot_id,batch_id 0 62 miss% 0.9674032319049691
plot_id,batch_id 0 63 miss% 0.9636695461896096
plot_id,batch_id 0 64 miss% 0.9653266936082501
plot_id,batch_id 0 65 miss% 0.9586653565766021
plot_id,batch_id 0 66 miss% 0.9657247177542293
plot_id,batch_id 0 67 miss% 0.9618918073346
plot_id,batch_id 0 68 miss% 0.964010630211553
plot_id,batch_id 0 69 miss% 0.9652275929916201
plot_id,batch_id 0 70 miss% 0.9586958186106211
plot_id,batch_id 0 71 miss% 0.9750636720577697
plot_id,batch_id 0 72 miss% 0.9596105588128243
plot_id,batch_id 0 73 miss% 0.9671484611475897
plot_id,batch_id 0 74 miss% 0.9716168720627788
plot_id,batch_id 0 75 miss% 0.9600696686231945
plot_id,batch_id 0 76 miss% 0.9611323394761961
plot_id,batch_id 0 77 miss% 0.9626635990785821
plot_id,batch_id 0 78 miss% 0.9643582484993178
plot_id,batch_id 0 79 miss% 0.9635706418699257
plot_id,batch_id 0 80 miss% 0.9608424088714015
plot_id,batch_id 0 81 miss% 0.9645714080346856
plot_id,batch_id 0 82 miss% 0.967355165459952
plot_id,batch_id 0 83 miss% 0.9667385235337842
plot_id,batch_id 0 84 miss% 0.9659245595520625
plot_id,batch_id 0 85 miss% 0.9672387148053765
plot_id,batch_id 0 86 miss% 0.9638034091493616
plot_id,batch_id 0 87 miss% 0.9642609279744662
plot_id,batch_id 0 88 miss% 0.9637416514125204
plot_id,batch_id 0 89 miss% 0.9656029910893899
plot_id,batch_id 0 90 miss% 0.9638711270118725
plot_id,batch_id 0 91 miss% 0.963854437441737
plot_id,batch_id 0 92 miss% 0.9658439640157774
plot_id,batch_id 0 93 miss% 0.9613566946066743
plot_id,batch_id 0 94 miss% 0.9647826635322531
plot_id,batch_id 0 95 miss% 0.9606557570470944
plot_id,batch_id 0 96 miss% 0.9653609498924525
plot_id,batch_id 0 97 miss% 0.9586069795863684
plot_id,batch_id 0 98 miss% 0.9665271046122511
plot_id,batch_id 0 99 miss% 0.9650399306894786
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.958387   0.96481918 0.96642121 0.96851152 0.96635624 0.95960216
 0.96313967 0.96623329 0.96663693 0.96917573 0.96059463 0.96009537
 0.96153245 0.96385175 0.96734015 0.95851129 0.97424764 0.96267733
 0.96541026 0.97032674 0.96325256 0.96684886 0.96890206 0.96758532
 0.96721852 0.97126748 0.96945657 0.96793576 0.96600118 0.96806243
 0.96633988 0.96803788 0.96866159 0.9698938  0.97120924 0.95630254
 0.96374498 0.97012369 0.96891687 0.97056187 0.96519579 0.96636464
 0.96620335 0.96718574 0.96782931 0.96863731 0.96948484 0.96831609
 0.97018662 0.9699622  0.96092023 0.96472915 0.97047558 0.96689516
 0.9689825  0.96541682 0.97085106 0.96795602 0.96918956 0.96859583
 0.9621508  0.9617527  0.96740323 0.96366955 0.96532669 0.95866536
 0.96572472 0.96189181 0.96401063 0.96522759 0.95869582 0.97506367
 0.95961056 0.96714846 0.97161687 0.96006967 0.96113234 0.9626636
 0.96435825 0.96357064 0.96084241 0.96457141 0.96735517 0.96673852
 0.96592456 0.96723871 0.96380341 0.96426093 0.96374165 0.96560299
 0.96387113 0.96385444 0.96584396 0.96135669 0.96478266 0.96065576
 0.96536095 0.95860698 0.9665271  0.96503993]
for model  213 the mean error 0.9656330372623018
all id 213 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 31 out win 4 err 0.9656330372623018 time 20929.441180467606
Launcher: Job 214 completed in 21036 seconds.
Launcher: Task 219 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  265233
Epoch:0, Train loss:0.363305, valid loss:0.341393
Epoch:1, Train loss:0.063465, valid loss:0.001768
Epoch:2, Train loss:0.003240, valid loss:0.001317
Epoch:3, Train loss:0.002374, valid loss:0.001196
Epoch:4, Train loss:0.001956, valid loss:0.001019
Epoch:5, Train loss:0.001698, valid loss:0.000852
Epoch:6, Train loss:0.001570, valid loss:0.000808
Epoch:7, Train loss:0.001400, valid loss:0.000783
Epoch:8, Train loss:0.001270, valid loss:0.000754
Epoch:9, Train loss:0.001248, valid loss:0.000933
Epoch:10, Train loss:0.001161, valid loss:0.000832
Epoch:11, Train loss:0.000843, valid loss:0.000547
Epoch:12, Train loss:0.000809, valid loss:0.000545
Epoch:13, Train loss:0.000805, valid loss:0.000526
Epoch:14, Train loss:0.000777, valid loss:0.000576
Epoch:15, Train loss:0.000756, valid loss:0.000528
Epoch:16, Train loss:0.000734, valid loss:0.000482
Epoch:17, Train loss:0.000714, valid loss:0.000484
Epoch:18, Train loss:0.000691, valid loss:0.000494
Epoch:19, Train loss:0.000672, valid loss:0.000489
Epoch:20, Train loss:0.000659, valid loss:0.000527
Epoch:21, Train loss:0.000509, valid loss:0.000414
Epoch:22, Train loss:0.000509, valid loss:0.000436
Epoch:23, Train loss:0.000491, valid loss:0.000479
Epoch:24, Train loss:0.000481, valid loss:0.000401
Epoch:25, Train loss:0.000501, valid loss:0.000515
Epoch:26, Train loss:0.000463, valid loss:0.000415
Epoch:27, Train loss:0.000467, valid loss:0.000411
Epoch:28, Train loss:0.000474, valid loss:0.000473
Epoch:29, Train loss:0.000443, valid loss:0.000629
Epoch:30, Train loss:0.000450, valid loss:0.000477
Epoch:31, Train loss:0.000378, valid loss:0.000404
Epoch:32, Train loss:0.000370, valid loss:0.000405
Epoch:33, Train loss:0.000368, valid loss:0.000387
Epoch:34, Train loss:0.000368, valid loss:0.000392
Epoch:35, Train loss:0.000371, valid loss:0.000377
Epoch:36, Train loss:0.000354, valid loss:0.000371
Epoch:37, Train loss:0.000357, valid loss:0.000398
Epoch:38, Train loss:0.000352, valid loss:0.000387
Epoch:39, Train loss:0.000350, valid loss:0.000411
Epoch:40, Train loss:0.000344, valid loss:0.000388
Epoch:41, Train loss:0.000315, valid loss:0.000357
Epoch:42, Train loss:0.000310, valid loss:0.000369
Epoch:43, Train loss:0.000308, valid loss:0.000359
Epoch:44, Train loss:0.000305, valid loss:0.000363
Epoch:45, Train loss:0.000305, valid loss:0.000366
Epoch:46, Train loss:0.000307, valid loss:0.000362
Epoch:47, Train loss:0.000304, valid loss:0.000380
Epoch:48, Train loss:0.000298, valid loss:0.000398
Epoch:49, Train loss:0.000301, valid loss:0.000381
Epoch:50, Train loss:0.000297, valid loss:0.000396
Epoch:51, Train loss:0.000284, valid loss:0.000400
Epoch:52, Train loss:0.000282, valid loss:0.000364
Epoch:53, Train loss:0.000280, valid loss:0.000378
Epoch:54, Train loss:0.000279, valid loss:0.000364
Epoch:55, Train loss:0.000279, valid loss:0.000368
Epoch:56, Train loss:0.000278, valid loss:0.000362
Epoch:57, Train loss:0.000278, valid loss:0.000369
Epoch:58, Train loss:0.000278, valid loss:0.000373
Epoch:59, Train loss:0.000278, valid loss:0.000364
Epoch:60, Train loss:0.000277, valid loss:0.000367
training time 21250.487818479538
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.1733446576106053
plot_id,batch_id 0 1 miss% 0.2967206111651267
plot_id,batch_id 0 2 miss% 0.3572239522220021
plot_id,batch_id 0 3 miss% 0.2927455673770289
plot_id,batch_id 0 4 miss% 0.26830924549268914
plot_id,batch_id 0 5 miss% 0.21996489977244632
plot_id,batch_id 0 6 miss% 0.2895138216142406
plot_id,batch_id 0 7 miss% 0.40412808375330594
plot_id,batch_id 0 8 miss% 0.5312665205191719
plot_id,batch_id 0 9 miss% 0.39101125714271306
plot_id,batch_id 0 10 miss% 0.1498388134612136
plot_id,batch_id 0 11 miss% 0.28180270578448596
plot_id,batch_id 0 12 miss% 0.27395110579115584
plot_id,batch_id 0 13 miss% 0.25150196613861325
plot_id,batch_id 0 14 miss% 0.40386634391705767
plot_id,batch_id 0 15 miss% 0.1902589464333831
plot_id,batch_id 0 16 miss% 0.40843903428232214
plot_id,batch_id 0 17 miss% 0.32943118474507277
plot_id,batch_id 0 18 miss% 0.39046845953902204
plot_id,batch_id 0 19 miss% 0.35308957176778644
plot_id,batch_id 0 20 miss% 0.2339743275822757
plot_id,batch_id 0 21 miss% 0.344074483840727
plot_id,batch_id 0 22 miss% 0.35737281829736434
plot_id,batch_id 0 23 miss% 0.33912705379423663
plot_id,batch_id 0 24 miss% 0.2634207135560022
plot_id,batch_id 0 25 miss% 0.21868627597969714
plot_id,batch_id 0 26 miss% 0.3179343474335186
plot_id,batch_id 0 27 miss% 0.3245507400918177
plot_id,batch_id 0 28 miss% 0.356789834082952
plot_id,batch_id 0 29 miss% 0.37360567709134673
plot_id,batch_id 0 30 miss% 0.1920474017408192
plot_id,batch_id 0 31 miss% 0.34646929566895385
plot_id,batch_id 0 32 miss% 0.38145504217645015
plot_id,batch_id 0 33 miss% 0.4446387971826535
plot_id,batch_id 0 34 miss% 0.34944719073800223
plot_id,batch_id 0 35 miss% 0.19755364981458512
plot_id,batch_id 0 36 miss% 0.44408533183217896
plot_id,batch_id 0 37 miss% 0.3733699847262866
plot_id,batch_id 0 38 miss% 0.4140479679270062
plot_id,batch_id 0 39 miss% 0.4469070234891343
plot_id,batch_id 0 40 miss% 0.19628529839422515
plot_id,batch_id 0 41 miss% 0.2541990761049863
plot_id,batch_id 0 42 miss% 0.2672228242230613
plot_id,batch_id 0 43 miss% 0.21195082687767275
plot_id,batch_id 0 44 miss% 0.20654132623779803
plot_id,batch_id 0 45 miss% 0.29311489993606427
plot_id,batch_id 0 46 miss% 0.3434600342829318
plot_id,batch_id 0 47 miss% 0.3599510731988276
plot_id,batch_id 0 48 miss% 0.3126479270067509
plot_id,batch_id 0 49 miss% 0.2183588552924956
plot_id,batch_id 0 50 miss% 0.48495808876083435
plot_id,batch_id 0 51 miss% 0.38908107077105547
plot_id,batch_id 0 52 miss% 0.41071806145300604
plot_id,batch_id 0 53 miss% 0.3160293053035602
plot_id,batch_id 0 54 miss% 0.2739609336186491
plot_id,batch_id 0 55 miss% 0.3440948395903957
plot_id,batch_id 0 56 miss% 0.42956106210201844
plot_id,batch_id 0 57 miss% 0.4076416442830738
plot_id,batch_id 0 58 miss% 0.36198523584788467
plot_id,batch_id 0 59 miss% 0.44607162760401214
plot_id,batch_id 0 60 miss% 0.1302113807435289
plot_id,batch_id 0 61 miss% 0.17609522729407098
plot_id,batch_id 0 62 miss% 0.24760288171488276
plot_id,batch_id 0 63 miss% 0.23331418676531387
plot_id,batch_id 0 64 miss% 0.30534432991804317
plot_id,batch_id 0 65 miss% 0.16429277790723548
plot_id,batch_id 0 66 miss% 0.2584130833120139
plot_id,batch_id 0 67 miss% 0.1891801168847934
plot_id,batch_id 0 68 miss% 0.28657508437483054
plot_id,batch_id 0 69 miss% 0.2858409572192805
plot_id,batch_id 0 70 miss% 0.1592374249607456
plot_id,batch_id 0 71 miss% 0.20292818199784607
plot_id,batch_id 0 72 miss% 0.3025632897127812
plot_id,batch_id 0 73 miss% 0.20005465452277224
plot_id,batch_id 0 74 miss% 0.2636257432201235
plot_id,batch_id 0 75 miss% 0.11289825669129887
plot_id,batch_id 0 76 miss% 0.21025887034410862
plot_id,batch_id 0 77 miss% 0.16494812597430342
plot_id,batch_id 0 78 miss% 0.20011136428052573
plot_id,batch_id 0 79 miss% 0.2749206264992022
plot_id,batch_id 0 80 miss% 0.16585086056806905
plot_id,batch_id 0 81 miss% 0.2882265740800839
plot_id,batch_id 0 82 miss% 0.246032532014552
plot_id,batch_id 0 83 miss% 0.2829517007310448
plot_id,batch_id 0 84 miss% 0.2596497168808821
plot_id,batch_id 0 85 miss% 0.15449623434335685
plot_id,batch_id 0 86 miss% 0.2287367626793034
plot_id,batch_id 0 87 miss% 0.29978895076671125
plot_id,batch_id 0 88 miss% 0.29109028126011277
plot_id,batch_id 0 89 miss% 0.2865012441456313
plot_id,batch_id 0 90 miss% 0.1383875531919219
plot_id,batch_id 0 91 miss% 0.20102605260201278
plot_id,batch_id 0 92 miss% 0.21708690451819176
plot_id,batch_id 0 93 miss% 0.20882464151710722
plot_id,batch_id 0 94 miss% 0.3317834540984239
plot_id,batch_id 0 95 miss% 0.15861110153936678
plot_id,batch_id 0 96 miss% 0.1844092830294676
plot_id,batch_id 0 97 miss% 0.26422857526515653
plot_id,batch_id 0 98 miss% 0.2406347768213057
plot_id,batch_id 0 99 miss% 0.2794180413034532
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.17334466 0.29672061 0.35722395 0.29274557 0.26830925 0.2199649
 0.28951382 0.40412808 0.53126652 0.39101126 0.14983881 0.28180271
 0.27395111 0.25150197 0.40386634 0.19025895 0.40843903 0.32943118
 0.39046846 0.35308957 0.23397433 0.34407448 0.35737282 0.33912705
 0.26342071 0.21868628 0.31793435 0.32455074 0.35678983 0.37360568
 0.1920474  0.3464693  0.38145504 0.4446388  0.34944719 0.19755365
 0.44408533 0.37336998 0.41404797 0.44690702 0.1962853  0.25419908
 0.26722282 0.21195083 0.20654133 0.2931149  0.34346003 0.35995107
 0.31264793 0.21835886 0.48495809 0.38908107 0.41071806 0.31602931
 0.27396093 0.34409484 0.42956106 0.40764164 0.36198524 0.44607163
 0.13021138 0.17609523 0.24760288 0.23331419 0.30534433 0.16429278
 0.25841308 0.18918012 0.28657508 0.28584096 0.15923742 0.20292818
 0.30256329 0.20005465 0.26362574 0.11289826 0.21025887 0.16494813
 0.20011136 0.27492063 0.16585086 0.28822657 0.24603253 0.2829517
 0.25964972 0.15449623 0.22873676 0.29978895 0.29109028 0.28650124
 0.13838755 0.20102605 0.2170869  0.20882464 0.33178345 0.1586111
 0.18440928 0.26422858 0.24063478 0.27941804]
for model  186 the mean error 0.2860042052215861
all id 186 hidden_dim 32 learning_rate 0.0025 num_layers 5 frames 31 out win 4 err 0.2860042052215861 time 21250.487818479538
Launcher: Job 187 completed in 21478 seconds.
Launcher: Task 203 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  151697
Epoch:0, Train loss:0.362207, valid loss:0.326726
Epoch:1, Train loss:0.236670, valid loss:0.234015
Epoch:2, Train loss:0.230598, valid loss:0.233539
Epoch:3, Train loss:0.230090, valid loss:0.233404
Epoch:4, Train loss:0.229794, valid loss:0.233364
Epoch:5, Train loss:0.229576, valid loss:0.233218
Epoch:6, Train loss:0.229360, valid loss:0.233205
Epoch:7, Train loss:0.229249, valid loss:0.233086
Epoch:8, Train loss:0.229196, valid loss:0.233097
Epoch:9, Train loss:0.229125, valid loss:0.233125
Epoch:10, Train loss:0.229053, valid loss:0.233124
Epoch:11, Train loss:0.228749, valid loss:0.232963
Epoch:12, Train loss:0.228688, valid loss:0.232936
Epoch:13, Train loss:0.228657, valid loss:0.232940
Epoch:14, Train loss:0.228648, valid loss:0.232910
Epoch:15, Train loss:0.228630, valid loss:0.232890
Epoch:16, Train loss:0.228605, valid loss:0.232952
Epoch:17, Train loss:0.228588, valid loss:0.232896
Epoch:18, Train loss:0.228572, valid loss:0.232907
Epoch:19, Train loss:0.228527, valid loss:0.232986
Epoch:20, Train loss:0.228533, valid loss:0.232906
Epoch:21, Train loss:0.228368, valid loss:0.232858
Epoch:22, Train loss:0.228365, valid loss:0.232858
Epoch:23, Train loss:0.228337, valid loss:0.232875
Epoch:24, Train loss:0.228363, valid loss:0.232920
Epoch:25, Train loss:0.228324, valid loss:0.232873
Epoch:26, Train loss:0.228322, valid loss:0.232844
Epoch:27, Train loss:0.228320, valid loss:0.232851
Epoch:28, Train loss:0.228317, valid loss:0.232877
Epoch:29, Train loss:0.228296, valid loss:0.232842
Epoch:30, Train loss:0.228296, valid loss:0.232954
Epoch:31, Train loss:0.228222, valid loss:0.232806
Epoch:32, Train loss:0.228206, valid loss:0.232818
Epoch:33, Train loss:0.228209, valid loss:0.232814
Epoch:34, Train loss:0.228204, valid loss:0.232836
Epoch:35, Train loss:0.228204, valid loss:0.232825
Epoch:36, Train loss:0.228205, valid loss:0.232825
Epoch:37, Train loss:0.228204, valid loss:0.232833
Epoch:38, Train loss:0.228196, valid loss:0.232824
Epoch:39, Train loss:0.228182, valid loss:0.232815
Epoch:40, Train loss:0.228184, valid loss:0.232844
Epoch:41, Train loss:0.228150, valid loss:0.232824
Epoch:42, Train loss:0.228145, valid loss:0.232830
Epoch:43, Train loss:0.228147, valid loss:0.232840
Epoch:44, Train loss:0.228143, valid loss:0.232824
Epoch:45, Train loss:0.228142, valid loss:0.232813
Epoch:46, Train loss:0.228138, valid loss:0.232843
Epoch:47, Train loss:0.228137, valid loss:0.232825
Epoch:48, Train loss:0.228135, valid loss:0.232823
Epoch:49, Train loss:0.228136, valid loss:0.232840
Epoch:50, Train loss:0.228130, valid loss:0.232840
Epoch:51, Train loss:0.228115, valid loss:0.232839
Epoch:52, Train loss:0.228113, valid loss:0.232840
Epoch:53, Train loss:0.228112, valid loss:0.232843
Epoch:54, Train loss:0.228112, valid loss:0.232837
Epoch:55, Train loss:0.228111, valid loss:0.232839
Epoch:56, Train loss:0.228111, valid loss:0.232837
Epoch:57, Train loss:0.228111, valid loss:0.232837
Epoch:58, Train loss:0.228110, valid loss:0.232837
Epoch:59, Train loss:0.228110, valid loss:0.232836
Epoch:60, Train loss:0.228110, valid loss:0.232835
training time 21425.25596523285
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.8552102042251504
plot_id,batch_id 0 1 miss% 0.8216359011136124
plot_id,batch_id 0 2 miss% 0.8178235349168285
plot_id,batch_id 0 3 miss% 0.8161594725406818
plot_id,batch_id 0 4 miss% 0.8142291243640087
plot_id,batch_id 0 5 miss% 0.8583650787977135
plot_id,batch_id 0 6 miss% 0.8226498963366573
plot_id,batch_id 0 7 miss% 0.819153047001192
plot_id,batch_id 0 8 miss% 0.8161637670356364
plot_id,batch_id 0 9 miss% 0.8139528224738909
plot_id,batch_id 0 10 miss% 0.855501905766895
plot_id,batch_id 0 11 miss% 0.8255762068464035
plot_id,batch_id 0 12 miss% 0.8189720295853694
plot_id,batch_id 0 13 miss% 0.813190041593229
plot_id,batch_id 0 14 miss% 0.8147049629542055
plot_id,batch_id 0 15 miss% 0.8549713696177889
plot_id,batch_id 0 16 miss% 0.8303866588366131
plot_id,batch_id 0 17 miss% 0.8199028233495218
plot_id,batch_id 0 18 miss% 0.8193399609487088
plot_id,batch_id 0 19 miss% 0.8179239915629206
plot_id,batch_id 0 20 miss% 0.8400405726347093
plot_id,batch_id 0 21 miss% 0.8158926111103273
plot_id,batch_id 0 22 miss% 0.8162233931095209
plot_id,batch_id 0 23 miss% 0.8111907523459102
plot_id,batch_id 0 24 miss% 0.8108920210026785
plot_id,batch_id 0 25 miss% 0.8374525903524775
plot_id,batch_id 0 26 miss% 0.8169254682773357
plot_id,batch_id 0 27 miss% 0.8155409446151605
plot_id,batch_id 0 28 miss% 0.8113898136355077
plot_id,batch_id 0 29 miss% 0.8091039791983948
plot_id,batch_id 0 30 miss% 0.8439065135861377
plot_id,batch_id 0 31 miss% 0.8182384246669081
plot_id,batch_id 0 32 miss% 0.8161252549333351
plot_id,batch_id 0 33 miss% 0.8166337589621178
plot_id,batch_id 0 34 miss% 0.8111414792976495
plot_id,batch_id 0 35 miss% 0.838230463707308
plot_id,batch_id 0 36 miss% 0.8193716496662333
plot_id,batch_id 0 37 miss% 0.8208503061624913
plot_id,batch_id 0 38 miss% 0.8138612284225945
plot_id,batch_id 0 39 miss% 0.8118517879145639
plot_id,batch_id 0 40 miss% 0.850308828698809
plot_id,batch_id 0 41 miss% 0.814190615967038
plot_id,batch_id 0 42 miss% 0.8105636926886169
plot_id,batch_id 0 43 miss% 0.8122668521109068
plot_id,batch_id 0 44 miss% 0.806328020853289
plot_id,batch_id 0 45 miss% 0.8229093878938978
plot_id,batch_id 0 46 miss% 0.8152097376926797
plot_id,batch_id 0 47 miss% 0.8117556780194078
plot_id,batch_id 0 48 miss% 0.8115662970767638
plot_id,batch_id 0 49 miss% 0.8053550885403226
plot_id,batch_id 0 50 miss% 0.821321323211161
plot_id,batch_id 0 51 miss% 0.8149969102893965
plot_id,batch_id 0 52 miss% 0.8123509983709533
plot_id,batch_id 0 53 miss% 0.8071355463132491
plot_id,batch_id 0 54 miss% 0.8100471098947365
plot_id,batch_id 0 55 miss% 0.8216899600953382
plot_id,batch_id 0 56 miss% 0.8170814338077664
plot_id,batch_id 0 57 miss% 0.8167188392528
plot_id,batch_id 0 58 miss% 0.8113144238520633
plot_id,batch_id 0 59 miss% 0.8083311019834544
plot_id,batch_id 0 60 miss% 0.886608570396833
plot_id,batch_id 0 61 miss% 0.8402467522306062
plot_id,batch_id 0 62 miss% 0.8276747792820137
plot_id,batch_id 0 63 miss% 0.8220587932456214
plot_id,batch_id 0 64 miss% 0.8188118324804173
plot_id,batch_id 0 65 miss% 0.8903970204993575
plot_id,batch_id 0 66 miss% 0.8488619211720739
plot_id,batch_id 0 67 miss% 0.8329614128419472
plot_id,batch_id 0 68 miss% 0.8216336051000305
plot_id,batch_id 0 69 miss% 0.818885873440799
plot_id,batch_id 0 70 miss% 0.8941624267388447
plot_id,batch_id 0 71 miss% 0.8579286790039611
plot_id,batch_id 0 72 miss% 0.8342324267945118
plot_id,batch_id 0 73 miss% 0.8346864528006703
plot_id,batch_id 0 74 miss% 0.820143307591703
plot_id,batch_id 0 75 miss% 0.8759862622949329
plot_id,batch_id 0 76 miss% 0.8428630452880412
plot_id,batch_id 0 77 miss% 0.8320698026863936
plot_id,batch_id 0 78 miss% 0.8218911108493367
plot_id,batch_id 0 79 miss% 0.8183049092736412
plot_id,batch_id 0 80 miss% 0.8754649744043884
plot_id,batch_id 0 81 miss% 0.831725455658666
plot_id,batch_id 0 82 miss% 0.8220471629111669
plot_id,batch_id 0 83 miss% 0.8171700988490002
plot_id,batch_id 0 84 miss% 0.814241212415526
plot_id,batch_id 0 85 miss% 0.8716265050375658
plot_id,batch_id 0 86 miss% 0.8321495062482749
plot_id,batch_id 0 87 miss% 0.8235291946153954
plot_id,batch_id 0 88 miss% 0.8192826200384503
plot_id,batch_id 0 89 miss% 0.8169761757411751
plot_id,batch_id 0 90 miss% 0.9019185890161735
plot_id,batch_id 0 91 miss% 0.835614728948134
plot_id,batch_id 0 92 miss% 0.8344106842374958
plot_id,batch_id 0 93 miss% 0.8238864709195887
plot_id,batch_id 0 94 miss% 0.8186929323017891
plot_id,batch_id 0 95 miss% 0.8886331514556334
plot_id,batch_id 0 96 miss% 0.850529475885794
plot_id,batch_id 0 97 miss% 0.8268500353617412
plot_id,batch_id 0 98 miss% 0.8229806272689001
plot_id,batch_id 0 99 miss% 0.820151223862321
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.8552102  0.8216359  0.81782353 0.81615947 0.81422912 0.85836508
 0.8226499  0.81915305 0.81616377 0.81395282 0.85550191 0.82557621
 0.81897203 0.81319004 0.81470496 0.85497137 0.83038666 0.81990282
 0.81933996 0.81792399 0.84004057 0.81589261 0.81622339 0.81119075
 0.81089202 0.83745259 0.81692547 0.81554094 0.81138981 0.80910398
 0.84390651 0.81823842 0.81612525 0.81663376 0.81114148 0.83823046
 0.81937165 0.82085031 0.81386123 0.81185179 0.85030883 0.81419062
 0.81056369 0.81226685 0.80632802 0.82290939 0.81520974 0.81175568
 0.8115663  0.80535509 0.82132132 0.81499691 0.812351   0.80713555
 0.81004711 0.82168996 0.81708143 0.81671884 0.81131442 0.8083311
 0.88660857 0.84024675 0.82767478 0.82205879 0.81881183 0.89039702
 0.84886192 0.83296141 0.82163361 0.81888587 0.89416243 0.85792868
 0.83423243 0.83468645 0.82014331 0.87598626 0.84286305 0.8320698
 0.82189111 0.81830491 0.87546497 0.83172546 0.82204716 0.8171701
 0.81424121 0.87162651 0.83214951 0.82352919 0.81928262 0.81697618
 0.90191859 0.83561473 0.83441068 0.82388647 0.81869293 0.88863315
 0.85052948 0.82685004 0.82298063 0.82015122]
for model  212 the mean error 0.8281040746926996
all id 212 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 31 out win 6 err 0.8281040746926996 time 21425.25596523285
Launcher: Job 213 completed in 21551 seconds.
Launcher: Task 120 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 33600
total number of trained parameters  265233
Epoch:0, Train loss:0.479342, valid loss:0.452080
Epoch:1, Train loss:0.392494, valid loss:0.227098
Epoch:2, Train loss:0.063346, valid loss:0.011807
Epoch:3, Train loss:0.026735, valid loss:0.011202
Epoch:4, Train loss:0.024893, valid loss:0.009379
Epoch:5, Train loss:0.022983, valid loss:0.008401
Epoch:6, Train loss:0.021468, valid loss:0.008062
Epoch:7, Train loss:0.020493, valid loss:0.006979
Epoch:8, Train loss:0.019120, valid loss:0.008082
Epoch:9, Train loss:0.017421, valid loss:0.005568
Epoch:10, Train loss:0.015316, valid loss:0.004845
Epoch:11, Train loss:0.013794, valid loss:0.004406
Epoch:12, Train loss:0.013585, valid loss:0.004285
Epoch:13, Train loss:0.012839, valid loss:0.004169
Epoch:14, Train loss:0.011950, valid loss:0.003920
Epoch:15, Train loss:0.011643, valid loss:0.004324
Epoch:16, Train loss:0.011439, valid loss:0.003907
Epoch:17, Train loss:0.011287, valid loss:0.003685
Epoch:18, Train loss:0.011051, valid loss:0.003528
Epoch:19, Train loss:0.009968, valid loss:0.003145
Epoch:20, Train loss:0.008999, valid loss:0.003479
Epoch:21, Train loss:0.007364, valid loss:0.002921
Epoch:22, Train loss:0.007166, valid loss:0.002907
Epoch:23, Train loss:0.006175, valid loss:0.001943
Epoch:24, Train loss:0.004176, valid loss:0.001840
Epoch:25, Train loss:0.003746, valid loss:0.001786
Epoch:26, Train loss:0.003609, valid loss:0.001869
Epoch:27, Train loss:0.003476, valid loss:0.001847
Epoch:28, Train loss:0.003349, valid loss:0.001506
Epoch:29, Train loss:0.003238, valid loss:0.001649
Epoch:30, Train loss:0.003131, valid loss:0.001582
Epoch:31, Train loss:0.002781, valid loss:0.001491
Epoch:32, Train loss:0.002677, valid loss:0.001399
Epoch:33, Train loss:0.002612, valid loss:0.001408
Epoch:34, Train loss:0.002554, valid loss:0.001327
Epoch:35, Train loss:0.002508, valid loss:0.001346
Epoch:36, Train loss:0.002457, valid loss:0.001357
Epoch:37, Train loss:0.002387, valid loss:0.001299
Epoch:38, Train loss:0.002335, valid loss:0.001449
Epoch:39, Train loss:0.002301, valid loss:0.001293
Epoch:40, Train loss:0.002237, valid loss:0.001262
Epoch:41, Train loss:0.002048, valid loss:0.001221
Epoch:42, Train loss:0.002010, valid loss:0.001176
Epoch:43, Train loss:0.001990, valid loss:0.001193
Epoch:44, Train loss:0.001952, valid loss:0.001195
Epoch:45, Train loss:0.001913, valid loss:0.001173
Epoch:46, Train loss:0.001887, valid loss:0.001194
Epoch:47, Train loss:0.001859, valid loss:0.001196
Epoch:48, Train loss:0.001840, valid loss:0.001188
Epoch:49, Train loss:0.001810, valid loss:0.001197
Epoch:50, Train loss:0.001789, valid loss:0.001155
Epoch:51, Train loss:0.001671, valid loss:0.001124
Epoch:52, Train loss:0.001655, valid loss:0.001114
Epoch:53, Train loss:0.001648, valid loss:0.001118
Epoch:54, Train loss:0.001645, valid loss:0.001110
Epoch:55, Train loss:0.001641, valid loss:0.001113
Epoch:56, Train loss:0.001639, valid loss:0.001107
Epoch:57, Train loss:0.001638, valid loss:0.001105
Epoch:58, Train loss:0.001636, valid loss:0.001105
Epoch:59, Train loss:0.001635, valid loss:0.001116
Epoch:60, Train loss:0.001635, valid loss:0.001104
training time 21531.648034334183
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.25
nondim time 0.5
nondim time 0.75
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.27889366303629953
plot_id,batch_id 0 1 miss% 0.26648759558244955
plot_id,batch_id 0 2 miss% 0.41791272949195357
plot_id,batch_id 0 3 miss% 0.336678884261672
plot_id,batch_id 0 4 miss% 0.3257179076794159
plot_id,batch_id 0 5 miss% 0.28168286290550243
plot_id,batch_id 0 6 miss% 0.3725446427311586
plot_id,batch_id 0 7 miss% 0.48728418955430347
plot_id,batch_id 0 8 miss% 0.39019606744925156
plot_id,batch_id 0 9 miss% 0.31881686500528206
plot_id,batch_id 0 10 miss% 0.3573614332518243
plot_id,batch_id 0 11 miss% 0.39024498532698726
plot_id,batch_id 0 12 miss% 0.338407737553484
plot_id,batch_id 0 13 miss% 0.4045230622533953
plot_id,batch_id 0 14 miss% 0.41292749000205914
plot_id,batch_id 0 15 miss% 0.3377648579172925
plot_id,batch_id 0 16 miss% 0.45168402308910055
plot_id,batch_id 0 17 miss% 0.5020646606085604
plot_id,batch_id 0 18 miss% 0.362522863129956
plot_id,batch_id 0 19 miss% 0.36336920158478414
plot_id,batch_id 0 20 miss% 0.29011352616785535
plot_id,batch_id 0 21 miss% 0.27359601021609825
plot_id,batch_id 0 22 miss% 0.3399539854659511
plot_id,batch_id 0 23 miss% 0.25253648664302264
plot_id,batch_id 0 24 miss% 0.34510101697477263
plot_id,batch_id 0 25 miss% 0.355434392633931
plot_id,batch_id 0 26 miss% 0.3192551636725753
plot_id,batch_id 0 27 miss% 0.31543197214403373
plot_id,batch_id 0 28 miss% 0.4762368948763691
plot_id,batch_id 0 29 miss% 0.31561130677924887
plot_id,batch_id 0 30 miss% 0.33177166174825007
plot_id,batch_id 0 31 miss% 0.32907444691062804
plot_id,batch_id 0 32 miss% 0.41592603452913646
plot_id,batch_id 0 33 miss% 0.34420217930832925
plot_id,batch_id 0 34 miss% 0.40366838549973677
plot_id,batch_id 0 35 miss% 0.33184269027913876
plot_id,batch_id 0 36 miss% 0.34524310940925584
plot_id,batch_id 0 37 miss% 0.34832458599782135
plot_id,batch_id 0 38 miss% 0.26795790208154646
plot_id,batch_id 0 39 miss% 0.35308310350934424
plot_id,batch_id 0 40 miss% 0.3217509382271021
plot_id,batch_id 0 41 miss% 0.29007935641428245
plot_id,batch_id 0 42 miss% 0.2467657982910547
plot_id,batch_id 0 43 miss% 0.34074330755494003
plot_id,batch_id 0 44 miss% 0.2640100595032237
plot_id,batch_id 0 45 miss% 0.22863291384970438
plot_id,batch_id 0 46 miss% 0.4001839172157569
plot_id,batch_id 0 47 miss% 0.342688624583587
plot_id,batch_id 0 48 miss% 0.35079146860170807
plot_id,batch_id 0 49 miss% 0.33515853806407153
plot_id,batch_id 0 50 miss% 0.3674441317320813
plot_id,batch_id 0 51 miss% 0.4293427861657977
plot_id,batch_id 0 52 miss% 0.4081967940712976
plot_id,batch_id 0 53 miss% 0.2893039878411235
plot_id,batch_id 0 54 miss% 0.45898279451663
plot_id,batch_id 0 55 miss% 0.33302952994630575
plot_id,batch_id 0 56 miss% 0.38059342498107235
plot_id,batch_id 0 57 miss% 0.43573132536429693
plot_id,batch_id 0 58 miss% 0.37594630168264564
plot_id,batch_id 0 59 miss% 0.35644060727670557
plot_id,batch_id 0 60 miss% 0.19589714668066432
plot_id,batch_id 0 61 miss% 0.2838058721481782
plot_id,batch_id 0 62 miss% 0.35508442978745053
plot_id,batch_id 0 63 miss% 0.33153932987171353
plot_id,batch_id 0 64 miss% 0.3045644278843345
plot_id,batch_id 0 65 miss% 0.3300402942667171
plot_id,batch_id 0 66 miss% 0.38146983593000433
plot_id,batch_id 0 67 miss% 0.2703520947221986
plot_id,batch_id 0 68 miss% 0.36140677335991156
plot_id,batch_id 0 69 miss% 0.3495902879027609
plot_id,batch_id 0 70 miss% 0.30290214147843764
plot_id,batch_id 0 71 miss% 0.34217850943247347
plot_id,batch_id 0 72 miss% 0.3798194980835942
plot_id,batch_id 0 73 miss% 0.5165235742517913
plot_id,batch_id 0 74 miss% 0.39768051889457356
plot_id,batch_id 0 75 miss% 0.19501893522211924
plot_id,batch_id 0 76 miss% 0.3881443246640624
plot_id,batch_id 0 77 miss% 0.34169802180012
plot_id,batch_id 0 78 miss% 0.3672069530590337
plot_id,batch_id 0 79 miss% 0.29347462292962234
plot_id,batch_id 0 80 miss% 0.2675287379868735
plot_id,batch_id 0 81 miss% 0.36018108189348474
plot_id,batch_id 0 82 miss% 0.29238594074098007
plot_id,batch_id 0 83 miss% 0.34980802827760116
plot_id,batch_id 0 84 miss% 0.3321486543384198
plot_id,batch_id 0 85 miss% 0.24604558892322217
plot_id,batch_id 0 86 miss% 0.3576047139242991
plot_id,batch_id 0 87 miss% 0.39235739236338363
plot_id,batch_id 0 88 miss% 0.3190897489132597
plot_id,batch_id 0 89 miss% 0.42205372960150594
plot_id,batch_id 0 90 miss% 0.17762424326126114
plot_id,batch_id 0 91 miss% 0.33256312772675495
plot_id,batch_id 0 92 miss% 0.24603660571348485
plot_id,batch_id 0 93 miss% 0.322486095016204
plot_id,batch_id 0 94 miss% 0.4289175736632435
plot_id,batch_id 0 95 miss% 0.2935872204834256
plot_id,batch_id 0 96 miss% 0.32048884216937695
plot_id,batch_id 0 97 miss% 0.42728930130397735
plot_id,batch_id 0 98 miss% 0.39525630617887164
plot_id,batch_id 0 99 miss% 0.4087341713430735
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.27889366 0.2664876  0.41791273 0.33667888 0.32571791 0.28168286
 0.37254464 0.48728419 0.39019607 0.31881687 0.35736143 0.39024499
 0.33840774 0.40452306 0.41292749 0.33776486 0.45168402 0.50206466
 0.36252286 0.3633692  0.29011353 0.27359601 0.33995399 0.25253649
 0.34510102 0.35543439 0.31925516 0.31543197 0.47623689 0.31561131
 0.33177166 0.32907445 0.41592603 0.34420218 0.40366839 0.33184269
 0.34524311 0.34832459 0.2679579  0.3530831  0.32175094 0.29007936
 0.2467658  0.34074331 0.26401006 0.22863291 0.40018392 0.34268862
 0.35079147 0.33515854 0.36744413 0.42934279 0.40819679 0.28930399
 0.45898279 0.33302953 0.38059342 0.43573133 0.3759463  0.35644061
 0.19589715 0.28380587 0.35508443 0.33153933 0.30456443 0.33004029
 0.38146984 0.27035209 0.36140677 0.34959029 0.30290214 0.34217851
 0.3798195  0.51652357 0.39768052 0.19501894 0.38814432 0.34169802
 0.36720695 0.29347462 0.26752874 0.36018108 0.29238594 0.34980803
 0.33214865 0.24604559 0.35760471 0.39235739 0.31908975 0.42205373
 0.17762424 0.33256313 0.24603661 0.3224861  0.42891757 0.29358722
 0.32048884 0.4272893  0.39525631 0.40873417]
for model  161 the mean error 0.3448785187933763
all id 161 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 25 out win 6 err 0.3448785187933763 time 21531.648034334183
Launcher: Job 162 completed in 21751 seconds.
Launcher: Task 17 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  209681
Epoch:0, Train loss:0.322047, valid loss:0.328354
Epoch:1, Train loss:0.013827, valid loss:0.002254
Epoch:2, Train loss:0.003632, valid loss:0.001389
Epoch:3, Train loss:0.002610, valid loss:0.001063
Epoch:4, Train loss:0.002204, valid loss:0.001065
Epoch:5, Train loss:0.001934, valid loss:0.000875
Epoch:6, Train loss:0.001742, valid loss:0.000972
Epoch:7, Train loss:0.001610, valid loss:0.000748
Epoch:8, Train loss:0.001457, valid loss:0.000912
Epoch:9, Train loss:0.001381, valid loss:0.000710
Epoch:10, Train loss:0.001312, valid loss:0.000650
Epoch:11, Train loss:0.000930, valid loss:0.000611
Epoch:12, Train loss:0.000888, valid loss:0.000532
Epoch:13, Train loss:0.000875, valid loss:0.000617
Epoch:14, Train loss:0.000831, valid loss:0.000521
Epoch:15, Train loss:0.000800, valid loss:0.000522
Epoch:16, Train loss:0.000798, valid loss:0.000586
Epoch:17, Train loss:0.000760, valid loss:0.000528
Epoch:18, Train loss:0.000737, valid loss:0.000683
Epoch:19, Train loss:0.000739, valid loss:0.000564
Epoch:20, Train loss:0.000704, valid loss:0.000479
Epoch:21, Train loss:0.000529, valid loss:0.000460
Epoch:22, Train loss:0.000518, valid loss:0.000479
Epoch:23, Train loss:0.000514, valid loss:0.000454
Epoch:24, Train loss:0.000496, valid loss:0.000475
Epoch:25, Train loss:0.000482, valid loss:0.000500
Epoch:26, Train loss:0.000487, valid loss:0.000452
Epoch:27, Train loss:0.000461, valid loss:0.000483
Epoch:28, Train loss:0.000468, valid loss:0.000458
Epoch:29, Train loss:0.000465, valid loss:0.000508
Epoch:30, Train loss:0.000441, valid loss:0.000470
Epoch:31, Train loss:0.000368, valid loss:0.000450
Epoch:32, Train loss:0.000359, valid loss:0.000431
Epoch:33, Train loss:0.000363, valid loss:0.000432
Epoch:34, Train loss:0.000359, valid loss:0.000448
Epoch:35, Train loss:0.000354, valid loss:0.000442
Epoch:36, Train loss:0.000348, valid loss:0.000432
Epoch:37, Train loss:0.000352, valid loss:0.000443
Epoch:38, Train loss:0.000337, valid loss:0.000435
Epoch:39, Train loss:0.000336, valid loss:0.000425
Epoch:40, Train loss:0.000338, valid loss:0.000425
Epoch:41, Train loss:0.000301, valid loss:0.000413
Epoch:42, Train loss:0.000298, valid loss:0.000445
Epoch:43, Train loss:0.000294, valid loss:0.000426
Epoch:44, Train loss:0.000293, valid loss:0.000428
Epoch:45, Train loss:0.000291, valid loss:0.000438
Epoch:46, Train loss:0.000287, valid loss:0.000423
Epoch:47, Train loss:0.000289, valid loss:0.000417
Epoch:48, Train loss:0.000285, valid loss:0.000440
Epoch:49, Train loss:0.000285, valid loss:0.000436
Epoch:50, Train loss:0.000283, valid loss:0.000431
Epoch:51, Train loss:0.000269, valid loss:0.000421
Epoch:52, Train loss:0.000266, valid loss:0.000418
Epoch:53, Train loss:0.000265, valid loss:0.000419
Epoch:54, Train loss:0.000264, valid loss:0.000418
Epoch:55, Train loss:0.000264, valid loss:0.000416
Epoch:56, Train loss:0.000263, valid loss:0.000417
Epoch:57, Train loss:0.000263, valid loss:0.000416
Epoch:58, Train loss:0.000263, valid loss:0.000416
Epoch:59, Train loss:0.000263, valid loss:0.000416
Epoch:60, Train loss:0.000263, valid loss:0.000416
training time 21826.092371702194
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07562135894188157
plot_id,batch_id 0 1 miss% 0.058836776141014645
plot_id,batch_id 0 2 miss% 0.09289950176601121
plot_id,batch_id 0 3 miss% 0.06217292808548899
plot_id,batch_id 0 4 miss% 0.054399478393649836
plot_id,batch_id 0 5 miss% 0.050760604314450626
plot_id,batch_id 0 6 miss% 0.04361164578241464
plot_id,batch_id 0 7 miss% 0.08103872406974015
plot_id,batch_id 0 8 miss% 0.06384706149938547
plot_id,batch_id 0 9 miss% 0.07509890687351889
plot_id,batch_id 0 10 miss% 0.022534484234967816
plot_id,batch_id 0 11 miss% 0.03981666676096167
plot_id,batch_id 0 12 miss% 0.041619099631680964
plot_id,batch_id 0 13 miss% 0.061545472254936576
plot_id,batch_id 0 14 miss% 0.049029118258932615
plot_id,batch_id 0 15 miss% 0.030051710028285307
plot_id,batch_id 0 16 miss% 0.044216852179672424
plot_id,batch_id 0 17 miss% 0.031535763142574474
plot_id,batch_id 0 18 miss% 0.06481248649214424
plot_id,batch_id 0 19 miss% 0.05178380748509808
plot_id,batch_id 0 20 miss% 0.034264083175949
plot_id,batch_id 0 21 miss% 0.06120539505014444
plot_id,batch_id 0 22 miss% 0.04629638637335051
plot_id,batch_id 0 23 miss% 0.04582630012757857
plot_id,batch_id 0 24 miss% 0.043115214494641854
plot_id,batch_id 0 25 miss% 0.04215112293110783
plot_id,batch_id 0 26 miss% 0.034927904670327255
plot_id,batch_id 0 27 miss% 0.05233199714601337
plot_id,batch_id 0 28 miss% 0.04078940870381928
plot_id,batch_id 0 29 miss% 0.0526022022751085
plot_id,batch_id 0 30 miss% 0.0320757670990692
plot_id,batch_id 0 31 miss% 0.06758858812517911
plot_id,batch_id 0 32 miss% 0.11461078025542784
plot_id,batch_id 0 33 miss% 0.045290367352231964
plot_id,batch_id 0 34 miss% 0.06404411375218265
plot_id,batch_id 0 35 miss% 0.03195958098480032
plot_id,batch_id 0 36 miss% 0.06861207036896
plot_id,batch_id 0 37 miss% 0.06346380698619784
plot_id,batch_id 0 38 miss% 0.041337514253453424
plot_id,batch_id 0 39 miss% 0.04275789003986729
plot_id,batch_id 0 40 miss% 0.06259713306037847
plot_id,batch_id 0 41 miss% 0.025765354822115664
plot_id,batch_id 0 42 miss% 0.04144844168825525
plot_id,batch_id 0 43 miss% 0.06845212269569406
plot_id,batch_id 0 44 miss% 0.03915776351108384
plot_id,batch_id 0 45 miss% 0.05498531195330016
plot_id,batch_id 0 46 miss% 0.06602417732070771
plot_id,batch_id 0 47 miss% 0.037657011173636
plot_id,batch_id 0 48 miss% 0.04336115098602479
plot_id,batch_id 0 49 miss% 0.055635489282968624
plot_id,batch_id 0 50 miss% 0.09253160406608424
plot_id,batch_id 0 51 miss% 0.04880353544800551
plot_id,batch_id 0 52 miss% 0.0450277088979642
plot_id,batch_id 0 53 miss% 0.020585426451489856
plot_id,batch_id 0 54 miss% 0.07304755594909423
plot_id,batch_id 0 55 miss% 0.045630075639284055
plot_id,batch_id 0 56 miss% 0.09529351434888408
plot_id,batch_id 0 57 miss% 0.05332811716068456
plot_id,batch_id 0 58 miss% 0.041061015772911484
plot_id,batch_id 0 59 miss% 0.04572696104803152
plot_id,batch_id 0 60 miss% 0.040501688441604525
plot_id,batch_id 0 61 miss% 0.032444844253310845
plot_id,batch_id 0 62 miss% 0.06511964480806891
plot_id,batch_id 0 63 miss% 0.03226443244198974
plot_id,batch_id 0 64 miss% 0.07861251901825794
plot_id,batch_id 0 65 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  151697
Epoch:0, Train loss:0.362207, valid loss:0.326726
Epoch:1, Train loss:0.236113, valid loss:0.233819
Epoch:2, Train loss:0.230674, valid loss:0.233581
Epoch:3, Train loss:0.230135, valid loss:0.233432
Epoch:4, Train loss:0.229900, valid loss:0.233336
Epoch:5, Train loss:0.229665, valid loss:0.233426
Epoch:6, Train loss:0.229558, valid loss:0.233373
Epoch:7, Train loss:0.229443, valid loss:0.233280
Epoch:8, Train loss:0.229395, valid loss:0.233223
Epoch:9, Train loss:0.229292, valid loss:0.233210
Epoch:10, Train loss:0.229204, valid loss:0.233127
Epoch:11, Train loss:0.228844, valid loss:0.233098
Epoch:12, Train loss:0.228808, valid loss:0.232935
Epoch:13, Train loss:0.228769, valid loss:0.232967
Epoch:14, Train loss:0.228781, valid loss:0.232928
Epoch:15, Train loss:0.228761, valid loss:0.232972
Epoch:16, Train loss:0.228691, valid loss:0.232994
Epoch:17, Train loss:0.228678, valid loss:0.232913
Epoch:18, Train loss:0.228659, valid loss:0.232930
Epoch:19, Train loss:0.228648, valid loss:0.232940
Epoch:20, Train loss:0.228613, valid loss:0.233004
Epoch:21, Train loss:0.228448, valid loss:0.232862
Epoch:22, Train loss:0.228408, valid loss:0.232886
Epoch:23, Train loss:0.228389, valid loss:0.232898
Epoch:24, Train loss:0.228414, valid loss:0.232928
Epoch:25, Train loss:0.228384, valid loss:0.232906
Epoch:26, Train loss:0.228373, valid loss:0.232946
Epoch:27, Train loss:0.228373, valid loss:0.232893
Epoch:28, Train loss:0.228362, valid loss:0.232895
Epoch:29, Train loss:0.228353, valid loss:0.232856
Epoch:30, Train loss:0.228345, valid loss:0.232906
Epoch:31, Train loss:0.228253, valid loss:0.232829
Epoch:32, Train loss:0.228234, valid loss:0.232814
Epoch:33, Train loss:0.228244, valid loss:0.232812
Epoch:34, Train loss:0.228231, valid loss:0.232807
Epoch:35, Train loss:0.228220, valid loss:0.232817
Epoch:36, Train loss:0.228232, valid loss:0.232847
Epoch:37, Train loss:0.228210, valid loss:0.232803
Epoch:38, Train loss:0.228229, valid loss:0.232847
Epoch:39, Train loss:0.228206, valid loss:0.232805
Epoch:40, Train loss:0.228202, valid loss:0.232851
Epoch:41, Train loss:0.228159, valid loss:0.232809
Epoch:42, Train loss:0.228153, valid loss:0.232814
Epoch:43, Train loss:0.228151, valid loss:0.232840
Epoch:44, Train loss:0.228156, valid loss:0.232805
Epoch:45, Train loss:0.228151, valid loss:0.232826
Epoch:46, Train loss:0.228152, valid loss:0.232822
Epoch:47, Train loss:0.228143, valid loss:0.232806
Epoch:48, Train loss:0.228142, valid loss:0.232803
Epoch:49, Train loss:0.228142, valid loss:0.232803
Epoch:50, Train loss:0.228143, valid loss:0.232811
Epoch:51, Train loss:0.228123, valid loss:0.232804
Epoch:52, Train loss:0.228121, valid loss:0.232806
Epoch:53, Train loss:0.228121, valid loss:0.232806
Epoch:54, Train loss:0.228120, valid loss:0.232807
Epoch:55, Train loss:0.228120, valid loss:0.232807
Epoch:56, Train loss:0.228119, valid loss:0.232807
Epoch:57, Train loss:0.228118, valid loss:0.232806
Epoch:58, Train loss:0.228118, valid loss:0.232808
Epoch:59, Train loss:0.228118, valid loss:0.232809
Epoch:60, Train loss:0.228118, valid loss:0.232807
training time 21940.453662633896
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.969971147253164
plot_id,batch_id 0 1 miss% 0.9683966089949312
plot_id,batch_id 0 2 miss% 0.9647294163709924
plot_id,batch_id 0 3 miss% 0.9756262448991041
plot_id,batch_id 0 4 miss% 0.9795398151349577
plot_id,batch_id 0 5 miss% 0.9675114322748525
plot_id,batch_id 0 6 miss% 0.964059998426131
plot_id,batch_id 0 7 miss% 0.9672206542940965
plot_id,batch_id 0 8 miss% 0.980278709060643
plot_id,batch_id 0 9 miss% 0.9659056018122036
plot_id,batch_id 0 10 miss% 0.975535271968265
plot_id,batch_id 0 11 miss% 0.963715899641155
plot_id,batch_id 0 12 miss% 0.9653962552039894
plot_id,batch_id 0 13 miss% 0.9632098115106666
plot_id,batch_id 0 14 miss% 0.9666599942480498
plot_id,batch_id 0 15 miss% 0.9679047008905994
plot_id,batch_id 0 16 miss% 0.9625195716771371
plot_id,batch_id 0 17 miss% 0.9728444821500455
plot_id,batch_id 0 18 miss% 0.9707291362342946
plot_id,batch_id 0 19 miss% 0.9648900612805509
plot_id,batch_id 0 20 miss% 0.9662012664842337
plot_id,batch_id 0 21 miss% 0.9731873124885032
plot_id,batch_id 0 22 miss% 0.9656114029942621
plot_id,batch_id 0 23 miss% 0.9668287398737887
plot_id,batch_id 0 24 miss% 0.9683183712340172
plot_id,batch_id 0 25 miss% 0.9637930258651406
plot_id,batch_id 0 26 miss% 0.965581929207337
plot_id,batch_id 0 27 miss% 0.9701007994057451
plot_id,batch_id 0 28 miss% 0.9819394472162554
plot_id,batch_id 0 29 miss% 0.9647440681661089
plot_id,batch_id 0 30 miss% 0.9660332872725553
plot_id,batch_id 0 31 miss% 0.9642405317884958
plot_id,batch_id 0 32 miss% 0.9700885323398101
plot_id,batch_id 0 33 miss% 0.964674013231745
plot_id,batch_id 0 34 miss% 0.9662510830858849
plot_id,batch_id 0 35 miss% 0.9927845890954514
plot_id,batch_id 0 36 miss% 0.963307774726899
plot_id,batch_id 0 37 miss% 0.9652540879469642
plot_id,batch_id 0 38 miss% 0.964289473487686
plot_id,batch_id 0 39 miss% 0.9658666583363639
plot_id,batch_id 0 40 miss% 0.9645904836205323
plot_id,batch_id 0 41 miss% 0.9703938827418552
plot_id,batch_id 0 42 miss% 0.970883784787512
plot_id,batch_id 0 43 miss% 0.9824310491856382
plot_id,batch_id 0 44 miss% 0.965274049373061
plot_id,batch_id 0 45 miss% 0.9641719720247265
plot_id,batch_id 0 46 miss% 0.9659297699472672
plot_id,batch_id 0 47 miss% 0.9677114676522524
plot_id,batch_id 0 48 miss% 0.9711627305275874
plot_id,batch_id 0 49 miss% 0.9710428520183894
plot_id,batch_id 0 50 miss% 0.9654808595042225
plot_id,batch_id 0 51 miss% 0.9649956801631319
plot_id,batch_id 0 52 miss% 0.9648462786150116
plot_id,batch_id 0 53 miss% 0.9683198959113184
plot_id,batch_id 0 54 miss% 0.982919827273349
plot_id,batch_id 0 55 miss% 0.9637944111173211
plot_id,batch_id 0 56 miss% 0.9641138209018403
plot_id,batch_id 0 57 miss% 0.9657343377520031
plot_id,batch_id 0 58 miss% 0.9654666802127141
plot_id,batch_id 0 59 miss% 0.9684809096474115
plot_id,batch_id 0 60 miss% 0.9648672202301343
plot_id,batch_id 0 61 miss% 0.9652827804657208
plot_id,batch_id 0 62 miss% 0.966647608743507
plot_id,batch_id 0 63 miss% 0.9676478901220932
plot_id,batch_id 0 64 miss% 0.9658763554779688
plot_id,batch_id 0 65 miss% 0.9550523346280501
plot_id,batch_id 0 66 miss% 0.9647403323460276
plot_id,batch_id 0 67 miss% 0.9750252723822558
plot_id,batch_id 0 68 miss% 0.9658044930119136
plot_id,batch_id 0 69 miss% 0.9654298050773958
plot_id,batch_id 0 70 miss% 0.9550208447061348
plot_id,batch_id 0 71 miss% 0.9699655840796126
plot_id,batch_id 0 72 miss% 0.9630302567992746
plot_id,batch_id 0 73 miss% 0.9658439059821391
plot_id,batch_id 0 74 miss% 0.9636406941337611
plot_id,batch_id 0 75 miss% 0.9915670449444351
plot_id,batch_id 0 76 miss% 0.9770559358043263
plot_id,batch_id 0 77 miss% 0.9641731309146793
plot_id,batch_id 0 78 miss% 0.9708502602490868
plot_id,batch_id 0 79 miss% 0.9645120567293388
plot_id,batch_id 0 80 miss% 0.9600566015423182
plot_id,batch_id 0 81 miss% 0.9634814587146125
plot_id,batch_id 0 82 miss% 0.9684377844247225
plot_id,batch_id 0 83 miss% 0.9726762663033909
plot_id,batch_id 0 84 miss% 0.9779589759412687
plot_id,batch_id 0 85 miss% 0.9586545876652199
plot_id,batch_id 0 86 miss% 0.9648678480464672
plot_id,batch_id 0 87 miss% 0.970005963713382
plot_id,batch_id 0 88 miss% 0.968683337140581
plot_id,batch_id 0 89 miss% 0.9654511722405857
plot_id,batch_id 0 90 miss% 0.9622301288248519
plot_id,batch_id 0 91 miss% 0.964629190951059
plot_id,batch_id 0 92 miss% 0.9678439627742407
plot_id,batch_id 0 93 miss% 0.9646201138412467
plot_id,batch_id 0 94 miss% 0.9648081747597262
plot_id,batch_id 0 95 miss% 0.9685349066718625
plot_id,batch_id 0 96 miss% 0.9955075238365206
plot_id,batch_id 0 97 miss% 0.9653642485326255
plot_id,batch_id 0 98 miss% 0.9662716478422381
plot_id,batch_id 0 99 miss% 0.9656044158309413
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.96997115 0.96839661 0.96472942 0.97562624 0.97953982 0.96751143
 0.96406    0.96722065 0.98027871 0.9659056  0.97553527 0.9637159
 0.96539626 0.96320981 0.96665999 0.9679047  0.96251957 0.97284448
 0.97072914 0.96489006 0.96620127 0.97318731 0.9656114  0.96682874
 0.96831837 0.96379303 0.96558193 0.9701008  0.98193945 0.96474407
 0.96603329 0.96424053 0.97008853 0.96467401 0.96625108 0.99278459
 0.96330777 0.96525409 0.96428947 0.96586666 0.96459048 0.97039388
 0.97088378 0.98243105 0.96527405 0.96417197 0.96592977 0.96771147
 0.97116273 0.97104285 0.96548086 0.96499568 0.96484628 0.9683199
 0.98291983 0.96379441 0.96411382 0.96573434 0.96546668 0.96848091
 0.96486722 0.96528278 0.96664761 0.96764789 0.96587636 0.95505233
 0.96474033 0.97502527 0.96580449 0.96542981 0.95502084 0.96996558
 0.96303026 0.96584391 0.96364069 0.99156704 0.97705594 0.96417313
 0.97085026 0.96451206 0.9600566  0.96348146 0.96843778 0.97267627
 0.97795898 0.95865459 0.96486785 0.97000596 0.96868334 0.96545117
 0.96223013 0.96462919 0.96784396 0.96462011 0.96480817 0.96853491
 0.99550752 0.96536425 0.96627165 0.96560442]
for model  239 the mean error 0.9681120806697594
all id 239 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 31 out win 6 err 0.9681120806697594 time 21940.453662633896
Launcher: Job 240 completed in 22046 seconds.
Launcher: Task 58 done. Exiting.
0.09761493476883946
plot_id,batch_id 0 66 miss% 0.023951986313054348
plot_id,batch_id 0 67 miss% 0.033112395818860586
plot_id,batch_id 0 68 miss% 0.028644093700059135
plot_id,batch_id 0 69 miss% 0.061829986576669826
plot_id,batch_id 0 70 miss% 0.04307626846872947
plot_id,batch_id 0 71 miss% 0.03682704870362456
plot_id,batch_id 0 72 miss% 0.04418999742974682
plot_id,batch_id 0 73 miss% 0.06348408078097514
plot_id,batch_id 0 74 miss% 0.056214390416540146
plot_id,batch_id 0 75 miss% 0.05975023989460187
plot_id,batch_id 0 76 miss% 0.04912582752640298
plot_id,batch_id 0 77 miss% 0.0573696256829475
plot_id,batch_id 0 78 miss% 0.05496050463842975
plot_id,batch_id 0 79 miss% 0.05880228804576047
plot_id,batch_id 0 80 miss% 0.03861252409745938
plot_id,batch_id 0 81 miss% 0.08366302628140371
plot_id,batch_id 0 82 miss% 0.049496177516061016
plot_id,batch_id 0 83 miss% 0.0724301494806569
plot_id,batch_id 0 84 miss% 0.06169213879403356
plot_id,batch_id 0 85 miss% 0.03115361795338856
plot_id,batch_id 0 86 miss% 0.03816913962797443
plot_id,batch_id 0 87 miss% 0.0601283685613376
plot_id,batch_id 0 88 miss% 0.08232469220781058
plot_id,batch_id 0 89 miss% 0.04248113111896006
plot_id,batch_id 0 90 miss% 0.05508530235047802
plot_id,batch_id 0 91 miss% 0.027757516535266414
plot_id,batch_id 0 92 miss% 0.04730494517091272
plot_id,batch_id 0 93 miss% 0.051143490183172255
plot_id,batch_id 0 94 miss% 0.035470144842810254
plot_id,batch_id 0 95 miss% 0.055455568682896104
plot_id,batch_id 0 96 miss% 0.05508749148623196
plot_id,batch_id 0 97 miss% 0.027512924751590583
plot_id,batch_id 0 98 miss% 0.035739041331971144
plot_id,batch_id 0 99 miss% 0.06615957182700158
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07562136 0.05883678 0.0928995  0.06217293 0.05439948 0.0507606
 0.04361165 0.08103872 0.06384706 0.07509891 0.02253448 0.03981667
 0.0416191  0.06154547 0.04902912 0.03005171 0.04421685 0.03153576
 0.06481249 0.05178381 0.03426408 0.0612054  0.04629639 0.0458263
 0.04311521 0.04215112 0.0349279  0.052332   0.04078941 0.0526022
 0.03207577 0.06758859 0.11461078 0.04529037 0.06404411 0.03195958
 0.06861207 0.06346381 0.04133751 0.04275789 0.06259713 0.02576535
 0.04144844 0.06845212 0.03915776 0.05498531 0.06602418 0.03765701
 0.04336115 0.05563549 0.0925316  0.04880354 0.04502771 0.02058543
 0.07304756 0.04563008 0.09529351 0.05332812 0.04106102 0.04572696
 0.04050169 0.03244484 0.06511964 0.03226443 0.07861252 0.09761493
 0.02395199 0.0331124  0.02864409 0.06182999 0.04307627 0.03682705
 0.04419    0.06348408 0.05621439 0.05975024 0.04912583 0.05736963
 0.0549605  0.05880229 0.03861252 0.08366303 0.04949618 0.07243015
 0.06169214 0.03115362 0.03816914 0.06012837 0.08232469 0.04248113
 0.0550853  0.02775752 0.04730495 0.05114349 0.03547014 0.05545557
 0.05508749 0.02751292 0.03573904 0.06615957]
for model  205 the mean error 0.052113661723087096
all id 205 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 31 out win 5 err 0.052113661723087096 time 21826.092371702194
Launcher: Job 206 completed in 22072 seconds.
Launcher: Task 51 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  209681
Epoch:0, Train loss:0.322047, valid loss:0.328354
Epoch:1, Train loss:0.019845, valid loss:0.002751
Epoch:2, Train loss:0.007729, valid loss:0.002057
Epoch:3, Train loss:0.004904, valid loss:0.001373
Epoch:4, Train loss:0.002466, valid loss:0.001027
Epoch:5, Train loss:0.002003, valid loss:0.000927
Epoch:6, Train loss:0.001797, valid loss:0.000992
Epoch:7, Train loss:0.001634, valid loss:0.000773
Epoch:8, Train loss:0.001524, valid loss:0.000900
Epoch:9, Train loss:0.001404, valid loss:0.000747
Epoch:10, Train loss:0.001304, valid loss:0.000662
Epoch:11, Train loss:0.000988, valid loss:0.000649
Epoch:12, Train loss:0.000961, valid loss:0.000547
Epoch:13, Train loss:0.000923, valid loss:0.000579
Epoch:14, Train loss:0.000892, valid loss:0.000602
Epoch:15, Train loss:0.000867, valid loss:0.000612
Epoch:16, Train loss:0.000844, valid loss:0.000551
Epoch:17, Train loss:0.000810, valid loss:0.000539
Epoch:18, Train loss:0.000780, valid loss:0.000487
Epoch:19, Train loss:0.000765, valid loss:0.000573
Epoch:20, Train loss:0.000764, valid loss:0.000528
Epoch:21, Train loss:0.000588, valid loss:0.000444
Epoch:22, Train loss:0.000574, valid loss:0.000460
Epoch:23, Train loss:0.000570, valid loss:0.000445
Epoch:24, Train loss:0.000565, valid loss:0.000455
Epoch:25, Train loss:0.000546, valid loss:0.000442
Epoch:26, Train loss:0.000540, valid loss:0.000546
Epoch:27, Train loss:0.000532, valid loss:0.000580
Epoch:28, Train loss:0.000529, valid loss:0.000426
Epoch:29, Train loss:0.000509, valid loss:0.000438
Epoch:30, Train loss:0.000522, valid loss:0.000461
Epoch:31, Train loss:0.000434, valid loss:0.000393
Epoch:32, Train loss:0.000415, valid loss:0.000439
Epoch:33, Train loss:0.000417, valid loss:0.000428
Epoch:34, Train loss:0.000416, valid loss:0.000451
Epoch:35, Train loss:0.000410, valid loss:0.000418
Epoch:36, Train loss:0.000407, valid loss:0.000426
Epoch:37, Train loss:0.000414, valid loss:0.000404
Epoch:38, Train loss:0.000409, valid loss:0.000444
Epoch:39, Train loss:0.000397, valid loss:0.000436
Epoch:40, Train loss:0.000388, valid loss:0.000410
Epoch:41, Train loss:0.000356, valid loss:0.000414
Epoch:42, Train loss:0.000352, valid loss:0.000412
Epoch:43, Train loss:0.000350, valid loss:0.000450
Epoch:44, Train loss:0.000351, valid loss:0.000399
Epoch:45, Train loss:0.000347, valid loss:0.000443
Epoch:46, Train loss:0.000344, valid loss:0.000418
Epoch:47, Train loss:0.000346, valid loss:0.000397
Epoch:48, Train loss:0.000341, valid loss:0.000423
Epoch:49, Train loss:0.000338, valid loss:0.000418
Epoch:50, Train loss:0.000336, valid loss:0.000394
Epoch:51, Train loss:0.000319, valid loss:0.000395
Epoch:52, Train loss:0.000315, valid loss:0.000398
Epoch:53, Train loss:0.000314, valid loss:0.000398
Epoch:54, Train loss:0.000313, valid loss:0.000401
Epoch:55, Train loss:0.000313, valid loss:0.000400
Epoch:56, Train loss:0.000313, valid loss:0.000402
Epoch:57, Train loss:0.000312, valid loss:0.000401
Epoch:58, Train loss:0.000312, valid loss:0.000400
Epoch:59, Train loss:0.000312, valid loss:0.000400
Epoch:60, Train loss:0.000312, valid loss:0.000400
training time 22025.184165239334
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07376732673619749
plot_id,batch_id 0 1 miss% 0.0410847859661065
plot_id,batch_id 0 2 miss% 0.08745236091231935
plot_id,batch_id 0 3 miss% 0.054901339353628634
plot_id,batch_id 0 4 miss% 0.03732998018515248
plot_id,batch_id 0 5 miss% 0.042115749021195685
plot_id,batch_id 0 6 miss% 0.03308197630790166
plot_id,batch_id 0 7 miss% 0.0784896115355765
plot_id,batch_id 0 8 miss% 0.06595933153549585
plot_id,batch_id 0 9 miss% 0.053238192897671774
plot_id,batch_id 0 10 miss% 0.042081613688849565
plot_id,batch_id 0 11 miss% 0.07202931819162646
plot_id,batch_id 0 12 miss% 0.05629957735137797
plot_id,batch_id 0 13 miss% 0.05368078150365704
plot_id,batch_id 0 14 miss% 0.08894969797367482
plot_id,batch_id 0 15 miss% 0.025731848923648517
plot_id,batch_id 0 16 miss% 0.04194732371380522
plot_id,batch_id 0 17 miss% 0.03884541022830798
plot_id,batch_id 0 18 miss% 0.09447607829624965
plot_id,batch_id 0 19 miss% 0.09837407644015089
plot_id,batch_id 0 20 miss% 0.06395354921154667
plot_id,batch_id 0 21 miss% 0.03701029207745533
plot_id,batch_id 0 22 miss% 0.03514339308795993
plot_id,batch_id 0 23 miss% 0.03988157311483575
plot_id,batch_id 0 24 miss% 0.06497881154070997
plot_id,batch_id 0 25 miss% 0.06783440606149692
plot_id,batch_id 0 26 miss% 0.041891583276077025
plot_id,batch_id 0 27 miss% 0.038162908331226784
plot_id,batch_id 0 28 miss% 0.04430617703559909
plot_id,batch_id 0 29 miss% 0.03581692341341252
plot_id,batch_id 0 30 miss% 0.02938530894128807
plot_id,batch_id 0 31 miss% 0.10338844934462746
plot_id,batch_id 0 32 miss% 0.09255080310958251
plot_id,batch_id 0 33 miss% 0.06306804464831382
plot_id,batch_id 0 34 miss% 0.04008446330767067
plot_id,batch_id 0 35 miss% 0.03599006856656258
plot_id,batch_id 0 36 miss% 0.11605494827433008
plot_id,batch_id 0 37 miss% 0.04555722731122957
plot_id,batch_id 0 38 miss% 0.021530488875929975
plot_id,batch_id 0 39 miss% 0.03247708839151472
plot_id,batch_id 0 40 miss% 0.07046528916185747
plot_id,batch_id 0 41 miss% 0.05497768105157538
plot_id,batch_id 0 42 miss% 0.02859730836585935
plot_id,batch_id 0 43 miss% 0.09859488995547287
plot_id,batch_id 0 44 miss% 0.028717491864733557
plot_id,batch_id 0 45 miss% 0.055290883230461635
plot_id,batch_id 0 46 miss% 0.05757145403759879
plot_id,batch_id 0 47 miss% 0.048928500491854834
plot_id,batch_id 0 48 miss% 0.02651111685824398
plot_id,batch_id 0 49 miss% 0.03495092328758486
plot_id,batch_id 0 50 miss% 0.08672551977539167
plot_id,batch_id 0 51 miss% 0.05087312696915464
plot_id,batch_id 0 52 miss% 0.04880646377380406
plot_id,batch_id 0 53 miss% 0.023497072502572144
plot_id,batch_id 0 54 miss% 0.02937583015442623
plot_id,batch_id 0 55 miss% 0.061523816169323425
plot_id,batch_id 0 56 miss% 0.07052983344051346
plot_id,batch_id 0 57 miss% 0.032106125219955484
plot_id,batch_id 0 58 miss% 0.05361188730677805
plot_id,batch_id 0 59 miss% 0.07515537615917876
plot_id,batch_id 0 60 miss% 0.044407292134889136
plot_id,batch_id 0 61 miss% 0.03582990989417142
plot_id,batch_id 0 62 miss% 0.029838605791747186
plot_id,batch_id 0 63 miss% 0.06320359837017812
plot_id,batch_id 0 64 miss% 0.04010637550009481
plot_id,batch_id 0 65 miss% 0.03690923006688431
plot_id,batch_id 0 66 miss% 0.03874810017379853
plot_id,batch_id 0 67 miss% 0.036868678491472816
plot_id,batch_id 0 68 miss% 0.03805448225429938
plot_id,batch_id 0 69 miss% 0.0593102160624531
plot_id,batch_id 0 70 miss% 0.04590959491387528
plot_id,batch_id 0 71 miss% 0.03810941513613414
plot_id,batch_id 0 72 miss% 0.09025879533300615
plot_id,batch_id 0 73 miss% 0.05927039657820938
plot_id,batch_id 0 74 miss% 0.08798952014033544
plot_id,batch_id 0 75 miss% 0.04463114984612085
plot_id,batch_id 0 76 miss% 0.05801118255125314
plot_id,batch_id 0 77 miss% 0.06612949412557231
plot_id,batch_id 0 78 miss% 0.036877949967659474
plot_id,batch_id 0 79 miss% 0.06911542781352006
plot_id,batch_id 0 80 miss% 0.0645402116998519
plot_id,batch_id 0 81 miss% 0.06860188427477706
plot_id,batch_id 0 82 miss% 0.048906467489476385
plot_id,batch_id 0 83 miss% 0.11678666177556174
plot_id,batch_id 0 84 miss% 0.042849123691095135
plot_id,batch_id 0 85 miss% 0.027085280392957597
plot_id,batch_id 0 86 miss% 0.06253649862455446
plot_id,batch_id 0 87 miss% 0.06777623681034547
plot_id,batch_id 0 88 miss% 0.09840606179262407
plot_id,batch_id 0 89 miss% 0.05446877224374407
plot_id,batch_id 0 90 miss% 0.0438059007824331
plot_id,batch_id 0 91 miss% 0.05363350580681415
plot_id,batch_id 0 92 miss% 0.04293084875349459
plot_id,batch_id 0 93 miss% 0.037066389281282565
plot_id,batch_id 0 94 miss% 0.05229080474683619
plot_id,batch_id 0 95 miss% 0.04109792487410739
plot_id,batch_id 0 96 miss% 0.05020937196659289
plot_id,batch_id 0 97 miss% 0.03946002768044414
plot_id,batch_id 0 98 miss% 0.04536486661221847
plot_id,batch_id 0 99 miss% 0.10681406217270527
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07376733 0.04108479 0.08745236 0.05490134 0.03732998 0.04211575
 0.03308198 0.07848961 0.06595933 0.05323819 0.04208161 0.07202932
 0.05629958 0.05368078 0.0889497  0.02573185 0.04194732 0.03884541
 0.09447608 0.09837408 0.06395355 0.03701029 0.03514339 0.03988157
 0.06497881 0.06783441 0.04189158 0.03816291 0.04430618 0.03581692
 0.02938531 0.10338845 0.0925508  0.06306804 0.04008446 0.03599007
 0.11605495 0.04555723 0.02153049 0.03247709 0.07046529 0.05497768
 0.02859731 0.09859489 0.02871749 0.05529088 0.05757145 0.0489285
 0.02651112 0.03495092 0.08672552 0.05087313 0.04880646 0.02349707
 0.02937583 0.06152382 0.07052983 0.03210613 0.05361189 0.07515538
 0.04440729 0.03582991 0.02983861 0.0632036  0.04010638 0.03690923
 0.0387481  0.03686868 0.03805448 0.05931022 0.04590959 0.03810942
 0.0902588  0.0592704  0.08798952 0.04463115 0.05801118 0.06612949
 0.03687795 0.06911543 0.06454021 0.06860188 0.04890647 0.11678666
 0.04284912 0.02708528 0.0625365  0.06777624 0.09840606 0.05446877
 0.0438059  0.05363351 0.04293085 0.03706639 0.0522908  0.04109792
 0.05020937 0.03946003 0.04536487 0.10681406]
for model  178 the mean error 0.054499237950778955
all id 178 hidden_dim 32 learning_rate 0.0025 num_layers 4 frames 31 out win 5 err 0.054499237950778955 time 22025.184165239334
Launcher: Job 179 completed in 22271 seconds.
Launcher: Task 103 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  209681
Epoch:0, Train loss:0.322047, valid loss:0.328354
Epoch:1, Train loss:0.021724, valid loss:0.003238
Epoch:2, Train loss:0.006025, valid loss:0.001854
Epoch:3, Train loss:0.004776, valid loss:0.001391
Epoch:4, Train loss:0.003835, valid loss:0.001399
Epoch:5, Train loss:0.003475, valid loss:0.001169
Epoch:6, Train loss:0.002642, valid loss:0.001141
Epoch:7, Train loss:0.001888, valid loss:0.000964
Epoch:8, Train loss:0.001785, valid loss:0.001272
Epoch:9, Train loss:0.001692, valid loss:0.000904
Epoch:10, Train loss:0.001567, valid loss:0.000752
Epoch:11, Train loss:0.001103, valid loss:0.000677
Epoch:12, Train loss:0.001055, valid loss:0.000541
Epoch:13, Train loss:0.001047, valid loss:0.000629
Epoch:14, Train loss:0.001004, valid loss:0.000565
Epoch:15, Train loss:0.000948, valid loss:0.000563
Epoch:16, Train loss:0.000963, valid loss:0.000565
Epoch:17, Train loss:0.000896, valid loss:0.000570
Epoch:18, Train loss:0.000889, valid loss:0.000534
Epoch:19, Train loss:0.000855, valid loss:0.000613
Epoch:20, Train loss:0.000871, valid loss:0.000548
Epoch:21, Train loss:0.000619, valid loss:0.000483
Epoch:22, Train loss:0.000589, valid loss:0.000445
Epoch:23, Train loss:0.000588, valid loss:0.000461
Epoch:24, Train loss:0.000573, valid loss:0.000426
Epoch:25, Train loss:0.000563, valid loss:0.000421
Epoch:26, Train loss:0.000551, valid loss:0.000442
Epoch:27, Train loss:0.000534, valid loss:0.000481
Epoch:28, Train loss:0.000526, valid loss:0.000449
Epoch:29, Train loss:0.000528, valid loss:0.000477
Epoch:30, Train loss:0.000511, valid loss:0.000477
Epoch:31, Train loss:0.000405, valid loss:0.000429
Epoch:32, Train loss:0.000387, valid loss:0.000428
Epoch:33, Train loss:0.000398, valid loss:0.000399
Epoch:34, Train loss:0.000381, valid loss:0.000384
Epoch:35, Train loss:0.000378, valid loss:0.000394
Epoch:36, Train loss:0.000383, valid loss:0.000410
Epoch:37, Train loss:0.000365, valid loss:0.000390
Epoch:38, Train loss:0.000373, valid loss:0.000413
Epoch:39, Train loss:0.000364, valid loss:0.000394
Epoch:40, Train loss:0.000361, valid loss:0.000390
Epoch:41, Train loss:0.000319, valid loss:0.000379
Epoch:42, Train loss:0.000309, valid loss:0.000393
Epoch:43, Train loss:0.000302, valid loss:0.000387
Epoch:44, Train loss:0.000309, valid loss:0.000374
Epoch:45, Train loss:0.000300, valid loss:0.000401
Epoch:46, Train loss:0.000299, valid loss:0.000416
Epoch:47, Train loss:0.000299, valid loss:0.000399
Epoch:48, Train loss:0.000293, valid loss:0.000417
Epoch:49, Train loss:0.000293, valid loss:0.000414
Epoch:50, Train loss:0.000294, valid loss:0.000403
Epoch:51, Train loss:0.000271, valid loss:0.000391
Epoch:52, Train loss:0.000270, valid loss:0.000393
Epoch:53, Train loss:0.000269, valid loss:0.000396
Epoch:54, Train loss:0.000269, valid loss:0.000398
Epoch:55, Train loss:0.000269, valid loss:0.000396
Epoch:56, Train loss:0.000268, valid loss:0.000394
Epoch:57, Train loss:0.000268, valid loss:0.000395
Epoch:58, Train loss:0.000268, valid loss:0.000395
Epoch:59, Train loss:0.000268, valid loss:0.000397
Epoch:60, Train loss:0.000268, valid loss:0.000394
training time 22116.626256465912
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07685344966507202
plot_id,batch_id 0 1 miss% 0.060212933003468916
plot_id,batch_id 0 2 miss% 0.11777960163447174
plot_id,batch_id 0 3 miss% 0.038296687753046336
plot_id,batch_id 0 4 miss% 0.030916286677904282
plot_id,batch_id 0 5 miss% 0.06534637279068922
plot_id,batch_id 0 6 miss% 0.03155742216784622
plot_id,batch_id 0 7 miss% 0.10513344662007607
plot_id,batch_id 0 8 miss% 0.07593002111819327
plot_id,batch_id 0 9 miss% 0.060557185396453016
plot_id,batch_id 0 10 miss% 0.031353766242030436
plot_id,batch_id 0 11 miss% 0.055663852339078515
plot_id,batch_id 0 12 miss% 0.12322817116678639
plot_id,batch_id 0 13 miss% 0.048265314053853134
plot_id,batch_id 0 14 miss% 0.0796008101273429
plot_id,batch_id 0 15 miss% 0.039819985216733224
plot_id,batch_id 0 16 miss% 0.1929174985998807
plot_id,batch_id 0 17 miss% 0.07896986349184204
plot_id,batch_id 0 18 miss% 0.07360688813909838
plot_id,batch_id 0 19 miss% 0.08671476598432756
plot_id,batch_id 0 20 miss% 0.137063732611132
plot_id,batch_id 0 21 miss% 0.05777395936566393
plot_id,batch_id 0 22 miss% 0.0410472738750103
plot_id,batch_id 0 23 miss% 0.029366436672230628
plot_id,batch_id 0 24 miss% 0.022008447557292105
plot_id,batch_id 0 25 miss% 0.06030752313999077
plot_id,batch_id 0 26 miss% 0.05918956167179973
plot_id,batch_id 0 27 miss% 0.05239294463688108
plot_id,batch_id 0 28 miss% 0.03147276046383664
plot_id,batch_id 0 29 miss% 0.02320908098896289
plot_id,batch_id 0 30 miss% 0.043331411805639805
plot_id,batch_id 0 31 miss% 0.06554563288131673
plot_id,batch_id 0 32 miss% 0.08314408143503246
plot_id,batch_id 0 33 miss% 0.05821351245925827
plot_id,batch_id 0 34 miss% 0.03351144896874582
plot_id,batch_id 0 35 miss% 0.053957859879169394
plot_id,batch_id 0 36 miss% 0.07342763925970036
plot_id,batch_id 0 37 miss% 0.07100159977956416
plot_id,batch_id 0 38 miss% 0.03077616760424486
plot_id,batch_id 0 39 miss% 0.021070504861875658
plot_id,batch_id 0 40 miss% 0.059105816953973406
plot_id,batch_id 0 41 miss% 0.0366107640271986
plot_id,batch_id 0 42 miss% 0.027730354817020113
plot_id,batch_id 0 43 miss% 0.06462459418091347
plot_id,batch_id 0 44 miss% 0.02675433225075925
plot_id,batch_id 0 45 miss% 0.029922749240437304
plot_id,batch_id 0 46 miss% 0.023526054679247278
plot_id,batch_id 0 47 miss% 0.028046095197003674
plot_id,batch_id 0 48 miss% 0.0291695354791291
plot_id,batch_id 0 49 miss% 0.02018163595213313
plot_id,batch_id 0 50 miss% 0.15998865644259405
plot_id,batch_id 0 51 miss% 0.033046575382219126
plot_id,batch_id 0 52 miss% 0.039860814480173465
plot_id,batch_id 0 53 miss% 0.015535200476537277
plot_id,batch_id 0 54 miss% 0.05742374406421395
plot_id,batch_id 0 55 miss% 0.06092796206783773
plot_id,batch_id 0 56 miss% 0.08552011173205246
plot_id,batch_id 0 57 miss% 0.03426963584960657
plot_id,batch_id 0 58 miss% 0.03014612468784431
plot_id,batch_id 0 59 miss% 0.029433437580914126
plot_id,batch_id 0 60 miss% 0.02959676504752394
plot_id,batch_id 0 61 miss% 0.028982869203831294
plot_id,batch_id 0 62 miss% 0.04884488756858975
plot_id,batch_id 0 63 miss% 0.03797484834607219
plot_id,batch_id 0 64 miss% 0.06403570304467296
plot_id,batch_id 0 65 miss% 0.10588279604249078
plot_id,batch_id 0 66 miss% 0.05478954889373885
plot_id,batch_id 0 67 miss% 0.019032187823888956
plot_id,batch_id 0 68 miss% 0.056756855646797284
plot_id,batch_id 0 69 miss% 0.06691177607697121
plot_id,batch_id 0 70 miss% 0.04133032539039542
plot_id,batch_id 0 71 miss% 0.03585493508678841
plot_id,batch_id 0 72 miss% 0.061284694039500126
plot_id,batch_id 0 73 miss% 0.05927372742827003
plot_id,batch_id 0 74 miss% 0.10333417627980881
plot_id,batch_id 0 75 miss% 0.02943178110795038
plot_id,batch_id 0 76 miss% 0.0517528041719592
plot_id,batch_id 0 77 miss% 0.05506159766909036
plot_id,batch_id 0 78 miss% 0.024815302147262165
plot_id,batch_id 0 79 miss% 0.04118451092174771
plot_id,batch_id 0 80 miss% 0.0848521947352504
plot_id,batch_id 0 81 miss% 0.0783465582581714
plot_id,batch_id 0 82 miss% 0.048293468523613374
plot_id,batch_id 0 83 miss% 0.05640762322936526
plot_id,batch_id 0 84 miss% 0.059369067216161545
plot_id,batch_id 0 85 miss% 0.04889105580238042
plot_id,batch_id 0 86 miss% 0.029924697627308258
plot_id,batch_id 0 87 miss% 0.06823924715533009
plot_id,batch_id 0 88 miss% 0.07035576116050883
plot_id,batch_id 0 89 miss% 0.0554382889332559
plot_id,batch_id 0 90 miss% 0.04672719878567613
plot_id,batch_id 0 91 miss% 0.05650892296595247
plot_id,batch_id 0 92 miss% 0.04395285539484556
plot_id,batch_id 0 93 miss% 0.045455101825965245
plot_id,batch_id 0 94 miss% 0.10766960472923004
plot_id,batch_id 0 95 miss% 0.04544017626169877
plot_id,batch_id 0 96 miss% 0.0458591448770039
plot_id,batch_id 0 97 miss% 0.047157484786459265
plot_id,batch_id 0 98 miss% 0.040171442178821945
plot_id,batch_id 0 99 miss% 0.08642576792275197
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07685345 0.06021293 0.1177796  0.03829669 0.03091629 0.06534637
 0.03155742 0.10513345 0.07593002 0.06055719 0.03135377 0.05566385
 0.12322817 0.04826531 0.07960081 0.03981999 0.1929175  0.07896986
 0.07360689 0.08671477 0.13706373 0.05777396 0.04104727 0.02936644
 0.02200845 0.06030752 0.05918956 0.05239294 0.03147276 0.02320908
 0.04333141 0.06554563 0.08314408 0.05821351 0.03351145 0.05395786
 0.07342764 0.0710016  0.03077617 0.0210705  0.05910582 0.03661076
 0.02773035 0.06462459 0.02675433 0.02992275 0.02352605 0.0280461
 0.02916954 0.02018164 0.15998866 0.03304658 0.03986081 0.0155352
 0.05742374 0.06092796 0.08552011 0.03426964 0.03014612 0.02943344
 0.02959677 0.02898287 0.04884489 0.03797485 0.0640357  0.1058828
 0.05478955 0.01903219 0.05675686 0.06691178 0.04133033 0.03585494
 0.06128469 0.05927373 0.10333418 0.02943178 0.0517528  0.0550616
 0.0248153  0.04118451 0.08485219 0.07834656 0.04829347 0.05640762
 0.05936907 0.04889106 0.0299247  0.06823925 0.07035576 0.05543829
 0.0467272  0.05650892 0.04395286 0.0454551  0.1076696  0.04544018
 0.04585914 0.04715748 0.04017144 0.08642577]
for model  232 the mean error 0.05593977851954451
all id 232 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 31 out win 5 err 0.05593977851954451 time 22116.626256465912
Launcher: Job 233 completed in 22359 seconds.
Launcher: Task 20 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  265233
Epoch:0, Train loss:0.351839, valid loss:0.328640
Epoch:1, Train loss:0.072816, valid loss:0.002071
Epoch:2, Train loss:0.003584, valid loss:0.001320
Epoch:3, Train loss:0.002721, valid loss:0.001193
Epoch:4, Train loss:0.002323, valid loss:0.001026
Epoch:5, Train loss:0.002056, valid loss:0.000961
Epoch:6, Train loss:0.001801, valid loss:0.000772
Epoch:7, Train loss:0.001631, valid loss:0.000751
Epoch:8, Train loss:0.001497, valid loss:0.000794
Epoch:9, Train loss:0.001414, valid loss:0.000774
Epoch:10, Train loss:0.001332, valid loss:0.000704
Epoch:11, Train loss:0.000974, valid loss:0.000645
Epoch:12, Train loss:0.000925, valid loss:0.000604
Epoch:13, Train loss:0.000903, valid loss:0.000596
Epoch:14, Train loss:0.000887, valid loss:0.000548
Epoch:15, Train loss:0.000837, valid loss:0.000598
Epoch:16, Train loss:0.000821, valid loss:0.000527
Epoch:17, Train loss:0.000778, valid loss:0.000533
Epoch:18, Train loss:0.000774, valid loss:0.000556
Epoch:19, Train loss:0.000742, valid loss:0.000557
Epoch:20, Train loss:0.000723, valid loss:0.000514
Epoch:21, Train loss:0.000565, valid loss:0.000470
Epoch:22, Train loss:0.000545, valid loss:0.000474
Epoch:23, Train loss:0.000535, valid loss:0.000454
Epoch:24, Train loss:0.000522, valid loss:0.000485
Epoch:25, Train loss:0.000507, valid loss:0.000551
Epoch:26, Train loss:0.000515, valid loss:0.000456
Epoch:27, Train loss:0.000487, valid loss:0.000441
Epoch:28, Train loss:0.000497, valid loss:0.000440
Epoch:29, Train loss:0.000491, valid loss:0.000444
Epoch:30, Train loss:0.000471, valid loss:0.000468
Epoch:31, Train loss:0.000391, valid loss:0.000417
Epoch:32, Train loss:0.000391, valid loss:0.000421
Epoch:33, Train loss:0.000384, valid loss:0.000412
Epoch:34, Train loss:0.000383, valid loss:0.000435
Epoch:35, Train loss:0.000375, valid loss:0.000423
Epoch:36, Train loss:0.000364, valid loss:0.000403
Epoch:37, Train loss:0.000368, valid loss:0.000423
Epoch:38, Train loss:0.000364, valid loss:0.000408
Epoch:39, Train loss:0.000359, valid loss:0.000422
Epoch:40, Train loss:0.000357, valid loss:0.000427
Epoch:41, Train loss:0.000320, valid loss:0.000394
Epoch:42, Train loss:0.000317, valid loss:0.000417
Epoch:43, Train loss:0.000313, valid loss:0.000404
Epoch:44, Train loss:0.000310, valid loss:0.000400
Epoch:45, Train loss:0.000312, valid loss:0.000423
Epoch:46, Train loss:0.000308, valid loss:0.000418
Epoch:47, Train loss:0.000307, valid loss:0.000406
Epoch:48, Train loss:0.000303, valid loss:0.000408
Epoch:49, Train loss:0.000302, valid loss:0.000419
Epoch:50, Train loss:0.000300, valid loss:0.000424
Epoch:51, Train loss:0.000286, valid loss:0.000409
Epoch:52, Train loss:0.000283, valid loss:0.000405
Epoch:53, Train loss:0.000281, valid loss:0.000400
Epoch:54, Train loss:0.000281, valid loss:0.000399
Epoch:55, Train loss:0.000280, valid loss:0.000403
Epoch:56, Train loss:0.000279, valid loss:0.000401
Epoch:57, Train loss:0.000279, valid loss:0.000400
Epoch:58, Train loss:0.000279, valid loss:0.000398
Epoch:59, Train loss:0.000278, valid loss:0.000395
Epoch:60, Train loss:0.000279, valid loss:0.000398
training time 22908.269745111465
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.2822678215548995
plot_id,batch_id 0 1 miss% 0.3410645910998304
plot_id,batch_id 0 2 miss% 0.41495872857334815
plot_id,batch_id 0 3 miss% 0.32976712335753766
plot_id,batch_id 0 4 miss% 0.3187404725632343
plot_id,batch_id 0 5 miss% 0.2721361648012562
plot_id,batch_id 0 6 miss% 0.30658186657234665
plot_id,batch_id 0 7 miss% 0.49339079984477713
plot_id,batch_id 0 8 miss% 0.6419639551027164
plot_id,batch_id 0 9 miss% 0.4442833856592135
plot_id,batch_id 0 10 miss% 0.2416083007073005
plot_id,batch_id 0 11 miss% 0.2913767746871082
plot_id,batch_id 0 12 miss% 0.37747945291559803
plot_id,batch_id 0 13 miss% 0.3276604537288379
plot_id,batch_id 0 14 miss% 0.42311276293162386
plot_id,batch_id 0 15 miss% 0.2456547641887322
plot_id,batch_id 0 16 miss% 0.38800629673312215
plot_id,batch_id 0 17 miss% 0.36968921085876405
plot_id,batch_id 0 18 miss% 0.41578426625407666
plot_id,batch_id 0 19 miss% 0.3879725510055553
plot_id,batch_id 0 20 miss% 0.31761379451618366
plot_id,batch_id 0 21 miss% 0.38888196901617944
plot_id,batch_id 0 22 miss% 0.4375176177606887
plot_id,batch_id 0 23 miss% 0.43201360594651383
plot_id,batch_id 0 24 miss% 0.3881810191917744
plot_id,batch_id 0 25 miss% 0.2830871809563991
plot_id,batch_id 0 26 miss% 0.3583128590943537
plot_id,batch_id 0 27 miss% 0.365617292674487
plot_id,batch_id 0 28 miss% 0.4647639309290368
plot_id,batch_id 0 29 miss% 0.4305198151802806
plot_id,batch_id 0 30 miss% 0.276138041179694
plot_id,batch_id 0 31 miss% 0.4824108151513753
plot_id,batch_id 0 32 miss% 0.43254426579012656
plot_id,batch_id 0 33 miss% 0.5251840793915697
plot_id,batch_id 0 34 miss% 0.4063191321149906
plot_id,batch_id 0 35 miss% 0.23870818776883962
plot_id,batch_id 0 36 miss% 0.5265217169222578
plot_id,batch_id 0 37 miss% 0.38757321865184124
plot_id,batch_id 0 38 miss% 0.46746332433723886
plot_id,batch_id 0 39 miss% 0.4541269875768375
plot_id,batch_id 0 40 miss% 0.447149622458565
plot_id,batch_id 0 41 miss% 0.41892795990860277
plot_id,batch_id 0 42 miss% 0.39125242501893237
plot_id,batch_id 0 43 miss% 0.3554336510278932
plot_id,batch_id 0 44 miss% 0.33249368547581293
plot_id,batch_id 0 45 miss% 0.3726283384235588
plot_id,batch_id 0 46 miss% 0.3844477300847891
plot_id,batch_id 0 47 miss% 0.4674262076194702
plot_id,batch_id 0 48 miss% 0.44035079378874364
plot_id,batch_id 0 49 miss% 0.2855479347289426
plot_id,batch_id 0 50 miss% 0.46500876735894514
plot_id,batch_id 0 51 miss% 0.4897252810501175
plot_id,batch_id 0 52 miss% 0.47812034982517715
plot_id,batch_id 0 53 miss% 0.3670807379454609
plot_id,batch_id 0 54 miss% 0.3531848989810854
plot_id,batch_id 0 55 miss% 0.4844879324724696
plot_id,batch_id 0 56 miss% 0.5335328996779332
plot_id,batch_id 0 57 miss% 0.49304286987428037
plot_id,batch_id 0 58 miss% 0.4836518199035775
plot_id,batch_id 0 59 miss% 0.4821997748420564
plot_id,batch_id 0 60 miss% 0.22925009306083965
plot_id,batch_id 0 61 miss% 0.26374873532270615
plot_id,batch_id 0 62 miss% 0.4579687089316792
plot_id,batch_id 0 63 miss% 0.317908044965004
plot_id,batch_id 0 64 miss% 0.34074058905272653
plot_id,batch_id 0 65 miss% 0.36755927652671955
plot_id,batch_id 0 66 miss% 0.3229144848340294
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  265233
Epoch:0, Train loss:0.351839, valid loss:0.328640
Epoch:1, Train loss:0.065311, valid loss:0.001847
Epoch:2, Train loss:0.003395, valid loss:0.001328
Epoch:3, Train loss:0.002680, valid loss:0.001211
Epoch:4, Train loss:0.002328, valid loss:0.001173
Epoch:5, Train loss:0.002059, valid loss:0.001046
Epoch:6, Train loss:0.001818, valid loss:0.000889
Epoch:7, Train loss:0.001669, valid loss:0.000857
Epoch:8, Train loss:0.001517, valid loss:0.000757
Epoch:9, Train loss:0.001433, valid loss:0.000907
Epoch:10, Train loss:0.001363, valid loss:0.000666
Epoch:11, Train loss:0.000964, valid loss:0.000607
Epoch:12, Train loss:0.000917, valid loss:0.000623
Epoch:13, Train loss:0.000904, valid loss:0.000599
Epoch:14, Train loss:0.000900, valid loss:0.000546
Epoch:15, Train loss:0.000842, valid loss:0.000534
Epoch:16, Train loss:0.000833, valid loss:0.000610
Epoch:17, Train loss:0.000784, valid loss:0.000509
Epoch:18, Train loss:0.000790, valid loss:0.000540
Epoch:19, Train loss:0.000743, valid loss:0.000521
Epoch:20, Train loss:0.000748, valid loss:0.000536
Epoch:21, Train loss:0.000551, valid loss:0.000470
Epoch:22, Train loss:0.000534, valid loss:0.000445
Epoch:23, Train loss:0.000527, valid loss:0.000496
Epoch:24, Train loss:0.000525, valid loss:0.000470
Epoch:25, Train loss:0.000497, valid loss:0.000475
Epoch:26, Train loss:0.000503, valid loss:0.000420
Epoch:27, Train loss:0.000489, valid loss:0.000450
Epoch:28, Train loss:0.000489, valid loss:0.000409
Epoch:29, Train loss:0.000479, valid loss:0.000416
Epoch:30, Train loss:0.000478, valid loss:0.000391
Epoch:31, Train loss:0.000379, valid loss:0.000401
Epoch:32, Train loss:0.000372, valid loss:0.000403
Epoch:33, Train loss:0.000371, valid loss:0.000369
Epoch:34, Train loss:0.000373, valid loss:0.000372
Epoch:35, Train loss:0.000361, valid loss:0.000385
Epoch:36, Train loss:0.000360, valid loss:0.000373
Epoch:37, Train loss:0.000365, valid loss:0.000386
Epoch:38, Train loss:0.000358, valid loss:0.000409
Epoch:39, Train loss:0.000346, valid loss:0.000391
Epoch:40, Train loss:0.000345, valid loss:0.000420
Epoch:41, Train loss:0.000312, valid loss:0.000373
Epoch:42, Train loss:0.000300, valid loss:0.000378
Epoch:43, Train loss:0.000300, valid loss:0.000386
Epoch:44, Train loss:0.000299, valid loss:0.000386
Epoch:45, Train loss:0.000297, valid loss:0.000373
Epoch:46, Train loss:0.000297, valid loss:0.000377
Epoch:47, Train loss:0.000291, valid loss:0.000371
Epoch:48, Train loss:0.000290, valid loss:0.000369
Epoch:49, Train loss:0.000287, valid loss:0.000389
Epoch:50, Train loss:0.000286, valid loss:0.000379
Epoch:51, Train loss:0.000276, valid loss:0.000372
Epoch:52, Train loss:0.000273, valid loss:0.000367
Epoch:53, Train loss:0.000272, valid loss:0.000367
Epoch:54, Train loss:0.000271, valid loss:0.000364
Epoch:55, Train loss:0.000270, valid loss:0.000366
Epoch:56, Train loss:0.000269, valid loss:0.000367
Epoch:57, Train loss:0.000269, valid loss:0.000367
Epoch:58, Train loss:0.000268, valid loss:0.000363
Epoch:59, Train loss:0.000268, valid loss:0.000363
Epoch:60, Train loss:0.000268, valid loss:0.000363
training time 22934.404978513718
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.22886832430156467
plot_id,batch_id 0 1 miss% 0.3560080033816679
plot_id,batch_id 0 2 miss% 0.4517197538878602
plot_id,batch_id 0 3 miss% 0.3447400206951905
plot_id,batch_id 0 4 miss% 0.27478444554636033
plot_id,batch_id 0 5 miss% 0.32289069337307197
plot_id,batch_id 0 6 miss% 0.3075462749309579
plot_id,batch_id 0 7 miss% 0.4348838835595953
plot_id,batch_id 0 8 miss% 0.4525596872274005
plot_id,batch_id 0 9 miss% 0.40222016052864124
plot_id,batch_id 0 10 miss% 0.2834845231955992
plot_id,batch_id 0 11 miss% 0.2687082862017421
plot_id,batch_id 0 12 miss% 0.3124404658204526
plot_id,batch_id 0 13 miss% 0.2753073239854793
plot_id,batch_id 0 14 miss% 0.42156233961379813
plot_id,batch_id 0 15 miss% 0.3154807917991379
plot_id,batch_id 0 16 miss% 0.4045950903113589
plot_id,batch_id 0 17 miss% 0.3473743887761155
plot_id,batch_id 0 18 miss% 0.43519668870595907
plot_id,batch_id 0 19 miss% 0.37786245353058084
plot_id,batch_id 0 20 miss% 0.30592570724903995
plot_id,batch_id 0 21 miss% 0.46281303485576003
plot_id,batch_id 0 22 miss% 0.36715901105038745
plot_id,batch_id 0 23 miss% 0.44030689421122676
plot_id,batch_id 0 24 miss% 0.32539347107786715
plot_id,batch_id 0 25 miss% 0.262270357779286
plot_id,batch_id 0 26 miss% 0.29766662357512763
plot_id,batch_id 0 27 miss% 0.3206066801890557
plot_id,batch_id 0 28 miss% 0.47671720055015365
plot_id,batch_id 0 29 miss% 0.344699176561181
plot_id,batch_id 0 30 miss% 0.2982682991956891
plot_id,batch_id 0 31 miss% 0.36455851131975936
plot_id,batch_id 0 32 miss% 0.3835385294718357
plot_id,batch_id 0 33 miss% 0.4497051472302241
plot_id,batch_id 0 34 miss% 0.3271636069541865
plot_id,batch_id 0 35 miss% 0.2696453750162625
plot_id,batch_id 0 36 miss% 0.42977744243357024
plot_id,batch_id 0 37 miss% 0.3402555385880861
plot_id,batch_id 0 38 miss% 0.3555846202638038
plot_id,batch_id 0 39 miss% 0.4343033629853354
plot_id,batch_id 0 40 miss% 0.2721856002435485
plot_id,batch_id 0 41 miss% 0.40048595665366793
plot_id,batch_id 0 42 miss% 0.2476242585904104
plot_id,batch_id 0 43 miss% 0.32902204118908773
plot_id,batch_id 0 44 miss% 0.3319755256760909
plot_id,batch_id 0 45 miss% 0.3128906427077814
plot_id,batch_id 0 46 miss% 0.3905431605425301
plot_id,batch_id 0 47 miss% 0.3555236118131741
plot_id,batch_id 0 48 miss% 0.4009668959508884
plot_id,batch_id 0 49 miss% 0.27766873188428304
plot_id,batch_id 0 50 miss% 0.5685897695357333
plot_id,batch_id 0 51 miss% 0.35010477279564084
plot_id,batch_id 0 52 miss% 0.4079762714469862
plot_id,batch_id 0 53 miss% 0.2844214316127801
plot_id,batch_id 0 54 miss% 0.3041058656139254
plot_id,batch_id 0 55 miss% 0.3696891560533056
plot_id,batch_id 0 56 miss% 0.4844902105363639
plot_id,batch_id 0 57 miss% 0.4452506689159736
plot_id,batch_id 0 58 miss% 0.334673512501759
plot_id,batch_id 0 59 miss% 0.4526551437832098
plot_id,batch_id 0 60 miss% 0.20475669147024458
plot_id,batch_id 0 61 miss% 0.2323433930411032
plot_id,batch_id 0 62 miss% 0.43532762947490933
plot_id,batch_id 0 63 miss% 0.39499173814312466
plot_id,batch_id 0 64 miss% 0.3645491166499054
plot_id,batch_id 0 65 miss% 0.2641221139634554
plot_id,batch_id 0 66 miss% 0.30001312944139386
plot_id,batch_id 0 67 miss% 0.22756344381050034
plot_id,batch_id 0 68 miss% 0.3919683423194877
plot_id,batch_id 0 69 miss% 0.34024443064971555
plot_id,batch_id 0 70 miss% 0.21922042401630648
plot_id,batch_id 0 71 miss% 0.338233872396514
plot_id,batch_id 0 72 miss% 0.33508367007727646
plot_id,batch_id 0 73 miss% 0.2865102143485766
plot_id,batch_id 0 74 miss% 0.32669843856461495
plot_id,batch_id 0 75 miss% 0.22813006853310738
plot_id,batch_id 0 76 miss% 0.28638816606259393
plot_id,batch_id 0 77 miss% 0.22419918406265654
plot_id,batch_id 0 78 miss% 0.3004742112306398
plot_id,batch_id 0 79 miss% 0.3177266704639964
plot_id,batch_id 0 80 miss% 0.23475843658443266
plot_id,batch_id 0 81 miss% 0.40753116037529785
plot_id,batch_id 0 82 miss% 0.3029780567464159
plot_id,batch_id 0 83 miss% 0.4270872964253007
plot_id,batch_id 0 84 miss% 0.33563580950913297
plot_id,batch_id 0 85 miss% 0.22573974323806148
plot_id,batch_id 0 86 miss% 0.2860309662640162
plot_id,batch_id 0 87 miss% 0.3593335584032531
plot_id,batch_id 0 88 miss% 0.36259495461810615
plot_id,batch_id 0 89 miss% 0.35081323512691076
plot_id,batch_id 0 90 miss% 0.1883371417822005
plot_id,batch_id 0 91 miss% 0.28383491635762786
plot_id,batch_id 0 92 miss% 0.31789519036008684
plot_id,batch_id 0 93 miss% 0.25998935314689525
plot_id,batch_id 0 94 miss% 0.3493733698004
plot_id,batch_id 0 95 miss% 0.16318945108494115
plot_id,batch_id 0 96 miss% 0.23448128193641743
plot_id,batch_id 0 97 miss% 0.3340231126038865
plot_id,batch_id 0 98 miss% 0.3077998343646628
plot_id,batch_id 0 99 miss% 0.29913567704853056
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.28226782 0.34106459 0.41495873 0.32976712 0.31874047 0.27213616
 0.30658187 0.4933908  0.64196396 0.44428339 0.2416083  0.29137677
 0.37747945 0.32766045 0.42311276 0.24565476 0.3880063  0.36968921
 0.41578427 0.38797255 0.31761379 0.38888197 0.43751762 0.43201361
 0.38818102 0.28308718 0.35831286 0.36561729 0.46476393 0.43051982
 0.27613804 0.48241082 0.43254427 0.52518408 0.40631913 0.23870819
 0.52652172 0.38757322 0.46746332 0.45412699 0.44714962 0.41892796
 0.39125243 0.35543365 0.33249369 0.37262834 0.38444773 0.46742621
 0.44035079 0.28554793 0.46500877 0.48972528 0.47812035 0.36708074
 0.3531849  0.48448793 0.5335329  0.49304287 0.48365182 0.48219977
 0.22925009 0.26374874 0.45796871 0.31790804 0.34074059 0.36755928
 0.32291448 0.22756344 0.39196834 0.34024443 0.21922042 0.33823387
 0.33508367 0.28651021 0.32669844 0.22813007 0.28638817 0.22419918
 0.30047421 0.31772667 0.23475844 0.40753116 0.30297806 0.4270873
 0.33563581 0.22573974 0.28603097 0.35933356 0.36259495 0.35081324
 0.18833714 0.28383492 0.31789519 0.25998935 0.34937337 0.16318945
 0.23448128 0.33402311 0.30779983 0.29913568]
for model  187 the mean error 0.3605578589276323
all id 187 hidden_dim 32 learning_rate 0.0025 num_layers 5 frames 31 out win 5 err 0.3605578589276323 time 22908.269745111465
Launcher: Job 188 completed in 23133 seconds.
Launcher: Task 167 done. Exiting.
plot_id,batch_id 0 67 miss% 0.252017756797686
plot_id,batch_id 0 68 miss% 0.3644110800401377
plot_id,batch_id 0 69 miss% 0.36712181552535933
plot_id,batch_id 0 70 miss% 0.2154011064238059
plot_id,batch_id 0 71 miss% 0.29654516565055483
plot_id,batch_id 0 72 miss% 0.39916116122100587
plot_id,batch_id 0 73 miss% 0.29859901682517614
plot_id,batch_id 0 74 miss% 0.32895489361288266
plot_id,batch_id 0 75 miss% 0.21575282103010082
plot_id,batch_id 0 76 miss% 0.2946946509469885
plot_id,batch_id 0 77 miss% 0.2669433130420459
plot_id,batch_id 0 78 miss% 0.3016258555692847
plot_id,batch_id 0 79 miss% 0.31938064551871875
plot_id,batch_id 0 80 miss% 0.2099381839706425
plot_id,batch_id 0 81 miss% 0.38323268135327604
plot_id,batch_id 0 82 miss% 0.3414350892774307
plot_id,batch_id 0 83 miss% 0.3734474449260756
plot_id,batch_id 0 84 miss% 0.3049223378761752
plot_id,batch_id 0 85 miss% 0.21543692130608053
plot_id,batch_id 0 86 miss% 0.3497089089076889
plot_id,batch_id 0 87 miss% 0.37465961896607614
plot_id,batch_id 0 88 miss% 0.35520548496611687
plot_id,batch_id 0 89 miss% 0.3511615540319719
plot_id,batch_id 0 90 miss% 0.21955649488986803
plot_id,batch_id 0 91 miss% 0.2698328944914149
plot_id,batch_id 0 92 miss% 0.27471956728753155
plot_id,batch_id 0 93 miss% 0.2693553625816177
plot_id,batch_id 0 94 miss% 0.4510301384196103
plot_id,batch_id 0 95 miss% 0.23196508500373678
plot_id,batch_id 0 96 miss% 0.27010519625804824
plot_id,batch_id 0 97 miss% 0.3891051132752869
plot_id,batch_id 0 98 miss% 0.35338104015400545
plot_id,batch_id 0 99 miss% 0.31668142594372756
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.22886832 0.356008   0.45171975 0.34474002 0.27478445 0.32289069
 0.30754627 0.43488388 0.45255969 0.40222016 0.28348452 0.26870829
 0.31244047 0.27530732 0.42156234 0.31548079 0.40459509 0.34737439
 0.43519669 0.37786245 0.30592571 0.46281303 0.36715901 0.44030689
 0.32539347 0.26227036 0.29766662 0.32060668 0.4767172  0.34469918
 0.2982683  0.36455851 0.38353853 0.44970515 0.32716361 0.26964538
 0.42977744 0.34025554 0.35558462 0.43430336 0.2721856  0.40048596
 0.24762426 0.32902204 0.33197553 0.31289064 0.39054316 0.35552361
 0.4009669  0.27766873 0.56858977 0.35010477 0.40797627 0.28442143
 0.30410587 0.36968916 0.48449021 0.44525067 0.33467351 0.45265514
 0.20475669 0.23234339 0.43532763 0.39499174 0.36454912 0.26412211
 0.30001313 0.25201776 0.36441108 0.36712182 0.21540111 0.29654517
 0.39916116 0.29859902 0.32895489 0.21575282 0.29469465 0.26694331
 0.30162586 0.31938065 0.20993818 0.38323268 0.34143509 0.37344744
 0.30492234 0.21543692 0.34970891 0.37465962 0.35520548 0.35116155
 0.21955649 0.26983289 0.27471957 0.26935536 0.45103014 0.23196509
 0.2701052  0.38910511 0.35338104 0.31668143]
for model  214 the mean error 0.34017029056251774
all id 214 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 31 out win 5 err 0.34017029056251774 time 22934.404978513718
Launcher: Job 215 completed in 23159 seconds.
Launcher: Task 198 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 4 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 57600
total number of trained parameters  265233
Epoch:0, Train loss:0.363305, valid loss:0.341393
Epoch:1, Train loss:0.233018, valid loss:0.232931
Epoch:2, Train loss:0.226662, valid loss:0.232887
Epoch:3, Train loss:0.226135, valid loss:0.232411
Epoch:4, Train loss:0.225945, valid loss:0.232351
Epoch:5, Train loss:0.225752, valid loss:0.232484
Epoch:6, Train loss:0.225627, valid loss:0.232200
Epoch:7, Train loss:0.225504, valid loss:0.232087
Epoch:8, Train loss:0.225451, valid loss:0.232247
Epoch:9, Train loss:0.225355, valid loss:0.232140
Epoch:10, Train loss:0.225316, valid loss:0.232161
Epoch:11, Train loss:0.224956, valid loss:0.231994
Epoch:12, Train loss:0.224911, valid loss:0.231982
Epoch:13, Train loss:0.224901, valid loss:0.231990
Epoch:14, Train loss:0.224867, valid loss:0.232061
Epoch:15, Train loss:0.224843, valid loss:0.231944
Epoch:16, Train loss:0.224831, valid loss:0.231907
Epoch:17, Train loss:0.224804, valid loss:0.231977
Epoch:18, Train loss:0.224813, valid loss:0.231909
Epoch:19, Train loss:0.224780, valid loss:0.231917
Epoch:20, Train loss:0.224760, valid loss:0.232118
Epoch:21, Train loss:0.224608, valid loss:0.231831
Epoch:22, Train loss:0.224585, valid loss:0.231830
Epoch:23, Train loss:0.224570, valid loss:0.231925
Epoch:24, Train loss:0.224567, valid loss:0.231864
Epoch:25, Train loss:0.224547, valid loss:0.231960
Epoch:26, Train loss:0.224543, valid loss:0.231854
Epoch:27, Train loss:0.224540, valid loss:0.231859
Epoch:28, Train loss:0.224545, valid loss:0.231817
Epoch:29, Train loss:0.224557, valid loss:0.231863
Epoch:30, Train loss:0.224509, valid loss:0.231858
Epoch:31, Train loss:0.224435, valid loss:0.231810
Epoch:32, Train loss:0.224426, valid loss:0.231831
Epoch:33, Train loss:0.224430, valid loss:0.231811
Epoch:34, Train loss:0.224422, valid loss:0.231819
Epoch:35, Train loss:0.224418, valid loss:0.231840
Epoch:36, Train loss:0.224419, valid loss:0.231811
Epoch:37, Train loss:0.224417, valid loss:0.231819
Epoch:38, Train loss:0.224410, valid loss:0.231790
Epoch:39, Train loss:0.224408, valid loss:0.231797
Epoch:40, Train loss:0.224409, valid loss:0.231889
Epoch:41, Train loss:0.224373, valid loss:0.231798
Epoch:42, Train loss:0.224366, valid loss:0.231805
Epoch:43, Train loss:0.224366, valid loss:0.231806
Epoch:44, Train loss:0.224363, valid loss:0.231808
Epoch:45, Train loss:0.224361, valid loss:0.231806
Epoch:46, Train loss:0.224359, valid loss:0.231810
Epoch:47, Train loss:0.224361, valid loss:0.231808
Epoch:48, Train loss:0.224354, valid loss:0.231814
Epoch:49, Train loss:0.224355, valid loss:0.231797
Epoch:50, Train loss:0.224355, valid loss:0.231829
Epoch:51, Train loss:0.224347, valid loss:0.231806
Epoch:52, Train loss:0.224344, valid loss:0.231802
Epoch:53, Train loss:0.224343, valid loss:0.231802
Epoch:54, Train loss:0.224343, valid loss:0.231801
Epoch:55, Train loss:0.224342, valid loss:0.231802
Epoch:56, Train loss:0.224342, valid loss:0.231800
Epoch:57, Train loss:0.224342, valid loss:0.231800
Epoch:58, Train loss:0.224342, valid loss:0.231802
Epoch:59, Train loss:0.224341, valid loss:0.231800
Epoch:60, Train loss:0.224341, valid loss:0.231799
training time 23078.787751674652
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.13333333333333333
nondim time 0.26666666666666666
nondim time 0.4
nondim time 0.5333333333333333
nondim time 0.6666666666666666
nondim time 0.8
nondim time 0.9333333333333333
plot_id,batch_id 0 0 miss% 0.9606421176552049
plot_id,batch_id 0 1 miss% 0.9616817981762052
plot_id,batch_id 0 2 miss% 0.9645190406980149
plot_id,batch_id 0 3 miss% 0.9639942582456338
plot_id,batch_id 0 4 miss% 0.9647021621655233
plot_id,batch_id 0 5 miss% 0.9667912393405786
plot_id,batch_id 0 6 miss% 0.9622798681313478
plot_id,batch_id 0 7 miss% 0.9672247742564787
plot_id,batch_id 0 8 miss% 0.9645846974799505
plot_id,batch_id 0 9 miss% 0.9670038058236358
plot_id,batch_id 0 10 miss% 0.9560507023940289
plot_id,batch_id 0 11 miss% 0.9611584083938692
plot_id,batch_id 0 12 miss% 0.963531738110306
plot_id,batch_id 0 13 miss% 0.9642228756720891
plot_id,batch_id 0 14 miss% 0.9636932246667709
plot_id,batch_id 0 15 miss% 0.9569427787224787
plot_id,batch_id 0 16 miss% 0.9642059947950252
plot_id,batch_id 0 17 miss% 0.9695514992655176
plot_id,batch_id 0 18 miss% 0.9684992069361037
plot_id,batch_id 0 19 miss% 0.9671862580326092
plot_id,batch_id 0 20 miss% 0.9627743996238229
plot_id,batch_id 0 21 miss% 0.9669971260743639
plot_id,batch_id 0 22 miss% 0.9660443130154026
plot_id,batch_id 0 23 miss% 0.9648191416332857
plot_id,batch_id 0 24 miss% 0.9647604459998587
plot_id,batch_id 0 25 miss% 0.9705885349930566
plot_id,batch_id 0 26 miss% 0.9654753995726026
plot_id,batch_id 0 27 miss% 0.9653660757506749
plot_id,batch_id 0 28 miss% 0.973822802858745
plot_id,batch_id 0 29 miss% 0.9664216922841208
plot_id,batch_id 0 30 miss% 0.9569382883394832
plot_id,batch_id 0 31 miss% 0.967283112159467
plot_id,batch_id 0 32 miss% 0.9662531003391541
plot_id,batch_id 0 33 miss% 0.9674313185903195
plot_id,batch_id 0 34 miss% 0.9652307202645429
plot_id,batch_id 0 35 miss% 0.9576284174544284
plot_id,batch_id 0 36 miss% 0.9614181667058486
plot_id,batch_id 0 37 miss% 0.9676955969796157
plot_id,batch_id 0 38 miss% 0.9675987134619939
plot_id,batch_id 0 39 miss% 0.9685841066209899
plot_id,batch_id 0 40 miss% 0.9621260973678225
plot_id,batch_id 0 41 miss% 0.9655943214601327
plot_id,batch_id 0 42 miss% 0.9646103654108539
plot_id,batch_id 0 43 miss% 0.9656958836157488
plot_id,batch_id 0 44 miss% 0.9654310478322592
plot_id,batch_id 0 45 miss% 0.9644505158151321
plot_id,batch_id 0 46 miss% 0.9702943364175971
plot_id,batch_id 0 47 miss% 0.9656674598848031
plot_id,batch_id 0 48 miss% 0.9679515552200216
plot_id,batch_id 0 49 miss% 0.9649886482024672
plot_id,batch_id 0 50 miss% 0.9705091492146979
plot_id,batch_id 0 51 miss% 0.9678187446443172
plot_id,batch_id 0 52 miss% 0.9676095068931468
plot_id,batch_id 0 53 miss% 0.966501504082118
plot_id,batch_id 0 54 miss% 0.9692232846214608
plot_id,batch_id 0 55 miss% 0.9628725466707416
plot_id,batch_id 0 56 miss% 0.9704784201211096
plot_id,batch_id 0 57 miss% 0.9732274900387735
plot_id,batch_id 0 58 miss% 0.967566895748015
plot_id,batch_id 0 59 miss% 0.9667643081653076
plot_id,batch_id 0 60 miss% 0.9610757464665622
plot_id,batch_id 0 61 miss% 0.9606569412113922
plot_id,batch_id 0 62 miss% 0.9631220771115443
plot_id,batch_id 0 63 miss% 0.9633785606326887
plot_id,batch_id 0 64 miss% 0.9631950220015921
plot_id,batch_id 0 65 miss% 0.959215159909318
plot_id,batch_id 0 66 miss% 0.9715258708257454
plot_id,batch_id 0 67 miss% 0.9613202015017186
plot_id,batch_id 0 68 miss% 0.9619456989029118
plot_id,batch_id 0 69 miss% 0.9615009468388058
plot_id,batch_id 0 70 miss% 0.9500658720370609
plot_id,batch_id 0 71 miss% 0.9670009831605052
plot_id,batch_id 0 72 miss% 0.9618964997582408
plot_id,batch_id 0 73 miss% 0.9723768609386833
plot_id,batch_id 0 74 miss% 0.9615744008017694
plot_id,batch_id 0 75 miss% 0.9644148747164019
plot_id,batch_id 0 76 miss% 0.9586868672274579
plot_id,batch_id 0 77 miss% 0.9624018553014253
plot_id,batch_id 0 78 miss% 0.9617568998837929
plot_id,batch_id 0 79 miss% 0.9642580637326561
plot_id,batch_id 0 80 miss% 0.9627068609788915
plot_id,batch_id 0 81 miss% 0.9621752644695712
plot_id,batch_id 0 82 miss% 0.9641565997391253
plot_id,batch_id 0 83 miss% 0.9633886215549726
plot_id,batch_id 0 84 miss% 0.9637940187738185
plot_id,batch_id 0 85 miss% 0.9633823116493293
plot_id,batch_id 0 86 miss% 0.9594850794237562
plot_id,batch_id 0 87 miss% 0.9613037563735485
plot_id,batch_id 0 88 miss% 0.9654917738972195
plot_id,batch_id 0 89 miss% 0.9636241873043433
plot_id,batch_id 0 90 miss% 0.9659673277566454
plot_id,batch_id 0 91 miss% 0.9633600861940651
plot_id,batch_id 0 92 miss% 0.962730147839911
plot_id,batch_id 0 93 miss% 0.963225399253962
plot_id,batch_id 0 94 miss% 0.96397637384392
plot_id,batch_id 0 95 miss% 0.9645672200088423
plot_id,batch_id 0 96 miss% 0.9619512999731844
plot_id,batch_id 0 97 miss% 0.9623361820324158
plot_id,batch_id 0 98 miss% 0.9604415532505239
plot_id,batch_id 0 99 miss% 0.96399630947658
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.96064212 0.9616818  0.96451904 0.96399426 0.96470216 0.96679124
 0.96227987 0.96722477 0.9645847  0.96700381 0.9560507  0.96115841
 0.96353174 0.96422288 0.96369322 0.95694278 0.96420599 0.9695515
 0.96849921 0.96718626 0.9627744  0.96699713 0.96604431 0.96481914
 0.96476045 0.97058853 0.9654754  0.96536608 0.9738228  0.96642169
 0.95693829 0.96728311 0.9662531  0.96743132 0.96523072 0.95762842
 0.96141817 0.9676956  0.96759871 0.96858411 0.9621261  0.96559432
 0.96461037 0.96569588 0.96543105 0.96445052 0.97029434 0.96566746
 0.96795156 0.96498865 0.97050915 0.96781874 0.96760951 0.9665015
 0.96922328 0.96287255 0.97047842 0.97322749 0.9675669  0.96676431
 0.96107575 0.96065694 0.96312208 0.96337856 0.96319502 0.95921516
 0.97152587 0.9613202  0.9619457  0.96150095 0.95006587 0.96700098
 0.9618965  0.97237686 0.9615744  0.96441487 0.95868687 0.96240186
 0.9617569  0.96425806 0.96270686 0.96217526 0.9641566  0.96338862
 0.96379402 0.96338231 0.95948508 0.96130376 0.96549177 0.96362419
 0.96596733 0.96336009 0.96273015 0.9632254  0.96397637 0.96456722
 0.9619513  0.96233618 0.96044155 0.96399631]
for model  240 the mean error 0.9644440977986457
all id 240 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 31 out win 4 err 0.9644440977986457 time 23078.787751674652
Launcher: Job 241 completed in 23184 seconds.
Launcher: Task 84 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  209681
Epoch:0, Train loss:0.317336, valid loss:0.319458
Epoch:1, Train loss:0.016622, valid loss:0.002530
Epoch:2, Train loss:0.005176, valid loss:0.001620
Epoch:3, Train loss:0.004051, valid loss:0.001442
Epoch:4, Train loss:0.003060, valid loss:0.001056
Epoch:5, Train loss:0.002001, valid loss:0.000953
Epoch:6, Train loss:0.001847, valid loss:0.000968
Epoch:7, Train loss:0.001687, valid loss:0.001064
Epoch:8, Train loss:0.001576, valid loss:0.000775
Epoch:9, Train loss:0.001442, valid loss:0.000783
Epoch:10, Train loss:0.001391, valid loss:0.000707
Epoch:11, Train loss:0.001001, valid loss:0.000721
Epoch:12, Train loss:0.000956, valid loss:0.000604
Epoch:13, Train loss:0.000916, valid loss:0.000644
Epoch:14, Train loss:0.000884, valid loss:0.000660
Epoch:15, Train loss:0.000867, valid loss:0.000614
Epoch:16, Train loss:0.000836, valid loss:0.000750
Epoch:17, Train loss:0.000812, valid loss:0.000633
Epoch:18, Train loss:0.000806, valid loss:0.000574
Epoch:19, Train loss:0.000771, valid loss:0.000612
Epoch:20, Train loss:0.000745, valid loss:0.000628
Epoch:21, Train loss:0.000565, valid loss:0.000499
Epoch:22, Train loss:0.000546, valid loss:0.000498
Epoch:23, Train loss:0.000533, valid loss:0.000462
Epoch:24, Train loss:0.000540, valid loss:0.000472
Epoch:25, Train loss:0.000518, valid loss:0.000555
Epoch:26, Train loss:0.000505, valid loss:0.000476
Epoch:27, Train loss:0.000487, valid loss:0.000476
Epoch:28, Train loss:0.000499, valid loss:0.000529
Epoch:29, Train loss:0.000503, valid loss:0.000527
Epoch:30, Train loss:0.000493, valid loss:0.000478
Epoch:31, Train loss:0.000385, valid loss:0.000477
Epoch:32, Train loss:0.000383, valid loss:0.000438
Epoch:33, Train loss:0.000374, valid loss:0.000484
Epoch:34, Train loss:0.000374, valid loss:0.000450
Epoch:35, Train loss:0.000371, valid loss:0.000456
Epoch:36, Train loss:0.000359, valid loss:0.000451
Epoch:37, Train loss:0.000359, valid loss:0.000467
Epoch:38, Train loss:0.000351, valid loss:0.000444
Epoch:39, Train loss:0.000354, valid loss:0.000460
Epoch:40, Train loss:0.000354, valid loss:0.000447
Epoch:41, Train loss:0.000308, valid loss:0.000440
Epoch:42, Train loss:0.000305, valid loss:0.000438
Epoch:43, Train loss:0.000303, valid loss:0.000434
Epoch:44, Train loss:0.000303, valid loss:0.000440
Epoch:45, Train loss:0.000298, valid loss:0.000455
Epoch:46, Train loss:0.000299, valid loss:0.000445
Epoch:47, Train loss:0.000295, valid loss:0.000450
Epoch:48, Train loss:0.000292, valid loss:0.000438
Epoch:49, Train loss:0.000296, valid loss:0.000456
Epoch:50, Train loss:0.000294, valid loss:0.000441
Epoch:51, Train loss:0.000275, valid loss:0.000436
Epoch:52, Train loss:0.000272, valid loss:0.000436
Epoch:53, Train loss:0.000271, valid loss:0.000434
Epoch:54, Train loss:0.000270, valid loss:0.000436
Epoch:55, Train loss:0.000270, valid loss:0.000436
Epoch:56, Train loss:0.000269, valid loss:0.000435
Epoch:57, Train loss:0.000269, valid loss:0.000435
Epoch:58, Train loss:0.000269, valid loss:0.000435
Epoch:59, Train loss:0.000268, valid loss:0.000435
Epoch:60, Train loss:0.000268, valid loss:0.000434
training time 23039.485804080963
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07091609497763207
plot_id,batch_id 0 1 miss% 0.07204881559365646
plot_id,batch_id 0 2 miss% 0.0968420681796475
plot_id,batch_id 0 3 miss% 0.04236442278513624
plot_id,batch_id 0 4 miss% 0.06286619162340713
plot_id,batch_id 0 5 miss% 0.05596328972926728
plot_id,batch_id 0 6 miss% 0.04030194885521073
plot_id,batch_id 0 7 miss% 0.05896060266016889
plot_id,batch_id 0 8 miss% 0.056588599069268185
plot_id,batch_id 0 9 miss% 0.05402086639452895
plot_id,batch_id 0 10 miss% 0.02602382187629793
plot_id,batch_id 0 11 miss% 0.04387359047082422
plot_id,batch_id 0 12 miss% 0.06048526122966623
plot_id,batch_id 0 13 miss% 0.04422612071759023
plot_id,batch_id 0 14 miss% 0.06791245681654785
plot_id,batch_id 0 15 miss% 0.033854031437115954
plot_id,batch_id 0 16 miss% 0.08804536433317584
plot_id,batch_id 0 17 miss% 0.0313631471194751
plot_id,batch_id 0 18 miss% 0.06609376256406767
plot_id,batch_id 0 19 miss% 0.05168848519062006
plot_id,batch_id 0 20 miss% 0.07682101593063248
plot_id,batch_id 0 21 miss% 0.051346899848827995
plot_id,batch_id 0 22 miss% 0.04652813323839365
plot_id,batch_id 0 23 miss% 0.029793077119372846
plot_id,batch_id 0 24 miss% 0.07015113489291389
plot_id,batch_id 0 25 miss% 0.03070270009907401
plot_id,batch_id 0 26 miss% 0.05175490649359745
plot_id,batch_id 0 27 miss% 0.037529755465928995
plot_id,batch_id 0 28 miss% 0.03312670074848437
plot_id,batch_id 0 29 miss% 0.03213263297632184
plot_id,batch_id 0 30 miss% 0.05124789856248253
plot_id,batch_id 0 31 miss% 0.0594948965852003
plot_id,batch_id 0 32 miss% 0.08749143861364106
plot_id,batch_id 0 33 miss% 0.048949851297985174
plot_id,batch_id 0 34 miss% 0.03836559721925524
plot_id,batch_id 0 35 miss% 0.023585388890707718
plot_id,batch_id 0 36 miss% 0.09097675006844788
plot_id,batch_id 0 37 miss% 0.0594440624908381
plot_id,batch_id 0 38 miss% 0.06030721769505626
plot_id,batch_id 0 39 miss% 0.04204773918106022
plot_id,batch_id 0 40 miss% 0.06651262053737195
plot_id,batch_id 0 41 miss% 0.06664133276287916
plot_id,batch_id 0 42 miss% 0.06136408371234693
plot_id,batch_id 0 43 miss% 0.08456823194938135
plot_id,batch_id 0 44 miss% 0.04093098524874454
plot_id,batch_id 0 45 miss% 0.03656795635892137
plot_id,batch_id 0 46 miss% 0.034344609409873716
plot_id,batch_id 0 47 miss% 0.03585634782075645
plot_id,batch_id 0 48 miss% 0.021290295046815476
plot_id,batch_id 0 49 miss% 0.0461696884291246
plot_id,batch_id 0 50 miss% 0.13179162171786324
plot_id,batch_id 0 51 miss% 0.028967822898394977
plot_id,batch_id 0 52 miss% 0.02344781368955198
plot_id,batch_id 0 53 miss% 0.017776711779991232
plot_id,batch_id 0 54 miss% 0.0329936499412127
plot_id,batch_id 0 55 miss% 0.037983034991965925
plot_id,batch_id 0 56 miss% 0.0738252967157061
plot_id,batch_id 0 57 miss% 0.0354499323311644
plot_id,batch_id 0 58 miss% 0.022860359166047065
plot_id,batch_id 0 59 miss% 0.018077608075290786
plot_id,batch_id 0 60 miss% 0.028165660085979063
plot_id,batch_id 0 61 miss% 0.0235860550734923
plot_id,batch_id 0 62 miss% 0.05075332867697195
plot_id,batch_id 0 63 miss% 0.04704323316572845
plot_id,batch_id 0 64 miss% 0.06620165376055659
plot_id,batch_id 0 65 miss% 0.044992852051698375
plot_id,batch_id 0 66 miss% 0.025622461047039066
plot_id,batch_id 0 67 miss% the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  209681
Epoch:0, Train loss:0.317336, valid loss:0.319458
Epoch:1, Train loss:0.021618, valid loss:0.003327
Epoch:2, Train loss:0.009528, valid loss:0.002450
Epoch:3, Train loss:0.004423, valid loss:0.001411
Epoch:4, Train loss:0.002582, valid loss:0.001218
Epoch:5, Train loss:0.002309, valid loss:0.001127
Epoch:6, Train loss:0.002166, valid loss:0.001188
Epoch:7, Train loss:0.001996, valid loss:0.000953
Epoch:8, Train loss:0.001880, valid loss:0.000949
Epoch:9, Train loss:0.001701, valid loss:0.000940
Epoch:10, Train loss:0.001628, valid loss:0.000832
Epoch:11, Train loss:0.001192, valid loss:0.000605
Epoch:12, Train loss:0.001102, valid loss:0.000646
Epoch:13, Train loss:0.001082, valid loss:0.000619
Epoch:14, Train loss:0.001045, valid loss:0.000619
Epoch:15, Train loss:0.001009, valid loss:0.000650
Epoch:16, Train loss:0.000980, valid loss:0.000674
Epoch:17, Train loss:0.000934, valid loss:0.000627
Epoch:18, Train loss:0.000911, valid loss:0.000585
Epoch:19, Train loss:0.000898, valid loss:0.000620
Epoch:20, Train loss:0.000884, valid loss:0.000580
Epoch:21, Train loss:0.000643, valid loss:0.000497
Epoch:22, Train loss:0.000623, valid loss:0.000484
Epoch:23, Train loss:0.000609, valid loss:0.000478
Epoch:24, Train loss:0.000596, valid loss:0.000478
Epoch:25, Train loss:0.000584, valid loss:0.000490
Epoch:26, Train loss:0.000597, valid loss:0.000486
Epoch:27, Train loss:0.000575, valid loss:0.000485
Epoch:28, Train loss:0.000551, valid loss:0.000493
Epoch:29, Train loss:0.000544, valid loss:0.000508
Epoch:30, Train loss:0.000546, valid loss:0.000516
Epoch:31, Train loss:0.000420, valid loss:0.000452
Epoch:32, Train loss:0.000412, valid loss:0.000460
Epoch:33, Train loss:0.000404, valid loss:0.000450
Epoch:34, Train loss:0.000405, valid loss:0.000460
Epoch:35, Train loss:0.000401, valid loss:0.000430
Epoch:36, Train loss:0.000395, valid loss:0.000429
Epoch:37, Train loss:0.000379, valid loss:0.000447
Epoch:38, Train loss:0.000388, valid loss:0.000423
Epoch:39, Train loss:0.000376, valid loss:0.000406
Epoch:40, Train loss:0.000383, valid loss:0.000420
Epoch:41, Train loss:0.000328, valid loss:0.000429
Epoch:42, Train loss:0.000321, valid loss:0.000422
Epoch:43, Train loss:0.000316, valid loss:0.000432
Epoch:44, Train loss:0.000316, valid loss:0.000434
Epoch:45, Train loss:0.000316, valid loss:0.000427
Epoch:46, Train loss:0.000319, valid loss:0.000421
Epoch:47, Train loss:0.000305, valid loss:0.000422
Epoch:48, Train loss:0.000306, valid loss:0.000409
Epoch:49, Train loss:0.000304, valid loss:0.000433
Epoch:50, Train loss:0.000305, valid loss:0.000451
Epoch:51, Train loss:0.000291, valid loss:0.000442
Epoch:52, Train loss:0.000286, valid loss:0.000439
Epoch:53, Train loss:0.000284, valid loss:0.000437
Epoch:54, Train loss:0.000282, valid loss:0.000436
Epoch:55, Train loss:0.000282, valid loss:0.000434
Epoch:56, Train loss:0.000281, valid loss:0.000433
Epoch:57, Train loss:0.000281, valid loss:0.000433
Epoch:58, Train loss:0.000280, valid loss:0.000432
Epoch:59, Train loss:0.000280, valid loss:0.000431
Epoch:60, Train loss:0.000279, valid loss:0.000431
training time 23062.626544475555
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.07822624201607808
plot_id,batch_id 0 1 miss% 0.05402056371060046
plot_id,batch_id 0 2 miss% 0.09443679362361393
plot_id,batch_id 0 3 miss% 0.03925961103570709
plot_id,batch_id 0 4 miss% 0.0424461946797357
plot_id,batch_id 0 5 miss% 0.07095962369483115
plot_id,batch_id 0 6 miss% 0.04995787598800847
plot_id,batch_id 0 7 miss% 0.0665936730602814
plot_id,batch_id 0 8 miss% 0.06391928434706537
plot_id,batch_id 0 9 miss% 0.03336967918346041
plot_id,batch_id 0 10 miss% 0.026108115274516563
plot_id,batch_id 0 11 miss% 0.042809377005721815
plot_id,batch_id 0 12 miss% 0.053442463116845106
plot_id,batch_id 0 13 miss% 0.042476113807932606
plot_id,batch_id 0 14 miss% 0.08943366043743838
plot_id,batch_id 0 15 miss% 0.056410930779829505
plot_id,batch_id 0 16 miss% 0.07719991212373287
plot_id,batch_id 0 17 miss% 0.052517508989107356
plot_id,batch_id 0 18 miss% 0.051315063513551945
plot_id,batch_id 0 19 miss% 0.049362303596316305
plot_id,batch_id 0 20 miss% 0.05691704258858356
plot_id,batch_id 0 21 miss% 0.02650853804278133
plot_id,batch_id 0 22 miss% 0.04632275077393481
plot_id,batch_id 0 23 miss% 0.036913475504017544
plot_id,batch_id 0 24 miss% 0.03666855456121782
plot_id,batch_id 0 25 miss% 0.07617165811274856
plot_id,batch_id 0 26 miss% 0.03370347550775461
plot_id,batch_id 0 27 miss% 0.039986794611691405
plot_id,batch_id 0 28 miss% 0.048003408947403466
plot_id,batch_id 0 29 miss% 0.02361252397411831
plot_id,batch_id 0 30 miss% 0.04728217797098344
plot_id,batch_id 0 31 miss% 0.07142913135289432
plot_id,batch_id 0 32 miss% 0.0774187466012527
plot_id,batch_id 0 33 miss% 0.04607587231550122
plot_id,batch_id 0 34 miss% 0.03560194324254961
plot_id,batch_id 0 35 miss% 0.06993582476098927
plot_id,batch_id 0 36 miss% 0.090195756685064
plot_id,batch_id 0 37 miss% 0.034746814827576473
plot_id,batch_id 0 38 miss% 0.038740690561028
plot_id,batch_id 0 39 miss% 0.03145968971846192
plot_id,batch_id 0 40 miss% 0.040531269787421345
plot_id,batch_id 0 41 miss% 0.061435176680602846
plot_id,batch_id 0 42 miss% 0.023031382779433833
plot_id,batch_id 0 43 miss% 0.03927712636264197
plot_id,batch_id 0 44 miss% 0.024048866850354046
plot_id,batch_id 0 45 miss% 0.03330826400610385
plot_id,batch_id 0 46 miss% 0.06080711543610707
plot_id,batch_id 0 47 miss% 0.02557098847101367
plot_id,batch_id 0 48 miss% 0.02886750968038724
plot_id,batch_id 0 49 miss% 0.037603675081141104
plot_id,batch_id 0 50 miss% 0.1435079699780201
plot_id,batch_id 0 51 miss% 0.035043713656091775
plot_id,batch_id 0 52 miss% 0.029506549175802255
plot_id,batch_id 0 53 miss% 0.022284261972604635
plot_id,batch_id 0 54 miss% 0.019981779819014042
plot_id,batch_id 0 55 miss% 0.0724637187626265
plot_id,batch_id 0 56 miss% 0.07031258147848125
plot_id,batch_id 0 57 miss% 0.03405983278065409
plot_id,batch_id 0 58 miss% 0.04943202823895712
plot_id,batch_id 0 59 miss% 0.02903763660995163
plot_id,batch_id 0 60 miss% 0.03360326424798556
plot_id,batch_id 0 61 miss% 0.024790382252755913
plot_id,batch_id 0 62 miss% 0.05960807231340621
plot_id,batch_id 0 63 miss% 0.03733357932127427
plot_id,batch_id 0 64 miss% 0.038734102965884566
plot_id,batch_id 0 65 miss% 0.033917236328932066
plot_id,batch_id 0 66 miss% 0.04290184373180604
plot_id,batch_id 0 67 0.028770884506366678
plot_id,batch_id 0 68 miss% 0.04145988588987605
plot_id,batch_id 0 69 miss% 0.09497358533758253
plot_id,batch_id 0 70 miss% 0.041259121851122266
plot_id,batch_id 0 71 miss% 0.02066286290085843
plot_id,batch_id 0 72 miss% 0.038730861237471804
plot_id,batch_id 0 73 miss% 0.027264529109829128
plot_id,batch_id 0 74 miss% 0.04217470048373191
plot_id,batch_id 0 75 miss% 0.041434598341268
plot_id,batch_id 0 76 miss% 0.04947122186110344
plot_id,batch_id 0 77 miss% 0.026231349567787086
plot_id,batch_id 0 78 miss% 0.02149861601417546
plot_id,batch_id 0 79 miss% 0.06328998977553489
plot_id,batch_id 0 80 miss% 0.04996994336126346
plot_id,batch_id 0 81 miss% 0.08802309825840983
plot_id,batch_id 0 82 miss% 0.04614312174622055
plot_id,batch_id 0 83 miss% 0.10061346278453615
plot_id,batch_id 0 84 miss% 0.03815138485470697
plot_id,batch_id 0 85 miss% 0.05592559366847953
plot_id,batch_id 0 86 miss% 0.06618218202257545
plot_id,batch_id 0 87 miss% 0.06081415070999476
plot_id,batch_id 0 88 miss% 0.06829909205524838
plot_id,batch_id 0 89 miss% 0.039240757397870515
plot_id,batch_id 0 90 miss% 0.03355004907926623
plot_id,batch_id 0 91 miss% 0.03183038775053543
plot_id,batch_id 0 92 miss% 0.035544762685838886
plot_id,batch_id 0 93 miss% 0.024662132310132517
plot_id,batch_id 0 94 miss% 0.08681891820813029
plot_id,batch_id 0 95 miss% 0.048050887903970656
plot_id,batch_id 0 96 miss% 0.05952363878491898
plot_id,batch_id 0 97 miss% 0.03882666535745682
plot_id,batch_id 0 98 miss% 0.03267893040879488
plot_id,batch_id 0 99 miss% 0.08904017407871934
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07091609 0.07204882 0.09684207 0.04236442 0.06286619 0.05596329
 0.04030195 0.0589606  0.0565886  0.05402087 0.02602382 0.04387359
 0.06048526 0.04422612 0.06791246 0.03385403 0.08804536 0.03136315
 0.06609376 0.05168849 0.07682102 0.0513469  0.04652813 0.02979308
 0.07015113 0.0307027  0.05175491 0.03752976 0.0331267  0.03213263
 0.0512479  0.0594949  0.08749144 0.04894985 0.0383656  0.02358539
 0.09097675 0.05944406 0.06030722 0.04204774 0.06651262 0.06664133
 0.06136408 0.08456823 0.04093099 0.03656796 0.03434461 0.03585635
 0.0212903  0.04616969 0.13179162 0.02896782 0.02344781 0.01777671
 0.03299365 0.03798303 0.0738253  0.03544993 0.02286036 0.01807761
 0.02816566 0.02358606 0.05075333 0.04704323 0.06620165 0.04499285
 0.02562246 0.02877088 0.04145989 0.09497359 0.04125912 0.02066286
 0.03873086 0.02726453 0.0421747  0.0414346  0.04947122 0.02623135
 0.02149862 0.06328999 0.04996994 0.0880231  0.04614312 0.10061346
 0.03815138 0.05592559 0.06618218 0.06081415 0.06829909 0.03924076
 0.03355005 0.03183039 0.03554476 0.02466213 0.08681892 0.04805089
 0.05952364 0.03882667 0.03267893 0.08904017]
for model  206 the mean error 0.04981133525790173
all id 206 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 31 out win 6 err 0.04981133525790173 time 23039.485804080963
Launcher: Job 207 completed in 23282 seconds.
Launcher: Task 44 done. Exiting.
miss% 0.02176895989709246
plot_id,batch_id 0 68 miss% 0.05253201561551957
plot_id,batch_id 0 69 miss% 0.06565582491569778
plot_id,batch_id 0 70 miss% 0.03621156280333772
plot_id,batch_id 0 71 miss% 0.030580188750204178
plot_id,batch_id 0 72 miss% 0.07632018307719138
plot_id,batch_id 0 73 miss% 0.06836932488595697
plot_id,batch_id 0 74 miss% 0.0696918622636849
plot_id,batch_id 0 75 miss% 0.03573678729808932
plot_id,batch_id 0 76 miss% 0.11379832151278202
plot_id,batch_id 0 77 miss% 0.03837385328939608
plot_id,batch_id 0 78 miss% 0.018329975905997428
plot_id,batch_id 0 79 miss% 0.0415163042643807
plot_id,batch_id 0 80 miss% 0.07795321414894839
plot_id,batch_id 0 81 miss% 0.08542939783133247
plot_id,batch_id 0 82 miss% 0.030476647566315705
plot_id,batch_id 0 83 miss% 0.06790403727490159
plot_id,batch_id 0 84 miss% 0.038495069677545006
plot_id,batch_id 0 85 miss% 0.02922470576310728
plot_id,batch_id 0 86 miss% 0.0841556875710537
plot_id,batch_id 0 87 miss% 0.06624401224736948
plot_id,batch_id 0 88 miss% 0.08845645779133521
plot_id,batch_id 0 89 miss% 0.06847867355145214
plot_id,batch_id 0 90 miss% 0.04765348230370153
plot_id,batch_id 0 91 miss% 0.06289779701575077
plot_id,batch_id 0 92 miss% 0.04556440771568044
plot_id,batch_id 0 93 miss% 0.050076042098444636
plot_id,batch_id 0 94 miss% 0.0583752068850279
plot_id,batch_id 0 95 miss% 0.08122856756710461
plot_id,batch_id 0 96 miss% 0.03960677643516825
plot_id,batch_id 0 97 miss% 0.024469462822921006
plot_id,batch_id 0 98 miss% 0.056467248215885085
plot_id,batch_id 0 99 miss% 0.12139123248260138
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.07822624 0.05402056 0.09443679 0.03925961 0.04244619 0.07095962
 0.04995788 0.06659367 0.06391928 0.03336968 0.02610812 0.04280938
 0.05344246 0.04247611 0.08943366 0.05641093 0.07719991 0.05251751
 0.05131506 0.0493623  0.05691704 0.02650854 0.04632275 0.03691348
 0.03666855 0.07617166 0.03370348 0.03998679 0.04800341 0.02361252
 0.04728218 0.07142913 0.07741875 0.04607587 0.03560194 0.06993582
 0.09019576 0.03474681 0.03874069 0.03145969 0.04053127 0.06143518
 0.02303138 0.03927713 0.02404887 0.03330826 0.06080712 0.02557099
 0.02886751 0.03760368 0.14350797 0.03504371 0.02950655 0.02228426
 0.01998178 0.07246372 0.07031258 0.03405983 0.04943203 0.02903764
 0.03360326 0.02479038 0.05960807 0.03733358 0.0387341  0.03391724
 0.04290184 0.02176896 0.05253202 0.06565582 0.03621156 0.03058019
 0.07632018 0.06836932 0.06969186 0.03573679 0.11379832 0.03837385
 0.01832998 0.0415163  0.07795321 0.0854294  0.03047665 0.06790404
 0.03849507 0.02922471 0.08415569 0.06624401 0.08845646 0.06847867
 0.04765348 0.0628978  0.04556441 0.05007604 0.05837521 0.08122857
 0.03960678 0.02446946 0.05646725 0.12139123]
for model  233 the mean error 0.05146393080859359
all id 233 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 31 out win 6 err 0.05146393080859359 time 23062.626544475555
Launcher: Job 234 completed in 23304 seconds.
Launcher: Task 81 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  209681
Epoch:0, Train loss:0.317336, valid loss:0.319458
Epoch:1, Train loss:0.021432, valid loss:0.002926
Epoch:2, Train loss:0.007465, valid loss:0.001926
Epoch:3, Train loss:0.005266, valid loss:0.001678
Epoch:4, Train loss:0.004762, valid loss:0.001654
Epoch:5, Train loss:0.003827, valid loss:0.001182
Epoch:6, Train loss:0.002607, valid loss:0.000949
Epoch:7, Train loss:0.001673, valid loss:0.000907
Epoch:8, Train loss:0.001533, valid loss:0.000869
Epoch:9, Train loss:0.001412, valid loss:0.000714
Epoch:10, Train loss:0.001347, valid loss:0.000716
Epoch:11, Train loss:0.001010, valid loss:0.000683
Epoch:12, Train loss:0.000979, valid loss:0.000617
Epoch:13, Train loss:0.000932, valid loss:0.000555
Epoch:14, Train loss:0.000899, valid loss:0.000576
Epoch:15, Train loss:0.000880, valid loss:0.000549
Epoch:16, Train loss:0.000839, valid loss:0.000679
Epoch:17, Train loss:0.000817, valid loss:0.000533
Epoch:18, Train loss:0.000777, valid loss:0.000538
Epoch:19, Train loss:0.000771, valid loss:0.000598
Epoch:20, Train loss:0.000738, valid loss:0.000569
Epoch:21, Train loss:0.000580, valid loss:0.000531
Epoch:22, Train loss:0.000565, valid loss:0.000506
Epoch:23, Train loss:0.000543, valid loss:0.000479
Epoch:24, Train loss:0.000556, valid loss:0.000491
Epoch:25, Train loss:0.000532, valid loss:0.000554
Epoch:26, Train loss:0.000520, valid loss:0.000466
Epoch:27, Train loss:0.000513, valid loss:0.000491
Epoch:28, Train loss:0.000519, valid loss:0.000441
Epoch:29, Train loss:0.000489, valid loss:0.000513
Epoch:30, Train loss:0.000512, valid loss:0.000515
Epoch:31, Train loss:0.000409, valid loss:0.000427
Epoch:32, Train loss:0.000409, valid loss:0.000440
Epoch:33, Train loss:0.000407, valid loss:0.000442
Epoch:34, Train loss:0.000401, valid loss:0.000434
Epoch:35, Train loss:0.000397, valid loss:0.000430
Epoch:36, Train loss:0.000383, valid loss:0.000449
Epoch:37, Train loss:0.000396, valid loss:0.000417
Epoch:38, Train loss:0.000385, valid loss:0.000438
Epoch:39, Train loss:0.000389, valid loss:0.000420
Epoch:40, Train loss:0.000377, valid loss:0.000408
Epoch:41, Train loss:0.000339, valid loss:0.000420
Epoch:42, Train loss:0.000335, valid loss:0.000416
Epoch:43, Train loss:0.000334, valid loss:0.000412
Epoch:44, Train loss:0.000332, valid loss:0.000403
Epoch:45, Train loss:0.000332, valid loss:0.000418
Epoch:46, Train loss:0.000328, valid loss:0.000413
Epoch:47, Train loss:0.000327, valid loss:0.000400
Epoch:48, Train loss:0.000324, valid loss:0.000404
Epoch:49, Train loss:0.000328, valid loss:0.000402
Epoch:50, Train loss:0.000322, valid loss:0.000411
Epoch:51, Train loss:0.000302, valid loss:0.000408
Epoch:52, Train loss:0.000299, valid loss:0.000400
Epoch:53, Train loss:0.000298, valid loss:0.000395
Epoch:54, Train loss:0.000298, valid loss:0.000398
Epoch:55, Train loss:0.000297, valid loss:0.000398
Epoch:56, Train loss:0.000297, valid loss:0.000399
Epoch:57, Train loss:0.000297, valid loss:0.000397
Epoch:58, Train loss:0.000297, valid loss:0.000400
Epoch:59, Train loss:0.000296, valid loss:0.000398
Epoch:60, Train loss:0.000296, valid loss:0.000399
training time 23257.568288087845
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.11180842813308241
plot_id,batch_id 0 1 miss% 0.060500951686566044
plot_id,batch_id 0 2 miss% 0.09594328890148204
plot_id,batch_id 0 3 miss% 0.05599322200262722
plot_id,batch_id 0 4 miss% 0.03437391573247971
plot_id,batch_id 0 5 miss% 0.09627604078199879
plot_id,batch_id 0 6 miss% 0.053595053699234244
plot_id,batch_id 0 7 miss% 0.10286373012067208
plot_id,batch_id 0 8 miss% 0.06737762638908645
plot_id,batch_id 0 9 miss% 0.03145253546220112
plot_id,batch_id 0 10 miss% 0.035789876994274154
plot_id,batch_id 0 11 miss% 0.06735153989270631
plot_id,batch_id 0 12 miss% 0.05209595703873059
plot_id,batch_id 0 13 miss% 0.0970915584949945
plot_id,batch_id 0 14 miss% 0.0870904320531228
plot_id,batch_id 0 15 miss% 0.0449083281120389
plot_id,batch_id 0 16 miss% 0.07775907980643985
plot_id,batch_id 0 17 miss% 0.056683598129488616
plot_id,batch_id 0 18 miss% 0.07719980565922667
plot_id,batch_id 0 19 miss% 0.05902748865312928
plot_id,batch_id 0 20 miss% 0.0881976391698959
plot_id,batch_id 0 21 miss% 0.035132795239609285
plot_id,batch_id 0 22 miss% 0.057533304878447815
plot_id,batch_id 0 23 miss% 0.07433740231032998
plot_id,batch_id 0 24 miss% 0.06005220704512658
plot_id,batch_id 0 25 miss% 0.07051658876797698
plot_id,batch_id 0 26 miss% 0.06662245996672186
plot_id,batch_id 0 27 miss% 0.0590087772258908
plot_id,batch_id 0 28 miss% 0.047625352200753535
plot_id,batch_id 0 29 miss% 0.034740802631970664
plot_id,batch_id 0 30 miss% 0.05345333627717377
plot_id,batch_id 0 31 miss% 0.09721787530489427
plot_id,batch_id 0 32 miss% 0.12040929369105975
plot_id,batch_id 0 33 miss% 0.04477023148972909
plot_id,batch_id 0 34 miss% 0.055353613890779814
plot_id,batch_id 0 35 miss% 0.04115910601276257
plot_id,batch_id 0 36 miss% 0.09156618716274831
plot_id,batch_id 0 37 miss% 0.057969245278319675
plot_id,batch_id 0 38 miss% 0.07098507906508725
plot_id,batch_id 0 39 miss% 0.030552052254104945
plot_id,batch_id 0 40 miss% 0.11330733179303112
plot_id,batch_id 0 41 miss% 0.03958926636158112
plot_id,batch_id 0 42 miss% 0.036385252505641466
plot_id,batch_id 0 43 miss% 0.07307109853167096
plot_id,batch_id 0 44 miss% 0.02447522051023498
plot_id,batch_id 0 45 miss% 0.02674373178636769
plot_id,batch_id 0 46 miss% 0.03373859749981838
plot_id,batch_id 0 47 miss% 0.0267428617592422
plot_id,batch_id 0 48 miss% 0.053735329587478065
plot_id,batch_id 0 49 miss% 0.026966431474493274
plot_id,batch_id 0 50 miss% 0.12506132504004974
plot_id,batch_id 0 51 miss% 0.03385431540759058
plot_id,batch_id 0 52 miss% 0.04430874217949332
plot_id,batch_id 0 53 miss% 0.020156462475032506
plot_id,batch_id 0 54 miss% 0.0575492769025196
plot_id,batch_id 0 55 miss% 0.07069722124486812
plot_id,batch_id 0 56 miss% 0.08790377078859934
plot_id,batch_id 0 57 miss% 0.04243610940465722
plot_id,batch_id 0 58 miss% 0.05077324101417268
plot_id,batch_id 0 59 miss% 0.050618859672160156
plot_id,batch_id 0 60 miss% 0.04770068944942426
plot_id,batch_id 0 61 miss% 0.03346330554849776
plot_id,batch_id 0 62 miss% 0.05236423755165739
plot_id,batch_id 0 63 miss% 0.04976435581977894
plot_id,batch_id 0 64 miss% 0.0870508456867975
plot_id,batch_id 0 65 miss% 0.036136885466577895
plot_id,batch_id 0 66 miss% 0.03412695239948949
plot_id,batch_id 0 67 miss% 0.026328831635911855
plot_id,batch_id 0 68 miss% 0.04888201605062298
plot_id,batch_id 0 69 miss% 0.1287895187962185
plot_id,batch_id 0 70 miss% 0.029998176448326336
plot_id,batch_id 0 71 miss% 0.03516636041371224
plot_id,batch_id 0 72 miss% 0.12545320672468974
plot_id,batch_id 0 73 miss% 0.05235499616717352
plot_id,batch_id 0 74 miss% 0.07962129292929328
plot_id,batch_id 0 75 miss% 0.08874055146157896
plot_id,batch_id 0 76 miss% 0.13130674970800502
plot_id,batch_id 0 77 miss% 0.033304666249946664
plot_id,batch_id 0 78 miss% 0.02167001412991116
plot_id,batch_id 0 79 miss% 0.0696126183975541
plot_id,batch_id 0 80 miss% 0.057913826849272446
plot_id,batch_id 0 81 miss% 0.0869304356387291
plot_id,batch_id 0 82 miss% 0.031296891284101694
plot_id,batch_id 0 83 miss% 0.07777897459541806
plot_id,batch_id 0 84 miss% 0.08425303325819625
plot_id,batch_id 0 85 miss% 0.03825318586919676
plot_id,batch_id 0 86 miss% 0.0730118036656281
plot_id,batch_id 0 87 miss% 0.058877552767843136
plot_id,batch_id 0 88 miss% 0.08708164910921627
plot_id,batch_id 0 89 miss% 0.05570579754585228
plot_id,batch_id 0 90 miss% 0.026267476961199594
plot_id,batch_id 0 91 miss% 0.05680967058359209
plot_id,batch_id 0 92 miss% 0.04575530684160069
plot_id,batch_id 0 93 miss% 0.03765263978272639
plot_id,batch_id 0 94 miss% 0.0772094622475548
plot_id,batch_id 0 95 miss% 0.06517451279083955
plot_id,batch_id 0 96 miss% 0.0385824863557222
plot_id,batch_id 0 97 miss% 0.033460764665853515
plot_id,batch_id 0 98 miss% 0.05673307365428182
plot_id,batch_id 0 99 miss% 0.06550708335687642
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.11180843 0.06050095 0.09594329 0.05599322 0.03437392 0.09627604
 0.05359505 0.10286373 0.06737763 0.03145254 0.03578988 0.06735154
 0.05209596 0.09709156 0.08709043 0.04490833 0.07775908 0.0566836
 0.07719981 0.05902749 0.08819764 0.0351328  0.0575333  0.0743374
 0.06005221 0.07051659 0.06662246 0.05900878 0.04762535 0.0347408
 0.05345334 0.09721788 0.12040929 0.04477023 0.05535361 0.04115911
 0.09156619 0.05796925 0.07098508 0.03055205 0.11330733 0.03958927
 0.03638525 0.0730711  0.02447522 0.02674373 0.0337386  0.02674286
 0.05373533 0.02696643 0.12506133 0.03385432 0.04430874 0.02015646
 0.05754928 0.07069722 0.08790377 0.04243611 0.05077324 0.05061886
 0.04770069 0.03346331 0.05236424 0.04976436 0.08705085 0.03613689
 0.03412695 0.02632883 0.04888202 0.12878952 0.02999818 0.03516636
 0.12545321 0.052355   0.07962129 0.08874055 0.13130675 0.03330467
 0.02167001 0.06961262 0.05791383 0.08693044 0.03129689 0.07777897
 0.08425303 0.03825319 0.0730118  0.05887755 0.08708165 0.0557058
 0.02626748 0.05680967 0.04575531 0.03765264 0.07720946 0.06517451
 0.03858249 0.03346076 0.05673307 0.06550708]
for model  179 the mean error 0.06026592152404535
all id 179 hidden_dim 32 learning_rate 0.0025 num_layers 4 frames 31 out win 6 err 0.06026592152404535 time 23257.568288087845
Launcher: Job 180 completed in 23502 seconds.
Launcher: Task 55 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.0025
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  265233
Epoch:0, Train loss:0.341427, valid loss:0.317617
Epoch:1, Train loss:0.236004, valid loss:0.233987
Epoch:2, Train loss:0.230655, valid loss:0.233602
Epoch:3, Train loss:0.230034, valid loss:0.233490
Epoch:4, Train loss:0.229751, valid loss:0.233353
Epoch:5, Train loss:0.229480, valid loss:0.233230
Epoch:6, Train loss:0.229350, valid loss:0.233188
Epoch:7, Train loss:0.229183, valid loss:0.233152
Epoch:8, Train loss:0.229069, valid loss:0.233067
Epoch:9, Train loss:0.228977, valid loss:0.233100
Epoch:10, Train loss:0.228933, valid loss:0.233065
Epoch:11, Train loss:0.228635, valid loss:0.233047
Epoch:12, Train loss:0.228619, valid loss:0.232999
Epoch:13, Train loss:0.228584, valid loss:0.232969
Epoch:14, Train loss:0.228578, valid loss:0.232924
Epoch:15, Train loss:0.228544, valid loss:0.232930
Epoch:16, Train loss:0.228527, valid loss:0.232925
Epoch:17, Train loss:0.228490, valid loss:0.232957
Epoch:18, Train loss:0.228500, valid loss:0.233006
Epoch:19, Train loss:0.228469, valid loss:0.232945
Epoch:20, Train loss:0.228450, valid loss:0.232943
Epoch:21, Train loss:0.228307, valid loss:0.232858
Epoch:22, Train loss:0.228296, valid loss:0.232871
Epoch:23, Train loss:0.228309, valid loss:0.232875
Epoch:24, Train loss:0.228282, valid loss:0.232900
Epoch:25, Train loss:0.228276, valid loss:0.232869
Epoch:26, Train loss:0.228258, valid loss:0.232876
Epoch:27, Train loss:0.228283, valid loss:0.232856
Epoch:28, Train loss:0.228251, valid loss:0.232887
Epoch:29, Train loss:0.228245, valid loss:0.232874
Epoch:30, Train loss:0.228246, valid loss:0.232852
Epoch:31, Train loss:0.228174, valid loss:0.232854
Epoch:32, Train loss:0.228167, valid loss:0.232858
Epoch:33, Train loss:0.228162, valid loss:0.232843
Epoch:34, Train loss:0.228154, valid loss:0.232868
Epoch:35, Train loss:0.228161, valid loss:0.232833
Epoch:36, Train loss:0.228147, valid loss:0.232865
Epoch:37, Train loss:0.228154, valid loss:0.232870
Epoch:38, Train loss:0.228153, valid loss:0.232861
Epoch:39, Train loss:0.228140, valid loss:0.232843
Epoch:40, Train loss:0.228137, valid loss:0.232850
Epoch:41, Train loss:0.228107, valid loss:0.232831
Epoch:42, Train loss:0.228101, valid loss:0.232854
Epoch:43, Train loss:0.228102, valid loss:0.232850
Epoch:44, Train loss:0.228102, valid loss:0.232846
Epoch:45, Train loss:0.228106, valid loss:0.232824
Epoch:46, Train loss:0.228096, valid loss:0.232863
Epoch:47, Train loss:0.228096, valid loss:0.232844
Epoch:48, Train loss:0.228097, valid loss:0.232832
Epoch:49, Train loss:0.228093, valid loss:0.232831
Epoch:50, Train loss:0.228092, valid loss:0.232830
Epoch:51, Train loss:0.228092, valid loss:0.232836
Epoch:52, Train loss:0.228085, valid loss:0.232838
Epoch:53, Train loss:0.228082, valid loss:0.232834
Epoch:54, Train loss:0.228081, valid loss:0.232837
Epoch:55, Train loss:0.228079, valid loss:0.232831
Epoch:56, Train loss:0.228078, valid loss:0.232838
Epoch:57, Train loss:0.228078, valid loss:0.232834
Epoch:58, Train loss:0.228077, valid loss:0.232833
Epoch:59, Train loss:0.228076, valid loss:0.232834
Epoch:60, Train loss:0.228076, valid loss:0.232834
training time 24121.272218942642
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.9593813510515088
plot_id,batch_id 0 1 miss% 0.9634269356366434
plot_id,batch_id 0 2 miss% 0.9653310121773766
plot_id,batch_id 0 3 miss% 0.9660570978183243
plot_id,batch_id 0 4 miss% 0.967067074943316
plot_id,batch_id 0 5 miss% 0.9651477701229288
plot_id,batch_id 0 6 miss% 0.9631304422784599
plot_id,batch_id 0 7 miss% 0.9662114087052941
plot_id,batch_id 0 8 miss% 0.9679753113857992
plot_id,batch_id 0 9 miss% 0.9680147051443871
plot_id,batch_id 0 10 miss% 0.9552429824229283
plot_id,batch_id 0 11 miss% 0.9604476351241911
plot_id,batch_id 0 12 miss% 0.9636560160754085
plot_id,batch_id 0 13 miss% 0.9662587473513368
plot_id,batch_id 0 14 miss% 0.9672021632065316
plot_id,batch_id 0 15 miss% 0.9451441205647819
plot_id,batch_id 0 16 miss% 0.9583028564912376
plot_id,batch_id 0 17 miss% 0.961793636385742
plot_id,batch_id 0 18 miss% 0.9690193367076633
plot_id,batch_id 0 19 miss% 0.9687834137965565
plot_id,batch_id 0 20 miss% 0.9613718074176814
plot_id,batch_id 0 21 miss% 0.9683400652469193
plot_id,batch_id 0 22 miss% 0.9660818274704718
plot_id,batch_id 0 23 miss% 0.9687305487010114
plot_id,batch_id 0 24 miss% 0.9655517220426998
plot_id,batch_id 0 25 miss% 0.9610944356216538
plot_id,batch_id 0 26 miss% 0.9668819093244535
plot_id,batch_id 0 27 miss% 0.9669646336367574
plot_id,batch_id 0 28 miss% 0.9669395747924446
plot_id,batch_id 0 29 miss% 0.9670610689399146
plot_id,batch_id 0 30 miss% 0.9573048049746039
plot_id,batch_id 0 31 miss% 0.967382776620786
plot_id,batch_id 0 32 miss% 0.968344983603196
plot_id,batch_id 0 33 miss% 0.9681431530718593
plot_id,batch_id 0 34 miss% 0.9696266676451377
plot_id,batch_id 0 35 miss% 0.9551936032142456
plot_id,batch_id 0 36 miss% 0.9669793890708233
plot_id,batch_id 0 37 miss% 0.9663784242786274
plot_id,batch_id 0 38 miss% 0.9690448886164871
plot_id,batch_id 0 39 miss% 0.9689566041852908
plot_id,batch_id 0 40 miss% 0.9629773271231823
plot_id,batch_id 0 41 miss% 0.9680501270595462
plot_id,batch_id 0 42 miss% 0.9655565505294335
plot_id,batch_id 0 43 miss% 0.9651401578327499
plot_id,batch_id 0 44 miss% 0.9668760828414008
plot_id,batch_id 0 45 miss% 0.9654577810676314
plot_id,batch_id 0 46 miss% 0.9679902534866425
plot_id,batch_id 0 47 miss% 0.9671647162822495
plot_id,batch_id 0 48 miss% 0.9696183127143105
plot_id,batch_id 0 49 miss% 0.9671183864231204
plot_id,batch_id 0 50 miss% 0.9610761991391852
plot_id,batch_id 0 51 miss% 0.9700146052499692
plot_id,batch_id 0 52 miss% 0.969613967511958
plot_id,batch_id 0 53 miss% 0.9683398149497927
plot_id,batch_id 0 54 miss% 0.9690082791683605
plot_id,batch_id 0 55 miss% 0.9632261162522354
plot_id,batch_id 0 56 miss% 0.9720801271640265
plot_id,batch_id 0 57 miss% 0.9703869470167308
plot_id,batch_id 0 58 miss% 0.9691312144390034
plot_id,batch_id 0 59 miss% 0.9692237094098757
plot_id,batch_id 0 60 miss% 0.9626820173951471
plot_id,batch_id 0 61 miss% 0.9635814332958529
plot_id,batch_id 0 62 miss% 0.9653408056233692
plot_id,batch_id 0 63 miss% 0.9633326526228494
plot_id,batch_id 0 64 miss% 0.9653811090412012
plot_id,batch_id 0 65 miss% 0.9616107512719082
plot_id,batch_id 0 66 miss% 0.9635464906519455
plot_id,batch_id 0 67 miss% 0.9635472304592596
plot_id,batch_id 0 68 miss% 0.9666158583894274
plot_id,batch_id 0 69 miss% 0.9638522455551956
plot_id,batch_id 0 70 miss% 0.9597743966034271
plot_id,batch_id 0 71 miss% 0.9760184029435705
plot_id,batch_id 0 72 miss% 0.9590565574571334
plot_id,batch_id 0 73 miss% 0.9689742064557352
plot_id,batch_id 0 74 miss% 0.9656559975959457
plot_id,batch_id 0 75 miss% 0.9673877494264237
plot_id,batch_id 0 76 miss% 0.9582376380687618
plot_id,batch_id 0 77 miss% 0.9624292860990692
plot_id,batch_id 0 78 miss% 0.9639468445907481
plot_id,batch_id 0 79 miss% 0.9674694233145249
plot_id,batch_id 0 80 miss% 0.9622254953627736
plot_id,batch_id 0 81 miss% 0.9653326591018689
plot_id,batch_id 0 82 miss% 0.9644841197897454
plot_id,batch_id 0 83 miss% 0.9657440896651419
plot_id,batch_id 0 84 miss% 0.966113293246364
plot_id,batch_id 0 85 miss% 0.9603515427137409
plot_id,batch_id 0 86 miss% 0.9623286846194258
plot_id,batch_id 0 87 miss% 0.9636151908166508
plot_id,batch_id 0 88 miss% 0.9669869048872238
plot_id,batch_id 0 89 miss% 0.9658422185322191
plot_id,batch_id 0 90 miss% 0.9638635632722492
plot_id,batch_id 0 91 miss% 0.9667528451198688
plot_id,batch_id 0 92 miss% 0.9630494626260799
plot_id,batch_id 0 93 miss% 0.9655744992271316
plot_id,batch_id 0 94 miss% 0.9684054196610675
plot_id,batch_id 0 95 miss% 0.9610538747234486
plot_id,batch_id 0 96 miss% 0.9638726215981922
plot_id,batch_id 0 97 miss% 0.9612203988783645
plot_id,batch_id 0 98 miss% 0.9682021493724535
plot_id,batch_id 0 99 miss% 0.9690539411017546
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.95938135 0.96342694 0.96533101 0.9660571  0.96706707 0.96514777
 0.96313044 0.96621141 0.96797531 0.96801471 0.95524298 0.96044764
 0.96365602 0.96625875 0.96720216 0.94514412 0.95830286 0.96179364
 0.96901934 0.96878341 0.96137181 0.96834007 0.96608183 0.96873055
 0.96555172 0.96109444 0.96688191 0.96696463 0.96693957 0.96706107
 0.9573048  0.96738278 0.96834498 0.96814315 0.96962667 0.9551936
 0.96697939 0.96637842 0.96904489 0.9689566  0.96297733 0.96805013
 0.96555655 0.96514016 0.96687608 0.96545778 0.96799025 0.96716472
 0.96961831 0.96711839 0.9610762  0.97001461 0.96961397 0.96833981
 0.96900828 0.96322612 0.97208013 0.97038695 0.96913121 0.96922371
 0.96268202 0.96358143 0.96534081 0.96333265 0.96538111 0.96161075
 0.96354649 0.96354723 0.96661586 0.96385225 0.9597744  0.9760184
 0.95905656 0.96897421 0.965656   0.96738775 0.95823764 0.96242929
 0.96394684 0.96746942 0.9622255  0.96533266 0.96448412 0.96574409
 0.96611329 0.96035154 0.96232868 0.96361519 0.9669869  0.96584222
 0.96386356 0.96675285 0.96304946 0.9655745  0.96840542 0.96105387
 0.96387262 0.9612204  0.96820215 0.96905394]
for model  188 the mean error 0.9651453165270415
all id 188 hidden_dim 32 learning_rate 0.0025 num_layers 5 frames 31 out win 6 err 0.9651453165270415 time 24121.272218942642
Launcher: Job 189 completed in 24229 seconds.
Launcher: Task 140 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 5 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  265233
Epoch:0, Train loss:0.351839, valid loss:0.328640
Epoch:1, Train loss:0.233968, valid loss:0.233610
Epoch:2, Train loss:0.228843, valid loss:0.233095
Epoch:3, Train loss:0.228430, valid loss:0.233119
Epoch:4, Train loss:0.228076, valid loss:0.232860
Epoch:5, Train loss:0.227944, valid loss:0.232855
Epoch:6, Train loss:0.227784, valid loss:0.232786
Epoch:7, Train loss:0.227618, valid loss:0.232862
Epoch:8, Train loss:0.227622, valid loss:0.232746
Epoch:9, Train loss:0.227523, valid loss:0.232750
Epoch:10, Train loss:0.227477, valid loss:0.232634
Epoch:11, Train loss:0.227085, valid loss:0.232574
Epoch:12, Train loss:0.227035, valid loss:0.232541
Epoch:13, Train loss:0.227039, valid loss:0.232607
Epoch:14, Train loss:0.227003, valid loss:0.232595
Epoch:15, Train loss:0.226996, valid loss:0.232500
Epoch:16, Train loss:0.226943, valid loss:0.232670
Epoch:17, Train loss:0.226939, valid loss:0.232488
Epoch:18, Train loss:0.226913, valid loss:0.232518
Epoch:19, Train loss:0.226905, valid loss:0.232555
Epoch:20, Train loss:0.226879, valid loss:0.232514
Epoch:21, Train loss:0.226718, valid loss:0.232472
Epoch:22, Train loss:0.226690, valid loss:0.232423
Epoch:23, Train loss:0.226693, valid loss:0.232440
Epoch:24, Train loss:0.226661, valid loss:0.232438
Epoch:25, Train loss:0.226678, valid loss:0.232454
Epoch:26, Train loss:0.226656, valid loss:0.232443
Epoch:27, Train loss:0.226652, valid loss:0.232446
Epoch:28, Train loss:0.226633, valid loss:0.232402
Epoch:29, Train loss:0.226640, valid loss:0.232437
Epoch:30, Train loss:0.226640, valid loss:0.232412
Epoch:31, Train loss:0.226545, valid loss:0.232399
Epoch:32, Train loss:0.226528, valid loss:0.232408
Epoch:33, Train loss:0.226539, valid loss:0.232410
Epoch:34, Train loss:0.226526, valid loss:0.232403
Epoch:35, Train loss:0.226519, valid loss:0.232411
Epoch:36, Train loss:0.226526, valid loss:0.232385
Epoch:37, Train loss:0.226520, valid loss:0.232411
Epoch:38, Train loss:0.226519, valid loss:0.232410
Epoch:39, Train loss:0.226529, valid loss:0.232406
Epoch:40, Train loss:0.226502, valid loss:0.232395
Epoch:41, Train loss:0.226471, valid loss:0.232382
Epoch:42, Train loss:0.226466, valid loss:0.232391
Epoch:43, Train loss:0.226465, valid loss:0.232396
Epoch:44, Train loss:0.226464, valid loss:0.232390
Epoch:45, Train loss:0.226462, valid loss:0.232397
Epoch:46, Train loss:0.226462, valid loss:0.232410
Epoch:47, Train loss:0.226456, valid loss:0.232381
Epoch:48, Train loss:0.226456, valid loss:0.232390
Epoch:49, Train loss:0.226457, valid loss:0.232380
Epoch:50, Train loss:0.226458, valid loss:0.232390
Epoch:51, Train loss:0.226446, valid loss:0.232386
Epoch:52, Train loss:0.226443, valid loss:0.232385
Epoch:53, Train loss:0.226442, valid loss:0.232385
Epoch:54, Train loss:0.226441, valid loss:0.232384
Epoch:55, Train loss:0.226440, valid loss:0.232385
Epoch:56, Train loss:0.226440, valid loss:0.232385
Epoch:57, Train loss:0.226439, valid loss:0.232386
Epoch:58, Train loss:0.226439, valid loss:0.232384
Epoch:59, Train loss:0.226439, valid loss:0.232385
Epoch:60, Train loss:0.226438, valid loss:0.232384
training time 24856.184172153473
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.16666666666666666
nondim time 0.3333333333333333
nondim time 0.5
nondim time 0.6666666666666666
nondim time 0.8333333333333334
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.958192435519463
plot_id,batch_id 0 1 miss% 0.9616792378989264
plot_id,batch_id 0 2 miss% 0.9635358838594955
plot_id,batch_id 0 3 miss% 0.9648240887312273
plot_id,batch_id 0 4 miss% 0.9648461862313468
plot_id,batch_id 0 5 miss% 0.9624142330705191
plot_id,batch_id 0 6 miss% 0.9628481929572664
plot_id,batch_id 0 7 miss% 0.9666631237611538
plot_id,batch_id 0 8 miss% 0.9659766612804767
plot_id,batch_id 0 9 miss% 0.968358328118708
plot_id,batch_id 0 10 miss% 0.9578036552589899
plot_id,batch_id 0 11 miss% 0.9612934728366218
plot_id,batch_id 0 12 miss% 0.9633115887973726
plot_id,batch_id 0 13 miss% 0.9640217599606282
plot_id,batch_id 0 14 miss% 0.9662728829094768
plot_id,batch_id 0 15 miss% 0.9438652093024233
plot_id,batch_id 0 16 miss% 0.9604614380420504
plot_id,batch_id 0 17 miss% 0.9645297194552379
plot_id,batch_id 0 18 miss% 0.9698340008239195
plot_id,batch_id 0 19 miss% 0.9702244292184491
plot_id,batch_id 0 20 miss% 0.9617847987157278
plot_id,batch_id 0 21 miss% 0.9649430683352179
plot_id,batch_id 0 22 miss% 0.9652900590982305
plot_id,batch_id 0 23 miss% 0.96620524885901
plot_id,batch_id 0 24 miss% 0.9655892638277866
plot_id,batch_id 0 25 miss% 0.9634434600225201
plot_id,batch_id 0 26 miss% 0.9711343401861425
plot_id,batch_id 0 27 miss% 0.965338022885609
plot_id,batch_id 0 28 miss% 0.9658992166266784
plot_id,batch_id 0 29 miss% 0.9662002293683807
plot_id,batch_id 0 30 miss% 0.959005663970041
plot_id,batch_id 0 31 miss% 0.9628577090391188
plot_id,batch_id 0 32 miss% 0.9682174419272823
plot_id,batch_id 0 33 miss% 0.9678229366996269
plot_id,batch_id 0 34 miss% 0.9665366989783594
plot_id,batch_id 0 35 miss% 0.9557083451291178
plot_id,batch_id 0 36 miss% 0.9674393449770853
plot_id,batch_id 0 37 miss% 0.9705175336554812
plot_id,batch_id 0 38 miss% 0.9685085492516631
plot_id,batch_id 0 39 miss% 0.9676804780041925
plot_id,batch_id 0 40 miss% 0.9653198583334616
plot_id,batch_id 0 41 miss% 0.9666749053204314
plot_id,batch_id 0 42 miss% 0.9639277136730198
plot_id,batch_id 0 43 miss% 0.9652431773639917
plot_id,batch_id 0 44 miss% 0.9668102700225548
plot_id,batch_id 0 45 miss% 0.9660509859768989
plot_id,batch_id 0 46 miss% 0.9662838038333031
plot_id,batch_id 0 47 miss% 0.966307611000816
plot_id,batch_id 0 48 miss% 0.9679393467092094
plot_id,batch_id 0 49 miss% 0.9651416201338456
plot_id,batch_id 0 50 miss% 0.9707294894258173
plot_id,batch_id 0 51 miss% 0.9651667030907847
plot_id,batch_id 0 52 miss% 0.9661246182960488
plot_id,batch_id 0 53 miss% 0.966330670204854
plot_id,batch_id 0 54 miss% 0.9660297756451337
plot_id,batch_id 0 55 miss% 0.962238437593864
plot_id,batch_id 0 56 miss% 0.9682334968930525
plot_id,batch_id 0 57 miss% 0.9689953587037539
plot_id,batch_id 0 58 miss% 0.9660493909312438
plot_id,batch_id 0 59 miss% 0.9660620064232771
plot_id,batch_id 0 60 miss% 0.96082630086011
plot_id,batch_id 0 61 miss% 0.9606503596446927
plot_id,batch_id 0 62 miss% 0.9643980283569364
plot_id,batch_id 0 63 miss% 0.9622513213372967
plot_id,batch_id 0 64 miss% 0.9638741207017009
plot_id,batch_id 0 65 miss% 0.9615311413488533
plot_id,batch_id 0 66 miss% 0.9666781811578202
plot_id,batch_id 0 67 miss% 0.9660344446723573
plot_id,batch_id 0 68 miss% 0.9640190845199895
plot_id,batch_id 0 69 miss% 0.9636536595098715
plot_id,batch_id 0 70 miss% 0.9504540964893005
plot_id,batch_id 0 71 miss% 0.973503239202453
plot_id,batch_id 0 72 miss% 0.962093051213758
plot_id,batch_id 0 73 miss% 0.9662059167356636
plot_id,batch_id 0 74 miss% 0.9636174518010878
plot_id,batch_id 0 75 miss% 0.9542690682867961
plot_id,batch_id 0 76 miss% 0.959832826569082
plot_id,batch_id 0 77 miss% 0.962202177504196
plot_id,batch_id 0 78 miss% 0.9614726981163834
plot_id,batch_id 0 79 miss% 0.9684508827198905
plot_id,batch_id 0 80 miss% 0.9612084257288632
plot_id,batch_id 0 81 miss% 0.9645985561956745
plot_id,batch_id 0 82 miss% 0.9641105759973835
plot_id,batch_id 0 83 miss% 0.9644209144477838
plot_id,batch_id 0 84 miss% 0.9646684967699838
plot_id,batch_id 0 85 miss% 0.955809100685091
plot_id,batch_id 0 86 miss% 0.9609126106443225
plot_id,batch_id 0 87 miss% 0.9625143460456055
plot_id,batch_id 0 88 miss% 0.9650227696462863
plot_id,batch_id 0 89 miss% 0.9646629160100669
plot_id,batch_id 0 90 miss% 0.9656180808547173
plot_id,batch_id 0 91 miss% 0.9653495519837617
plot_id,batch_id 0 92 miss% 0.9649688149796394
plot_id,batch_id 0 93 miss% 0.9648071080211785
plot_id,batch_id 0 94 miss% 0.9677993211482823
plot_id,batch_id 0 95 miss% 0.9773863399797939
plot_id,batch_id 0 96 miss% 0.9642821496636185
plot_id,batch_id 0 97 miss% 0.961592408286288
plot_id,batch_id 0 98 miss% 0.9667761158167609
plot_id,batch_id 0 99 miss% 0.9683840272047648
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.95819244 0.96167924 0.96353588 0.96482409 0.96484619 0.96241423
 0.96284819 0.96666312 0.96597666 0.96835833 0.95780366 0.96129347
 0.96331159 0.96402176 0.96627288 0.94386521 0.96046144 0.96452972
 0.969834   0.97022443 0.9617848  0.96494307 0.96529006 0.96620525
 0.96558926 0.96344346 0.97113434 0.96533802 0.96589922 0.96620023
 0.95900566 0.96285771 0.96821744 0.96782294 0.9665367  0.95570835
 0.96743934 0.97051753 0.96850855 0.96768048 0.96531986 0.96667491
 0.96392771 0.96524318 0.96681027 0.96605099 0.9662838  0.96630761
 0.96793935 0.96514162 0.97072949 0.9651667  0.96612462 0.96633067
 0.96602978 0.96223844 0.9682335  0.96899536 0.96604939 0.96606201
 0.9608263  0.96065036 0.96439803 0.96225132 0.96387412 0.96153114
 0.96667818 0.96603444 0.96401908 0.96365366 0.9504541  0.97350324
 0.96209305 0.96620592 0.96361745 0.95426907 0.95983283 0.96220218
 0.9614727  0.96845088 0.96120843 0.96459856 0.96411058 0.96442091
 0.9646685  0.9558091  0.96091261 0.96251435 0.96502277 0.96466292
 0.96561808 0.96534955 0.96496881 0.96480711 0.96779932 0.97738634
 0.96428215 0.96159241 0.96677612 0.96838403]
for model  241 the mean error 0.9644764885802449
all id 241 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 31 out win 5 err 0.9644764885802449 time 24856.184172153473
Launcher: Job 242 completed in 24962 seconds.
Launcher: Task 36 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  265233
Epoch:0, Train loss:0.341427, valid loss:0.317617
Epoch:1, Train loss:0.076382, valid loss:0.002156
Epoch:2, Train loss:0.004568, valid loss:0.001756
Epoch:3, Train loss:0.003843, valid loss:0.001596
Epoch:4, Train loss:0.003465, valid loss:0.001333
Epoch:5, Train loss:0.003190, valid loss:0.001142
Epoch:6, Train loss:0.002977, valid loss:0.001154
Epoch:7, Train loss:0.002811, valid loss:0.001120
Epoch:8, Train loss:0.001987, valid loss:0.000820
Epoch:9, Train loss:0.001493, valid loss:0.000872
Epoch:10, Train loss:0.001418, valid loss:0.000696
Epoch:11, Train loss:0.001049, valid loss:0.000733
Epoch:12, Train loss:0.001013, valid loss:0.000627
Epoch:13, Train loss:0.000943, valid loss:0.000599
Epoch:14, Train loss:0.000947, valid loss:0.000628
Epoch:15, Train loss:0.000894, valid loss:0.000549
Epoch:16, Train loss:0.000891, valid loss:0.000590
Epoch:17, Train loss:0.000837, valid loss:0.000644
Epoch:18, Train loss:0.000820, valid loss:0.000693
Epoch:19, Train loss:0.000803, valid loss:0.000563
Epoch:20, Train loss:0.000789, valid loss:0.000659
Epoch:21, Train loss:0.000600, valid loss:0.000479
Epoch:22, Train loss:0.000565, valid loss:0.000544
Epoch:23, Train loss:0.000567, valid loss:0.000525
Epoch:24, Train loss:0.000553, valid loss:0.000521
Epoch:25, Train loss:0.000544, valid loss:0.000477
Epoch:26, Train loss:0.000526, valid loss:0.000455
Epoch:27, Train loss:0.000522, valid loss:0.000439
Epoch:28, Train loss:0.000502, valid loss:0.000417
Epoch:29, Train loss:0.000501, valid loss:0.000502
Epoch:30, Train loss:0.000494, valid loss:0.000508
Epoch:31, Train loss:0.000399, valid loss:0.000423
Epoch:32, Train loss:0.000398, valid loss:0.000425
Epoch:33, Train loss:0.000383, valid loss:0.000482
Epoch:34, Train loss:0.000380, valid loss:0.000438
Epoch:35, Train loss:0.000372, valid loss:0.000453
Epoch:36, Train loss:0.000377, valid loss:0.000448
Epoch:37, Train loss:0.000372, valid loss:0.000472
Epoch:38, Train loss:0.000373, valid loss:0.000430
Epoch:39, Train loss:0.000348, valid loss:0.000415
Epoch:40, Train loss:0.000356, valid loss:0.000471
Epoch:41, Train loss:0.000318, valid loss:0.000380
Epoch:42, Train loss:0.000310, valid loss:0.000410
Epoch:43, Train loss:0.000307, valid loss:0.000398
Epoch:44, Train loss:0.000309, valid loss:0.000392
Epoch:45, Train loss:0.000299, valid loss:0.000393
Epoch:46, Train loss:0.000301, valid loss:0.000420
Epoch:47, Train loss:0.000295, valid loss:0.000405
Epoch:48, Train loss:0.000298, valid loss:0.000411
Epoch:49, Train loss:0.000294, valid loss:0.000399
Epoch:50, Train loss:0.000291, valid loss:0.000412
Epoch:51, Train loss:0.000276, valid loss:0.000397
Epoch:52, Train loss:0.000273, valid loss:0.000395
Epoch:53, Train loss:0.000272, valid loss:0.000394
Epoch:54, Train loss:0.000271, valid loss:0.000393
Epoch:55, Train loss:0.000270, valid loss:0.000393
Epoch:56, Train loss:0.000270, valid loss:0.000395
Epoch:57, Train loss:0.000269, valid loss:0.000391
Epoch:58, Train loss:0.000269, valid loss:0.000392
Epoch:59, Train loss:0.000269, valid loss:0.000392
Epoch:60, Train loss:0.000269, valid loss:0.000392
training time 24915.86141514778
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.3424214745052531
plot_id,batch_id 0 1 miss% 0.37979875803945434
plot_id,batch_id 0 2 miss% 0.3479435700945917
plot_id,batch_id 0 3 miss% 0.3035160328335532
plot_id,batch_id 0 4 miss% 0.2954882638261062
plot_id,batch_id 0 5 miss% 0.3216497414895978
plot_id,batch_id 0 6 miss% 0.3608803986502382
plot_id,batch_id 0 7 miss% 0.45982036427079603
plot_id,batch_id 0 8 miss% 0.460441945425345
plot_id,batch_id 0 9 miss% 0.3351651118170685
plot_id,batch_id 0 10 miss% 0.29502442346461166
plot_id,batch_id 0 11 miss% 0.2940064744971103
plot_id,batch_id 0 12 miss% 0.3837875475600763
plot_id,batch_id 0 13 miss% 0.3440755838937334
plot_id,batch_id 0 14 miss% 0.4110487897481993
plot_id,batch_id 0 15 miss% 0.3442176750266625
plot_id,batch_id 0 16 miss% 0.4890787372558352
plot_id,batch_id 0 17 miss% 0.42400725664550304
plot_id,batch_id 0 18 miss% 0.3901625739678834
plot_id,batch_id 0 19 miss% 0.3571289176495387
plot_id,batch_id 0 20 miss% 0.3309235223252859
plot_id,batch_id 0 21 miss% 0.32362071642821943
plot_id,batch_id 0 22 miss% 0.31444594389056424
plot_id,batch_id 0 23 miss% 0.26203154575317184
plot_id,batch_id 0 24 miss% 0.35505750561353216
plot_id,batch_id 0 25 miss% 0.34879884551582274
plot_id,batch_id 0 26 miss% 0.3433992764123914
plot_id,batch_id 0 27 miss% 0.35395106086932665
plot_id,batch_id 0 28 miss% 0.3498549045476994
plot_id,batch_id 0 29 miss% 0.34013639864246653
plot_id,batch_id 0 30 miss% 0.32552036347565844
plot_id,batch_id 0 31 miss% 0.4127411322827041
plot_id,batch_id 0 32 miss% 0.4239987974834492
plot_id,batch_id 0 33 miss% 0.3619942008894088
plot_id,batch_id 0 34 miss% 0.2775986473213708
plot_id,batch_id 0 35 miss% 0.3149134571994421
plot_id,batch_id 0 36 miss% 0.5047925728108532
plot_id,batch_id 0 37 miss% 0.38221188672294615
plot_id,batch_id 0 38 miss% 0.4137041103750665
plot_id,batch_id 0 39 miss% 0.32040006989666114
plot_id,batch_id 0 40 miss% 0.3691574415834168
plot_id,batch_id 0 41 miss% 0.4208013611068038
plot_id,batch_id 0 42 miss% 0.2707587123404789
plot_id,batch_id 0 43 miss% 0.2859429699139626
plot_id,batch_id 0 44 miss% 0.23470448174470862
plot_id,batch_id 0 45 miss% 0.24248695422519975
plot_id,batch_id 0 46 miss% 0.33276315327527983
plot_id,batch_id 0 47 miss% 0.30373048891787
plot_id,batch_id 0 48 miss% 0.3527795375599021
plot_id,batch_id 0 49 miss% 0.2866650393552115
plot_id,batch_id 0 50 miss% 0.5452726215540932
plot_id,batch_id 0 51 miss% 0.3972257964899275
plot_id,batch_id 0 52 miss% 0.28265334559266
plot_id,batch_id 0 53 miss% 0.3048211740639919
plot_id,batch_id 0 54 miss% 0.2617505537588119
plot_id,batch_id 0 55 miss% 0.5420717059369163
plot_id,batch_id 0 56 miss% 0.4748162236141987
plot_id,batch_id 0 57 miss% 0.3790518956273921
plot_id,batch_id 0 58 miss% 0.2895303083956181
plot_id,batch_id 0 59 miss% 0.3353528760010039
plot_id,batch_id 0 60 miss% 0.2536683962388944
plot_id,batch_id 0 61 miss% 0.27199483566356164
plot_id,batch_id 0 62 miss% 0.3456476441592936
plot_id,batch_id 0 63 miss% 0.3640972784553665
plot_id,batch_id 0 64 miss% 0.38496927141286524
plot_id,batch_id 0 65 miss% 0.4353570211997948
plot_id,batch_id 0 66 miss% 0.35818392838257895
plot_id,batch_id 0 67 miss% 0.275484758592908
plot_id,batch_id 0 68 miss% 0.4054088098431697
plot_id,batch_id 0 69 miss% 0.40204512306297396
plot_id,batch_id 0 70 miss% 0.2523547794416574
plot_id,batch_id 0 71 miss% 0.401898635014377
plot_id,batch_id 0 72 miss% 0.40435639992343725
plot_id,batch_id 0 73 miss% 0.3940724917458938
plot_id,batch_id 0 74 miss% 0.3500320691208453
plot_id,batch_id 0 75 miss% 0.2073067447708738
plot_id,batch_id 0 76 miss% 0.3180990869825918
plot_id,batch_id 0 77 miss% 0.3529274267289526
plot_id,batch_id 0 78 miss% 0.3690810386567186
plot_id,batch_id 0 79 miss% 0.31632665302714363
plot_id,batch_id 0 80 miss% 0.25264151619383607
plot_id,batch_id 0 81 miss% 0.3594451506549586
plot_id,batch_id 0 82 miss% 0.3134518679623299
plot_id,batch_id 0 83 miss% 0.4031204161487962
plot_id,batch_id 0 84 miss% 0.3255251282846123
plot_id,batch_id 0 85 miss% 0.2755475223951006
plot_id,batch_id 0 86 miss% 0.3298479016359069
plot_id,batch_id 0 87 miss% 0.3321214763927765
plot_id,batch_id 0 88 miss% 0.3987724884393613
plot_id,batch_id 0 89 miss% 0.4192970151358575
plot_id,batch_id 0 90 miss% 0.2658299229612667
plot_id,batch_id 0 91 miss% 0.31272838779676454
plot_id,batch_id 0 92 miss% 0.32893807232418076
plot_id,batch_id 0 93 miss% 0.3064950554963
plot_id,batch_id 0 94 miss% 0.39623617958786905
plot_id,batch_id 0 95 miss% 0.22864362535764973
plot_id,batch_id 0 96 miss% 0.3362673845664248
plot_id,batch_id 0 97 miss% 0.4473043944898966
plot_id,batch_id 0 98 miss% 0.44092184999981493
plot_id,batch_id 0 99 miss% 0.3999439466908394
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.34242147 0.37979876 0.34794357 0.30351603 0.29548826 0.32164974
 0.3608804  0.45982036 0.46044195 0.33516511 0.29502442 0.29400647
 0.38378755 0.34407558 0.41104879 0.34421768 0.48907874 0.42400726
 0.39016257 0.35712892 0.33092352 0.32362072 0.31444594 0.26203155
 0.35505751 0.34879885 0.34339928 0.35395106 0.3498549  0.3401364
 0.32552036 0.41274113 0.4239988  0.3619942  0.27759865 0.31491346
 0.50479257 0.38221189 0.41370411 0.32040007 0.36915744 0.42080136
 0.27075871 0.28594297 0.23470448 0.24248695 0.33276315 0.30373049
 0.35277954 0.28666504 0.54527262 0.3972258  0.28265335 0.30482117
 0.26175055 0.54207171 0.47481622 0.3790519  0.28953031 0.33535288
 0.2536684  0.27199484 0.34564764 0.36409728 0.38496927 0.43535702
 0.35818393 0.27548476 0.40540881 0.40204512 0.25235478 0.40189864
 0.4043564  0.39407249 0.35003207 0.20730674 0.31809909 0.35292743
 0.36908104 0.31632665 0.25264152 0.35944515 0.31345187 0.40312042
 0.32552513 0.27554752 0.3298479  0.33212148 0.39877249 0.41929702
 0.26582992 0.31272839 0.32893807 0.30649506 0.39623618 0.22864363
 0.33626738 0.44730439 0.44092185 0.39994395]
for model  215 the mean error 0.3504848693511309
all id 215 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 31 out win 6 err 0.3504848693511309 time 24915.86141514778
Launcher: Job 216 completed in 25136 seconds.
Launcher: Task 183 done. Exiting.
the mode is train
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 6 out_win 6
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  265233
Epoch:0, Train loss:0.341427, valid loss:0.317617
Epoch:1, Train loss:0.169918, valid loss:0.010072
Epoch:2, Train loss:0.021569, valid loss:0.009909
Epoch:3, Train loss:0.021617, valid loss:0.009141
Epoch:4, Train loss:0.021587, valid loss:0.010531
Epoch:5, Train loss:0.021557, valid loss:0.009784
Epoch:6, Train loss:0.021555, valid loss:0.009349
Epoch:7, Train loss:0.021569, valid loss:0.009385
Epoch:8, Train loss:0.021604, valid loss:0.009112
Epoch:9, Train loss:0.021561, valid loss:0.009054
Epoch:10, Train loss:0.021601, valid loss:0.009358
Epoch:11, Train loss:0.021307, valid loss:0.008508
Epoch:12, Train loss:0.021322, valid loss:0.008770
Epoch:13, Train loss:0.021341, valid loss:0.008848
Epoch:14, Train loss:0.021339, valid loss:0.009139
Epoch:15, Train loss:0.021341, valid loss:0.008400
Epoch:16, Train loss:0.021328, valid loss:0.009176
Epoch:17, Train loss:0.021327, valid loss:0.009004
Epoch:18, Train loss:0.021371, valid loss:0.009563
Epoch:19, Train loss:0.021347, valid loss:0.008813
Epoch:20, Train loss:0.021364, valid loss:0.009785
Epoch:21, Train loss:0.021177, valid loss:0.008539
Epoch:22, Train loss:0.021177, valid loss:0.008823
Epoch:23, Train loss:0.021199, valid loss:0.009029
Epoch:24, Train loss:0.021190, valid loss:0.009562
Epoch:25, Train loss:0.021211, valid loss:0.008723
Epoch:26, Train loss:0.021205, valid loss:0.008687
Epoch:27, Train loss:0.021213, valid loss:0.009661
Epoch:28, Train loss:0.021209, valid loss:0.008911
Epoch:29, Train loss:0.021180, valid loss:0.008707
Epoch:30, Train loss:0.021182, valid loss:0.009172
Epoch:31, Train loss:0.021104, valid loss:0.009017
Epoch:32, Train loss:0.021121, valid loss:0.008683
Epoch:33, Train loss:0.021113, valid loss:0.008601
Epoch:34, Train loss:0.021112, valid loss:0.009000
Epoch:35, Train loss:0.021115, valid loss:0.009031
Epoch:36, Train loss:0.021119, valid loss:0.008891
Epoch:37, Train loss:0.021116, valid loss:0.008838
Epoch:38, Train loss:0.021118, valid loss:0.008761
Epoch:39, Train loss:0.021110, valid loss:0.008844
Epoch:40, Train loss:0.021110, valid loss:0.008999
Epoch:41, Train loss:0.021073, valid loss:0.008560
Epoch:42, Train loss:0.021070, valid loss:0.008879
Epoch:43, Train loss:0.021073, valid loss:0.009091
Epoch:44, Train loss:0.021072, valid loss:0.008759
Epoch:45, Train loss:0.021071, valid loss:0.008595
Epoch:46, Train loss:0.021074, valid loss:0.008721
Epoch:47, Train loss:0.021075, valid loss:0.008651
Epoch:48, Train loss:0.021068, valid loss:0.008775
Epoch:49, Train loss:0.021069, valid loss:0.008970
Epoch:50, Train loss:0.021071, valid loss:0.008784
Epoch:51, Train loss:0.021116, valid loss:0.009096
Epoch:52, Train loss:0.021120, valid loss:0.009227
Epoch:53, Train loss:0.021107, valid loss:0.008558
Epoch:54, Train loss:0.021114, valid loss:0.009093
Epoch:55, Train loss:0.021119, valid loss:0.008658
Epoch:56, Train loss:0.021115, valid loss:0.008984
Epoch:57, Train loss:0.021120, valid loss:0.008847
Epoch:58, Train loss:0.021110, valid loss:0.008680
Epoch:59, Train loss:0.021112, valid loss:0.008693
Epoch:60, Train loss:0.021117, valid loss:0.008654
training time 26609.598132371902
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.2
nondim time 0.4
nondim time 0.6
nondim time 0.8
nondim time 1.0
plot_id,batch_id 0 0 miss% 0.38357404157036956
plot_id,batch_id 0 1 miss% 0.36932165437136266
plot_id,batch_id 0 2 miss% 0.5465378109130513
plot_id,batch_id 0 3 miss% 0.44264955479464985
plot_id,batch_id 0 4 miss% 0.4124973293593981
plot_id,batch_id 0 5 miss% 0.4740090193652767
plot_id,batch_id 0 6 miss% 0.3638549031523427
plot_id,batch_id 0 7 miss% 0.5911721127939544
plot_id,batch_id 0 8 miss% 0.43696623597931666
plot_id,batch_id 0 9 miss% 0.5098900591527201
plot_id,batch_id 0 10 miss% 0.42147832793302076
plot_id,batch_id 0 11 miss% 0.4519407726099917
plot_id,batch_id 0 12 miss% 0.43771888121741115
plot_id,batch_id 0 13 miss% 0.42735496498154246
plot_id,batch_id 0 14 miss% 0.5369390085981338
plot_id,batch_id 0 15 miss% 0.44041940337694474
plot_id,batch_id 0 16 miss% 0.4225596137987574
plot_id,batch_id 0 17 miss% 0.5781404333205162
plot_id,batch_id 0 18 miss% 0.4307395646781338
plot_id,batch_id 0 19 miss% 0.3692591601748091
plot_id,batch_id 0 20 miss% 0.27731167975495785
plot_id,batch_id 0 21 miss% 0.477171683897027
plot_id,batch_id 0 22 miss% 0.5489805485768007
plot_id,batch_id 0 23 miss% 0.43600259758606413
plot_id,batch_id 0 24 miss% 0.5800293997445636
plot_id,batch_id 0 25 miss% 0.6474114143219238
plot_id,batch_id 0 26 miss% 0.34416213635937104
plot_id,batch_id 0 27 miss% 0.6238756979185452
plot_id,batch_id 0 28 miss% 0.5452312603626156
plot_id,batch_id 0 29 miss% 0.40248776952799087
plot_id,batch_id 0 30 miss% 0.3680013370083497
plot_id,batch_id 0 31 miss% 0.4646264277469429
plot_id,batch_id 0 32 miss% 0.5918063590135426
plot_id,batch_id 0 33 miss% 0.366563855696338
plot_id,batch_id 0 34 miss% 0.5432317009239704
plot_id,batch_id 0 35 miss% 0.43904253973887625
plot_id,batch_id 0 36 miss% 0.4883497152161069
plot_id,batch_id 0 37 miss% 0.4836782620284481
plot_id,batch_id 0 38 miss% 0.3527919350715003
plot_id,batch_id 0 39 miss% 0.36320422116633727
plot_id,batch_id 0 40 miss% 0.22959017185052094
plot_id,batch_id 0 41 miss% 0.3487630362951621
plot_id,batch_id 0 42 miss% 0.39601709106600524
plot_id,batch_id 0 43 miss% 0.48236950223612823
plot_id,batch_id 0 44 miss% 0.3296564502105364
plot_id,batch_id 0 45 miss% 0.33363377049627696
plot_id,batch_id 0 46 miss% 0.3897722090302318
plot_id,batch_id 0 47 miss% 0.45577097008163303
plot_id,batch_id 0 48 miss% 0.3859260968016348
plot_id,batch_id 0 49 miss% 0.5155012284446852
plot_id,batch_id 0 50 miss% 0.35403906963555914
plot_id,batch_id 0 51 miss% 0.5547121240479228
plot_id,batch_id 0 52 miss% 0.49057025186093317
plot_id,batch_id 0 53 miss% 0.3274272847464986
plot_id,batch_id 0 54 miss% 0.6584989209338262
plot_id,batch_id 0 55 miss% 0.29267183099784044
plot_id,batch_id 0 56 miss% 0.5817658132663526
plot_id,batch_id 0 57 miss% 0.42363844959361874
plot_id,batch_id 0 58 miss% 0.38318011406342806
plot_id,batch_id 0 59 miss% 0.3561809640647023
plot_id,batch_id 0 60 miss% 0.44536230516465597
plot_id,batch_id 0 61 miss% 0.2905112132576946
plot_id,batch_id 0 62 miss% 0.34010169168460075
plot_id,batch_id 0 63 miss% 0.4917234183008338
plot_id,batch_id 0 64 miss% 0.35834879723072033
plot_id,batch_id 0 65 miss% 0.5243131413290171
plot_id,batch_id 0 66 miss% 0.34714612256398597
plot_id,batch_id 0 67 miss% 0.3309796702864984
plot_id,batch_id 0 68 miss% 0.3320107193005317
plot_id,batch_id 0 69 miss% 0.28705729296921423
plot_id,batch_id 0 70 miss% 0.4044119162695692
plot_id,batch_id 0 71 miss% 0.3580185042010027
plot_id,batch_id 0 72 miss% 0.41481003280256523
plot_id,batch_id 0 73 miss% 0.37433585623398336
plot_id,batch_id 0 74 miss% 0.45213226381343746
plot_id,batch_id 0 75 miss% 0.44700290983455426
plot_id,batch_id 0 76 miss% 0.48360279283872376
plot_id,batch_id 0 77 miss% 0.3352385238927036
plot_id,batch_id 0 78 miss% 0.3487786923808649
plot_id,batch_id 0 79 miss% 0.4836137145000465
plot_id,batch_id 0 80 miss% 0.46496291680670715
plot_id,batch_id 0 81 miss% 0.2984485598557764
plot_id,batch_id 0 82 miss% 0.5191264732218998
plot_id,batch_id 0 83 miss% 0.4943452803906881
plot_id,batch_id 0 84 miss% 0.47323775462962675
plot_id,batch_id 0 85 miss% 0.5533068675682208
plot_id,batch_id 0 86 miss% 0.29991450032452144
plot_id,batch_id 0 87 miss% 0.4557927112163405
plot_id,batch_id 0 88 miss% 0.44571381811220223
plot_id,batch_id 0 89 miss% 0.281705069056448
plot_id,batch_id 0 90 miss% 0.3013690437499976
plot_id,batch_id 0 91 miss% 0.28290927688340706
plot_id,batch_id 0 92 miss% 0.3838576961022073
plot_id,batch_id 0 93 miss% 0.412589441879985
plot_id,batch_id 0 94 miss% 0.46197140889593036
plot_id,batch_id 0 95 miss% 0.4438388013451288
plot_id,batch_id 0 96 miss% 0.2885956870913682
plot_id,batch_id 0 97 miss% 0.4839467639389106
plot_id,batch_id 0 98 miss% 0.46565770751042274
plot_id,batch_id 0 99 miss% 0.4239232680086378
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.38357404 0.36932165 0.54653781 0.44264955 0.41249733 0.47400902
 0.3638549  0.59117211 0.43696624 0.50989006 0.42147833 0.45194077
 0.43771888 0.42735496 0.53693901 0.4404194  0.42255961 0.57814043
 0.43073956 0.36925916 0.27731168 0.47717168 0.54898055 0.4360026
 0.5800294  0.64741141 0.34416214 0.6238757  0.54523126 0.40248777
 0.36800134 0.46462643 0.59180636 0.36656386 0.5432317  0.43904254
 0.48834972 0.48367826 0.35279194 0.36320422 0.22959017 0.34876304
 0.39601709 0.4823695  0.32965645 0.33363377 0.38977221 0.45577097
 0.3859261  0.51550123 0.35403907 0.55471212 0.49057025 0.32742728
 0.65849892 0.29267183 0.58176581 0.42363845 0.38318011 0.35618096
 0.44536231 0.29051121 0.34010169 0.49172342 0.3583488  0.52431314
 0.34714612 0.33097967 0.33201072 0.28705729 0.40441192 0.3580185
 0.41481003 0.37433586 0.45213226 0.44700291 0.48360279 0.33523852
 0.34877869 0.48361371 0.46496292 0.29844856 0.51912647 0.49434528
 0.47323775 0.55330687 0.2999145  0.45579271 0.44571382 0.28170507
 0.30136904 0.28290928 0.3838577  0.41258944 0.46197141 0.4438388
 0.28859569 0.48394676 0.46565771 0.42392327]
for model  242 the mean error 0.427653813689691
all id 242 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 31 out win 6 err 0.427653813689691 time 26609.598132371902
Launcher: Job 243 completed in 26830 seconds.
Launcher: Task 102 done. Exiting.
Launcher: Done. Job exited without errors
