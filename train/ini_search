/scratch/07428/ygqin/ROM_grain/ROM_NN_PF/train
Sat Mar 19 19:49:16 CDT 2022
Launcher: Setup complete.

------------- SUMMARY ---------------
   Number of hosts:    2
   Working directory:  /scratch/07428/ygqin/ROM_grain/ROM_NN_PF/train
   Processes per host: 128
   Total processes:    256
   Total jobs:         243
   Scheduling method:  dynamic

-------------------------------------
Launcher: Starting parallel tasks...
Launcher: Task 41 running job 2 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 2)
Launcher: Task 5 running job 3 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 3)
Launcher: Task 9 running job 1 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 1)
Launcher: Task 4 running job 4 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 4)
Launcher: Task 18 running job 6 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 6)
Launcher: Task 32 running job 5 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 5)
Launcher: Task 51 running job 7 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 7)
Launcher: Task 36 running job 8 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 8)
Launcher: Task 152 running job 9 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 9)
Launcher: Task 167 running job 11 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 11)
Launcher: Task 137 running job 10 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 10)
Launcher: Task 157 running job 12 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 12)
Launcher: Task 148 running job 13 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 13)
Launcher: Task 89 running job 16 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 16)
Launcher: Task 178 running job 15 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 15)
Launcher: Task 201 running job 14 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 14)
Launcher: Task 194 running job 17 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 17)
Launcher: Task 181 running job 18 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 18)
Launcher: Task 92 running job 19 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 19)
Launcher: Task 70 running job 20 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 20)
Launcher: Task 93 running job 21 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 21)
Launcher: Task 96 running job 22 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 22)
Launcher: Task 224 running job 23 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 23)
Launcher: Task 212 running job 24 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 24)
Launcher: Task 200 running job 25 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 25)
Launcher: Task 206 running job 26 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 26)
Launcher: Task 68 running job 27 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 27)
Launcher: Task 107 running job 28 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 28)
Launcher: Task 90 running job 29 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 29)
Launcher: Task 105 running job 30 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 30)
Launcher: Task 88 running job 31 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 31)
Launcher: Task 10 running job 32 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 32)
Launcher: Task 1 running job 34 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 34)
Launcher: Task 17 running job 33 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 33)
Launcher: Task 11 running job 36 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 36)
Launcher: Task 2 running job 35 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 35)
Launcher: Task 3 running job 37 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 37)
Launcher: Task 16 running job 38 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 38)
Launcher: Task 22 running job 39 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 39)
Launcher: Task 39 running job 40 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 40)
Launcher: Task 42 running job 41 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 41)
Launcher: Task 47 running job 42 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 42)
Launcher: Task 168 running job 43 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 43)
Launcher: Task 147 running job 44 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 44)
Launcher: Task 145 running job 45 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 45)
Launcher: Task 183 running job 46 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 46)
Launcher: Task 155 running job 47 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 47)
Launcher: Task 95 running job 49 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 49)
Launcher: Task 205 running job 48 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 48)
Launcher: Task 119 running job 51 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 51)
Launcher: Task 237 running job 50 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 50)
Launcher: Task 191 running job 52 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 52)
Launcher: Task 213 running job 53 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 53)
Launcher: Task 112 running job 55 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 55)
Launcher: Task 209 running job 56 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 56)
Launcher: Task 97 running job 57 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 57)
Launcher: Task 242 running job 54 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 54)
Launcher: Task 217 running job 58 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 58)
Launcher: Task 66 running job 59 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 59)
Launcher: Task 125 running job 60 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 60)
Launcher: Task 94 running job 61 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 61)
Launcher: Task 82 running job 62 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 62)
Launcher: Task 23 running job 63 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 63)
Launcher: Task 44 running job 65 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 65)
Launcher: Task 154 running job 64 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 64)
Launcher: Task 156 running job 66 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 66)
Launcher: Task 166 running job 68 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 68)
Launcher: Task 135 running job 67 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 67)
Launcher: Task 91 running job 74 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 74)
Launcher: Task 72 running job 76 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 76)
Launcher: Task 225 running job 69 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 69)
Launcher: Task 179 running job 70 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 70)
Launcher: Task 172 running job 72 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 72)
Launcher: Task 186 running job 71 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 71)
Launcher: Task 132 running job 73 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 73)
Launcher: Task 227 running job 77 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 77)
Launcher: Task 165 running job 75 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 75)
Launcher: Task 233 running job 78 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 78)
Launcher: Task 234 running job 80 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 80)
Launcher: Task 187 running job 82 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 82)
Launcher: Task 220 running job 79 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 79)
Launcher: Task 243 running job 81 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 81)
Launcher: Task 100 running job 83 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 83)
Launcher: Task 98 running job 84 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 84)
Launcher: Task 122 running job 85 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 85)
Launcher: Task 114 running job 86 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 86)
Launcher: Task 131 running job 87 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 87)
Launcher: Task 0 running job 88 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 88)
Launcher: Task 15 running job 89 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 89)
Launcher: Task 26 running job 90 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 90)
Launcher: Task 149 running job 91 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 91)
Launcher: Task 35 running job 92 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 92)
Launcher: Task 24 running job 93 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 93)
Launcher: Task 53 running job 94 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 94)
Launcher: Task 40 running job 95 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 95)
Launcher: Task 164 running job 96 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 96)
Launcher: Task 146 running job 98 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 98)
Launcher: Task 140 running job 97 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 97)
Launcher: Task 196 running job 99 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 99)
Launcher: Task 199 running job 100 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 100)
Launcher: Task 173 running job 101 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 101)
Launcher: Task 162 running job 102 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 102)
Launcher: Task 198 running job 103 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 103)
Launcher: Task 248 running job 106 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 106)
Launcher: Task 84 running job 104 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 104)
Launcher: Task 247 running job 107 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 107)
Launcher: Task 252 running job 105 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 105)
Launcher: Task 245 running job 108 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 108)
Launcher: Task 78 running job 109 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 109)
Launcher: Task 81 running job 110 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 110)
Launcher: Task 76 running job 111 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 111)
Launcher: Task 123 running job 112 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 112)
Launcher: Task 128 running job 113 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 113)
Launcher: Task 33 running job 114 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 114)
Launcher: Task 6 running job 115 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 115)
Launcher: Task 19 running job 116 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 116)
Launcher: Task 8 running job 117 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 117)
Launcher: Task 21 running job 118 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 118)
Launcher: Task 48 running job 119 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 119)
Launcher: Task 37 running job 120 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 120)
Launcher: Task 59 running job 121 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 121)
Launcher: Task 151 running job 125 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 125)
Launcher: Task 28 running job 122 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 122)
Launcher: Task 54 running job 124 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 124)
Launcher: Task 31 running job 123 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 123)
Launcher: Task 57 running job 126 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 126)
Launcher: Task 55 running job 127 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 127)
Launcher: Task 64 running job 128 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 128)
Launcher: Task 63 running job 129 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 129)
Launcher: Task 65 running job 130 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 130)
Launcher: Task 141 running job 131 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 131)
Launcher: Task 170 running job 132 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 132)
Launcher: Task 158 running job 133 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 133)
Launcher: Task 73 running job 136 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 136)
Launcher: Task 211 running job 135 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 135)
Launcher: Task 230 running job 134 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 134)
Launcher: Task 215 running job 137 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 137)
Launcher: Task 83 running job 138 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 138)
Launcher: Task 253 running job 139 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 139)
Launcher: Task 239 running job 140 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 140)
Launcher: Task 246 running job 141 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 141)
Launcher: Task 249 running job 142 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 142)
Launcher: Task 202 running job 143 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 143)
Launcher: Task 108 running job 146 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 146)
Launcher: Task 80 running job 144 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 144)
Launcher: Task 71 running job 145 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 145)
Launcher: Task 7 running job 147 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 147)
Launcher: Task 34 running job 148 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 148)
Launcher: Task 43 running job 149 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 149)
Launcher: Task 49 running job 150 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 150)
Launcher: Task 29 running job 151 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 151)
Launcher: Task 60 running job 154 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 154)
Launcher: Task 159 running job 153 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 153)
Launcher: Task 129 running job 152 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 152)
Launcher: Task 161 running job 155 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 155)
Launcher: Task 142 running job 156 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 156)
Launcher: Task 174 running job 157 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 157)
Launcher: Task 133 running job 158 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 158)
Launcher: Task 163 running job 159 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 159)
Launcher: Task 150 running job 160 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 160)
Launcher: Task 120 running job 162 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 162)
Launcher: Task 218 running job 161 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 161)
Launcher: Task 111 running job 163 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 163)
Launcher: Task 204 running job 164 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 164)
Launcher: Task 238 running job 165 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 165)
Launcher: Task 104 running job 166 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 166)
Launcher: Task 207 running job 167 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 167)
Launcher: Task 223 running job 168 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 168)
Launcher: Task 126 running job 169 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 169)
Launcher: Task 117 running job 170 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 170)
Launcher: Task 102 running job 171 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 171)
Launcher: Task 85 running job 172 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 172)
Launcher: Task 14 running job 173 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 173)
Launcher: Task 25 running job 174 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 174)
Launcher: Task 67 running job 177 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 177)
Launcher: Task 144 running job 175 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 175)
Launcher: Task 134 running job 176 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 176)
Launcher: Task 46 running job 178 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 178)
Launcher: Task 62 running job 179 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 179)
Launcher: Task 52 running job 180 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 180)
Launcher: Task 169 running job 181 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 181)
Launcher: Task 176 running job 182 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 182)
Launcher: Task 197 running job 184 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 184)
Launcher: Task 153 running job 185 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 185)
Launcher: Task 185 running job 183 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 183)
Launcher: Task 180 running job 186 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 186)
Launcher: Task 214 running job 187 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 187)
Launcher: Task 86 running job 188 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 188)
Launcher: Task 235 running job 190 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 190)
Launcher: Task 216 running job 189 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 189)
Launcher: Task 254 running job 191 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 191)
Launcher: Task 236 running job 192 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 192)
Launcher: Task 228 running job 193 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 193)
Launcher: Task 244 running job 195 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 195)
Launcher: Task 219 running job 194 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 194)
Launcher: Task 109 running job 196 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 196)
Launcher: Task 27 running job 197 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 197)
Launcher: Task 130 running job 198 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 198)
Launcher: Task 13 running job 199 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 199)
Launcher: Task 56 running job 200 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 200)
Launcher: Task 30 running job 201 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 201)
Launcher: Task 38 running job 202 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 202)
Launcher: Task 136 running job 203 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 203)
Launcher: Task 69 running job 204 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 204)
Launcher: Task 182 running job 205 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 205)
Launcher: Task 226 running job 206 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 206)
Launcher: Task 189 running job 207 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 207)
Launcher: Task 250 running job 209 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 209)
Launcher: Task 184 running job 208 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 208)
Launcher: Task 222 running job 210 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 210)
Launcher: Task 210 running job 212 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 212)
Launcher: Task 124 running job 211 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 211)
Launcher: Task 101 running job 214 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 214)
Launcher: Task 221 running job 215 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 215)
Launcher: Task 255 running job 216 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 216)
Launcher: Task 192 running job 213 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 213)
Launcher: Task 74 running job 217 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 217)
Launcher: Task 208 running job 218 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 218)
Launcher: Task 229 running job 219 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 219)
Launcher: Task 87 running job 220 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 220)
Launcher: Task 118 running job 221 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 221)
Launcher: Task 77 running job 222 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 222)
Launcher: Task 79 running job 223 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 223)
Launcher: Task 106 running job 224 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 224)
Launcher: Task 110 running job 225 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 225)
Launcher: Task 20 running job 226 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 226)
Launcher: Task 12 running job 227 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 227)
Launcher: Task 45 running job 228 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 228)
Launcher: Task 58 running job 229 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 229)
Launcher: Task 50 running job 230 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 230)
Launcher: Task 61 running job 231 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 231)
Launcher: Task 160 running job 232 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 232)
Launcher: Task 139 running job 234 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 234)
Launcher: Task 175 running job 233 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 233)
Launcher: Task 171 running job 235 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 235)
Launcher: Task 143 running job 236 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 236)
Launcher: Task 177 running job 237 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 237)
Launcher: Task 232 running job 239 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 239)
Launcher: Task 138 running job 238 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 238)
Launcher: Task 121 running job 240 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 240)
Launcher: Task 190 running job 242 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 242)
Launcher: Task 195 running job 241 on c306-021.ls6.tacc.utexas.edu (python3 grainNN.py ini 241)
Launcher: Task 103 running job 243 on c306-020.ls6.tacc.utexas.edu (python3 grainNN.py ini 243)
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  21969
Epoch:0, Train loss:0.774681, valid loss:0.736311
Epoch:1, Train loss:0.510470, valid loss:0.518214
Epoch:2, Train loss:0.494068, valid loss:0.515625
Epoch:3, Train loss:0.491683, valid loss:0.515211
Epoch:4, Train loss:0.490550, valid loss:0.514361
Epoch:5, Train loss:0.489651, valid loss:0.514148
Epoch:6, Train loss:0.489055, valid loss:0.514315
Epoch:7, Train loss:0.488671, valid loss:0.513776
Epoch:8, Train loss:0.488293, valid loss:0.513620
Epoch:9, Train loss:0.488015, valid loss:0.513495
Epoch:10, Train loss:0.487813, valid loss:0.513218
Epoch:11, Train loss:0.486980, valid loss:0.513153
Epoch:12, Train loss:0.486929, valid loss:0.513295
Epoch:13, Train loss:0.486868, valid loss:0.512921
Epoch:14, Train loss:0.486772, valid loss:0.512819
Epoch:15, Train loss:0.486765, valid loss:0.512699
Epoch:16, Train loss:0.486730, valid loss:0.512880
Epoch:17, Train loss:0.486625, valid loss:0.512851
Epoch:18, Train loss:0.486617, valid loss:0.512897
Epoch:19, Train loss:0.486550, valid loss:0.512553
Epoch:20, Train loss:0.486528, valid loss:0.512757
Epoch:21, Train loss:0.486159, valid loss:0.512502
Epoch:22, Train loss:0.486124, valid loss:0.512445
Epoch:23, Train loss:0.486110, valid loss:0.512548
Epoch:24, Train loss:0.486097, valid loss:0.512437
Epoch:25, Train loss:0.486073, valid loss:0.512397
Epoch:26, Train loss:0.486059, valid loss:0.512426
Epoch:27, Train loss:0.486045, valid loss:0.512373
Epoch:28, Train loss:0.486027, valid loss:0.512490
Epoch:29, Train loss:0.485981, valid loss:0.512442
Epoch:30, Train loss:0.485995, valid loss:0.512464
Epoch:31, Train loss:0.485809, valid loss:0.512289
Epoch:32, Train loss:0.485802, valid loss:0.512263
Epoch:33, Train loss:0.485791, valid loss:0.512328
Epoch:34, Train loss:0.485784, valid loss:0.512289
Epoch:35, Train loss:0.485783, valid loss:0.512272
Epoch:36, Train loss:0.485773, valid loss:0.512300
Epoch:37, Train loss:0.485754, valid loss:0.512271
Epoch:38, Train loss:0.485751, valid loss:0.512288
Epoch:39, Train loss:0.485737, valid loss:0.512249
Epoch:40, Train loss:0.485736, valid loss:0.512241
Epoch:41, Train loss:0.485651, valid loss:0.512222
Epoch:42, Train loss:0.485640, valid loss:0.512202
Epoch:43, Train loss:0.485642, valid loss:0.512263
Epoch:44, Train loss:0.485640, valid loss:0.512224
Epoch:45, Train loss:0.485628, valid loss:0.512213
Epoch:46, Train loss:0.485626, valid loss:0.512175
Epoch:47, Train loss:0.485626, valid loss:0.512199
Epoch:48, Train loss:0.485622, valid loss:0.512222
Epoch:49, Train loss:0.485614, valid loss:0.512185
Epoch:50, Train loss:0.485617, valid loss:0.512183
Epoch:51, Train loss:0.485570, valid loss:0.512154
Epoch:52, Train loss:0.485569, valid loss:0.512172
Epoch:53, Train loss:0.485564, valid loss:0.512172
Epoch:54, Train loss:0.485563, valid loss:0.512160
Epoch:55, Train loss:0.485562, valid loss:0.512152
Epoch:56, Train loss:0.485562, valid loss:0.512154
Epoch:57, Train loss:0.485560, valid loss:0.512143
Epoch:58, Train loss:0.485555, valid loss:0.512156
Epoch:59, Train loss:0.485554, valid loss:0.512157
Epoch:60, Train loss:0.485552, valid loss:0.512165
training time 2793.8089270591736
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.6942252215148138
plot_id,batch_id 0 1 miss% 0.7739660623756317
plot_id,batch_id 0 2 miss% 0.7788555162106588
plot_id,batch_id 0 3 miss% 0.7900875575579532
plot_id,batch_id 0 4 miss% 0.7939698145801821
plot_id,batch_id 0 5 miss% 0.6934220447980889
plot_id,batch_id 0 6 miss% 0.7684036924771874
plot_id,batch_id 0 7 miss% 0.7828854418053655
plot_id,batch_id 0 8 miss% 0.7917451157199804
plot_id,batch_id 0 9 miss% 0.7994607966030364
plot_id,batch_id 0 10 miss% 0.6732581931917728
plot_id,batch_id 0 11 miss% 0.7687714234766925
plot_id,batch_id 0 12 miss% 0.7853941086979569
plot_id,batch_id 0 13 miss% 0.789303662589016
plot_id,batch_id 0 14 miss% 0.7904718103644088
plot_id,batch_id 0 15 miss% 0.684945166224037
plot_id,batch_id 0 16 miss% 0.7605142732626579
plot_id,batch_id 0 17 miss% 0.7809782998381669
plot_id,batch_id 0 18 miss% 0.7849395755157486
plot_id,batch_id 0 19 miss% 0.7918863163850522
plot_id,batch_id 0 20 miss% 0.7383197950954518
plot_id,batch_id 0 21 miss% 0.7816194853290308
plot_id,batch_id 0 22 miss% 0.7888101299313292
plot_id,batch_id 0 23 miss% 0.7944190078219194
plot_id,batch_id 0 24 miss% 0.7963021431257618
plot_id,batch_id 0 25 miss% 0.7299689909233096
plot_id,batch_id 0 26 miss% 0.7828742031663146
plot_id,batch_id 0 27 miss% 0.7877576120919868
plot_id,batch_id 0 28 miss% 0.7929178019630946
plot_id,batch_id 0 29 miss% 0.7959785834253632
plot_id,batch_id 0 30 miss% 0.7356370977671998
plot_id,batch_id 0 31 miss% 0.777003979051162
plot_id,batch_id 0 32 miss% 0.7912770452720437
plot_id,batch_id 0 33 miss% 0.7946074535400153
plot_id,batch_id 0 34 miss% 0.7960628692085476
plot_id,batch_id 0 35 miss% 0.7217690611989362
plot_id,batch_id 0 36 miss% 0.7869313058278109
plot_id,batch_id 0 37 miss% 0.782506939632952
plot_id,batch_id 0 38 miss% 0.7964133825791376
plot_id,batch_id 0 39 miss% 0.7973231871925586
plot_id,batch_id 0 40 miss% 0.7681679821025436
plot_id,batch_id 0 41 miss% 0.7897107766604741
plot_id,batch_id 0 42 miss% 0.7921146219616083
plot_id,batch_id 0 43 miss% 0.7986233516677775
plot_id,batch_id 0 44 miss% 0.797610807251423
plot_id,batch_id 0 45 miss% 0.7612890087228334
plot_id,batch_id 0 46 miss% 0.7881486477357097
plot_id,batch_id 0 47 miss% 0.7932856164039813
plot_id,batch_id 0 48 miss% 0.8005638235480791
plot_id,batch_id 0 49 miss% 0.7988910169119712
plot_id,batch_id 0 50 miss% 0.7639757739471497
plot_id,batch_id 0 51 miss% 0.7860322661857297
plot_id,batch_id 0 52 miss% 0.7922652673971716
plot_id,batch_id 0 53 miss% 0.8005870376950541
plot_id,batch_id 0 54 miss% 0.8022066908144461
plot_id,batch_id 0 55 miss% 0.75366371790208
plot_id,batch_id 0 56 miss% 0.7893503223253167
plot_id,batch_id 0 57 miss% 0.7954390844790261
plot_id,batch_id 0 58 miss% 0.7941962087469112
plot_id,batch_id 0 59 miss% 0.7976370651130379
plot_id,batch_id 0 60 miss% 0.6120323675157459
plot_id,batch_id 0 61 miss% 0.730045377864301
plot_id,batch_id 0 62 miss% 0.7596764835499024
plot_id,batch_id 0 63 miss% 0.7764941131773235
plot_id,batch_id 0 64 miss% 0.7832921510370855
plot_id,batch_id 0 65 miss% 0.6082908189139149
plot_id,batch_id 0 66 miss% 0.7354380179817731
plot_id,batch_id 0 67 miss% 0.7428831283844858
plot_id,batch_id 0 68 miss% 0.7781514802125957
plot_id,batch_id 0 69 miss% 0.7775254113461162
plot_id,batch_id 0 70 miss% 0.577802562451349
plot_id,batch_id 0 71 miss% 0.7427125689242255
plot_id,batch_id 0 72 miss% 0.7447759106002317
plot_id,batch_id 0 73 miss% 0.7620322030950215
plot_id,batch_id 0 74 miss% 0.7753821529476361
plot_id,batch_id 0 75 miss% 0.5763398532657517
plot_id,batch_id 0 76 miss% 0.6889695583647976
plot_id,batch_id 0 77 miss% 0.7296776424152829
plot_id,batch_id 0 78 miss% 0.7553402732435404
plot_id,batch_id 0 79 miss% 0.7704637735382569
plot_id,batch_id 0 80 miss% 0.6375179882472594
plot_id,batch_id 0 81 miss% 0.7520057773949337
plot_id,batch_id 0 82 miss% 0.7727595330090065
plot_id,batch_id 0 83 miss% 0.7834509103294585
plot_id,batch_id 0 84 miss% 0.7830386422593651
plot_id,batch_id 0 85 miss% 0.6405285916337341
plot_id,batch_id 0 86 miss% 0.7499187588574516
plot_id,batch_id 0 87 miss% 0.7657254963079599
plot_id,batch_id 0 88 miss% 0.7824700503906741
plot_id,batch_id 0 89 miss% 0.7831626663869541
plot_id,batch_id 0 90 miss% 0.5992237430716157
plot_id,batch_id 0 91 miss% 0.7495939322926856
plot_id,batch_id 0 92 miss% 0.7615431818014273
plot_id,batch_id 0 93 miss% 0.7828952049575546
plot_id,batch_id 0 94 miss% 0.7817872707002975
plot_id,batch_id 0 95 miss% 0.6032686095903554
plot_id,batch_id 0 96 miss% 0.7329840766521034
plot_id,batch_id 0 97 miss% 0.7593247807734033
plot_id,batch_id 0 98 miss% 0.7676568643631932
plot_id,batch_id 0 99 miss% 0.7767152782913417
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.69422522 0.77396606 0.77885552 0.79008756 0.79396981 0.69342204
 0.76840369 0.78288544 0.79174512 0.7994608  0.67325819 0.76877142
 0.78539411 0.78930366 0.79047181 0.68494517 0.76051427 0.7809783
 0.78493958 0.79188632 0.7383198  0.78161949 0.78881013 0.79441901
 0.79630214 0.72996899 0.7828742  0.78775761 0.7929178  0.79597858
 0.7356371  0.77700398 0.79127705 0.79460745 0.79606287 0.72176906
 0.78693131 0.78250694 0.79641338 0.79732319 0.76816798 0.78971078
 0.79211462 0.79862335 0.79761081 0.76128901 0.78814865 0.79328562
 0.80056382 0.79889102 0.76397577 0.78603227 0.79226527 0.80058704
 0.80220669 0.75366372 0.78935032 0.79543908 0.79419621 0.79763707
 0.61203237 0.73004538 0.75967648 0.77649411 0.78329215 0.60829082
 0.73543802 0.74288313 0.77815148 0.77752541 0.57780256 0.74271257
 0.74477591 0.7620322  0.77538215 0.57633985 0.68896956 0.72967764
 0.75534027 0.77046377 0.63751799 0.75200578 0.77275953 0.78345091
 0.78303864 0.64052859 0.74991876 0.7657255  0.78247005 0.78316267
 0.59922374 0.74959393 0.76154318 0.7828952  0.78178727 0.60326861
 0.73298408 0.75932478 0.76765686 0.77671528]
for model  0 the mean error 0.7580264046104476
all id 0 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 21 out win 3 err 0.7580264046104476
Launcher: Job 1 completed in 3006 seconds.
Launcher: Task 9 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  21969
Epoch:0, Train loss:0.774681, valid loss:0.736311
Epoch:1, Train loss:0.506914, valid loss:0.517307
Epoch:2, Train loss:0.493331, valid loss:0.515339
Epoch:3, Train loss:0.490737, valid loss:0.514777
Epoch:4, Train loss:0.489651, valid loss:0.514267
Epoch:5, Train loss:0.489114, valid loss:0.514277
Epoch:6, Train loss:0.488692, valid loss:0.514224
Epoch:7, Train loss:0.488421, valid loss:0.513875
Epoch:8, Train loss:0.488325, valid loss:0.513166
Epoch:9, Train loss:0.487976, valid loss:0.513229
Epoch:10, Train loss:0.487985, valid loss:0.513395
Epoch:11, Train loss:0.486851, valid loss:0.512712
Epoch:12, Train loss:0.486815, valid loss:0.512720
Epoch:13, Train loss:0.486747, valid loss:0.512883
Epoch:14, Train loss:0.486685, valid loss:0.512842
Epoch:15, Train loss:0.486659, valid loss:0.512666
Epoch:16, Train loss:0.486646, valid loss:0.512845
Epoch:17, Train loss:0.486625, valid loss:0.512644
Epoch:18, Train loss:0.486510, valid loss:0.512521
Epoch:19, Train loss:0.486539, valid loss:0.512738
Epoch:20, Train loss:0.486486, valid loss:0.513155
Epoch:21, Train loss:0.486019, valid loss:0.512547
Epoch:22, Train loss:0.486003, valid loss:0.512423
Epoch:23, Train loss:0.485959, valid loss:0.512408
Epoch:24, Train loss:0.486037, valid loss:0.512526
Epoch:25, Train loss:0.485917, valid loss:0.512355
Epoch:26, Train loss:0.485995, valid loss:0.512448
Epoch:27, Train loss:0.485915, valid loss:0.512340
Epoch:28, Train loss:0.485896, valid loss:0.512260
Epoch:29, Train loss:0.485907, valid loss:0.512550
Epoch:30, Train loss:0.485901, valid loss:0.512413
Epoch:31, Train loss:0.485642, valid loss:0.512238
Epoch:32, Train loss:0.485623, valid loss:0.512236
Epoch:33, Train loss:0.485630, valid loss:0.512299
Epoch:34, Train loss:0.485616, valid loss:0.512208
Epoch:35, Train loss:0.485632, valid loss:0.512264
Epoch:36, Train loss:0.485613, valid loss:0.512484
Epoch:37, Train loss:0.485622, valid loss:0.512176
Epoch:38, Train loss:0.485590, valid loss:0.512205
Epoch:39, Train loss:0.485594, valid loss:0.512209
Epoch:40, Train loss:0.485585, valid loss:0.512245
Epoch:41, Train loss:0.485478, valid loss:0.512150
Epoch:42, Train loss:0.485468, valid loss:0.512153
Epoch:43, Train loss:0.485470, valid loss:0.512154
Epoch:44, Train loss:0.485470, valid loss:0.512130
Epoch:45, Train loss:0.485466, valid loss:0.512186
Epoch:46, Train loss:0.485452, valid loss:0.512136
Epoch:47, Train loss:0.485460, valid loss:0.512140
Epoch:48, Train loss:0.485448, valid loss:0.512212
Epoch:49, Train loss:0.485448, valid loss:0.512133
Epoch:50, Train loss:0.485458, valid loss:0.512204
Epoch:51, Train loss:0.485396, valid loss:0.512101
Epoch:52, Train loss:0.485393, valid loss:0.512112
Epoch:53, Train loss:0.485387, valid loss:0.512145
Epoch:54, Train loss:0.485388, valid loss:0.512121
Epoch:55, Train loss:0.485385, valid loss:0.512111
Epoch:56, Train loss:0.485385, valid loss:0.512113
Epoch:57, Train loss:0.485387, valid loss:0.512119
Epoch:58, Train loss:0.485378, valid loss:0.512130
Epoch:59, Train loss:0.485378, valid loss:0.512153
Epoch:60, Train loss:0.485383, valid loss:0.512111
training time 2939.528538942337
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.6983860883063054
plot_id,batch_id 0 1 miss% 0.77305118926561
plot_id,batch_id 0 2 miss% 0.7801189362286314
plot_id,batch_id 0 3 miss% 0.7891962516105959
plot_id,batch_id 0 4 miss% 0.792894373503538
plot_id,batch_id 0 5 miss% 0.6974718133039948
plot_id,batch_id 0 6 miss% 0.7679810026089465
plot_id,batch_id 0 7 miss% 0.7827356033871982
plot_id,batch_id 0 8 miss% 0.7876306872699721
plot_id,batch_id 0 9 miss% 0.7975613583559328
plot_id,batch_id 0 10 miss% 0.6768645945438346
plot_id,batch_id 0 11 miss% 0.7672673177431678
plot_id,batch_id 0 12 miss% 0.7788043910300814
plot_id,batch_id 0 13 miss% 0.7882272863123013
plot_id,batch_id 0 14 miss% 0.7893868469334117
plot_id,batch_id 0 15 miss% 0.6836267834584518
plot_id,batch_id 0 16 miss% 0.76410376682235
plot_id,batch_id 0 17 miss% 0.7801066860076665
plot_id,batch_id 0 18 miss% 0.78580154110744
plot_id,batch_id 0 19 miss% 0.789717762010974
plot_id,batch_id 0 20 miss% 0.755196940697482
plot_id,batch_id 0 21 miss% 0.7818304076761341
plot_id,batch_id 0 22 miss% 0.7882474931913743
plot_id,batch_id 0 23 miss% 0.7918365918416911
plot_id,batch_id 0 24 miss% 0.7949559654637718
plot_id,batch_id 0 25 miss% 0.7308056421450438
plot_id,batch_id 0 26 miss% 0.7818234517425879
plot_id,batch_id 0 27 miss% 0.7854727974590336
plot_id,batch_id 0 28 miss% 0.7908176127647328
plot_id,batch_id 0 29 miss% 0.7927497485593464
plot_id,batch_id 0 30 miss% 0.7334013178588916
plot_id,batch_id 0 31 miss% 0.779340646330598
plot_id,batch_id 0 32 miss% 0.7877485662240309
plot_id,batch_id 0 33 miss% 0.7912965414396442
plot_id,batch_id 0 34 miss% 0.7924868938648945
plot_id,batch_id 0 35 miss% 0.7311881957123283
plot_id,batch_id 0 36 miss% 0.7817893013449553
plot_id,batch_id 0 37 miss% 0.7822073280847044
plot_id,batch_id 0 38 miss% 0.7930852129308087
plot_id,batch_id 0 39 miss% 0.7937394535019523
plot_id,batch_id 0 40 miss% 0.767423696079251
plot_id,batch_id 0 41 miss% 0.7890682506789656
plot_id,batch_id 0 42 miss% 0.792175394625171
plot_id,batch_id 0 43 miss% 0.796529342798779
plot_id,batch_id 0 44 miss% 0.7971041836975027
plot_id,batch_id 0 45 miss% 0.7601622137627743
plot_id,batch_id 0 46 miss% 0.7874895101219781
plot_id,batch_id 0 47 miss% 0.7918248771939194
plot_id,batch_id 0 48 miss% 0.7977611730741981
plot_id,batch_id 0 49 miss% 0.7974321799271211
plot_id,batch_id 0 50 miss% 0.7681280982757328
plot_id,batch_id 0 51 miss% 0.785887918408589
plot_id,batch_id 0 52 miss% 0.79216668519765
plot_id,batch_id 0 53 miss% 0.7958169996684048
plot_id,batch_id 0 54 miss% 0.7995914696442256
plot_id,batch_id 0 55 miss% 0.7528683520671607
plot_id,batch_id 0 56 miss% 0.7840455830270694
plot_id,batch_id 0 57 miss% 0.7943964591514867
plot_id,batch_id 0 58 miss% 0.7933908336963187
plot_id,batch_id 0 59 miss% 0.7966718992436512
plot_id,batch_id 0 60 miss% 0.6142614403296507
plot_id,batch_id 0 61 miss% 0.7325231114099079
plot_id,batch_id 0 62 miss% 0.7642327172840379
plot_id,batch_id 0 63 miss% 0.7779047362655295
plot_id,batch_id 0 64 miss% 0.7803683148865643
plot_id,batch_id 0 65 miss% 0.6122061445886576
plot_id,batch_id 0 66 miss% 0.7329509649123969
plot_id,batch_id 0 67 miss% 0.7452078976049655
plot_id,batch_id 0 68 miss% 0.7739196691182596
plot_id,batch_id 0 69 miss% 0.78047541849331
plot_id,batch_id 0 70 miss% 0.5762842744349727
plot_id,batch_id 0 71 miss% 0.7436237093587553
plot_id,batch_id 0 72 miss% 0.7430758654527191
plot_id,batch_id 0 73 miss% 0.7612226896531549
plot_id,batch_id 0 74 miss% 0.7699631391228924
plot_id,batch_id 0 75 miss% 0.5620911091927525
plot_id,batch_id 0 76 miss% 0.691627463451464
plot_id,batch_id 0 77 miss% 0.7384952997642572
plot_id,batch_id 0 78 miss% 0.766911604066939
plot_id,batch_id 0 79 miss% 0.7650972991829413
plot_id,batch_id 0 80 miss% 0.6317296908903338
plot_id,batch_id 0 81 miss% 0.7565080085868959
plot_id,batch_id 0 82 miss% 0.7734515671920822
plot_id,batch_id 0 83 miss% 0.7835414947481425
plot_id,batch_id 0 84 miss% 0.7826532861947846
plot_id,batch_id 0 85 miss% 0.6345989401067675
plot_id,batch_id 0 86 miss% 0.7496577859425735
plot_id,batch_id 0 87 miss% 0.7695282595263988
plot_id,batch_id 0 88 miss% 0.7827432625143692
plot_id,batch_id 0 89 miss% 0.7865324743541293
plot_id,batch_id 0 90 miss% 0.6090449254790681
plot_id,batch_id 0 91 miss% 0.7424286935126456
plot_id,batch_id 0 92 miss% 0.7594092387117798
plot_id,batch_id 0 93 miss% 0.7740099134654439
plot_id,batch_id 0 94 miss% 0.7825641718299837
plot_id,batch_id 0 95 miss% 0.6035829419675723
plot_id,batch_id 0 96 miss% 0.7221794157208021
plot_id,batch_id 0 97 miss% 0.7542451995520226
plot_id,batch_id 0 98 miss% 0.7693891959489402
plot_id,batch_id 0 99 miss% 0.7747270274156214
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.69838609 0.77305119 0.78011894 0.78919625 0.79289437 0.69747181
 0.767981   0.7827356  0.78763069 0.79756136 0.67686459 0.76726732
 0.77880439 0.78822729 0.78938685 0.68362678 0.76410377 0.78010669
 0.78580154 0.78971776 0.75519694 0.78183041 0.78824749 0.79183659
 0.79495597 0.73080564 0.78182345 0.7854728  0.79081761 0.79274975
 0.73340132 0.77934065 0.78774857 0.79129654 0.79248689 0.7311882
 0.7817893  0.78220733 0.79308521 0.79373945 0.7674237  0.78906825
 0.79217539 0.79652934 0.79710418 0.76016221 0.78748951 0.79182488
 0.79776117 0.79743218 0.7681281  0.78588792 0.79216669 0.795817
 0.79959147 0.75286835 0.78404558 0.79439646 0.79339083 0.7966719
 0.61426144 0.73252311 0.76423272 0.77790474 0.78036831 0.61220614
 0.73295096 0.7452079  0.77391967 0.78047542 0.57628427 0.74362371
 0.74307587 0.76122269 0.76996314 0.56209111 0.69162746 0.7384953
 0.7669116  0.7650973  0.63172969 0.75650801 0.77345157 0.78354149
 0.78265329 0.63459894 0.74965779 0.76952826 0.78274326 0.78653247
 0.60904493 0.74242869 0.75940924 0.77400991 0.78256417 0.60358294
 0.72217942 0.7542452  0.7693892  0.77472703]
for model  27 the mean error 0.7574585787121886
all id 27 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 21 out win 3 err 0.7574585787121886
Launcher: Job 28 completed in 3122 seconds.
Launcher: Task 107 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  21969
Epoch:0, Train loss:0.774681, valid loss:0.736311
Epoch:1, Train loss:0.506836, valid loss:0.517388
Epoch:2, Train loss:0.493806, valid loss:0.516123
Epoch:3, Train loss:0.491313, valid loss:0.514492
Epoch:4, Train loss:0.490155, valid loss:0.514636
Epoch:5, Train loss:0.489972, valid loss:0.515132
Epoch:6, Train loss:0.489388, valid loss:0.514666
Epoch:7, Train loss:0.489082, valid loss:0.514348
Epoch:8, Train loss:0.488890, valid loss:0.514988
Epoch:9, Train loss:0.488614, valid loss:0.513601
Epoch:10, Train loss:0.488582, valid loss:0.514013
Epoch:11, Train loss:0.487141, valid loss:0.513323
Epoch:12, Train loss:0.487058, valid loss:0.513094
Epoch:13, Train loss:0.487133, valid loss:0.512736
Epoch:14, Train loss:0.487109, valid loss:0.513232
Epoch:15, Train loss:0.486921, valid loss:0.512755
Epoch:16, Train loss:0.487044, valid loss:0.512795
Epoch:17, Train loss:0.486853, valid loss:0.512594
Epoch:18, Train loss:0.486774, valid loss:0.513208
Epoch:19, Train loss:0.486821, valid loss:0.512660
Epoch:20, Train loss:0.486907, valid loss:0.513664
Epoch:21, Train loss:0.486150, valid loss:0.512311
Epoch:22, Train loss:0.486054, valid loss:0.512341
Epoch:23, Train loss:0.486123, valid loss:0.512351
Epoch:24, Train loss:0.486133, valid loss:0.512420
Epoch:25, Train loss:0.486089, valid loss:0.512572
Epoch:26, Train loss:0.486064, valid loss:0.512269
Epoch:27, Train loss:0.486059, valid loss:0.512260
Epoch:28, Train loss:0.486036, valid loss:0.512372
Epoch:29, Train loss:0.486035, valid loss:0.512401
Epoch:30, Train loss:0.486071, valid loss:0.512493
Epoch:31, Train loss:0.485705, valid loss:0.512221
Epoch:32, Train loss:0.485703, valid loss:0.512120
Epoch:33, Train loss:0.485690, valid loss:0.512109
Epoch:34, Train loss:0.485684, valid loss:0.512233
Epoch:35, Train loss:0.485711, valid loss:0.512180
Epoch:36, Train loss:0.485678, valid loss:0.512116
Epoch:37, Train loss:0.485701, valid loss:0.512219
Epoch:38, Train loss:0.485661, valid loss:0.512245
Epoch:39, Train loss:0.485681, valid loss:0.512135
Epoch:40, Train loss:0.485665, valid loss:0.512279
Epoch:41, Train loss:0.485511, valid loss:0.512069
Epoch:42, Train loss:0.485504, valid loss:0.512086
Epoch:43, Train loss:0.485504, valid loss:0.512022
Epoch:44, Train loss:0.485509, valid loss:0.512081
Epoch:45, Train loss:0.485506, valid loss:0.512141
Epoch:46, Train loss:0.485491, valid loss:0.512107
Epoch:47, Train loss:0.485492, valid loss:0.512123
Epoch:48, Train loss:0.485496, valid loss:0.512079
Epoch:49, Train loss:0.485478, valid loss:0.512061
Epoch:50, Train loss:0.485491, valid loss:0.512086
Epoch:51, Train loss:0.485406, valid loss:0.512014
Epoch:52, Train loss:0.485404, valid loss:0.512010
Epoch:53, Train loss:0.485405, valid loss:0.511998
Epoch:54, Train loss:0.485404, valid loss:0.512009
Epoch:55, Train loss:0.485399, valid loss:0.512006
Epoch:56, Train loss:0.485392, valid loss:0.512047
Epoch:57, Train loss:0.485406, valid loss:0.512025
Epoch:58, Train loss:0.485396, valid loss:0.511982
Epoch:59, Train loss:0.485392, valid loss:0.512013
Epoch:60, Train loss:0.485391, valid loss:0.512020
training time 3015.7219457626343
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.6959379313298917
plot_id,batch_id 0 1 miss% 0.7730353556341459
plot_id,batch_id 0 2 miss% 0.7821172524962651
plot_id,batch_id 0 3 miss% 0.7891341365009518
plot_id,batch_id 0 4 miss% 0.7931402755635516
plot_id,batch_id 0 5 miss% 0.6912933270210984
plot_id,batch_id 0 6 miss% 0.7676853089159347
plot_id,batch_id 0 7 miss% 0.7817247055735856
plot_id,batch_id 0 8 miss% 0.7889024788381019
plot_id,batch_id 0 9 miss% 0.7966081591645188
plot_id,batch_id 0 10 miss% 0.6669733235039799
plot_id,batch_id 0 11 miss% 0.766705659605735
plot_id,batch_id 0 12 miss% 0.7785195380039401
plot_id,batch_id 0 13 miss% 0.7870034146421978
plot_id,batch_id 0 14 miss% 0.787562300642822
plot_id,batch_id 0 15 miss% 0.683861224197484
plot_id,batch_id 0 16 miss% 0.763133720642907
plot_id,batch_id 0 17 miss% 0.7810134388467634
plot_id,batch_id 0 18 miss% 0.7845039992215043
plot_id,batch_id 0 19 miss% 0.7905117497703733
plot_id,batch_id 0 20 miss% 0.7412866692865917
plot_id,batch_id 0 21 miss% 0.784460304466851
plot_id,batch_id 0 22 miss% 0.7893109726773297
plot_id,batch_id 0 23 miss% 0.7944493471284445
plot_id,batch_id 0 24 miss% 0.794725677075932
plot_id,batch_id 0 25 miss% 0.7271705885929747
plot_id,batch_id 0 26 miss% 0.7813711363661853
plot_id,batch_id 0 27 miss% 0.7872298031984899
plot_id,batch_id 0 28 miss% 0.7928742148799124
plot_id,batch_id 0 29 miss% 0.7948906053927279
plot_id,batch_id 0 30 miss% 0.7330906815570706
plot_id,batch_id 0 31 miss% 0.7788900962663108
plot_id,batch_id 0 32 miss% 0.7900834716671625
plot_id,batch_id 0 33 miss% 0.7919658198109394
plot_id,batch_id 0 34 miss% 0.7962302412832593
plot_id,batch_id 0 35 miss% 0.7266869460173523
plot_id,batch_id 0 36 miss% 0.7849975803130996
plot_id,batch_id 0 37 miss% 0.7812333106873588
plot_id,batch_id 0 38 miss% 0.7943750407269193
plot_id,batch_id 0 39 miss% 0.7975408943950343
plot_id,batch_id 0 40 miss% 0.7627830684772193
plot_id,batch_id 0 41 miss% 0.7887625018607861
plot_id,batch_id 0 42 miss% 0.7901289853490963
plot_id,batch_id 0 43 miss% 0.7990555517502638
plot_id,batch_id 0 44 miss% 0.7985569977828508
plot_id,batch_id 0 45 miss% 0.7623586204126968
plot_id,batch_id 0 46 miss% 0.7875435844377876
plot_id,batch_id 0 47 miss% 0.7930088461862084
plot_id,batch_id 0 48 miss% 0.7985040646488696
plot_id,batch_id 0 49 miss% 0.7983620107421754
plot_id,batch_id 0 50 miss% 0.7655314194935333
plot_id,batch_id 0 51 miss% 0.7870401270201383
plot_id,batch_id 0 52 miss% 0.7922270045978128
plot_id,batch_id 0 53 miss% 0.8000585862405131
plot_id,batch_id 0 54 miss% 0.8011307958319861
plot_id,batch_id 0 55 miss% 0.7595852130054795
plot_id,batch_id 0 56 miss% 0.7820360596085421
plot_id,batch_id 0 57 miss% 0.7949210370251815
plot_id,batch_id 0 58 miss% 0.7945679868062175
plot_id,batch_id 0 59 miss% 0.7969078394472712
plot_id,batch_id 0 60 miss% 0.6101072012990363
plot_id,batch_id 0 61 miss% 0.734317946851925
plot_id,batch_id 0 62 miss% 0.7597111198676634
plot_id,batch_id 0 63 miss% 0.7788251692419469
plot_id,batch_id 0 64 miss% 0.7838439265752918
plot_id,batch_id 0 65 miss% 0.6015693099174305
plot_id,batch_id 0 66 miss% 0.7376321454018735
plot_id,batch_id 0 67 miss% 0.7472262114717768
plot_id,batch_id 0 68 miss% 0.7795381087162087
plot_id,batch_id 0 69 miss% 0.7809892979858436
plot_id,batch_id 0 70 miss% 0.570473482026859
plot_id,batch_id 0 71 miss% 0.7374014377826604
plot_id,batch_id 0 72 miss% 0.7464606867107126
plot_id,batch_id 0 73 miss% 0.7642943693408097
plot_id,batch_id 0 74 miss% 0.7697342832573671
plot_id,batch_id 0 75 miss% 0.5697227743546254
plot_id,batch_id 0 76 miss% 0.689918509441118
plot_id,batch_id 0 77 miss% 0.7359531006998717
plot_id,batch_id 0 78 miss% 0.7593420801482487
plot_id,batch_id 0 79 miss% 0.7682586915661531
plot_id,batch_id 0 80 miss% 0.6371160289949039
plot_id,batch_id 0 81 miss% 0.754666029533215
plot_id,batch_id 0 82 miss% 0.7743811469188762
plot_id,batch_id 0 83 miss% 0.7837792175419135
plot_id,batch_id 0 84 miss% 0.7846708017422296
plot_id,batch_id 0 85 miss% 0.6332260474900271
plot_id,batch_id 0 86 miss% 0.74732627103876
plot_id,batch_id 0 87 miss% 0.7687224811339406
plot_id,batch_id 0 88 miss% 0.7807048088246854
plot_id,batch_id 0 89 miss% 0.7825685298613168
plot_id,batch_id 0 90 miss% 0.6017771833159253
plot_id,batch_id 0 91 miss% 0.7441406937693027
plot_id,batch_id 0 92 miss% 0.7657149558393435
plot_id,batch_id 0 93 miss% 0.7736857120783294
plot_id,batch_id 0 94 miss% 0.7817317116061023
plot_id,batch_id 0 95 miss% 0.604122789998654
plot_id,batch_id 0 96 miss% 0.7294542318342764
plot_id,batch_id 0 97 miss% 0.7537869774189848
plot_id,batch_id 0 98 miss% 0.7703001076435924
plot_id,batch_id 0 99 miss% 0.7785344498463898
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.69593793 0.77303536 0.78211725 0.78913414 0.79314028 0.69129333
 0.76768531 0.78172471 0.78890248 0.79660816 0.66697332 0.76670566
 0.77851954 0.78700341 0.7875623  0.68386122 0.76313372 0.78101344
 0.784504   0.79051175 0.74128667 0.7844603  0.78931097 0.79444935
 0.79472568 0.72717059 0.78137114 0.7872298  0.79287421 0.79489061
 0.73309068 0.7788901  0.79008347 0.79196582 0.79623024 0.72668695
 0.78499758 0.78123331 0.79437504 0.79754089 0.76278307 0.7887625
 0.79012899 0.79905555 0.798557   0.76235862 0.78754358 0.79300885
 0.79850406 0.79836201 0.76553142 0.78704013 0.792227   0.80005859
 0.8011308  0.75958521 0.78203606 0.79492104 0.79456799 0.79690784
 0.6101072  0.73431795 0.75971112 0.77882517 0.78384393 0.60156931
 0.73763215 0.74722621 0.77953811 0.7809893  0.57047348 0.73740144
 0.74646069 0.76429437 0.76973428 0.56972277 0.68991851 0.7359531
 0.75934208 0.76825869 0.63711603 0.75466603 0.77438115 0.78377922
 0.7846708  0.63322605 0.74732627 0.76872248 0.78070481 0.78256853
 0.60177718 0.74414069 0.76571496 0.77368571 0.78173171 0.60412279
 0.72945423 0.75378698 0.77030011 0.77853445]
for model  54 the mean error 0.7574103103171852
all id 54 hidden_dim 16 learning_rate 0.02 num_layers 3 frames 21 out win 3 err 0.7574103103171852
Launcher: Job 55 completed in 3198 seconds.
Launcher: Task 112 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  21969
Epoch:0, Train loss:0.731444, valid loss:0.695949
Epoch:1, Train loss:0.514891, valid loss:0.519916
Epoch:2, Train loss:0.498766, valid loss:0.517912
Epoch:3, Train loss:0.496433, valid loss:0.517290
Epoch:4, Train loss:0.495232, valid loss:0.516755
Epoch:5, Train loss:0.494202, valid loss:0.516300
Epoch:6, Train loss:0.493431, valid loss:0.516254
Epoch:7, Train loss:0.492945, valid loss:0.515519
Epoch:8, Train loss:0.492542, valid loss:0.516031
Epoch:9, Train loss:0.492247, valid loss:0.515688
Epoch:10, Train loss:0.492054, valid loss:0.515246
Epoch:11, Train loss:0.491113, valid loss:0.515149
Epoch:12, Train loss:0.491005, valid loss:0.514939
Epoch:13, Train loss:0.490950, valid loss:0.515103
Epoch:14, Train loss:0.490858, valid loss:0.514882
Epoch:15, Train loss:0.490811, valid loss:0.514956
Epoch:16, Train loss:0.490720, valid loss:0.514794
Epoch:17, Train loss:0.490635, valid loss:0.514954
Epoch:18, Train loss:0.490707, valid loss:0.514859
Epoch:19, Train loss:0.490529, valid loss:0.514833
Epoch:20, Train loss:0.490523, valid loss:0.514709
Epoch:21, Train loss:0.490067, valid loss:0.514735
Epoch:22, Train loss:0.490057, valid loss:0.514518
Epoch:23, Train loss:0.490031, valid loss:0.514588
Epoch:24, Train loss:0.490055, valid loss:0.514520
Epoch:25, Train loss:0.490008, valid loss:0.514633
Epoch:26, Train loss:0.489976, valid loss:0.514540
Epoch:27, Train loss:0.489949, valid loss:0.514633
Epoch:28, Train loss:0.489956, valid loss:0.514426
Epoch:29, Train loss:0.489932, valid loss:0.514760
Epoch:30, Train loss:0.489877, valid loss:0.514422
Epoch:31, Train loss:0.489681, valid loss:0.514344
Epoch:32, Train loss:0.489678, valid loss:0.514332
Epoch:33, Train loss:0.489670, valid loss:0.514315
Epoch:34, Train loss:0.489644, valid loss:0.514360
Epoch:35, Train loss:0.489651, valid loss:0.514367
Epoch:36, Train loss:0.489635, valid loss:0.514327
Epoch:37, Train loss:0.489620, valid loss:0.514290
Epoch:38, Train loss:0.489630, valid loss:0.514320
Epoch:39, Train loss:0.489601, valid loss:0.514349
Epoch:40, Train loss:0.489599, valid loss:0.514227
Epoch:41, Train loss:0.489501, valid loss:0.514232
Epoch:42, Train loss:0.489481, valid loss:0.514259
Epoch:43, Train loss:0.489478, valid loss:0.514319
Epoch:44, Train loss:0.489475, valid loss:0.514258
Epoch:45, Train loss:0.489471, valid loss:0.514222
Epoch:46, Train loss:0.489463, valid loss:0.514227
Epoch:47, Train loss:0.489465, valid loss:0.514212
Epoch:48, Train loss:0.489458, valid loss:0.514253
Epoch:49, Train loss:0.489450, valid loss:0.514239
Epoch:50, Train loss:0.489452, valid loss:0.514255
Epoch:51, Train loss:0.489397, valid loss:0.514200
Epoch:52, Train loss:0.489392, valid loss:0.514201
Epoch:53, Train loss:0.489389, valid loss:0.514214
Epoch:54, Train loss:0.489389, valid loss:0.514207
Epoch:55, Train loss:0.489384, valid loss:0.514191
Epoch:56, Train loss:0.489383, valid loss:0.514259
Epoch:57, Train loss:0.489381, valid loss:0.514189
Epoch:58, Train loss:0.489382, valid loss:0.514208
Epoch:59, Train loss:0.489375, valid loss:0.514191
Epoch:60, Train loss:0.489371, valid loss:0.514193
training time 3466.201190471649
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.7755532389582087
plot_id,batch_id 0 1 miss% 0.8249741421697525
plot_id,batch_id 0 2 miss% 0.8319002078307021
plot_id,batch_id 0 3 miss% 0.8340428091956512
plot_id,batch_id 0 4 miss% 0.8354609234537054
plot_id,batch_id 0 5 miss% 0.7667595792661333
plot_id,batch_id 0 6 miss% 0.8174579630128963
plot_id,batch_id 0 7 miss% 0.8286301521009903
plot_id,batch_id 0 8 miss% 0.8360632235601309
plot_id,batch_id 0 9 miss% 0.8381762177618731
plot_id,batch_id 0 10 miss% 0.7482358172223558
plot_id,batch_id 0 11 miss% 0.8230420428979652
plot_id,batch_id 0 12 miss% 0.8262024165832372
plot_id,batch_id 0 13 miss% 0.8326560129165012
plot_id,batch_id 0 14 miss% 0.8328327889623166
plot_id,batch_id 0 15 miss% 0.7728378069947827
plot_id,batch_id 0 16 miss% 0.8167154136462667
plot_id,batch_id 0 17 miss% 0.8312431157409186
plot_id,batch_id 0 18 miss% 0.8322659854475613
plot_id,batch_id 0 19 miss% 0.8356107397007764
plot_id,batch_id 0 20 miss% 0.7938228895368544
plot_id,batch_id 0 21 miss% 0.8303222975781934
plot_id,batch_id 0 22 miss% 0.8378853929155949
plot_id,batch_id 0 23 miss% 0.8363042411464515
plot_id,batch_id 0 24 miss% 0.842056805517882
plot_id,batch_id 0 25 miss% 0.7947177337391993
plot_id,batch_id 0 26 miss% 0.8285005080718382
plot_id,batch_id 0 27 miss% 0.8321216619491701
plot_id,batch_id 0 28 miss% 0.8351206792756842
plot_id,batch_id 0 29 miss% 0.8374353794532764
plot_id,batch_id 0 30 miss% 0.7926306067423907
plot_id,batch_id 0 31 miss% 0.8228124118831257
plot_id,batch_id 0 32 miss% 0.8346314530948247
plot_id,batch_id 0 33 miss% 0.8360783486954733
plot_id,batch_id 0 34 miss% 0.8378419108474082
plot_id,batch_id 0 35 miss% 0.7878624189278431
plot_id,batch_id 0 36 miss% 0.8297911266825292
plot_id,batch_id 0 37 miss% 0.832452598527677
plot_id,batch_id 0 38 miss% 0.837554924500357
plot_id,batch_id 0 39 miss% 0.8397272218500987
plot_id,batch_id 0 40 miss% 0.8143193965585095
plot_id,batch_id 0 41 miss% 0.8369463676524145
plot_id,batch_id 0 42 miss% 0.8394925376914149
plot_id,batch_id 0 43 miss% 0.842810702116413
plot_id,batch_id 0 44 miss% 0.8454734530772867
plot_id,batch_id 0 45 miss% 0.8138309507753027
plot_id,batch_id 0 46 miss% 0.833876455348236
plot_id,batch_id 0 47 miss% 0.839509114205201
plot_id,batch_id 0 48 miss% 0.8401609405578018
plot_id,batch_id 0 49 miss% 0.8450072014974579
plot_id,batch_id 0 50 miss% 0.8208412255103038
plot_id,batch_id 0 51 miss% 0.8365988982024711
plot_id,batch_id 0 52 miss% 0.8354881514846505
plot_id,batch_id 0 53 miss% 0.8411521837211416
plot_id,batch_id 0 54 miss% 0.8458034214619615
plot_id,batch_id 0 55 miss% 0.8160667900385724
plot_id,batch_id 0 56 miss% 0.8342513592882257
plot_id,batch_id 0 57 miss% 0.8382325448446161
plot_id,batch_id 0 58 miss% 0.8392117182605527
plot_id,batch_id 0 59 miss% 0.8433866961311332
plot_id,batch_id 0 60 miss% 0.7132961786847388
plot_id,batch_id 0 61 miss% 0.8051192018720064
plot_id,batch_id 0 62 miss% 0.8156184237258147
plot_id,batch_id 0 63 miss% 0.8278957951763399
plot_id,batch_id 0 64 miss% 0.8298099430694666
plot_id,batch_id 0 65 miss% 0.7079159787237131
plot_id,batch_id 0 66 miss% 0.7915552313208503
plot_id,batch_id 0 67 miss% 0.8111632809339582
plot_id,batch_id 0 68 miss% 0.8238796158455373
plot_id,batch_id 0 69 miss% 0.8278569074232136
plot_id,batch_id 0 70 miss% 0.668030407663686
plot_id,batch_id 0 71 miss% 0.7961042953980434
plot_id,batch_id 0 72 miss% 0.8025844312504943
plot_id,batch_id 0 73 miss% 0.815789601532244
plot_id,batch_id 0 74 miss% 0.8219939752869335
plot_id,batch_id 0 75 miss% 0.6643777272444427
plot_id,batch_id 0 76 miss% 0.7875805491607791
plot_id,batch_id 0 77 miss% 0.7907276304575
plot_id,batch_id 0 78 miss% 0.8109918566817674
plot_id,batch_id 0 79 miss% 0.8142883906352217
plot_id,batch_id 0 80 miss% 0.7341615615643189
plot_id,batch_id 0 81 miss% 0.8197657186954803
plot_id,batch_id 0 82 miss% 0.8262120739962967
plot_id,batch_id 0 83 miss% 0.8320979570393217
plot_id,batch_id 0 84 miss% 0.8333975942154451
plot_id,batch_id 0 85 miss% 0.7248754668435714
plot_id,batch_id 0 86 miss% 0.8052960031394714
plot_id,batch_id 0 87 miss% 0.8217363699140542
plot_id,batch_id 0 88 miss% 0.8294808542410754
plot_id,batch_id 0 89 miss% 0.8315409039752925
plot_id,batch_id 0 90 miss% 0.7019895403412236
plot_id,batch_id 0 91 miss% 0.800808067687774
plot_id,batch_id 0 92 miss% 0.8164665427966084
plot_id,batch_id 0 93 miss% 0.8285952210365521
plot_id,batch_id 0 94 miss% 0.8297241444018009
plot_id,batch_id 0 95 miss% 0.7129277246420016
plot_id,batch_id 0 96 miss% 0.7932787837003881
plot_id,batch_id 0 97 miss% 0.8096804960549797
plot_id,batch_id 0 98 miss% 0.8201846536772973
plot_id,batch_id 0 99 miss% 0.8265302408406736
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.77555324 0.82497414 0.83190021 0.83404281 0.83546092 0.76675958
 0.81745796 0.82863015 0.83606322 0.83817622 0.74823582 0.82304204
 0.82620242 0.83265601 0.83283279 0.77283781 0.81671541 0.83124312
 0.83226599 0.83561074 0.79382289 0.8303223  0.83788539 0.83630424
 0.84205681 0.79471773 0.82850051 0.83212166 0.83512068 0.83743538
 0.79263061 0.82281241 0.83463145 0.83607835 0.83784191 0.78786242
 0.82979113 0.8324526  0.83755492 0.83972722 0.8143194  0.83694637
 0.83949254 0.8428107  0.84547345 0.81383095 0.83387646 0.83950911
 0.84016094 0.8450072  0.82084123 0.8365989  0.83548815 0.84115218
 0.84580342 0.81606679 0.83425136 0.83823254 0.83921172 0.8433867
 0.71329618 0.8051192  0.81561842 0.8278958  0.82980994 0.70791598
 0.79155523 0.81116328 0.82387962 0.82785691 0.66803041 0.7961043
 0.80258443 0.8157896  0.82199398 0.66437773 0.78758055 0.79072763
 0.81099186 0.81428839 0.73416156 0.81976572 0.82621207 0.83209796
 0.83339759 0.72487547 0.805296   0.82173637 0.82948085 0.8315409
 0.70198954 0.80080807 0.81646654 0.82859522 0.82972414 0.71292772
 0.79327878 0.8096805  0.82018465 0.82653024]
for model  1 the mean error 0.8128812065767264
all id 1 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 21 out win 4 err 0.8128812065767264
Launcher: Job 2 completed in 3649 seconds.
Launcher: Task 41 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  21969
Epoch:0, Train loss:0.613368, valid loss:0.573821
Epoch:1, Train loss:0.351575, valid loss:0.358948
Epoch:2, Train loss:0.341841, valid loss:0.357992
Epoch:3, Train loss:0.340528, valid loss:0.357402
Epoch:4, Train loss:0.339836, valid loss:0.357044
Epoch:5, Train loss:0.339396, valid loss:0.357481
Epoch:6, Train loss:0.338998, valid loss:0.356943
Epoch:7, Train loss:0.338847, valid loss:0.356354
Epoch:8, Train loss:0.338630, valid loss:0.356843
Epoch:9, Train loss:0.338523, valid loss:0.357542
Epoch:10, Train loss:0.338367, valid loss:0.356647
Epoch:11, Train loss:0.337610, valid loss:0.356055
Epoch:12, Train loss:0.337648, valid loss:0.356458
Epoch:13, Train loss:0.337568, valid loss:0.355855
Epoch:14, Train loss:0.337549, valid loss:0.355919
Epoch:15, Train loss:0.337486, valid loss:0.355928
Epoch:16, Train loss:0.337480, valid loss:0.356016
Epoch:17, Train loss:0.337459, valid loss:0.356027
Epoch:18, Train loss:0.337406, valid loss:0.355985
Epoch:19, Train loss:0.337454, valid loss:0.355859
Epoch:20, Train loss:0.337331, valid loss:0.355997
Epoch:21, Train loss:0.337005, valid loss:0.355634
Epoch:22, Train loss:0.336993, valid loss:0.355649
Epoch:23, Train loss:0.337000, valid loss:0.355728
Epoch:24, Train loss:0.337007, valid loss:0.355692
Epoch:25, Train loss:0.336963, valid loss:0.355687
Epoch:26, Train loss:0.336981, valid loss:0.355958
Epoch:27, Train loss:0.336969, valid loss:0.355727
Epoch:28, Train loss:0.336976, valid loss:0.355680
Epoch:29, Train loss:0.336939, valid loss:0.355649
Epoch:30, Train loss:0.336916, valid loss:0.355691
Epoch:31, Train loss:0.336772, valid loss:0.355573
Epoch:32, Train loss:0.336769, valid loss:0.355604
Epoch:33, Train loss:0.336767, valid loss:0.355582
Epoch:34, Train loss:0.336754, valid loss:0.355579
Epoch:35, Train loss:0.336762, valid loss:0.355559
Epoch:36, Train loss:0.336744, valid loss:0.355544
Epoch:37, Train loss:0.336747, valid loss:0.355582
Epoch:38, Train loss:0.336749, valid loss:0.355589
Epoch:39, Train loss:0.336725, valid loss:0.355561
Epoch:40, Train loss:0.336738, valid loss:0.355581
Epoch:41, Train loss:0.336651, valid loss:0.355518
Epoch:42, Train loss:0.336646, valid loss:0.355527
Epoch:43, Train loss:0.336639, valid loss:0.355523
Epoch:44, Train loss:0.336640, valid loss:0.355557
Epoch:45, Train loss:0.336637, valid loss:0.355511
Epoch:46, Train loss:0.336633, valid loss:0.355527
Epoch:47, Train loss:0.336633, valid loss:0.355545
Epoch:48, Train loss:0.336627, valid loss:0.355517
Epoch:49, Train loss:0.336627, valid loss:0.355545
Epoch:50, Train loss:0.336625, valid loss:0.355549
Epoch:51, Train loss:0.336588, valid loss:0.355488
Epoch:52, Train loss:0.336579, valid loss:0.355529
Epoch:53, Train loss:0.336579, valid loss:0.355527
Epoch:54, Train loss:0.336580, valid loss:0.355518
Epoch:55, Train loss:0.336578, valid loss:0.355498
Epoch:56, Train loss:0.336576, valid loss:0.355504
Epoch:57, Train loss:0.336578, valid loss:0.355518
Epoch:58, Train loss:0.336574, valid loss:0.355514
Epoch:59, Train loss:0.336575, valid loss:0.355522
Epoch:60, Train loss:0.336579, valid loss:0.355521
training time 3498.343291759491
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.6350238155748135
plot_id,batch_id 0 1 miss% 0.7341619620755954
plot_id,batch_id 0 2 miss% 0.7471822356546509
plot_id,batch_id 0 3 miss% 0.7568333213193151
plot_id,batch_id 0 4 miss% 0.761348887971042
plot_id,batch_id 0 5 miss% 0.6437830836438992
plot_id,batch_id 0 6 miss% 0.72587949853952
plot_id,batch_id 0 7 miss% 0.7497550581031236
plot_id,batch_id 0 8 miss% 0.7581698706821592
plot_id,batch_id 0 9 miss% 0.75558557352636
plot_id,batch_id 0 10 miss% 0.6194138534006141
plot_id,batch_id 0 11 miss% 0.7279644422732847
plot_id,batch_id 0 12 miss% 0.7369635964535678
plot_id,batch_id 0 13 miss% 0.7515609878105063
plot_id,batch_id 0 14 miss% 0.7589210565248242
plot_id,batch_id 0 15 miss% 0.6293659275790559
plot_id,batch_id 0 16 miss% 0.7201427533132819
plot_id,batch_id 0 17 miss% 0.7444730453505026
plot_id,batch_id 0 18 miss% 0.7509878073716129
plot_id,batch_id 0 19 miss% 0.7542357250077784
plot_id,batch_id 0 20 miss% 0.6914390463652554
plot_id,batch_id 0 21 miss% 0.7489385524120173
plot_id,batch_id 0 22 miss% 0.755117361926601
plot_id,batch_id 0 23 miss% 0.7623911813055081
plot_id,batch_id 0 24 miss% 0.7670183584396589
plot_id,batch_id 0 25 miss% 0.6843461911144875
plot_id,batch_id 0 26 miss% 0.740771684515598
plot_id,batch_id 0 27 miss% 0.756597977631776
plot_id,batch_id 0 28 miss% 0.7607116007829762
plot_id,batch_id 0 29 miss% 0.7634866637764762
plot_id,batch_id 0 30 miss% 0.6688254923531346
plot_id,batch_id 0 31 miss% 0.7391037656346545
plot_id,batch_id 0 32 miss% 0.7498758708421372
plot_id,batch_id 0 33 miss% 0.7603514061848624
plot_id,batch_id 0 34 miss% 0.7680809152849225
plot_id,batch_id 0 35 miss% 0.680690168745671
plot_id,batch_id 0 36 miss% 0.7418566632101584
plot_id,batch_id 0 37 miss% 0.7494653527883274
plot_id,batch_id 0 38 miss% 0.7593696074048596
plot_id,batch_id 0 39 miss% 0.7594883379026169
plot_id,batch_id 0 40 miss% 0.7142737859188882
plot_id,batch_id 0 41 miss% 0.7565607200696893
plot_id,batch_id 0 42 miss% 0.7635804604947191
plot_id,batch_id 0 43 miss% 0.7679072630599667
plot_id,batch_id 0 44 miss% 0.7693430222449906
plot_id,batch_id 0 45 miss% 0.7193763990392702
plot_id,batch_id 0 46 miss% 0.7556507303924143
plot_id,batch_id 0 47 miss% 0.7651766553095027
plot_id,batch_id 0 48 miss% 0.7645098382668579
plot_id,batch_id 0 49 miss% 0.7705040643094938
plot_id,batch_id 0 50 miss% 0.723692568639826
plot_id,batch_id 0 51 miss% 0.7560265406250117
plot_id,batch_id 0 52 miss% 0.7573779258939687
plot_id,batch_id 0 53 miss% 0.7655119806089801
plot_id,batch_id 0 54 miss% 0.7719204582882313
plot_id,batch_id 0 55 miss% 0.7376719923204239
plot_id,batch_id 0 56 miss% 0.754484658360534
plot_id,batch_id 0 57 miss% 0.7613631671886991
plot_id,batch_id 0 58 miss% 0.7645577027153134
plot_id,batch_id 0 59 miss% 0.7691026244231753
plot_id,batch_id 0 60 miss% 0.5435737414141116
plot_id,batch_id 0 61 miss% 0.6911529012494855
plot_id,batch_id 0 62 miss% 0.7201314293709126
plot_id,batch_id 0 63 miss% 0.7372758435262953
plot_id,batch_id 0 64 miss% 0.745663991751181
plot_id,batch_id 0 65 miss% 0.5352408159652048
plot_id,batch_id 0 66 miss% 0.6802381891116797
plot_id,batch_id 0 67 miss% 0.7082573845956884
plot_id,batch_id 0 68 miss% 0.7355888765032464
plot_id,batch_id 0 69 miss% 0.7411397990808556
plot_id,batch_id 0 70 miss% 0.5162173612115801
plot_id,batch_id 0 71 miss% 0.6921561832315203
plot_id,batch_id 0 72 miss% 0.6928886571930476
plot_id,batch_id 0 73 miss% 0.7202305027113622
plot_id,batch_id 0 74 miss% 0.7294601733880529
plot_id,batch_id 0 75 miss% 0.4970524693035055
plot_id,batch_id 0 76 miss% 0.6440911505022113
plot_id,batch_id 0 77 miss% 0.689221831696097
plot_id,batch_id 0 78 miss% 0.721907393684027
plot_id,batch_id 0 79 miss% 0.7339713842390717
plot_id,batch_id 0 80 miss% 0.5663578934163651
plot_id,batch_id 0 81 miss% 0.7093767549793608
plot_id,batch_id 0 82 miss% 0.733929223576668
plot_id,batch_id 0 83 miss% 0.7463868936882957
plot_id,batch_id 0 84 miss% 0.7483731493276231
plot_id,batch_id 0 85 miss% 0.565506696268939
plot_id,batch_id 0 86 miss% 0.7050836181583022
plot_id,batch_id 0 87 miss% 0.7271062203972752
plot_id,batch_id 0 88 miss% 0.7447881272917669
plot_id,batch_id 0 89 miss% 0.7478627808664885
plot_id,batch_id 0 90 miss% 0.5346744428497746
plot_id,batch_id 0 91 miss% 0.7028925572150002
plot_id,batch_id 0 92 miss% 0.717601612210874
plot_id,batch_id 0 93 miss% 0.7351057491928525
plot_id,batch_id 0 94 miss% 0.7474477556752561
plot_id,batch_id 0 95 miss% 0.531635871005265
plot_id,batch_id 0 96 miss% 0.6776595621475047
plot_id,batch_id 0 97 miss% 0.7146261318548426
plot_id,batch_id 0 98 miss% 0.7293629105133722
plot_id,batch_id 0 99 miss% 0.739069839665198
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.63502382 0.73416196 0.74718224 0.75683332 0.76134889 0.64378308
 0.7258795  0.74975506 0.75816987 0.75558557 0.61941385 0.72796444
 0.7369636  0.75156099 0.75892106 0.62936593 0.72014275 0.74447305
 0.75098781 0.75423573 0.69143905 0.74893855 0.75511736 0.76239118
 0.76701836 0.68434619 0.74077168 0.75659798 0.7607116  0.76348666
 0.66882549 0.73910377 0.74987587 0.76035141 0.76808092 0.68069017
 0.74185666 0.74946535 0.75936961 0.75948834 0.71427379 0.75656072
 0.76358046 0.76790726 0.76934302 0.7193764  0.75565073 0.76517666
 0.76450984 0.77050406 0.72369257 0.75602654 0.75737793 0.76551198
 0.77192046 0.73767199 0.75448466 0.76136317 0.7645577  0.76910262
 0.54357374 0.6911529  0.72013143 0.73727584 0.74566399 0.53524082
 0.68023819 0.70825738 0.73558888 0.7411398  0.51621736 0.69215618
 0.69288866 0.7202305  0.72946017 0.49705247 0.64409115 0.68922183
 0.72190739 0.73397138 0.56635789 0.70937675 0.73392922 0.74638689
 0.74837315 0.5655067  0.70508362 0.72710622 0.74478813 0.74786278
 0.53467444 0.70289256 0.71760161 0.73510575 0.74744776 0.53163587
 0.67765956 0.71462613 0.72936291 0.73906984]
for model  108 the mean error 0.7167857912899731
all id 108 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 25 out win 3 err 0.7167857912899731
Launcher: Job 109 completed in 3677 seconds.
Launcher: Task 78 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  28945
Epoch:0, Train loss:0.642002, valid loss:0.642832
Epoch:1, Train loss:0.065017, valid loss:0.017893
Epoch:2, Train loss:0.026117, valid loss:0.007775
Epoch:3, Train loss:0.012873, valid loss:0.006457
Epoch:4, Train loss:0.010058, valid loss:0.004611
Epoch:5, Train loss:0.008781, valid loss:0.005666
Epoch:6, Train loss:0.008210, valid loss:0.005370
Epoch:7, Train loss:0.007640, valid loss:0.004784
Epoch:8, Train loss:0.007035, valid loss:0.003701
Epoch:9, Train loss:0.006467, valid loss:0.002909
Epoch:10, Train loss:0.005739, valid loss:0.003185
Epoch:11, Train loss:0.004367, valid loss:0.002333
Epoch:12, Train loss:0.004213, valid loss:0.002508
Epoch:13, Train loss:0.004090, valid loss:0.002596
Epoch:14, Train loss:0.003912, valid loss:0.002125
Epoch:15, Train loss:0.003756, valid loss:0.002089
Epoch:16, Train loss:0.003638, valid loss:0.002172
Epoch:17, Train loss:0.003546, valid loss:0.002133
Epoch:18, Train loss:0.003493, valid loss:0.001998
Epoch:19, Train loss:0.003399, valid loss:0.002118
Epoch:20, Train loss:0.003277, valid loss:0.001898
Epoch:21, Train loss:0.002812, valid loss:0.001736
Epoch:22, Train loss:0.002801, valid loss:0.001684
Epoch:23, Train loss:0.002761, valid loss:0.001668
Epoch:24, Train loss:0.002743, valid loss:0.001702
Epoch:25, Train loss:0.002694, valid loss:0.001734
Epoch:26, Train loss:0.002642, valid loss:0.001707
Epoch:27, Train loss:0.002608, valid loss:0.001672
Epoch:28, Train loss:0.002618, valid loss:0.001643
Epoch:29, Train loss:0.002572, valid loss:0.001658
Epoch:30, Train loss:0.002550, valid loss:0.001537
Epoch:31, Train loss:0.002298, valid loss:0.001463
Epoch:32, Train loss:0.002296, valid loss:0.001526
Epoch:33, Train loss:0.002283, valid loss:0.001496
Epoch:34, Train loss:0.002267, valid loss:0.001474
Epoch:35, Train loss:0.002244, valid loss:0.001413
Epoch:36, Train loss:0.002242, valid loss:0.001527
Epoch:37, Train loss:0.002230, valid loss:0.001546
Epoch:38, Train loss:0.002222, valid loss:0.001427
Epoch:39, Train loss:0.002191, valid loss:0.001423
Epoch:40, Train loss:0.002187, valid loss:0.001408
Epoch:41, Train loss:0.002081, valid loss:0.001355
Epoch:42, Train loss:0.002061, valid loss:0.001354
Epoch:43, Train loss:0.002054, valid loss:0.001371
Epoch:44, Train loss:0.002051, valid loss:0.001380
Epoch:45, Train loss:0.002042, valid loss:0.001372
Epoch:46, Train loss:0.002046, valid loss:0.001339
Epoch:47, Train loss:0.002025, valid loss:0.001361
Epoch:48, Train loss:0.002026, valid loss:0.001325
Epoch:49, Train loss:0.002029, valid loss:0.001345
Epoch:50, Train loss:0.002009, valid loss:0.001337
Epoch:51, Train loss:0.001959, valid loss:0.001321
Epoch:52, Train loss:0.001952, valid loss:0.001305
Epoch:53, Train loss:0.001948, valid loss:0.001310
Epoch:54, Train loss:0.001946, valid loss:0.001300
Epoch:55, Train loss:0.001946, valid loss:0.001309
Epoch:56, Train loss:0.001941, valid loss:0.001304
Epoch:57, Train loss:0.001937, valid loss:0.001278
Epoch:58, Train loss:0.001934, valid loss:0.001271
Epoch:59, Train loss:0.001928, valid loss:0.001278
Epoch:60, Train loss:0.001926, valid loss:0.001281
training time 3516.7698380947113
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.021779749976390834
plot_id,batch_id 0 1 miss% 0.03508725387175243
plot_id,batch_id 0 2 miss% 0.03413929884497392
plot_id,batch_id 0 3 miss% 0.025004044356140773
plot_id,batch_id 0 4 miss% 0.02493067356718335
plot_id,batch_id 0 5 miss% 0.04915914520438461
plot_id,batch_id 0 6 miss% 0.03013344393350736
plot_id,batch_id 0 7 miss% 0.024799181763542705
plot_id,batch_id 0 8 miss% 0.03447716725159025
plot_id,batch_id 0 9 miss% 0.030237541158141167
plot_id,batch_id 0 10 miss% 0.035973413418788444
plot_id,batch_id 0 11 miss% 0.04188873708607589
plot_id,batch_id 0 12 miss% 0.03143175870647251
plot_id,batch_id 0 13 miss% 0.04387716316384713
plot_id,batch_id 0 14 miss% 0.04339852936115418
plot_id,batch_id 0 15 miss% 0.04429031464293251
plot_id,batch_id 0 16 miss% 0.0254569486362403
plot_id,batch_id 0 17 miss% 0.04328456092242223
plot_id,batch_id 0 18 miss% 0.03493809226645121
plot_id,batch_id 0 19 miss% 0.03969933527647503
plot_id,batch_id 0 20 miss% 0.029557316246574476
plot_id,batch_id 0 21 miss% 0.03176454785771534
plot_id,batch_id 0 22 miss% 0.03487497823677271
plot_id,batch_id 0 23 miss% 0.03283774494843187
plot_id,batch_id 0 24 miss% 0.024257832901553665
plot_id,batch_id 0 25 miss% 0.05468702034325086
plot_id,batch_id 0 26 miss% 0.03734626744388151
plot_id,batch_id 0 27 miss% 0.02757715792245276
plot_id,batch_id 0 28 miss% 0.03287361141012761
plot_id,batch_id 0 29 miss% 0.03248916614512003
plot_id,batch_id 0 30 miss% 0.04340136505955171
plot_id,batch_id 0 31 miss% 0.0347373475674603
plot_id,batch_id 0 32 miss% 0.03928747445189864
plot_id,batch_id 0 33 miss% 0.03125316204447858
plot_id,batch_id 0 34 miss% 0.023282248955461143
plot_id,batch_id 0 35 miss% 0.043730864926896186
plot_id,batch_id 0 36 miss% 0.03626023712527337
plot_id,batch_id 0 37 miss% 0.04565950120237704
plot_id,batch_id 0 38 miss% 0.030695306855097824
plot_id,batch_id 0 39 miss% 0.025941479347851126
plot_id,batch_id 0 40 miss% 0.07568331632042131
plot_id,batch_id 0 41 miss% 0.028350575412295364
plot_id,batch_id 0 42 miss% 0.028063529815548455
plot_id,batch_id 0 43 miss% 0.03448648948091845
plot_id,batch_id 0 44 miss% 0.03613093842795425
plot_id,batch_id 0 45 miss% 0.039949858182813575
plot_id,batch_id 0 46 miss% 0.027229002836030667
plot_id,batch_id 0 47 miss% 0.03046530639051365
plot_id,batch_id 0 48 miss% 0.03679503904818394
plot_id,batch_id 0 49 miss% 0.03157947930604099
plot_id,batch_id 0 50 miss% 0.03719389610954897
plot_id,batch_id 0 51 miss% 0.014552740332485204
plot_id,batch_id 0 52 miss% 0.02896154824679977
plot_id,batch_id 0 53 miss% 0.015438258669806545
plot_id,batch_id 0 54 miss% 0.02988380476894805
plot_id,batch_id 0 55 miss% 0.03161026660411702
plot_id,batch_id 0 56 miss% 0.03379838762495832
plot_id,batch_id 0 57 miss% 0.03039636800211991
plot_id,batch_id 0 58 miss% 0.03387237067795834
plot_id,batch_id 0 59 miss% 0.02807821810622454
plot_id,batch_id 0 60 miss% 0.0334330311918831
plot_id,batch_id 0 61 miss% 0.023743832563008323
plot_id,batch_id 0 62 miss% 0.04027536181515362
plot_id,batch_id 0 63 miss% 0.02561139897239677
plot_id,batch_id 0 64 miss% 0.02148374602189009
plot_id,batch_id 0 65 miss% 0.05129039381276955
plot_id,batch_id 0 66 miss% 0.05763962963850802
plot_id,batch_id 0 67 miss% 0.03867008536065222
plot_id,batch_id 0 68 miss% 0.04130796556604589
plot_id,batch_id 0 69 miss% 0.040902444062563666
plot_id,batch_id 0 70 miss% 0.05389860419652213
plot_id,batch_id 0 71 miss% 0.06072387438678496
plot_id,batch_id 0 72 miss% 0.03868993803311364
plot_id,batch_id 0 73 miss% 0.04363591297476983
plot_id,batch_id 0 74 miss% 0.04719922304848029
plot_id,batch_id 0 75 miss% 0.05884880909858429
plot_id,batch_id 0 76 miss% 0.06895131988836987
plot_id,batch_id 0 77 miss% 0.034072827027712686
plot_id,batch_id 0 78 miss% 0.053436633411465886
plot_id,batch_id 0 79 miss% 0.06282806787588469
plot_id,batch_id 0 80 miss% 0.04616595321966876
plot_id,batch_id 0 81 miss% 0.026612972221430328
plot_id,batch_id 0 82 miss% 0.02413150309411491
plot_id,batch_id 0 83 miss% 0.032940796761258086
plot_id,batch_id 0 84 miss% 0.026165610875369727
plot_id,batch_id 0 85 miss% 0.064311013002138
plot_id,batch_id 0 86 miss% 0.02922013171251592
plot_id,batch_id 0 87 miss% 0.026491417496831607
plot_id,batch_id 0 88 miss% 0.0491977962646413
plot_id,batch_id 0 89 miss% 0.03184670097739724
plot_id,batch_id 0 90 miss% 0.050594637089303035
plot_id,batch_id 0 91 miss% 0.043608968637120264
plot_id,batch_id 0 92 miss% 0.04319671833681575
plot_id,batch_id 0 93 miss% 0.026590852955361708
plot_id,batch_id 0 94 miss% 0.03805888281793441
plot_id,batch_id 0 95 miss% 0.0658959895934428
plot_id,batch_id 0 96 miss% 0.03612133404596315
plot_id,batch_id 0 97 miss% 0.06646569949032322
plot_id,batch_id 0 98 miss% 0.04151195653870591
plot_id,batch_id 0 99 miss% 0.056281483364922814
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02177975 0.03508725 0.0341393  0.02500404 0.02493067 0.04915915
 0.03013344 0.02479918 0.03447717 0.03023754 0.03597341 0.04188874
 0.03143176 0.04387716 0.04339853 0.04429031 0.02545695 0.04328456
 0.03493809 0.03969934 0.02955732 0.03176455 0.03487498 0.03283774
 0.02425783 0.05468702 0.03734627 0.02757716 0.03287361 0.03248917
 0.04340137 0.03473735 0.03928747 0.03125316 0.02328225 0.04373086
 0.03626024 0.0456595  0.03069531 0.02594148 0.07568332 0.02835058
 0.02806353 0.03448649 0.03613094 0.03994986 0.027229   0.03046531
 0.03679504 0.03157948 0.0371939  0.01455274 0.02896155 0.01543826
 0.0298838  0.03161027 0.03379839 0.03039637 0.03387237 0.02807822
 0.03343303 0.02374383 0.04027536 0.0256114  0.02148375 0.05129039
 0.05763963 0.03867009 0.04130797 0.04090244 0.0538986  0.06072387
 0.03868994 0.04363591 0.04719922 0.05884881 0.06895132 0.03407283
 0.05343663 0.06282807 0.04616595 0.02661297 0.0241315  0.0329408
 0.02616561 0.06431101 0.02922013 0.02649142 0.0491978  0.0318467
 0.05059464 0.04360897 0.04319672 0.02659085 0.03805888 0.06589599
 0.03612133 0.0664657  0.04151196 0.05628148]
for model  9 the mean error 0.037610738962062
all id 9 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 21 out win 3 err 0.037610738962062
Launcher: Job 10 completed in 3721 seconds.
Launcher: Task 137 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  21969
Epoch:0, Train loss:0.613368, valid loss:0.573821
Epoch:1, Train loss:0.353282, valid loss:0.359756
Epoch:2, Train loss:0.342421, valid loss:0.357816
Epoch:3, Train loss:0.340855, valid loss:0.357302
Epoch:4, Train loss:0.339920, valid loss:0.356938
Epoch:5, Train loss:0.339287, valid loss:0.356873
Epoch:6, Train loss:0.338820, valid loss:0.356872
Epoch:7, Train loss:0.338593, valid loss:0.356445
Epoch:8, Train loss:0.338348, valid loss:0.356518
Epoch:9, Train loss:0.338217, valid loss:0.356329
Epoch:10, Train loss:0.338088, valid loss:0.356289
Epoch:11, Train loss:0.337538, valid loss:0.355920
Epoch:12, Train loss:0.337493, valid loss:0.355981
Epoch:13, Train loss:0.337468, valid loss:0.355871
Epoch:14, Train loss:0.337444, valid loss:0.355812
Epoch:15, Train loss:0.337404, valid loss:0.355903
Epoch:16, Train loss:0.337348, valid loss:0.355834
Epoch:17, Train loss:0.337356, valid loss:0.355957
Epoch:18, Train loss:0.337328, valid loss:0.355825
Epoch:19, Train loss:0.337282, valid loss:0.355785
Epoch:20, Train loss:0.337272, valid loss:0.356019
Epoch:21, Train loss:0.336998, valid loss:0.355607
Epoch:22, Train loss:0.336972, valid loss:0.355745
Epoch:23, Train loss:0.336988, valid loss:0.355673
Epoch:24, Train loss:0.336965, valid loss:0.355534
Epoch:25, Train loss:0.336950, valid loss:0.355614
Epoch:26, Train loss:0.336938, valid loss:0.355996
Epoch:27, Train loss:0.336938, valid loss:0.355587
Epoch:28, Train loss:0.336907, valid loss:0.355648
Epoch:29, Train loss:0.336911, valid loss:0.355569
Epoch:30, Train loss:0.336900, valid loss:0.355649
Epoch:31, Train loss:0.336770, valid loss:0.355516
Epoch:32, Train loss:0.336758, valid loss:0.355512
Epoch:33, Train loss:0.336762, valid loss:0.355516
Epoch:34, Train loss:0.336758, valid loss:0.355581
Epoch:35, Train loss:0.336757, valid loss:0.355550
Epoch:36, Train loss:0.336754, valid loss:0.355514
Epoch:37, Train loss:0.336746, valid loss:0.355485
Epoch:38, Train loss:0.336741, valid loss:0.355503
Epoch:39, Train loss:0.336736, valid loss:0.355504
Epoch:40, Train loss:0.336731, valid loss:0.355489
Epoch:41, Train loss:0.336663, valid loss:0.355479
Epoch:42, Train loss:0.336665, valid loss:0.355474
Epoch:43, Train loss:0.336663, valid loss:0.355461
Epoch:44, Train loss:0.336660, valid loss:0.355498
Epoch:45, Train loss:0.336659, valid loss:0.355464
Epoch:46, Train loss:0.336652, valid loss:0.355552
Epoch:47, Train loss:0.336652, valid loss:0.355462
Epoch:48, Train loss:0.336646, valid loss:0.355475
Epoch:49, Train loss:0.336650, valid loss:0.355496
Epoch:50, Train loss:0.336645, valid loss:0.355463
Epoch:51, Train loss:0.336615, valid loss:0.355450
Epoch:52, Train loss:0.336610, valid loss:0.355453
Epoch:53, Train loss:0.336612, valid loss:0.355457
Epoch:54, Train loss:0.336607, valid loss:0.355459
Epoch:55, Train loss:0.336608, valid loss:0.355468
Epoch:56, Train loss:0.336607, valid loss:0.355456
Epoch:57, Train loss:0.336606, valid loss:0.355447
Epoch:58, Train loss:0.336605, valid loss:0.355460
Epoch:59, Train loss:0.336603, valid loss:0.355441
Epoch:60, Train loss:0.336601, valid loss:0.355455
training time 3594.0257041454315
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.6431498097551474
plot_id,batch_id 0 1 miss% 0.7344167434592846
plot_id,batch_id 0 2 miss% 0.7439813874658362
plot_id,batch_id 0 3 miss% 0.756060110141516
plot_id,batch_id 0 4 miss% 0.7605828451480521
plot_id,batch_id 0 5 miss% 0.6360236876195269
plot_id,batch_id 0 6 miss% 0.7258154638707864
plot_id,batch_id 0 7 miss% 0.7467776399440768
plot_id,batch_id 0 8 miss% 0.7568977330534883
plot_id,batch_id 0 9 miss% 0.75688804489964
plot_id,batch_id 0 10 miss% 0.6092590086038834
plot_id,batch_id 0 11 miss% 0.7328808121886301
plot_id,batch_id 0 12 miss% 0.7408818986917317
plot_id,batch_id 0 13 miss% 0.7513709101858876
plot_id,batch_id 0 14 miss% 0.7585276078141989
plot_id,batch_id 0 15 miss% 0.6238767338538518
plot_id,batch_id 0 16 miss% 0.7230650451286428
plot_id,batch_id 0 17 miss% 0.7503385090490481
plot_id,batch_id 0 18 miss% 0.750378810602423
plot_id,batch_id 0 19 miss% 0.7520768778117465
plot_id,batch_id 0 20 miss% 0.6888944778884372
plot_id,batch_id 0 21 miss% 0.7473639431748348
plot_id,batch_id 0 22 miss% 0.7545946855509188
plot_id,batch_id 0 23 miss% 0.7614583531840167
plot_id,batch_id 0 24 miss% 0.7636691851509261
plot_id,batch_id 0 25 miss% 0.6817040039168727
plot_id,batch_id 0 26 miss% 0.7460469231351018
plot_id,batch_id 0 27 miss% 0.7555225854562737
plot_id,batch_id 0 28 miss% 0.7611179967121748
plot_id,batch_id 0 29 miss% 0.7631699514159735
plot_id,batch_id 0 30 miss% 0.6691390529489643
plot_id,batch_id 0 31 miss% 0.7382448523382011
plot_id,batch_id 0 32 miss% 0.7489462255239406
plot_id,batch_id 0 33 miss% 0.762747783278297
plot_id,batch_id 0 34 miss% 0.7631135276106359
plot_id,batch_id 0 35 miss% 0.6670114463954645
plot_id,batch_id 0 36 miss% 0.742981585574188
plot_id,batch_id 0 37 miss% 0.7503298723883974
plot_id,batch_id 0 38 miss% 0.7560157176030573
plot_id,batch_id 0 39 miss% 0.7587404595972101
plot_id,batch_id 0 40 miss% 0.7290509967450515
plot_id,batch_id 0 41 miss% 0.7541570954702673
plot_id,batch_id 0 42 miss% 0.7618545572750912
plot_id,batch_id 0 43 miss% 0.7665946032077636
plot_id,batch_id 0 44 miss% 0.7672881957226896
plot_id,batch_id 0 45 miss% 0.7201737573186058
plot_id,batch_id 0 46 miss% 0.7533103525643473
plot_id,batch_id 0 47 miss% 0.7624912045568701
plot_id,batch_id 0 48 miss% 0.7629082928782881
plot_id,batch_id 0 49 miss% 0.7675695183731643
plot_id,batch_id 0 50 miss% 0.7180575991795789
plot_id,batch_id 0 51 miss% 0.753887064544705
plot_id,batch_id 0 52 miss% 0.7566074990933818
plot_id,batch_id 0 53 miss% 0.7621581750968315
plot_id,batch_id 0 54 miss% 0.7701265240763768
plot_id,batch_id 0 55 miss% 0.728292393434531
plot_id,batch_id 0 56 miss% 0.7507051617695792
plot_id,batch_id 0 57 miss% 0.7579028137367508
plot_id,batch_id 0 58 miss% 0.7638925395097607
plot_id,batch_id 0 59 miss% 0.7692678279336534
plot_id,batch_id 0 60 miss% 0.5355218274375779
plot_id,batch_id 0 61 miss% 0.6862492341267662
plot_id,batch_id 0 62 miss% 0.7240746698554315
plot_id,batch_id 0 63 miss% 0.74049734890452
plot_id,batch_id 0 64 miss% 0.7444500084773441
plot_id,batch_id 0 65 miss% 0.527022273238919
plot_id,batch_id 0 66 miss% 0.6817861481570037
plot_id,batch_id 0 67 miss% 0.7089771749328867
plot_id,batch_id 0 68 miss% 0.7355178858636319
plot_id,batch_id 0 69 miss% 0.7434679068855921
plot_id,batch_id 0 70 miss% 0.5056166352715992
plot_id,batch_id 0 71 miss% 0.6825232294770295
plot_id,batch_id 0 72 miss% 0.6967015081485914
plot_id,batch_id 0 73 miss% 0.7208046579847276
plot_id,batch_id 0 74 miss% 0.7287550549540888
plot_id,batch_id 0 75 miss% 0.49257921553774375
plot_id,batch_id 0 76 miss% 0.6475056761660488
plot_id,batch_id 0 77 miss% 0.6872674660605204
plot_id,batch_id 0 78 miss% 0.7202173859003358
plot_id,batch_id 0 79 miss% 0.7384706307178918
plot_id,batch_id 0 80 miss% 0.5664232068451287
plot_id,batch_id 0 81 miss% 0.7068790235237414
plot_id,batch_id 0 82 miss% 0.7342066646735953
plot_id,batch_id 0 83 miss% 0.7460563960669965
plot_id,batch_id 0 84 miss% 0.7487861221793686
plot_id,batch_id 0 85 miss% 0.5648608062961045
plot_id,batch_id 0 86 miss% 0.7004438827687204
plot_id,batch_id 0 87 miss% 0.7280617198311206
plot_id,batch_id 0 88 miss% 0.7458381556947694
plot_id,batch_id 0 89 miss% 0.7507853947792503
plot_id,batch_id 0 90 miss% 0.5342735247532072
plot_id,batch_id 0 91 miss% 0.6975231260635634
plot_id,batch_id 0 92 miss% 0.7206474368747788
plot_id,batch_id 0 93 miss% 0.7420183148418765
plot_id,batch_id 0 94 miss% 0.745329439186838
plot_id,batch_id 0 95 miss% 0.542409610247221
plot_id,batch_id 0 96 miss% 0.6778367468086649
plot_id,batch_id 0 97 miss% 0.7134530868528401
plot_id,batch_id 0 98 miss% 0.7313033648302142
plot_id,batch_id 0 99 miss% 0.7362942432077231
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.64314981 0.73441674 0.74398139 0.75606011 0.76058285 0.63602369
 0.72581546 0.74677764 0.75689773 0.75688804 0.60925901 0.73288081
 0.7408819  0.75137091 0.75852761 0.62387673 0.72306505 0.75033851
 0.75037881 0.75207688 0.68889448 0.74736394 0.75459469 0.76145835
 0.76366919 0.681704   0.74604692 0.75552259 0.761118   0.76316995
 0.66913905 0.73824485 0.74894623 0.76274778 0.76311353 0.66701145
 0.74298159 0.75032987 0.75601572 0.75874046 0.729051   0.7541571
 0.76185456 0.7665946  0.7672882  0.72017376 0.75331035 0.7624912
 0.76290829 0.76756952 0.7180576  0.75388706 0.7566075  0.76215818
 0.77012652 0.72829239 0.75070516 0.75790281 0.76389254 0.76926783
 0.53552183 0.68624923 0.72407467 0.74049735 0.74445001 0.52702227
 0.68178615 0.70897717 0.73551789 0.74346791 0.50561664 0.68252323
 0.69670151 0.72080466 0.72875505 0.49257922 0.64750568 0.68726747
 0.72021739 0.73847063 0.56642321 0.70687902 0.73420666 0.7460564
 0.74878612 0.56486081 0.70044388 0.72806172 0.74583816 0.75078539
 0.53427352 0.69752313 0.72064744 0.74201831 0.74532944 0.54240961
 0.67783675 0.71345309 0.73130336 0.73629424]
for model  81 the mean error 0.7159179468504254
all id 81 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 25 out win 3 err 0.7159179468504254
Launcher: Job 82 completed in 3775 seconds.
Launcher: Task 187 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  21969
Epoch:0, Train loss:0.731444, valid loss:0.695949
Epoch:1, Train loss:0.513501, valid loss:0.519993
Epoch:2, Train loss:0.499658, valid loss:0.518565
Epoch:3, Train loss:0.497343, valid loss:0.518946
Epoch:4, Train loss:0.495930, valid loss:0.518069
Epoch:5, Train loss:0.495322, valid loss:0.517813
Epoch:6, Train loss:0.494325, valid loss:0.517093
Epoch:7, Train loss:0.494074, valid loss:0.516294
Epoch:8, Train loss:0.493862, valid loss:0.516221
Epoch:9, Train loss:0.493683, valid loss:0.516430
Epoch:10, Train loss:0.493298, valid loss:0.515712
Epoch:11, Train loss:0.491624, valid loss:0.515239
Epoch:12, Train loss:0.491598, valid loss:0.515240
Epoch:13, Train loss:0.491433, valid loss:0.515550
Epoch:14, Train loss:0.491522, valid loss:0.515198
Epoch:15, Train loss:0.491451, valid loss:0.515611
Epoch:16, Train loss:0.491308, valid loss:0.515033
Epoch:17, Train loss:0.491229, valid loss:0.515716
Epoch:18, Train loss:0.491217, valid loss:0.515242
Epoch:19, Train loss:0.491269, valid loss:0.514825
Epoch:20, Train loss:0.491032, valid loss:0.515084
Epoch:21, Train loss:0.490289, valid loss:0.514775
Epoch:22, Train loss:0.490307, valid loss:0.514732
Epoch:23, Train loss:0.490288, valid loss:0.514854
Epoch:24, Train loss:0.490252, valid loss:0.514904
Epoch:25, Train loss:0.490168, valid loss:0.514778
Epoch:26, Train loss:0.490255, valid loss:0.514613
Epoch:27, Train loss:0.490138, valid loss:0.514985
Epoch:28, Train loss:0.490160, valid loss:0.514732
Epoch:29, Train loss:0.490135, valid loss:0.514930
Epoch:30, Train loss:0.490064, valid loss:0.514703
Epoch:31, Train loss:0.489720, valid loss:0.514676
Epoch:32, Train loss:0.489725, valid loss:0.514538
Epoch:33, Train loss:0.489706, valid loss:0.514463
Epoch:34, Train loss:0.489712, valid loss:0.514425
Epoch:35, Train loss:0.489662, valid loss:0.514429
Epoch:36, Train loss:0.489740, valid loss:0.514573
Epoch:37, Train loss:0.489661, valid loss:0.514338
Epoch:38, Train loss:0.489701, valid loss:0.514597
Epoch:39, Train loss:0.489682, valid loss:0.514364
Epoch:40, Train loss:0.489612, valid loss:0.514413
Epoch:41, Train loss:0.489440, valid loss:0.514325
Epoch:42, Train loss:0.489426, valid loss:0.514326
Epoch:43, Train loss:0.489431, valid loss:0.514430
Epoch:44, Train loss:0.489428, valid loss:0.514282
Epoch:45, Train loss:0.489407, valid loss:0.514295
Epoch:46, Train loss:0.489396, valid loss:0.514398
Epoch:47, Train loss:0.489437, valid loss:0.514370
Epoch:48, Train loss:0.489408, valid loss:0.514365
Epoch:49, Train loss:0.489394, valid loss:0.514340
Epoch:50, Train loss:0.489388, valid loss:0.514385
Epoch:51, Train loss:0.489295, valid loss:0.514288
Epoch:52, Train loss:0.489288, valid loss:0.514288
Epoch:53, Train loss:0.489294, valid loss:0.514277
Epoch:54, Train loss:0.489276, valid loss:0.514296
Epoch:55, Train loss:0.489283, valid loss:0.514262
Epoch:56, Train loss:0.489283, valid loss:0.514291
Epoch:57, Train loss:0.489274, valid loss:0.514253
Epoch:58, Train loss:0.489278, valid loss:0.514279
Epoch:59, Train loss:0.489271, valid loss:0.514308
Epoch:60, Train loss:0.489264, valid loss:0.514283
training time 3720.6939182281494
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.781768021768504
plot_id,batch_id 0 1 miss% 0.8259268788316741
plot_id,batch_id 0 2 miss% 0.8330876918161318
plot_id,batch_id 0 3 miss% 0.8336476919433503
plot_id,batch_id 0 4 miss% 0.8382217207147751
plot_id,batch_id 0 5 miss% 0.7725007915238052
plot_id,batch_id 0 6 miss% 0.8181808231116632
plot_id,batch_id 0 7 miss% 0.8284991478946421
plot_id,batch_id 0 8 miss% 0.836323076154117
plot_id,batch_id 0 9 miss% 0.843732034083397
plot_id,batch_id 0 10 miss% 0.7529458111858742
plot_id,batch_id 0 11 miss% 0.8184458579204059
plot_id,batch_id 0 12 miss% 0.8278817232034822
plot_id,batch_id 0 13 miss% 0.8346595497168584
plot_id,batch_id 0 14 miss% 0.8359893725522966
plot_id,batch_id 0 15 miss% 0.7606466390264947
plot_id,batch_id 0 16 miss% 0.8141923100359567
plot_id,batch_id 0 17 miss% 0.8302421367790438
plot_id,batch_id 0 18 miss% 0.8366991379890159
plot_id,batch_id 0 19 miss% 0.8359278481769757
plot_id,batch_id 0 20 miss% 0.8022928788617716
plot_id,batch_id 0 21 miss% 0.8286615956126085
plot_id,batch_id 0 22 miss% 0.8404008978735639
plot_id,batch_id 0 23 miss% 0.8378248474686898
plot_id,batch_id 0 24 miss% 0.844971684268684
plot_id,batch_id 0 25 miss% 0.8011930542416176
plot_id,batch_id 0 26 miss% 0.8290113271842466
plot_id,batch_id 0 27 miss% 0.8339984109230741
plot_id,batch_id 0 28 miss% 0.8368958055921334
plot_id,batch_id 0 29 miss% 0.8396503508687536
plot_id,batch_id 0 30 miss% 0.7968901653981182
plot_id,batch_id 0 31 miss% 0.8263465165847995
plot_id,batch_id 0 32 miss% 0.8353828676143968
plot_id,batch_id 0 33 miss% 0.838551792675408
plot_id,batch_id 0 34 miss% 0.8393176723210073
plot_id,batch_id 0 35 miss% 0.7869896544706099
plot_id,batch_id 0 36 miss% 0.8321532531539727
plot_id,batch_id 0 37 miss% 0.8344311608104786
plot_id,batch_id 0 38 miss% 0.8381064798700402
plot_id,batch_id 0 39 miss% 0.8401348662103437
plot_id,batch_id 0 40 miss% 0.8185323897549325
plot_id,batch_id 0 41 miss% 0.83433931090488
plot_id,batch_id 0 42 miss% 0.8385338487993156
plot_id,batch_id 0 43 miss% 0.8404931931389258
plot_id,batch_id 0 44 miss% 0.8453211690419811
plot_id,batch_id 0 45 miss% 0.8143680790934897
plot_id,batch_id 0 46 miss% 0.8341371170178087
plot_id,batch_id 0 47 miss% 0.8404007511911957
plot_id,batch_id 0 48 miss% 0.8396094148869798
plot_id,batch_id 0 49 miss% 0.8442996986680829
plot_id,batch_id 0 50 miss% 0.8220049386298789
plot_id,batch_id 0 51 miss% 0.8335082531202332
plot_id,batch_id 0 52 miss% 0.8347050522698477
plot_id,batch_id 0 53 miss% 0.8408826952605007
plot_id,batch_id 0 54 miss% 0.8442870767012436
plot_id,batch_id 0 55 miss% 0.8206631230288738
plot_id,batch_id 0 56 miss% 0.8333674385313785
plot_id,batch_id 0 57 miss% 0.8363808256104716
plot_id,batch_id 0 58 miss% 0.8397782645610458
plot_id,batch_id 0 59 miss% 0.8449620945151282
plot_id,batch_id 0 60 miss% 0.7205029654081566
plot_id,batch_id 0 61 miss% 0.8013289798710199
plot_id,batch_id 0 62 miss% 0.816172316115941
plot_id,batch_id 0 63 miss% 0.8269654798047184
plot_id,batch_id 0 64 miss% 0.8316611541476537
plot_id,batch_id 0 65 miss% 0.7140174959773877
plot_id,batch_id 0 66 miss% 0.7898945600407044
plot_id,batch_id 0 67 miss% 0.8106100456387215
plot_id,batch_id 0 68 miss% 0.8224015147001983
plot_id,batch_id 0 69 miss% 0.8296918763976237
plot_id,batch_id 0 70 miss% 0.6805970558683639
plot_id,batch_id 0 71 miss% 0.7932413727978814
plot_id,batch_id 0 72 miss% 0.8021836149099685
plot_id,batch_id 0 73 miss% 0.8185751598123705
plot_id,batch_id 0 74 miss% 0.8253220200136285
plot_id,batch_id 0 75 miss% 0.6726097192240924
plot_id,batch_id 0 76 miss% 0.7879304040282142
plot_id,batch_id 0 77 miss% 0.7934180222209335
plot_id,batch_id 0 78 miss% 0.8105220492123919
plot_id,batch_id 0 79 miss% 0.8119726704032352
plot_id,batch_id 0 80 miss% 0.7348833723606566
plot_id,batch_id 0 81 miss% 0.8214503354934743
plot_id,batch_id 0 82 miss% 0.8239833306065297
plot_id,batch_id 0 83 miss% 0.8331693475097082
plot_id,batch_id 0 84 miss% 0.8332195746615456
plot_id,batch_id 0 85 miss% 0.7324710538852522
plot_id,batch_id 0 86 miss% 0.8067218032907594
plot_id,batch_id 0 87 miss% 0.8234651486690713
plot_id,batch_id 0 88 miss% 0.8287390106764784
plot_id,batch_id 0 89 miss% 0.8320804511414016
plot_id,batch_id 0 90 miss% 0.686124850186247
plot_id,batch_id 0 91 miss% 0.8012741271460877
plot_id,batch_id 0 92 miss% 0.8166370203741092
plot_id,batch_id 0 93 miss% 0.8198077460725031
plot_id,batch_id 0 94 miss% 0.8297651618381058
plot_id,batch_id 0 95 miss% 0.7228260702099512
plot_id,batch_id 0 96 miss% 0.7940163520376409
plot_id,batch_id 0 97 miss% 0.8115798305170044
plot_id,batch_id 0 98 miss% 0.8180513951683163
plot_id,batch_id 0 99 miss% 0.8238428368242029
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.78176802 0.82592688 0.83308769 0.83364769 0.83822172 0.77250079
 0.81818082 0.82849915 0.83632308 0.84373203 0.75294581 0.81844586
 0.82788172 0.83465955 0.83598937 0.76064664 0.81419231 0.83024214
 0.83669914 0.83592785 0.80229288 0.8286616  0.8404009  0.83782485
 0.84497168 0.80119305 0.82901133 0.83399841 0.83689581 0.83965035
 0.79689017 0.82634652 0.83538287 0.83855179 0.83931767 0.78698965
 0.83215325 0.83443116 0.83810648 0.84013487 0.81853239 0.83433931
 0.83853385 0.84049319 0.84532117 0.81436808 0.83413712 0.84040075
 0.83960941 0.8442997  0.82200494 0.83350825 0.83470505 0.8408827
 0.84428708 0.82066312 0.83336744 0.83638083 0.83977826 0.84496209
 0.72050297 0.80132898 0.81617232 0.82696548 0.83166115 0.7140175
 0.78989456 0.81061005 0.82240151 0.82969188 0.68059706 0.79324137
 0.80218361 0.81857516 0.82532202 0.67260972 0.7879304  0.79341802
 0.81052205 0.81197267 0.73488337 0.82145034 0.82398333 0.83316935
 0.83321957 0.73247105 0.8067218  0.82346515 0.82873901 0.83208045
 0.68612485 0.80127413 0.81663702 0.81980775 0.82976516 0.72282607
 0.79401635 0.81157983 0.8180514  0.82384284]
for model  55 the mean error 0.8137702557642121
all id 55 hidden_dim 16 learning_rate 0.02 num_layers 3 frames 21 out win 4 err 0.8137702557642121
Launcher: Job 56 completed in 3903 seconds.
Launcher: Task 209 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  21969
Epoch:0, Train loss:0.731444, valid loss:0.695949
Epoch:1, Train loss:0.512875, valid loss:0.519383
Epoch:2, Train loss:0.498332, valid loss:0.517811
Epoch:3, Train loss:0.496254, valid loss:0.516976
Epoch:4, Train loss:0.494772, valid loss:0.516793
Epoch:5, Train loss:0.493902, valid loss:0.516196
Epoch:6, Train loss:0.493283, valid loss:0.516180
Epoch:7, Train loss:0.493010, valid loss:0.515763
Epoch:8, Train loss:0.492807, valid loss:0.515672
Epoch:9, Train loss:0.492452, valid loss:0.516257
Epoch:10, Train loss:0.492327, valid loss:0.515539
Epoch:11, Train loss:0.491047, valid loss:0.514967
Epoch:12, Train loss:0.491005, valid loss:0.514911
Epoch:13, Train loss:0.491051, valid loss:0.515293
Epoch:14, Train loss:0.490950, valid loss:0.514770
Epoch:15, Train loss:0.490930, valid loss:0.515155
Epoch:16, Train loss:0.490720, valid loss:0.514858
Epoch:17, Train loss:0.490778, valid loss:0.514982
Epoch:18, Train loss:0.490804, valid loss:0.514712
Epoch:19, Train loss:0.490652, valid loss:0.514885
Epoch:20, Train loss:0.490631, valid loss:0.514517
Epoch:21, Train loss:0.490019, valid loss:0.514586
Epoch:22, Train loss:0.490044, valid loss:0.514407
Epoch:23, Train loss:0.489972, valid loss:0.514535
Epoch:24, Train loss:0.489993, valid loss:0.514564
Epoch:25, Train loss:0.489988, valid loss:0.514451
Epoch:26, Train loss:0.489950, valid loss:0.514532
Epoch:27, Train loss:0.489970, valid loss:0.514438
Epoch:28, Train loss:0.489891, valid loss:0.514674
Epoch:29, Train loss:0.489909, valid loss:0.514561
Epoch:30, Train loss:0.489885, valid loss:0.514687
Epoch:31, Train loss:0.489612, valid loss:0.514271
Epoch:32, Train loss:0.489575, valid loss:0.514290
Epoch:33, Train loss:0.489611, valid loss:0.514361
Epoch:34, Train loss:0.489571, valid loss:0.514323
Epoch:35, Train loss:0.489557, valid loss:0.514351
Epoch:36, Train loss:0.489567, valid loss:0.514289
Epoch:37, Train loss:0.489551, valid loss:0.514255
Epoch:38, Train loss:0.489550, valid loss:0.514286
Epoch:39, Train loss:0.489543, valid loss:0.514283
Epoch:40, Train loss:0.489512, valid loss:0.514209
Epoch:41, Train loss:0.489383, valid loss:0.514184
Epoch:42, Train loss:0.489375, valid loss:0.514222
Epoch:43, Train loss:0.489383, valid loss:0.514240
Epoch:44, Train loss:0.489372, valid loss:0.514206
Epoch:45, Train loss:0.489360, valid loss:0.514222
Epoch:46, Train loss:0.489365, valid loss:0.514194
Epoch:47, Train loss:0.489360, valid loss:0.514236
Epoch:48, Train loss:0.489349, valid loss:0.514253
Epoch:49, Train loss:0.489351, valid loss:0.514235
Epoch:50, Train loss:0.489338, valid loss:0.514242
Epoch:51, Train loss:0.489282, valid loss:0.514212
Epoch:52, Train loss:0.489275, valid loss:0.514157
Epoch:53, Train loss:0.489275, valid loss:0.514171
Epoch:54, Train loss:0.489269, valid loss:0.514223
Epoch:55, Train loss:0.489271, valid loss:0.514171
Epoch:56, Train loss:0.489266, valid loss:0.514223
Epoch:57, Train loss:0.489266, valid loss:0.514167
Epoch:58, Train loss:0.489263, valid loss:0.514172
Epoch:59, Train loss:0.489257, valid loss:0.514135
Epoch:60, Train loss:0.489258, valid loss:0.514182
training time 3749.6875989437103
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.780288915902413
plot_id,batch_id 0 1 miss% 0.8257691862841606
plot_id,batch_id 0 2 miss% 0.833668426370308
plot_id,batch_id 0 3 miss% 0.8334540324521905
plot_id,batch_id 0 4 miss% 0.8363188460630744
plot_id,batch_id 0 5 miss% 0.7749790579688695
plot_id,batch_id 0 6 miss% 0.8172708393695979
plot_id,batch_id 0 7 miss% 0.8300433796036132
plot_id,batch_id 0 8 miss% 0.8359066275427289
plot_id,batch_id 0 9 miss% 0.8414865034351454
plot_id,batch_id 0 10 miss% 0.7522709356691347
plot_id,batch_id 0 11 miss% 0.820310801867581
plot_id,batch_id 0 12 miss% 0.8272951895589589
plot_id,batch_id 0 13 miss% 0.8366843602216129
plot_id,batch_id 0 14 miss% 0.8331541546703388
plot_id,batch_id 0 15 miss% 0.7590034161094135
plot_id,batch_id 0 16 miss% 0.8187165296691593
plot_id,batch_id 0 17 miss% 0.8260207239440953
plot_id,batch_id 0 18 miss% 0.8344790416717397
plot_id,batch_id 0 19 miss% 0.8352094447008535
plot_id,batch_id 0 20 miss% 0.797255552814399
plot_id,batch_id 0 21 miss% 0.8297186979665608
plot_id,batch_id 0 22 miss% 0.8401299697304818
plot_id,batch_id 0 23 miss% 0.8384311058964332
plot_id,batch_id 0 24 miss% 0.8436890938588265
plot_id,batch_id 0 25 miss% 0.8050348910739721
plot_id,batch_id 0 26 miss% 0.829703451431659
plot_id,batch_id 0 27 miss% 0.833071139833918
plot_id,batch_id 0 28 miss% 0.8379301609368325
plot_id,batch_id 0 29 miss% 0.8373638630731699
plot_id,batch_id 0 30 miss% 0.7931677650585258
plot_id,batch_id 0 31 miss% 0.8259871337164597
plot_id,batch_id 0 32 miss% 0.8331635684000094
plot_id,batch_id 0 33 miss% 0.837546018228632
plot_id,batch_id 0 34 miss% 0.8373717705168001
plot_id,batch_id 0 35 miss% 0.7873002874266338
plot_id,batch_id 0 36 miss% 0.832631210596674
plot_id,batch_id 0 37 miss% 0.8333119102399612
plot_id,batch_id 0 38 miss% 0.8385926803722329
plot_id,batch_id 0 39 miss% 0.839034045015297
plot_id,batch_id 0 40 miss% 0.819944285597552
plot_id,batch_id 0 41 miss% 0.834535111829225
plot_id,batch_id 0 42 miss% 0.8386018557832541
plot_id,batch_id 0 43 miss% 0.8417511392704967
plot_id,batch_id 0 44 miss% 0.845093533136592
plot_id,batch_id 0 45 miss% 0.8140184004127592
plot_id,batch_id 0 46 miss% 0.8343165158408077
plot_id,batch_id 0 47 miss% 0.8394527848158435
plot_id,batch_id 0 48 miss% 0.840821354000531
plot_id,batch_id 0 49 miss% 0.8439270711204044
plot_id,batch_id 0 50 miss% 0.8189517400306124
plot_id,batch_id 0 51 miss% 0.8344492818634707
plot_id,batch_id 0 52 miss% 0.8359198448255348
plot_id,batch_id 0 53 miss% 0.8399641428503759
plot_id,batch_id 0 54 miss% 0.8447789558374957
plot_id,batch_id 0 55 miss% 0.8123156064897566
plot_id,batch_id 0 56 miss% 0.8329190330737974
plot_id,batch_id 0 57 miss% 0.8379890350689416
plot_id,batch_id 0 58 miss% 0.8409983461421257
plot_id,batch_id 0 59 miss% 0.8437156877374891
plot_id,batch_id 0 60 miss% 0.7085611603147292
plot_id,batch_id 0 61 miss% 0.8044806593252671
plot_id,batch_id 0 62 miss% 0.8186492563619482
plot_id,batch_id 0 63 miss% 0.8283064832970979
plot_id,batch_id 0 64 miss% 0.8307188392880861
plot_id,batch_id 0 65 miss% 0.7093723075243586
plot_id,batch_id 0 66 miss% 0.7906935611694765
plot_id,batch_id 0 67 miss% 0.8111346957150972
plot_id,batch_id 0 68 miss% 0.8239664024288725
plot_id,batch_id 0 69 miss% 0.8275668669673737
plot_id,batch_id 0 70 miss% 0.6727759740517085
plot_id,batch_id 0 71 miss% 0.7964287532243461
plot_id,batch_id 0 72 miss% 0.8054579569777099
plot_id,batch_id 0 73 miss% 0.8143150998873779
plot_id,batch_id 0 74 miss% 0.8222134622406015
plot_id,batch_id 0 75 miss% 0.6767349316304011
plot_id,batch_id 0 76 miss% 0.7886796034124465
plot_id,batch_id 0 77 miss% 0.7961685003931895
plot_id,batch_id 0 78 miss% 0.817525295977126
plot_id,batch_id 0 79 miss% 0.8167153535479573
plot_id,batch_id 0 80 miss% 0.7358201573384158
plot_id,batch_id 0 81 miss% 0.8166466474636428
plot_id,batch_id 0 82 miss% 0.8251817134297191
plot_id,batch_id 0 83 miss% 0.8334397113950831
plot_id,batch_id 0 84 miss% 0.8337036868673106
plot_id,batch_id 0 85 miss% 0.7287575094937565
plot_id,batch_id 0 86 miss% 0.8044047633081692
plot_id,batch_id 0 87 miss% 0.8193083071273554
plot_id,batch_id 0 88 miss% 0.8297993592030753
plot_id,batch_id 0 89 miss% 0.8320820254134544
plot_id,batch_id 0 90 miss% 0.6911378615246156
plot_id,batch_id 0 91 miss% 0.8003725259320119
plot_id,batch_id 0 92 miss% 0.8193095627245502
plot_id,batch_id 0 93 miss% 0.8265226930968564
plot_id,batch_id 0 94 miss% 0.8305095198577248
plot_id,batch_id 0 95 miss% 0.7209260413807038
plot_id,batch_id 0 96 miss% 0.7986709859917556
plot_id,batch_id 0 97 miss% 0.8088641987924488
plot_id,batch_id 0 98 miss% 0.8200677542019026
plot_id,batch_id 0 99 miss% 0.8229583031556857
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.78028892 0.82576919 0.83366843 0.83345403 0.83631885 0.77497906
 0.81727084 0.83004338 0.83590663 0.8414865  0.75227094 0.8203108
 0.82729519 0.83668436 0.83315415 0.75900342 0.81871653 0.82602072
 0.83447904 0.83520944 0.79725555 0.8297187  0.84012997 0.83843111
 0.84368909 0.80503489 0.82970345 0.83307114 0.83793016 0.83736386
 0.79316777 0.82598713 0.83316357 0.83754602 0.83737177 0.78730029
 0.83263121 0.83331191 0.83859268 0.83903405 0.81994429 0.83453511
 0.83860186 0.84175114 0.84509353 0.8140184  0.83431652 0.83945278
 0.84082135 0.84392707 0.81895174 0.83444928 0.83591984 0.83996414
 0.84477896 0.81231561 0.83291903 0.83798904 0.84099835 0.84371569
 0.70856116 0.80448066 0.81864926 0.82830648 0.83071884 0.70937231
 0.79069356 0.8111347  0.8239664  0.82756687 0.67277597 0.79642875
 0.80545796 0.8143151  0.82221346 0.67673493 0.7886796  0.7961685
 0.8175253  0.81671535 0.73582016 0.81664665 0.82518171 0.83343971
 0.83370369 0.72875751 0.80440476 0.81930831 0.82979936 0.83208203
 0.69113786 0.80037253 0.81930956 0.82652269 0.83050952 0.72092604
 0.79867099 0.8088642  0.82006775 0.8229583 ]
for model  28 the mean error 0.8135617694642295
all id 28 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 21 out win 4 err 0.8135617694642295
Launcher: Job 29 completed in 3932 seconds.
Launcher: Task 90 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  28945
Epoch:0, Train loss:0.642002, valid loss:0.642832
Epoch:1, Train loss:0.046675, valid loss:0.012452
Epoch:2, Train loss:0.013566, valid loss:0.005719
Epoch:3, Train loss:0.009952, valid loss:0.004506
Epoch:4, Train loss:0.008719, valid loss:0.004058
Epoch:5, Train loss:0.007601, valid loss:0.004355
Epoch:6, Train loss:0.006781, valid loss:0.003962
Epoch:7, Train loss:0.006180, valid loss:0.003195
Epoch:8, Train loss:0.005840, valid loss:0.003270
Epoch:9, Train loss:0.005456, valid loss:0.003170
Epoch:10, Train loss:0.005654, valid loss:0.003427
Epoch:11, Train loss:0.003772, valid loss:0.002339
Epoch:12, Train loss:0.003699, valid loss:0.002314
Epoch:13, Train loss:0.003686, valid loss:0.001820
Epoch:14, Train loss:0.003678, valid loss:0.002005
Epoch:15, Train loss:0.003531, valid loss:0.002625
Epoch:16, Train loss:0.003604, valid loss:0.002372
Epoch:17, Train loss:0.003441, valid loss:0.002121
Epoch:18, Train loss:0.003429, valid loss:0.002275
Epoch:19, Train loss:0.003308, valid loss:0.002072
Epoch:20, Train loss:0.003313, valid loss:0.001818
Epoch:21, Train loss:0.002526, valid loss:0.001439
Epoch:22, Train loss:0.002507, valid loss:0.001723
Epoch:23, Train loss:0.002536, valid loss:0.001401
Epoch:24, Train loss:0.002615, valid loss:0.001690
Epoch:25, Train loss:0.002449, valid loss:0.001438
Epoch:26, Train loss:0.002393, valid loss:0.001481
Epoch:27, Train loss:0.002454, valid loss:0.001481
Epoch:28, Train loss:0.002379, valid loss:0.001574
Epoch:29, Train loss:0.002415, valid loss:0.001452
Epoch:30, Train loss:0.002411, valid loss:0.001462
Epoch:31, Train loss:0.002001, valid loss:0.001232
Epoch:32, Train loss:0.001982, valid loss:0.001233
Epoch:33, Train loss:0.001983, valid loss:0.001240
Epoch:34, Train loss:0.001985, valid loss:0.001260
Epoch:35, Train loss:0.001971, valid loss:0.001335
Epoch:36, Train loss:0.001937, valid loss:0.001161
Epoch:37, Train loss:0.001959, valid loss:0.001305
Epoch:38, Train loss:0.001938, valid loss:0.001314
Epoch:39, Train loss:0.001933, valid loss:0.001117
Epoch:40, Train loss:0.001907, valid loss:0.001267
Epoch:41, Train loss:0.001742, valid loss:0.001077
Epoch:42, Train loss:0.001725, valid loss:0.001103
Epoch:43, Train loss:0.001730, valid loss:0.001122
Epoch:44, Train loss:0.001745, valid loss:0.001141
Epoch:45, Train loss:0.001723, valid loss:0.001084
Epoch:46, Train loss:0.001720, valid loss:0.001081
Epoch:47, Train loss:0.001714, valid loss:0.001115
Epoch:48, Train loss:0.001705, valid loss:0.001108
Epoch:49, Train loss:0.001711, valid loss:0.001135
Epoch:50, Train loss:0.001699, valid loss:0.001142
Epoch:51, Train loss:0.001612, valid loss:0.001049
Epoch:52, Train loss:0.001602, valid loss:0.001073
Epoch:53, Train loss:0.001607, valid loss:0.001081
Epoch:54, Train loss:0.001602, valid loss:0.001094
Epoch:55, Train loss:0.001598, valid loss:0.001076
Epoch:56, Train loss:0.001600, valid loss:0.001061
Epoch:57, Train loss:0.001594, valid loss:0.001081
Epoch:58, Train loss:0.001592, valid loss:0.001111
Epoch:59, Train loss:0.001590, valid loss:0.001055
Epoch:60, Train loss:0.001591, valid loss:0.001089
training time 3826.032487154007
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.05529983154514429
plot_id,batch_id 0 1 miss% 0.025229910644628976
plot_id,batch_id 0 2 miss% 0.020067454064442564
plot_id,batch_id 0 3 miss% 0.024345874337875554
plot_id,batch_id 0 4 miss% 0.020620206217501526
plot_id,batch_id 0 5 miss% 0.037556646259372724
plot_id,batch_id 0 6 miss% 0.02720859930363071
plot_id,batch_id 0 7 miss% 0.023523346259777127
plot_id,batch_id 0 8 miss% 0.022819268934516163
plot_id,batch_id 0 9 miss% 0.024292483445084216
plot_id,batch_id 0 10 miss% 0.05933286476509439
plot_id,batch_id 0 11 miss% 0.0499325285568448
plot_id,batch_id 0 12 miss% 0.027252066218425887
plot_id,batch_id 0 13 miss% 0.022534484415849185
plot_id,batch_id 0 14 miss% 0.027440000453939364
plot_id,batch_id 0 15 miss% 0.047743154300767754
plot_id,batch_id 0 16 miss% 0.03707156561491447
plot_id,batch_id 0 17 miss% 0.031693470797853236
plot_id,batch_id 0 18 miss% 0.021984246784665297
plot_id,batch_id 0 19 miss% 0.0382647862472434
plot_id,batch_id 0 20 miss% 0.06326204340336565
plot_id,batch_id 0 21 miss% 0.01755734472783766
plot_id,batch_id 0 22 miss% 0.021119097998648185
plot_id,batch_id 0 23 miss% 0.012209388728197256
plot_id,batch_id 0 24 miss% 0.016271342816520214
plot_id,batch_id 0 25 miss% 0.0298104738061179
plot_id,batch_id 0 26 miss% 0.034239037038579674
plot_id,batch_id 0 27 miss% 0.024479808844565243
plot_id,batch_id 0 28 miss% 0.014591255506586319
plot_id,batch_id 0 29 miss% 0.023126820041514076
plot_id,batch_id 0 30 miss% 0.03589015785306866
plot_id,batch_id 0 31 miss% 0.03014014138613614
plot_id,batch_id 0 32 miss% 0.030339791911605827
plot_id,batch_id 0 33 miss% 0.02802801909310726
plot_id,batch_id 0 34 miss% 0.02119394311745056
plot_id,batch_id 0 35 miss% 0.042590553596560554
plot_id,batch_id 0 36 miss% 0.03663329813282154
plot_id,batch_id 0 37 miss% 0.04970618816213431
plot_id,batch_id 0 38 miss% 0.035363718120251755
plot_id,batch_id 0 39 miss% 0.023460202917641358
plot_id,batch_id 0 40 miss% 0.07114261799243397
plot_id,batch_id 0 41 miss% 0.018041794196051092
plot_id,batch_id 0 42 miss% 0.01340495013708777
plot_id,batch_id 0 43 miss% 0.013227040920960347
plot_id,batch_id 0 44 miss% 0.017368535724427092
plot_id,batch_id 0 45 miss% 0.04347817687143619
plot_id,batch_id 0 46 miss% 0.02528662111973645
plot_id,batch_id 0 47 miss% 0.016538905338872055
plot_id,batch_id 0 48 miss% 0.014636193139382813
plot_id,batch_id 0 49 miss% 0.018966688874712576
plot_id,batch_id 0 50 miss% 0.03621701116036949
plot_id,batch_id 0 51 miss% 0.015642671516608836
plot_id,batch_id 0 52 miss% 0.02178069162582501
plot_id,batch_id 0 53 miss% 0.025926370088684748
plot_id,batch_id 0 54 miss% 0.029105100171677622
plot_id,batch_id 0 55 miss% 0.0384842723089402
plot_id,batch_id 0 56 miss% 0.021276898365370955
plot_id,batch_id 0 57 miss% 0.01896020797126237
plot_id,batch_id 0 58 miss% 0.018878871442171954
plot_id,batch_id 0 59 miss% 0.013311745640760148
plot_id,batch_id 0 60 miss% 0.04652838251874402
plot_id,batch_id 0 61 miss% 0.03366816491895425
plot_id,batch_id 0 62 miss% 0.026992407781920576
plot_id,batch_id 0 63 miss% 0.03576391942455891
plot_id,batch_id 0 64 miss% 0.037827467243311146
plot_id,batch_id 0 65 miss% 0.052295305168169076
plot_id,batch_id 0 66 miss% 0.0494770902305118
plot_id,batch_id 0 67 miss% 0.03794557528812067
plot_id,batch_id 0 68 miss% 0.027496105477092244
plot_id,batch_id 0 69 miss% 0.02334757505961462
plot_id,batch_id 0 70 miss% 0.03352059909287005
plot_id,batch_id 0 71 miss% 0.07204057005513705
plot_id,batch_id 0 72 miss% 0.04159675885238983
plot_id,batch_id 0 73 miss% 0.029306654050618632
plot_id,batch_id 0 74 miss% 0.038368734423034546
plot_id,batch_id 0 75 miss% 0.04216094734590014
plot_id,batch_id 0 76 miss% 0.06201744463206601
plot_id,batch_id 0 77 miss% 0.01735158609756517
plot_id,batch_id 0 78 miss% 0.025683872155100033
plot_id,batch_id 0 79 miss% 0.046589828714746304
plot_id,batch_id 0 80 miss% 0.02724205418624323
plot_id,batch_id 0 81 miss% 0.03843234633692354
plot_id,batch_id 0 82 miss% 0.021032843488910852
plot_id,batch_id 0 83 miss% 0.02798040554723338
plot_id,batch_id 0 84 miss% 0.023305098033165383
plot_id,batch_id 0 85 miss% 0.03843474569137693
plot_id,batch_id 0 86 miss% 0.03134556585548516
plot_id,batch_id 0 87 miss% 0.027839844159800304
plot_id,batch_id 0 88 miss% 0.038675814919262634
plot_id,batch_id 0 89 miss% 0.020521950553712014
plot_id,batch_id 0 90 miss% 0.05107568238606148
plot_id,batch_id 0 91 miss% 0.025624159746843008
plot_id,batch_id 0 92 miss% 0.030682871381698007
plot_id,batch_id 0 93 miss% 0.03246422293090623
plot_id,batch_id 0 94 miss% 0.03072660650589097
plot_id,batch_id 0 95 miss% 0.04051953306338688
plot_id,batch_id 0 96 miss% 0.030193728303993302
plot_id,batch_id 0 97 miss% 0.0643906584366666
plot_id,batch_id 0 98 miss% 0.03924358645595964
plot_id,batch_id 0 99 miss% 0.03240803747960609
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05529983 0.02522991 0.02006745 0.02434587 0.02062021 0.03755665
 0.0272086  0.02352335 0.02281927 0.02429248 0.05933286 0.04993253
 0.02725207 0.02253448 0.02744    0.04774315 0.03707157 0.03169347
 0.02198425 0.03826479 0.06326204 0.01755734 0.0211191  0.01220939
 0.01627134 0.02981047 0.03423904 0.02447981 0.01459126 0.02312682
 0.03589016 0.03014014 0.03033979 0.02802802 0.02119394 0.04259055
 0.0366333  0.04970619 0.03536372 0.0234602  0.07114262 0.01804179
 0.01340495 0.01322704 0.01736854 0.04347818 0.02528662 0.01653891
 0.01463619 0.01896669 0.03621701 0.01564267 0.02178069 0.02592637
 0.0291051  0.03848427 0.0212769  0.01896021 0.01887887 0.01331175
 0.04652838 0.03366816 0.02699241 0.03576392 0.03782747 0.05229531
 0.04947709 0.03794558 0.02749611 0.02334758 0.0335206  0.07204057
 0.04159676 0.02930665 0.03836873 0.04216095 0.06201744 0.01735159
 0.02568387 0.04658983 0.02724205 0.03843235 0.02103284 0.02798041
 0.0233051  0.03843475 0.03134557 0.02783984 0.03867581 0.02052195
 0.05107568 0.02562416 0.03068287 0.03246422 0.03072661 0.04051953
 0.03019373 0.06439066 0.03924359 0.03240804]
for model  63 the mean error 0.031780195249706045
all id 63 hidden_dim 16 learning_rate 0.02 num_layers 4 frames 21 out win 3 err 0.031780195249706045
Launcher: Job 64 completed in 4028 seconds.
Launcher: Task 154 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  21969
Epoch:0, Train loss:0.613368, valid loss:0.573821
Epoch:1, Train loss:0.352085, valid loss:0.358730
Epoch:2, Train loss:0.342764, valid loss:0.358668
Epoch:3, Train loss:0.341194, valid loss:0.357071
Epoch:4, Train loss:0.340235, valid loss:0.357488
Epoch:5, Train loss:0.339918, valid loss:0.357666
Epoch:6, Train loss:0.339659, valid loss:0.356821
Epoch:7, Train loss:0.339292, valid loss:0.356495
Epoch:8, Train loss:0.339299, valid loss:0.356889
Epoch:9, Train loss:0.339111, valid loss:0.356861
Epoch:10, Train loss:0.338941, valid loss:0.357177
Epoch:11, Train loss:0.337863, valid loss:0.355979
Epoch:12, Train loss:0.337886, valid loss:0.356396
Epoch:13, Train loss:0.337832, valid loss:0.355998
Epoch:14, Train loss:0.337815, valid loss:0.355982
Epoch:15, Train loss:0.337740, valid loss:0.356295
Epoch:16, Train loss:0.337776, valid loss:0.356321
Epoch:17, Train loss:0.337710, valid loss:0.356070
Epoch:18, Train loss:0.337682, valid loss:0.355938
Epoch:19, Train loss:0.337637, valid loss:0.356392
Epoch:20, Train loss:0.337729, valid loss:0.355924
Epoch:21, Train loss:0.337168, valid loss:0.355916
Epoch:22, Train loss:0.337113, valid loss:0.355784
Epoch:23, Train loss:0.337124, valid loss:0.355960
Epoch:24, Train loss:0.337108, valid loss:0.355674
Epoch:25, Train loss:0.337135, valid loss:0.355826
Epoch:26, Train loss:0.337107, valid loss:0.355632
Epoch:27, Train loss:0.337168, valid loss:0.355746
Epoch:28, Train loss:0.337074, valid loss:0.355945
Epoch:29, Train loss:0.337053, valid loss:0.355624
Epoch:30, Train loss:0.337031, valid loss:0.355714
Epoch:31, Train loss:0.336793, valid loss:0.355556
Epoch:32, Train loss:0.336806, valid loss:0.355509
Epoch:33, Train loss:0.336799, valid loss:0.355642
Epoch:34, Train loss:0.336785, valid loss:0.355571
Epoch:35, Train loss:0.336797, valid loss:0.355505
Epoch:36, Train loss:0.336794, valid loss:0.355589
Epoch:37, Train loss:0.336782, valid loss:0.355618
Epoch:38, Train loss:0.336784, valid loss:0.355504
Epoch:39, Train loss:0.336767, valid loss:0.355600
Epoch:40, Train loss:0.336773, valid loss:0.355513
Epoch:41, Train loss:0.336633, valid loss:0.355481
Epoch:42, Train loss:0.336630, valid loss:0.355485
Epoch:43, Train loss:0.336632, valid loss:0.355488
Epoch:44, Train loss:0.336627, valid loss:0.355574
Epoch:45, Train loss:0.336629, valid loss:0.355506
Epoch:46, Train loss:0.336633, valid loss:0.355592
Epoch:47, Train loss:0.336617, valid loss:0.355484
Epoch:48, Train loss:0.336614, valid loss:0.355453
Epoch:49, Train loss:0.336616, valid loss:0.355474
Epoch:50, Train loss:0.336618, valid loss:0.355484
Epoch:51, Train loss:0.336546, valid loss:0.355456
Epoch:52, Train loss:0.336548, valid loss:0.355429
Epoch:53, Train loss:0.336547, valid loss:0.355448
Epoch:54, Train loss:0.336546, valid loss:0.355469
Epoch:55, Train loss:0.336542, valid loss:0.355460
Epoch:56, Train loss:0.336541, valid loss:0.355435
Epoch:57, Train loss:0.336540, valid loss:0.355423
Epoch:58, Train loss:0.336539, valid loss:0.355428
Epoch:59, Train loss:0.336542, valid loss:0.355432
Epoch:60, Train loss:0.336535, valid loss:0.355432
training time 3911.0538573265076
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.6387971008218185
plot_id,batch_id 0 1 miss% 0.7345788377749404
plot_id,batch_id 0 2 miss% 0.7450774117540849
plot_id,batch_id 0 3 miss% 0.7572062183807328
plot_id,batch_id 0 4 miss% 0.7630502842910337
plot_id,batch_id 0 5 miss% 0.6371477870651068
plot_id,batch_id 0 6 miss% 0.7289917806374437
plot_id,batch_id 0 7 miss% 0.7483471133235255
plot_id,batch_id 0 8 miss% 0.7573957129162875
plot_id,batch_id 0 9 miss% 0.7610252587949374
plot_id,batch_id 0 10 miss% 0.6094906576119551
plot_id,batch_id 0 11 miss% 0.7341370897670527
plot_id,batch_id 0 12 miss% 0.7410376614884191
plot_id,batch_id 0 13 miss% 0.7502202588835845
plot_id,batch_id 0 14 miss% 0.7566441966559703
plot_id,batch_id 0 15 miss% 0.6214392356922964
plot_id,batch_id 0 16 miss% 0.7215664773928626
plot_id,batch_id 0 17 miss% 0.7463681054300589
plot_id,batch_id 0 18 miss% 0.7518609430652432
plot_id,batch_id 0 19 miss% 0.7548837048620085
plot_id,batch_id 0 20 miss% 0.7049725107631153
plot_id,batch_id 0 21 miss% 0.7519097945511413
plot_id,batch_id 0 22 miss% 0.7590262413461158
plot_id,batch_id 0 23 miss% 0.7641676254807518
plot_id,batch_id 0 24 miss% 0.7649210529103274
plot_id,batch_id 0 25 miss% 0.6841175357221724
plot_id,batch_id 0 26 miss% 0.7388228611445087
plot_id,batch_id 0 27 miss% 0.7571226306513585
plot_id,batch_id 0 28 miss% 0.7637307654479935
plot_id,batch_id 0 29 miss% 0.7662291351956675
plot_id,batch_id 0 30 miss% 0.6655605137454843
plot_id,batch_id 0 31 miss% 0.7424034814967378
plot_id,batch_id 0 32 miss% 0.7504683217265841
plot_id,batch_id 0 33 miss% 0.7649574124985162
plot_id,batch_id 0 34 miss% 0.765785332408801
plot_id,batch_id 0 35 miss% 0.6654634743662187
plot_id,batch_id 0 36 miss% 0.7445041291789805
plot_id,batch_id 0 37 miss% 0.7499526712287675
plot_id,batch_id 0 38 miss% 0.7611070576780014
plot_id,batch_id 0 39 miss% 0.7598833192605245
plot_id,batch_id 0 40 miss% 0.7217154124565545
plot_id,batch_id 0 41 miss% 0.755706894992902
plot_id,batch_id 0 42 miss% 0.7630069001647513
plot_id,batch_id 0 43 miss% 0.7663307639377426
plot_id,batch_id 0 44 miss% 0.7688104548732196
plot_id,batch_id 0 45 miss% 0.7269598145105781
plot_id,batch_id 0 46 miss% 0.7537851547580228
plot_id,batch_id 0 47 miss% 0.7628407117315423
plot_id,batch_id 0 48 miss% 0.7639200794750971
plot_id,batch_id 0 49 miss% 0.7694917239422597
plot_id,batch_id 0 50 miss% 0.7192304830865017
plot_id,batch_id 0 51 miss% 0.7550593567179403
plot_id,batch_id 0 52 miss% 0.7576257142106362
plot_id,batch_id 0 53 miss% 0.7665597224630489
plot_id,batch_id 0 54 miss% 0.7716065056795619
plot_id,batch_id 0 55 miss% 0.7183966996652834
plot_id,batch_id 0 56 miss% 0.7518424709738196
plot_id,batch_id 0 57 miss% 0.7605900180380403
plot_id,batch_id 0 58 miss% 0.7637563211087229
plot_id,batch_id 0 59 miss% 0.7658539013472023
plot_id,batch_id 0 60 miss% 0.5407124703381085
plot_id,batch_id 0 61 miss% 0.6937971239885204
plot_id,batch_id 0 62 miss% 0.7232691362170793
plot_id,batch_id 0 63 miss% 0.7432834073238813
plot_id,batch_id 0 64 miss% 0.743183558900039
plot_id,batch_id 0 65 miss% 0.5382221595338945
plot_id,batch_id 0 66 miss% 0.6897893405016875
plot_id,batch_id 0 67 miss% 0.7075290408480578
plot_id,batch_id 0 68 miss% 0.7389192427051742
plot_id,batch_id 0 69 miss% 0.7424730169842116
plot_id,batch_id 0 70 miss% 0.5141688883570044
plot_id,batch_id 0 71 miss% 0.6850583600726922
plot_id,batch_id 0 72 miss% 0.6975737431357231
plot_id,batch_id 0 73 miss% 0.7247023283929369
plot_id,batch_id 0 74 miss% 0.7362234372647607
plot_id,batch_id 0 75 miss% 0.49076419776036523
plot_id,batch_id 0 76 miss% 0.6446230267334788
plot_id,batch_id 0 77 miss% 0.6840457129855498
plot_id,batch_id 0 78 miss% 0.7202676754395251
plot_id,batch_id 0 79 miss% 0.7291862379067041
plot_id,batch_id 0 80 miss% 0.563183721852913
plot_id,batch_id 0 81 miss% 0.7088318457580487
plot_id,batch_id 0 82 miss% 0.7321112065329237
plot_id,batch_id 0 83 miss% 0.7471150779428184
plot_id,batch_id 0 84 miss% 0.74904292460097
plot_id,batch_id 0 85 miss% 0.5631523289626925
plot_id,batch_id 0 86 miss% 0.7020672640986335
plot_id,batch_id 0 87 miss% 0.7239857334541266
plot_id,batch_id 0 88 miss% 0.7436693409137861
plot_id,batch_id 0 89 miss% 0.7515777175475632
plot_id,batch_id 0 90 miss% 0.5299357892128587
plot_id,batch_id 0 91 miss% 0.7026930688669967
plot_id,batch_id 0 92 miss% 0.7180166491677041
plot_id,batch_id 0 93 miss% 0.7496309600828335
plot_id,batch_id 0 94 miss% 0.748601384394762
plot_id,batch_id 0 95 miss% 0.5368208353158032
plot_id,batch_id 0 96 miss% 0.6792687771775904
plot_id,batch_id 0 97 miss% 0.7152018724878125
plot_id,batch_id 0 98 miss% 0.725261341827446
plot_id,batch_id 0 99 miss% 0.7394411161186313
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.6387971  0.73457884 0.74507741 0.75720622 0.76305028 0.63714779
 0.72899178 0.74834711 0.75739571 0.76102526 0.60949066 0.73413709
 0.74103766 0.75022026 0.7566442  0.62143924 0.72156648 0.74636811
 0.75186094 0.7548837  0.70497251 0.75190979 0.75902624 0.76416763
 0.76492105 0.68411754 0.73882286 0.75712263 0.76373077 0.76622914
 0.66556051 0.74240348 0.75046832 0.76495741 0.76578533 0.66546347
 0.74450413 0.74995267 0.76110706 0.75988332 0.72171541 0.75570689
 0.7630069  0.76633076 0.76881045 0.72695981 0.75378515 0.76284071
 0.76392008 0.76949172 0.71923048 0.75505936 0.75762571 0.76655972
 0.77160651 0.7183967  0.75184247 0.76059002 0.76375632 0.7658539
 0.54071247 0.69379712 0.72326914 0.74328341 0.74318356 0.53822216
 0.68978934 0.70752904 0.73891924 0.74247302 0.51416889 0.68505836
 0.69757374 0.72470233 0.73622344 0.4907642  0.64462303 0.68404571
 0.72026768 0.72918624 0.56318372 0.70883185 0.73211121 0.74711508
 0.74904292 0.56315233 0.70206726 0.72398573 0.74366934 0.75157772
 0.52993579 0.70269307 0.71801665 0.74963096 0.74860138 0.53682084
 0.67926878 0.71520187 0.72526134 0.73944112]
for model  135 the mean error 0.7168486186925087
all id 135 hidden_dim 16 learning_rate 0.02 num_layers 3 frames 25 out win 3 err 0.7168486186925087
Launcher: Job 136 completed in 4089 seconds.
Launcher: Task 73 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  46193
Epoch:0, Train loss:0.728772, valid loss:0.688975
Epoch:1, Train loss:0.507976, valid loss:0.517093
Epoch:2, Train loss:0.494154, valid loss:0.515529
Epoch:3, Train loss:0.491208, valid loss:0.514740
Epoch:4, Train loss:0.489679, valid loss:0.513644
Epoch:5, Train loss:0.489167, valid loss:0.513963
Epoch:6, Train loss:0.488849, valid loss:0.514492
Epoch:7, Train loss:0.488677, valid loss:0.514736
Epoch:8, Train loss:0.488264, valid loss:0.513532
Epoch:9, Train loss:0.488052, valid loss:0.513787
Epoch:10, Train loss:0.488028, valid loss:0.513488
Epoch:11, Train loss:0.486567, valid loss:0.512659
Epoch:12, Train loss:0.486617, valid loss:0.512903
Epoch:13, Train loss:0.486658, valid loss:0.513062
Epoch:14, Train loss:0.486562, valid loss:0.512629
Epoch:15, Train loss:0.486407, valid loss:0.512608
Epoch:16, Train loss:0.486504, valid loss:0.512575
Epoch:17, Train loss:0.486382, valid loss:0.512497
Epoch:18, Train loss:0.486380, valid loss:0.512402
Epoch:19, Train loss:0.486348, valid loss:0.512584
Epoch:20, Train loss:0.486377, valid loss:0.512349
Epoch:21, Train loss:0.485645, valid loss:0.512165
Epoch:22, Train loss:0.485601, valid loss:0.512143
Epoch:23, Train loss:0.485667, valid loss:0.512233
Epoch:24, Train loss:0.485696, valid loss:0.512292
Epoch:25, Train loss:0.485677, valid loss:0.512196
Epoch:26, Train loss:0.485601, valid loss:0.512239
Epoch:27, Train loss:0.485593, valid loss:0.512149
Epoch:28, Train loss:0.485581, valid loss:0.512018
Epoch:29, Train loss:0.485508, valid loss:0.512268
Epoch:30, Train loss:0.485513, valid loss:0.512248
Epoch:31, Train loss:0.485258, valid loss:0.511954
Epoch:32, Train loss:0.485227, valid loss:0.511973
Epoch:33, Train loss:0.485236, valid loss:0.512041
Epoch:34, Train loss:0.485257, valid loss:0.512111
Epoch:35, Train loss:0.485235, valid loss:0.511997
Epoch:36, Train loss:0.485204, valid loss:0.511981
Epoch:37, Train loss:0.485214, valid loss:0.512076
Epoch:38, Train loss:0.485188, valid loss:0.511909
Epoch:39, Train loss:0.485186, valid loss:0.512048
Epoch:40, Train loss:0.485180, valid loss:0.511930
Epoch:41, Train loss:0.485055, valid loss:0.511935
Epoch:42, Train loss:0.485027, valid loss:0.511919
Epoch:43, Train loss:0.485039, valid loss:0.511954
Epoch:44, Train loss:0.485029, valid loss:0.511900
Epoch:45, Train loss:0.485015, valid loss:0.511893
Epoch:46, Train loss:0.485011, valid loss:0.511873
Epoch:47, Train loss:0.485021, valid loss:0.511862
Epoch:48, Train loss:0.485028, valid loss:0.511898
Epoch:49, Train loss:0.485017, valid loss:0.511875
Epoch:50, Train loss:0.484996, valid loss:0.511919
Epoch:51, Train loss:0.484942, valid loss:0.511826
Epoch:52, Train loss:0.484941, valid loss:0.511878
Epoch:53, Train loss:0.484936, valid loss:0.511834
Epoch:54, Train loss:0.484930, valid loss:0.511818
Epoch:55, Train loss:0.484936, valid loss:0.511835
Epoch:56, Train loss:0.484928, valid loss:0.511805
Epoch:57, Train loss:0.484931, valid loss:0.511832
Epoch:58, Train loss:0.484926, valid loss:0.511812
Epoch:59, Train loss:0.484928, valid loss:0.511812
Epoch:60, Train loss:0.484923, valid loss:0.511827
training time 3913.569276571274
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.6978174460196703
plot_id,batch_id 0 1 miss% 0.7731313860640895
plot_id,batch_id 0 2 miss% 0.7817339233006634
plot_id,batch_id 0 3 miss% 0.7888002871013168
plot_id,batch_id 0 4 miss% 0.7942845113091133
plot_id,batch_id 0 5 miss% 0.6920394604616049
plot_id,batch_id 0 6 miss% 0.7672783655218691
plot_id,batch_id 0 7 miss% 0.7829404887561119
plot_id,batch_id 0 8 miss% 0.7901334643073499
plot_id,batch_id 0 9 miss% 0.7973516280651685
plot_id,batch_id 0 10 miss% 0.6679870935760628
plot_id,batch_id 0 11 miss% 0.7667116190019
plot_id,batch_id 0 12 miss% 0.7772937168100942
plot_id,batch_id 0 13 miss% 0.7880420398207754
plot_id,batch_id 0 14 miss% 0.7898658846621893
plot_id,batch_id 0 15 miss% 0.681978674153584
plot_id,batch_id 0 16 miss% 0.7559942331236886
plot_id,batch_id 0 17 miss% 0.7814397501104671
plot_id,batch_id 0 18 miss% 0.7850703772321951
plot_id,batch_id 0 19 miss% 0.7895778305638577
plot_id,batch_id 0 20 miss% 0.7507887714910874
plot_id,batch_id 0 21 miss% 0.7827698303109201
plot_id,batch_id 0 22 miss% 0.7903791026747037
plot_id,batch_id 0 23 miss% 0.7940491258474559
plot_id,batch_id 0 24 miss% 0.7963154055772966
plot_id,batch_id 0 25 miss% 0.728901133247539
plot_id,batch_id 0 26 miss% 0.783635521073006
plot_id,batch_id 0 27 miss% 0.787063887358401
plot_id,batch_id 0 28 miss% 0.7925316619126866
plot_id,batch_id 0 29 miss% 0.794203176117692
plot_id,batch_id 0 30 miss% 0.7277822809272543
plot_id,batch_id 0 31 miss% 0.7765058064130922
plot_id,batch_id 0 32 miss% 0.7884854988353058
plot_id,batch_id 0 33 miss% 0.7915158926847009
plot_id,batch_id 0 34 miss% 0.7948794096983666
plot_id,batch_id 0 35 miss% 0.7259726070032728
plot_id,batch_id 0 36 miss% 0.7833032909906086
plot_id,batch_id 0 37 miss% 0.7821772118401538
plot_id,batch_id 0 38 miss% 0.7940655743024474
plot_id,batch_id 0 39 miss% 0.7953039510094645
plot_id,batch_id 0 40 miss% 0.7651809685418292
plot_id,batch_id 0 41 miss% 0.7887133554062252
plot_id,batch_id 0 42 miss% 0.7919585777350889
plot_id,batch_id 0 43 miss% 0.798837534778378
plot_id,batch_id 0 44 miss% 0.7978057762811991
plot_id,batch_id 0 45 miss% 0.7602717419593602
plot_id,batch_id 0 46 miss% 0.7870591222334985
plot_id,batch_id 0 47 miss% 0.7940104646909467
plot_id,batch_id 0 48 miss% 0.7996836871708433
plot_id,batch_id 0 49 miss% 0.7995501474397991
plot_id,batch_id 0 50 miss% 0.7676913450777337
plot_id,batch_id 0 51 miss% 0.7863397930103836
plot_id,batch_id 0 52 miss% 0.7924022586496439
plot_id,batch_id 0 53 miss% 0.7994335772303124
plot_id,batch_id 0 54 miss% 0.8024786751459184
plot_id,batch_id 0 55 miss% 0.7655524824108912
plot_id,batch_id 0 56 miss% 0.786138325624086
plot_id,batch_id 0 57 miss% 0.7950452154621134
plot_id,batch_id 0 58 miss% 0.7966994925160432
plot_id,batch_id 0 59 miss% 0.7976516554341284
plot_id,batch_id 0 60 miss% 0.6128122852747049
plot_id,batch_id 0 61 miss% 0.7321852841994225
plot_id,batch_id 0 62 miss% 0.7635761700722503
plot_id,batch_id 0 63 miss% 0.7759684915845131
plot_id,batch_id 0 64 miss% 0.7832835384063614
plot_id,batch_id 0 65 miss% 0.6067178328779109
plot_id,batch_id 0 66 miss% 0.729083268285028
plot_id,batch_id 0 67 miss% 0.7499875462147814
plot_id,batch_id 0 68 miss% 0.7734650158622248
plot_id,batch_id 0 69 miss% 0.7800483117336005
plot_id,batch_id 0 70 miss% 0.5753330871765913
plot_id,batch_id 0 71 miss% 0.7320224774235616
plot_id,batch_id 0 72 miss% 0.7413327225713239
plot_id,batch_id 0 73 miss% 0.7589241027879207
plot_id,batch_id 0 74 miss% 0.7755798976322942
plot_id,batch_id 0 75 miss% 0.5663616815922684
plot_id,batch_id 0 76 miss% 0.6881598594332392
plot_id,batch_id 0 77 miss% 0.7291230318584824
plot_id,batch_id 0 78 miss% 0.7564317534211765
plot_id,batch_id 0 79 miss% 0.7719720676595845
plot_id,batch_id 0 80 miss% 0.6368098537702407
plot_id,batch_id 0 81 miss% 0.7525933811985731
plot_id,batch_id 0 82 miss% 0.7728400590698995
plot_id,batch_id 0 83 miss% 0.7844406719390256
plot_id,batch_id 0 84 miss% 0.7836184602476868
plot_id,batch_id 0 85 miss% 0.6405896677400181
plot_id,batch_id 0 86 miss% 0.7467633019701487
plot_id,batch_id 0 87 miss% 0.7673571242274448
plot_id,batch_id 0 88 miss% 0.7800053922479538
plot_id,batch_id 0 89 miss% 0.7818702229224936
plot_id,batch_id 0 90 miss% 0.6022188438916893
plot_id,batch_id 0 91 miss% 0.7444501620811561
plot_id,batch_id 0 92 miss% 0.7588439879275956
plot_id,batch_id 0 93 miss% 0.775841273155082
plot_id,batch_id 0 94 miss% 0.78053360336314
plot_id,batch_id 0 95 miss% 0.6131567902668534
plot_id,batch_id 0 96 miss% 0.723459280012038
plot_id,batch_id 0 97 miss% 0.7564231687596451
plot_id,batch_id 0 98 miss% 0.7687586994607839
plot_id,batch_id 0 99 miss% 0.7779351266823887
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.69781745 0.77313139 0.78173392 0.78880029 0.79428451 0.69203946
 0.76727837 0.78294049 0.79013346 0.79735163 0.66798709 0.76671162
 0.77729372 0.78804204 0.78986588 0.68197867 0.75599423 0.78143975
 0.78507038 0.78957783 0.75078877 0.78276983 0.7903791  0.79404913
 0.79631541 0.72890113 0.78363552 0.78706389 0.79253166 0.79420318
 0.72778228 0.77650581 0.7884855  0.79151589 0.79487941 0.72597261
 0.78330329 0.78217721 0.79406557 0.79530395 0.76518097 0.78871336
 0.79195858 0.79883753 0.79780578 0.76027174 0.78705912 0.79401046
 0.79968369 0.79955015 0.76769135 0.78633979 0.79240226 0.79943358
 0.80247868 0.76555248 0.78613833 0.79504522 0.79669949 0.79765166
 0.61281229 0.73218528 0.76357617 0.77596849 0.78328354 0.60671783
 0.72908327 0.74998755 0.77346502 0.78004831 0.57533309 0.73202248
 0.74133272 0.7589241  0.7755799  0.56636168 0.68815986 0.72912303
 0.75643175 0.77197207 0.63680985 0.75259338 0.77284006 0.78444067
 0.78361846 0.64058967 0.7467633  0.76735712 0.78000539 0.78187022
 0.60221884 0.74445016 0.75884399 0.77584127 0.7805336  0.61315679
 0.72345928 0.75642317 0.7687587  0.77793513]
for model  57 the mean error 0.7573348301307675
all id 57 hidden_dim 24 learning_rate 0.02 num_layers 3 frames 21 out win 3 err 0.7573348301307675
Launcher: Job 58 completed in 4096 seconds.
Launcher: Task 217 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  28945
Epoch:0, Train loss:0.642002, valid loss:0.642832
Epoch:1, Train loss:0.061693, valid loss:0.019660
Epoch:2, Train loss:0.018358, valid loss:0.006565
Epoch:3, Train loss:0.010994, valid loss:0.005866
Epoch:4, Train loss:0.009063, valid loss:0.004131
Epoch:5, Train loss:0.008058, valid loss:0.003742
Epoch:6, Train loss:0.007310, valid loss:0.004594
Epoch:7, Train loss:0.006356, valid loss:0.003583
Epoch:8, Train loss:0.005760, valid loss:0.002489
Epoch:9, Train loss:0.005429, valid loss:0.002317
Epoch:10, Train loss:0.004965, valid loss:0.003503
Epoch:11, Train loss:0.003604, valid loss:0.001795
Epoch:12, Train loss:0.003519, valid loss:0.001710
Epoch:13, Train loss:0.003520, valid loss:0.002092
Epoch:14, Train loss:0.003449, valid loss:0.001655
Epoch:15, Train loss:0.003317, valid loss:0.002485
Epoch:16, Train loss:0.003202, valid loss:0.001818
Epoch:17, Train loss:0.003162, valid loss:0.001863
Epoch:18, Train loss:0.003147, valid loss:0.001846
Epoch:19, Train loss:0.003113, valid loss:0.001614
Epoch:20, Train loss:0.002986, valid loss:0.001743
Epoch:21, Train loss:0.002353, valid loss:0.001412
Epoch:22, Train loss:0.002368, valid loss:0.001358
Epoch:23, Train loss:0.002313, valid loss:0.001391
Epoch:24, Train loss:0.002284, valid loss:0.001411
Epoch:25, Train loss:0.002325, valid loss:0.001237
Epoch:26, Train loss:0.002211, valid loss:0.001253
Epoch:27, Train loss:0.002236, valid loss:0.001548
Epoch:28, Train loss:0.002224, valid loss:0.001718
Epoch:29, Train loss:0.002227, valid loss:0.001352
Epoch:30, Train loss:0.002140, valid loss:0.001348
Epoch:31, Train loss:0.001854, valid loss:0.001106
Epoch:32, Train loss:0.001812, valid loss:0.001180
Epoch:33, Train loss:0.001830, valid loss:0.001248
Epoch:34, Train loss:0.001799, valid loss:0.001100
Epoch:35, Train loss:0.001813, valid loss:0.001066
Epoch:36, Train loss:0.001804, valid loss:0.001083
Epoch:37, Train loss:0.001797, valid loss:0.001244
Epoch:38, Train loss:0.001771, valid loss:0.001186
Epoch:39, Train loss:0.001778, valid loss:0.001080
Epoch:40, Train loss:0.001738, valid loss:0.001050
Epoch:41, Train loss:0.001597, valid loss:0.000993
Epoch:42, Train loss:0.001586, valid loss:0.000974
Epoch:43, Train loss:0.001588, valid loss:0.000993
Epoch:44, Train loss:0.001588, valid loss:0.001097
Epoch:45, Train loss:0.001563, valid loss:0.001042
Epoch:46, Train loss:0.001576, valid loss:0.001023
Epoch:47, Train loss:0.001582, valid loss:0.000966
Epoch:48, Train loss:0.001552, valid loss:0.001006
Epoch:49, Train loss:0.001555, valid loss:0.001059
Epoch:50, Train loss:0.001552, valid loss:0.000922
Epoch:51, Train loss:0.001470, valid loss:0.000948
Epoch:52, Train loss:0.001461, valid loss:0.000950
Epoch:53, Train loss:0.001467, valid loss:0.000940
Epoch:54, Train loss:0.001461, valid loss:0.000936
Epoch:55, Train loss:0.001463, valid loss:0.000997
Epoch:56, Train loss:0.001458, valid loss:0.000928
Epoch:57, Train loss:0.001455, valid loss:0.000962
Epoch:58, Train loss:0.001453, valid loss:0.000969
Epoch:59, Train loss:0.001451, valid loss:0.000926
Epoch:60, Train loss:0.001447, valid loss:0.000926
training time 4134.828996896744
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03304532702842104
plot_id,batch_id 0 1 miss% 0.026307558584542975
plot_id,batch_id 0 2 miss% 0.02829118275923841
plot_id,batch_id 0 3 miss% 0.031094867885145094
plot_id,batch_id 0 4 miss% 0.034227512741018956
plot_id,batch_id 0 5 miss% 0.04049044220880585
plot_id,batch_id 0 6 miss% 0.03567977542751468
plot_id,batch_id 0 7 miss% 0.027034261297786
plot_id,batch_id 0 8 miss% 0.02751624990019827
plot_id,batch_id 0 9 miss% 0.01746388913548402
plot_id,batch_id 0 10 miss% 0.05320024207662295
plot_id,batch_id 0 11 miss% 0.05222774042296587
plot_id,batch_id 0 12 miss% 0.029176327638349753
plot_id,batch_id 0 13 miss% 0.03021639093602179
plot_id,batch_id 0 14 miss% 0.03087956756459167
plot_id,batch_id 0 15 miss% 0.046386420828241876
plot_id,batch_id 0 16 miss% 0.02882818393918443
plot_id,batch_id 0 17 miss% 0.053444289930440154
plot_id,batch_id 0 18 miss% 0.019592220830714106
plot_id,batch_id 0 19 miss% 0.03903084168521543
plot_id,batch_id 0 20 miss% 0.04464235962584996
plot_id,batch_id 0 21 miss% 0.02551417633447726
plot_id,batch_id 0 22 miss% 0.029561724317023565
plot_id,batch_id 0 23 miss% 0.033222011809162
plot_id,batch_id 0 24 miss% 0.038137401501463775
plot_id,batch_id 0 25 miss% 0.050454205768998235
plot_id,batch_id 0 26 miss% 0.03463278406373819
plot_id,batch_id 0 27 miss% 0.02648723493055257
plot_id,batch_id 0 28 miss% 0.031198400534465806
plot_id,batch_id 0 29 miss% 0.03507630382702444
plot_id,batch_id 0 30 miss% 0.04792497641921354
plot_id,batch_id 0 31 miss% 0.03434944624853114
plot_id,batch_id 0 32 miss% 0.03136431346564878
plot_id,batch_id 0 33 miss% 0.03172962091288421
plot_id,batch_id 0 34 miss% 0.04233664410510388
plot_id,batch_id 0 35 miss% 0.0479349039393882
plot_id,batch_id 0 36 miss% 0.044000403277488345
plot_id,batch_id 0 37 miss% 0.036533161826204676
plot_id,batch_id 0 38 miss% 0.037035322698990604
plot_id,batch_id 0 39 miss% 0.023290129828043248
plot_id,batch_id 0 40 miss% 0.08173848581772536
plot_id,batch_id 0 41 miss% 0.034625204987151964
plot_id,batch_id 0 42 miss% 0.023636113578426085
plot_id,batch_id 0 43 miss% 0.04493020458732633
plot_id,batch_id 0 44 miss% 0.03944936448327358
plot_id,batch_id 0 45 miss% 0.029855034527786355
plot_id,batch_id 0 46 miss% 0.03250756277805184
plot_id,batch_id 0 47 miss% 0.02632363043568406
plot_id,batch_id 0 48 miss% 0.04300416344362859
plot_id,batch_id 0 49 miss% 0.029492433820664896
plot_id,batch_id 0 50 miss% 0.04358661365627886
plot_id,batch_id 0 51 miss% 0.024958224449404685
plot_id,batch_id 0 52 miss% 0.02321529555537561
plot_id,batch_id 0 53 miss% 0.025424746807649296
plot_id,batch_id 0 54 miss% 0.036468042487348064
plot_id,batch_id 0 55 miss% 0.044580236749194084
plot_id,batch_id 0 56 miss% 0.04231722885392278
plot_id,batch_id 0 57 miss% 0.030570979224788276
plot_id,batch_id 0 58 miss% 0.033045956237689966
plot_id,batch_id 0 59 miss% 0.03499018082251492
plot_id,batch_id 0 60 miss% 0.04719159125667872
plot_id,batch_id 0 61 miss% 0.027770051177612484
plot_id,batch_id 0 62 miss% 0.02911160114408451
plot_id,batch_id 0 63 miss% 0.028289646261229578
plot_id,batch_id 0 64 miss% 0.03183945305675142
plot_id,batch_id 0 65 miss% 0.053707631698318395
plot_id,batch_id 0 66 miss% 0.02998298482779347
plot_id,batch_id 0 67 miss% 0.032772096580041835
plot_id,batch_id 0 68 miss% 0.0340300706779962
plot_id,batch_id 0 69 miss% 0.022683217822909597
plot_id,batch_id 0 70 miss% 0.044800043099732435
plot_id,batch_id 0 71 miss% 0.06629896832100472
plot_id,batch_id 0 72 miss% 0.04873240347870948
plot_id,batch_id 0 73 miss% 0.038296346048879853
plot_id,batch_id 0 74 miss% 0.04146353062145149
plot_id,batch_id 0 75 miss% 0.05170047029710523
plot_id,batch_id 0 76 miss% 0.042996303375714716
plot_id,batch_id 0 77 miss% 0.044594433193508094
plot_id,batch_id 0 78 miss% 0.05085471440233865
plot_id,batch_id 0 79 miss% 0.045205721141261354
plot_id,batch_id 0 80 miss% 0.049534510281794654
plot_id,batch_id 0 81 miss% 0.03793938841370664
plot_id,batch_id 0 82 miss% 0.020468169430763367
plot_id,batch_id 0 83 miss% 0.0298597134047953
plot_id,batch_id 0 84 miss% 0.03256024970459927
plot_id,batch_id 0 85 miss% 0.03844358329833282
plot_id,batch_id 0 86 miss% 0.025643419835230982
plot_id,batch_id 0 87 miss% 0.026798794254741482
plot_id,batch_id 0 88 miss% 0.03395104404099002
plot_id,batch_id 0 89 miss% 0.033204707396829604
plot_id,batch_id 0 90 miss% 0.05663964487157535
plot_id,batch_id 0 91 miss% 0.03510310830056986
plot_id,batch_id 0 92 miss% 0.030100490340698557
plot_id,batch_id 0 93 miss% 0.031251325889480826
plot_id,batch_id 0 94 miss% 0.03533767425896321
plot_id,batch_id 0 95 miss% 0.04574236924823692
plot_id,batch_id 0 96 miss% 0.042923494859822386
plot_id,batch_id 0 97 miss% 0.06775493112991707
plot_id,batch_id 0 98 miss% 0.03034600602143421
plot_id,batch_id 0 99 miss% 0.031050524026404472
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03304533 0.02630756 0.02829118 0.03109487 0.03422751 0.04049044
 0.03567978 0.02703426 0.02751625 0.01746389 0.05320024 0.05222774
 0.02917633 0.03021639 0.03087957 0.04638642 0.02882818 0.05344429
 0.01959222 0.03903084 0.04464236 0.02551418 0.02956172 0.03322201
 0.0381374  0.05045421 0.03463278 0.02648723 0.0311984  0.0350763
 0.04792498 0.03434945 0.03136431 0.03172962 0.04233664 0.0479349
 0.0440004  0.03653316 0.03703532 0.02329013 0.08173849 0.0346252
 0.02363611 0.0449302  0.03944936 0.02985503 0.03250756 0.02632363
 0.04300416 0.02949243 0.04358661 0.02495822 0.0232153  0.02542475
 0.03646804 0.04458024 0.04231723 0.03057098 0.03304596 0.03499018
 0.04719159 0.02777005 0.0291116  0.02828965 0.03183945 0.05370763
 0.02998298 0.0327721  0.03403007 0.02268322 0.04480004 0.06629897
 0.0487324  0.03829635 0.04146353 0.05170047 0.0429963  0.04459443
 0.05085471 0.04520572 0.04953451 0.03793939 0.02046817 0.02985971
 0.03256025 0.03844358 0.02564342 0.02679879 0.03395104 0.03320471
 0.05663964 0.03510311 0.03010049 0.03125133 0.03533767 0.04574237
 0.04292349 0.06775493 0.03034601 0.03105052]
for model  36 the mean error 0.03681252949050651
all id 36 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 21 out win 3 err 0.03681252949050651
Launcher: Job 37 completed in 4341 seconds.
Launcher: Task 3 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  21969
Epoch:0, Train loss:0.698731, valid loss:0.666920
Epoch:1, Train loss:0.516007, valid loss:0.520900
Epoch:2, Train loss:0.502531, valid loss:0.519941
Epoch:3, Train loss:0.499826, valid loss:0.519112
Epoch:4, Train loss:0.498258, valid loss:0.518109
Epoch:5, Train loss:0.497283, valid loss:0.518149
Epoch:6, Train loss:0.496740, valid loss:0.517588
Epoch:7, Train loss:0.496451, valid loss:0.517288
Epoch:8, Train loss:0.495948, valid loss:0.517381
Epoch:9, Train loss:0.495957, valid loss:0.516915
Epoch:10, Train loss:0.495574, valid loss:0.517399
Epoch:11, Train loss:0.494173, valid loss:0.516785
Epoch:12, Train loss:0.494243, valid loss:0.516828
Epoch:13, Train loss:0.494127, valid loss:0.516393
Epoch:14, Train loss:0.494082, valid loss:0.516340
Epoch:15, Train loss:0.494051, valid loss:0.516426
Epoch:16, Train loss:0.493901, valid loss:0.516308
Epoch:17, Train loss:0.493814, valid loss:0.516495
Epoch:18, Train loss:0.493919, valid loss:0.516255
Epoch:19, Train loss:0.493749, valid loss:0.516533
Epoch:20, Train loss:0.493809, valid loss:0.516408
Epoch:21, Train loss:0.493101, valid loss:0.516307
Epoch:22, Train loss:0.493029, valid loss:0.515854
Epoch:23, Train loss:0.493027, valid loss:0.516046
Epoch:24, Train loss:0.493063, valid loss:0.516100
Epoch:25, Train loss:0.493048, valid loss:0.516107
Epoch:26, Train loss:0.493018, valid loss:0.515966
Epoch:27, Train loss:0.493072, valid loss:0.516009
Epoch:28, Train loss:0.492984, valid loss:0.516148
Epoch:29, Train loss:0.492967, valid loss:0.515819
Epoch:30, Train loss:0.492923, valid loss:0.515909
Epoch:31, Train loss:0.492617, valid loss:0.515829
Epoch:32, Train loss:0.492616, valid loss:0.515835
Epoch:33, Train loss:0.492614, valid loss:0.515776
Epoch:34, Train loss:0.492581, valid loss:0.515782
Epoch:35, Train loss:0.492597, valid loss:0.515827
Epoch:36, Train loss:0.492624, valid loss:0.515833
Epoch:37, Train loss:0.492546, valid loss:0.515782
Epoch:38, Train loss:0.492581, valid loss:0.515830
Epoch:39, Train loss:0.492558, valid loss:0.515861
Epoch:40, Train loss:0.492530, valid loss:0.515868
Epoch:41, Train loss:0.492389, valid loss:0.515813
Epoch:42, Train loss:0.492365, valid loss:0.515740
Epoch:43, Train loss:0.492405, valid loss:0.515827
Epoch:44, Train loss:0.492384, valid loss:0.515775
Epoch:45, Train loss:0.492389, valid loss:0.515827
Epoch:46, Train loss:0.492365, valid loss:0.515676
Epoch:47, Train loss:0.492342, valid loss:0.515735
Epoch:48, Train loss:0.492354, valid loss:0.515738
Epoch:49, Train loss:0.492353, valid loss:0.515712
Epoch:50, Train loss:0.492344, valid loss:0.515729
Epoch:51, Train loss:0.492261, valid loss:0.515698
Epoch:52, Train loss:0.492260, valid loss:0.515669
Epoch:53, Train loss:0.492257, valid loss:0.515695
Epoch:54, Train loss:0.492252, valid loss:0.515696
Epoch:55, Train loss:0.492261, valid loss:0.515706
Epoch:56, Train loss:0.492247, valid loss:0.515674
Epoch:57, Train loss:0.492250, valid loss:0.515698
Epoch:58, Train loss:0.492245, valid loss:0.515669
Epoch:59, Train loss:0.492249, valid loss:0.515676
Epoch:60, Train loss:0.492235, valid loss:0.515697
training time 4191.268636226654
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.8235052709775122
plot_id,batch_id 0 1 miss% 0.8575266914484884
plot_id,batch_id 0 2 miss% 0.8621348471671934
plot_id,batch_id 0 3 miss% 0.8653254175610926
plot_id,batch_id 0 4 miss% 0.8676754445421828
plot_id,batch_id 0 5 miss% 0.8163694723338625
plot_id,batch_id 0 6 miss% 0.8530511033813611
plot_id,batch_id 0 7 miss% 0.8623061613047265
plot_id,batch_id 0 8 miss% 0.8644471398405467
plot_id,batch_id 0 9 miss% 0.8659933071041499
plot_id,batch_id 0 10 miss% 0.8057339954887127
plot_id,batch_id 0 11 miss% 0.8548322714936225
plot_id,batch_id 0 12 miss% 0.8632187705105784
plot_id,batch_id 0 13 miss% 0.863091413804632
plot_id,batch_id 0 14 miss% 0.867375176307866
plot_id,batch_id 0 15 miss% 0.8116490526211056
plot_id,batch_id 0 16 miss% 0.8498533962493122
plot_id,batch_id 0 17 miss% 0.8610376532144233
plot_id,batch_id 0 18 miss% 0.8631057526768238
plot_id,batch_id 0 19 miss% 0.8654674111106605
plot_id,batch_id 0 20 miss% 0.8384561779342359
plot_id,batch_id 0 21 miss% 0.8632882340479301
plot_id,batch_id 0 22 miss% 0.8657268086104876
plot_id,batch_id 0 23 miss% 0.8677232010659831
plot_id,batch_id 0 24 miss% 0.868562904374348
plot_id,batch_id 0 25 miss% 0.8402705382829185
plot_id,batch_id 0 26 miss% 0.8572213331740525
plot_id,batch_id 0 27 miss% 0.8649956092266444
plot_id,batch_id 0 28 miss% 0.8669508796802589
plot_id,batch_id 0 29 miss% 0.8686682127633146
plot_id,batch_id 0 30 miss% 0.8257257540775703
plot_id,batch_id 0 31 miss% 0.8596156864467375
plot_id,batch_id 0 32 miss% 0.8633394480979858
plot_id,batch_id 0 33 miss% 0.8639568602018798
plot_id,batch_id 0 34 miss% 0.8665930525042291
plot_id,batch_id 0 35 miss% 0.8255821290657763
plot_id,batch_id 0 36 miss% 0.8591589183454639
plot_id,batch_id 0 37 miss% 0.8635083320418815
plot_id,batch_id 0 38 miss% 0.8672262629085005
plot_id,batch_id 0 39 miss% 0.865226028654108
plot_id,batch_id 0 40 miss% 0.8511354314979932
plot_id,batch_id 0 41 miss% 0.8635900924154678
plot_id,batch_id 0 42 miss% 0.865617554353595
plot_id,batch_id 0 43 miss% 0.8684758684669491
plot_id,batch_id 0 44 miss% 0.8695880544462873
plot_id,batch_id 0 45 miss% 0.8481298679755965
plot_id,batch_id 0 46 miss% 0.8614096093910035
plot_id,batch_id 0 47 miss% 0.8658708084911652
plot_id,batch_id 0 48 miss% 0.8702881070292511
plot_id,batch_id 0 49 miss% 0.8698308860640884
plot_id,batch_id 0 50 miss% 0.8550481387564902
plot_id,batch_id 0 51 miss% 0.8627136457816045
plot_id,batch_id 0 52 miss% 0.8655485179389869
plot_id,batch_id 0 53 miss% 0.8710315846330321
plot_id,batch_id 0 54 miss% 0.8709859722336346
plot_id,batch_id 0 55 miss% 0.8494605251758437
plot_id,batch_id 0 56 miss% 0.8656394132855733
plot_id,batch_id 0 57 miss% 0.8671617581174733
plot_id,batch_id 0 58 miss% 0.8695309589209168
plot_id,batch_id 0 59 miss% 0.8684270199899928
plot_id,batch_id 0 60 miss% 0.772637823576354
plot_id,batch_id 0 61 miss% 0.8407600639907168
plot_id,batch_id 0 62 miss% 0.8539674837724399
plot_id,batch_id 0 63 miss% 0.8592018002643569
plot_id,batch_id 0 64 miss% 0.8624979365617272
plot_id,batch_id 0 65 miss% 0.771013053879429
plot_id,batch_id 0 66 miss% 0.8336945633000367
plot_id,batch_id 0 67 miss% 0.843726601338621
plot_id,batch_id 0 68 miss% 0.8582276841654098
plot_id,batch_id 0 69 miss% 0.8595420128048207
plot_id,batch_id 0 70 miss% 0.7222097698415137
plot_id,batch_id 0 71 miss% 0.8359713217457628
plot_id,batch_id 0 72 miss% 0.8387295486213661
plot_id,batch_id 0 73 miss% 0.8484479340924898
plot_id,batch_id 0 74 miss% 0.855062898295494
plot_id,batch_id 0 75 miss% 0.7689130562130065
plot_id,batch_id 0 76 miss% 0.817212442670998
plot_id,batch_id 0 77 miss% 0.8340771604248454
plot_id,batch_id 0 78 miss% 0.840280366263087
plot_id,batch_id 0 79 miss% 0.8523860653216301
plot_id,batch_id 0 80 miss% 0.7906222515795807
plot_id,batch_id 0 81 miss% 0.8496296524831874
plot_id,batch_id 0 82 miss% 0.8597051427574625
plot_id,batch_id 0 83 miss% 0.8603642317792349
plot_id,batch_id 0 84 miss% 0.8636950233438068
plot_id,batch_id 0 85 miss% 0.777215746991416
plot_id,batch_id 0 86 miss% 0.8409844394353694
plot_id,batch_id 0 87 miss% 0.8542965893820762
plot_id,batch_id 0 88 miss% 0.8609913801162601
plot_id,batch_id 0 89 miss% 0.8644903566718808
plot_id,batch_id 0 90 miss% 0.7602210635080526
plot_id,batch_id 0 91 miss% 0.8353680235577435
plot_id,batch_id 0 92 miss% 0.8495911796683241
plot_id,batch_id 0 93 miss% 0.8518489928253024
plot_id,batch_id 0 94 miss% 0.861573282258028
plot_id,batch_id 0 95 miss% 0.7697506870462251
plot_id,batch_id 0 96 miss% 0.8265761233242289
plot_id,batch_id 0 97 miss% 0.8464357488863808
plot_id,batch_id 0 98 miss% 0.8524192666964109
plot_id,batch_id 0 99 miss% 0.8579553168477165
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.82350527 0.85752669 0.86213485 0.86532542 0.86767544 0.81636947
 0.8530511  0.86230616 0.86444714 0.86599331 0.805734   0.85483227
 0.86321877 0.86309141 0.86737518 0.81164905 0.8498534  0.86103765
 0.86310575 0.86546741 0.83845618 0.86328823 0.86572681 0.8677232
 0.8685629  0.84027054 0.85722133 0.86499561 0.86695088 0.86866821
 0.82572575 0.85961569 0.86333945 0.86395686 0.86659305 0.82558213
 0.85915892 0.86350833 0.86722626 0.86522603 0.85113543 0.86359009
 0.86561755 0.86847587 0.86958805 0.84812987 0.86140961 0.86587081
 0.87028811 0.86983089 0.85504814 0.86271365 0.86554852 0.87103158
 0.87098597 0.84946053 0.86563941 0.86716176 0.86953096 0.86842702
 0.77263782 0.84076006 0.85396748 0.8592018  0.86249794 0.77101305
 0.83369456 0.8437266  0.85822768 0.85954201 0.72220977 0.83597132
 0.83872955 0.84844793 0.8550629  0.76891306 0.81721244 0.83407716
 0.84028037 0.85238607 0.79062225 0.84962965 0.85970514 0.86036423
 0.86369502 0.77721575 0.84098444 0.85429659 0.86099138 0.86449036
 0.76022106 0.83536802 0.84959118 0.85184899 0.86157328 0.76975069
 0.82657612 0.84643575 0.85241927 0.85795532]
for model  29 the mean error 0.8478727402191091
all id 29 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 21 out win 5 err 0.8478727402191091
Launcher: Job 30 completed in 4374 seconds.
Launcher: Task 105 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  46193
Epoch:0, Train loss:0.728772, valid loss:0.688975
Epoch:1, Train loss:0.506602, valid loss:0.516674
Epoch:2, Train loss:0.493201, valid loss:0.515655
Epoch:3, Train loss:0.491109, valid loss:0.515819
Epoch:4, Train loss:0.489657, valid loss:0.514442
Epoch:5, Train loss:0.488863, valid loss:0.513886
Epoch:6, Train loss:0.488397, valid loss:0.513570
Epoch:7, Train loss:0.488170, valid loss:0.513216
Epoch:8, Train loss:0.487814, valid loss:0.513497
Epoch:9, Train loss:0.487571, valid loss:0.513519
Epoch:10, Train loss:0.487400, valid loss:0.512856
Epoch:11, Train loss:0.486369, valid loss:0.512678
Epoch:12, Train loss:0.486335, valid loss:0.512733
Epoch:13, Train loss:0.486362, valid loss:0.512479
Epoch:14, Train loss:0.486235, valid loss:0.512507
Epoch:15, Train loss:0.486207, valid loss:0.512794
Epoch:16, Train loss:0.486185, valid loss:0.512633
Epoch:17, Train loss:0.486151, valid loss:0.512396
Epoch:18, Train loss:0.486146, valid loss:0.512409
Epoch:19, Train loss:0.486040, valid loss:0.512660
Epoch:20, Train loss:0.486023, valid loss:0.512258
Epoch:21, Train loss:0.485594, valid loss:0.512175
Epoch:22, Train loss:0.485531, valid loss:0.512213
Epoch:23, Train loss:0.485554, valid loss:0.512225
Epoch:24, Train loss:0.485596, valid loss:0.512147
Epoch:25, Train loss:0.485496, valid loss:0.512303
Epoch:26, Train loss:0.485513, valid loss:0.512228
Epoch:27, Train loss:0.485501, valid loss:0.512422
Epoch:28, Train loss:0.485498, valid loss:0.512132
Epoch:29, Train loss:0.485497, valid loss:0.512167
Epoch:30, Train loss:0.485511, valid loss:0.512198
Epoch:31, Train loss:0.485255, valid loss:0.512066
Epoch:32, Train loss:0.485226, valid loss:0.512026
Epoch:33, Train loss:0.485234, valid loss:0.512026
Epoch:34, Train loss:0.485241, valid loss:0.512114
Epoch:35, Train loss:0.485217, valid loss:0.512091
Epoch:36, Train loss:0.485204, valid loss:0.512043
Epoch:37, Train loss:0.485210, valid loss:0.512114
Epoch:38, Train loss:0.485216, valid loss:0.512048
Epoch:39, Train loss:0.485217, valid loss:0.512032
Epoch:40, Train loss:0.485198, valid loss:0.512021
Epoch:41, Train loss:0.485092, valid loss:0.511968
Epoch:42, Train loss:0.485084, valid loss:0.512030
Epoch:43, Train loss:0.485086, valid loss:0.512044
Epoch:44, Train loss:0.485083, valid loss:0.512032
Epoch:45, Train loss:0.485079, valid loss:0.512000
Epoch:46, Train loss:0.485084, valid loss:0.512003
Epoch:47, Train loss:0.485071, valid loss:0.512004
Epoch:48, Train loss:0.485071, valid loss:0.512020
Epoch:49, Train loss:0.485063, valid loss:0.511987
Epoch:50, Train loss:0.485073, valid loss:0.511981
Epoch:51, Train loss:0.485015, valid loss:0.511943
Epoch:52, Train loss:0.485013, valid loss:0.511985
Epoch:53, Train loss:0.485014, valid loss:0.511967
Epoch:54, Train loss:0.485009, valid loss:0.511983
Epoch:55, Train loss:0.485009, valid loss:0.512005
Epoch:56, Train loss:0.485006, valid loss:0.511997
Epoch:57, Train loss:0.485010, valid loss:0.511968
Epoch:58, Train loss:0.485003, valid loss:0.511973
Epoch:59, Train loss:0.485005, valid loss:0.511972
Epoch:60, Train loss:0.485002, valid loss:0.511984
training time 4246.977157592773
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.6954216654700108
plot_id,batch_id 0 1 miss% 0.7742190131247693
plot_id,batch_id 0 2 miss% 0.7786967868236935
plot_id,batch_id 0 3 miss% 0.7878988887317111
plot_id,batch_id 0 4 miss% 0.7919002873397183
plot_id,batch_id 0 5 miss% 0.6900201288669723
plot_id,batch_id 0 6 miss% 0.7664104781392979
plot_id,batch_id 0 7 miss% 0.7814077230556092
plot_id,batch_id 0 8 miss% 0.7893631573277347
plot_id,batch_id 0 9 miss% 0.7948580684673979
plot_id,batch_id 0 10 miss% 0.6740071448724198
plot_id,batch_id 0 11 miss% 0.7699435034212333
plot_id,batch_id 0 12 miss% 0.7790410754998897
plot_id,batch_id 0 13 miss% 0.7903102671254808
plot_id,batch_id 0 14 miss% 0.7899954806933634
plot_id,batch_id 0 15 miss% 0.685314854852463
plot_id,batch_id 0 16 miss% 0.7564589614281731
plot_id,batch_id 0 17 miss% 0.7830587199379664
plot_id,batch_id 0 18 miss% 0.7854894490644394
plot_id,batch_id 0 19 miss% 0.7925932031980362
plot_id,batch_id 0 20 miss% 0.7409596386321182
plot_id,batch_id 0 21 miss% 0.7814696500911975
plot_id,batch_id 0 22 miss% 0.7887898558538189
plot_id,batch_id 0 23 miss% 0.7943523343477755
plot_id,batch_id 0 24 miss% 0.7934872280367045
plot_id,batch_id 0 25 miss% 0.7295617842560563
plot_id,batch_id 0 26 miss% 0.7823781313897249
plot_id,batch_id 0 27 miss% 0.7864901398569436
plot_id,batch_id 0 28 miss% 0.7933013590393075
plot_id,batch_id 0 29 miss% 0.7946118121085696
plot_id,batch_id 0 30 miss% 0.7299290309629685
plot_id,batch_id 0 31 miss% 0.775083073818818
plot_id,batch_id 0 32 miss% 0.7877132504443004
plot_id,batch_id 0 33 miss% 0.7927154925555967
plot_id,batch_id 0 34 miss% 0.7939340023198
plot_id,batch_id 0 35 miss% 0.7313733816586422
plot_id,batch_id 0 36 miss% 0.7824736799986048
plot_id,batch_id 0 37 miss% 0.782748773270592
plot_id,batch_id 0 38 miss% 0.7951098264810736
plot_id,batch_id 0 39 miss% 0.7964664990169262
plot_id,batch_id 0 40 miss% 0.7673858981790961
plot_id,batch_id 0 41 miss% 0.7878188135617104
plot_id,batch_id 0 42 miss% 0.7920836252767797
plot_id,batch_id 0 43 miss% 0.7977501690462369
plot_id,batch_id 0 44 miss% 0.7976787540361965
plot_id,batch_id 0 45 miss% 0.7581592587190489
plot_id,batch_id 0 46 miss% 0.78553260821827
plot_id,batch_id 0 47 miss% 0.7920194399517458
plot_id,batch_id 0 48 miss% 0.7999931708862501
plot_id,batch_id 0 49 miss% 0.7980684176253978
plot_id,batch_id 0 50 miss% 0.7623676538677671
plot_id,batch_id 0 51 miss% 0.7860832969821894
plot_id,batch_id 0 52 miss% 0.7913261538927421
plot_id,batch_id 0 53 miss% 0.7982693895188533
plot_id,batch_id 0 54 miss% 0.7993391488386105
plot_id,batch_id 0 55 miss% 0.7530842008125352
plot_id,batch_id 0 56 miss% 0.786606897926228
plot_id,batch_id 0 57 miss% 0.7936760754479519
plot_id,batch_id 0 58 miss% 0.794865608512889
plot_id,batch_id 0 59 miss% 0.7969292695382769
plot_id,batch_id 0 60 miss% 0.610912136498649
plot_id,batch_id 0 61 miss% 0.7364363878822544
plot_id,batch_id 0 62 miss% 0.7588238442586975
plot_id,batch_id 0 63 miss% 0.7765608737307582
plot_id,batch_id 0 64 miss% 0.7830589734345725
plot_id,batch_id 0 65 miss% 0.6101844629576599
plot_id,batch_id 0 66 miss% 0.72791078058463
plot_id,batch_id 0 67 miss% 0.743784942375591
plot_id,batch_id 0 68 miss% 0.7737469476567945
plot_id,batch_id 0 69 miss% 0.7803828617086542
plot_id,batch_id 0 70 miss% 0.5734659501730761
plot_id,batch_id 0 71 miss% 0.7345877266449077
plot_id,batch_id 0 72 miss% 0.7425120714363318
plot_id,batch_id 0 73 miss% 0.7606034249992858
plot_id,batch_id 0 74 miss% 0.7751831260878246
plot_id,batch_id 0 75 miss% 0.5649006931200157
plot_id,batch_id 0 76 miss% 0.686397418080017
plot_id,batch_id 0 77 miss% 0.727836504406285
plot_id,batch_id 0 78 miss% 0.7552789398891214
plot_id,batch_id 0 79 miss% 0.7645189777887622
plot_id,batch_id 0 80 miss% 0.6374089293918233
plot_id,batch_id 0 81 miss% 0.7516980607583799
plot_id,batch_id 0 82 miss% 0.7736026124329151
plot_id,batch_id 0 83 miss% 0.7823593928721208
plot_id,batch_id 0 84 miss% 0.7843459746687891
plot_id,batch_id 0 85 miss% 0.636590390087639
plot_id,batch_id 0 86 miss% 0.745860957439537
plot_id,batch_id 0 87 miss% 0.7680889322331301
plot_id,batch_id 0 88 miss% 0.7826668266618418
plot_id,batch_id 0 89 miss% 0.7855711539152135
plot_id,batch_id 0 90 miss% 0.5976725424529338
plot_id,batch_id 0 91 miss% 0.7429424607417687
plot_id,batch_id 0 92 miss% 0.7605984303297418
plot_id,batch_id 0 93 miss% 0.7713064614247774
plot_id,batch_id 0 94 miss% 0.7799461572995626
plot_id,batch_id 0 95 miss% 0.605905245047053
plot_id,batch_id 0 96 miss% 0.7235335732208985
plot_id,batch_id 0 97 miss% 0.7510542082178568
plot_id,batch_id 0 98 miss% 0.7681625462474015
plot_id,batch_id 0 99 miss% 0.7752573771445384
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.69542167 0.77421901 0.77869679 0.78789889 0.79190029 0.69002013
 0.76641048 0.78140772 0.78936316 0.79485807 0.67400714 0.7699435
 0.77904108 0.79031027 0.78999548 0.68531485 0.75645896 0.78305872
 0.78548945 0.7925932  0.74095964 0.78146965 0.78878986 0.79435233
 0.79348723 0.72956178 0.78237813 0.78649014 0.79330136 0.79461181
 0.72992903 0.77508307 0.78771325 0.79271549 0.793934   0.73137338
 0.78247368 0.78274877 0.79510983 0.7964665  0.7673859  0.78781881
 0.79208363 0.79775017 0.79767875 0.75815926 0.78553261 0.79201944
 0.79999317 0.79806842 0.76236765 0.7860833  0.79132615 0.79826939
 0.79933915 0.7530842  0.7866069  0.79367608 0.79486561 0.79692927
 0.61091214 0.73643639 0.75882384 0.77656087 0.78305897 0.61018446
 0.72791078 0.74378494 0.77374695 0.78038286 0.57346595 0.73458773
 0.74251207 0.76060342 0.77518313 0.56490069 0.68639742 0.7278365
 0.75527894 0.76451898 0.63740893 0.75169806 0.77360261 0.78235939
 0.78434597 0.63659039 0.74586096 0.76808893 0.78266683 0.78557115
 0.59767254 0.74294246 0.76059843 0.77130646 0.77994616 0.60590525
 0.72353357 0.75105421 0.76816255 0.77525738]
for model  30 the mean error 0.7566005492872051
all id 30 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 21 out win 3 err 0.7566005492872051
Launcher: Job 31 completed in 4429 seconds.
Launcher: Task 88 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  46193
Epoch:0, Train loss:0.728772, valid loss:0.688975
Epoch:1, Train loss:0.508613, valid loss:0.518277
Epoch:2, Train loss:0.493785, valid loss:0.515082
Epoch:3, Train loss:0.491318, valid loss:0.514860
Epoch:4, Train loss:0.489936, valid loss:0.514274
Epoch:5, Train loss:0.488981, valid loss:0.513926
Epoch:6, Train loss:0.488400, valid loss:0.513611
Epoch:7, Train loss:0.488101, valid loss:0.513480
Epoch:8, Train loss:0.487715, valid loss:0.513434
Epoch:9, Train loss:0.487490, valid loss:0.514012
Epoch:10, Train loss:0.487279, valid loss:0.513301
Epoch:11, Train loss:0.486426, valid loss:0.512644
Epoch:12, Train loss:0.486347, valid loss:0.512663
Epoch:13, Train loss:0.486322, valid loss:0.512747
Epoch:14, Train loss:0.486255, valid loss:0.512666
Epoch:15, Train loss:0.486243, valid loss:0.512507
Epoch:16, Train loss:0.486144, valid loss:0.512416
Epoch:17, Train loss:0.486136, valid loss:0.512560
Epoch:18, Train loss:0.486045, valid loss:0.512422
Epoch:19, Train loss:0.486005, valid loss:0.512688
Epoch:20, Train loss:0.485966, valid loss:0.512410
Epoch:21, Train loss:0.485618, valid loss:0.512200
Epoch:22, Train loss:0.485587, valid loss:0.512186
Epoch:23, Train loss:0.485592, valid loss:0.512174
Epoch:24, Train loss:0.485575, valid loss:0.512273
Epoch:25, Train loss:0.485558, valid loss:0.512267
Epoch:26, Train loss:0.485541, valid loss:0.512164
Epoch:27, Train loss:0.485520, valid loss:0.512178
Epoch:28, Train loss:0.485480, valid loss:0.512205
Epoch:29, Train loss:0.485496, valid loss:0.512254
Epoch:30, Train loss:0.485475, valid loss:0.512239
Epoch:31, Train loss:0.485313, valid loss:0.512110
Epoch:32, Train loss:0.485295, valid loss:0.512115
Epoch:33, Train loss:0.485301, valid loss:0.512066
Epoch:34, Train loss:0.485288, valid loss:0.512119
Epoch:35, Train loss:0.485275, valid loss:0.512112
Epoch:36, Train loss:0.485272, valid loss:0.512097
Epoch:37, Train loss:0.485259, valid loss:0.512106
Epoch:38, Train loss:0.485258, valid loss:0.512111
Epoch:39, Train loss:0.485259, valid loss:0.512093
Epoch:40, Train loss:0.485255, valid loss:0.512063
Epoch:41, Train loss:0.485167, valid loss:0.512018
Epoch:42, Train loss:0.485161, valid loss:0.512025
Epoch:43, Train loss:0.485165, valid loss:0.512020
Epoch:44, Train loss:0.485155, valid loss:0.512057
Epoch:45, Train loss:0.485158, valid loss:0.512055
Epoch:46, Train loss:0.485151, valid loss:0.512032
Epoch:47, Train loss:0.485149, valid loss:0.512038
Epoch:48, Train loss:0.485147, valid loss:0.512025
Epoch:49, Train loss:0.485141, valid loss:0.512022
Epoch:50, Train loss:0.485140, valid loss:0.512042
Epoch:51, Train loss:0.485102, valid loss:0.512014
Epoch:52, Train loss:0.485099, valid loss:0.512010
Epoch:53, Train loss:0.485098, valid loss:0.512010
Epoch:54, Train loss:0.485099, valid loss:0.512000
Epoch:55, Train loss:0.485097, valid loss:0.512024
Epoch:56, Train loss:0.485092, valid loss:0.512010
Epoch:57, Train loss:0.485092, valid loss:0.511996
Epoch:58, Train loss:0.485090, valid loss:0.512002
Epoch:59, Train loss:0.485088, valid loss:0.512000
Epoch:60, Train loss:0.485086, valid loss:0.512006
training time 4334.401542663574
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.6994865952711122
plot_id,batch_id 0 1 miss% 0.7725746434254848
plot_id,batch_id 0 2 miss% 0.7810315655617626
plot_id,batch_id 0 3 miss% 0.7881695557744951
plot_id,batch_id 0 4 miss% 0.7919303419442059
plot_id,batch_id 0 5 miss% 0.695840956045198
plot_id,batch_id 0 6 miss% 0.7677933544102665
plot_id,batch_id 0 7 miss% 0.7814482931160683
plot_id,batch_id 0 8 miss% 0.7905595818905391
plot_id,batch_id 0 9 miss% 0.7962452694662109
plot_id,batch_id 0 10 miss% 0.6670059073961362
plot_id,batch_id 0 11 miss% 0.7683867262462768
plot_id,batch_id 0 12 miss% 0.7784997039219126
plot_id,batch_id 0 13 miss% 0.7865780271293407
plot_id,batch_id 0 14 miss% 0.7909226255495073
plot_id,batch_id 0 15 miss% 0.6861923821450032
plot_id,batch_id 0 16 miss% 0.7581731287901768
plot_id,batch_id 0 17 miss% 0.7818722948758284
plot_id,batch_id 0 18 miss% 0.7843230428501273
plot_id,batch_id 0 19 miss% 0.7916388766719946
plot_id,batch_id 0 20 miss% 0.7470522723146991
plot_id,batch_id 0 21 miss% 0.7801901192620563
plot_id,batch_id 0 22 miss% 0.7907017406188285
plot_id,batch_id 0 23 miss% 0.7943381779783578
plot_id,batch_id 0 24 miss% 0.7944047803180696
plot_id,batch_id 0 25 miss% 0.7299027848464602
plot_id,batch_id 0 26 miss% 0.7801644408392957
plot_id,batch_id 0 27 miss% 0.7867141962244384
plot_id,batch_id 0 28 miss% 0.792627541189212
plot_id,batch_id 0 29 miss% 0.7945778573951922
plot_id,batch_id 0 30 miss% 0.7307328274208286
plot_id,batch_id 0 31 miss% 0.7752437872893289
plot_id,batch_id 0 32 miss% 0.7891547987416232
plot_id,batch_id 0 33 miss% 0.792024764054554
plot_id,batch_id 0 34 miss% 0.7923888847368499
plot_id,batch_id 0 35 miss% 0.7212855755609077
plot_id,batch_id 0 36 miss% 0.7862589180664193
plot_id,batch_id 0 37 miss% 0.7806833958364738
plot_id,batch_id 0 38 miss% 0.7945525507988832
plot_id,batch_id 0 39 miss% 0.7935484106428862
plot_id,batch_id 0 40 miss% 0.7682664522405627
plot_id,batch_id 0 41 miss% 0.7875214642198011
plot_id,batch_id 0 42 miss% 0.7910031715367908
plot_id,batch_id 0 43 miss% 0.7939383639066668
plot_id,batch_id 0 44 miss% 0.7968148339959097
plot_id,batch_id 0 45 miss% 0.762023342157898
plot_id,batch_id 0 46 miss% 0.7860765588870413
plot_id,batch_id 0 47 miss% 0.7932976868923399
plot_id,batch_id 0 48 miss% 0.8008585943590371
plot_id,batch_id 0 49 miss% 0.7969344642791553
plot_id,batch_id 0 50 miss% 0.7609687824459233
plot_id,batch_id 0 51 miss% 0.7879819253497194
plot_id,batch_id 0 52 miss% 0.7921972180855296
plot_id,batch_id 0 53 miss% 0.7976108930626294
plot_id,batch_id 0 54 miss% 0.7998018129313675
plot_id,batch_id 0 55 miss% 0.7549932332675142
plot_id,batch_id 0 56 miss% 0.7853670904095704
plot_id,batch_id 0 57 miss% 0.7930763166442966
plot_id,batch_id 0 58 miss% 0.7930090150555797
plot_id,batch_id 0 59 miss% 0.7981037835872914
plot_id,batch_id 0 60 miss% 0.6163161721509307
plot_id,batch_id 0 61 miss% 0.7369314854629823
plot_id,batch_id 0 62 miss% 0.7611807636791854
plot_id,batch_id 0 63 miss% 0.781198338801083
plot_id,batch_id 0 64 miss% 0.782073400275191
plot_id,batch_id 0 65 miss% 0.6054531305593917
plot_id,batch_id 0 66 miss% 0.7293422317777292
plot_id,batch_id 0 67 miss% 0.7470773124297554
plot_id,batch_id 0 68 miss% 0.7745501053883564
plot_id,batch_id 0 69 miss% 0.7768626192789854
plot_id,batch_id 0 70 miss% 0.5774413841324458
plot_id,batch_id 0 71 miss% 0.7420800647729765
plot_id,batch_id 0 72 miss% 0.7431902404653483
plot_id,batch_id 0 73 miss% 0.7632973768511827
plot_id,batch_id 0 74 miss% 0.7771536716868414
plot_id,batch_id 0 75 miss% 0.570700089483257
plot_id,batch_id 0 76 miss% 0.6894763704797585
plot_id,batch_id 0 77 miss% 0.7317484885810501
plot_id,batch_id 0 78 miss% 0.7592022420219733
plot_id,batch_id 0 79 miss% 0.7649162409044632
plot_id,batch_id 0 80 miss% 0.6379260529026727
plot_id,batch_id 0 81 miss% 0.7534464241617669
plot_id,batch_id 0 82 miss% 0.7734431349642855
plot_id,batch_id 0 83 miss% 0.7819231864330176
plot_id,batch_id 0 84 miss% 0.7831927738659592
plot_id,batch_id 0 85 miss% 0.632273279338471
plot_id,batch_id 0 86 miss% 0.7461248257721533
plot_id,batch_id 0 87 miss% 0.7692099093549647
plot_id,batch_id 0 88 miss% 0.7793268109990169
plot_id,batch_id 0 89 miss% 0.7830800054072453
plot_id,batch_id 0 90 miss% 0.6035093996090339
plot_id,batch_id 0 91 miss% 0.7497604406218393
plot_id,batch_id 0 92 miss% 0.7615104716111587
plot_id,batch_id 0 93 miss% 0.7782297006385388
plot_id,batch_id 0 94 miss% 0.7822276042931023
plot_id,batch_id 0 95 miss% 0.6102610181037803
plot_id,batch_id 0 96 miss% 0.724201288997466
plot_id,batch_id 0 97 miss% 0.7551910744654683
plot_id,batch_id 0 98 miss% 0.7687651554335432
plot_id,batch_id 0 99 miss% 0.7733255223491866
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.6994866  0.77257464 0.78103157 0.78816956 0.79193034 0.69584096
 0.76779335 0.78144829 0.79055958 0.79624527 0.66700591 0.76838673
 0.7784997  0.78657803 0.79092263 0.68619238 0.75817313 0.78187229
 0.78432304 0.79163888 0.74705227 0.78019012 0.79070174 0.79433818
 0.79440478 0.72990278 0.78016444 0.7867142  0.79262754 0.79457786
 0.73073283 0.77524379 0.7891548  0.79202476 0.79238888 0.72128558
 0.78625892 0.7806834  0.79455255 0.79354841 0.76826645 0.78752146
 0.79100317 0.79393836 0.79681483 0.76202334 0.78607656 0.79329769
 0.80085859 0.79693446 0.76096878 0.78798193 0.79219722 0.79761089
 0.79980181 0.75499323 0.78536709 0.79307632 0.79300902 0.79810378
 0.61631617 0.73693149 0.76118076 0.78119834 0.7820734  0.60545313
 0.72934223 0.74707731 0.77455011 0.77686262 0.57744138 0.74208006
 0.74319024 0.76329738 0.77715367 0.57070009 0.68947637 0.73174849
 0.75920224 0.76491624 0.63792605 0.75344642 0.77344313 0.78192319
 0.78319277 0.63227328 0.74612483 0.76920991 0.77932681 0.78308001
 0.6035094  0.74976044 0.76151047 0.7782297  0.7822276  0.61026102
 0.72420129 0.75519107 0.76876516 0.77332552]
for model  3 the mean error 0.7572218550843719
all id 3 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 21 out win 3 err 0.7572218550843719
Launcher: Job 4 completed in 4517 seconds.
Launcher: Task 4 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  21969
Epoch:0, Train loss:0.698731, valid loss:0.666920
Epoch:1, Train loss:0.519021, valid loss:0.522276
Epoch:2, Train loss:0.503231, valid loss:0.519896
Epoch:3, Train loss:0.500674, valid loss:0.519630
Epoch:4, Train loss:0.499299, valid loss:0.519042
Epoch:5, Train loss:0.498383, valid loss:0.518869
Epoch:6, Train loss:0.497716, valid loss:0.519076
Epoch:7, Train loss:0.497286, valid loss:0.518145
Epoch:8, Train loss:0.496789, valid loss:0.517660
Epoch:9, Train loss:0.496511, valid loss:0.517793
Epoch:10, Train loss:0.496044, valid loss:0.517349
Epoch:11, Train loss:0.494887, valid loss:0.517026
Epoch:12, Train loss:0.494773, valid loss:0.516776
Epoch:13, Train loss:0.494706, valid loss:0.516705
Epoch:14, Train loss:0.494587, valid loss:0.516623
Epoch:15, Train loss:0.494427, valid loss:0.516600
Epoch:16, Train loss:0.494388, valid loss:0.516852
Epoch:17, Train loss:0.494209, valid loss:0.516655
Epoch:18, Train loss:0.494149, valid loss:0.516462
Epoch:19, Train loss:0.494084, valid loss:0.516506
Epoch:20, Train loss:0.493980, valid loss:0.516235
Epoch:21, Train loss:0.493472, valid loss:0.516190
Epoch:22, Train loss:0.493444, valid loss:0.516088
Epoch:23, Train loss:0.493405, valid loss:0.516066
Epoch:24, Train loss:0.493407, valid loss:0.516113
Epoch:25, Train loss:0.493335, valid loss:0.516099
Epoch:26, Train loss:0.493332, valid loss:0.516091
Epoch:27, Train loss:0.493267, valid loss:0.515921
Epoch:28, Train loss:0.493267, valid loss:0.516057
Epoch:29, Train loss:0.493218, valid loss:0.516018
Epoch:30, Train loss:0.493225, valid loss:0.516015
Epoch:31, Train loss:0.492954, valid loss:0.515873
Epoch:32, Train loss:0.492933, valid loss:0.515849
Epoch:33, Train loss:0.492920, valid loss:0.515873
Epoch:34, Train loss:0.492933, valid loss:0.515898
Epoch:35, Train loss:0.492883, valid loss:0.515913
Epoch:36, Train loss:0.492868, valid loss:0.515840
Epoch:37, Train loss:0.492864, valid loss:0.515813
Epoch:38, Train loss:0.492847, valid loss:0.515872
Epoch:39, Train loss:0.492855, valid loss:0.515783
Epoch:40, Train loss:0.492799, valid loss:0.515845
Epoch:41, Train loss:0.492698, valid loss:0.515806
Epoch:42, Train loss:0.492678, valid loss:0.515825
Epoch:43, Train loss:0.492681, valid loss:0.515782
Epoch:44, Train loss:0.492675, valid loss:0.515882
Epoch:45, Train loss:0.492681, valid loss:0.515832
Epoch:46, Train loss:0.492666, valid loss:0.515825
Epoch:47, Train loss:0.492648, valid loss:0.515776
Epoch:48, Train loss:0.492649, valid loss:0.515807
Epoch:49, Train loss:0.492646, valid loss:0.515794
Epoch:50, Train loss:0.492642, valid loss:0.515781
Epoch:51, Train loss:0.492571, valid loss:0.515763
Epoch:52, Train loss:0.492564, valid loss:0.515778
Epoch:53, Train loss:0.492569, valid loss:0.515795
Epoch:54, Train loss:0.492559, valid loss:0.515784
Epoch:55, Train loss:0.492565, valid loss:0.515747
Epoch:56, Train loss:0.492556, valid loss:0.515777
Epoch:57, Train loss:0.492552, valid loss:0.515748
Epoch:58, Train loss:0.492551, valid loss:0.515748
Epoch:59, Train loss:0.492547, valid loss:0.515786
Epoch:60, Train loss:0.492544, valid loss:0.515775
training time 4367.514063835144
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.8186007810328247
plot_id,batch_id 0 1 miss% 0.8549426150408252
plot_id,batch_id 0 2 miss% 0.8614610319817684
plot_id,batch_id 0 3 miss% 0.8659358943293521
plot_id,batch_id 0 4 miss% 0.8704291615109969
plot_id,batch_id 0 5 miss% 0.8129758521828364
plot_id,batch_id 0 6 miss% 0.8547173124410897
plot_id,batch_id 0 7 miss% 0.865926817092859
plot_id,batch_id 0 8 miss% 0.8670836780815695
plot_id,batch_id 0 9 miss% 0.8655709409494852
plot_id,batch_id 0 10 miss% 0.8058392318454188
plot_id,batch_id 0 11 miss% 0.8527114465947409
plot_id,batch_id 0 12 miss% 0.8606548398843735
plot_id,batch_id 0 13 miss% 0.860457180404726
plot_id,batch_id 0 14 miss% 0.8670306128586645
plot_id,batch_id 0 15 miss% 0.8079986472410688
plot_id,batch_id 0 16 miss% 0.8461552338442123
plot_id,batch_id 0 17 miss% 0.858698718891808
plot_id,batch_id 0 18 miss% 0.8620220117499672
plot_id,batch_id 0 19 miss% 0.8606704252027653
plot_id,batch_id 0 20 miss% 0.8420776113151712
plot_id,batch_id 0 21 miss% 0.8628002564197826
plot_id,batch_id 0 22 miss% 0.8668978986507135
plot_id,batch_id 0 23 miss% 0.8685923723754796
plot_id,batch_id 0 24 miss% 0.8706096558240252
plot_id,batch_id 0 25 miss% 0.8378858969918407
plot_id,batch_id 0 26 miss% 0.8573602823614231
plot_id,batch_id 0 27 miss% 0.8650085860624079
plot_id,batch_id 0 28 miss% 0.8673156291333955
plot_id,batch_id 0 29 miss% 0.8697337260065315
plot_id,batch_id 0 30 miss% 0.8233906471353175
plot_id,batch_id 0 31 miss% 0.858577717449501
plot_id,batch_id 0 32 miss% 0.8635964046377032
plot_id,batch_id 0 33 miss% 0.8639167202422805
plot_id,batch_id 0 34 miss% 0.8653491417858429
plot_id,batch_id 0 35 miss% 0.8257289640768569
plot_id,batch_id 0 36 miss% 0.8617417940944081
plot_id,batch_id 0 37 miss% 0.8621900686349473
plot_id,batch_id 0 38 miss% 0.8659358460995201
plot_id,batch_id 0 39 miss% 0.8659758352716204
plot_id,batch_id 0 40 miss% 0.852514149655955
plot_id,batch_id 0 41 miss% 0.8638963953744303
plot_id,batch_id 0 42 miss% 0.8653586301434856
plot_id,batch_id 0 43 miss% 0.8686001142159916
plot_id,batch_id 0 44 miss% 0.8697599719586901
plot_id,batch_id 0 45 miss% 0.8488232143206809
plot_id,batch_id 0 46 miss% 0.8611161961666567
plot_id,batch_id 0 47 miss% 0.8662659073604485
plot_id,batch_id 0 48 miss% 0.8712588461217288
plot_id,batch_id 0 49 miss% 0.8690248534368724
plot_id,batch_id 0 50 miss% 0.853855644736144
plot_id,batch_id 0 51 miss% 0.8622573427953607
plot_id,batch_id 0 52 miss% 0.8666405711349553
plot_id,batch_id 0 53 miss% 0.8709385889843808
plot_id,batch_id 0 54 miss% 0.8709021788819094
plot_id,batch_id 0 55 miss% 0.8500044464785402
plot_id,batch_id 0 56 miss% 0.8642730073940306
plot_id,batch_id 0 57 miss% 0.867060955043015
plot_id,batch_id 0 58 miss% 0.8692406045709095
plot_id,batch_id 0 59 miss% 0.8690438904038262
plot_id,batch_id 0 60 miss% 0.7665192137187594
plot_id,batch_id 0 61 miss% 0.8377159860439837
plot_id,batch_id 0 62 miss% 0.8557004237261546
plot_id,batch_id 0 63 miss% 0.8581332920830758
plot_id,batch_id 0 64 miss% 0.86178477649351
plot_id,batch_id 0 65 miss% 0.7597641056343143
plot_id,batch_id 0 66 miss% 0.8320405136315628
plot_id,batch_id 0 67 miss% 0.8430840881586633
plot_id,batch_id 0 68 miss% 0.8597585743178535
plot_id,batch_id 0 69 miss% 0.8578349073071526
plot_id,batch_id 0 70 miss% 0.7251463386660019
plot_id,batch_id 0 71 miss% 0.838494066354577
plot_id,batch_id 0 72 miss% 0.8410274851187954
plot_id,batch_id 0 73 miss% 0.8474444041058375
plot_id,batch_id 0 74 miss% 0.8546615160878144
plot_id,batch_id 0 75 miss% 0.7633882883456288
plot_id,batch_id 0 76 miss% 0.8177740535444815
plot_id,batch_id 0 77 miss% 0.8330880576962104
plot_id,batch_id 0 78 miss% 0.8464264017814733
plot_id,batch_id 0 79 miss% 0.8508743469602859
plot_id,batch_id 0 80 miss% 0.7917479026149286
plot_id,batch_id 0 81 miss% 0.8516576771782871
plot_id,batch_id 0 82 miss% 0.8608919125490058
plot_id,batch_id 0 83 miss% 0.8623818377889899
plot_id,batch_id 0 84 miss% 0.8641763781066747
plot_id,batch_id 0 85 miss% 0.7769911197413806
plot_id,batch_id 0 86 miss% 0.8441354806099232
plot_id,batch_id 0 87 miss% 0.8535452041690877
plot_id,batch_id 0 88 miss% 0.8634209211017635
plot_id,batch_id 0 89 miss% 0.861749628478829
plot_id,batch_id 0 90 miss% 0.7575963816128646
plot_id,batch_id 0 91 miss% 0.83886981763311
plot_id,batch_id 0 92 miss% 0.8490550712426492
plot_id,batch_id 0 93 miss% 0.8539777801935783
plot_id,batch_id 0 94 miss% 0.8612947766228216
plot_id,batch_id 0 95 miss% 0.7704691917942638
plot_id,batch_id 0 96 miss% 0.8278837840496894
plot_id,batch_id 0 97 miss% 0.8453862276861484
plot_id,batch_id 0 98 miss% 0.8526159599184003
plot_id,batch_id 0 99 miss% 0.8553389943821929
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.81860078 0.85494262 0.86146103 0.86593589 0.87042916 0.81297585
 0.85471731 0.86592682 0.86708368 0.86557094 0.80583923 0.85271145
 0.86065484 0.86045718 0.86703061 0.80799865 0.84615523 0.85869872
 0.86202201 0.86067043 0.84207761 0.86280026 0.8668979  0.86859237
 0.87060966 0.8378859  0.85736028 0.86500859 0.86731563 0.86973373
 0.82339065 0.85857772 0.8635964  0.86391672 0.86534914 0.82572896
 0.86174179 0.86219007 0.86593585 0.86597584 0.85251415 0.8638964
 0.86535863 0.86860011 0.86975997 0.84882321 0.8611162  0.86626591
 0.87125885 0.86902485 0.85385564 0.86225734 0.86664057 0.87093859
 0.87090218 0.85000445 0.86427301 0.86706096 0.8692406  0.86904389
 0.76651921 0.83771599 0.85570042 0.85813329 0.86178478 0.75976411
 0.83204051 0.84308409 0.85975857 0.85783491 0.72514634 0.83849407
 0.84102749 0.8474444  0.85466152 0.76338829 0.81777405 0.83308806
 0.8464264  0.85087435 0.7917479  0.85165768 0.86089191 0.86238184
 0.86417638 0.77699112 0.84413548 0.8535452  0.86342092 0.86174963
 0.75759638 0.83886982 0.84905507 0.85397778 0.86129478 0.77046919
 0.82788378 0.84538623 0.85261596 0.85533899]
for model  2 the mean error 0.8475924988415886
all id 2 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 21 out win 5 err 0.8475924988415886
Launcher: Job 3 completed in 4550 seconds.
Launcher: Task 5 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  21969
Epoch:0, Train loss:0.569933, valid loss:0.533519
Epoch:1, Train loss:0.355164, valid loss:0.362200
Epoch:2, Train loss:0.345778, valid loss:0.360929
Epoch:3, Train loss:0.344107, valid loss:0.359759
Epoch:4, Train loss:0.343136, valid loss:0.358832
Epoch:5, Train loss:0.342777, valid loss:0.358909
Epoch:6, Train loss:0.342421, valid loss:0.358243
Epoch:7, Train loss:0.342273, valid loss:0.358543
Epoch:8, Train loss:0.342095, valid loss:0.358270
Epoch:9, Train loss:0.341835, valid loss:0.358966
Epoch:10, Train loss:0.341760, valid loss:0.358778
Epoch:11, Train loss:0.340586, valid loss:0.358219
Epoch:12, Train loss:0.340629, valid loss:0.357474
Epoch:13, Train loss:0.340564, valid loss:0.357883
Epoch:14, Train loss:0.340497, valid loss:0.357900
Epoch:15, Train loss:0.340526, valid loss:0.357398
Epoch:16, Train loss:0.340416, valid loss:0.357339
Epoch:17, Train loss:0.340366, valid loss:0.358088
Epoch:18, Train loss:0.340359, valid loss:0.357600
Epoch:19, Train loss:0.340275, valid loss:0.357560
Epoch:20, Train loss:0.340259, valid loss:0.357896
Epoch:21, Train loss:0.339714, valid loss:0.357400
Epoch:22, Train loss:0.339733, valid loss:0.357233
Epoch:23, Train loss:0.339697, valid loss:0.357209
Epoch:24, Train loss:0.339692, valid loss:0.357147
Epoch:25, Train loss:0.339686, valid loss:0.358039
Epoch:26, Train loss:0.339643, valid loss:0.357158
Epoch:27, Train loss:0.339644, valid loss:0.357105
Epoch:28, Train loss:0.339618, valid loss:0.357133
Epoch:29, Train loss:0.339650, valid loss:0.357029
Epoch:30, Train loss:0.339646, valid loss:0.357276
Epoch:31, Train loss:0.339306, valid loss:0.356931
Epoch:32, Train loss:0.339321, valid loss:0.357095
Epoch:33, Train loss:0.339291, valid loss:0.356921
Epoch:34, Train loss:0.339293, valid loss:0.356963
Epoch:35, Train loss:0.339320, valid loss:0.356966
Epoch:36, Train loss:0.339283, valid loss:0.356874
Epoch:37, Train loss:0.339276, valid loss:0.356952
Epoch:38, Train loss:0.339282, valid loss:0.357099
Epoch:39, Train loss:0.339279, valid loss:0.356912
Epoch:40, Train loss:0.339264, valid loss:0.356954
Epoch:41, Train loss:0.339105, valid loss:0.356865
Epoch:42, Train loss:0.339104, valid loss:0.356868
Epoch:43, Train loss:0.339114, valid loss:0.356876
Epoch:44, Train loss:0.339102, valid loss:0.356867
Epoch:45, Train loss:0.339108, valid loss:0.356861
Epoch:46, Train loss:0.339103, valid loss:0.356923
Epoch:47, Train loss:0.339080, valid loss:0.356879
Epoch:48, Train loss:0.339089, valid loss:0.356850
Epoch:49, Train loss:0.339076, valid loss:0.356880
Epoch:50, Train loss:0.339095, valid loss:0.356882
Epoch:51, Train loss:0.339006, valid loss:0.356852
Epoch:52, Train loss:0.339004, valid loss:0.356799
Epoch:53, Train loss:0.339003, valid loss:0.356808
Epoch:54, Train loss:0.338998, valid loss:0.356851
Epoch:55, Train loss:0.338998, valid loss:0.356822
Epoch:56, Train loss:0.338997, valid loss:0.356891
Epoch:57, Train loss:0.338997, valid loss:0.356838
Epoch:58, Train loss:0.338988, valid loss:0.356829
Epoch:59, Train loss:0.338996, valid loss:0.356867
Epoch:60, Train loss:0.338988, valid loss:0.356830
training time 4447.917842626572
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7287008100170339
plot_id,batch_id 0 1 miss% 0.791238224298703
plot_id,batch_id 0 2 miss% 0.8007377998900239
plot_id,batch_id 0 3 miss% 0.8120713028722019
plot_id,batch_id 0 4 miss% 0.8100711793138703
plot_id,batch_id 0 5 miss% 0.7268647859988865
plot_id,batch_id 0 6 miss% 0.7841255891979854
plot_id,batch_id 0 7 miss% 0.8005060600904083
plot_id,batch_id 0 8 miss% 0.8054914895450936
plot_id,batch_id 0 9 miss% 0.8107769431055891
plot_id,batch_id 0 10 miss% 0.707804156125976
plot_id,batch_id 0 11 miss% 0.7872704831484949
plot_id,batch_id 0 12 miss% 0.7996051276222762
plot_id,batch_id 0 13 miss% 0.8045179427893159
plot_id,batch_id 0 14 miss% 0.8111398970358482
plot_id,batch_id 0 15 miss% 0.7115878283754116
plot_id,batch_id 0 16 miss% 0.7801701935000132
plot_id,batch_id 0 17 miss% 0.7975571118376708
plot_id,batch_id 0 18 miss% 0.8048837027466857
plot_id,batch_id 0 19 miss% 0.8070952585502937
plot_id,batch_id 0 20 miss% 0.7658248944803436
plot_id,batch_id 0 21 miss% 0.8060949523951423
plot_id,batch_id 0 22 miss% 0.8125617098746546
plot_id,batch_id 0 23 miss% 0.8152373266725781
plot_id,batch_id 0 24 miss% 0.8171407227067843
plot_id,batch_id 0 25 miss% 0.7554097087774857
plot_id,batch_id 0 26 miss% 0.7976190155604158
plot_id,batch_id 0 27 miss% 0.8048594318643562
plot_id,batch_id 0 28 miss% 0.807743190625546
plot_id,batch_id 0 29 miss% 0.8100944049967181
plot_id,batch_id 0 30 miss% 0.7552078356271439
plot_id,batch_id 0 31 miss% 0.7959359589900397
plot_id,batch_id 0 32 miss% 0.8039396142131017
plot_id,batch_id 0 33 miss% 0.8093196219615623
plot_id,batch_id 0 34 miss% 0.8111372407137161
plot_id,batch_id 0 35 miss% 0.7473882842820662
plot_id,batch_id 0 36 miss% 0.8006879500886603
plot_id,batch_id 0 37 miss% 0.8084033148670141
plot_id,batch_id 0 38 miss% 0.8117234714457681
plot_id,batch_id 0 39 miss% 0.8117884449209751
plot_id,batch_id 0 40 miss% 0.7857326058362534
plot_id,batch_id 0 41 miss% 0.8079810938892406
plot_id,batch_id 0 42 miss% 0.8115708979611814
plot_id,batch_id 0 43 miss% 0.8153998678842874
plot_id,batch_id 0 44 miss% 0.8183064784520381
plot_id,batch_id 0 45 miss% 0.7812603831817415
plot_id,batch_id 0 46 miss% 0.8057964643673604
plot_id,batch_id 0 47 miss% 0.8126440797481612
plot_id,batch_id 0 48 miss% 0.8128115109135946
plot_id,batch_id 0 49 miss% 0.8176172618266316
plot_id,batch_id 0 50 miss% 0.7855608430296878
plot_id,batch_id 0 51 miss% 0.8044304547148278
plot_id,batch_id 0 52 miss% 0.8087948017902614
plot_id,batch_id 0 53 miss% 0.8139203965896158
plot_id,batch_id 0 54 miss% 0.8195360671066424
plot_id,batch_id 0 55 miss% 0.768431767516393
plot_id,batch_id 0 56 miss% 0.8057729766720129
plot_id,batch_id 0 57 miss% 0.8100762461495191
plot_id,batch_id 0 58 miss% 0.81315240433449
plot_id,batch_id 0 59 miss% 0.8177256374760619
plot_id,batch_id 0 60 miss% 0.6506561744782042
plot_id,batch_id 0 61 miss% 0.7534255414495487
plot_id,batch_id 0 62 miss% 0.7864557733273745
plot_id,batch_id 0 63 miss% 0.7949512890068526
plot_id,batch_id 0 64 miss% 0.8030067487194624
plot_id,batch_id 0 65 miss% 0.6430585965263176
plot_id,batch_id 0 66 miss% 0.757330963352167
plot_id,batch_id 0 67 miss% 0.7718841314183633
plot_id,batch_id 0 68 miss% 0.7955865185863749
plot_id,batch_id 0 69 miss% 0.7989840799579505
plot_id,batch_id 0 70 miss% 0.6109818275290717
plot_id,batch_id 0 71 miss% 0.7670881305296785
plot_id,batch_id 0 72 miss% 0.7619560683338354
plot_id,batch_id 0 73 miss% 0.7823980395041042
plot_id,batch_id 0 74 miss% 0.7949479465351571
plot_id,batch_id 0 75 miss% 0.6118082216533567
plot_id,batch_id 0 76 miss% 0.7143060350212713
plot_id,batch_id 0 77 miss% 0.7525486827581734
plot_id,batch_id 0 78 miss% 0.7799249376751346
plot_id,batch_id 0 79 miss% 0.784090072304585
plot_id,batch_id 0 80 miss% 0.6721779240833514
plot_id,batch_id 0 81 miss% 0.778306877383806
plot_id,batch_id 0 82 miss% 0.7916582449463156
plot_id,batch_id 0 83 miss% 0.8000392770425685
plot_id,batch_id 0 84 miss% 0.8026120793727488
plot_id,batch_id 0 85 miss% 0.6687423432522397
plot_id,batch_id 0 86 miss% 0.7683610391466331
plot_id,batch_id 0 87 miss% 0.7910653149431814
plot_id,batch_id 0 88 miss% 0.7979308451337723
plot_id,batch_id 0 89 miss% 0.800437998060536
plot_id,batch_id 0 90 miss% 0.6421490061858225
plot_id,batch_id 0 91 miss% 0.7703922508671709
plot_id,batch_id 0 92 miss% 0.7836495344223697
plot_id,batch_id 0 93 miss% 0.7893681067127669
plot_id,batch_id 0 94 miss% 0.7985844475617881
plot_id,batch_id 0 95 miss% 0.6412604325281529
plot_id,batch_id 0 96 miss% 0.7497478560530177
plot_id,batch_id 0 97 miss% 0.7759218383647073
plot_id,batch_id 0 98 miss% 0.7844799913114227
plot_id,batch_id 0 99 miss% 0.795607771652569
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.72870081 0.79123822 0.8007378  0.8120713  0.81007118 0.72686479
 0.78412559 0.80050606 0.80549149 0.81077694 0.70780416 0.78727048
 0.79960513 0.80451794 0.8111399  0.71158783 0.78017019 0.79755711
 0.8048837  0.80709526 0.76582489 0.80609495 0.81256171 0.81523733
 0.81714072 0.75540971 0.79761902 0.80485943 0.80774319 0.8100944
 0.75520784 0.79593596 0.80393961 0.80931962 0.81113724 0.74738828
 0.80068795 0.80840331 0.81172347 0.81178844 0.78573261 0.80798109
 0.8115709  0.81539987 0.81830648 0.78126038 0.80579646 0.81264408
 0.81281151 0.81761726 0.78556084 0.80443045 0.8087948  0.8139204
 0.81953607 0.76843177 0.80577298 0.81007625 0.8131524  0.81772564
 0.65065617 0.75342554 0.78645577 0.79495129 0.80300675 0.6430586
 0.75733096 0.77188413 0.79558652 0.79898408 0.61098183 0.76708813
 0.76195607 0.78239804 0.79494795 0.61180822 0.71430604 0.75254868
 0.77992494 0.78409007 0.67217792 0.77830688 0.79165824 0.80003928
 0.80261208 0.66874234 0.76836104 0.79106531 0.79793085 0.800438
 0.64214901 0.77039225 0.78364953 0.78936811 0.79858445 0.64126043
 0.74974786 0.77592184 0.78447999 0.79560777]
for model  136 the mean error 0.778647381782578
all id 136 hidden_dim 16 learning_rate 0.02 num_layers 3 frames 25 out win 4 err 0.778647381782578
Launcher: Job 137 completed in 4629 seconds.
Launcher: Task 215 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  21969
Epoch:0, Train loss:0.569933, valid loss:0.533519
Epoch:1, Train loss:0.356899, valid loss:0.361717
Epoch:2, Train loss:0.345483, valid loss:0.359555
Epoch:3, Train loss:0.343876, valid loss:0.358910
Epoch:4, Train loss:0.342783, valid loss:0.358788
Epoch:5, Train loss:0.342129, valid loss:0.358063
Epoch:6, Train loss:0.341700, valid loss:0.358276
Epoch:7, Train loss:0.341481, valid loss:0.357676
Epoch:8, Train loss:0.341205, valid loss:0.358108
Epoch:9, Train loss:0.341137, valid loss:0.358093
Epoch:10, Train loss:0.340931, valid loss:0.357607
Epoch:11, Train loss:0.340311, valid loss:0.357584
Epoch:12, Train loss:0.340268, valid loss:0.357426
Epoch:13, Train loss:0.340217, valid loss:0.357377
Epoch:14, Train loss:0.340197, valid loss:0.357284
Epoch:15, Train loss:0.340144, valid loss:0.357427
Epoch:16, Train loss:0.340134, valid loss:0.357380
Epoch:17, Train loss:0.340114, valid loss:0.357469
Epoch:18, Train loss:0.340024, valid loss:0.357376
Epoch:19, Train loss:0.340018, valid loss:0.357343
Epoch:20, Train loss:0.339956, valid loss:0.357295
Epoch:21, Train loss:0.339641, valid loss:0.357112
Epoch:22, Train loss:0.339631, valid loss:0.357119
Epoch:23, Train loss:0.339601, valid loss:0.357253
Epoch:24, Train loss:0.339607, valid loss:0.357038
Epoch:25, Train loss:0.339574, valid loss:0.357214
Epoch:26, Train loss:0.339568, valid loss:0.357292
Epoch:27, Train loss:0.339551, valid loss:0.357068
Epoch:28, Train loss:0.339554, valid loss:0.357235
Epoch:29, Train loss:0.339523, valid loss:0.357223
Epoch:30, Train loss:0.339533, valid loss:0.357086
Epoch:31, Train loss:0.339361, valid loss:0.357028
Epoch:32, Train loss:0.339344, valid loss:0.357004
Epoch:33, Train loss:0.339343, valid loss:0.357079
Epoch:34, Train loss:0.339328, valid loss:0.357030
Epoch:35, Train loss:0.339339, valid loss:0.357013
Epoch:36, Train loss:0.339307, valid loss:0.357014
Epoch:37, Train loss:0.339311, valid loss:0.357009
Epoch:38, Train loss:0.339312, valid loss:0.357020
Epoch:39, Train loss:0.339292, valid loss:0.357018
Epoch:40, Train loss:0.339308, valid loss:0.357021
Epoch:41, Train loss:0.339214, valid loss:0.356974
Epoch:42, Train loss:0.339206, valid loss:0.357010
Epoch:43, Train loss:0.339209, valid loss:0.356986
Epoch:44, Train loss:0.339202, valid loss:0.357010
Epoch:45, Train loss:0.339201, valid loss:0.357015
Epoch:46, Train loss:0.339199, valid loss:0.356973
Epoch:47, Train loss:0.339193, valid loss:0.356985
Epoch:48, Train loss:0.339193, valid loss:0.356982
Epoch:49, Train loss:0.339185, valid loss:0.356967
Epoch:50, Train loss:0.339183, valid loss:0.356975
Epoch:51, Train loss:0.339146, valid loss:0.356949
Epoch:52, Train loss:0.339143, valid loss:0.356957
Epoch:53, Train loss:0.339140, valid loss:0.356960
Epoch:54, Train loss:0.339138, valid loss:0.356956
Epoch:55, Train loss:0.339137, valid loss:0.356949
Epoch:56, Train loss:0.339130, valid loss:0.356956
Epoch:57, Train loss:0.339135, valid loss:0.356955
Epoch:58, Train loss:0.339132, valid loss:0.356935
Epoch:59, Train loss:0.339136, valid loss:0.356966
Epoch:60, Train loss:0.339128, valid loss:0.356966
training time 4492.539385557175
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7307557761266801
plot_id,batch_id 0 1 miss% 0.7899732983349809
plot_id,batch_id 0 2 miss% 0.8026834873079656
plot_id,batch_id 0 3 miss% 0.8121833443796558
plot_id,batch_id 0 4 miss% 0.8103961902331618
plot_id,batch_id 0 5 miss% 0.7291279864687723
plot_id,batch_id 0 6 miss% 0.7834145892053224
plot_id,batch_id 0 7 miss% 0.7983226160284917
plot_id,batch_id 0 8 miss% 0.8070402484148922
plot_id,batch_id 0 9 miss% 0.8146391003470971
plot_id,batch_id 0 10 miss% 0.7003624290340524
plot_id,batch_id 0 11 miss% 0.7857468461686823
plot_id,batch_id 0 12 miss% 0.7968435861590968
plot_id,batch_id 0 13 miss% 0.8047806580527312
plot_id,batch_id 0 14 miss% 0.8083129204632115
plot_id,batch_id 0 15 miss% 0.7138114110238175
plot_id,batch_id 0 16 miss% 0.7827625050495275
plot_id,batch_id 0 17 miss% 0.7959722125214425
plot_id,batch_id 0 18 miss% 0.8076599834920395
plot_id,batch_id 0 19 miss% 0.8060268190962614
plot_id,batch_id 0 20 miss% 0.76520591597294
plot_id,batch_id 0 21 miss% 0.8044311680675859
plot_id,batch_id 0 22 miss% 0.8106267030050908
plot_id,batch_id 0 23 miss% 0.8153562239430591
plot_id,batch_id 0 24 miss% 0.8166958907173828
plot_id,batch_id 0 25 miss% 0.7558835267239011
plot_id,batch_id 0 26 miss% 0.7984683377384397
plot_id,batch_id 0 27 miss% 0.8036514551457583
plot_id,batch_id 0 28 miss% 0.8079730660898335
plot_id,batch_id 0 29 miss% 0.8100894645490401
plot_id,batch_id 0 30 miss% 0.7542637047200507
plot_id,batch_id 0 31 miss% 0.7933381488444472
plot_id,batch_id 0 32 miss% 0.8041717356637547
plot_id,batch_id 0 33 miss% 0.8077400141490143
plot_id,batch_id 0 34 miss% 0.8095558793639194
plot_id,batch_id 0 35 miss% 0.748758681270331
plot_id,batch_id 0 36 miss% 0.8012880517101472
plot_id,batch_id 0 37 miss% 0.8037883749763358
plot_id,batch_id 0 38 miss% 0.8115698707704712
plot_id,batch_id 0 39 miss% 0.8102796196911073
plot_id,batch_id 0 40 miss% 0.7925338787836621
plot_id,batch_id 0 41 miss% 0.8074557788803304
plot_id,batch_id 0 42 miss% 0.8101087026234643
plot_id,batch_id 0 43 miss% 0.8130476311553909
plot_id,batch_id 0 44 miss% 0.8170361668785607
plot_id,batch_id 0 45 miss% 0.7791436229774523
plot_id,batch_id 0 46 miss% 0.8075178726764008
plot_id,batch_id 0 47 miss% 0.8116330958884114
plot_id,batch_id 0 48 miss% 0.8132727498954639
plot_id,batch_id 0 49 miss% 0.8175964549667836
plot_id,batch_id 0 50 miss% 0.7860432797046663
plot_id,batch_id 0 51 miss% 0.8050379713990629
plot_id,batch_id 0 52 miss% 0.8081533902192841
plot_id,batch_id 0 53 miss% 0.8132935274497866
plot_id,batch_id 0 54 miss% 0.8203023808310507
plot_id,batch_id 0 55 miss% 0.77242607717139
plot_id,batch_id 0 56 miss% 0.8042798282957402
plot_id,batch_id 0 57 miss% 0.811167159617995
plot_id,batch_id 0 58 miss% 0.8147878804308387
plot_id,batch_id 0 59 miss% 0.8170869207545752
plot_id,batch_id 0 60 miss% 0.6473902276184902
plot_id,batch_id 0 61 miss% 0.7510334485890421
plot_id,batch_id 0 62 miss% 0.7846565663401298
plot_id,batch_id 0 63 miss% 0.7952629164312154
plot_id,batch_id 0 64 miss% 0.8032656171687936
plot_id,batch_id 0 65 miss% 0.6398434644790049
plot_id,batch_id 0 66 miss% 0.7567474055808523
plot_id,batch_id 0 67 miss% 0.77369842667553
plot_id,batch_id 0 68 miss% 0.7961138493934723
plot_id,batch_id 0 69 miss% 0.7948998636031965
plot_id,batch_id 0 70 miss% 0.6100595083114618
plot_id,batch_id 0 71 miss% 0.761038394648332
plot_id,batch_id 0 72 miss% 0.7663548491140345
plot_id,batch_id 0 73 miss% 0.7806244651356171
plot_id,batch_id 0 74 miss% 0.7949188850527759
plot_id,batch_id 0 75 miss% 0.6119174793622273
plot_id,batch_id 0 76 miss% 0.7274012465196583
plot_id,batch_id 0 77 miss% 0.7550029727068999
plot_id,batch_id 0 78 miss% 0.7793291137081335
plot_id,batch_id 0 79 miss% 0.7875725641430156
plot_id,batch_id 0 80 miss% 0.6731564048064576
plot_id,batch_id 0 81 miss% 0.7780396943949274
plot_id,batch_id 0 82 miss% 0.7907120348668969
plot_id,batch_id 0 83 miss% 0.8017155196725079
plot_id,batch_id 0 84 miss% 0.8033036776143113
plot_id,batch_id 0 85 miss% 0.6675257419476281
plot_id,batch_id 0 86 miss% 0.7711081128300888
plot_id,batch_id 0 87 miss% 0.7905877549586959
plot_id,batch_id 0 88 miss% 0.7977956820079032
plot_id,batch_id 0 89 miss% 0.7995343720924908
plot_id,batch_id 0 90 miss% 0.6430582090479833
plot_id,batch_id 0 91 miss% 0.7685291124780462
plot_id,batch_id 0 92 miss% 0.7809044361731992
plot_id,batch_id 0 93 miss% 0.7911421322570079
plot_id,batch_id 0 94 miss% 0.8010196337906677
plot_id,batch_id 0 95 miss% 0.6488491244895613
plot_id,batch_id 0 96 miss% 0.7508056942230267
plot_id,batch_id 0 97 miss% 0.7774328201500719
plot_id,batch_id 0 98 miss% 0.7898055788186633
plot_id,batch_id 0 99 miss% 0.7959329699592661
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73075578 0.7899733  0.80268349 0.81218334 0.81039619 0.72912799
 0.78341459 0.79832262 0.80704025 0.8146391  0.70036243 0.78574685
 0.79684359 0.80478066 0.80831292 0.71381141 0.78276251 0.79597221
 0.80765998 0.80602682 0.76520592 0.80443117 0.8106267  0.81535622
 0.81669589 0.75588353 0.79846834 0.80365146 0.80797307 0.81008946
 0.7542637  0.79333815 0.80417174 0.80774001 0.80955588 0.74875868
 0.80128805 0.80378837 0.81156987 0.81027962 0.79253388 0.80745578
 0.8101087  0.81304763 0.81703617 0.77914362 0.80751787 0.8116331
 0.81327275 0.81759645 0.78604328 0.80503797 0.80815339 0.81329353
 0.82030238 0.77242608 0.80427983 0.81116716 0.81478788 0.81708692
 0.64739023 0.75103345 0.78465657 0.79526292 0.80326562 0.63984346
 0.75674741 0.77369843 0.79611385 0.79489986 0.61005951 0.76103839
 0.76635485 0.78062447 0.79491889 0.61191748 0.72740125 0.75500297
 0.77932911 0.78757256 0.6731564  0.77803969 0.79071203 0.80171552
 0.80330368 0.66752574 0.77110811 0.79058775 0.79779568 0.79953437
 0.64305821 0.76852911 0.78090444 0.79114213 0.80101963 0.64884912
 0.75080569 0.77743282 0.78980558 0.79593297]
for model  82 the mean error 0.778739661827821
all id 82 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 25 out win 4 err 0.778739661827821
Launcher: Job 83 completed in 4673 seconds.
Launcher: Task 100 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  21969
Epoch:0, Train loss:0.480606, valid loss:0.440352
Epoch:1, Train loss:0.226023, valid loss:0.230130
Epoch:2, Train loss:0.218865, valid loss:0.229199
Epoch:3, Train loss:0.217919, valid loss:0.228889
Epoch:4, Train loss:0.217263, valid loss:0.228371
Epoch:5, Train loss:0.216823, valid loss:0.228334
Epoch:6, Train loss:0.216573, valid loss:0.228201
Epoch:7, Train loss:0.216430, valid loss:0.228242
Epoch:8, Train loss:0.216328, valid loss:0.228061
Epoch:9, Train loss:0.216255, valid loss:0.227825
Epoch:10, Train loss:0.216162, valid loss:0.227849
Epoch:11, Train loss:0.215736, valid loss:0.227982
Epoch:12, Train loss:0.215707, valid loss:0.227780
Epoch:13, Train loss:0.215699, valid loss:0.227614
Epoch:14, Train loss:0.215651, valid loss:0.227840
Epoch:15, Train loss:0.215646, valid loss:0.227657
Epoch:16, Train loss:0.215611, valid loss:0.227752
Epoch:17, Train loss:0.215575, valid loss:0.227626
Epoch:18, Train loss:0.215562, valid loss:0.227573
Epoch:19, Train loss:0.215547, valid loss:0.227599
Epoch:20, Train loss:0.215523, valid loss:0.227550
Epoch:21, Train loss:0.215320, valid loss:0.227476
Epoch:22, Train loss:0.215316, valid loss:0.227553
Epoch:23, Train loss:0.215304, valid loss:0.227479
Epoch:24, Train loss:0.215303, valid loss:0.227473
Epoch:25, Train loss:0.215278, valid loss:0.227490
Epoch:26, Train loss:0.215294, valid loss:0.227478
Epoch:27, Train loss:0.215279, valid loss:0.227459
Epoch:28, Train loss:0.215275, valid loss:0.227484
Epoch:29, Train loss:0.215264, valid loss:0.227471
Epoch:30, Train loss:0.215284, valid loss:0.227535
Epoch:31, Train loss:0.215163, valid loss:0.227423
Epoch:32, Train loss:0.215154, valid loss:0.227430
Epoch:33, Train loss:0.215159, valid loss:0.227398
Epoch:34, Train loss:0.215147, valid loss:0.227421
Epoch:35, Train loss:0.215146, valid loss:0.227392
Epoch:36, Train loss:0.215143, valid loss:0.227412
Epoch:37, Train loss:0.215141, valid loss:0.227506
Epoch:38, Train loss:0.215130, valid loss:0.227478
Epoch:39, Train loss:0.215136, valid loss:0.227403
Epoch:40, Train loss:0.215131, valid loss:0.227397
Epoch:41, Train loss:0.215083, valid loss:0.227394
Epoch:42, Train loss:0.215079, valid loss:0.227374
Epoch:43, Train loss:0.215077, valid loss:0.227390
Epoch:44, Train loss:0.215073, valid loss:0.227383
Epoch:45, Train loss:0.215073, valid loss:0.227380
Epoch:46, Train loss:0.215069, valid loss:0.227379
Epoch:47, Train loss:0.215068, valid loss:0.227383
Epoch:48, Train loss:0.215068, valid loss:0.227382
Epoch:49, Train loss:0.215067, valid loss:0.227366
Epoch:50, Train loss:0.215062, valid loss:0.227376
Epoch:51, Train loss:0.215039, valid loss:0.227364
Epoch:52, Train loss:0.215037, valid loss:0.227368
Epoch:53, Train loss:0.215037, valid loss:0.227364
Epoch:54, Train loss:0.215036, valid loss:0.227356
Epoch:55, Train loss:0.215036, valid loss:0.227359
Epoch:56, Train loss:0.215035, valid loss:0.227357
Epoch:57, Train loss:0.215034, valid loss:0.227360
Epoch:58, Train loss:0.215034, valid loss:0.227362
Epoch:59, Train loss:0.215031, valid loss:0.227352
Epoch:60, Train loss:0.215032, valid loss:0.227367
training time 4539.384622812271
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.5536038979371254
plot_id,batch_id 0 1 miss% 0.6791214682799152
plot_id,batch_id 0 2 miss% 0.6944346861406275
plot_id,batch_id 0 3 miss% 0.7112791989004564
plot_id,batch_id 0 4 miss% 0.7097336841527239
plot_id,batch_id 0 5 miss% 0.5528258967326328
plot_id,batch_id 0 6 miss% 0.6705048120249276
plot_id,batch_id 0 7 miss% 0.6974030467382982
plot_id,batch_id 0 8 miss% 0.7039414789096905
plot_id,batch_id 0 9 miss% 0.7137437819770417
plot_id,batch_id 0 10 miss% 0.527565282082089
plot_id,batch_id 0 11 miss% 0.6715535922739787
plot_id,batch_id 0 12 miss% 0.6891926437404352
plot_id,batch_id 0 13 miss% 0.7035371886211178
plot_id,batch_id 0 14 miss% 0.7133462146708099
plot_id,batch_id 0 15 miss% 0.5300402237727933
plot_id,batch_id 0 16 miss% 0.6725177220453806
plot_id,batch_id 0 17 miss% 0.7015049256439141
plot_id,batch_id 0 18 miss% 0.706964844694449
plot_id,batch_id 0 19 miss% 0.7075392725672645
plot_id,batch_id 0 20 miss% 0.6216056917568658
plot_id,batch_id 0 21 miss% 0.7001093297923915
plot_id,batch_id 0 22 miss% 0.7093797809015269
plot_id,batch_id 0 23 miss% 0.7182427734436625
plot_id,batch_id 0 24 miss% 0.7235546675918677
plot_id,batch_id 0 25 miss% 0.6094656942544328
plot_id,batch_id 0 26 miss% 0.6914044199579981
plot_id,batch_id 0 27 miss% 0.7106074016806316
plot_id,batch_id 0 28 miss% 0.7192534199682232
plot_id,batch_id 0 29 miss% 0.7237369701416758
plot_id,batch_id 0 30 miss% 0.6105231355047973
plot_id,batch_id 0 31 miss% 0.6875781871145346
plot_id,batch_id 0 32 miss% 0.7034227950064768
plot_id,batch_id 0 33 miss% 0.71239750513791
plot_id,batch_id 0 34 miss% 0.7121520190721856
plot_id,batch_id 0 35 miss% 0.5930737475341067
plot_id,batch_id 0 36 miss% 0.6919000410901623
plot_id,batch_id 0 37 miss% 0.7048571410651144
plot_id,batch_id 0 38 miss% 0.7146398466608694
plot_id,batch_id 0 39 miss% 0.7139150785650455
plot_id,batch_id 0 40 miss% 0.6685008669700595
plot_id,batch_id 0 41 miss% 0.7102619727769396
plot_id,batch_id 0 42 miss% 0.7125821702827753
plot_id,batch_id 0 43 miss% 0.7240302343269732
plot_id,batch_id 0 44 miss% 0.7263726087144758
plot_id,batch_id 0 45 miss% 0.6577595518545065
plot_id,batch_id 0 46 miss% 0.7095944449279543
plot_id,batch_id 0 47 miss% 0.7149836054508967
plot_id,batch_id 0 48 miss% 0.7207719141635696
plot_id,batch_id 0 49 miss% 0.7271768111466216
plot_id,batch_id 0 50 miss% 0.6662381498650293
plot_id,batch_id 0 51 miss% 0.7048834691598092
plot_id,batch_id 0 52 miss% 0.7147361267235336
plot_id,batch_id 0 53 miss% 0.7221293455615367
plot_id,batch_id 0 54 miss% 0.7307209341942801
plot_id,batch_id 0 55 miss% 0.6752619462308131
plot_id,batch_id 0 56 miss% 0.7083241537387837
plot_id,batch_id 0 57 miss% 0.7150423441844609
plot_id,batch_id 0 58 miss% 0.7219097684199832
plot_id,batch_id 0 59 miss% 0.7187767717044189
plot_id,batch_id 0 60 miss% 0.4432544404726651
plot_id,batch_id 0 61 miss% 0.6167829883901097
plot_id,batch_id 0 62 miss% 0.6652316825400616
plot_id,batch_id 0 63 miss% 0.6843852702655954
plot_id,batch_id 0 64 miss% 0.6942264605089323
plot_id,batch_id 0 65 miss% 0.430735246880418
plot_id,batch_id 0 66 miss% 0.6046556924918083
plot_id,batch_id 0 67 miss% 0.6459046777058056
plot_id,batch_id 0 68 miss% 0.6869101560219042
plot_id,batch_id 0 69 miss% 0.690670779309131
plot_id,batch_id 0 70 miss% 0.4073369652100698
plot_id,batch_id 0 71 miss% 0.5916477927154752
plot_id,batch_id 0 72 miss% 0.6311567388916192
plot_id,batch_id 0 73 miss% 0.6623037664686295
plot_id,batch_id 0 74 miss% 0.6870153393705679
plot_id,batch_id 0 75 miss% 0.39870976322559465
plot_id,batch_id 0 76 miss% 0.5669924276786595
plot_id,batch_id 0 77 miss% 0.625609452348154
plot_id,batch_id 0 78 miss% 0.6663809616380723
plot_id,batch_id 0 79 miss% 0.6833027329422076
plot_id,batch_id 0 80 miss% 0.4725648885758074
plot_id,batch_id 0 81 miss% 0.6402875352541644
plot_id,batch_id 0 82 miss% 0.6760521288344081
plot_id,batch_id 0 83 miss% 0.6949778197944497
plot_id,batch_id 0 84 miss% 0.6979895412332723
plot_id,batch_id 0 85 miss% 0.4671987253293501
plot_id,batch_id 0 86 miss% 0.6379725231992645
plot_id,batch_id 0 87 miss% 0.6658477631533088
plot_id,batch_id 0 88 miss% 0.6931215418802452
plot_id,batch_id 0 89 miss% 0.700430527360441
plot_id,batch_id 0 90 miss% 0.4385071426212894
plot_id,batch_id 0 91 miss% 0.6371063067538785
plot_id,batch_id 0 92 miss% 0.6699324547043198
plot_id,batch_id 0 93 miss% 0.6898050996210568
plot_id,batch_id 0 94 miss% 0.6986236902728472
plot_id,batch_id 0 95 miss% 0.43468112415778115
plot_id,batch_id 0 96 miss% 0.613496596591377
plot_id,batch_id 0 97 miss% 0.6619780753650348
plot_id,batch_id 0 98 miss% 0.673020616666024
plot_id,batch_id 0 99 miss% 0.6894693222401318
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.5536039  0.67912147 0.69443469 0.7112792  0.70973368 0.5528259
 0.67050481 0.69740305 0.70394148 0.71374378 0.52756528 0.67155359
 0.68919264 0.70353719 0.71334621 0.53004022 0.67251772 0.70150493
 0.70696484 0.70753927 0.62160569 0.70010933 0.70937978 0.71824277
 0.72355467 0.60946569 0.69140442 0.7106074  0.71925342 0.72373697
 0.61052314 0.68757819 0.7034228  0.71239751 0.71215202 0.59307375
 0.69190004 0.70485714 0.71463985 0.71391508 0.66850087 0.71026197
 0.71258217 0.72403023 0.72637261 0.65775955 0.70959444 0.71498361
 0.72077191 0.72717681 0.66623815 0.70488347 0.71473613 0.72212935
 0.73072093 0.67526195 0.70832415 0.71504234 0.72190977 0.71877677
 0.44325444 0.61678299 0.66523168 0.68438527 0.69422646 0.43073525
 0.60465569 0.64590468 0.68691016 0.69067078 0.40733697 0.59164779
 0.63115674 0.66230377 0.68701534 0.39870976 0.56699243 0.62560945
 0.66638096 0.68330273 0.47256489 0.64028754 0.67605213 0.69497782
 0.69798954 0.46719873 0.63797252 0.66584776 0.69312154 0.70043053
 0.43850714 0.63710631 0.66993245 0.6898051  0.69862369 0.43468112
 0.6134966  0.66197808 0.67302062 0.68946932]
for model  162 the mean error 0.6586850745722751
all id 162 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 31 out win 3 err 0.6586850745722751
Launcher: Job 163 completed in 4718 seconds.
Launcher: Task 111 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  21969
Epoch:0, Train loss:0.480606, valid loss:0.440352
Epoch:1, Train loss:0.224378, valid loss:0.229259
Epoch:2, Train loss:0.217962, valid loss:0.228743
Epoch:3, Train loss:0.217157, valid loss:0.228489
Epoch:4, Train loss:0.216800, valid loss:0.228221
Epoch:5, Train loss:0.216571, valid loss:0.228044
Epoch:6, Train loss:0.216417, valid loss:0.228729
Epoch:7, Train loss:0.216305, valid loss:0.228216
Epoch:8, Train loss:0.216194, valid loss:0.228218
Epoch:9, Train loss:0.216149, valid loss:0.228091
Epoch:10, Train loss:0.216112, valid loss:0.227929
Epoch:11, Train loss:0.215573, valid loss:0.227622
Epoch:12, Train loss:0.215596, valid loss:0.227709
Epoch:13, Train loss:0.215569, valid loss:0.227572
Epoch:14, Train loss:0.215522, valid loss:0.227846
Epoch:15, Train loss:0.215535, valid loss:0.227606
Epoch:16, Train loss:0.215495, valid loss:0.227737
Epoch:17, Train loss:0.215485, valid loss:0.227696
Epoch:18, Train loss:0.215494, valid loss:0.227579
Epoch:19, Train loss:0.215483, valid loss:0.227584
Epoch:20, Train loss:0.215481, valid loss:0.227656
Epoch:21, Train loss:0.215215, valid loss:0.227469
Epoch:22, Train loss:0.215230, valid loss:0.227468
Epoch:23, Train loss:0.215215, valid loss:0.227415
Epoch:24, Train loss:0.215200, valid loss:0.227498
Epoch:25, Train loss:0.215190, valid loss:0.227439
Epoch:26, Train loss:0.215200, valid loss:0.227430
Epoch:27, Train loss:0.215187, valid loss:0.227454
Epoch:28, Train loss:0.215176, valid loss:0.227391
Epoch:29, Train loss:0.215204, valid loss:0.227588
Epoch:30, Train loss:0.215182, valid loss:0.227435
Epoch:31, Train loss:0.215055, valid loss:0.227351
Epoch:32, Train loss:0.215058, valid loss:0.227385
Epoch:33, Train loss:0.215052, valid loss:0.227409
Epoch:34, Train loss:0.215055, valid loss:0.227434
Epoch:35, Train loss:0.215047, valid loss:0.227354
Epoch:36, Train loss:0.215040, valid loss:0.227384
Epoch:37, Train loss:0.215036, valid loss:0.227350
Epoch:38, Train loss:0.215039, valid loss:0.227395
Epoch:39, Train loss:0.215030, valid loss:0.227331
Epoch:40, Train loss:0.215036, valid loss:0.227337
Epoch:41, Train loss:0.214973, valid loss:0.227328
Epoch:42, Train loss:0.214972, valid loss:0.227339
Epoch:43, Train loss:0.214970, valid loss:0.227373
Epoch:44, Train loss:0.214967, valid loss:0.227325
Epoch:45, Train loss:0.214965, valid loss:0.227308
Epoch:46, Train loss:0.214961, valid loss:0.227320
Epoch:47, Train loss:0.214970, valid loss:0.227331
Epoch:48, Train loss:0.214960, valid loss:0.227319
Epoch:49, Train loss:0.214959, valid loss:0.227311
Epoch:50, Train loss:0.214964, valid loss:0.227313
Epoch:51, Train loss:0.214928, valid loss:0.227308
Epoch:52, Train loss:0.214926, valid loss:0.227289
Epoch:53, Train loss:0.214925, valid loss:0.227299
Epoch:54, Train loss:0.214926, valid loss:0.227286
Epoch:55, Train loss:0.214925, valid loss:0.227300
Epoch:56, Train loss:0.214924, valid loss:0.227291
Epoch:57, Train loss:0.214923, valid loss:0.227309
Epoch:58, Train loss:0.214922, valid loss:0.227305
Epoch:59, Train loss:0.214923, valid loss:0.227308
Epoch:60, Train loss:0.214921, valid loss:0.227296
training time 4542.978635787964
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.5497679600435417
plot_id,batch_id 0 1 miss% 0.6823015533738976
plot_id,batch_id 0 2 miss% 0.6939964743376971
plot_id,batch_id 0 3 miss% 0.7100562556387551
plot_id,batch_id 0 4 miss% 0.709237228011691
plot_id,batch_id 0 5 miss% 0.5587719805583073
plot_id,batch_id 0 6 miss% 0.6711923776460786
plot_id,batch_id 0 7 miss% 0.6980072768159619
plot_id,batch_id 0 8 miss% 0.7027637719763875
plot_id,batch_id 0 9 miss% 0.7113447328975564
plot_id,batch_id 0 10 miss% 0.5386367192330039
plot_id,batch_id 0 11 miss% 0.6669179025367526
plot_id,batch_id 0 12 miss% 0.6858855355258875
plot_id,batch_id 0 13 miss% 0.7024885413180766
plot_id,batch_id 0 14 miss% 0.7119750776460948
plot_id,batch_id 0 15 miss% 0.5260062134003768
plot_id,batch_id 0 16 miss% 0.6712251429474027
plot_id,batch_id 0 17 miss% 0.7030869454592417
plot_id,batch_id 0 18 miss% 0.70118115508479
plot_id,batch_id 0 19 miss% 0.703519315751699
plot_id,batch_id 0 20 miss% 0.6274390821922231
plot_id,batch_id 0 21 miss% 0.697929984355485
plot_id,batch_id 0 22 miss% 0.7088725236877739
plot_id,batch_id 0 23 miss% 0.7168823470983774
plot_id,batch_id 0 24 miss% 0.7209066344994169
plot_id,batch_id 0 25 miss% 0.6131112916608573
plot_id,batch_id 0 26 miss% 0.694792489622084
plot_id,batch_id 0 27 miss% 0.7091675775486848
plot_id,batch_id 0 28 miss% 0.7169120213036728
plot_id,batch_id 0 29 miss% 0.7209032089478342
plot_id,batch_id 0 30 miss% 0.6091664873675849
plot_id,batch_id 0 31 miss% 0.6938181208812686
plot_id,batch_id 0 32 miss% 0.7045759316381756
plot_id,batch_id 0 33 miss% 0.7106499682647306
plot_id,batch_id 0 34 miss% 0.71360777034474
plot_id,batch_id 0 35 miss% 0.5950977975048172
plot_id,batch_id 0 36 miss% 0.6955629087428121
plot_id,batch_id 0 37 miss% 0.7015692971069345
plot_id,batch_id 0 38 miss% 0.7104143231381497
plot_id,batch_id 0 39 miss% 0.7117683032876613
plot_id,batch_id 0 40 miss% 0.6666299241766983
plot_id,batch_id 0 41 miss% 0.7092936705201932
plot_id,batch_id 0 42 miss% 0.7114291575298067
plot_id,batch_id 0 43 miss% 0.7227702632749601
plot_id,batch_id 0 44 miss% 0.7302401675511169
plot_id,batch_id 0 45 miss% 0.6612631786756782
plot_id,batch_id 0 46 miss% 0.7118865313876259
plot_id,batch_id 0 47 miss% 0.7131223711955819
plot_id,batch_id 0 48 miss% 0.7209697255790959
plot_id,batch_id 0 49 miss% 0.7289526802531296
plot_id,batch_id 0 50 miss% 0.6744205263980563
plot_id,batch_id 0 51 miss% 0.7026656276890032
plot_id,batch_id 0 52 miss% 0.7139691660513552
plot_id,batch_id 0 53 miss% 0.7201582879247578
plot_id,batch_id 0 54 miss% 0.7316999494275962
plot_id,batch_id 0 55 miss% 0.6722268256913099
plot_id,batch_id 0 56 miss% 0.7072072838181429
plot_id,batch_id 0 57 miss% 0.7139223395370174
plot_id,batch_id 0 58 miss% 0.7219826061936783
plot_id,batch_id 0 59 miss% 0.7198562559022121
plot_id,batch_id 0 60 miss% 0.44855025788035346
plot_id,batch_id 0 61 miss% 0.6131431285107786
plot_id,batch_id 0 62 miss% 0.664277709604356
plot_id,batch_id 0 63 miss% 0.6857947368543682
plot_id,batch_id 0 64 miss% 0.6920488724310043
plot_id,batch_id 0 65 miss% 0.43222685352870555
plot_id,batch_id 0 66 miss% 0.6078043458237565
plot_id,batch_id 0 67 miss% 0.6467461055015616
plot_id,batch_id 0 68 miss% 0.6864984228666605
plot_id,batch_id 0 69 miss% 0.6910586100051475
plot_id,batch_id 0 70 miss% 0.4053884551355675
plot_id,batch_id 0 71 miss% 0.5905289366671808
plot_id,batch_id 0 72 miss% 0.631644181892186
plot_id,batch_id 0 73 miss% 0.6634374993486293
plot_id,batch_id 0 74 miss% 0.682458483365745
plot_id,batch_id 0 75 miss% 0.3993544598130065
plot_id,batch_id 0 76 miss% 0.571245687604778
plot_id,batch_id 0 77 miss% 0.6221017403348046
plot_id,batch_id 0 78 miss% 0.6645496065604259
plot_id,batch_id 0 79 miss% 0.6789289283486916
plot_id,batch_id 0 80 miss% 0.4757184440276106
plot_id,batch_id 0 81 miss% 0.6407281863129887
plot_id,batch_id 0 82 miss% 0.6766422392521163
plot_id,batch_id 0 83 miss% 0.6961705539181274
plot_id,batch_id 0 84 miss% 0.6960443893060005
plot_id,batch_id 0 85 miss% 0.47122349804924263
plot_id,batch_id 0 86 miss% 0.6379397428998491
plot_id,batch_id 0 87 miss% 0.6680363619529679
plot_id,batch_id 0 88 miss% 0.6912062447411124
plot_id,batch_id 0 89 miss% 0.7025974654973115
plot_id,batch_id 0 90 miss% 0.4415395822658182
plot_id,batch_id 0 91 miss% 0.6339665702332012
plot_id,batch_id 0 92 miss% 0.6707881610850223
plot_id,batch_id 0 93 miss% 0.687582558681751
plot_id,batch_id 0 94 miss% 0.6974140500780925
plot_id,batch_id 0 95 miss% 0.43422620165059456
plot_id,batch_id 0 96 miss% 0.6106462588585805
plot_id,batch_id 0 97 miss% 0.6558588903404821
plot_id,batch_id 0 98 miss% 0.6769014621546817
plot_id,batch_id 0 99 miss% 0.685059367107816
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.54976796 0.68230155 0.69399647 0.71005626 0.70923723 0.55877198
 0.67119238 0.69800728 0.70276377 0.71134473 0.53863672 0.6669179
 0.68588554 0.70248854 0.71197508 0.52600621 0.67122514 0.70308695
 0.70118116 0.70351932 0.62743908 0.69792998 0.70887252 0.71688235
 0.72090663 0.61311129 0.69479249 0.70916758 0.71691202 0.72090321
 0.60916649 0.69381812 0.70457593 0.71064997 0.71360777 0.5950978
 0.69556291 0.7015693  0.71041432 0.7117683  0.66662992 0.70929367
 0.71142916 0.72277026 0.73024017 0.66126318 0.71188653 0.71312237
 0.72096973 0.72895268 0.67442053 0.70266563 0.71396917 0.72015829
 0.73169995 0.67222683 0.70720728 0.71392234 0.72198261 0.71985626
 0.44855026 0.61314313 0.66427771 0.68579474 0.69204887 0.43222685
 0.60780435 0.64674611 0.68649842 0.69105861 0.40538846 0.59052894
 0.63164418 0.6634375  0.68245848 0.39935446 0.57124569 0.62210174
 0.66454961 0.67892893 0.47571844 0.64072819 0.67664224 0.69617055
 0.69604439 0.4712235  0.63793974 0.66803636 0.69120624 0.70259747
 0.44153958 0.63396657 0.67078816 0.68758256 0.69741405 0.4342262
 0.61064626 0.65585889 0.67690146 0.68505937]
for model  189 the mean error 0.6585425401866458
all id 189 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 31 out win 3 err 0.6585425401866458
Launcher: Job 190 completed in 4721 seconds.
Launcher: Task 235 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  21969
Epoch:0, Train loss:0.480606, valid loss:0.440352
Epoch:1, Train loss:0.224252, valid loss:0.229336
Epoch:2, Train loss:0.218522, valid loss:0.229238
Epoch:3, Train loss:0.217844, valid loss:0.229146
Epoch:4, Train loss:0.217479, valid loss:0.228656
Epoch:5, Train loss:0.217207, valid loss:0.228725
Epoch:6, Train loss:0.217091, valid loss:0.228232
Epoch:7, Train loss:0.216852, valid loss:0.228248
Epoch:8, Train loss:0.216788, valid loss:0.228530
Epoch:9, Train loss:0.216729, valid loss:0.228395
Epoch:10, Train loss:0.216721, valid loss:0.228128
Epoch:11, Train loss:0.215898, valid loss:0.227838
Epoch:12, Train loss:0.215881, valid loss:0.227915
Epoch:13, Train loss:0.215890, valid loss:0.227816
Epoch:14, Train loss:0.215841, valid loss:0.227754
Epoch:15, Train loss:0.215833, valid loss:0.227907
Epoch:16, Train loss:0.215812, valid loss:0.227702
Epoch:17, Train loss:0.215785, valid loss:0.227895
Epoch:18, Train loss:0.215781, valid loss:0.227804
Epoch:19, Train loss:0.215756, valid loss:0.227816
Epoch:20, Train loss:0.215751, valid loss:0.227796
Epoch:21, Train loss:0.215383, valid loss:0.227631
Epoch:22, Train loss:0.215410, valid loss:0.227522
Epoch:23, Train loss:0.215402, valid loss:0.227585
Epoch:24, Train loss:0.215377, valid loss:0.227591
Epoch:25, Train loss:0.215393, valid loss:0.227667
Epoch:26, Train loss:0.215372, valid loss:0.227561
Epoch:27, Train loss:0.215361, valid loss:0.227615
Epoch:28, Train loss:0.215366, valid loss:0.227576
Epoch:29, Train loss:0.215345, valid loss:0.227555
Epoch:30, Train loss:0.215363, valid loss:0.227693
Epoch:31, Train loss:0.215170, valid loss:0.227461
Epoch:32, Train loss:0.215174, valid loss:0.227577
Epoch:33, Train loss:0.215176, valid loss:0.227469
Epoch:34, Train loss:0.215161, valid loss:0.227590
Epoch:35, Train loss:0.215165, valid loss:0.227473
Epoch:36, Train loss:0.215156, valid loss:0.227444
Epoch:37, Train loss:0.215162, valid loss:0.227528
Epoch:38, Train loss:0.215151, valid loss:0.227562
Epoch:39, Train loss:0.215152, valid loss:0.227483
Epoch:40, Train loss:0.215152, valid loss:0.227434
Epoch:41, Train loss:0.215052, valid loss:0.227412
Epoch:42, Train loss:0.215049, valid loss:0.227469
Epoch:43, Train loss:0.215054, valid loss:0.227483
Epoch:44, Train loss:0.215042, valid loss:0.227494
Epoch:45, Train loss:0.215049, valid loss:0.227443
Epoch:46, Train loss:0.215051, valid loss:0.227415
Epoch:47, Train loss:0.215044, valid loss:0.227453
Epoch:48, Train loss:0.215039, valid loss:0.227455
Epoch:49, Train loss:0.215041, valid loss:0.227426
Epoch:50, Train loss:0.215034, valid loss:0.227439
Epoch:51, Train loss:0.214988, valid loss:0.227412
Epoch:52, Train loss:0.214987, valid loss:0.227431
Epoch:53, Train loss:0.214986, valid loss:0.227443
Epoch:54, Train loss:0.214987, valid loss:0.227407
Epoch:55, Train loss:0.214986, valid loss:0.227427
Epoch:56, Train loss:0.214983, valid loss:0.227431
Epoch:57, Train loss:0.214982, valid loss:0.227436
Epoch:58, Train loss:0.214983, valid loss:0.227422
Epoch:59, Train loss:0.214983, valid loss:0.227449
Epoch:60, Train loss:0.214980, valid loss:0.227408
training time 4547.502815961838
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.5517362574533138
plot_id,batch_id 0 1 miss% 0.678694090106769
plot_id,batch_id 0 2 miss% 0.6944236321312247
plot_id,batch_id 0 3 miss% 0.711487444823374
plot_id,batch_id 0 4 miss% 0.7090200942308879
plot_id,batch_id 0 5 miss% 0.5615976113584888
plot_id,batch_id 0 6 miss% 0.6734329908160849
plot_id,batch_id 0 7 miss% 0.6979865627397306
plot_id,batch_id 0 8 miss% 0.7044874416170379
plot_id,batch_id 0 9 miss% 0.711171409484977
plot_id,batch_id 0 10 miss% 0.5305891837206727
plot_id,batch_id 0 11 miss% 0.6694589974340567
plot_id,batch_id 0 12 miss% 0.6914577893293228
plot_id,batch_id 0 13 miss% 0.7023178357836596
plot_id,batch_id 0 14 miss% 0.7089327598152133
plot_id,batch_id 0 15 miss% 0.536298880088157
plot_id,batch_id 0 16 miss% 0.6697186458489178
plot_id,batch_id 0 17 miss% 0.6997046804034678
plot_id,batch_id 0 18 miss% 0.7032435692531722
plot_id,batch_id 0 19 miss% 0.7065580896404059
plot_id,batch_id 0 20 miss% 0.6311103783121836
plot_id,batch_id 0 21 miss% 0.6994847095783545
plot_id,batch_id 0 22 miss% 0.7125964340367157
plot_id,batch_id 0 23 miss% 0.7179626947419975
plot_id,batch_id 0 24 miss% 0.7221454312971611
plot_id,batch_id 0 25 miss% 0.6082609248048495
plot_id,batch_id 0 26 miss% 0.6931931189279855
plot_id,batch_id 0 27 miss% 0.7077035711073961
plot_id,batch_id 0 28 miss% 0.7161663003334935
plot_id,batch_id 0 29 miss% 0.7209513354313042
plot_id,batch_id 0 30 miss% 0.6101860754540569
plot_id,batch_id 0 31 miss% 0.6865643581980774
plot_id,batch_id 0 32 miss% 0.7046771059806392
plot_id,batch_id 0 33 miss% 0.7106303947966676
plot_id,batch_id 0 34 miss% 0.7123001780129543
plot_id,batch_id 0 35 miss% 0.5912463131674366
plot_id,batch_id 0 36 miss% 0.6970827049552669
plot_id,batch_id 0 37 miss% 0.7020322794383396
plot_id,batch_id 0 38 miss% 0.7132299408171283
plot_id,batch_id 0 39 miss% 0.7159109816469064
plot_id,batch_id 0 40 miss% 0.6672126854788668
plot_id,batch_id 0 41 miss% 0.7095041795115586
plot_id,batch_id 0 42 miss% 0.7164371875353619
plot_id,batch_id 0 43 miss% 0.722887904232756
plot_id,batch_id 0 44 miss% 0.7303720674202003
plot_id,batch_id 0 45 miss% 0.65374430761877
plot_id,batch_id 0 46 miss% 0.7085094591485562
plot_id,batch_id 0 47 miss% 0.7150539070880907
plot_id,batch_id 0 48 miss% 0.7219454821499327
plot_id,batch_id 0 49 miss% 0.7279271365875345
plot_id,batch_id 0 50 miss% 0.670850953705205
plot_id,batch_id 0 51 miss% 0.7053994602679009
plot_id,batch_id 0 52 miss% 0.7138190655696074
plot_id,batch_id 0 53 miss% 0.7203441381919762
plot_id,batch_id 0 54 miss% 0.7307618020927702
plot_id,batch_id 0 55 miss% 0.6788672244009367
plot_id,batch_id 0 56 miss% 0.7065147206424289
plot_id,batch_id 0 57 miss% 0.7141571561852204
plot_id,batch_id 0 58 miss% 0.7214151097071207
plot_id,batch_id 0 59 miss% 0.7194640973990118
plot_id,batch_id 0 60 miss% 0.4448392959664929
plot_id,batch_id 0 61 miss% 0.6236044900950517
plot_id,batch_id 0 62 miss% 0.6693306985707435
plot_id,batch_id 0 63 miss% 0.6903054687949504
plot_id,batch_id 0 64 miss% 0.6962511380408997
plot_id,batch_id 0 65 miss% 0.43687798251105053
plot_id,batch_id 0 66 miss% 0.6144179798298967
plot_id,batch_id 0 67 miss% 0.6477843381762088
plot_id,batch_id 0 68 miss% 0.6896615343660137
plot_id,batch_id 0 69 miss% 0.6948705286928355
plot_id,batch_id 0 70 miss% 0.4095081203303387
plot_id,batch_id 0 71 miss% 0.589830231777064
plot_id,batch_id 0 72 miss% 0.6294620834881971
plot_id,batch_id 0 73 miss% 0.6673936317795617
plot_id,batch_id 0 74 miss% 0.6919582584824627
plot_id,batch_id 0 75 miss% 0.39869063822304823
plot_id,batch_id 0 76 miss% 0.5675664274189517
plot_id,batch_id 0 77 miss% 0.6258315966232727
plot_id,batch_id 0 78 miss% 0.6670490371409123
plot_id,batch_id 0 79 miss% 0.6735417643642948
plot_id,batch_id 0 80 miss% 0.4757913016971338
plot_id,batch_id 0 81 miss% 0.6403340045631899
plot_id,batch_id 0 82 miss% 0.6780859957864651
plot_id,batch_id 0 83 miss% 0.6986417599888308
plot_id,batch_id 0 84 miss% 0.6989465311735975
plot_id,batch_id 0 85 miss% 0.4719578598566127
plot_id,batch_id 0 86 miss% 0.6376160528362087
plot_id,batch_id 0 87 miss% 0.6697213003595627
plot_id,batch_id 0 88 miss% 0.6945173533267149
plot_id,batch_id 0 89 miss% 0.6989872129904683
plot_id,batch_id 0 90 miss% 0.4469334987746364
plot_id,batch_id 0 91 miss% 0.6354010649375387
plot_id,batch_id 0 92 miss% 0.6652003539502013
plot_id,batch_id 0 93 miss% 0.6931338493079897
plot_id,batch_id 0 94 miss% 0.7017963019245059
plot_id,batch_id 0 95 miss% 0.4429195215788004
plot_id,batch_id 0 96 miss% 0.6153225393984009
plot_id,batch_id 0 97 miss% 0.661284224432519
plot_id,batch_id 0 98 miss% 0.6784581920100985
plot_id,batch_id 0 99 miss% 0.6850878713826468
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.55173626 0.67869409 0.69442363 0.71148744 0.70902009 0.56159761
 0.67343299 0.69798656 0.70448744 0.71117141 0.53058918 0.669459
 0.69145779 0.70231784 0.70893276 0.53629888 0.66971865 0.69970468
 0.70324357 0.70655809 0.63111038 0.69948471 0.71259643 0.71796269
 0.72214543 0.60826092 0.69319312 0.70770357 0.7161663  0.72095134
 0.61018608 0.68656436 0.70467711 0.71063039 0.71230018 0.59124631
 0.6970827  0.70203228 0.71322994 0.71591098 0.66721269 0.70950418
 0.71643719 0.7228879  0.73037207 0.65374431 0.70850946 0.71505391
 0.72194548 0.72792714 0.67085095 0.70539946 0.71381907 0.72034414
 0.7307618  0.67886722 0.70651472 0.71415716 0.72141511 0.7194641
 0.4448393  0.62360449 0.6693307  0.69030547 0.69625114 0.43687798
 0.61441798 0.64778434 0.68966153 0.69487053 0.40950812 0.58983023
 0.62946208 0.66739363 0.69195826 0.39869064 0.56756643 0.6258316
 0.66704904 0.67354176 0.4757913  0.640334   0.678086   0.69864176
 0.69894653 0.47195786 0.63761605 0.6697213  0.69451735 0.69898721
 0.4469335  0.63540106 0.66520035 0.69313385 0.7017963  0.44291952
 0.61532254 0.66128422 0.67845819 0.68508787]
for model  216 the mean error 0.6595985327700002
all id 216 hidden_dim 16 learning_rate 0.02 num_layers 3 frames 31 out win 3 err 0.6595985327700002
Launcher: Job 217 completed in 4723 seconds.
Launcher: Task 74 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  21969
Epoch:0, Train loss:0.698731, valid loss:0.666920
Epoch:1, Train loss:0.515765, valid loss:0.520935
Epoch:2, Train loss:0.502827, valid loss:0.520687
Epoch:3, Train loss:0.500543, valid loss:0.519531
Epoch:4, Train loss:0.499330, valid loss:0.518749
Epoch:5, Train loss:0.498373, valid loss:0.518279
Epoch:6, Train loss:0.497413, valid loss:0.517755
Epoch:7, Train loss:0.497485, valid loss:0.518177
Epoch:8, Train loss:0.496911, valid loss:0.517732
Epoch:9, Train loss:0.496667, valid loss:0.517228
Epoch:10, Train loss:0.496212, valid loss:0.517444
Epoch:11, Train loss:0.494545, valid loss:0.516902
Epoch:12, Train loss:0.494578, valid loss:0.516613
Epoch:13, Train loss:0.494589, valid loss:0.516596
Epoch:14, Train loss:0.494223, valid loss:0.516809
Epoch:15, Train loss:0.494233, valid loss:0.516291
Epoch:16, Train loss:0.494155, valid loss:0.516333
Epoch:17, Train loss:0.494125, valid loss:0.517426
Epoch:18, Train loss:0.494338, valid loss:0.516562
Epoch:19, Train loss:0.494087, valid loss:0.516648
Epoch:20, Train loss:0.493939, valid loss:0.516211
Epoch:21, Train loss:0.493129, valid loss:0.516045
Epoch:22, Train loss:0.493152, valid loss:0.516144
Epoch:23, Train loss:0.493186, valid loss:0.516184
Epoch:24, Train loss:0.493173, valid loss:0.515936
Epoch:25, Train loss:0.493167, valid loss:0.515984
Epoch:26, Train loss:0.493165, valid loss:0.515879
Epoch:27, Train loss:0.493056, valid loss:0.516072
Epoch:28, Train loss:0.493176, valid loss:0.515905
Epoch:29, Train loss:0.492998, valid loss:0.515819
Epoch:30, Train loss:0.493365, valid loss:0.516095
Epoch:31, Train loss:0.492598, valid loss:0.515740
Epoch:32, Train loss:0.492584, valid loss:0.515742
Epoch:33, Train loss:0.492587, valid loss:0.515712
Epoch:34, Train loss:0.492561, valid loss:0.515937
Epoch:35, Train loss:0.492567, valid loss:0.515813
Epoch:36, Train loss:0.492573, valid loss:0.515651
Epoch:37, Train loss:0.492541, valid loss:0.516024
Epoch:38, Train loss:0.492570, valid loss:0.515623
Epoch:39, Train loss:0.492502, valid loss:0.515769
Epoch:40, Train loss:0.492534, valid loss:0.515790
Epoch:41, Train loss:0.492317, valid loss:0.515582
Epoch:42, Train loss:0.492307, valid loss:0.515577
Epoch:43, Train loss:0.492310, valid loss:0.515614
Epoch:44, Train loss:0.492297, valid loss:0.515570
Epoch:45, Train loss:0.492308, valid loss:0.515602
Epoch:46, Train loss:0.492293, valid loss:0.515605
Epoch:47, Train loss:0.492289, valid loss:0.515592
Epoch:48, Train loss:0.492263, valid loss:0.515589
Epoch:49, Train loss:0.492272, valid loss:0.515602
Epoch:50, Train loss:0.492269, valid loss:0.515603
Epoch:51, Train loss:0.492163, valid loss:0.515611
Epoch:52, Train loss:0.492156, valid loss:0.515556
Epoch:53, Train loss:0.492151, valid loss:0.515546
Epoch:54, Train loss:0.492156, valid loss:0.515669
Epoch:55, Train loss:0.492148, valid loss:0.515527
Epoch:56, Train loss:0.492139, valid loss:0.515604
Epoch:57, Train loss:0.492141, valid loss:0.515537
Epoch:58, Train loss:0.492138, valid loss:0.515552
Epoch:59, Train loss:0.492127, valid loss:0.515505
Epoch:60, Train loss:0.492128, valid loss:0.515529
training time 4559.145123004913
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.820429573588347
plot_id,batch_id 0 1 miss% 0.8560737383545401
plot_id,batch_id 0 2 miss% 0.8610485444611544
plot_id,batch_id 0 3 miss% 0.8660214840266858
plot_id,batch_id 0 4 miss% 0.8706827731615625
plot_id,batch_id 0 5 miss% 0.8127647441546935
plot_id,batch_id 0 6 miss% 0.8532060503052381
plot_id,batch_id 0 7 miss% 0.8626199530151306
plot_id,batch_id 0 8 miss% 0.8629574107722282
plot_id,batch_id 0 9 miss% 0.8664693815713063
plot_id,batch_id 0 10 miss% 0.8056757183097708
plot_id,batch_id 0 11 miss% 0.8508725748983801
plot_id,batch_id 0 12 miss% 0.8613337425543659
plot_id,batch_id 0 13 miss% 0.8624444668636289
plot_id,batch_id 0 14 miss% 0.8662214214404478
plot_id,batch_id 0 15 miss% 0.8101181492284734
plot_id,batch_id 0 16 miss% 0.8488018937859555
plot_id,batch_id 0 17 miss% 0.8576932805456567
plot_id,batch_id 0 18 miss% 0.8621165935624518
plot_id,batch_id 0 19 miss% 0.8631336135717936
plot_id,batch_id 0 20 miss% 0.8514677697585123
plot_id,batch_id 0 21 miss% 0.8619861094618391
plot_id,batch_id 0 22 miss% 0.8660143758697216
plot_id,batch_id 0 23 miss% 0.8689613374004614
plot_id,batch_id 0 24 miss% 0.8708577606419422
plot_id,batch_id 0 25 miss% 0.837765320665295
plot_id,batch_id 0 26 miss% 0.8582794179133393
plot_id,batch_id 0 27 miss% 0.8637133258945877
plot_id,batch_id 0 28 miss% 0.8681899146337015
plot_id,batch_id 0 29 miss% 0.8698596301125973
plot_id,batch_id 0 30 miss% 0.8274376432952225
plot_id,batch_id 0 31 miss% 0.8558976446855977
plot_id,batch_id 0 32 miss% 0.8622957885810294
plot_id,batch_id 0 33 miss% 0.8635067309577322
plot_id,batch_id 0 34 miss% 0.8651561627540868
plot_id,batch_id 0 35 miss% 0.8269422207279259
plot_id,batch_id 0 36 miss% 0.8614625481664686
plot_id,batch_id 0 37 miss% 0.8627274992298375
plot_id,batch_id 0 38 miss% 0.8653627876481017
plot_id,batch_id 0 39 miss% 0.8651178018043642
plot_id,batch_id 0 40 miss% 0.8499281654523739
plot_id,batch_id 0 41 miss% 0.8624320621661087
plot_id,batch_id 0 42 miss% 0.864187914642717
plot_id,batch_id 0 43 miss% 0.8693035194762319
plot_id,batch_id 0 44 miss% 0.8691887623655011
plot_id,batch_id 0 45 miss% 0.8460220602222424
plot_id,batch_id 0 46 miss% 0.8623753457344324
plot_id,batch_id 0 47 miss% 0.8660496361683632
plot_id,batch_id 0 48 miss% 0.8708195512907855
plot_id,batch_id 0 49 miss% 0.8686458374970745
plot_id,batch_id 0 50 miss% 0.85191779927363
plot_id,batch_id 0 51 miss% 0.8615125186740148
plot_id,batch_id 0 52 miss% 0.8665118176578812
plot_id,batch_id 0 53 miss% 0.8695209388722988
plot_id,batch_id 0 54 miss% 0.870039121344746
plot_id,batch_id 0 55 miss% 0.8491397289277136
plot_id,batch_id 0 56 miss% 0.8640921822962229
plot_id,batch_id 0 57 miss% 0.8666064394463489
plot_id,batch_id 0 58 miss% 0.8697844351003147
plot_id,batch_id 0 59 miss% 0.8687389239679355
plot_id,batch_id 0 60 miss% 0.7757355742868193
plot_id,batch_id 0 61 miss% 0.8384463548735176
plot_id,batch_id 0 62 miss% 0.854701367811709
plot_id,batch_id 0 63 miss% 0.8574762851158002
plot_id,batch_id 0 64 miss% 0.8610347090732926
plot_id,batch_id 0 65 miss% 0.7660221933959174
plot_id,batch_id 0 66 miss% 0.834868360139401
plot_id,batch_id 0 67 miss% 0.8422916045720594
plot_id,batch_id 0 68 miss% 0.8606156303777547
plot_id,batch_id 0 69 miss% 0.8574297719346106
plot_id,batch_id 0 70 miss% 0.7263837456801363
plot_id,batch_id 0 71 miss% 0.8341060980767695
plot_id,batch_id 0 72 miss% 0.8402510189428426
plot_id,batch_id 0 73 miss% 0.8475043187771933
plot_id,batch_id 0 74 miss% 0.855360634432855
plot_id,batch_id 0 75 miss% 0.7606918461864194
plot_id,batch_id 0 76 miss% 0.8170592424294268
plot_id,batch_id 0 77 miss% 0.8288070198576051
plot_id,batch_id 0 78 miss% 0.8451460622262161
plot_id,batch_id 0 79 miss% 0.8650740531585123
plot_id,batch_id 0 80 miss% 0.792405067487837
plot_id,batch_id 0 81 miss% 0.8496573777576604
plot_id,batch_id 0 82 miss% 0.857892121862141
plot_id,batch_id 0 83 miss% 0.8605348058528924
plot_id,batch_id 0 84 miss% 0.864815838422588
plot_id,batch_id 0 85 miss% 0.7804062706315342
plot_id,batch_id 0 86 miss% 0.8414468009665128
plot_id,batch_id 0 87 miss% 0.8539002300171024
plot_id,batch_id 0 88 miss% 0.8624778948554194
plot_id,batch_id 0 89 miss% 0.8646196302152565
plot_id,batch_id 0 90 miss% 0.7534514859827849
plot_id,batch_id 0 91 miss% 0.8355436113908314
plot_id,batch_id 0 92 miss% 0.8471236857754313
plot_id,batch_id 0 93 miss% 0.8491821038639015
plot_id,batch_id 0 94 miss% 0.8641666302672512
plot_id,batch_id 0 95 miss% 0.7683175752640763
plot_id,batch_id 0 96 miss% 0.8311200165569139
plot_id,batch_id 0 97 miss% 0.8467809822788275
plot_id,batch_id 0 98 miss% 0.8542187334722917
plot_id,batch_id 0 99 miss% 0.8555630450798153
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.82042957 0.85607374 0.86104854 0.86602148 0.87068277 0.81276474
 0.85320605 0.86261995 0.86295741 0.86646938 0.80567572 0.85087257
 0.86133374 0.86244447 0.86622142 0.81011815 0.84880189 0.85769328
 0.86211659 0.86313361 0.85146777 0.86198611 0.86601438 0.86896134
 0.87085776 0.83776532 0.85827942 0.86371333 0.86818991 0.86985963
 0.82743764 0.85589764 0.86229579 0.86350673 0.86515616 0.82694222
 0.86146255 0.8627275  0.86536279 0.8651178  0.84992817 0.86243206
 0.86418791 0.86930352 0.86918876 0.84602206 0.86237535 0.86604964
 0.87081955 0.86864584 0.8519178  0.86151252 0.86651182 0.86952094
 0.87003912 0.84913973 0.86409218 0.86660644 0.86978444 0.86873892
 0.77573557 0.83844635 0.85470137 0.85747629 0.86103471 0.76602219
 0.83486836 0.8422916  0.86061563 0.85742977 0.72638375 0.8341061
 0.84025102 0.84750432 0.85536063 0.76069185 0.81705924 0.82880702
 0.84514606 0.86507405 0.79240507 0.84965738 0.85789212 0.86053481
 0.86481584 0.78040627 0.8414468  0.85390023 0.86247789 0.86461963
 0.75345149 0.83554361 0.84712369 0.8491821  0.86416663 0.76831758
 0.83112002 0.84678098 0.85421873 0.85556305]
for model  56 the mean error 0.8476313345830104
all id 56 hidden_dim 16 learning_rate 0.02 num_layers 3 frames 21 out win 5 err 0.8476313345830104
Launcher: Job 57 completed in 4741 seconds.
Launcher: Task 97 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  28945
Epoch:0, Train loss:0.484413, valid loss:0.483743
Epoch:1, Train loss:0.036214, valid loss:0.006190
Epoch:2, Train loss:0.009830, valid loss:0.004341
Epoch:3, Train loss:0.006473, valid loss:0.003118
Epoch:4, Train loss:0.005111, valid loss:0.002646
Epoch:5, Train loss:0.004650, valid loss:0.002940
Epoch:6, Train loss:0.004211, valid loss:0.002358
Epoch:7, Train loss:0.003997, valid loss:0.002218
Epoch:8, Train loss:0.003966, valid loss:0.002104
Epoch:9, Train loss:0.003826, valid loss:0.002163
Epoch:10, Train loss:0.003725, valid loss:0.002419
Epoch:11, Train loss:0.002559, valid loss:0.001414
Epoch:12, Train loss:0.002498, valid loss:0.001378
Epoch:13, Train loss:0.002529, valid loss:0.001213
Epoch:14, Train loss:0.002471, valid loss:0.001394
Epoch:15, Train loss:0.002406, valid loss:0.001426
Epoch:16, Train loss:0.002348, valid loss:0.001453
Epoch:17, Train loss:0.002320, valid loss:0.001146
Epoch:18, Train loss:0.002315, valid loss:0.001373
Epoch:19, Train loss:0.002312, valid loss:0.001315
Epoch:20, Train loss:0.002201, valid loss:0.001311
Epoch:21, Train loss:0.001708, valid loss:0.001056
Epoch:22, Train loss:0.001729, valid loss:0.001115
Epoch:23, Train loss:0.001775, valid loss:0.001031
Epoch:24, Train loss:0.001681, valid loss:0.001107
Epoch:25, Train loss:0.001718, valid loss:0.001431
Epoch:26, Train loss:0.001703, valid loss:0.001223
Epoch:27, Train loss:0.001664, valid loss:0.000910
Epoch:28, Train loss:0.001667, valid loss:0.000875
Epoch:29, Train loss:0.001659, valid loss:0.001120
Epoch:30, Train loss:0.001624, valid loss:0.001046
Epoch:31, Train loss:0.001359, valid loss:0.000862
Epoch:32, Train loss:0.001364, valid loss:0.000857
Epoch:33, Train loss:0.001359, valid loss:0.000800
Epoch:34, Train loss:0.001367, valid loss:0.000849
Epoch:35, Train loss:0.001352, valid loss:0.000816
Epoch:36, Train loss:0.001338, valid loss:0.000843
Epoch:37, Train loss:0.001339, valid loss:0.000810
Epoch:38, Train loss:0.001328, valid loss:0.000939
Epoch:39, Train loss:0.001310, valid loss:0.000837
Epoch:40, Train loss:0.001355, valid loss:0.000782
Epoch:41, Train loss:0.001189, valid loss:0.000784
Epoch:42, Train loss:0.001191, valid loss:0.000736
Epoch:43, Train loss:0.001192, valid loss:0.000739
Epoch:44, Train loss:0.001171, valid loss:0.000747
Epoch:45, Train loss:0.001180, valid loss:0.000765
Epoch:46, Train loss:0.001177, valid loss:0.000743
Epoch:47, Train loss:0.001166, valid loss:0.000750
Epoch:48, Train loss:0.001179, valid loss:0.000773
Epoch:49, Train loss:0.001159, valid loss:0.000754
Epoch:50, Train loss:0.001158, valid loss:0.000833
Epoch:51, Train loss:0.001092, valid loss:0.000704
Epoch:52, Train loss:0.001097, valid loss:0.000703
Epoch:53, Train loss:0.001093, valid loss:0.000690
Epoch:54, Train loss:0.001091, valid loss:0.000690
Epoch:55, Train loss:0.001091, valid loss:0.000711
Epoch:56, Train loss:0.001088, valid loss:0.000693
Epoch:57, Train loss:0.001086, valid loss:0.000700
Epoch:58, Train loss:0.001086, valid loss:0.000697
Epoch:59, Train loss:0.001079, valid loss:0.000689
Epoch:60, Train loss:0.001079, valid loss:0.000683
training time 4591.073976755142
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.026741854403431412
plot_id,batch_id 0 1 miss% 0.01963686860801837
plot_id,batch_id 0 2 miss% 0.025802591017078595
plot_id,batch_id 0 3 miss% 0.024762939563176666
plot_id,batch_id 0 4 miss% 0.02597281906884471
plot_id,batch_id 0 5 miss% 0.030192727930368965
plot_id,batch_id 0 6 miss% 0.028735730529510305
plot_id,batch_id 0 7 miss% 0.037473736601915913
plot_id,batch_id 0 8 miss% 0.027967290004721812
plot_id,batch_id 0 9 miss% 0.018531779347384612
plot_id,batch_id 0 10 miss% 0.047026793338077945
plot_id,batch_id 0 11 miss% 0.04619818827291908
plot_id,batch_id 0 12 miss% 0.03989745794233438
plot_id,batch_id 0 13 miss% 0.021633022462565845
plot_id,batch_id 0 14 miss% 0.03465455904266952
plot_id,batch_id 0 15 miss% 0.053005334233329975
plot_id,batch_id 0 16 miss% 0.043474270020540925
plot_id,batch_id 0 17 miss% 0.07087430611417576
plot_id,batch_id 0 18 miss% 0.03144045838851614
plot_id,batch_id 0 19 miss% 0.040703872226638196
plot_id,batch_id 0 20 miss% 0.044534381593828797
plot_id,batch_id 0 21 miss% 0.024459464047657118
plot_id,batch_id 0 22 miss% 0.03417863238893932
plot_id,batch_id 0 23 miss% 0.0260366296003866
plot_id,batch_id 0 24 miss% 0.02503243997060714
plot_id,batch_id 0 25 miss% 0.038969566303537206
plot_id,batch_id 0 26 miss% 0.029035121685457396
plot_id,batch_id 0 27 miss% 0.0249679609684478
plot_id,batch_id 0 28 miss% 0.023132150324422455
plot_id,batch_id 0 29 miss% 0.01857016398864681
plot_id,batch_id 0 30 miss% 0.04486231414275434
plot_id,batch_id 0 31 miss% 0.028045773091640032
plot_id,batch_id 0 32 miss% 0.021742178162798605
plot_id,batch_id 0 33 miss% 0.026471356927306063
plot_id,batch_id 0 34 miss% 0.024481482191254716
plot_id,batch_id 0 35 miss% 0.040990770861664416
plot_id,batch_id 0 36 miss% 0.03143955506665574
plot_id,batch_id 0 37 miss% 0.038412371874782573
plot_id,batch_id 0 38 miss% 0.02677281054209763
plot_id,batch_id 0 39 miss% 0.02225084011443735
plot_id,batch_id 0 40 miss% 0.059258561939577445
plot_id,batch_id 0 41 miss% 0.025346470425427686
plot_id,batch_id 0 42 miss% 0.013170980174697667
plot_id,batch_id 0 43 miss% 0.02918011792626547
plot_id,batch_id 0 44 miss% 0.017913173921705033
plot_id,batch_id 0 45 miss% 0.04161177229690755
plot_id,batch_id 0 46 miss% 0.023697958039393358
plot_id,batch_id 0 47 miss% 0.031058876072928895
plot_id,batch_id 0 48 miss% 0.019102991523070056
plot_id,batch_id 0 49 miss% 0.01313027810081964
plot_id,batch_id 0 50 miss% 0.03695425565913049
plot_id,batch_id 0 51 miss% 0.02481709586981772
plot_id,batch_id 0 52 miss% 0.016506288548175083
plot_id,batch_id 0 53 miss% 0.017321571528886782
plot_id,batch_id 0 54 miss% 0.030379799776821474
plot_id,batch_id 0 55 miss% 0.05147982602824346
plot_id,batch_id 0 56 miss% 0.027265837161548557
plot_id,batch_id 0 57 miss% 0.018076006095877825
plot_id,batch_id 0 58 miss% 0.02375912458172205
plot_id,batch_id 0 59 miss% 0.02765876470978838
plot_id,batch_id 0 60 miss% 0.06784320608227547
plot_id,batch_id 0 61 miss% 0.04111722151201726
plot_id,batch_id 0 62 miss% 0.024060284375660608
plot_id,batch_id 0 63 miss% 0.02167670273488603
plot_id,batch_id 0 64 miss% 0.0348858780337227
plot_id,batch_id 0 65 miss% 0.07044247832608387
plot_id,batch_id 0 66 miss% 0.03508474735350626
plot_id,batch_id 0 67 miss% 0.029832069731194136
plot_id,batch_id 0 68 miss% 0.03010079687506341
plot_id,batch_id 0 69 miss% 0.030466134872623955
plot_id,batch_id 0 70 miss% 0.050002084719505284
plot_id,batch_id 0 71 miss% 0.03381515117660781
plot_id,batch_id 0 72 miss% 0.03938858875917648
plot_id,batch_id 0 73 miss% 0.03671854828605592
plot_id,batch_id 0 74 miss% 0.04227627298416662
plot_id,batch_id 0 75 miss% 0.07251930585644488
plot_id,batch_id 0 76 miss% 0.05381684855071492
plot_id,batch_id 0 77 miss% 0.04004902468898797
plot_id,batch_id 0 78 miss% 0.04704622833347935
plot_id,batch_id 0 79 miss% 0.04838136649268438
plot_id,batch_id 0 80 miss% 0.05541246119776536
plot_id,batch_id 0 81 miss% 0.032695363186912046
plot_id,batch_id 0 82 miss% 0.036467398367139185
plot_id,batch_id 0 83 miss% 0.02527181111625248
plot_id,batch_id 0 84 miss% 0.04061769239511569
plot_id,batch_id 0 85 miss% 0.03392161139113604
plot_id,batch_id 0 86 miss% 0.01854020774086431
plot_id,batch_id 0 87 miss% 0.028957596324276746
plot_id,batch_id 0 88 miss% 0.03017107902588654
plot_id,batch_id 0 89 miss% 0.03161056329143554
plot_id,batch_id 0 90 miss% 0.03410614846321405
plot_id,batch_id 0 91 miss% 0.04847038274461571
plot_id,batch_id 0 92 miss% 0.0392323667125867
plot_id,batch_id 0 93 miss% 0.0362388937374036
plot_id,batch_id 0 94 miss% 0.04616435092923383
plot_id,batch_id 0 95 miss% 0.06785468021267904
plot_id,batch_id 0 96 miss% 0.03730451485105654
plot_id,batch_id 0 97 miss% 0.05177334049477979
plot_id,batch_id 0 98 miss% 0.027763303013876296
plot_id,batch_id 0 99 miss% 0.029142673349687898
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02674185 0.01963687 0.02580259 0.02476294 0.02597282 0.03019273
 0.02873573 0.03747374 0.02796729 0.01853178 0.04702679 0.04619819
 0.03989746 0.02163302 0.03465456 0.05300533 0.04347427 0.07087431
 0.03144046 0.04070387 0.04453438 0.02445946 0.03417863 0.02603663
 0.02503244 0.03896957 0.02903512 0.02496796 0.02313215 0.01857016
 0.04486231 0.02804577 0.02174218 0.02647136 0.02448148 0.04099077
 0.03143956 0.03841237 0.02677281 0.02225084 0.05925856 0.02534647
 0.01317098 0.02918012 0.01791317 0.04161177 0.02369796 0.03105888
 0.01910299 0.01313028 0.03695426 0.0248171  0.01650629 0.01732157
 0.0303798  0.05147983 0.02726584 0.01807601 0.02375912 0.02765876
 0.06784321 0.04111722 0.02406028 0.0216767  0.03488588 0.07044248
 0.03508475 0.02983207 0.0301008  0.03046613 0.05000208 0.03381515
 0.03938859 0.03671855 0.04227627 0.07251931 0.05381685 0.04004902
 0.04704623 0.04838137 0.05541246 0.03269536 0.0364674  0.02527181
 0.04061769 0.03392161 0.01854021 0.0289576  0.03017108 0.03161056
 0.03410615 0.04847038 0.03923237 0.03623889 0.04616435 0.06785468
 0.03730451 0.05177334 0.0277633  0.02914267]
for model  144 the mean error 0.03438041691635121
all id 144 hidden_dim 16 learning_rate 0.02 num_layers 4 frames 25 out win 3 err 0.03438041691635121
Launcher: Job 145 completed in 4786 seconds.
Launcher: Task 71 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  21969
Epoch:0, Train loss:0.569933, valid loss:0.533519
Epoch:1, Train loss:0.355571, valid loss:0.360350
Epoch:2, Train loss:0.344904, valid loss:0.359439
Epoch:3, Train loss:0.343252, valid loss:0.358517
Epoch:4, Train loss:0.342467, valid loss:0.359169
Epoch:5, Train loss:0.341988, valid loss:0.358541
Epoch:6, Train loss:0.341796, valid loss:0.358055
Epoch:7, Train loss:0.341519, valid loss:0.358062
Epoch:8, Train loss:0.341480, valid loss:0.358112
Epoch:9, Train loss:0.341171, valid loss:0.358461
Epoch:10, Train loss:0.341137, valid loss:0.357851
Epoch:11, Train loss:0.340255, valid loss:0.357509
Epoch:12, Train loss:0.340224, valid loss:0.357495
Epoch:13, Train loss:0.340224, valid loss:0.357390
Epoch:14, Train loss:0.340181, valid loss:0.357465
Epoch:15, Train loss:0.340147, valid loss:0.357362
Epoch:16, Train loss:0.340115, valid loss:0.357544
Epoch:17, Train loss:0.340120, valid loss:0.357324
Epoch:18, Train loss:0.340028, valid loss:0.357438
Epoch:19, Train loss:0.340063, valid loss:0.357663
Epoch:20, Train loss:0.339974, valid loss:0.357753
Epoch:21, Train loss:0.339594, valid loss:0.357303
Epoch:22, Train loss:0.339583, valid loss:0.357011
Epoch:23, Train loss:0.339558, valid loss:0.357093
Epoch:24, Train loss:0.339531, valid loss:0.357022
Epoch:25, Train loss:0.339527, valid loss:0.357071
Epoch:26, Train loss:0.339524, valid loss:0.357135
Epoch:27, Train loss:0.339532, valid loss:0.357092
Epoch:28, Train loss:0.339475, valid loss:0.357182
Epoch:29, Train loss:0.339522, valid loss:0.357535
Epoch:30, Train loss:0.339506, valid loss:0.357081
Epoch:31, Train loss:0.339251, valid loss:0.357006
Epoch:32, Train loss:0.339246, valid loss:0.356963
Epoch:33, Train loss:0.339245, valid loss:0.357023
Epoch:34, Train loss:0.339249, valid loss:0.357023
Epoch:35, Train loss:0.339243, valid loss:0.357014
Epoch:36, Train loss:0.339226, valid loss:0.357180
Epoch:37, Train loss:0.339231, valid loss:0.356939
Epoch:38, Train loss:0.339218, valid loss:0.356969
Epoch:39, Train loss:0.339221, valid loss:0.356984
Epoch:40, Train loss:0.339233, valid loss:0.356938
Epoch:41, Train loss:0.339104, valid loss:0.356916
Epoch:42, Train loss:0.339102, valid loss:0.356894
Epoch:43, Train loss:0.339097, valid loss:0.356896
Epoch:44, Train loss:0.339096, valid loss:0.356891
Epoch:45, Train loss:0.339089, valid loss:0.356925
Epoch:46, Train loss:0.339077, valid loss:0.356890
Epoch:47, Train loss:0.339080, valid loss:0.356871
Epoch:48, Train loss:0.339088, valid loss:0.356910
Epoch:49, Train loss:0.339073, valid loss:0.356930
Epoch:50, Train loss:0.339084, valid loss:0.356895
Epoch:51, Train loss:0.339018, valid loss:0.356861
Epoch:52, Train loss:0.339014, valid loss:0.356861
Epoch:53, Train loss:0.339012, valid loss:0.356854
Epoch:54, Train loss:0.339013, valid loss:0.356874
Epoch:55, Train loss:0.339008, valid loss:0.356874
Epoch:56, Train loss:0.339005, valid loss:0.356904
Epoch:57, Train loss:0.339015, valid loss:0.356870
Epoch:58, Train loss:0.339005, valid loss:0.356928
Epoch:59, Train loss:0.339008, valid loss:0.356864
Epoch:60, Train loss:0.339007, valid loss:0.356862
training time 4679.494873046875
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7275732973874924
plot_id,batch_id 0 1 miss% 0.789664888224927
plot_id,batch_id 0 2 miss% 0.8021066988143973
plot_id,batch_id 0 3 miss% 0.811270283984749
plot_id,batch_id 0 4 miss% 0.8075120085648962
plot_id,batch_id 0 5 miss% 0.7244012375722653
plot_id,batch_id 0 6 miss% 0.7836200119444967
plot_id,batch_id 0 7 miss% 0.7983289319987804
plot_id,batch_id 0 8 miss% 0.8054859912760537
plot_id,batch_id 0 9 miss% 0.8118403482764956
plot_id,batch_id 0 10 miss% 0.7032189163524715
plot_id,batch_id 0 11 miss% 0.7860920545857807
plot_id,batch_id 0 12 miss% 0.7951989930719039
plot_id,batch_id 0 13 miss% 0.8053907686812241
plot_id,batch_id 0 14 miss% 0.8119130053016765
plot_id,batch_id 0 15 miss% 0.7119976706327864
plot_id,batch_id 0 16 miss% 0.7779913001483606
plot_id,batch_id 0 17 miss% 0.7988300945996253
plot_id,batch_id 0 18 miss% 0.8052854314432488
plot_id,batch_id 0 19 miss% 0.8069209797505823
plot_id,batch_id 0 20 miss% 0.7708922897782098
plot_id,batch_id 0 21 miss% 0.8044409771489612
plot_id,batch_id 0 22 miss% 0.8127199970157394
plot_id,batch_id 0 23 miss% 0.8167437754203173
plot_id,batch_id 0 24 miss% 0.8191541097359835
plot_id,batch_id 0 25 miss% 0.758561383530514
plot_id,batch_id 0 26 miss% 0.7976669055618753
plot_id,batch_id 0 27 miss% 0.8044058375334603
plot_id,batch_id 0 28 miss% 0.8082119603357001
plot_id,batch_id 0 29 miss% 0.810108288276384
plot_id,batch_id 0 30 miss% 0.762325743670318
plot_id,batch_id 0 31 miss% 0.796121699286364
plot_id,batch_id 0 32 miss% 0.8051876257068677
plot_id,batch_id 0 33 miss% 0.8074979864765169
plot_id,batch_id 0 34 miss% 0.8099670428355865
plot_id,batch_id 0 35 miss% 0.7534344186205354
plot_id,batch_id 0 36 miss% 0.8001287225918023
plot_id,batch_id 0 37 miss% 0.8075631740386627
plot_id,batch_id 0 38 miss% 0.8095711238545688
plot_id,batch_id 0 39 miss% 0.8117735062729832
plot_id,batch_id 0 40 miss% 0.7847307007510271
plot_id,batch_id 0 41 miss% 0.8066670068563218
plot_id,batch_id 0 42 miss% 0.8135351612425478
plot_id,batch_id 0 43 miss% 0.8159981225345759
plot_id,batch_id 0 44 miss% 0.8217621213453713
plot_id,batch_id 0 45 miss% 0.7773458241284199
plot_id,batch_id 0 46 miss% 0.8058653372737109
plot_id,batch_id 0 47 miss% 0.8136607525617517
plot_id,batch_id 0 48 miss% 0.8138962279873644
plot_id,batch_id 0 49 miss% 0.8204038688836347
plot_id,batch_id 0 50 miss% 0.7836735402664384
plot_id,batch_id 0 51 miss% 0.8035877725504467
plot_id,batch_id 0 52 miss% 0.8088732895785238
plot_id,batch_id 0 53 miss% 0.8144686760675335
plot_id,batch_id 0 54 miss% 0.8205695942968659
plot_id,batch_id 0 55 miss% 0.7697678845692623
plot_id,batch_id 0 56 miss% 0.8036922482679687
plot_id,batch_id 0 57 miss% 0.80922927350006
plot_id,batch_id 0 58 miss% 0.8154109582387805
plot_id,batch_id 0 59 miss% 0.8197333214521472
plot_id,batch_id 0 60 miss% 0.6473933353426646
plot_id,batch_id 0 61 miss% 0.750519746450275
plot_id,batch_id 0 62 miss% 0.7885047201599654
plot_id,batch_id 0 63 miss% 0.7977061203392788
plot_id,batch_id 0 64 miss% 0.8050906374543302
plot_id,batch_id 0 65 miss% 0.642173484362536
plot_id,batch_id 0 66 miss% 0.7516430470849008
plot_id,batch_id 0 67 miss% 0.7703834871133299
plot_id,batch_id 0 68 miss% 0.797637313439
plot_id,batch_id 0 69 miss% 0.796697671802686
plot_id,batch_id 0 70 miss% 0.6141208599666989
plot_id,batch_id 0 71 miss% the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  46193
Epoch:0, Train loss:0.567454, valid loss:0.526344
Epoch:1, Train loss:0.180775, valid loss:0.005337
Epoch:2, Train loss:0.008325, valid loss:0.003849
Epoch:3, Train loss:0.006143, valid loss:0.003293
Epoch:4, Train loss:0.005058, valid loss:0.003121
Epoch:5, Train loss:0.004443, valid loss:0.002481
Epoch:6, Train loss:0.003919, valid loss:0.002051
Epoch:7, Train loss:0.003516, valid loss:0.001877
Epoch:8, Train loss:0.003342, valid loss:0.002019
Epoch:9, Train loss:0.003000, valid loss:0.001725
Epoch:10, Train loss:0.002858, valid loss:0.001870
Epoch:11, Train loss:0.002194, valid loss:0.001301
Epoch:12, Train loss:0.002136, valid loss:0.001275
Epoch:13, Train loss:0.002111, valid loss:0.001261
Epoch:14, Train loss:0.002045, valid loss:0.001392
Epoch:15, Train loss:0.002000, valid loss:0.001153
Epoch:16, Train loss:0.001974, valid loss:0.001102
Epoch:17, Train loss:0.001943, valid loss:0.001139
Epoch:18, Train loss:0.001922, valid loss:0.001237
Epoch:19, Train loss:0.001832, valid loss:0.000995
Epoch:20, Train loss:0.001817, valid loss:0.001233
Epoch:21, Train loss:0.001513, valid loss:0.000925
Epoch:22, Train loss:0.001511, valid loss:0.001128
Epoch:23, Train loss:0.001493, valid loss:0.000868
Epoch:24, Train loss:0.001466, valid loss:0.000903
Epoch:25, Train loss:0.001473, valid loss:0.000901
Epoch:26, Train loss:0.001458, valid loss:0.000882
Epoch:27, Train loss:0.001450, valid loss:0.001188
Epoch:28, Train loss:0.001444, valid loss:0.000917
Epoch:29, Train loss:0.001399, valid loss:0.000892
Epoch:30, Train loss:0.001408, valid loss:0.000859
Epoch:31, Train loss:0.001236, valid loss:0.000809
Epoch:32, Train loss:0.001216, valid loss:0.000880
Epoch:33, Train loss:0.001234, valid loss:0.000802
Epoch:34, Train loss:0.001220, valid loss:0.000803
Epoch:35, Train loss:0.001212, valid loss:0.000826
Epoch:36, Train loss:0.001221, valid loss:0.000816
Epoch:37, Train loss:0.001220, valid loss:0.000827
Epoch:38, Train loss:0.001200, valid loss:0.000803
Epoch:39, Train loss:0.001200, valid loss:0.000787
Epoch:40, Train loss:0.001186, valid loss:0.000815
Epoch:41, Train loss:0.001109, valid loss:0.000770
Epoch:42, Train loss:0.001100, valid loss:0.000771
Epoch:43, Train loss:0.001100, valid loss:0.000754
Epoch:44, Train loss:0.001102, valid loss:0.000780
Epoch:45, Train loss:0.001099, valid loss:0.000780
Epoch:46, Train loss:0.001096, valid loss:0.000766
Epoch:47, Train loss:0.001094, valid loss:0.000761
Epoch:48, Train loss:0.001092, valid loss:0.000780
Epoch:49, Train loss:0.001086, valid loss:0.000757
Epoch:50, Train loss:0.001093, valid loss:0.000734
Epoch:51, Train loss:0.001050, valid loss:0.000736
Epoch:52, Train loss:0.001044, valid loss:0.000738
Epoch:53, Train loss:0.001044, valid loss:0.000734
Epoch:54, Train loss:0.001043, valid loss:0.000739
Epoch:55, Train loss:0.001043, valid loss:0.000741
Epoch:56, Train loss:0.001041, valid loss:0.000733
Epoch:57, Train loss:0.001040, valid loss:0.000741
Epoch:58, Train loss:0.001038, valid loss:0.000728
Epoch:59, Train loss:0.001034, valid loss:0.000748
Epoch:60, Train loss:0.001034, valid loss:0.000722
training time 4668.650226831436
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.027394545791212668
plot_id,batch_id 0 1 miss% 0.025478063975919486
plot_id,batch_id 0 2 miss% 0.03217097715397498
plot_id,batch_id 0 3 miss% 0.029338896637612615
plot_id,batch_id 0 4 miss% 0.036093992634521524
plot_id,batch_id 0 5 miss% 0.04441332044154255
plot_id,batch_id 0 6 miss% 0.03674778689183821
plot_id,batch_id 0 7 miss% 0.029618230596128805
plot_id,batch_id 0 8 miss% 0.026987682691096138
plot_id,batch_id 0 9 miss% 0.02212156364639949
plot_id,batch_id 0 10 miss% 0.060633046814134145
plot_id,batch_id 0 11 miss% 0.05021216757300662
plot_id,batch_id 0 12 miss% 0.02692794799691169
plot_id,batch_id 0 13 miss% 0.015021701421975564
plot_id,batch_id 0 14 miss% 0.030437505820858617
plot_id,batch_id 0 15 miss% 0.052744255704496784
plot_id,batch_id 0 16 miss% 0.02465114668711121
plot_id,batch_id 0 17 miss% 0.03483567949286554
plot_id,batch_id 0 18 miss% 0.025074316127785778
plot_id,batch_id 0 19 miss% 0.023910306551806395
plot_id,batch_id 0 20 miss% 0.028894827529899587
plot_id,batch_id 0 21 miss% 0.02164392226537373
plot_id,batch_id 0 22 miss% 0.027190384994652064
plot_id,batch_id 0 23 miss% 0.02526557583797004
plot_id,batch_id 0 24 miss% 0.022923878179433
plot_id,batch_id 0 25 miss% 0.04314757961350945
plot_id,batch_id 0 26 miss% 0.03331302449878532
plot_id,batch_id 0 27 miss% 0.025766761226654368
plot_id,batch_id 0 28 miss% 0.03158891934202649
plot_id,batch_id 0 29 miss% 0.020199104874405696
plot_id,batch_id 0 30 miss% 0.03358983483505025
plot_id,batch_id 0 31 miss% 0.024915427716474706
plot_id,batch_id 0 32 miss% 0.0313682210117106
plot_id,batch_id 0 33 miss% 0.027381483479147078
plot_id,batch_id 0 34 miss% 0.02424292148592664
plot_id,batch_id 0 35 miss% 0.05612582724308479
plot_id,batch_id 0 36 miss% 0.027943670957226673
plot_id,batch_id 0 37 miss% 0.032108275045204845
plot_id,batch_id 0 38 miss% 0.03171687822683655
plot_id,batch_id 0 39 miss% 0.018789487518295112
plot_id,batch_id 0 40 miss% 0.07510583960308939
plot_id,batch_id 0 41 miss% 0.02904271584117314
plot_id,batch_id 0 42 miss% 0.02379576411777234
plot_id,batch_id 0 43 miss% 0.01875506089357688
plot_id,batch_id 0 44 miss% 0.022353806633448364
plot_id,batch_id 0 45 miss% 0.0408159246966444
plot_id,batch_id 0 46 miss% 0.028058605536061154
plot_id,batch_id 0 47 miss% 0.019662247465668917
plot_id,batch_id 0 48 miss% 0.020903916624086843
plot_id,batch_id 0 49 miss% 0.021366916156176318
plot_id,batch_id 0 50 miss% 0.03403484895325615
plot_id,batch_id 0 51 miss% 0.02369749185387504
plot_id,batch_id 0 52 miss% 0.026290604016108856
plot_id,batch_id 0 53 miss% 0.013366687312061516
plot_id,batch_id 0 54 miss% 0.03225212753374055
plot_id,batch_id 0 55 miss% 0.03774311307766603
plot_id,batch_id 0 56 miss% 0.02159654600301869
plot_id,batch_id 0 57 miss% 0.020546448693318536
plot_id,batch_id 0 58 miss% 0.021290633959374253
plot_id,batch_id 0 59 miss% 0.018252324729630728
plot_id,batch_id 0 60 miss% 0.04825995701099586
plot_id,batch_id 0 61 miss% 0.033895935379903826
plot_id,batch_id 0 62 miss% 0.02443295471112426
plot_id,batch_id 0 63 miss% 0.04159618502481402
plot_id,batch_id 0 64 miss% 0.034681874614098905
plot_id,batch_id 0 65 miss% 0.04043488020424533
plot_id,batch_id 0 66 miss% 0.047763645129992365
plot_id,batch_id 0 67 miss% 0.03113673974656244
plot_id,batch_id 0 68 miss% 0.03694384537307626
plot_id,batch_id 0 69 0.7603644191051793
plot_id,batch_id 0 72 miss% 0.7639576002852786
plot_id,batch_id 0 73 miss% 0.7816044428298106
plot_id,batch_id 0 74 miss% 0.7903853015829161
plot_id,batch_id 0 75 miss% 0.6076890049725475
plot_id,batch_id 0 76 miss% 0.7150166861795514
plot_id,batch_id 0 77 miss% 0.7527646503555508
plot_id,batch_id 0 78 miss% 0.7823003445694094
plot_id,batch_id 0 79 miss% 0.7905372885358093
plot_id,batch_id 0 80 miss% 0.6774771107058885
plot_id,batch_id 0 81 miss% 0.7793940517172588
plot_id,batch_id 0 82 miss% 0.794450742721744
plot_id,batch_id 0 83 miss% 0.800816376101446
plot_id,batch_id 0 84 miss% 0.8014223307755053
plot_id,batch_id 0 85 miss% 0.6682417949843374
plot_id,batch_id 0 86 miss% 0.7687494243612768
plot_id,batch_id 0 87 miss% 0.7946246476257302
plot_id,batch_id 0 88 miss% 0.7980383508410624
plot_id,batch_id 0 89 miss% 0.8005994000059012
plot_id,batch_id 0 90 miss% 0.6410778083781853
plot_id,batch_id 0 91 miss% 0.7675357288842657
plot_id,batch_id 0 92 miss% 0.7810355936092315
plot_id,batch_id 0 93 miss% 0.7907088140386954
plot_id,batch_id 0 94 miss% 0.7980286790620038
plot_id,batch_id 0 95 miss% 0.64723005754206
plot_id,batch_id 0 96 miss% 0.7518439683300794
plot_id,batch_id 0 97 miss% 0.7783265878546098
plot_id,batch_id 0 98 miss% 0.7871254900367053
plot_id,batch_id 0 99 miss% 0.7952494341282236
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.7275733  0.78966489 0.8021067  0.81127028 0.80751201 0.72440124
 0.78362001 0.79832893 0.80548599 0.81184035 0.70321892 0.78609205
 0.79519899 0.80539077 0.81191301 0.71199767 0.7779913  0.79883009
 0.80528543 0.80692098 0.77089229 0.80444098 0.81272    0.81674378
 0.81915411 0.75856138 0.79766691 0.80440584 0.80821196 0.81010829
 0.76232574 0.7961217  0.80518763 0.80749799 0.80996704 0.75343442
 0.80012872 0.80756317 0.80957112 0.81177351 0.7847307  0.80666701
 0.81353516 0.81599812 0.82176212 0.77734582 0.80586534 0.81366075
 0.81389623 0.82040387 0.78367354 0.80358777 0.80887329 0.81446868
 0.82056959 0.76976788 0.80369225 0.80922927 0.81541096 0.81973332
 0.64739334 0.75051975 0.78850472 0.79770612 0.80509064 0.64217348
 0.75164305 0.77038349 0.79763731 0.79669767 0.61412086 0.76036442
 0.7639576  0.78160444 0.7903853  0.607689   0.71501669 0.75276465
 0.78230034 0.79053729 0.67747711 0.77939405 0.79445074 0.80081638
 0.80142233 0.66824179 0.76874942 0.79462465 0.79803835 0.8005994
 0.64107781 0.76753573 0.78103559 0.79070881 0.79802868 0.64723006
 0.75184397 0.77832659 0.78712549 0.79524943]
for model  109 the mean error 0.7788245771629224
all id 109 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 25 out win 4 err 0.7788245771629224
Launcher: Job 110 completed in 4859 seconds.
Launcher: Task 81 done. Exiting.
miss% 0.03539115533735178
plot_id,batch_id 0 70 miss% 0.04418944165147859
plot_id,batch_id 0 71 miss% 0.05943919977467726
plot_id,batch_id 0 72 miss% 0.023495450731256105
plot_id,batch_id 0 73 miss% 0.024415140233037008
plot_id,batch_id 0 74 miss% 0.035566224328244946
plot_id,batch_id 0 75 miss% 0.028847964099955225
plot_id,batch_id 0 76 miss% 0.044443713380499815
plot_id,batch_id 0 77 miss% 0.030606953128890824
plot_id,batch_id 0 78 miss% 0.04483334862453579
plot_id,batch_id 0 79 miss% 0.051453296926876044
plot_id,batch_id 0 80 miss% 0.03234767337342832
plot_id,batch_id 0 81 miss% 0.01789441098353779
plot_id,batch_id 0 82 miss% 0.035319382222809705
plot_id,batch_id 0 83 miss% 0.03039482079367509
plot_id,batch_id 0 84 miss% 0.025615266895839883
plot_id,batch_id 0 85 miss% 0.04400768174822353
plot_id,batch_id 0 86 miss% 0.02273151724957879
plot_id,batch_id 0 87 miss% 0.030052845546678968
plot_id,batch_id 0 88 miss% 0.02655311109427516
plot_id,batch_id 0 89 miss% 0.025977658992900915
plot_id,batch_id 0 90 miss% 0.036324517699034706
plot_id,batch_id 0 91 miss% 0.04076059224351199
plot_id,batch_id 0 92 miss% 0.03805762561808043
plot_id,batch_id 0 93 miss% 0.0333819547220421
plot_id,batch_id 0 94 miss% 0.02624743344263105
plot_id,batch_id 0 95 miss% 0.04012526797720891
plot_id,batch_id 0 96 miss% 0.02905145967977291
plot_id,batch_id 0 97 miss% 0.0426287713283182
plot_id,batch_id 0 98 miss% 0.03436481125796144
plot_id,batch_id 0 99 miss% 0.030008358534627248
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02739455 0.02547806 0.03217098 0.0293389  0.03609399 0.04441332
 0.03674779 0.02961823 0.02698768 0.02212156 0.06063305 0.05021217
 0.02692795 0.0150217  0.03043751 0.05274426 0.02465115 0.03483568
 0.02507432 0.02391031 0.02889483 0.02164392 0.02719038 0.02526558
 0.02292388 0.04314758 0.03331302 0.02576676 0.03158892 0.0201991
 0.03358983 0.02491543 0.03136822 0.02738148 0.02424292 0.05612583
 0.02794367 0.03210828 0.03171688 0.01878949 0.07510584 0.02904272
 0.02379576 0.01875506 0.02235381 0.04081592 0.02805861 0.01966225
 0.02090392 0.02136692 0.03403485 0.02369749 0.0262906  0.01336669
 0.03225213 0.03774311 0.02159655 0.02054645 0.02129063 0.01825232
 0.04825996 0.03389594 0.02443295 0.04159619 0.03468187 0.04043488
 0.04776365 0.03113674 0.03694385 0.03539116 0.04418944 0.0594392
 0.02349545 0.02441514 0.03556622 0.02884796 0.04444371 0.03060695
 0.04483335 0.0514533  0.03234767 0.01789441 0.03531938 0.03039482
 0.02561527 0.04400768 0.02273152 0.03005285 0.02655311 0.02597766
 0.03632452 0.04076059 0.03805763 0.03338195 0.02624743 0.04012527
 0.02905146 0.04262877 0.03436481 0.03000836]
for model  84 the mean error 0.03195533831048367
all id 84 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 25 out win 3 err 0.03195533831048367
Launcher: Job 85 completed in 4865 seconds.
Launcher: Task 122 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  28945
Epoch:0, Train loss:0.631497, valid loss:0.627464
Epoch:1, Train loss:0.051100, valid loss:0.012924
Epoch:2, Train loss:0.014711, valid loss:0.006317
Epoch:3, Train loss:0.011306, valid loss:0.005135
Epoch:4, Train loss:0.009342, valid loss:0.004759
Epoch:5, Train loss:0.008338, valid loss:0.005890
Epoch:6, Train loss:0.007280, valid loss:0.003700
Epoch:7, Train loss:0.006968, valid loss:0.003316
Epoch:8, Train loss:0.006536, valid loss:0.003902
Epoch:9, Train loss:0.006315, valid loss:0.003566
Epoch:10, Train loss:0.006167, valid loss:0.004314
Epoch:11, Train loss:0.004102, valid loss:0.002387
Epoch:12, Train loss:0.004076, valid loss:0.002469
Epoch:13, Train loss:0.004064, valid loss:0.002001
Epoch:14, Train loss:0.004255, valid loss:0.002164
Epoch:15, Train loss:0.003913, valid loss:0.002402
Epoch:16, Train loss:0.004013, valid loss:0.002270
Epoch:17, Train loss:0.003922, valid loss:0.001787
Epoch:18, Train loss:0.003735, valid loss:0.002662
Epoch:19, Train loss:0.003810, valid loss:0.002079
Epoch:20, Train loss:0.003823, valid loss:0.002303
Epoch:21, Train loss:0.002854, valid loss:0.001587
Epoch:22, Train loss:0.002779, valid loss:0.002255
Epoch:23, Train loss:0.002795, valid loss:0.001705
Epoch:24, Train loss:0.002796, valid loss:0.001574
Epoch:25, Train loss:0.002797, valid loss:0.001541
Epoch:26, Train loss:0.002709, valid loss:0.001700
Epoch:27, Train loss:0.002815, valid loss:0.001996
Epoch:28, Train loss:0.002673, valid loss:0.001383
Epoch:29, Train loss:0.002652, valid loss:0.001515
Epoch:30, Train loss:0.002694, valid loss:0.001867
Epoch:31, Train loss:0.002216, valid loss:0.001466
Epoch:32, Train loss:0.002206, valid loss:0.001529
Epoch:33, Train loss:0.002158, valid loss:0.001316
Epoch:34, Train loss:0.002197, valid loss:0.001403
Epoch:35, Train loss:0.002173, valid loss:0.001415
Epoch:36, Train loss:0.002161, valid loss:0.001361
Epoch:37, Train loss:0.002172, valid loss:0.001398
Epoch:38, Train loss:0.002129, valid loss:0.001234
Epoch:39, Train loss:0.002125, valid loss:0.001333
Epoch:40, Train loss:0.002109, valid loss:0.001316
Epoch:41, Train loss:0.001879, valid loss:0.001120
Epoch:42, Train loss:0.001868, valid loss:0.001257
Epoch:43, Train loss:0.001873, valid loss:0.001247
Epoch:44, Train loss:0.001870, valid loss:0.001184
Epoch:45, Train loss:0.001874, valid loss:0.001138
Epoch:46, Train loss:0.001857, valid loss:0.001253
Epoch:47, Train loss:0.001853, valid loss:0.001275
Epoch:48, Train loss:0.001859, valid loss:0.001174
Epoch:49, Train loss:0.001847, valid loss:0.001160
Epoch:50, Train loss:0.001827, valid loss:0.001124
Epoch:51, Train loss:0.001717, valid loss:0.001112
Epoch:52, Train loss:0.001713, valid loss:0.001069
Epoch:53, Train loss:0.001710, valid loss:0.001136
Epoch:54, Train loss:0.001708, valid loss:0.001108
Epoch:55, Train loss:0.001714, valid loss:0.001077
Epoch:56, Train loss:0.001702, valid loss:0.001093
Epoch:57, Train loss:0.001705, valid loss:0.001093
Epoch:58, Train loss:0.001688, valid loss:0.001170
Epoch:59, Train loss:0.001701, valid loss:0.001074
Epoch:60, Train loss:0.001684, valid loss:0.001080
training time 4712.951328992844
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.04308671929123149
plot_id,batch_id 0 1 miss% 0.03398453546968364
plot_id,batch_id 0 2 miss% 0.030016633192096562
plot_id,batch_id 0 3 miss% 0.02834452349145222
plot_id,batch_id 0 4 miss% 0.035808758882934735
plot_id,batch_id 0 5 miss% 0.04433254648440832
plot_id,batch_id 0 6 miss% 0.03668024729381121
plot_id,batch_id 0 7 miss% 0.031061531942102472
plot_id,batch_id 0 8 miss% 0.03946770611358903
plot_id,batch_id 0 9 miss% 0.026009936431338237
plot_id,batch_id 0 10 miss% 0.032295205365183516
plot_id,batch_id 0 11 miss% 0.04821704831894682
plot_id,batch_id 0 12 miss% 0.03591715754084671
plot_id,batch_id 0 13 miss% 0.04511223864294596
plot_id,batch_id 0 14 miss% 0.04674255254751547
plot_id,batch_id 0 15 miss% 0.06932773487918656
plot_id,batch_id 0 16 miss% 0.0440754710826339
plot_id,batch_id 0 17 miss% 0.06345701914662369
plot_id,batch_id 0 18 miss% 0.04589692474025783
plot_id,batch_id 0 19 miss% 0.04597180620135288
plot_id,batch_id 0 20 miss% 0.07962982659170502
plot_id,batch_id 0 21 miss% 0.038441370372091596
plot_id,batch_id 0 22 miss% 0.016971383673592737
plot_id,batch_id 0 23 miss% 0.03230898028855244
plot_id,batch_id 0 24 miss% 0.034316823691801446
plot_id,batch_id 0 25 miss% 0.036644650860928774
plot_id,batch_id 0 26 miss% 0.018861489524338228
plot_id,batch_id 0 27 miss% 0.029805147378051036
plot_id,batch_id 0 28 miss% 0.03135882769262949
plot_id,batch_id 0 29 miss% 0.034555054610959486
plot_id,batch_id 0 30 miss% 0.05817045455152664
plot_id,batch_id 0 31 miss% 0.03730221276112916
plot_id,batch_id 0 32 miss% 0.03129194572846668
plot_id,batch_id 0 33 miss% 0.036828242221796155
plot_id,batch_id 0 34 miss% 0.03959011127256031
plot_id,batch_id 0 35 miss% 0.04859925338963902
plot_id,batch_id 0 36 miss% 0.056305449787139764
plot_id,batch_id 0 37 miss% 0.026531169860640184
plot_id,batch_id 0 38 miss% 0.033090307501396424
plot_id,batch_id 0 39 miss% 0.027827676578460046
plot_id,batch_id 0 40 miss% 0.056063385765025876
plot_id,batch_id 0 41 miss% 0.0319942601791715
plot_id,batch_id 0 42 miss% 0.02318228728284791
plot_id,batch_id 0 43 miss% 0.030313463309085357
plot_id,batch_id 0 44 miss% 0.02995995437202335
plot_id,batch_id 0 45 miss% 0.036563657835574075
plot_id,batch_id 0 46 miss% 0.029401634947115313
plot_id,batch_id 0 47 miss% 0.029314382091562556
plot_id,batch_id 0 48 miss% 0.03551627929496115
plot_id,batch_id 0 49 miss% 0.029569929494850187
plot_id,batch_id 0 50 miss% 0.0338417087774867
plot_id,batch_id 0 51 miss% 0.03310758463629137
plot_id,batch_id 0 52 miss% 0.02846788078585439
plot_id,batch_id 0 53 miss% 0.018902451747401128
plot_id,batch_id 0 54 miss% 0.04800054842540754
plot_id,batch_id 0 55 miss% 0.042610439816637126
plot_id,batch_id 0 56 miss% 0.02760030097452056
plot_id,batch_id 0 57 miss% 0.02828568504724141
plot_id,batch_id 0 58 miss% 0.039393699627279954
plot_id,batch_id 0 59 miss% 0.03467265273901219
plot_id,batch_id 0 60 miss% 0.04997924926719647
plot_id,batch_id 0 61 miss% 0.034160146743920546
plot_id,batch_id 0 62 miss% 0.037985706441189725
plot_id,batch_id 0 63 miss% 0.028673214285418386
plot_id,batch_id 0 64 miss% 0.04107582189155976
plot_id,batch_id 0 65 miss% 0.054406835381802264
plot_id,batch_id 0 66 miss% 0.06424793841605536
plot_id,batch_id 0 67 miss% 0.038822862999755375
plot_id,batch_id 0 68 miss% 0.026977010922913706
plot_id,batch_id 0 69 miss% 0.030374690526761408
plot_id,batch_id 0 70 miss% 0.06825134693819845
plot_id,batch_id 0 71 miss% 0.04815776521071005
plot_id,batch_id 0 72 miss% 0.043676154244579955
plot_id,batch_id 0 73 miss% 0.04742408369166131
plot_id,batch_id 0 74 miss% 0.03781972278687844
plot_id,batch_id 0 75 miss% 0.03529481973048366
plot_id,batch_id 0 76 miss% 0.05009207524856223
plot_id,batch_id 0 77 miss% 0.050061504033914996
plot_id,batch_id 0 78 miss% 0.027942861210689323
plot_id,batch_id 0 79 miss% 0.04291840878121829
plot_id,batch_id 0 80 miss% 0.06357000159667003
plot_id,batch_id 0 81 miss% 0.030338528637330543
plot_id,batch_id 0 82 miss% 0.030017971393605528
plot_id,batch_id 0 83 miss% 0.033882777440255016
plot_id,batch_id 0 84 miss% 0.022809371747141125
plot_id,batch_id 0 85 miss% 0.041939626736514024
plot_id,batch_id 0 86 miss% 0.03360003618999652
plot_id,batch_id 0 87 miss% 0.044239024042051996
plot_id,batch_id 0 88 miss% 0.04275070013159862
plot_id,batch_id 0 89 miss% 0.02962266691283402
plot_id,batch_id 0 90 miss% 0.04430758898104046
plot_id,batch_id 0 91 miss% 0.04486845603677948
plot_id,batch_id 0 92 miss% 0.0567916562496173
plot_id,batch_id 0 93 miss% 0.04402923990280506
plot_id,batch_id 0 94 miss% 0.060333892750923115
plot_id,batch_id 0 95 miss% 0.0772226352712147
plot_id,batch_id 0 96 miss% 0.05438272414086797
plot_id,batch_id 0 97 miss% 0.0803441708097877
plot_id,batch_id 0 98 miss% 0.03849361387134271
plot_id,batch_id 0 99 miss% 0.05401266324623378
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04308672 0.03398454 0.03001663 0.02834452 0.03580876 0.04433255
 0.03668025 0.03106153 0.03946771 0.02600994 0.03229521 0.04821705
 0.03591716 0.04511224 0.04674255 0.06932773 0.04407547 0.06345702
 0.04589692 0.04597181 0.07962983 0.03844137 0.01697138 0.03230898
 0.03431682 0.03664465 0.01886149 0.02980515 0.03135883 0.03455505
 0.05817045 0.03730221 0.03129195 0.03682824 0.03959011 0.04859925
 0.05630545 0.02653117 0.03309031 0.02782768 0.05606339 0.03199426
 0.02318229 0.03031346 0.02995995 0.03656366 0.02940163 0.02931438
 0.03551628 0.02956993 0.03384171 0.03310758 0.02846788 0.01890245
 0.04800055 0.04261044 0.0276003  0.02828569 0.0393937  0.03467265
 0.04997925 0.03416015 0.03798571 0.02867321 0.04107582 0.05440684
 0.06424794 0.03882286 0.02697701 0.03037469 0.06825135 0.04815777
 0.04367615 0.04742408 0.03781972 0.03529482 0.05009208 0.0500615
 0.02794286 0.04291841 0.06357    0.03033853 0.03001797 0.03388278
 0.02280937 0.04193963 0.03360004 0.04423902 0.0427507  0.02962267
 0.04430759 0.04486846 0.05679166 0.04402924 0.06033389 0.07722264
 0.05438272 0.08034417 0.03849361 0.05401266]
for model  64 the mean error 0.04026898427386975
all id 64 hidden_dim 16 learning_rate 0.02 num_layers 4 frames 21 out win 4 err 0.04026898427386975
Launcher: Job 65 completed in 4925 seconds.
Launcher: Task 44 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  46193
Epoch:0, Train loss:0.694666, valid loss:0.659432
Epoch:1, Train loss:0.510272, valid loss:0.518438
Epoch:2, Train loss:0.497499, valid loss:0.517926
Epoch:3, Train loss:0.495368, valid loss:0.516482
Epoch:4, Train loss:0.494208, valid loss:0.516588
Epoch:5, Train loss:0.493328, valid loss:0.516172
Epoch:6, Train loss:0.492849, valid loss:0.515886
Epoch:7, Train loss:0.492486, valid loss:0.516649
Epoch:8, Train loss:0.492286, valid loss:0.515639
Epoch:9, Train loss:0.491997, valid loss:0.515621
Epoch:10, Train loss:0.491834, valid loss:0.516083
Epoch:11, Train loss:0.490535, valid loss:0.515039
Epoch:12, Train loss:0.490471, valid loss:0.514763
Epoch:13, Train loss:0.490495, valid loss:0.514812
Epoch:14, Train loss:0.490435, valid loss:0.514665
Epoch:15, Train loss:0.490302, valid loss:0.514669
Epoch:16, Train loss:0.490286, valid loss:0.514624
Epoch:17, Train loss:0.490172, valid loss:0.514637
Epoch:18, Train loss:0.490216, valid loss:0.514678
Epoch:19, Train loss:0.490238, valid loss:0.514883
Epoch:20, Train loss:0.490182, valid loss:0.514925
Epoch:21, Train loss:0.489515, valid loss:0.514333
Epoch:22, Train loss:0.489449, valid loss:0.514513
Epoch:23, Train loss:0.489509, valid loss:0.514367
Epoch:24, Train loss:0.489462, valid loss:0.514384
Epoch:25, Train loss:0.489443, valid loss:0.514965
Epoch:26, Train loss:0.489450, valid loss:0.514984
Epoch:27, Train loss:0.489400, valid loss:0.514389
Epoch:28, Train loss:0.489397, valid loss:0.514633
Epoch:29, Train loss:0.489472, valid loss:0.514276
Epoch:30, Train loss:0.489380, valid loss:0.514395
Epoch:31, Train loss:0.489078, valid loss:0.514203
Epoch:32, Train loss:0.489042, valid loss:0.514138
Epoch:33, Train loss:0.489055, valid loss:0.514109
Epoch:34, Train loss:0.489087, valid loss:0.514119
Epoch:35, Train loss:0.489054, valid loss:0.514136
Epoch:36, Train loss:0.489023, valid loss:0.514098
Epoch:37, Train loss:0.489038, valid loss:0.514077
Epoch:38, Train loss:0.489067, valid loss:0.514282
Epoch:39, Train loss:0.488985, valid loss:0.514117
Epoch:40, Train loss:0.489003, valid loss:0.514412
Epoch:41, Train loss:0.488884, valid loss:0.514131
Epoch:42, Train loss:0.488859, valid loss:0.514070
Epoch:43, Train loss:0.488851, valid loss:0.514112
Epoch:44, Train loss:0.488851, valid loss:0.514039
Epoch:45, Train loss:0.488855, valid loss:0.514056
Epoch:46, Train loss:0.488844, valid loss:0.514102
Epoch:47, Train loss:0.488850, valid loss:0.514069
Epoch:48, Train loss:0.488843, valid loss:0.514052
Epoch:49, Train loss:0.488831, valid loss:0.514077
Epoch:50, Train loss:0.488823, valid loss:0.514090
Epoch:51, Train loss:0.488766, valid loss:0.514041
Epoch:52, Train loss:0.488762, valid loss:0.514016
Epoch:53, Train loss:0.488759, valid loss:0.514023
Epoch:54, Train loss:0.488755, valid loss:0.514014
Epoch:55, Train loss:0.488755, valid loss:0.514073
Epoch:56, Train loss:0.488757, valid loss:0.514009
Epoch:57, Train loss:0.488750, valid loss:0.514037
Epoch:58, Train loss:0.488745, valid loss:0.514052
Epoch:59, Train loss:0.488747, valid loss:0.514048
Epoch:60, Train loss:0.488746, valid loss:0.514081
training time 4792.009402990341
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.7713830370885351
plot_id,batch_id 0 1 miss% 0.8226025733757955
plot_id,batch_id 0 2 miss% 0.8311552005292077
plot_id,batch_id 0 3 miss% 0.8341565415011579
plot_id,batch_id 0 4 miss% 0.8355266248652611
plot_id,batch_id 0 5 miss% 0.7692943074383499
plot_id,batch_id 0 6 miss% 0.8162393868272091
plot_id,batch_id 0 7 miss% 0.8285174982713136
plot_id,batch_id 0 8 miss% 0.8339754969044174
plot_id,batch_id 0 9 miss% 0.8390854071284648
plot_id,batch_id 0 10 miss% 0.746540778187327
plot_id,batch_id 0 11 miss% 0.8181604585040647
plot_id,batch_id 0 12 miss% 0.8263780986204541
plot_id,batch_id 0 13 miss% 0.8317565340186445
plot_id,batch_id 0 14 miss% 0.8343668527849792
plot_id,batch_id 0 15 miss% 0.7636103245058613
plot_id,batch_id 0 16 miss% 0.8139341052945551
plot_id,batch_id 0 17 miss% 0.8265512475012953
plot_id,batch_id 0 18 miss% 0.8333428228681177
plot_id,batch_id 0 19 miss% 0.8350691695513653
plot_id,batch_id 0 20 miss% 0.7929262713291216
plot_id,batch_id 0 21 miss% 0.8294468738614317
plot_id,batch_id 0 22 miss% 0.8397507267481145
plot_id,batch_id 0 23 miss% 0.8374303298073781
plot_id,batch_id 0 24 miss% 0.8429942518483701
plot_id,batch_id 0 25 miss% 0.798707628371508
plot_id,batch_id 0 26 miss% 0.8285634838432775
plot_id,batch_id 0 27 miss% 0.8320452668990334
plot_id,batch_id 0 28 miss% 0.8355376622975573
plot_id,batch_id 0 29 miss% 0.8366986678688083
plot_id,batch_id 0 30 miss% 0.7865264356748632
plot_id,batch_id 0 31 miss% 0.8243123331065927
plot_id,batch_id 0 32 miss% 0.8321290551315003
plot_id,batch_id 0 33 miss% 0.8352056214600795
plot_id,batch_id 0 34 miss% 0.8392554635355132
plot_id,batch_id 0 35 miss% 0.7826311587418163
plot_id,batch_id 0 36 miss% 0.829633153314948
plot_id,batch_id 0 37 miss% 0.8335965943277711
plot_id,batch_id 0 38 miss% 0.8377825600549164
plot_id,batch_id 0 39 miss% 0.8374746558534022
plot_id,batch_id 0 40 miss% 0.8169776941553172
plot_id,batch_id 0 41 miss% 0.8359559690693894
plot_id,batch_id 0 42 miss% 0.8372310093259157
plot_id,batch_id 0 43 miss% 0.8418105327117699
plot_id,batch_id 0 44 miss% 0.8444903749493003
plot_id,batch_id 0 45 miss% 0.8143731927787425
plot_id,batch_id 0 46 miss% 0.834100619402281
plot_id,batch_id 0 47 miss% 0.8392864290553177
plot_id,batch_id 0 48 miss% 0.839428274793139
plot_id,batch_id 0 49 miss% 0.843801957648161
plot_id,batch_id 0 50 miss% 0.8235246448188699
plot_id,batch_id 0 51 miss% 0.8343701236905811
plot_id,batch_id 0 52 miss% 0.8357232105761363
plot_id,batch_id 0 53 miss% 0.8397157045047031
plot_id,batch_id 0 54 miss% 0.8441800622730178
plot_id,batch_id 0 55 miss% 0.8144274298657004
plot_id,batch_id 0 56 miss% 0.8336890368115375
plot_id,batch_id 0 57 miss% 0.8360332460838475
plot_id,batch_id 0 58 miss% 0.8396172580589017
plot_id,batch_id 0 59 miss% 0.8438687483995431
plot_id,batch_id 0 60 miss% 0.7099768013204507
plot_id,batch_id 0 61 miss% 0.8035582986985834
plot_id,batch_id 0 62 miss% 0.8171351115147136
plot_id,batch_id 0 63 miss% 0.8274514541490682
plot_id,batch_id 0 64 miss% 0.8291043734623558
plot_id,batch_id 0 65 miss% 0.7118425022316475
plot_id,batch_id 0 66 miss% 0.7920690296365722
plot_id,batch_id 0 67 miss% 0.8082424366222264
plot_id,batch_id 0 68 miss% 0.8248193321530831
plot_id,batch_id 0 69 miss% 0.8269063617573695
plot_id,batch_id 0 70 miss% 0.6697889333683485
plot_id,batch_id 0 71 miss% 0.786518171492265
plot_id,batch_id 0 72 miss% 0.8022644128302415
plot_id,batch_id 0 73 miss% 0.8149270524896052
plot_id,batch_id 0 74 miss% 0.82274798702425
plot_id,batch_id 0 75 miss% 0.6663658291488691
plot_id,batch_id 0 76 miss% 0.7886936821346073
plot_id,batch_id 0 77 miss% 0.794082234253029
plot_id,batch_id 0 78 miss% 0.8103070745259254
plot_id,batch_id 0 79 miss% 0.8164523403684154
plot_id,batch_id 0 80 miss% 0.7282035035266654
plot_id,batch_id 0 81 miss% 0.816641524501331
plot_id,batch_id 0 82 miss% 0.82553145567204
plot_id,batch_id 0 83 miss% 0.8313268088278879
plot_id,batch_id 0 84 miss% 0.8309499056545463
plot_id,batch_id 0 85 miss% 0.727121901359699
plot_id,batch_id 0 86 miss% 0.8028283903088088
plot_id,batch_id 0 87 miss% 0.8198030892405971
plot_id,batch_id 0 88 miss% 0.8277444180491227
plot_id,batch_id 0 89 miss% 0.8302835177071435
plot_id,batch_id 0 90 miss% 0.6888444385767603
plot_id,batch_id 0 91 miss% 0.798942658725636
plot_id,batch_id 0 92 miss% 0.8169178959194641
plot_id,batch_id 0 93 miss% 0.8189196509990243
plot_id,batch_id 0 94 miss% 0.8295116453752892
plot_id,batch_id 0 95 miss% 0.7087497064799348
plot_id,batch_id 0 96 miss% 0.7959600188028193
plot_id,batch_id 0 97 miss% 0.8096394916238878
plot_id,batch_id 0 98 miss% 0.8184506529943166
plot_id,batch_id 0 99 miss% 0.8266573092032491
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.77138304 0.82260257 0.8311552  0.83415654 0.83552662 0.76929431
 0.81623939 0.8285175  0.8339755  0.83908541 0.74654078 0.81816046
 0.8263781  0.83175653 0.83436685 0.76361032 0.81393411 0.82655125
 0.83334282 0.83506917 0.79292627 0.82944687 0.83975073 0.83743033
 0.84299425 0.79870763 0.82856348 0.83204527 0.83553766 0.83669867
 0.78652644 0.82431233 0.83212906 0.83520562 0.83925546 0.78263116
 0.82963315 0.83359659 0.83778256 0.83747466 0.81697769 0.83595597
 0.83723101 0.84181053 0.84449037 0.81437319 0.83410062 0.83928643
 0.83942827 0.84380196 0.82352464 0.83437012 0.83572321 0.8397157
 0.84418006 0.81442743 0.83368904 0.83603325 0.83961726 0.84386875
 0.7099768  0.8035583  0.81713511 0.82745145 0.82910437 0.7118425
 0.79206903 0.80824244 0.82481933 0.82690636 0.66978893 0.78651817
 0.80226441 0.81492705 0.82274799 0.66636583 0.78869368 0.79408223
 0.81030707 0.81645234 0.7282035  0.81664152 0.82553146 0.83132681
 0.83094991 0.7271219  0.80282839 0.81980309 0.82774442 0.83028352
 0.68884444 0.79894266 0.8169179  0.81891965 0.82951165 0.70874971
 0.79596002 0.80963949 0.81845065 0.82665731]
for model  31 the mean error 0.8119318158154439
all id 31 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 21 out win 4 err 0.8119318158154439
Launcher: Job 32 completed in 4973 seconds.
Launcher: Task 10 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  35921
Epoch:0, Train loss:0.831381, valid loss:0.823086
Epoch:1, Train loss:0.057354, valid loss:0.010386
Epoch:2, Train loss:0.014701, valid loss:0.006107
Epoch:3, Train loss:0.010450, valid loss:0.004916
Epoch:4, Train loss:0.008737, valid loss:0.005211
Epoch:5, Train loss:0.007364, valid loss:0.003249
Epoch:6, Train loss:0.006498, valid loss:0.003967
Epoch:7, Train loss:0.005626, valid loss:0.003105
Epoch:8, Train loss:0.005155, valid loss:0.002225
Epoch:9, Train loss:0.004622, valid loss:0.002758
Epoch:10, Train loss:0.004698, valid loss:0.002522
Epoch:11, Train loss:0.003256, valid loss:0.001858
Epoch:12, Train loss:0.003215, valid loss:0.001759
Epoch:13, Train loss:0.003103, valid loss:0.002230
Epoch:14, Train loss:0.003123, valid loss:0.001904
Epoch:15, Train loss:0.003021, valid loss:0.001765
Epoch:16, Train loss:0.003101, valid loss:0.001716
Epoch:17, Train loss:0.002948, valid loss:0.001965
Epoch:18, Train loss:0.002936, valid loss:0.001769
Epoch:19, Train loss:0.002825, valid loss:0.001905
Epoch:20, Train loss:0.002793, valid loss:0.001892
Epoch:21, Train loss:0.002213, valid loss:0.001251
Epoch:22, Train loss:0.002199, valid loss:0.001343
Epoch:23, Train loss:0.002192, valid loss:0.001285
Epoch:24, Train loss:0.002182, valid loss:0.001334
Epoch:25, Train loss:0.002117, valid loss:0.001409
Epoch:26, Train loss:0.002174, valid loss:0.001595
Epoch:27, Train loss:0.002107, valid loss:0.001549
Epoch:28, Train loss:0.002101, valid loss:0.001287
Epoch:29, Train loss:0.002061, valid loss:0.001361
Epoch:30, Train loss:0.002099, valid loss:0.001299
Epoch:31, Train loss:0.001763, valid loss:0.001160
Epoch:32, Train loss:0.001757, valid loss:0.001206
Epoch:33, Train loss:0.001751, valid loss:0.001171
Epoch:34, Train loss:0.001769, valid loss:0.001246
Epoch:35, Train loss:0.001761, valid loss:0.001149
Epoch:36, Train loss:0.001721, valid loss:0.001124
Epoch:37, Train loss:0.001735, valid loss:0.001126
Epoch:38, Train loss:0.001710, valid loss:0.001129
Epoch:39, Train loss:0.001702, valid loss:0.001165
Epoch:40, Train loss:0.001715, valid loss:0.001081
Epoch:41, Train loss:0.001555, valid loss:0.001097
Epoch:42, Train loss:0.001559, valid loss:0.001088
Epoch:43, Train loss:0.001543, valid loss:0.001071
Epoch:44, Train loss:0.001541, valid loss:0.001101
Epoch:45, Train loss:0.001540, valid loss:0.001099
Epoch:46, Train loss:0.001540, valid loss:0.001072
Epoch:47, Train loss:0.001528, valid loss:0.001046
Epoch:48, Train loss:0.001516, valid loss:0.001047
Epoch:49, Train loss:0.001518, valid loss:0.001056
Epoch:50, Train loss:0.001522, valid loss:0.001059
Epoch:51, Train loss:0.001449, valid loss:0.001010
Epoch:52, Train loss:0.001452, valid loss:0.001014
Epoch:53, Train loss:0.001448, valid loss:0.001020
Epoch:54, Train loss:0.001444, valid loss:0.001018
Epoch:55, Train loss:0.001441, valid loss:0.000997
Epoch:56, Train loss:0.001437, valid loss:0.001009
Epoch:57, Train loss:0.001436, valid loss:0.001030
Epoch:58, Train loss:0.001434, valid loss:0.001023
Epoch:59, Train loss:0.001428, valid loss:0.001035
Epoch:60, Train loss:0.001429, valid loss:0.001031
training time 4797.398053646088
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.02449337717243079
plot_id,batch_id 0 1 miss% 0.043070074312962364
plot_id,batch_id 0 2 miss% 0.03872672950650537
plot_id,batch_id 0 3 miss% 0.024849594407506385
plot_id,batch_id 0 4 miss% 0.031574996317464216
plot_id,batch_id 0 5 miss% 0.038173467616159834
plot_id,batch_id 0 6 miss% 0.03831292262196148
plot_id,batch_id 0 7 miss% 0.029556740023134097
plot_id,batch_id 0 8 miss% 0.03828051328796904
plot_id,batch_id 0 9 miss% 0.035101728787357483
plot_id,batch_id 0 10 miss% 0.0443860930777954
plot_id,batch_id 0 11 miss% 0.05201311983601104
plot_id,batch_id 0 12 miss% 0.03267496002629614
plot_id,batch_id 0 13 miss% 0.03521146959479191
plot_id,batch_id 0 14 miss% 0.03820181852853797
plot_id,batch_id 0 15 miss% 0.046406334634602926
plot_id,batch_id 0 16 miss% 0.04249890738308484
plot_id,batch_id 0 17 miss% 0.03673146961366552
plot_id,batch_id 0 18 miss% 0.02922914893273507
plot_id,batch_id 0 19 miss% 0.03294443210560898
plot_id,batch_id 0 20 miss% 0.04380289280598347
plot_id,batch_id 0 21 miss% 0.026648073009599274
plot_id,batch_id 0 22 miss% 0.030560174811464322
plot_id,batch_id 0 23 miss% 0.029392077100255775
plot_id,batch_id 0 24 miss% 0.04218760696164956
plot_id,batch_id 0 25 miss% 0.05117600862238919
plot_id,batch_id 0 26 miss% 0.02693733613630531
plot_id,batch_id 0 27 miss% 0.026581875272277684
plot_id,batch_id 0 28 miss% 0.0319582718066904
plot_id,batch_id 0 29 miss% 0.03759613740916793
plot_id,batch_id 0 30 miss% 0.046266232257181586
plot_id,batch_id 0 31 miss% 0.030663646261712925
plot_id,batch_id 0 32 miss% 0.02729517345625041
plot_id,batch_id 0 33 miss% 0.030828615269004597
plot_id,batch_id 0 34 miss% 0.03226974211393124
plot_id,batch_id 0 35 miss% 0.044090587207169275
plot_id,batch_id 0 36 miss% 0.02659386742085742
plot_id,batch_id 0 37 miss% 0.02854928590158117
plot_id,batch_id 0 38 miss% 0.023235645905824838
plot_id,batch_id 0 39 miss% 0.028636536091817797
plot_id,batch_id 0 40 miss% 0.08107940509204684
plot_id,batch_id 0 41 miss% 0.023294645336804795
plot_id,batch_id 0 42 miss% 0.0260325917323431
plot_id,batch_id 0 43 miss% 0.044145826833346995
plot_id,batch_id 0 44 miss% 0.030084393359307567
plot_id,batch_id 0 45 miss% 0.055346062911731886
plot_id,batch_id 0 46 miss% 0.023455203305859013
plot_id,batch_id 0 47 miss% 0.020146423069610866
plot_id,batch_id 0 48 miss% 0.029646589466064435
plot_id,batch_id 0 49 miss% 0.03340104503572959
plot_id,batch_id 0 50 miss% 0.03362506988619228
plot_id,batch_id 0 51 miss% 0.02586398630834693
plot_id,batch_id 0 52 miss% 0.020701332519220617
plot_id,batch_id 0 53 miss% 0.01699555714923268
plot_id,batch_id 0 54 miss% 0.031286373807963
plot_id,batch_id 0 55 miss% 0.03715562113386546
plot_id,batch_id 0 56 miss% 0.03638073417311968
plot_id,batch_id 0 57 miss% 0.023667826242600377
plot_id,batch_id 0 58 miss% 0.02874358311424588
plot_id,batch_id 0 59 miss% 0.03574385529203277
plot_id,batch_id 0 60 miss% 0.04871813622634666
plot_id,batch_id 0 61 miss% 0.0463509322521064
plot_id,batch_id 0 62 miss% 0.027661299719575056
plot_id,batch_id 0 63 miss% 0.04666953108872206
plot_id,batch_id 0 64 miss% 0.03699024489447111
plot_id,batch_id 0 65 miss% 0.06189968851419974
plot_id,batch_id 0 66 miss% 0.04440009275992193
plot_id,batch_id 0 67 miss% 0.030640541710255795
plot_id,batch_id 0 68 miss% 0.03528295924417113
plot_id,batch_id 0 69 miss% 0.0374651347597377
plot_id,batch_id 0 70 miss% 0.0494893497366937
plot_id,batch_id 0 71 miss% 0.05881654402223029
plot_id,batch_id 0 72 miss% 0.030646595615855244
plot_id,batch_id 0 73 miss% 0.029412156024120784
plot_id,batch_id 0 74 miss% 0.03710909410294294
plot_id,batch_id 0 75 miss% 0.04667320655562464
plot_id,batch_id 0 76 miss% 0.03489290961531062
plot_id,batch_id 0 77 miss% 0.027551821676157117
plot_id,batch_id 0 78 miss% 0.025034475552672377
plot_id,batch_id 0 79 miss% 0.05799689048551885
plot_id,batch_id 0 80 miss% 0.03586852537095299
plot_id,batch_id 0 81 miss% 0.024749439426459224
plot_id,batch_id 0 82 miss% 0.02778380277931132
plot_id,batch_id 0 83 miss% 0.034376928970581276
plot_id,batch_id 0 84 miss% 0.023180988027115424
plot_id,batch_id 0 85 miss% 0.059511360564965886
plot_id,batch_id 0 86 miss% 0.02043743703216806
plot_id,batch_id 0 87 miss% 0.029221181760407947
plot_id,batch_id 0 88 miss% 0.03404952732355092
plot_id,batch_id 0 89 miss% 0.02856271249998888
plot_id,batch_id 0 90 miss% 0.03427602692795737
plot_id,batch_id 0 91 miss% 0.033527439712796266
plot_id,batch_id 0 92 miss% 0.027207646036956655
plot_id,batch_id 0 93 miss% 0.032078623169928884
plot_id,batch_id 0 94 miss% 0.0426622149384101
plot_id,batch_id 0 95 miss% 0.054361891268403836
plot_id,batch_id 0 96 miss% 0.03377678802162295
plot_id,batch_id 0 97 miss% 0.051591061557918344
plot_id,batch_id 0 98 miss% 0.031823500978638224
plot_id,batch_id 0 99 miss% 0.03176016324542718
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02449338 0.04307007 0.03872673 0.02484959 0.031575   0.03817347
 0.03831292 0.02955674 0.03828051 0.03510173 0.04438609 0.05201312
 0.03267496 0.03521147 0.03820182 0.04640633 0.04249891 0.03673147
 0.02922915 0.03294443 0.04380289 0.02664807 0.03056017 0.02939208
 0.04218761 0.05117601 0.02693734 0.02658188 0.03195827 0.03759614
 0.04626623 0.03066365 0.02729517 0.03082862 0.03226974 0.04409059
 0.02659387 0.02854929 0.02323565 0.02863654 0.08107941 0.02329465
 0.02603259 0.04414583 0.03008439 0.05534606 0.0234552  0.02014642
 0.02964659 0.03340105 0.03362507 0.02586399 0.02070133 0.01699556
 0.03128637 0.03715562 0.03638073 0.02366783 0.02874358 0.03574386
 0.04871814 0.04635093 0.0276613  0.04666953 0.03699024 0.06189969
 0.04440009 0.03064054 0.03528296 0.03746513 0.04948935 0.05881654
 0.0306466  0.02941216 0.03710909 0.04667321 0.03489291 0.02755182
 0.02503448 0.05799689 0.03586853 0.02474944 0.0277838  0.03437693
 0.02318099 0.05951136 0.02043744 0.02922118 0.03404953 0.02856271
 0.03427603 0.03352744 0.02720765 0.03207862 0.04266221 0.05436189
 0.03377679 0.05159106 0.0318235  0.03176016]
for model  45 the mean error 0.03559012673547556
all id 45 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 21 out win 3 err 0.03559012673547556
Launcher: Job 46 completed in 5000 seconds.
Launcher: Task 183 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  35921
Epoch:0, Train loss:0.831381, valid loss:0.823086
Epoch:1, Train loss:0.069474, valid loss:0.024039
Epoch:2, Train loss:0.037908, valid loss:0.010994
Epoch:3, Train loss:0.015992, valid loss:0.007201
Epoch:4, Train loss:0.011344, valid loss:0.005675
Epoch:5, Train loss:0.009432, valid loss:0.004505
Epoch:6, Train loss:0.008006, valid loss:0.003627
Epoch:7, Train loss:0.006881, valid loss:0.003004
Epoch:8, Train loss:0.006045, valid loss:0.003038
Epoch:9, Train loss:0.005348, valid loss:0.003018
Epoch:10, Train loss:0.004796, valid loss:0.002186
Epoch:11, Train loss:0.003719, valid loss:0.002015
Epoch:12, Train loss:0.003574, valid loss:0.001983
Epoch:13, Train loss:0.003399, valid loss:0.001845
Epoch:14, Train loss:0.003290, valid loss:0.001796
Epoch:15, Train loss:0.003191, valid loss:0.001902
Epoch:16, Train loss:0.003153, valid loss:0.001625
Epoch:17, Train loss:0.003004, valid loss:0.001662
Epoch:18, Train loss:0.002941, valid loss:0.001611
Epoch:19, Train loss:0.002843, valid loss:0.001807
Epoch:20, Train loss:0.002862, valid loss:0.001862
Epoch:21, Train loss:0.002347, valid loss:0.001314
Epoch:22, Train loss:0.002336, valid loss:0.001401
Epoch:23, Train loss:0.002355, valid loss:0.001308
Epoch:24, Train loss:0.002316, valid loss:0.001372
Epoch:25, Train loss:0.002252, valid loss:0.001490
Epoch:26, Train loss:0.002307, valid loss:0.001640
Epoch:27, Train loss:0.002248, valid loss:0.001380
Epoch:28, Train loss:0.002200, valid loss:0.001245
Epoch:29, Train loss:0.002171, valid loss:0.001282
Epoch:30, Train loss:0.002182, valid loss:0.001239
Epoch:31, Train loss:0.001949, valid loss:0.001174
Epoch:32, Train loss:0.001936, valid loss:0.001209
Epoch:33, Train loss:0.001914, valid loss:0.001180
Epoch:34, Train loss:0.001918, valid loss:0.001197
Epoch:35, Train loss:0.001896, valid loss:0.001187
Epoch:36, Train loss:0.001872, valid loss:0.001113
Epoch:37, Train loss:0.001900, valid loss:0.001125
Epoch:38, Train loss:0.001855, valid loss:0.001133
Epoch:39, Train loss:0.001852, valid loss:0.001260
Epoch:40, Train loss:0.001844, valid loss:0.001128
Epoch:41, Train loss:0.001731, valid loss:0.001068
Epoch:42, Train loss:0.001727, valid loss:0.001167
Epoch:43, Train loss:0.001724, valid loss:0.001046
Epoch:44, Train loss:0.001713, valid loss:0.001043
Epoch:45, Train loss:0.001713, valid loss:0.001104
Epoch:46, Train loss:0.001715, valid loss:0.001081
Epoch:47, Train loss:0.001700, valid loss:0.001048
Epoch:48, Train loss:0.001702, valid loss:0.001126
Epoch:49, Train loss:0.001696, valid loss:0.001060
Epoch:50, Train loss:0.001691, valid loss:0.001070
Epoch:51, Train loss:0.001636, valid loss:0.001011
Epoch:52, Train loss:0.001635, valid loss:0.001004
Epoch:53, Train loss:0.001633, valid loss:0.001024
Epoch:54, Train loss:0.001625, valid loss:0.001025
Epoch:55, Train loss:0.001621, valid loss:0.001036
Epoch:56, Train loss:0.001614, valid loss:0.001038
Epoch:57, Train loss:0.001616, valid loss:0.001020
Epoch:58, Train loss:0.001615, valid loss:0.001055
Epoch:59, Train loss:0.001614, valid loss:0.001060
Epoch:60, Train loss:0.001608, valid loss:0.001002
training time 4841.893951892853
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.037493460802243674
plot_id,batch_id 0 1 miss% 0.030920035115086633
plot_id,batch_id 0 2 miss% 0.026152326790109013
plot_id,batch_id 0 3 miss% 0.02095352521682377
plot_id,batch_id 0 4 miss% 0.03498674953075053
plot_id,batch_id 0 5 miss% 0.03990836700423448
plot_id,batch_id 0 6 miss% 0.03580443417323286
plot_id,batch_id 0 7 miss% 0.023733058521448323
plot_id,batch_id 0 8 miss% 0.025868355808924985
plot_id,batch_id 0 9 miss% 0.02872313382454524
plot_id,batch_id 0 10 miss% 0.03401263521521733
plot_id,batch_id 0 11 miss% 0.03401042888202354
plot_id,batch_id 0 12 miss% 0.027078356969195277
plot_id,batch_id 0 13 miss% 0.027148671558814477
plot_id,batch_id 0 14 miss% 0.040288766901677944
plot_id,batch_id 0 15 miss% 0.036594444008654865
plot_id,batch_id 0 16 miss% 0.02559520245130126
plot_id,batch_id 0 17 miss% 0.04332068178364491
plot_id,batch_id 0 18 miss% 0.039699238105525864
plot_id,batch_id 0 19 miss% 0.03833868321707385
plot_id,batch_id 0 20 miss% 0.03306407317312194
plot_id,batch_id 0 21 miss% 0.020994649630026282
plot_id,batch_id 0 22 miss% 0.03590339808748234
plot_id,batch_id 0 23 miss% 0.030446270125682698
plot_id,batch_id 0 24 miss% 0.04013984017731639
plot_id,batch_id 0 25 miss% 0.03566665678198655
plot_id,batch_id 0 26 miss% 0.029259390273682507
plot_id,batch_id 0 27 miss% 0.02588964682598877
plot_id,batch_id 0 28 miss% 0.021699132387899445
plot_id,batch_id 0 29 miss% 0.025796522385663354
plot_id,batch_id 0 30 miss% 0.03760333807210487
plot_id,batch_id 0 31 miss% 0.03891543850517463
plot_id,batch_id 0 32 miss% 0.03267658666031399
plot_id,batch_id 0 33 miss% 0.026212815102455135
plot_id,batch_id 0 34 miss% 0.031131519508980143
plot_id,batch_id 0 35 miss% 0.036070431017018845
plot_id,batch_id 0 36 miss% 0.03617604229840193
plot_id,batch_id 0 37 miss% 0.03978101964902578
plot_id,batch_id 0 38 miss% 0.036697924048873926
plot_id,batch_id 0 39 miss% 0.01997943908662707
plot_id,batch_id 0 40 miss% 0.05560805320606265
plot_id,batch_id 0 41 miss% 0.027679934056198154
plot_id,batch_id 0 42 miss% 0.01944943270451482
plot_id,batch_id 0 43 miss% 0.04405891569976781
plot_id,batch_id 0 44 miss% 0.02411382838069462
plot_id,batch_id 0 45 miss% 0.0615143577947826
plot_id,batch_id 0 46 miss% 0.026960711744001692
plot_id,batch_id 0 47 miss% 0.02135223105137946
plot_id,batch_id 0 48 miss% 0.031163639123269873
plot_id,batch_id 0 49 miss% 0.03051709226633402
plot_id,batch_id 0 50 miss% 0.033056267676299364
plot_id,batch_id 0 51 miss% 0.028377312774587472
plot_id,batch_id 0 52 miss% 0.027383164340562694
plot_id,batch_id 0 53 miss% 0.015443550510723516
plot_id,batch_id 0 54 miss% 0.017729679680886453
plot_id,batch_id 0 55 miss% 0.045473352033568476
plot_id,batch_id 0 56 miss% 0.05956069685539169
plot_id,batch_id 0 57 miss% 0.020143653839449372
plot_id,batch_id 0 58 miss% 0.030862220330292945
plot_id,batch_id 0 59 miss% 0.024827248917594972
plot_id,batch_id 0 60 miss% 0.05344716804836689
plot_id,batch_id 0 61 miss% 0.048413100589398766
plot_id,batch_id 0 62 miss% 0.03918498148738684
plot_id,batch_id 0 63 miss% 0.041587726374853855
plot_id,batch_id 0 64 miss% 0.03257974799424128
plot_id,batch_id 0 65 miss% 0.06604970498129875
plot_id,batch_id 0 66 miss% 0.03976863771422547
plot_id,batch_id 0 67 miss% 0.026421206976374883
plot_id,batch_id 0 68 miss% 0.025660726301415405
plot_id,batch_id 0 69 miss% 0.027797056576975374the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  28945
Epoch:0, Train loss:0.484413, valid loss:0.483743
Epoch:1, Train loss:0.041382, valid loss:0.013256
Epoch:2, Train loss:0.012616, valid loss:0.004869
Epoch:3, Train loss:0.007282, valid loss:0.004304
Epoch:4, Train loss:0.005840, valid loss:0.003486
Epoch:5, Train loss:0.005037, valid loss:0.002707
Epoch:6, Train loss:0.004376, valid loss:0.002167
Epoch:7, Train loss:0.003848, valid loss:0.001980
Epoch:8, Train loss:0.003498, valid loss:0.001772
Epoch:9, Train loss:0.003231, valid loss:0.001544
Epoch:10, Train loss:0.003155, valid loss:0.001639
Epoch:11, Train loss:0.002437, valid loss:0.001422
Epoch:12, Train loss:0.002399, valid loss:0.001618
Epoch:13, Train loss:0.002338, valid loss:0.001315
Epoch:14, Train loss:0.002300, valid loss:0.001358
Epoch:15, Train loss:0.002232, valid loss:0.001433
Epoch:16, Train loss:0.002223, valid loss:0.001267
Epoch:17, Train loss:0.002154, valid loss:0.001217
Epoch:18, Train loss:0.002118, valid loss:0.001237
Epoch:19, Train loss:0.002108, valid loss:0.001093
Epoch:20, Train loss:0.002029, valid loss:0.001175
Epoch:21, Train loss:0.001732, valid loss:0.001011
Epoch:22, Train loss:0.001718, valid loss:0.001215
Epoch:23, Train loss:0.001698, valid loss:0.001014
Epoch:24, Train loss:0.001694, valid loss:0.001069
Epoch:25, Train loss:0.001676, valid loss:0.001072
Epoch:26, Train loss:0.001642, valid loss:0.000993
Epoch:27, Train loss:0.001631, valid loss:0.001066
Epoch:28, Train loss:0.001628, valid loss:0.001072
Epoch:29, Train loss:0.001605, valid loss:0.001073
Epoch:30, Train loss:0.001626, valid loss:0.001158
Epoch:31, Train loss:0.001444, valid loss:0.000878
Epoch:32, Train loss:0.001438, valid loss:0.000925
Epoch:33, Train loss:0.001431, valid loss:0.000911
Epoch:34, Train loss:0.001429, valid loss:0.000881
Epoch:35, Train loss:0.001428, valid loss:0.000913
Epoch:36, Train loss:0.001412, valid loss:0.000925
Epoch:37, Train loss:0.001406, valid loss:0.000887
Epoch:38, Train loss:0.001405, valid loss:0.000958
Epoch:39, Train loss:0.001402, valid loss:0.000854
Epoch:40, Train loss:0.001386, valid loss:0.000919
Epoch:41, Train loss:0.001322, valid loss:0.000878
Epoch:42, Train loss:0.001313, valid loss:0.000849
Epoch:43, Train loss:0.001311, valid loss:0.000866
Epoch:44, Train loss:0.001318, valid loss:0.000862
Epoch:45, Train loss:0.001305, valid loss:0.000873
Epoch:46, Train loss:0.001306, valid loss:0.000877
Epoch:47, Train loss:0.001297, valid loss:0.000851
Epoch:48, Train loss:0.001295, valid loss:0.000846
Epoch:49, Train loss:0.001294, valid loss:0.000838
Epoch:50, Train loss:0.001291, valid loss:0.000885
Epoch:51, Train loss:0.001253, valid loss:0.000844
Epoch:52, Train loss:0.001252, valid loss:0.000853
Epoch:53, Train loss:0.001248, valid loss:0.000853
Epoch:54, Train loss:0.001247, valid loss:0.000860
Epoch:55, Train loss:0.001245, valid loss:0.000841
Epoch:56, Train loss:0.001244, valid loss:0.000843
Epoch:57, Train loss:0.001240, valid loss:0.000843
Epoch:58, Train loss:0.001243, valid loss:0.000854
Epoch:59, Train loss:0.001239, valid loss:0.000835
Epoch:60, Train loss:0.001239, valid loss:0.000842
training time 4847.411759853363
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.040204464340245494
plot_id,batch_id 0 1 miss% 0.021616256442050563
plot_id,batch_id 0 2 miss% 0.023487759949161052
plot_id,batch_id 0 3 miss% 0.024828664016981406
plot_id,batch_id 0 4 miss% 0.0492195768535373
plot_id,batch_id 0 5 miss% 0.024993759005591056
plot_id,batch_id 0 6 miss% 0.026159716545535294
plot_id,batch_id 0 7 miss% 0.02657635840507213
plot_id,batch_id 0 8 miss% 0.04522138466197866
plot_id,batch_id 0 9 miss% 0.03029250565467327
plot_id,batch_id 0 10 miss% 0.03394690551280708
plot_id,batch_id 0 11 miss% 0.040364989494732446
plot_id,batch_id 0 12 miss% 0.027938721102650072
plot_id,batch_id 0 13 miss% 0.032742263279120755
plot_id,batch_id 0 14 miss% 0.03936382905092329
plot_id,batch_id 0 15 miss% 0.04465750048974347
plot_id,batch_id 0 16 miss% 0.02086487239700265
plot_id,batch_id 0 17 miss% 0.04025728783531343
plot_id,batch_id 0 18 miss% 0.021606574664729254
plot_id,batch_id 0 19 miss% 0.030038734003259994
plot_id,batch_id 0 20 miss% 0.03916289700997674
plot_id,batch_id 0 21 miss% 0.025277968732611844
plot_id,batch_id 0 22 miss% 0.023428619717641378
plot_id,batch_id 0 23 miss% 0.017371551298734484
plot_id,batch_id 0 24 miss% 0.03890742717689328
plot_id,batch_id 0 25 miss% 0.0403739397164143
plot_id,batch_id 0 26 miss% 0.030240169837470396
plot_id,batch_id 0 27 miss% 0.024118610135243017
plot_id,batch_id 0 28 miss% 0.019172673412891885
plot_id,batch_id 0 29 miss% 0.036540157881650354
plot_id,batch_id 0 30 miss% 0.050695498375531105
plot_id,batch_id 0 31 miss% 0.023056915915249696
plot_id,batch_id 0 32 miss% 0.02957451471768738
plot_id,batch_id 0 33 miss% 0.03129115209134139
plot_id,batch_id 0 34 miss% 0.03318428953432536
plot_id,batch_id 0 35 miss% 0.03873392574802392
plot_id,batch_id 0 36 miss% 0.03585379048956115
plot_id,batch_id 0 37 miss% 0.02189313002560168
plot_id,batch_id 0 38 miss% 0.03944531579822317
plot_id,batch_id 0 39 miss% 0.02630207405797132
plot_id,batch_id 0 40 miss% 0.10955893249140894
plot_id,batch_id 0 41 miss% 0.028003986018902673
plot_id,batch_id 0 42 miss% 0.03726338239137808
plot_id,batch_id 0 43 miss% 0.04085235963735038
plot_id,batch_id 0 44 miss% 0.03153325794566693
plot_id,batch_id 0 45 miss% 0.06403177085911144
plot_id,batch_id 0 46 miss% 0.025845807860485175
plot_id,batch_id 0 47 miss% 0.031208759021774647
plot_id,batch_id 0 48 miss% 0.03357506447899457
plot_id,batch_id 0 49 miss% 0.036829542168217745
plot_id,batch_id 0 50 miss% 0.03188953529469354
plot_id,batch_id 0 51 miss% 0.023743517454429618
plot_id,batch_id 0 52 miss% 0.02202528730963531
plot_id,batch_id 0 53 miss% 0.02029193458443296
plot_id,batch_id 0 54 miss% 0.026805506771934513
plot_id,batch_id 0 55 miss% 0.03700648042231796
plot_id,batch_id 0 56 miss% 0.04042935482823088
plot_id,batch_id 0 57 miss% 0.03853611356452399
plot_id,batch_id 0 58 miss% 0.025472116337183853
plot_id,batch_id 0 59 miss% 0.018490844582029528
plot_id,batch_id 0 60 miss% 0.046383397097772984
plot_id,batch_id 0 61 miss% 0.04669401740172257
plot_id,batch_id 0 62 miss% 0.025958372555519735
plot_id,batch_id 0 63 miss% 0.03670130728318932
plot_id,batch_id 0 64 miss% 0.030156943333234033
plot_id,batch_id 0 65 miss% 0.05938254164543629
plot_id,batch_id 0 66 miss% 0.043708017034760745
plot_id,batch_id 0 67 miss% 0.030049136385333356
plot_id,batch_id 0 68 miss% 0.03242141650863333
plot_id,batch_id 0 69 
plot_id,batch_id 0 70 miss% 0.07063014490037577
plot_id,batch_id 0 71 miss% 0.041272005002362874
plot_id,batch_id 0 72 miss% 0.03146702607245677
plot_id,batch_id 0 73 miss% 0.03200601894287834
plot_id,batch_id 0 74 miss% 0.05254621710360539
plot_id,batch_id 0 75 miss% 0.04590983239872371
plot_id,batch_id 0 76 miss% 0.050876979820316304
plot_id,batch_id 0 77 miss% 0.03553849972473166
plot_id,batch_id 0 78 miss% 0.034793260523042546
plot_id,batch_id 0 79 miss% 0.03782879232973424
plot_id,batch_id 0 80 miss% 0.02705647434147558
plot_id,batch_id 0 81 miss% 0.02813827645629261
plot_id,batch_id 0 82 miss% 0.03409472648429985
plot_id,batch_id 0 83 miss% 0.027622931487925047
plot_id,batch_id 0 84 miss% 0.03187847095854178
plot_id,batch_id 0 85 miss% 0.03881077080590132
plot_id,batch_id 0 86 miss% 0.019964784148780775
plot_id,batch_id 0 87 miss% 0.03398048984067873
plot_id,batch_id 0 88 miss% 0.02467513297658317
plot_id,batch_id 0 89 miss% 0.01609614536629234
plot_id,batch_id 0 90 miss% 0.0332085229263622
plot_id,batch_id 0 91 miss% 0.032633788068079934
plot_id,batch_id 0 92 miss% 0.030399772476144095
plot_id,batch_id 0 93 miss% 0.029165724892115063
plot_id,batch_id 0 94 miss% 0.03212040825771232
plot_id,batch_id 0 95 miss% 0.06470063309643716
plot_id,batch_id 0 96 miss% 0.037206222671271695
plot_id,batch_id 0 97 miss% 0.04683827419032236
plot_id,batch_id 0 98 miss% 0.03164201106262665
plot_id,batch_id 0 99 miss% 0.030901600876913723
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03749346 0.03092004 0.02615233 0.02095353 0.03498675 0.03990837
 0.03580443 0.02373306 0.02586836 0.02872313 0.03401264 0.03401043
 0.02707836 0.02714867 0.04028877 0.03659444 0.0255952  0.04332068
 0.03969924 0.03833868 0.03306407 0.02099465 0.0359034  0.03044627
 0.04013984 0.03566666 0.02925939 0.02588965 0.02169913 0.02579652
 0.03760334 0.03891544 0.03267659 0.02621282 0.03113152 0.03607043
 0.03617604 0.03978102 0.03669792 0.01997944 0.05560805 0.02767993
 0.01944943 0.04405892 0.02411383 0.06151436 0.02696071 0.02135223
 0.03116364 0.03051709 0.03305627 0.02837731 0.02738316 0.01544355
 0.01772968 0.04547335 0.0595607  0.02014365 0.03086222 0.02482725
 0.05344717 0.0484131  0.03918498 0.04158773 0.03257975 0.0660497
 0.03976864 0.02642121 0.02566073 0.02779706 0.07063014 0.04127201
 0.03146703 0.03200602 0.05254622 0.04590983 0.05087698 0.0355385
 0.03479326 0.03782879 0.02705647 0.02813828 0.03409473 0.02762293
 0.03187847 0.03881077 0.01996478 0.03398049 0.02467513 0.01609615
 0.03320852 0.03263379 0.03039977 0.02916572 0.03212041 0.06470063
 0.03720622 0.04683827 0.03164201 0.0309016 ]
for model  18 the mean error 0.03414924027912239
all id 18 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 21 out win 3 err 0.03414924027912239
Launcher: Job 19 completed in 5044 seconds.
Launcher: Task 92 done. Exiting.
miss% 0.04108623541463179
plot_id,batch_id 0 70 miss% 0.032409845724140136
plot_id,batch_id 0 71 miss% 0.06664053204309374
plot_id,batch_id 0 72 miss% 0.030944234980609095
plot_id,batch_id 0 73 miss% 0.031314005315688044
plot_id,batch_id 0 74 miss% 0.032130070176473915
plot_id,batch_id 0 75 miss% 0.04197713002905527
plot_id,batch_id 0 76 miss% 0.050851925121382396
plot_id,batch_id 0 77 miss% 0.03503128637874379
plot_id,batch_id 0 78 miss% 0.04249046897137353
plot_id,batch_id 0 79 miss% 0.04738377255932924
plot_id,batch_id 0 80 miss% 0.03940481080675832
plot_id,batch_id 0 81 miss% 0.0298461336589851
plot_id,batch_id 0 82 miss% 0.03248186281111579
plot_id,batch_id 0 83 miss% 0.02743498688737006
plot_id,batch_id 0 84 miss% 0.025694990024182535
plot_id,batch_id 0 85 miss% 0.051913665132854336
plot_id,batch_id 0 86 miss% 0.039481361740495935
plot_id,batch_id 0 87 miss% 0.03521017217035388
plot_id,batch_id 0 88 miss% 0.03272837970205293
plot_id,batch_id 0 89 miss% 0.03336053349636848
plot_id,batch_id 0 90 miss% 0.028852613824260336
plot_id,batch_id 0 91 miss% 0.03876842542205666
plot_id,batch_id 0 92 miss% 0.03730700057908668
plot_id,batch_id 0 93 miss% 0.028345192716167367
plot_id,batch_id 0 94 miss% 0.02932833647307101
plot_id,batch_id 0 95 miss% 0.046454489693484015
plot_id,batch_id 0 96 miss% 0.02365755216026775
plot_id,batch_id 0 97 miss% 0.0467180466972142
plot_id,batch_id 0 98 miss% 0.03157865583266486
plot_id,batch_id 0 99 miss% 0.03184691492671453
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04020446 0.02161626 0.02348776 0.02482866 0.04921958 0.02499376
 0.02615972 0.02657636 0.04522138 0.03029251 0.03394691 0.04036499
 0.02793872 0.03274226 0.03936383 0.0446575  0.02086487 0.04025729
 0.02160657 0.03003873 0.0391629  0.02527797 0.02342862 0.01737155
 0.03890743 0.04037394 0.03024017 0.02411861 0.01917267 0.03654016
 0.0506955  0.02305692 0.02957451 0.03129115 0.03318429 0.03873393
 0.03585379 0.02189313 0.03944532 0.02630207 0.10955893 0.02800399
 0.03726338 0.04085236 0.03153326 0.06403177 0.02584581 0.03120876
 0.03357506 0.03682954 0.03188954 0.02374352 0.02202529 0.02029193
 0.02680551 0.03700648 0.04042935 0.03853611 0.02547212 0.01849084
 0.0463834  0.04669402 0.02595837 0.03670131 0.03015694 0.05938254
 0.04370802 0.03004914 0.03242142 0.04108624 0.03240985 0.06664053
 0.03094423 0.03131401 0.03213007 0.04197713 0.05085193 0.03503129
 0.04249047 0.04738377 0.03940481 0.02984613 0.03248186 0.02743499
 0.02569499 0.05191367 0.03948136 0.03521017 0.03272838 0.03336053
 0.02885261 0.03876843 0.037307   0.02834519 0.02932834 0.04645449
 0.02365755 0.04671805 0.03157866 0.03184691]
for model  90 the mean error 0.03486529080116503
all id 90 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 25 out win 3 err 0.03486529080116503
Launcher: Job 91 completed in 5044 seconds.
Launcher: Task 149 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  46193
Epoch:0, Train loss:0.694666, valid loss:0.659432
Epoch:1, Train loss:0.513650, valid loss:0.519863
Epoch:2, Train loss:0.497905, valid loss:0.517937
Epoch:3, Train loss:0.495663, valid loss:0.516848
Epoch:4, Train loss:0.494127, valid loss:0.516183
Epoch:5, Train loss:0.493220, valid loss:0.516071
Epoch:6, Train loss:0.492462, valid loss:0.515421
Epoch:7, Train loss:0.492161, valid loss:0.515496
Epoch:8, Train loss:0.491593, valid loss:0.515336
Epoch:9, Train loss:0.491555, valid loss:0.515477
Epoch:10, Train loss:0.491422, valid loss:0.515032
Epoch:11, Train loss:0.490336, valid loss:0.514814
Epoch:12, Train loss:0.490302, valid loss:0.514571
Epoch:13, Train loss:0.490270, valid loss:0.514583
Epoch:14, Train loss:0.490184, valid loss:0.514845
Epoch:15, Train loss:0.490162, valid loss:0.514504
Epoch:16, Train loss:0.490040, valid loss:0.514505
Epoch:17, Train loss:0.490035, valid loss:0.514593
Epoch:18, Train loss:0.490000, valid loss:0.514649
Epoch:19, Train loss:0.489954, valid loss:0.514816
Epoch:20, Train loss:0.489920, valid loss:0.514575
Epoch:21, Train loss:0.489488, valid loss:0.514375
Epoch:22, Train loss:0.489471, valid loss:0.514383
Epoch:23, Train loss:0.489496, valid loss:0.514434
Epoch:24, Train loss:0.489431, valid loss:0.514326
Epoch:25, Train loss:0.489422, valid loss:0.514307
Epoch:26, Train loss:0.489427, valid loss:0.514296
Epoch:27, Train loss:0.489415, valid loss:0.514327
Epoch:28, Train loss:0.489383, valid loss:0.514320
Epoch:29, Train loss:0.489358, valid loss:0.514273
Epoch:30, Train loss:0.489377, valid loss:0.514234
Epoch:31, Train loss:0.489142, valid loss:0.514289
Epoch:32, Train loss:0.489125, valid loss:0.514245
Epoch:33, Train loss:0.489107, valid loss:0.514188
Epoch:34, Train loss:0.489115, valid loss:0.514236
Epoch:35, Train loss:0.489111, valid loss:0.514212
Epoch:36, Train loss:0.489083, valid loss:0.514242
Epoch:37, Train loss:0.489106, valid loss:0.514262
Epoch:38, Train loss:0.489123, valid loss:0.514221
Epoch:39, Train loss:0.489074, valid loss:0.514233
Epoch:40, Train loss:0.489069, valid loss:0.514224
Epoch:41, Train loss:0.488967, valid loss:0.514145
Epoch:42, Train loss:0.488966, valid loss:0.514182
Epoch:43, Train loss:0.488962, valid loss:0.514164
Epoch:44, Train loss:0.488956, valid loss:0.514168
Epoch:45, Train loss:0.488954, valid loss:0.514144
Epoch:46, Train loss:0.488970, valid loss:0.514150
Epoch:47, Train loss:0.488949, valid loss:0.514141
Epoch:48, Train loss:0.488941, valid loss:0.514187
Epoch:49, Train loss:0.488946, valid loss:0.514155
Epoch:50, Train loss:0.488935, valid loss:0.514162
Epoch:51, Train loss:0.488891, valid loss:0.514135
Epoch:52, Train loss:0.488889, valid loss:0.514127
Epoch:53, Train loss:0.488889, valid loss:0.514140
Epoch:54, Train loss:0.488881, valid loss:0.514119
Epoch:55, Train loss:0.488884, valid loss:0.514153
Epoch:56, Train loss:0.488882, valid loss:0.514119
Epoch:57, Train loss:0.488880, valid loss:0.514137
Epoch:58, Train loss:0.488876, valid loss:0.514158
Epoch:59, Train loss:0.488875, valid loss:0.514140
Epoch:60, Train loss:0.488870, valid loss:0.514136
training time 4890.275202512741
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.7750035987779753
plot_id,batch_id 0 1 miss% 0.8223188417201974
plot_id,batch_id 0 2 miss% 0.8305181251836556
plot_id,batch_id 0 3 miss% 0.8341835839889822
plot_id,batch_id 0 4 miss% 0.8361209566282874
plot_id,batch_id 0 5 miss% 0.7697783770328241
plot_id,batch_id 0 6 miss% 0.8179676346750387
plot_id,batch_id 0 7 miss% 0.8296916846694444
plot_id,batch_id 0 8 miss% 0.8336941394084024
plot_id,batch_id 0 9 miss% 0.8388445756510063
plot_id,batch_id 0 10 miss% 0.750056503235109
plot_id,batch_id 0 11 miss% 0.8181377304410432
plot_id,batch_id 0 12 miss% 0.8271012163091954
plot_id,batch_id 0 13 miss% 0.8332076790010493
plot_id,batch_id 0 14 miss% 0.8362543757196046
plot_id,batch_id 0 15 miss% 0.7644086314822686
plot_id,batch_id 0 16 miss% 0.8162597025764154
plot_id,batch_id 0 17 miss% 0.8283214793036067
plot_id,batch_id 0 18 miss% 0.8346599560235136
plot_id,batch_id 0 19 miss% 0.834399673823976
plot_id,batch_id 0 20 miss% 0.8102077566396029
plot_id,batch_id 0 21 miss% 0.8303322794981495
plot_id,batch_id 0 22 miss% 0.8382422184441871
plot_id,batch_id 0 23 miss% 0.8368966635961375
plot_id,batch_id 0 24 miss% 0.8415467686114442
plot_id,batch_id 0 25 miss% 0.7949366943584689
plot_id,batch_id 0 26 miss% 0.8283632725032266
plot_id,batch_id 0 27 miss% 0.8325304183934418
plot_id,batch_id 0 28 miss% 0.8358902495876381
plot_id,batch_id 0 29 miss% 0.836438273277921
plot_id,batch_id 0 30 miss% 0.7877778534795626
plot_id,batch_id 0 31 miss% 0.8253241215921133
plot_id,batch_id 0 32 miss% 0.8339022270827982
plot_id,batch_id 0 33 miss% 0.8355457797297068
plot_id,batch_id 0 34 miss% 0.8380318444249308
plot_id,batch_id 0 35 miss% 0.7873703087734525
plot_id,batch_id 0 36 miss% 0.8307053578726894
plot_id,batch_id 0 37 miss% 0.8327957559465813
plot_id,batch_id 0 38 miss% 0.8378174269128444
plot_id,batch_id 0 39 miss% 0.8387828279847231
plot_id,batch_id 0 40 miss% 0.825684816335351
plot_id,batch_id 0 41 miss% 0.8361293022778913
plot_id,batch_id 0 42 miss% 0.8381293502515433
plot_id,batch_id 0 43 miss% 0.8421189807726858
plot_id,batch_id 0 44 miss% 0.8435799861485745
plot_id,batch_id 0 45 miss% 0.8139781729980842
plot_id,batch_id 0 46 miss% 0.8344823046640435
plot_id,batch_id 0 47 miss% 0.8387966279249517
plot_id,batch_id 0 48 miss% 0.8393997310923279
plot_id,batch_id 0 49 miss% 0.8436596408107307
plot_id,batch_id 0 50 miss% 0.8196324072006009
plot_id,batch_id 0 51 miss% 0.8341083651765271
plot_id,batch_id 0 52 miss% 0.8363096687563302
plot_id,batch_id 0 53 miss% 0.8400215005813187
plot_id,batch_id 0 54 miss% 0.8449507382446318
plot_id,batch_id 0 55 miss% 0.8104803210885881
plot_id,batch_id 0 56 miss% 0.8335535952487374
plot_id,batch_id 0 57 miss% 0.8376083586519746
plot_id,batch_id 0 58 miss% 0.8391855437527695
plot_id,batch_id 0 59 miss% 0.8437448856662902
plot_id,batch_id 0 60 miss% 0.7067108083369731
plot_id,batch_id 0 61 miss% 0.8081763864512781
plot_id,batch_id 0 62 miss% 0.8170450910093422
plot_id,batch_id 0 63 miss% 0.8284026874990617
plot_id,batch_id 0 64 miss% 0.8301141018938053
plot_id,batch_id 0 65 miss% 0.709724769711667
plot_id,batch_id 0 66 miss% 0.7969376818289783
plot_id,batch_id 0 67 miss% 0.8142952462847556
plot_id,batch_id 0 68 miss% 0.8243224619918887
plot_id,batch_id 0 69 miss% 0.8282164894855899
plot_id,batch_id 0 70 miss% 0.6665331183248291
plot_id,batch_id 0 71 miss% 0.787674370255007
plot_id,batch_id 0 72 miss% 0.801172469483891
plot_id,batch_id 0 73 miss% 0.8180774629650971
plot_id,batch_id 0 74 miss% 0.8255168116873411
plot_id,batch_id 0 75 miss% 0.6699860485470743
plot_id,batch_id 0 76 miss% 0.7863780872014136
plot_id,batch_id 0 77 miss% 0.7962366755303334
plot_id,batch_id 0 78 miss% 0.811784798356416
plot_id,batch_id 0 79 miss% 0.8190093216722434
plot_id,batch_id 0 80 miss% 0.7315940147660873
plot_id,batch_id 0 81 miss% 0.815266073166299
plot_id,batch_id 0 82 miss% 0.8258917325089598
plot_id,batch_id 0 83 miss% 0.8303475891647468
plot_id,batch_id 0 84 miss% 0.833651988227672
plot_id,batch_id 0 85 miss% 0.7258469405046902
plot_id,batch_id 0 86 miss% 0.8023959654913522
plot_id,batch_id 0 87 miss% 0.8226963080944399
plot_id,batch_id 0 88 miss% 0.8294591131693664
plot_id,batch_id 0 89 miss% 0.8314639383470643
plot_id,batch_id 0 90 miss% 0.6865775169302182
plot_id,batch_id 0 91 miss% 0.8020456711763105
plot_id,batch_id 0 92 miss% 0.8172202322849645
plot_id,batch_id 0 93 miss% 0.8184391467438641
plot_id,batch_id 0 94 miss% 0.8302780137602109
plot_id,batch_id 0 95 miss% 0.7181550755100798
plot_id,batch_id 0 96 miss% 0.7951022282969131
plot_id,batch_id 0 97 miss% 0.8077597299795178
plot_id,batch_id 0 98 miss% 0.8191604409874375
plot_id,batch_id 0 99 miss% 0.8221372402819862
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.7750036  0.82231884 0.83051813 0.83418358 0.83612096 0.76977838
 0.81796763 0.82969168 0.83369414 0.83884458 0.7500565  0.81813773
 0.82710122 0.83320768 0.83625438 0.76440863 0.8162597  0.82832148
 0.83465996 0.83439967 0.81020776 0.83033228 0.83824222 0.83689666
 0.84154677 0.79493669 0.82836327 0.83253042 0.83589025 0.83643827
 0.78777785 0.82532412 0.83390223 0.83554578 0.83803184 0.78737031
 0.83070536 0.83279576 0.83781743 0.83878283 0.82568482 0.8361293
 0.83812935 0.84211898 0.84357999 0.81397817 0.8344823  0.83879663
 0.83939973 0.84365964 0.81963241 0.83410837 0.83630967 0.8400215
 0.84495074 0.81048032 0.8335536  0.83760836 0.83918554 0.84374489
 0.70671081 0.80817639 0.81704509 0.82840269 0.8301141  0.70972477
 0.79693768 0.81429525 0.82432246 0.82821649 0.66653312 0.78767437
 0.80117247 0.81807746 0.82551681 0.66998605 0.78637809 0.79623668
 0.8117848  0.81900932 0.73159401 0.81526607 0.82589173 0.83034759
 0.83365199 0.72584694 0.80239597 0.82269631 0.82945911 0.83146394
 0.68657752 0.80204567 0.81722023 0.81843915 0.83027801 0.71815508
 0.79510223 0.80775973 0.81916044 0.82213724]
for model  4 the mean error 0.8127172470991433
all id 4 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 21 out win 4 err 0.8127172470991433
Launcher: Job 5 completed in 5072 seconds.
Launcher: Task 32 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  46193
Epoch:0, Train loss:0.694666, valid loss:0.659432
Epoch:1, Train loss:0.514354, valid loss:0.519897
Epoch:2, Train loss:0.499025, valid loss:0.518573
Epoch:3, Train loss:0.495896, valid loss:0.516229
Epoch:4, Train loss:0.494970, valid loss:0.517146
Epoch:5, Train loss:0.494173, valid loss:0.517366
Epoch:6, Train loss:0.493335, valid loss:0.515756
Epoch:7, Train loss:0.492885, valid loss:0.515840
Epoch:8, Train loss:0.492867, valid loss:0.516172
Epoch:9, Train loss:0.492772, valid loss:0.516544
Epoch:10, Train loss:0.492156, valid loss:0.516284
Epoch:11, Train loss:0.490680, valid loss:0.514814
Epoch:12, Train loss:0.490669, valid loss:0.514630
Epoch:13, Train loss:0.490683, valid loss:0.514654
Epoch:14, Train loss:0.490729, valid loss:0.514999
Epoch:15, Train loss:0.490590, valid loss:0.514495
Epoch:16, Train loss:0.490558, valid loss:0.514742
Epoch:17, Train loss:0.490427, valid loss:0.514704
Epoch:18, Train loss:0.490458, valid loss:0.515385
Epoch:19, Train loss:0.490422, valid loss:0.514701
Epoch:20, Train loss:0.490216, valid loss:0.514640
Epoch:21, Train loss:0.489491, valid loss:0.514372
Epoch:22, Train loss:0.489545, valid loss:0.514234
Epoch:23, Train loss:0.489504, valid loss:0.514400
Epoch:24, Train loss:0.489539, valid loss:0.514148
Epoch:25, Train loss:0.489474, valid loss:0.514576
Epoch:26, Train loss:0.489502, valid loss:0.514335
Epoch:27, Train loss:0.489487, valid loss:0.514509
Epoch:28, Train loss:0.489417, valid loss:0.514133
Epoch:29, Train loss:0.489423, valid loss:0.514236
Epoch:30, Train loss:0.489396, valid loss:0.514521
Epoch:31, Train loss:0.489022, valid loss:0.514184
Epoch:32, Train loss:0.489037, valid loss:0.513993
Epoch:33, Train loss:0.488993, valid loss:0.514067
Epoch:34, Train loss:0.489030, valid loss:0.514067
Epoch:35, Train loss:0.488985, valid loss:0.514006
Epoch:36, Train loss:0.489012, valid loss:0.514060
Epoch:37, Train loss:0.489003, valid loss:0.514220
Epoch:38, Train loss:0.488980, valid loss:0.514029
Epoch:39, Train loss:0.488965, valid loss:0.514252
Epoch:40, Train loss:0.488991, valid loss:0.514011
Epoch:41, Train loss:0.488793, valid loss:0.513928
Epoch:42, Train loss:0.488778, valid loss:0.513941
Epoch:43, Train loss:0.488781, valid loss:0.513895
Epoch:44, Train loss:0.488782, valid loss:0.513917
Epoch:45, Train loss:0.488776, valid loss:0.513942
Epoch:46, Train loss:0.488769, valid loss:0.513958
Epoch:47, Train loss:0.488780, valid loss:0.513979
Epoch:48, Train loss:0.488777, valid loss:0.514060
Epoch:49, Train loss:0.488774, valid loss:0.513953
Epoch:50, Train loss:0.488744, valid loss:0.514129
Epoch:51, Train loss:0.488670, valid loss:0.513895
Epoch:52, Train loss:0.488671, valid loss:0.513921
Epoch:53, Train loss:0.488666, valid loss:0.513919
Epoch:54, Train loss:0.488659, valid loss:0.513896
Epoch:55, Train loss:0.488663, valid loss:0.513906
Epoch:56, Train loss:0.488657, valid loss:0.513866
Epoch:57, Train loss:0.488654, valid loss:0.513914
Epoch:58, Train loss:0.488656, valid loss:0.513903
Epoch:59, Train loss:0.488653, valid loss:0.513993
Epoch:60, Train loss:0.488651, valid loss:0.513935
training time 5017.055646896362
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.776757348671106
plot_id,batch_id 0 1 miss% 0.8224602221251895
plot_id,batch_id 0 2 miss% 0.830454930835407
plot_id,batch_id 0 3 miss% 0.8328932496060947
plot_id,batch_id 0 4 miss% 0.8348486701011492
plot_id,batch_id 0 5 miss% 0.7734149739505141
plot_id,batch_id 0 6 miss% 0.8177951015956638
plot_id,batch_id 0 7 miss% 0.8282224272053403
plot_id,batch_id 0 8 miss% 0.835036495891122
plot_id,batch_id 0 9 miss% 0.8380085194865181
plot_id,batch_id 0 10 miss% 0.7585019595913767
plot_id,batch_id 0 11 miss% 0.8187479411178731
plot_id,batch_id 0 12 miss% 0.8270671909654636
plot_id,batch_id 0 13 miss% 0.8345589385206068
plot_id,batch_id 0 14 miss% 0.8338124159273641
plot_id,batch_id 0 15 miss% 0.7709842052857478
plot_id,batch_id 0 16 miss% 0.8175743578209242
plot_id,batch_id 0 17 miss% 0.8271065208107583
plot_id,batch_id 0 18 miss% 0.833644816487383
plot_id,batch_id 0 19 miss% 0.832920258402964
plot_id,batch_id 0 20 miss% 0.8043097154067161
plot_id,batch_id 0 21 miss% 0.8293566413004849
plot_id,batch_id 0 22 miss% 0.8359622154069832
plot_id,batch_id 0 23 miss% 0.8368719328507698
plot_id,batch_id 0 24 miss% 0.8421711211448997
plot_id,batch_id 0 25 miss% 0.8001763379265007
plot_id,batch_id 0 26 miss% 0.8285425928262211
plot_id,batch_id 0 27 miss% 0.831590115630774
plot_id,batch_id 0 28 miss% 0.8344043132011297
plot_id,batch_id 0 29 miss% 0.8368894167977188
plot_id,batch_id 0 30 miss% 0.7917891622852357
plot_id,batch_id 0 31 miss% 0.8254072035133221
plot_id,batch_id 0 32 miss% 0.8326507856446095
plot_id,batch_id 0 33 miss% 0.8344257226137779
plot_id,batch_id 0 34 miss% 0.8375064905747492
plot_id,batch_id 0 35 miss% 0.785009727943802
plot_id,batch_id 0 36 miss% 0.8324858883069737
plot_id,batch_id 0 37 miss% 0.8326044289646509
plot_id,batch_id 0 38 miss% 0.8365109554494062
plot_id,batch_id 0 39 miss% 0.8379127761563347
plot_id,batch_id 0 40 miss% 0.8151911263151342
plot_id,batch_id 0 41 miss% 0.834196157205808
plot_id,batch_id 0 42 miss% 0.8366637255518538
plot_id,batch_id 0 43 miss% 0.8407106141807701
plot_id,batch_id 0 44 miss% 0.8431975359221283
plot_id,batch_id 0 45 miss% 0.8137914241934737
plot_id,batch_id 0 46 miss% 0.8338381970117693
plot_id,batch_id 0 47 miss% 0.8375738090897156
plot_id,batch_id 0 48 miss% 0.8386402849275063
plot_id,batch_id 0 49 miss% 0.8418389482220225
plot_id,batch_id 0 50 miss% 0.8184808586399579
plot_id,batch_id 0 51 miss% 0.8327367161276714
plot_id,batch_id 0 52 miss% 0.8353197110544358
plot_id,batch_id 0 53 miss% 0.8377375657580938
plot_id,batch_id 0 54 miss% 0.8433852543593279
plot_id,batch_id 0 55 miss% 0.8130197921834518
plot_id,batch_id 0 56 miss% 0.8330745381312503
plot_id,batch_id 0 57 miss% 0.8362864714851304
plot_id,batch_id 0 58 miss% 0.839251332303547
plot_id,batch_id 0 59 miss% 0.8424862314902087
plot_id,batch_id 0 60 miss% 0.7138162283775914
plot_id,batch_id 0 61 miss% 0.8017982384750876
plot_id,batch_id 0 62 miss% 0.816747329064205
plot_id,batch_id 0 63 miss% 0.8258326268998086
plot_id,batch_id 0 64 miss% 0.8292262096350531
plot_id,batch_id 0 65 miss% 0.7168424102598001
plot_id,batch_id 0 66 miss% 0.7909507748424148
plot_id,batch_id 0 67 miss% 0.8111131952088662
plot_id,batch_id 0 68 miss% 0.8234420142830078
plot_id,batch_id 0 69 miss% 0.8270041390182447
plot_id,batch_id 0 70 miss% 0.6671534378884004
plot_id,batch_id 0 71 miss% 0.7926575342058995
plot_id,batch_id 0 72 miss% 0.8015710578951488
plot_id,batch_id 0 73 miss% 0.8151850647909836
plot_id,batch_id 0 74 miss% 0.8233261335033026
plot_id,batch_id 0 75 miss% 0.6733558814180426
plot_id,batch_id 0 76 miss% 0.7938394187496673
plot_id,batch_id 0 77 miss% 0.7921970747829911
plot_id,batch_id 0 78 miss% 0.8133853018982903
plot_id,batch_id 0 79 miss% 0.8158237104156488
plot_id,batch_id 0 80 miss% 0.7372969106518932
plot_id,batch_id 0 81 miss% 0.8186163798164717
plot_id,batch_id 0 82 miss% 0.8248004518505887
plot_id,batch_id 0 83 miss% 0.8328857078757442
plot_id,batch_id 0 84 miss% 0.8322879770157741
plot_id,batch_id 0 85 miss% 0.7299799489418015
plot_id,batch_id 0 86 miss% 0.8028052347234997
plot_id,batch_id 0 87 miss% 0.8211686202829905
plot_id,batch_id 0 88 miss% 0.8291504184767841
plot_id,batch_id 0 89 miss% 0.8308884004714859
plot_id,batch_id 0 90 miss% 0.6916835389388234
plot_id,batch_id 0 91 miss% 0.7956741686751242
plot_id,batch_id 0 92 miss% 0.8121197219612969
plot_id,batch_id 0 93 miss% 0.8226507325392377
plot_id,batch_id 0 94 miss% 0.8294686241275969
plot_id,batch_id 0 95 miss% 0.7163819067170231
plot_id,batch_id 0 96 miss% 0.7868404371450919
plot_id,batch_id 0 97 miss% 0.8083367326971882
plot_id,batch_id 0 98 miss% 0.8215384692451451
plot_id,batch_id 0 99 miss% 0.824312589479188
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.77675735 0.82246022 0.83045493 0.83289325 0.83484867 0.77341497
 0.8177951  0.82822243 0.8350365  0.83800852 0.75850196 0.81874794
 0.82706719 0.83455894 0.83381242 0.77098421 0.81757436 0.82710652
 0.83364482 0.83292026 0.80430972 0.82935664 0.83596222 0.83687193
 0.84217112 0.80017634 0.82854259 0.83159012 0.83440431 0.83688942
 0.79178916 0.8254072  0.83265079 0.83442572 0.83750649 0.78500973
 0.83248589 0.83260443 0.83651096 0.83791278 0.81519113 0.83419616
 0.83666373 0.84071061 0.84319754 0.81379142 0.8338382  0.83757381
 0.83864028 0.84183895 0.81848086 0.83273672 0.83531971 0.83773757
 0.84338525 0.81301979 0.83307454 0.83628647 0.83925133 0.84248623
 0.71381623 0.80179824 0.81674733 0.82583263 0.82922621 0.71684241
 0.79095077 0.8111132  0.82344201 0.82700414 0.66715344 0.79265753
 0.80157106 0.81518506 0.82332613 0.67335588 0.79383942 0.79219707
 0.8133853  0.81582371 0.73729691 0.81861638 0.82480045 0.83288571
 0.83228798 0.72997995 0.80280523 0.82116862 0.82915042 0.8308884
 0.69168354 0.79567417 0.81211972 0.82265073 0.82946862 0.71638191
 0.78684044 0.80833673 0.82153847 0.82431259]
for model  58 the mean error 0.8125296313574207
all id 58 hidden_dim 24 learning_rate 0.02 num_layers 3 frames 21 out win 4 err 0.8125296313574207
Launcher: Job 59 completed in 5198 seconds.
Launcher: Task 66 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  46193
Epoch:0, Train loss:0.567454, valid loss:0.526344
Epoch:1, Train loss:0.350100, valid loss:0.358082
Epoch:2, Train loss:0.341315, valid loss:0.357754
Epoch:3, Train loss:0.340073, valid loss:0.356980
Epoch:4, Train loss:0.339224, valid loss:0.357342
Epoch:5, Train loss:0.338895, valid loss:0.356993
Epoch:6, Train loss:0.338609, valid loss:0.356409
Epoch:7, Train loss:0.338397, valid loss:0.356570
Epoch:8, Train loss:0.338194, valid loss:0.356110
Epoch:9, Train loss:0.338032, valid loss:0.356080
Epoch:10, Train loss:0.337930, valid loss:0.356078
Epoch:11, Train loss:0.337193, valid loss:0.355693
Epoch:12, Train loss:0.337169, valid loss:0.355927
Epoch:13, Train loss:0.337151, valid loss:0.355922
Epoch:14, Train loss:0.337157, valid loss:0.355654
Epoch:15, Train loss:0.337076, valid loss:0.356122
Epoch:16, Train loss:0.337027, valid loss:0.355673
Epoch:17, Train loss:0.337060, valid loss:0.355937
Epoch:18, Train loss:0.336999, valid loss:0.355720
Epoch:19, Train loss:0.336948, valid loss:0.355669
Epoch:20, Train loss:0.336964, valid loss:0.355719
Epoch:21, Train loss:0.336586, valid loss:0.355579
Epoch:22, Train loss:0.336622, valid loss:0.355551
Epoch:23, Train loss:0.336600, valid loss:0.355585
Epoch:24, Train loss:0.336590, valid loss:0.355569
Epoch:25, Train loss:0.336592, valid loss:0.355620
Epoch:26, Train loss:0.336596, valid loss:0.355436
Epoch:27, Train loss:0.336548, valid loss:0.355817
Epoch:28, Train loss:0.336541, valid loss:0.355480
Epoch:29, Train loss:0.336544, valid loss:0.355541
Epoch:30, Train loss:0.336542, valid loss:0.355595
Epoch:31, Train loss:0.336369, valid loss:0.355419
Epoch:32, Train loss:0.336358, valid loss:0.355465
Epoch:33, Train loss:0.336369, valid loss:0.355450
Epoch:34, Train loss:0.336359, valid loss:0.355493
Epoch:35, Train loss:0.336366, valid loss:0.355444
Epoch:36, Train loss:0.336356, valid loss:0.355448
Epoch:37, Train loss:0.336364, valid loss:0.355431
Epoch:38, Train loss:0.336349, valid loss:0.355477
Epoch:39, Train loss:0.336353, valid loss:0.355429
Epoch:40, Train loss:0.336343, valid loss:0.355483
Epoch:41, Train loss:0.336260, valid loss:0.355414
Epoch:42, Train loss:0.336262, valid loss:0.355433
Epoch:43, Train loss:0.336261, valid loss:0.355389
Epoch:44, Train loss:0.336258, valid loss:0.355453
Epoch:45, Train loss:0.336256, valid loss:0.355425
Epoch:46, Train loss:0.336253, valid loss:0.355388
Epoch:47, Train loss:0.336253, valid loss:0.355464
Epoch:48, Train loss:0.336251, valid loss:0.355422
Epoch:49, Train loss:0.336253, valid loss:0.355415
Epoch:50, Train loss:0.336252, valid loss:0.355384
Epoch:51, Train loss:0.336212, valid loss:0.355392
Epoch:52, Train loss:0.336209, valid loss:0.355385
Epoch:53, Train loss:0.336207, valid loss:0.355383
Epoch:54, Train loss:0.336208, valid loss:0.355390
Epoch:55, Train loss:0.336206, valid loss:0.355378
Epoch:56, Train loss:0.336208, valid loss:0.355389
Epoch:57, Train loss:0.336205, valid loss:0.355374
Epoch:58, Train loss:0.336202, valid loss:0.355392
Epoch:59, Train loss:0.336205, valid loss:0.355392
Epoch:60, Train loss:0.336203, valid loss:0.355392
training time 5036.136194944382
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.6407580384474197
plot_id,batch_id 0 1 miss% 0.7307734549185864
plot_id,batch_id 0 2 miss% 0.745678821267758
plot_id,batch_id 0 3 miss% 0.7583636880006337
plot_id,batch_id 0 4 miss% 0.7622149956856805
plot_id,batch_id 0 5 miss% 0.6344904853722159
plot_id,batch_id 0 6 miss% 0.7256643747246462
plot_id,batch_id 0 7 miss% 0.7459074896508068
plot_id,batch_id 0 8 miss% 0.7559975337507268
plot_id,batch_id 0 9 miss% 0.7599369274039499
plot_id,batch_id 0 10 miss% 0.6073148421727106
plot_id,batch_id 0 11 miss% 0.725864308565683
plot_id,batch_id 0 12 miss% 0.7384248239903789
plot_id,batch_id 0 13 miss% 0.751504885540625
plot_id,batch_id 0 14 miss% 0.755702959472475
plot_id,batch_id 0 15 miss% 0.6284563628661577
plot_id,batch_id 0 16 miss% 0.7189400634543947
plot_id,batch_id 0 17 miss% 0.7467684299299548
plot_id,batch_id 0 18 miss% 0.7499024072690776
plot_id,batch_id 0 19 miss% 0.7513408093580514
plot_id,batch_id 0 20 miss% 0.6857753056056001
plot_id,batch_id 0 21 miss% 0.7520239785956699
plot_id,batch_id 0 22 miss% 0.7571571409642369
plot_id,batch_id 0 23 miss% 0.7645168520686393
plot_id,batch_id 0 24 miss% 0.7641440499824232
plot_id,batch_id 0 25 miss% 0.6795620792633749
plot_id,batch_id 0 26 miss% 0.7405603118227536
plot_id,batch_id 0 27 miss% 0.7537632029041114
plot_id,batch_id 0 28 miss% 0.7627138294369988
plot_id,batch_id 0 29 miss% 0.7637968925701074
plot_id,batch_id 0 30 miss% 0.6662711932105562
plot_id,batch_id 0 31 miss% 0.7429175350688914
plot_id,batch_id 0 32 miss% 0.7524537430306257
plot_id,batch_id 0 33 miss% 0.7617980771177134
plot_id,batch_id 0 34 miss% 0.7633547330620086
plot_id,batch_id 0 35 miss% 0.6692878650545359
plot_id,batch_id 0 36 miss% 0.7492625633412853
plot_id,batch_id 0 37 miss% 0.7488616184662121
plot_id,batch_id 0 38 miss% 0.7573259807911644
plot_id,batch_id 0 39 miss% 0.7602329596873063
plot_id,batch_id 0 40 miss% 0.713981760494638
plot_id,batch_id 0 41 miss% 0.7547282387173423
plot_id,batch_id 0 42 miss% 0.7617457222817883
plot_id,batch_id 0 43 miss% 0.7655839180674413
plot_id,batch_id 0 44 miss% 0.7728047255712969
plot_id,batch_id 0 45 miss% 0.7226180715536642
plot_id,batch_id 0 46 miss% 0.7533625394311781
plot_id,batch_id 0 47 miss% 0.762684149306311
plot_id,batch_id 0 48 miss% 0.7654888353814981
plot_id,batch_id 0 49 miss% 0.7690011917630798
plot_id,batch_id 0 50 miss% 0.7307943315241351
plot_id,batch_id 0 51 miss% 0.755267707730983
plot_id,batch_id 0 52 miss% 0.758522877490603
plot_id,batch_id 0 53 miss% 0.7633851090255398
plot_id,batch_id 0 54 miss% 0.7747538542354535
plot_id,batch_id 0 55 miss% 0.7294809565684357
plot_id,batch_id 0 56 miss% 0.7511809988363004
plot_id,batch_id 0 57 miss% 0.7605373985879571
plot_id,batch_id 0 58 miss% 0.7655004919693708
plot_id,batch_id 0 59 miss% 0.7678305835330463
plot_id,batch_id 0 60 miss% 0.5385814968567152
plot_id,batch_id 0 61 miss% 0.6894338466095433
plot_id,batch_id 0 62 miss% 0.7237850113634101
plot_id,batch_id 0 63 miss% 0.7403059840161446
plot_id,batch_id 0 64 miss% 0.7461630167334308
plot_id,batch_id 0 65 miss% 0.5288988514030563
plot_id,batch_id 0 66 miss% 0.6844634051125363
plot_id,batch_id 0 67 miss% 0.7056008911768221
plot_id,batch_id 0 68 miss% 0.7361202836954311
plot_id,batch_id 0 69 miss% 0.7457586042229182
plot_id,batch_id 0 70 miss% 0.5009477911143471
plot_id,batch_id 0 71 miss% 0.686394293421976
plot_id,batch_id 0 72 miss% 0.6931680711529297
plot_id,batch_id 0 73 miss% 0.720020877896183
plot_id,batch_id 0 74 miss% 0.7323656420844803
plot_id,batch_id 0 75 miss% 0.49478688594723774
plot_id,batch_id 0 76 miss% 0.6463331393611866
plot_id,batch_id 0 77 miss% 0.687921439225834
plot_id,batch_id 0 78 miss% 0.7151171354874442
plot_id,batch_id 0 79 miss% 0.7321409431192607
plot_id,batch_id 0 80 miss% 0.5637755134898966
plot_id,batch_id 0 81 miss% 0.7094845087200428
plot_id,batch_id 0 82 miss% 0.7331667865579385
plot_id,batch_id 0 83 miss% 0.7454474248593439
plot_id,batch_id 0 84 miss% 0.7497556706276512
plot_id,batch_id 0 85 miss% 0.5616098049418733
plot_id,batch_id 0 86 miss% 0.7033281150671294
plot_id,batch_id 0 87 miss% 0.7262268640190264
plot_id,batch_id 0 88 miss% 0.7431949155429436
plot_id,batch_id 0 89 miss% 0.7499108099425809
plot_id,batch_id 0 90 miss% 0.5309374251106855
plot_id,batch_id 0 91 miss% 0.6982876926316856
plot_id,batch_id 0 92 miss% 0.71553533324095
plot_id,batch_id 0 93 miss% 0.7424851796648836
plot_id,batch_id 0 94 miss% 0.7464793749954663
plot_id,batch_id 0 95 miss% 0.5418309663934325
plot_id,batch_id 0 96 miss% 0.6744039419158296
plot_id,batch_id 0 97 miss% 0.7162534789079986
plot_id,batch_id 0 98 miss% 0.7248924550404853
plot_id,batch_id 0 99 miss% 0.7400825723395587
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.64075804 0.73077345 0.74567882 0.75836369 0.762215   0.63449049
 0.72566437 0.74590749 0.75599753 0.75993693 0.60731484 0.72586431
 0.73842482 0.75150489 0.75570296 0.62845636 0.71894006 0.74676843
 0.74990241 0.75134081 0.68577531 0.75202398 0.75715714 0.76451685
 0.76414405 0.67956208 0.74056031 0.7537632  0.76271383 0.76379689
 0.66627119 0.74291754 0.75245374 0.76179808 0.76335473 0.66928787
 0.74926256 0.74886162 0.75732598 0.76023296 0.71398176 0.75472824
 0.76174572 0.76558392 0.77280473 0.72261807 0.75336254 0.76268415
 0.76548884 0.76900119 0.73079433 0.75526771 0.75852288 0.76338511
 0.77475385 0.72948096 0.751181   0.7605374  0.76550049 0.76783058
 0.5385815  0.68943385 0.72378501 0.74030598 0.74616302 0.52889885
 0.68446341 0.70560089 0.73612028 0.7457586  0.50094779 0.68639429
 0.69316807 0.72002088 0.73236564 0.49478689 0.64633314 0.68792144
 0.71511714 0.73214094 0.56377551 0.70948451 0.73316679 0.74544742
 0.74975567 0.5616098  0.70332812 0.72622686 0.74319492 0.74991081
 0.53093743 0.69828769 0.71553533 0.74248518 0.74647937 0.54183097
 0.67440394 0.71625348 0.72489246 0.74008257]
for model  111 the mean error 0.7159443551996649
all id 111 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 25 out win 3 err 0.7159443551996649
Launcher: Job 112 completed in 5215 seconds.
Launcher: Task 123 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  28945
Epoch:0, Train loss:0.484413, valid loss:0.483743
Epoch:1, Train loss:0.033697, valid loss:0.006073
Epoch:2, Train loss:0.008539, valid loss:0.003625
Epoch:3, Train loss:0.006314, valid loss:0.003228
Epoch:4, Train loss:0.005354, valid loss:0.002473
Epoch:5, Train loss:0.004494, valid loss:0.002440
Epoch:6, Train loss:0.004146, valid loss:0.001755
Epoch:7, Train loss:0.003623, valid loss:0.001982
Epoch:8, Train loss:0.003570, valid loss:0.003221
Epoch:9, Train loss:0.003283, valid loss:0.002131
Epoch:10, Train loss:0.003323, valid loss:0.001725
Epoch:11, Train loss:0.002345, valid loss:0.001209
Epoch:12, Train loss:0.002297, valid loss:0.001420
Epoch:13, Train loss:0.002323, valid loss:0.001275
Epoch:14, Train loss:0.002280, valid loss:0.001225
Epoch:15, Train loss:0.002161, valid loss:0.001540
Epoch:16, Train loss:0.002163, valid loss:0.001080
Epoch:17, Train loss:0.002151, valid loss:0.001152
Epoch:18, Train loss:0.002095, valid loss:0.001299
Epoch:19, Train loss:0.002107, valid loss:0.001119
Epoch:20, Train loss:0.002053, valid loss:0.001063
Epoch:21, Train loss:0.001596, valid loss:0.000951
Epoch:22, Train loss:0.001612, valid loss:0.001034
Epoch:23, Train loss:0.001618, valid loss:0.000931
Epoch:24, Train loss:0.001590, valid loss:0.001004
Epoch:25, Train loss:0.001594, valid loss:0.001021
Epoch:26, Train loss:0.001586, valid loss:0.000896
Epoch:27, Train loss:0.001533, valid loss:0.000959
Epoch:28, Train loss:0.001564, valid loss:0.001011
Epoch:29, Train loss:0.001613, valid loss:0.000974
Epoch:30, Train loss:0.001508, valid loss:0.001120
Epoch:31, Train loss:0.001320, valid loss:0.000776
Epoch:32, Train loss:0.001282, valid loss:0.000795
Epoch:33, Train loss:0.001299, valid loss:0.000803
Epoch:34, Train loss:0.001283, valid loss:0.000807
Epoch:35, Train loss:0.001291, valid loss:0.000794
Epoch:36, Train loss:0.001274, valid loss:0.000884
Epoch:37, Train loss:0.001289, valid loss:0.000760
Epoch:38, Train loss:0.001274, valid loss:0.000813
Epoch:39, Train loss:0.001286, valid loss:0.000785
Epoch:40, Train loss:0.001257, valid loss:0.000822
Epoch:41, Train loss:0.001147, valid loss:0.000795
Epoch:42, Train loss:0.001155, valid loss:0.000713
Epoch:43, Train loss:0.001154, valid loss:0.000699
Epoch:44, Train loss:0.001151, valid loss:0.000721
Epoch:45, Train loss:0.001137, valid loss:0.000739
Epoch:46, Train loss:0.001139, valid loss:0.000722
Epoch:47, Train loss:0.001126, valid loss:0.000724
Epoch:48, Train loss:0.001136, valid loss:0.000697
Epoch:49, Train loss:0.001122, valid loss:0.000709
Epoch:50, Train loss:0.001125, valid loss:0.000728
Epoch:51, Train loss:0.001077, valid loss:0.000678
Epoch:52, Train loss:0.001073, valid loss:0.000707
Epoch:53, Train loss:0.001071, valid loss:0.000714
Epoch:54, Train loss:0.001071, valid loss:0.000667
Epoch:55, Train loss:0.001072, valid loss:0.000710
Epoch:56, Train loss:0.001065, valid loss:0.000669
Epoch:57, Train loss:0.001065, valid loss:0.000661
Epoch:58, Train loss:0.001063, valid loss:0.000706
Epoch:59, Train loss:0.001063, valid loss:0.000702
Epoch:60, Train loss:0.001059, valid loss:0.000694
training time 5035.7830691337585
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.031685418981110026
plot_id,batch_id 0 1 miss% 0.021772845488911865
plot_id,batch_id 0 2 miss% 0.0237955248040878
plot_id,batch_id 0 3 miss% 0.029678027150764067
plot_id,batch_id 0 4 miss% 0.027851746770202353
plot_id,batch_id 0 5 miss% 0.03782667969450075
plot_id,batch_id 0 6 miss% 0.02659107686263864
plot_id,batch_id 0 7 miss% 0.0253134030928161
plot_id,batch_id 0 8 miss% 0.026388078958131268
plot_id,batch_id 0 9 miss% 0.03613832656343463
plot_id,batch_id 0 10 miss% 0.056836991454225404
plot_id,batch_id 0 11 miss% 0.04599209619163737
plot_id,batch_id 0 12 miss% 0.02470837006078867
plot_id,batch_id 0 13 miss% 0.02160208788065794
plot_id,batch_id 0 14 miss% 0.03328497696867204
plot_id,batch_id 0 15 miss% 0.043648843064375435
plot_id,batch_id 0 16 miss% 0.03527881079015484
plot_id,batch_id 0 17 miss% 0.043742173313704565
plot_id,batch_id 0 18 miss% 0.027904895606210695
plot_id,batch_id 0 19 miss% 0.029406899966581888
plot_id,batch_id 0 20 miss% 0.03279369884102045
plot_id,batch_id 0 21 miss% 0.02030543552795399
plot_id,batch_id 0 22 miss% 0.020867334504048436
plot_id,batch_id 0 23 miss% 0.024466986862636673
plot_id,batch_id 0 24 miss% 0.029001262464611908
plot_id,batch_id 0 25 miss% 0.04511317872107747
plot_id,batch_id 0 26 miss% 0.0390806934538585
plot_id,batch_id 0 27 miss% 0.02660174808855276
plot_id,batch_id 0 28 miss% 0.02546412903857432
plot_id,batch_id 0 29 miss% 0.028204581165103403
plot_id,batch_id 0 30 miss% 0.03909636702113732
plot_id,batch_id 0 31 miss% 0.033258122568158466
plot_id,batch_id 0 32 miss% 0.025393303970961834
plot_id,batch_id 0 33 miss% 0.027025436459694414
plot_id,batch_id 0 34 miss% 0.021782215341344065
plot_id,batch_id 0 35 miss% 0.04394877642079894
plot_id,batch_id 0 36 miss% 0.04241180263880369
plot_id,batch_id 0 37 miss% 0.03802854288900166
plot_id,batch_id 0 38 miss% 0.03931856973873731
plot_id,batch_id 0 39 miss% 0.03139767659803205
plot_id,batch_id 0 40 miss% 0.07124313822709961
plot_id,batch_id 0 41 miss% 0.02688995219192779
plot_id,batch_id 0 42 miss% 0.02509380328692731
plot_id,batch_id 0 43 miss% 0.026749161718403736
plot_id,batch_id 0 44 miss% 0.024117511662993557
plot_id,batch_id 0 45 miss% 0.034218216476847696
plot_id,batch_id 0 46 miss% 0.024705464670467685
plot_id,batch_id 0 47 miss% 0.025677380989518564
plot_id,batch_id 0 48 miss% 0.021899232897586177
plot_id,batch_id 0 49 miss% 0.020991744767885
plot_id,batch_id 0 50 miss% 0.034014420919279614
plot_id,batch_id 0 51 miss% 0.021452986415852478
plot_id,batch_id 0 52 miss% 0.021716049986335285
plot_id,batch_id 0 53 miss% 0.023661928263783646
plot_id,batch_id 0 54 miss% 0.02445038541944469
plot_id,batch_id 0 55 miss% 0.033775817466891615
plot_id,batch_id 0 56 miss% 0.02595608366503668
plot_id,batch_id 0 57 miss% 0.025578720085193667
plot_id,batch_id 0 58 miss% 0.025351988771579168
plot_id,batch_id 0 59 miss% 0.03043242945797503
plot_id,batch_id 0 60 miss% 0.032923234099407644
plot_id,batch_id 0 61 miss% 0.02923340954550134
plot_id,batch_id 0 62 miss% 0.024256926842846832
plot_id,batch_id 0 63 miss% 0.030324664111390138
plot_id,batch_id 0 64 miss% 0.03126329253938921
plot_id,batch_id 0 65 miss% 0.030176054100463656
plot_id,batch_id 0 66 miss% 0.05048608075886306
plot_id,batch_id 0 67 miss% 0.028172691357656852
plot_id,batch_id 0 68 miss% 0.02849657935243674
plot_id,batch_id 0 69 miss% 0.02260076934483758
plot_id,batch_id 0 70 miss% 0.041879246114084445
plot_id,batch_id 0 71 miss% 0.07215751622089087
plot_id,batch_id 0 72 miss% 0.04059593550666855
plot_id,batch_id 0 73 miss% 0.03363883396977982
plot_id,batch_id 0 74 miss% 0.04041357771047716
plot_id,batch_id 0 75 miss% 0.0475997291847811
plot_id,batch_id 0 76 miss% 0.05902556900596743
plot_id,batch_id 0 77 miss% 0.026824055348025067
plot_id,batch_id 0 78 miss% 0.039559225835197574
plot_id,batch_id 0 79 miss% 0.05198363382914415
plot_id,batch_id 0 80 miss% 0.036307256443720036
plot_id,batch_id 0 81 miss% 0.01797839709016889
plot_id,batch_id 0 82 miss% 0.026423479006299024
plot_id,batch_id 0 83 miss% 0.0218304379692146
plot_id,batch_id 0 84 miss% 0.032470160140466726
plot_id,batch_id 0 85 miss% 0.03573280912372757
plot_id,batch_id 0 86 miss% 0.020782737923535716
plot_id,batch_id 0 87 miss% 0.02640092584409769
plot_id,batch_id 0 88 miss% 0.027052463498343192
plot_id,batch_id 0 89 miss% 0.027213327378401878
plot_id,batch_id 0 90 miss% 0.047248389823773516
plot_id,batch_id 0 91 miss% 0.03706404591708515
plot_id,batch_id 0 92 miss% 0.028530940804412565
plot_id,batch_id 0 93 miss% 0.027239564517210938
plot_id,batch_id 0 94 miss% 0.025359992197463436
plot_id,batch_id 0 95 miss% 0.05557343076687928
plot_id,batch_id 0 96 miss% 0.031691093089887136
plot_id,batch_id 0 97 miss% 0.058163374432141014
plot_id,batch_id 0 98 miss% 0.037047837227148946
plot_id,batch_id 0 99 miss% 0.03189399528412954
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03168542 0.02177285 0.02379552 0.02967803 0.02785175 0.03782668
 0.02659108 0.0253134  0.02638808 0.03613833 0.05683699 0.0459921
 0.02470837 0.02160209 0.03328498 0.04364884 0.03527881 0.04374217
 0.0279049  0.0294069  0.0327937  0.02030544 0.02086733 0.02446699
 0.02900126 0.04511318 0.03908069 0.02660175 0.02546413 0.02820458
 0.03909637 0.03325812 0.0253933  0.02702544 0.02178222 0.04394878
 0.0424118  0.03802854 0.03931857 0.03139768 0.07124314 0.02688995
 0.0250938  0.02674916 0.02411751 0.03421822 0.02470546 0.02567738
 0.02189923 0.02099174 0.03401442 0.02145299 0.02171605 0.02366193
 0.02445039 0.03377582 0.02595608 0.02557872 0.02535199 0.03043243
 0.03292323 0.02923341 0.02425693 0.03032466 0.03126329 0.03017605
 0.05048608 0.02817269 0.02849658 0.02260077 0.04187925 0.07215752
 0.04059594 0.03363883 0.04041358 0.04759973 0.05902557 0.02682406
 0.03955923 0.05198363 0.03630726 0.0179784  0.02642348 0.02183044
 0.03247016 0.03573281 0.02078274 0.02640093 0.02705246 0.02721333
 0.04724839 0.03706405 0.02853094 0.02723956 0.02535999 0.05557343
 0.03169109 0.05816337 0.03704784 0.031894  ]
for model  117 the mean error 0.03264599236574659
all id 117 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 25 out win 3 err 0.03264599236574659
Launcher: Job 118 completed in 5231 seconds.
Launcher: Task 21 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  35921
Epoch:0, Train loss:0.831381, valid loss:0.823086
Epoch:1, Train loss:0.061752, valid loss:0.017290
Epoch:2, Train loss:0.017194, valid loss:0.006644
Epoch:3, Train loss:0.011751, valid loss:0.005355
Epoch:4, Train loss:0.009976, valid loss:0.004055
Epoch:5, Train loss:0.008766, valid loss:0.004089
Epoch:6, Train loss:0.007635, valid loss:0.004317
Epoch:7, Train loss:0.007099, valid loss:0.004628
Epoch:8, Train loss:0.006647, valid loss:0.003084
Epoch:9, Train loss:0.005958, valid loss:0.003342
Epoch:10, Train loss:0.005829, valid loss:0.002602
Epoch:11, Train loss:0.003978, valid loss:0.002093
Epoch:12, Train loss:0.004058, valid loss:0.002456
Epoch:13, Train loss:0.003822, valid loss:0.002400
Epoch:14, Train loss:0.003924, valid loss:0.001860
Epoch:15, Train loss:0.003827, valid loss:0.002182
Epoch:16, Train loss:0.003713, valid loss:0.002201
Epoch:17, Train loss:0.003603, valid loss:0.002223
Epoch:18, Train loss:0.003578, valid loss:0.001805
Epoch:19, Train loss:0.003521, valid loss:0.002051
Epoch:20, Train loss:0.003502, valid loss:0.002017
Epoch:21, Train loss:0.002565, valid loss:0.001796
Epoch:22, Train loss:0.002594, valid loss:0.001465
Epoch:23, Train loss:0.002543, valid loss:0.001817
Epoch:24, Train loss:0.002550, valid loss:0.001549
Epoch:25, Train loss:0.002466, valid loss:0.001526
Epoch:26, Train loss:0.002480, valid loss:0.001648
Epoch:27, Train loss:0.002405, valid loss:0.001985
Epoch:28, Train loss:0.002509, valid loss:0.002193
Epoch:29, Train loss:0.002431, valid loss:0.001426
Epoch:30, Train loss:0.002416, valid loss:0.001598
Epoch:31, Train loss:0.001972, valid loss:0.001278
Epoch:32, Train loss:0.001961, valid loss:0.001358
Epoch:33, Train loss:0.001928, valid loss:0.001271
Epoch:34, Train loss:0.001950, valid loss:0.001282
Epoch:35, Train loss:0.001919, valid loss:0.001268
Epoch:36, Train loss:0.001927, valid loss:0.001321
Epoch:37, Train loss:0.001944, valid loss:0.001291
Epoch:38, Train loss:0.001872, valid loss:0.001244
Epoch:39, Train loss:0.001862, valid loss:0.001287
Epoch:40, Train loss:0.001894, valid loss:0.001287
Epoch:41, Train loss:0.001680, valid loss:0.001187
Epoch:42, Train loss:0.001650, valid loss:0.001149
Epoch:43, Train loss:0.001656, valid loss:0.001161
Epoch:44, Train loss:0.001640, valid loss:0.001160
Epoch:45, Train loss:0.001644, valid loss:0.001265
Epoch:46, Train loss:0.001650, valid loss:0.001173
Epoch:47, Train loss:0.001626, valid loss:0.001105
Epoch:48, Train loss:0.001654, valid loss:0.001262
Epoch:49, Train loss:0.001611, valid loss:0.001148
Epoch:50, Train loss:0.001637, valid loss:0.001087
Epoch:51, Train loss:0.001519, valid loss:0.001158
Epoch:52, Train loss:0.001509, valid loss:0.001064
Epoch:53, Train loss:0.001531, valid loss:0.001108
Epoch:54, Train loss:0.001511, valid loss:0.001083
Epoch:55, Train loss:0.001500, valid loss:0.001048
Epoch:56, Train loss:0.001507, valid loss:0.001088
Epoch:57, Train loss:0.001502, valid loss:0.001110
Epoch:58, Train loss:0.001514, valid loss:0.001104
Epoch:59, Train loss:0.001501, valid loss:0.001095
Epoch:60, Train loss:0.001495, valid loss:0.001062
training time 5040.237218379974
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.028176204607453976
plot_id,batch_id 0 1 miss% 0.03212710792580542
plot_id,batch_id 0 2 miss% 0.024965481562446195
plot_id,batch_id 0 3 miss% 0.021874183649915616
plot_id,batch_id 0 4 miss% 0.03190723278854632
plot_id,batch_id 0 5 miss% 0.041755128733690784
plot_id,batch_id 0 6 miss% 0.041365101599862425
plot_id,batch_id 0 7 miss% 0.033284679602093074
plot_id,batch_id 0 8 miss% 0.03325918625973562
plot_id,batch_id 0 9 miss% 0.033545574940404006
plot_id,batch_id 0 10 miss% 0.05967076845822947
plot_id,batch_id 0 11 miss% 0.050237488476486486
plot_id,batch_id 0 12 miss% 0.03553187072700005
plot_id,batch_id 0 13 miss% 0.022375696680149854
plot_id,batch_id 0 14 miss% 0.033658757767192266
plot_id,batch_id 0 15 miss% 0.043538567674911766
plot_id,batch_id 0 16 miss% 0.028844628502292455
plot_id,batch_id 0 17 miss% 0.04559838878324809
plot_id,batch_id 0 18 miss% 0.03247844765651795
plot_id,batch_id 0 19 miss% 0.023190795112427318
plot_id,batch_id 0 20 miss% 0.034561723148407844
plot_id,batch_id 0 21 miss% 0.03502593700847144
plot_id,batch_id 0 22 miss% 0.03463139228537337
plot_id,batch_id 0 23 miss% 0.020672959459970535
plot_id,batch_id 0 24 miss% 0.029307058582577497
plot_id,batch_id 0 25 miss% 0.06286948569636801
plot_id,batch_id 0 26 miss% 0.03418842204479982
plot_id,batch_id 0 27 miss% 0.023278887833851042
plot_id,batch_id 0 28 miss% 0.021208687140450305
plot_id,batch_id 0 29 miss% 0.017494228170971654
plot_id,batch_id 0 30 miss% 0.041647196738849965
plot_id,batch_id 0 31 miss% 0.03046758397441806
plot_id,batch_id 0 32 miss% 0.02893244951195366
plot_id,batch_id 0 33 miss% 0.0313419160936683
plot_id,batch_id 0 34 miss% 0.026841260051202413
plot_id,batch_id 0 35 miss% 0.04959630503612555
plot_id,batch_id 0 36 miss% 0.03268097648170829
plot_id,batch_id 0 37 miss% 0.03670794885262913
plot_id,batch_id 0 38 miss% 0.026714248724805628
plot_id,batch_id 0 39 miss% 0.01954652720702864
plot_id,batch_id 0 40 miss% 0.09779047497470358
plot_id,batch_id 0 41 miss% 0.022609523701872428
plot_id,batch_id 0 42 miss% 0.012868207100676578
plot_id,batch_id 0 43 miss% 0.03309353864655479
plot_id,batch_id 0 44 miss% 0.026162973587608992
plot_id,batch_id 0 45 miss% 0.028645569677751313
plot_id,batch_id 0 46 miss% 0.028929545346786566
plot_id,batch_id 0 47 miss% 0.022411234758042787
plot_id,batch_id 0 48 miss% 0.020617865386745326
plot_id,batch_id 0 49 miss% 0.02117314464467851
plot_id,batch_id 0 50 miss% 0.048886135329885755
plot_id,batch_id 0 51 miss% 0.02433591054857055
plot_id,batch_id 0 52 miss% 0.022885653424408063
plot_id,batch_id 0 53 miss% 0.014812399225851244
plot_id,batch_id 0 54 miss% 0.02211181092149314
plot_id,batch_id 0 55 miss% 0.04757931029093421
plot_id,batch_id 0 56 miss% 0.04231949166611131
plot_id,batch_id 0 57 miss% 0.026903178266806972
plot_id,batch_id 0 58 miss% 0.022762504823558835
plot_id,batch_id 0 59 miss% 0.019747300665459464
plot_id,batch_id 0 60 miss% 0.07257232177999717
plot_id,batch_id 0 61 miss% 0.025357514573470456
plot_id,batch_id 0 62 miss% 0.016716948281990735
plot_id,batch_id 0 63 miss% 0.020887522627914146
plot_id,batch_id 0 64 miss% 0.040323223578685584
plot_id,batch_id 0 65 miss% 0.0662077265599602
plot_id,batch_id 0 66 miss% 0.03621396413697624
plot_id,batch_id 0 67 miss% 0.029816834297438845
plot_id,batch_id 0 68 miss% 0.030390948479568487
plot_id,batch_id 0 69 miss% 0.023148867453972793
plot_id,batch_id 0 70 miss% 0.06581398134791668
plot_id,batch_id 0 71 miss% 0.05519684509522003
plot_id,batch_id 0 72 miss% 0.041214365528777865
plot_id,batch_id 0 73 miss% 0.033268898569899195
plot_id,batch_id 0 74 miss% 0.03671581895550468
plot_id,batch_id 0 75 miss% 0.054626743757296536
plot_id,batch_id 0 76 miss% 0.05654163548886463
plot_id,batch_id 0 77 miss% 0.03119158022184487
plot_id,batch_id 0 78 miss% 0.03958148418143267
plot_id,batch_id 0 79 miss% 0.05896439136467331
plot_id,batch_id 0 80 miss% 0.05604501490459935
plot_id,batch_id 0 81 miss% 0.024213918757882924
plot_id,batch_id 0 82 miss% 0.028100046289562065
plot_id,batch_id 0 83 miss% 0.03408600171573013
plot_id,batch_id 0 84 miss% 0.027047983970869778
plot_id,batch_id 0 85 miss% 0.051646808688949566
plot_id,batch_id 0 86 miss% 0.03178085643093163
plot_id,batch_id 0 87 miss% 0.032217879127317384
plot_id,batch_id 0 88 miss% 0.033437646095670515
plot_id,batch_id 0 89 miss% 0.031143199440925105
plot_id,batch_id 0 90 miss% 0.0336708937965639
plot_id,batch_id 0 91 miss% 0.04219651088005581
plot_id,batch_id 0 92 miss% 0.024190859507708225
plot_id,batch_id 0 93 miss% 0.03015664226107968
plot_id,batch_id 0 94 miss% 0.031921572035339886
plot_id,batch_id 0 95 miss% 0.04375696164100423
plot_id,batch_id 0 96 miss% 0.02536065897739092
plot_id,batch_id 0 97 miss% 0.04370351671536521
plot_id,batch_id 0 98 miss% 0.02511941750726244
plot_id,batch_id 0 99 miss% 0.03398908268089064
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0281762  0.03212711 0.02496548 0.02187418 0.03190723 0.04175513
 0.0413651  0.03328468 0.03325919 0.03354557 0.05967077 0.05023749
 0.03553187 0.0223757  0.03365876 0.04353857 0.02884463 0.04559839
 0.03247845 0.0231908  0.03456172 0.03502594 0.03463139 0.02067296
 0.02930706 0.06286949 0.03418842 0.02327889 0.02120869 0.01749423
 0.0416472  0.03046758 0.02893245 0.03134192 0.02684126 0.04959631
 0.03268098 0.03670795 0.02671425 0.01954653 0.09779047 0.02260952
 0.01286821 0.03309354 0.02616297 0.02864557 0.02892955 0.02241123
 0.02061787 0.02117314 0.04888614 0.02433591 0.02288565 0.0148124
 0.02211181 0.04757931 0.04231949 0.02690318 0.0227625  0.0197473
 0.07257232 0.02535751 0.01671695 0.02088752 0.04032322 0.06620773
 0.03621396 0.02981683 0.03039095 0.02314887 0.06581398 0.05519685
 0.04121437 0.0332689  0.03671582 0.05462674 0.05654164 0.03119158
 0.03958148 0.05896439 0.05604501 0.02421392 0.02810005 0.034086
 0.02704798 0.05164681 0.03178086 0.03221788 0.03343765 0.0311432
 0.03367089 0.04219651 0.02419086 0.03015664 0.03192157 0.04375696
 0.02536066 0.04370352 0.02511942 0.03398908]
for model  72 the mean error 0.03460283346247046
all id 72 hidden_dim 16 learning_rate 0.02 num_layers 5 frames 21 out win 3 err 0.03460283346247046
Launcher: Job 73 completed in 5243 seconds.
Launcher: Task 132 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  28945
Epoch:0, Train loss:0.631497, valid loss:0.627464
Epoch:1, Train loss:0.067104, valid loss:0.019681
Epoch:2, Train loss:0.030133, valid loss:0.008608
Epoch:3, Train loss:0.014311, valid loss:0.007046
Epoch:4, Train loss:0.011126, valid loss:0.004866
Epoch:5, Train loss:0.009367, valid loss:0.004845
Epoch:6, Train loss:0.008206, valid loss:0.004496
Epoch:7, Train loss:0.007124, valid loss:0.003921
Epoch:8, Train loss:0.006355, valid loss:0.003913
Epoch:9, Train loss:0.005869, valid loss:0.003104
Epoch:10, Train loss:0.005400, valid loss:0.003470
Epoch:11, Train loss:0.004280, valid loss:0.002268
Epoch:12, Train loss:0.004161, valid loss:0.002177
Epoch:13, Train loss:0.004000, valid loss:0.002289
Epoch:14, Train loss:0.004046, valid loss:0.002351
Epoch:15, Train loss:0.003787, valid loss:0.002086
Epoch:16, Train loss:0.003673, valid loss:0.002265
Epoch:17, Train loss:0.003635, valid loss:0.001723
Epoch:18, Train loss:0.003506, valid loss:0.001939
Epoch:19, Train loss:0.003487, valid loss:0.002026
Epoch:20, Train loss:0.003529, valid loss:0.001828
Epoch:21, Train loss:0.002896, valid loss:0.001837
Epoch:22, Train loss:0.002867, valid loss:0.001737
Epoch:23, Train loss:0.002813, valid loss:0.001676
Epoch:24, Train loss:0.002831, valid loss:0.001695
Epoch:25, Train loss:0.002782, valid loss:0.001628
Epoch:26, Train loss:0.002758, valid loss:0.001506
Epoch:27, Train loss:0.002765, valid loss:0.001624
Epoch:28, Train loss:0.002722, valid loss:0.001831
Epoch:29, Train loss:0.002694, valid loss:0.001637
Epoch:30, Train loss:0.002680, valid loss:0.001666
Epoch:31, Train loss:0.002387, valid loss:0.001481
Epoch:32, Train loss:0.002389, valid loss:0.001471
Epoch:33, Train loss:0.002388, valid loss:0.001408
Epoch:34, Train loss:0.002366, valid loss:0.001437
Epoch:35, Train loss:0.002389, valid loss:0.001438
Epoch:36, Train loss:0.002340, valid loss:0.001515
Epoch:37, Train loss:0.002330, valid loss:0.001452
Epoch:38, Train loss:0.002324, valid loss:0.001454
Epoch:39, Train loss:0.002298, valid loss:0.001440
Epoch:40, Train loss:0.002297, valid loss:0.001395
Epoch:41, Train loss:0.002169, valid loss:0.001335
Epoch:42, Train loss:0.002164, valid loss:0.001362
Epoch:43, Train loss:0.002145, valid loss:0.001317
Epoch:44, Train loss:0.002144, valid loss:0.001336
Epoch:45, Train loss:0.002149, valid loss:0.001354
Epoch:46, Train loss:0.002144, valid loss:0.001341
Epoch:47, Train loss:0.002132, valid loss:0.001305
Epoch:48, Train loss:0.002120, valid loss:0.001304
Epoch:49, Train loss:0.002115, valid loss:0.001333
Epoch:50, Train loss:0.002115, valid loss:0.001299
Epoch:51, Train loss:0.002046, valid loss:0.001297
Epoch:52, Train loss:0.002043, valid loss:0.001283
Epoch:53, Train loss:0.002038, valid loss:0.001290
Epoch:54, Train loss:0.002034, valid loss:0.001270
Epoch:55, Train loss:0.002039, valid loss:0.001267
Epoch:56, Train loss:0.002031, valid loss:0.001310
Epoch:57, Train loss:0.002033, valid loss:0.001313
Epoch:58, Train loss:0.002023, valid loss:0.001300
Epoch:59, Train loss:0.002020, valid loss:0.001304
Epoch:60, Train loss:0.002024, valid loss:0.001274
training time 5075.610228538513
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03200275232337927
plot_id,batch_id 0 1 miss% 0.032779891086806866
plot_id,batch_id 0 2 miss% 0.03362123870314913
plot_id,batch_id 0 3 miss% 0.024178918643233776
plot_id,batch_id 0 4 miss% 0.018147117133593082
plot_id,batch_id 0 5 miss% 0.05320003026882904
plot_id,batch_id 0 6 miss% 0.035156386919882446
plot_id,batch_id 0 7 miss% 0.034307162322704636
plot_id,batch_id 0 8 miss% 0.040417520421720275
plot_id,batch_id 0 9 miss% 0.02141099595625003
plot_id,batch_id 0 10 miss% 0.03172545002021972
plot_id,batch_id 0 11 miss% 0.046915503389959604
plot_id,batch_id 0 12 miss% 0.032730612894575864
plot_id,batch_id 0 13 miss% 0.03442951450405547
plot_id,batch_id 0 14 miss% 0.05322247608316996
plot_id,batch_id 0 15 miss% 0.05819019586382279
plot_id,batch_id 0 16 miss% 0.03342219076544005
plot_id,batch_id 0 17 miss% 0.057876052186225733
plot_id,batch_id 0 18 miss% 0.04855391193439186
plot_id,batch_id 0 19 miss% 0.054096728696733425
plot_id,batch_id 0 20 miss% 0.08283453558234574
plot_id,batch_id 0 21 miss% 0.024195934907890747
plot_id,batch_id 0 22 miss% 0.024728534053029366
plot_id,batch_id 0 23 miss% 0.020273252763559704
plot_id,batch_id 0 24 miss% 0.023494533968716293
plot_id,batch_id 0 25 miss% 0.039648342740599744
plot_id,batch_id 0 26 miss% 0.028056406393500453
plot_id,batch_id 0 27 miss% 0.04501999812211038
plot_id,batch_id 0 28 miss% 0.022211324720384295
plot_id,batch_id 0 29 miss% 0.02665669518309186
plot_id,batch_id 0 30 miss% 0.03329702340166883
plot_id,batch_id 0 31 miss% 0.055032899400211985
plot_id,batch_id 0 32 miss% 0.05090066421611766
plot_id,batch_id 0 33 miss% 0.03693020972293392
plot_id,batch_id 0 34 miss% 0.04243772664329101
plot_id,batch_id 0 35 miss% 0.05479834017054731
plot_id,batch_id 0 36 miss% 0.07089818276551313
plot_id,batch_id 0 37 miss% 0.03249752935638387
plot_id,batch_id 0 38 miss% 0.035415533223464794
plot_id,batch_id 0 39 miss% 0.02557637721440954
plot_id,batch_id 0 40 miss% 0.07859664950024878
plot_id,batch_id 0 41 miss% 0.02576576863522663
plot_id,batch_id 0 42 miss% 0.014774756164143319
plot_id,batch_id 0 43 miss% 0.025046478570954364
plot_id,batch_id 0 44 miss% 0.016382592699293177
plot_id,batch_id 0 45 miss% 0.018689199956135702
plot_id,batch_id 0 46 miss% 0.03080747728467674
plot_id,batch_id 0 47 miss% 0.02917216073400514
plot_id,batch_id 0 48 miss% 0.02470568630092674
plot_id,batch_id 0 49 miss% 0.020793365483530966
plot_id,batch_id 0 50 miss% 0.03210733780796551
plot_id,batch_id 0 51 miss% 0.034765705209407166
plot_id,batch_id 0 52 miss% 0.03390797375896502
plot_id,batch_id 0 53 miss% 0.015021181394848291
plot_id,batch_id 0 54 miss% 0.0339499577922105
plot_id,batch_id 0 55 miss% 0.056768497156153036
plot_id,batch_id 0 56 miss% 0.03375131947171021
plot_id,batch_id 0 57 miss% 0.026194820660357394
plot_id,batch_id 0 58 miss% 0.03174924649366309
plot_id,batch_id 0 59 miss% 0.025670112607478573
plot_id,batch_id 0 60 miss% 0.05511718961452266
plot_id,batch_id 0 61 miss% 0.03931024527313317
plot_id,batch_id 0 62 miss% 0.031945531595487305
plot_id,batch_id 0 63 miss% 0.03256103794710771
plot_id,batch_id 0 64 miss% 0.02919365925706458
plot_id,batch_id 0 65 miss% 0.04479177902537866
plot_id,batch_id 0 66 miss% 0.054822518629982345
plot_id,batch_id 0 67 miss% 0.0391597326773269
plot_id,batch_id 0 68 miss% 0.045160782293056964
plot_id,batch_id 0 69 miss% 0.03498991589783073
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  28945
Epoch:0, Train loss:0.631497, valid loss:0.627464
Epoch:1, Train loss:0.054595, valid loss:0.010005
Epoch:2, Train loss:0.015528, valid loss:0.006271
Epoch:3, Train loss:0.012006, valid loss:0.004977
Epoch:4, Train loss:0.010291, valid loss:0.005272
Epoch:5, Train loss:0.008916, valid loss:0.006758
Epoch:6, Train loss:0.007940, valid loss:0.003518
Epoch:7, Train loss:0.007145, valid loss:0.003710
Epoch:8, Train loss:0.006597, valid loss:0.004311
Epoch:9, Train loss:0.006377, valid loss:0.003250
Epoch:10, Train loss:0.006023, valid loss:0.003030
Epoch:11, Train loss:0.004452, valid loss:0.002597
Epoch:12, Train loss:0.004427, valid loss:0.002590
Epoch:13, Train loss:0.004399, valid loss:0.002530
Epoch:14, Train loss:0.004366, valid loss:0.002193
Epoch:15, Train loss:0.004316, valid loss:0.002074
Epoch:16, Train loss:0.004124, valid loss:0.002917
Epoch:17, Train loss:0.004067, valid loss:0.002286
Epoch:18, Train loss:0.004058, valid loss:0.001980
Epoch:19, Train loss:0.003922, valid loss:0.002511
Epoch:20, Train loss:0.004059, valid loss:0.002161
Epoch:21, Train loss:0.003195, valid loss:0.001910
Epoch:22, Train loss:0.003145, valid loss:0.001849
Epoch:23, Train loss:0.003145, valid loss:0.001750
Epoch:24, Train loss:0.003122, valid loss:0.001845
Epoch:25, Train loss:0.003144, valid loss:0.002028
Epoch:26, Train loss:0.003026, valid loss:0.001773
Epoch:27, Train loss:0.003090, valid loss:0.001702
Epoch:28, Train loss:0.003003, valid loss:0.001857
Epoch:29, Train loss:0.002998, valid loss:0.001938
Epoch:30, Train loss:0.002961, valid loss:0.001759
Epoch:31, Train loss:0.002577, valid loss:0.001644
Epoch:32, Train loss:0.002570, valid loss:0.001752
Epoch:33, Train loss:0.002525, valid loss:0.001622
Epoch:34, Train loss:0.002536, valid loss:0.001668
Epoch:35, Train loss:0.002533, valid loss:0.001678
Epoch:36, Train loss:0.002513, valid loss:0.001649
Epoch:37, Train loss:0.002500, valid loss:0.001548
Epoch:38, Train loss:0.002478, valid loss:0.001516
Epoch:39, Train loss:0.002487, valid loss:0.001613
Epoch:40, Train loss:0.002469, valid loss:0.001440
Epoch:41, Train loss:0.002263, valid loss:0.001494
Epoch:42, Train loss:0.002249, valid loss:0.001579
Epoch:43, Train loss:0.002243, valid loss:0.001521
Epoch:44, Train loss:0.002242, valid loss:0.001412
Epoch:45, Train loss:0.002238, valid loss:0.001413
Epoch:46, Train loss:0.002235, valid loss:0.001428
Epoch:47, Train loss:0.002222, valid loss:0.001423
Epoch:48, Train loss:0.002208, valid loss:0.001534
Epoch:49, Train loss:0.002202, valid loss:0.001407
Epoch:50, Train loss:0.002202, valid loss:0.001396
Epoch:51, Train loss:0.002108, valid loss:0.001395
Epoch:52, Train loss:0.002097, valid loss:0.001345
Epoch:53, Train loss:0.002095, valid loss:0.001398
Epoch:54, Train loss:0.002089, valid loss:0.001394
Epoch:55, Train loss:0.002088, valid loss:0.001389
Epoch:56, Train loss:0.002086, valid loss:0.001411
Epoch:57, Train loss:0.002085, valid loss:0.001397
Epoch:58, Train loss:0.002078, valid loss:0.001372
Epoch:59, Train loss:0.002072, valid loss:0.001393
Epoch:60, Train loss:0.002074, valid loss:0.001389
training time 5080.268172979355
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.04571226952392432
plot_id,batch_id 0 1 miss% 0.023519875356431085
plot_id,batch_id 0 2 miss% 0.026358485050280323
plot_id,batch_id 0 3 miss% 0.029800372818434003
plot_id,batch_id 0 4 miss% 0.022134636840323206
plot_id,batch_id 0 5 miss% 0.0326903219678766
plot_id,batch_id 0 6 miss% 0.03301589126888982
plot_id,batch_id 0 7 miss% 0.02504527647962048
plot_id,batch_id 0 8 miss% 0.0435075223720177
plot_id,batch_id 0 9 miss% 0.03931720311939027
plot_id,batch_id 0 10 miss% 0.07104304919362446
plot_id,batch_id 0 11 miss% 0.05556999211805044
plot_id,batch_id 0 12 miss% 0.03241230982927646
plot_id,batch_id 0 13 miss% 0.024053038688350646
plot_id,batch_id 0 14 miss% 0.03942788386756788
plot_id,batch_id 0 15 miss% 0.04534585123323799
plot_id,batch_id 0 16 miss% 0.034878378105013805
plot_id,batch_id 0 17 miss% 0.06253750278435857
plot_id,batch_id 0 18 miss% 0.042059369599262625
plot_id,batch_id 0 19 miss% 0.03742089324130864
plot_id,batch_id 0 20 miss% 0.08171709189449738
plot_id,batch_id 0 21 miss% 0.031218630416642596
plot_id,batch_id 0 22 miss% 0.031392105710836525
plot_id,batch_id 0 23 miss% 0.03920533764535412
plot_id,batch_id 0 24 miss% 0.02473640768129833
plot_id,batch_id 0 25 miss% 0.028167841271467046
plot_id,batch_id 0 26 miss% 0.035905501928499686
plot_id,batch_id 0 27 miss% 0.025642100139118322
plot_id,batch_id 0 28 miss% 0.02055885153553553
plot_id,batch_id 0 29 miss% 0.030617051535720818
plot_id,batch_id 0 30 miss% 0.05456902056458534
plot_id,batch_id 0 31 miss% 0.035501803935133
plot_id,batch_id 0 32 miss% 0.040608905340767595
plot_id,batch_id 0 33 miss% 0.030519764068546368
plot_id,batch_id 0 34 miss% 0.03875489758496542
plot_id,batch_id 0 35 miss% 0.053080430792638925
plot_id,batch_id 0 36 miss% 0.05293394320134455
plot_id,batch_id 0 37 miss% 0.03615328284195156
plot_id,batch_id 0 38 miss% 0.026632829924632652
plot_id,batch_id 0 39 miss% 0.022028716204894147
plot_id,batch_id 0 40 miss% 0.07251064574805936
plot_id,batch_id 0 41 miss% 0.02672971565727073
plot_id,batch_id 0 42 miss% 0.019656518655559577
plot_id,batch_id 0 43 miss% 0.027086013201604642
plot_id,batch_id 0 44 miss% 0.03673575204284079
plot_id,batch_id 0 45 miss% 0.03798987379678952
plot_id,batch_id 0 46 miss% 0.023737010535727836
plot_id,batch_id 0 47 miss% 0.033143938047444255
plot_id,batch_id 0 48 miss% 0.03647823587073092
plot_id,batch_id 0 49 miss% 0.041888068823369325
plot_id,batch_id 0 50 miss% 0.030099436823294596
plot_id,batch_id 0 51 miss% 0.024551781897721614
plot_id,batch_id 0 52 miss% 0.03199523512137856
plot_id,batch_id 0 53 miss% 0.019957080917764436
plot_id,batch_id 0 54 miss% 0.04517396745273013
plot_id,batch_id 0 55 miss% 0.029952926888745887
plot_id,batch_id 0 56 miss% 0.0259732328637857
plot_id,batch_id 0 57 miss% 0.04014458960682101
plot_id,batch_id 0 58 miss% 0.029957706180869938
plot_id,batch_id 0 59 miss% 0.03061332700340814
plot_id,batch_id 0 60 miss% 0.04813895708436152
plot_id,batch_id 0 61 miss% 0.033919171085706856
plot_id,batch_id 0 62 miss% 0.042327028939589575
plot_id,batch_id 0 63 miss% 0.03228490999262629
plot_id,batch_id 0 64 miss% 0.0488385231250587
plot_id,batch_id 0 65 miss% 0.06331162011130652
plot_id,batch_id 0 66 miss% 0.0635706371430864
plot_id,batch_id 0 67 miss% 0.03569002730185832
plot_id,batch_id 0 68 miss% 0.02998110505153381
plot_id,batch_id 0 69 miss% 0.04072322964571171
plot_id,batch_id 0 70 miss% 0.0533853004081382
plot_id,batch_id 0 71 miss% 0.04431030946499943
plot_id,batch_id 0 72 miss% 0.03932478818825811
plot_id,batch_id 0 73 miss% 0.030541595338337058
plot_id,batch_id 0 74 miss% 0.04982692145316315
plot_id,batch_id 0 75 miss% 0.0630664316278283
plot_id,batch_id 0 76 miss% 0.07692969252061294
plot_id,batch_id 0 77 miss% 0.03745194641405109
plot_id,batch_id 0 78 miss% 0.0422815484011118
plot_id,batch_id 0 79 miss% 0.07090131422552194
plot_id,batch_id 0 80 miss% 0.048433937148072884
plot_id,batch_id 0 81 miss% 0.031346720620170125
plot_id,batch_id 0 82 miss% 0.02591211877807541
plot_id,batch_id 0 83 miss% 0.031294805165836714
plot_id,batch_id 0 84 miss% 0.021116639197644914
plot_id,batch_id 0 85 miss% 0.05014925772262397
plot_id,batch_id 0 86 miss% 0.03850546267667818
plot_id,batch_id 0 87 miss% 0.04379429452771079
plot_id,batch_id 0 88 miss% 0.04037961185191181
plot_id,batch_id 0 89 miss% 0.03912999148882121
plot_id,batch_id 0 90 miss% 0.035244379602314406
plot_id,batch_id 0 91 miss% 0.035231390725553195
plot_id,batch_id 0 92 miss% 0.05044140589925771
plot_id,batch_id 0 93 miss% 0.03647365641479011
plot_id,batch_id 0 94 miss% 0.06062154812205248
plot_id,batch_id 0 95 miss% 0.05340194287197173
plot_id,batch_id 0 96 miss% 0.030794163685303362
plot_id,batch_id 0 97 miss% 0.048303771350305655
plot_id,batch_id 0 98 miss% 0.04394401787989538
plot_id,batch_id 0 99 miss% 0.06128262881989323
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03200275 0.03277989 0.03362124 0.02417892 0.01814712 0.05320003
 0.03515639 0.03430716 0.04041752 0.021411   0.03172545 0.0469155
 0.03273061 0.03442951 0.05322248 0.0581902  0.03342219 0.05787605
 0.04855391 0.05409673 0.08283454 0.02419593 0.02472853 0.02027325
 0.02349453 0.03964834 0.02805641 0.04502    0.02221132 0.0266567
 0.03329702 0.0550329  0.05090066 0.03693021 0.04243773 0.05479834
 0.07089818 0.03249753 0.03541553 0.02557638 0.07859665 0.02576577
 0.01477476 0.02504648 0.01638259 0.0186892  0.03080748 0.02917216
 0.02470569 0.02079337 0.03210734 0.03476571 0.03390797 0.01502118
 0.03394996 0.0567685  0.03375132 0.02619482 0.03174925 0.02567011
 0.05511719 0.03931025 0.03194553 0.03256104 0.02919366 0.04479178
 0.05482252 0.03915973 0.04516078 0.03498992 0.0533853  0.04431031
 0.03932479 0.0305416  0.04982692 0.06306643 0.07692969 0.03745195
 0.04228155 0.07090131 0.04843394 0.03134672 0.02591212 0.03129481
 0.02111664 0.05014926 0.03850546 0.04379429 0.04037961 0.03912999
 0.03524438 0.03523139 0.05044141 0.03647366 0.06062155 0.05340194
 0.03079416 0.04830377 0.04394402 0.06128263]
for model  10 the mean error 0.03900782967151611
all id 10 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 21 out win 4 err 0.03900782967151611
Launcher: Job 11 completed in 5287 seconds.
Launcher: Task 167 done. Exiting.
plot_id,batch_id 0 70 miss% 0.05344991484899297
plot_id,batch_id 0 71 miss% 0.053052419882084714
plot_id,batch_id 0 72 miss% 0.03393642600929827
plot_id,batch_id 0 73 miss% 0.033535246199076406
plot_id,batch_id 0 74 miss% 0.039773472444059245
plot_id,batch_id 0 75 miss% 0.05068793161756882
plot_id,batch_id 0 76 miss% 0.05295803418427317
plot_id,batch_id 0 77 miss% 0.0620327730401718
plot_id,batch_id 0 78 miss% 0.04476656447846651
plot_id,batch_id 0 79 miss% 0.04808827728896511
plot_id,batch_id 0 80 miss% 0.0572226283938317
plot_id,batch_id 0 81 miss% 0.036554862230255586
plot_id,batch_id 0 82 miss% 0.025938036367509732
plot_id,batch_id 0 83 miss% 0.04057436840862038
plot_id,batch_id 0 84 miss% 0.03880903611282043
plot_id,batch_id 0 85 miss% 0.051790821941566534
plot_id,batch_id 0 86 miss% 0.036145048057678965
plot_id,batch_id 0 87 miss% 0.0401692635901655
plot_id,batch_id 0 88 miss% 0.044808122313224126
plot_id,batch_id 0 89 miss% 0.03637340709483772
plot_id,batch_id 0 90 miss% 0.04644152498866721
plot_id,batch_id 0 91 miss% 0.0393892924238572
plot_id,batch_id 0 92 miss% 0.05298950767310693
plot_id,batch_id 0 93 miss% 0.04295165941935419
plot_id,batch_id 0 94 miss% 0.028838598048102716
plot_id,batch_id 0 95 miss% 0.06731310755820051
plot_id,batch_id 0 96 miss% 0.03171026668285137
plot_id,batch_id 0 97 miss% 0.05476007219070466
plot_id,batch_id 0 98 miss% 0.03764179220889444
plot_id,batch_id 0 99 miss% 0.040814782205805554
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04571227 0.02351988 0.02635849 0.02980037 0.02213464 0.03269032
 0.03301589 0.02504528 0.04350752 0.0393172  0.07104305 0.05556999
 0.03241231 0.02405304 0.03942788 0.04534585 0.03487838 0.0625375
 0.04205937 0.03742089 0.08171709 0.03121863 0.03139211 0.03920534
 0.02473641 0.02816784 0.0359055  0.0256421  0.02055885 0.03061705
 0.05456902 0.0355018  0.04060891 0.03051976 0.0387549  0.05308043
 0.05293394 0.03615328 0.02663283 0.02202872 0.07251065 0.02672972
 0.01965652 0.02708601 0.03673575 0.03798987 0.02373701 0.03314394
 0.03647824 0.04188807 0.03009944 0.02455178 0.03199524 0.01995708
 0.04517397 0.02995293 0.02597323 0.04014459 0.02995771 0.03061333
 0.04813896 0.03391917 0.04232703 0.03228491 0.04883852 0.06331162
 0.06357064 0.03569003 0.02998111 0.04072323 0.05344991 0.05305242
 0.03393643 0.03353525 0.03977347 0.05068793 0.05295803 0.06203277
 0.04476656 0.04808828 0.05722263 0.03655486 0.02593804 0.04057437
 0.03880904 0.05179082 0.03614505 0.04016926 0.04480812 0.03637341
 0.04644152 0.03938929 0.05298951 0.04295166 0.0288386  0.06731311
 0.03171027 0.05476007 0.03764179 0.04081478]
for model  37 the mean error 0.03932472162195438
all id 37 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 21 out win 4 err 0.03932472162195438
Launcher: Job 38 completed in 5291 seconds.
Launcher: Task 16 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  46193
Epoch:0, Train loss:0.567454, valid loss:0.526344
Epoch:1, Train loss:0.353189, valid loss:0.358962
Epoch:2, Train loss:0.342443, valid loss:0.357429
Epoch:3, Train loss:0.340840, valid loss:0.357888
Epoch:4, Train loss:0.339833, valid loss:0.357440
Epoch:5, Train loss:0.339549, valid loss:0.357336
Epoch:6, Train loss:0.339264, valid loss:0.356577
Epoch:7, Train loss:0.338937, valid loss:0.357045
Epoch:8, Train loss:0.338956, valid loss:0.356631
Epoch:9, Train loss:0.339929, valid loss:0.356467
Epoch:10, Train loss:0.338673, valid loss:0.356606
Epoch:11, Train loss:0.337582, valid loss:0.356043
Epoch:12, Train loss:0.337571, valid loss:0.356317
Epoch:13, Train loss:0.337583, valid loss:0.356009
Epoch:14, Train loss:0.337561, valid loss:0.355915
Epoch:15, Train loss:0.337488, valid loss:0.355886
Epoch:16, Train loss:0.337424, valid loss:0.355906
Epoch:17, Train loss:0.337391, valid loss:0.356117
Epoch:18, Train loss:0.337339, valid loss:0.355788
Epoch:19, Train loss:0.337285, valid loss:0.355992
Epoch:20, Train loss:0.337283, valid loss:0.356009
Epoch:21, Train loss:0.336825, valid loss:0.355674
Epoch:22, Train loss:0.336813, valid loss:0.355651
Epoch:23, Train loss:0.336829, valid loss:0.355607
Epoch:24, Train loss:0.336795, valid loss:0.355583
Epoch:25, Train loss:0.336777, valid loss:0.355525
Epoch:26, Train loss:0.336768, valid loss:0.355579
Epoch:27, Train loss:0.336784, valid loss:0.355594
Epoch:28, Train loss:0.336713, valid loss:0.355632
Epoch:29, Train loss:0.336740, valid loss:0.355599
Epoch:30, Train loss:0.336705, valid loss:0.355685
Epoch:31, Train loss:0.336489, valid loss:0.355482
Epoch:32, Train loss:0.336492, valid loss:0.355516
Epoch:33, Train loss:0.336486, valid loss:0.355499
Epoch:34, Train loss:0.336479, valid loss:0.355471
Epoch:35, Train loss:0.336463, valid loss:0.355489
Epoch:36, Train loss:0.336484, valid loss:0.355467
Epoch:37, Train loss:0.336453, valid loss:0.355475
Epoch:38, Train loss:0.336464, valid loss:0.355471
Epoch:39, Train loss:0.336482, valid loss:0.355488
Epoch:40, Train loss:0.336454, valid loss:0.355546
Epoch:41, Train loss:0.336335, valid loss:0.355473
Epoch:42, Train loss:0.336329, valid loss:0.355484
Epoch:43, Train loss:0.336339, valid loss:0.355427
Epoch:44, Train loss:0.336343, valid loss:0.355396
Epoch:45, Train loss:0.336334, valid loss:0.355444
Epoch:46, Train loss:0.336333, valid loss:0.355393
Epoch:47, Train loss:0.336322, valid loss:0.355475
Epoch:48, Train loss:0.336318, valid loss:0.355398
Epoch:49, Train loss:0.336322, valid loss:0.355431
Epoch:50, Train loss:0.336324, valid loss:0.355422
Epoch:51, Train loss:0.336265, valid loss:0.355401
Epoch:52, Train loss:0.336264, valid loss:0.355397
Epoch:53, Train loss:0.336262, valid loss:0.355392
Epoch:54, Train loss:0.336257, valid loss:0.355430
Epoch:55, Train loss:0.336259, valid loss:0.355431
Epoch:56, Train loss:0.336264, valid loss:0.355404
Epoch:57, Train loss:0.336260, valid loss:0.355404
Epoch:58, Train loss:0.336250, valid loss:0.355421
Epoch:59, Train loss:0.336252, valid loss:0.355389
Epoch:60, Train loss:0.336258, valid loss:0.355386
training time 5199.270207643509
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.6406321278582198
plot_id,batch_id 0 1 miss% 0.7346814107379528
plot_id,batch_id 0 2 miss% 0.7458227266132829
plot_id,batch_id 0 3 miss% 0.7550901645013869
plot_id,batch_id 0 4 miss% 0.7598995486930863
plot_id,batch_id 0 5 miss% 0.6358327414598249
plot_id,batch_id 0 6 miss% 0.729452491885171
plot_id,batch_id 0 7 miss% 0.7479200468941728
plot_id,batch_id 0 8 miss% 0.7575869515972504
plot_id,batch_id 0 9 miss% 0.7575398213609801
plot_id,batch_id 0 10 miss% 0.6124467231803453
plot_id,batch_id 0 11 miss% 0.7325146934045457
plot_id,batch_id 0 12 miss% 0.7404130612057142
plot_id,batch_id 0 13 miss% 0.7509615054147378
plot_id,batch_id 0 14 miss% 0.7603045740235682
plot_id,batch_id 0 15 miss% 0.6267224113692909
plot_id,batch_id 0 16 miss% 0.719854272257321
plot_id,batch_id 0 17 miss% 0.7455389347634354
plot_id,batch_id 0 18 miss% 0.7497960347643157
plot_id,batch_id 0 19 miss% 0.7534716701021722
plot_id,batch_id 0 20 miss% 0.6966292718825386
plot_id,batch_id 0 21 miss% 0.7484079354179547
plot_id,batch_id 0 22 miss% 0.7569768684265796
plot_id,batch_id 0 23 miss% 0.7631368230006995
plot_id,batch_id 0 24 miss% 0.764518522221489
plot_id,batch_id 0 25 miss% 0.681596437983885
plot_id,batch_id 0 26 miss% 0.7412487640076436
plot_id,batch_id 0 27 miss% 0.7544524938925937
plot_id,batch_id 0 28 miss% 0.762446331803886
plot_id,batch_id 0 29 miss% 0.7647929281810265
plot_id,batch_id 0 30 miss% 0.6652529089930006
plot_id,batch_id 0 31 miss% 0.7439761574227381
plot_id,batch_id 0 32 miss% 0.7505913415927413
plot_id,batch_id 0 33 miss% 0.7632225204747998
plot_id,batch_id 0 34 miss% 0.7626760865512251
plot_id,batch_id 0 35 miss% 0.6701339146055422
plot_id,batch_id 0 36 miss% 0.7426619150416974
plot_id,batch_id 0 37 miss% 0.75176821382183
plot_id,batch_id 0 38 miss% 0.7587587369064348
plot_id,batch_id 0 39 miss% 0.7597807599938593
plot_id,batch_id 0 40 miss% 0.7142358430458946
plot_id,batch_id 0 41 miss% 0.7553626170732957
plot_id,batch_id 0 42 miss% 0.7617628893707828
plot_id,batch_id 0 43 miss% 0.7647692918440352
plot_id,batch_id 0 44 miss% 0.7685106511901313
plot_id,batch_id 0 45 miss% 0.7225967401947134
plot_id,batch_id 0 46 miss% 0.7553211397624887
plot_id,batch_id 0 47 miss% 0.7647166123591047
plot_id,batch_id 0 48 miss% 0.763880282551975
plot_id,batch_id 0 49 miss% 0.7698334602358273
plot_id,batch_id 0 50 miss% 0.7195719390440259
plot_id,batch_id 0 51 miss% 0.7561081221381192
plot_id,batch_id 0 52 miss% 0.7569246721563125
plot_id,batch_id 0 53 miss% 0.7622908953334763
plot_id,batch_id 0 54 miss% 0.7687808448434718
plot_id,batch_id 0 55 miss% 0.717969321492191
plot_id,batch_id 0 56 miss% 0.7549708125982377
plot_id,batch_id 0 57 miss% 0.7612901965318507
plot_id,batch_id 0 58 miss% 0.7658718104961767
plot_id,batch_id 0 59 miss% 0.7655963845803326
plot_id,batch_id 0 60 miss% 0.5365463224665864
plot_id,batch_id 0 61 miss% 0.6894070400087003
plot_id,batch_id 0 62 miss% 0.7235534845294255
plot_id,batch_id 0 63 miss% 0.7440976030203461
plot_id,batch_id 0 64 miss% 0.746651242503113
plot_id,batch_id 0 65 miss% 0.5264406633132583
plot_id,batch_id 0 66 miss% 0.6878401884289103
plot_id,batch_id 0 67 miss% 0.7057607931041339
plot_id,batch_id 0 68 miss% 0.7384628158525746
plot_id,batch_id 0 69 miss% 0.7446840044272194
plot_id,batch_id 0 70 miss% 0.5004220823627921
plot_id,batch_id 0 71 miss% 0.7021185186221045
plot_id,batch_id 0 72 miss% 0.6960167223572755
plot_id,batch_id 0 73 miss% 0.7238168025700011
plot_id,batch_id 0 74 miss% 0.7356627849847384
plot_id,batch_id 0 75 miss% 0.4904474198690989
plot_id,batch_id 0 76 miss% 0.6419896003108975
plot_id,batch_id 0 77 miss% 0.6856164650887538
plot_id,batch_id 0 78 miss% 0.7232360817423589
plot_id,batch_id 0 79 miss% 0.7370208307515145
plot_id,batch_id 0 80 miss% 0.5691272174513918
plot_id,batch_id 0 81 miss% 0.7090956388994437
plot_id,batch_id 0 82 miss% 0.7329949857625286
plot_id,batch_id 0 83 miss% 0.7469320234643533
plot_id,batch_id 0 84 miss% 0.7477213569855679
plot_id,batch_id 0 85 miss% 0.5633392330813811
plot_id,batch_id 0 86 miss% 0.7022919640947144
plot_id,batch_id 0 87 miss% 0.7267381341390677
plot_id,batch_id 0 88 miss% 0.744053426386545
plot_id,batch_id 0 89 miss% 0.7495946785901372
plot_id,batch_id 0 90 miss% 0.5346871358910584
plot_id,batch_id 0 91 miss% 0.7062855447826333
plot_id,batch_id 0 92 miss% 0.7156551558180813
plot_id,batch_id 0 93 miss% 0.7420664460105947
plot_id,batch_id 0 94 miss% 0.7464378985167218
plot_id,batch_id 0 95 miss% 0.5316631599534263
plot_id,batch_id 0 96 miss% 0.6805764751064478
plot_id,batch_id 0 97 miss% 0.7179863843622354
plot_id,batch_id 0 98 miss% 0.7298765282800207
plot_id,batch_id 0 99 miss% 0.7425662708138617
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.64063213 0.73468141 0.74582273 0.75509016 0.75989955 0.63583274
 0.72945249 0.74792005 0.75758695 0.75753982 0.61244672 0.73251469
 0.74041306 0.75096151 0.76030457 0.62672241 0.71985427 0.74553893
 0.74979603 0.75347167 0.69662927 0.74840794 0.75697687 0.76313682
 0.76451852 0.68159644 0.74124876 0.75445249 0.76244633 0.76479293
 0.66525291 0.74397616 0.75059134 0.76322252 0.76267609 0.67013391
 0.74266192 0.75176821 0.75875874 0.75978076 0.71423584 0.75536262
 0.76176289 0.76476929 0.76851065 0.72259674 0.75532114 0.76471661
 0.76388028 0.76983346 0.71957194 0.75610812 0.75692467 0.7622909
 0.76878084 0.71796932 0.75497081 0.7612902  0.76587181 0.76559638
 0.53654632 0.68940704 0.72355348 0.7440976  0.74665124 0.52644066
 0.68784019 0.70576079 0.73846282 0.744684   0.50042208 0.70211852
 0.69601672 0.7238168  0.73566278 0.49044742 0.6419896  0.68561647
 0.72323608 0.73702083 0.56912722 0.70909564 0.73299499 0.74693202
 0.74772136 0.56333923 0.70229196 0.72673813 0.74405343 0.74959468
 0.53468714 0.70628554 0.71565516 0.74206645 0.7464379  0.53166316
 0.68057648 0.71798638 0.72987653 0.74256627]
for model  138 the mean error 0.716493564958554
all id 138 hidden_dim 24 learning_rate 0.02 num_layers 3 frames 25 out win 3 err 0.716493564958554
Launcher: Job 139 completed in 5378 seconds.
Launcher: Task 253 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  61841
Epoch:0, Train loss:0.759093, valid loss:0.763515
Epoch:1, Train loss:0.512642, valid loss:0.516635
Epoch:2, Train loss:0.494537, valid loss:0.516212
Epoch:3, Train loss:0.491874, valid loss:0.514426
Epoch:4, Train loss:0.490028, valid loss:0.514211
Epoch:5, Train loss:0.489621, valid loss:0.514555
Epoch:6, Train loss:0.488843, valid loss:0.514624
Epoch:7, Train loss:0.488661, valid loss:0.513285
Epoch:8, Train loss:0.488593, valid loss:0.513669
Epoch:9, Train loss:0.488372, valid loss:0.514148
Epoch:10, Train loss:0.488018, valid loss:0.513400
Epoch:11, Train loss:0.486606, valid loss:0.512535
Epoch:12, Train loss:0.486559, valid loss:0.512572
Epoch:13, Train loss:0.486522, valid loss:0.512752
Epoch:14, Train loss:0.486530, valid loss:0.512648
Epoch:15, Train loss:0.486474, valid loss:0.512703
Epoch:16, Train loss:0.486323, valid loss:0.512287
Epoch:17, Train loss:0.486369, valid loss:0.512510
Epoch:18, Train loss:0.486340, valid loss:0.513139
Epoch:19, Train loss:0.486360, valid loss:0.512277
Epoch:20, Train loss:0.486132, valid loss:0.512523
Epoch:21, Train loss:0.485553, valid loss:0.512386
Epoch:22, Train loss:0.485565, valid loss:0.512144
Epoch:23, Train loss:0.485553, valid loss:0.512124
Epoch:24, Train loss:0.485549, valid loss:0.512470
Epoch:25, Train loss:0.485511, valid loss:0.512140
Epoch:26, Train loss:0.485598, valid loss:0.512599
Epoch:27, Train loss:0.485515, valid loss:0.512177
Epoch:28, Train loss:0.485506, valid loss:0.512335
Epoch:29, Train loss:0.485518, valid loss:0.512205
Epoch:30, Train loss:0.485419, valid loss:0.512442
Epoch:31, Train loss:0.485137, valid loss:0.512034
Epoch:32, Train loss:0.485141, valid loss:0.511955
Epoch:33, Train loss:0.485129, valid loss:0.512018
Epoch:34, Train loss:0.485126, valid loss:0.511999
Epoch:35, Train loss:0.485109, valid loss:0.511997
Epoch:36, Train loss:0.485099, valid loss:0.512020
Epoch:37, Train loss:0.485110, valid loss:0.511966
Epoch:38, Train loss:0.485144, valid loss:0.511929
Epoch:39, Train loss:0.485067, valid loss:0.512013
Epoch:40, Train loss:0.485070, valid loss:0.511966
Epoch:41, Train loss:0.484942, valid loss:0.511936
Epoch:42, Train loss:0.484930, valid loss:0.511907
Epoch:43, Train loss:0.484939, valid loss:0.511902
Epoch:44, Train loss:0.484928, valid loss:0.512046
Epoch:45, Train loss:0.484941, valid loss:0.511931
Epoch:46, Train loss:0.484921, valid loss:0.511943
Epoch:47, Train loss:0.484916, valid loss:0.511898
Epoch:48, Train loss:0.484909, valid loss:0.511940
Epoch:49, Train loss:0.484915, valid loss:0.511954
Epoch:50, Train loss:0.484911, valid loss:0.511899
Epoch:51, Train loss:0.484838, valid loss:0.511873
Epoch:52, Train loss:0.484843, valid loss:0.511882
Epoch:53, Train loss:0.484835, valid loss:0.511886
Epoch:54, Train loss:0.484838, valid loss:0.511915
Epoch:55, Train loss:0.484832, valid loss:0.511894
Epoch:56, Train loss:0.484830, valid loss:0.511915
Epoch:57, Train loss:0.484825, valid loss:0.511891
Epoch:58, Train loss:0.484825, valid loss:0.511886
Epoch:59, Train loss:0.484821, valid loss:0.511889
Epoch:60, Train loss:0.484835, valid loss:0.511858
training time 5230.922058105469
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.7012944469269897
plot_id,batch_id 0 1 miss% 0.7720360873938844
plot_id,batch_id 0 2 miss% 0.7814289187256765
plot_id,batch_id 0 3 miss% 0.7887190195750534
plot_id,batch_id 0 4 miss% 0.7921337465701286
plot_id,batch_id 0 5 miss% 0.6937823862299656
plot_id,batch_id 0 6 miss% 0.76734232445887
plot_id,batch_id 0 7 miss% 0.7824143713618988
plot_id,batch_id 0 8 miss% 0.7898521744083794
plot_id,batch_id 0 9 miss% 0.7975388301052116
plot_id,batch_id 0 10 miss% 0.6695454546241842
plot_id,batch_id 0 11 miss% 0.7678629591250546
plot_id,batch_id 0 12 miss% 0.7777034665539871
plot_id,batch_id 0 13 miss% 0.7903365925315374
plot_id,batch_id 0 14 miss% 0.789368656562482
plot_id,batch_id 0 15 miss% 0.6867332508856022
plot_id,batch_id 0 16 miss% 0.7602209554820558
plot_id,batch_id 0 17 miss% 0.7780366609947276
plot_id,batch_id 0 18 miss% 0.7854096234690495
plot_id,batch_id 0 19 miss% 0.7910426956311406
plot_id,batch_id 0 20 miss% 0.746974485611943
plot_id,batch_id 0 21 miss% 0.784805969467113
plot_id,batch_id 0 22 miss% 0.7881958309781667
plot_id,batch_id 0 23 miss% 0.7942206509526856
plot_id,batch_id 0 24 miss% 0.7955007121535503
plot_id,batch_id 0 25 miss% 0.7301753107938431
plot_id,batch_id 0 26 miss% 0.7789415129330152
plot_id,batch_id 0 27 miss% 0.7873415279387798
plot_id,batch_id 0 28 miss% 0.792164612625408
plot_id,batch_id 0 29 miss% 0.7943936379101687
plot_id,batch_id 0 30 miss% 0.7290413314956844
plot_id,batch_id 0 31 miss% 0.7755927969670477
plot_id,batch_id 0 32 miss% 0.7894467615736913
plot_id,batch_id 0 33 miss% 0.7918918694650736
plot_id,batch_id 0 34 miss% 0.7961191855271676
plot_id,batch_id 0 35 miss% 0.7223798118008659
plot_id,batch_id 0 36 miss% 0.7819277235355321
plot_id,batch_id 0 37 miss% 0.7835175401222588
plot_id,batch_id 0 38 miss% 0.7960476405702412
plot_id,batch_id 0 39 miss% 0.7953036488374047
plot_id,batch_id 0 40 miss% 0.7645813730077079
plot_id,batch_id 0 41 miss% 0.7890141085553533
plot_id,batch_id 0 42 miss% 0.7901648882549649
plot_id,batch_id 0 43 miss% 0.7989864708827455
plot_id,batch_id 0 44 miss% 0.7974986859841267
plot_id,batch_id 0 45 miss% 0.7607829322295732
plot_id,batch_id 0 46 miss% 0.7911771919086765
plot_id,batch_id 0 47 miss% 0.7921829139812473
plot_id,batch_id 0 48 miss% 0.7986635561636064
plot_id,batch_id 0 49 miss% 0.7971544047917264
plot_id,batch_id 0 50 miss% 0.7646206176819258
plot_id,batch_id 0 51 miss% 0.7890136651477867
plot_id,batch_id 0 52 miss% 0.7920551918716733
plot_id,batch_id 0 53 miss% 0.7959619301811377
plot_id,batch_id 0 54 miss% 0.7995034104413818
plot_id,batch_id 0 55 miss% 0.758505530836812
plot_id,batch_id 0 56 miss% 0.7865805045448967
plot_id,batch_id 0 57 miss% 0.7952559761457219
plot_id,batch_id 0 58 miss% 0.7959120357487335
plot_id,batch_id 0 59 miss% 0.797495106789982
plot_id,batch_id 0 60 miss% 0.6096396541013666
plot_id,batch_id 0 61 miss% 0.7319341576318295
plot_id,batch_id 0 62 miss% 0.7604444964612298
plot_id,batch_id 0 63 miss% 0.7809037002383229
plot_id,batch_id 0 64 miss% 0.7808982702878761
plot_id,batch_id 0 65 miss% 0.6125838416889635
plot_id,batch_id 0 66 miss% 0.7259728821440644
plot_id,batch_id 0 67 miss% 0.7465664059127914
plot_id,batch_id 0 68 miss% 0.7749265444220179
plot_id,batch_id 0 69 miss% 0.777100025866618
plot_id,batch_id 0 70 miss% 0.5705920885998554
plot_id,batch_id 0 71 miss% 0.742887575114978
plot_id,batch_id 0 72 miss% 0.7431478469379832
plot_id,batch_id 0 73 miss% 0.7607488866742538
plot_id,batch_id 0 74 miss% 0.7703254724332987
plot_id,batch_id 0 75 miss% 0.5616877473045201
plot_id,batch_id 0 76 miss% 0.6870809142663955
plot_id,batch_id 0 77 miss% 0.7251685922384229
plot_id,batch_id 0 78 miss% 0.7580028268521419
plot_id,batch_id 0 79 miss% 0.7667082080428755
plot_id,batch_id 0 80 miss% 0.6324904608410763
plot_id,batch_id 0 81 miss% 0.7534583808891908
plot_id,batch_id 0 82 miss% 0.7731591972481097
plot_id,batch_id 0 83 miss% 0.7830679783904113
plot_id,batch_id 0 84 miss% 0.7840452288901902
plot_id,batch_id 0 85 miss% 0.6364331548608154
plot_id,batch_id 0 86 miss% 0.745316733443559
plot_id,batch_id 0 87 miss% 0.7651851691941485
plot_id,batch_id 0 88 miss% 0.7798406098535422
plot_id,batch_id 0 89 miss% 0.7818240370899979
plot_id,batch_id 0 90 miss% 0.6066046988330223
plot_id,batch_id 0 91 miss% 0.7417762228155541
plot_id,batch_id 0 92 miss% 0.7601199112526312
plot_id,batch_id 0 93 miss% 0.7713991620044708
plot_id,batch_id 0 94 miss% 0.7806133409988988
plot_id,batch_id 0 95 miss% 0.6111140407795973
plot_id,batch_id 0 96 miss% 0.7234929988106422
plot_id,batch_id 0 97 miss% 0.7525487918320326
plot_id,batch_id 0 98 miss% 0.7714781982085053
plot_id,batch_id 0 99 miss% 0.7758763751231054
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.70129445 0.77203609 0.78142892 0.78871902 0.79213375 0.69378239
 0.76734232 0.78241437 0.78985217 0.79753883 0.66954545 0.76786296
 0.77770347 0.79033659 0.78936866 0.68673325 0.76022096 0.77803666
 0.78540962 0.7910427  0.74697449 0.78480597 0.78819583 0.79422065
 0.79550071 0.73017531 0.77894151 0.78734153 0.79216461 0.79439364
 0.72904133 0.7755928  0.78944676 0.79189187 0.79611919 0.72237981
 0.78192772 0.78351754 0.79604764 0.79530365 0.76458137 0.78901411
 0.79016489 0.79898647 0.79749869 0.76078293 0.79117719 0.79218291
 0.79866356 0.7971544  0.76462062 0.78901367 0.79205519 0.79596193
 0.79950341 0.75850553 0.7865805  0.79525598 0.79591204 0.79749511
 0.60963965 0.73193416 0.7604445  0.7809037  0.78089827 0.61258384
 0.72597288 0.74656641 0.77492654 0.77710003 0.57059209 0.74288758
 0.74314785 0.76074889 0.77032547 0.56168775 0.68708091 0.72516859
 0.75800283 0.76670821 0.63249046 0.75345838 0.7731592  0.78306798
 0.78404523 0.63643315 0.74531673 0.76518517 0.77984061 0.78182404
 0.6066047  0.74177622 0.76011991 0.77139916 0.78061334 0.61111404
 0.723493   0.75254879 0.7714782  0.77587638]
for model  66 the mean error 0.7568906050660459
all id 66 hidden_dim 24 learning_rate 0.02 num_layers 4 frames 21 out win 3 err 0.7568906050660459
Launcher: Job 67 completed in 5413 seconds.
Launcher: Task 135 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  35921
Epoch:0, Train loss:0.778793, valid loss:0.763398
Epoch:1, Train loss:0.071289, valid loss:0.026829
Epoch:2, Train loss:0.042123, valid loss:0.011711
Epoch:3, Train loss:0.018141, valid loss:0.006513
Epoch:4, Train loss:0.012897, valid loss:0.005236
Epoch:5, Train loss:0.010968, valid loss:0.005453
Epoch:6, Train loss:0.009564, valid loss:0.005113
Epoch:7, Train loss:0.008668, valid loss:0.004432
Epoch:8, Train loss:0.007814, valid loss:0.004300
Epoch:9, Train loss:0.007157, valid loss:0.004199
Epoch:10, Train loss:0.006563, valid loss:0.003587
Epoch:11, Train loss:0.005097, valid loss:0.002963
Epoch:12, Train loss:0.004858, valid loss:0.003339
Epoch:13, Train loss:0.004528, valid loss:0.002512
Epoch:14, Train loss:0.004377, valid loss:0.002648
Epoch:15, Train loss:0.004332, valid loss:0.002316
Epoch:16, Train loss:0.004088, valid loss:0.002529
Epoch:17, Train loss:0.003937, valid loss:0.002409
Epoch:18, Train loss:0.003838, valid loss:0.002223
Epoch:19, Train loss:0.003703, valid loss:0.001900
Epoch:20, Train loss:0.003645, valid loss:0.002085
Epoch:21, Train loss:0.003019, valid loss:0.001705
Epoch:22, Train loss:0.003021, valid loss:0.002033
Epoch:23, Train loss:0.002964, valid loss:0.001819
Epoch:24, Train loss:0.002947, valid loss:0.001773
Epoch:25, Train loss:0.002955, valid loss:0.001713
Epoch:26, Train loss:0.002831, valid loss:0.001674
Epoch:27, Train loss:0.002863, valid loss:0.001695
Epoch:28, Train loss:0.002797, valid loss:0.001800
Epoch:29, Train loss:0.002747, valid loss:0.001650
Epoch:30, Train loss:0.002741, valid loss:0.001847
Epoch:31, Train loss:0.002445, valid loss:0.001493
Epoch:32, Train loss:0.002436, valid loss:0.001482
Epoch:33, Train loss:0.002418, valid loss:0.001548
Epoch:34, Train loss:0.002386, valid loss:0.001442
Epoch:35, Train loss:0.002419, valid loss:0.001442
Epoch:36, Train loss:0.002390, valid loss:0.001493
Epoch:37, Train loss:0.002344, valid loss:0.001496
Epoch:38, Train loss:0.002357, valid loss:0.001456
Epoch:39, Train loss:0.002325, valid loss:0.001406
Epoch:40, Train loss:0.002344, valid loss:0.001486
Epoch:41, Train loss:0.002174, valid loss:0.001379
Epoch:42, Train loss:0.002152, valid loss:0.001440
Epoch:43, Train loss:0.002153, valid loss:0.001350
Epoch:44, Train loss:0.002150, valid loss:0.001363
Epoch:45, Train loss:0.002145, valid loss:0.001369
Epoch:46, Train loss:0.002131, valid loss:0.001377
Epoch:47, Train loss:0.002128, valid loss:0.001375
Epoch:48, Train loss:0.002130, valid loss:0.001412
Epoch:49, Train loss:0.002102, valid loss:0.001362
Epoch:50, Train loss:0.002101, valid loss:0.001353
Epoch:51, Train loss:0.002030, valid loss:0.001291
Epoch:52, Train loss:0.002025, valid loss:0.001311
Epoch:53, Train loss:0.002024, valid loss:0.001321
Epoch:54, Train loss:0.002016, valid loss:0.001287
Epoch:55, Train loss:0.002013, valid loss:0.001302
Epoch:56, Train loss:0.002014, valid loss:0.001303
Epoch:57, Train loss:0.002015, valid loss:0.001280
Epoch:58, Train loss:0.002003, valid loss:0.001318
Epoch:59, Train loss:0.002003, valid loss:0.001340
Epoch:60, Train loss:0.002000, valid loss:0.001278
training time 5229.063996076584
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.04237679582720014
plot_id,batch_id 0 1 miss% 0.035259002574031806
plot_id,batch_id 0 2 miss% 0.023984679644429456
plot_id,batch_id 0 3 miss% 0.026994036742065772
plot_id,batch_id 0 4 miss% 0.019695053268210642
plot_id,batch_id 0 5 miss% 0.03754249841765851
plot_id,batch_id 0 6 miss% 0.028181353243617948
plot_id,batch_id 0 7 miss% 0.025143494361130357
plot_id,batch_id 0 8 miss% 0.024715478234035977
plot_id,batch_id 0 9 miss% 0.025627030896459707
plot_id,batch_id 0 10 miss% 0.05252472561312634
plot_id,batch_id 0 11 miss% 0.034434408980028565
plot_id,batch_id 0 12 miss% 0.024085564319281973
plot_id,batch_id 0 13 miss% 0.030658813560638355
plot_id,batch_id 0 14 miss% 0.0465073661168328
plot_id,batch_id 0 15 miss% 0.037137042016297264
plot_id,batch_id 0 16 miss% 0.03438201118546876
plot_id,batch_id 0 17 miss% 0.03683732590558907
plot_id,batch_id 0 18 miss% 0.03226252934401425
plot_id,batch_id 0 19 miss% 0.05435681554311247
plot_id,batch_id 0 20 miss% 0.05934310730955591
plot_id,batch_id 0 21 miss% 0.03141152645721044
plot_id,batch_id 0 22 miss% 0.033564003802228744
plot_id,batch_id 0 23 miss% 0.0384023508072637
plot_id,batch_id 0 24 miss% 0.029410462113434135
plot_id,batch_id 0 25 miss% 0.05464523035243485
plot_id,batch_id 0 26 miss% 0.028902668108125005
plot_id,batch_id 0 27 miss% 0.04136174165191876
plot_id,batch_id 0 28 miss% 0.03431706726328386
plot_id,batch_id 0 29 miss% 0.023160751869163016
plot_id,batch_id 0 30 miss% 0.04304407514407603
plot_id,batch_id 0 31 miss% 0.03266096434824739
plot_id,batch_id 0 32 miss% 0.03697339358996024
plot_id,batch_id 0 33 miss% 0.029415589456845036
plot_id,batch_id 0 34 miss% 0.02703897934650459
plot_id,batch_id 0 35 miss% 0.04963930504248954
plot_id,batch_id 0 36 miss% 0.06464976097158093
plot_id,batch_id 0 37 miss% 0.03549454395650329
plot_id,batch_id 0 38 miss% 0.035452742437595344
plot_id,batch_id 0 39 miss% 0.032414180204925286
plot_id,batch_id 0 40 miss% 0.05348058302137602
plot_id,batch_id 0 41 miss% 0.030545964959299333
plot_id,batch_id 0 42 miss% 0.024031806181160745
plot_id,batch_id 0 43 miss% 0.024175233719331342
plot_id,batch_id 0 44 miss% 0.023667837866058826
plot_id,batch_id 0 45 miss% 0.04679334344765225
plot_id,batch_id 0 46 miss% 0.02281231837469133
plot_id,batch_id 0 47 miss% 0.02673062657495303
plot_id,batch_id 0 48 miss% 0.03155480441188403
plot_id,batch_id 0 49 miss% 0.028273040722825705
plot_id,batch_id 0 50 miss% 0.044258217175272244
plot_id,batch_id 0 51 miss% 0.02718020905492879
plot_id,batch_id 0 52 miss% 0.02939824794180329
plot_id,batch_id 0 53 miss% 0.03276951016884251
plot_id,batch_id 0 54 miss% 0.0296035919095585
plot_id,batch_id 0 55 miss% 0.05887209420174724
plot_id,batch_id 0 56 miss% 0.03161065420003307
plot_id,batch_id 0 57 miss% 0.03972567481619206
plot_id,batch_id 0 58 miss% 0.020885969001089102
plot_id,batch_id 0 59 miss% 0.03156620601344668
plot_id,batch_id 0 60 miss% 0.03620778063301489
plot_id,batch_id 0 61 miss% 0.043792284203406306
plot_id,batch_id 0 62 miss% 0.03993750887993686
plot_id,batch_id 0 63 miss% 0.04337378290204892
plot_id,batch_id 0 64 miss% 0.051466235105753044
plot_id,batch_id 0 65 miss% 0.05627967130278813
plot_id,batch_id 0 66 miss% 0.02527672337526427
plot_id,batch_id 0 67 miss% 0.03463049139318734
plot_id,batch_id 0 68 miss% 0.039920309256422364
plot_id,batch_id 0 69 miss% 0.031529965033712734
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  21969
Epoch:0, Train loss:0.537238, valid loss:0.504722
Epoch:1, Train loss:0.358089, valid loss:0.363320
Epoch:2, Train loss:0.348391, valid loss:0.360971
Epoch:3, Train loss:0.346543, valid loss:0.360436
Epoch:4, Train loss:0.345647, valid loss:0.360865
Epoch:5, Train loss:0.345198, valid loss:0.360421
Epoch:6, Train loss:0.345041, valid loss:0.360642
Epoch:7, Train loss:0.344772, valid loss:0.360135
Epoch:8, Train loss:0.344431, valid loss:0.359407
Epoch:9, Train loss:0.344381, valid loss:0.359039
Epoch:10, Train loss:0.344359, valid loss:0.359651
Epoch:11, Train loss:0.343003, valid loss:0.358943
Epoch:12, Train loss:0.342916, valid loss:0.359048
Epoch:13, Train loss:0.342978, valid loss:0.358934
Epoch:14, Train loss:0.342974, valid loss:0.359127
Epoch:15, Train loss:0.342839, valid loss:0.358721
Epoch:16, Train loss:0.342833, valid loss:0.358896
Epoch:17, Train loss:0.342763, valid loss:0.358799
Epoch:18, Train loss:0.342871, valid loss:0.359522
Epoch:19, Train loss:0.342633, valid loss:0.358536
Epoch:20, Train loss:0.342583, valid loss:0.358596
Epoch:21, Train loss:0.262895, valid loss:0.173543
Epoch:22, Train loss:0.097992, valid loss:0.026872
Epoch:23, Train loss:0.038199, valid loss:0.020480
Epoch:24, Train loss:0.034844, valid loss:0.018138
Epoch:25, Train loss:0.032853, valid loss:0.016917
Epoch:26, Train loss:0.031169, valid loss:0.016116
Epoch:27, Train loss:0.029632, valid loss:0.015173
Epoch:28, Train loss:0.028097, valid loss:0.014179
Epoch:29, Train loss:0.026229, valid loss:0.012467
Epoch:30, Train loss:0.023969, valid loss:0.010417
Epoch:31, Train loss:0.021986, valid loss:0.009739
Epoch:32, Train loss:0.020557, valid loss:0.008952
Epoch:33, Train loss:0.018560, valid loss:0.008308
Epoch:34, Train loss:0.016340, valid loss:0.007360
Epoch:35, Train loss:0.014641, valid loss:0.006732
Epoch:36, Train loss:0.013405, valid loss:0.006249
Epoch:37, Train loss:0.012313, valid loss:0.005699
Epoch:38, Train loss:0.011314, valid loss:0.005403
Epoch:39, Train loss:0.010376, valid loss:0.005279
Epoch:40, Train loss:0.009543, valid loss:0.004651
Epoch:41, Train loss:0.008716, valid loss:0.004533
Epoch:42, Train loss:0.008362, valid loss:0.004205
Epoch:43, Train loss:0.008000, valid loss:0.003940
Epoch:44, Train loss:0.007486, valid loss:0.003557
Epoch:45, Train loss:0.007130, valid loss:0.003515
Epoch:46, Train loss:0.006848, valid loss:0.003269
Epoch:47, Train loss:0.006618, valid loss:0.003243
Epoch:48, Train loss:0.006390, valid loss:0.003159
Epoch:49, Train loss:0.006203, valid loss:0.003161
Epoch:50, Train loss:0.006018, valid loss:0.002931
Epoch:51, Train loss:0.005791, valid loss:0.002912
Epoch:52, Train loss:0.005708, valid loss:0.002916
Epoch:53, Train loss:0.005631, valid loss:0.002937
Epoch:54, Train loss:0.005561, valid loss:0.002811
Epoch:55, Train loss:0.005486, valid loss:0.002866
Epoch:56, Train loss:0.005417, valid loss:0.002759
Epoch:57, Train loss:0.005345, valid loss:0.002666
Epoch:58, Train loss:0.005277, valid loss:0.002702
Epoch:59, Train loss:0.005198, valid loss:0.002677
Epoch:60, Train loss:0.005144, valid loss:0.002719
training time 5235.204292535782
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.03992518943323136
plot_id,batch_id 0 1 miss% 0.031346263243462416
plot_id,batch_id 0 2 miss% 0.057281066399234006
plot_id,batch_id 0 3 miss% 0.053894086838774447
plot_id,batch_id 0 4 miss% 0.058617519989449196
plot_id,batch_id 0 5 miss% 0.05136244492463356
plot_id,batch_id 0 6 miss% 0.06469866026616439
plot_id,batch_id 0 7 miss% 0.06296159636875456
plot_id,batch_id 0 8 miss% 0.06616005284956523
plot_id,batch_id 0 9 miss% 0.05118988715304664
plot_id,batch_id 0 10 miss% 0.0622552802907829
plot_id,batch_id 0 11 miss% 0.044164060302032786
plot_id,batch_id 0 12 miss% 0.062445008567748844
plot_id,batch_id 0 13 miss% 0.07627338560501336
plot_id,batch_id 0 14 miss% 0.0810041553613627
plot_id,batch_id 0 15 miss% 0.08851165343918611
plot_id,batch_id 0 16 miss% 0.06532741149027162
plot_id,batch_id 0 17 miss% 0.07122336366781887
plot_id,batch_id 0 18 miss% 0.07411565251899373
plot_id,batch_id 0 19 miss% 0.08296348176768263
plot_id,batch_id 0 20 miss% 0.05988357925019666
plot_id,batch_id 0 21 miss% 0.06324613017558241
plot_id,batch_id 0 22 miss% 0.07139205270462953
plot_id,batch_id 0 23 miss% 0.05342977211122627
plot_id,batch_id 0 24 miss% 0.05957702224590486
plot_id,batch_id 0 25 miss% 0.10048719283838455
plot_id,batch_id 0 26 miss% 0.06097475857796261
plot_id,batch_id 0 27 miss% 0.05691029685049592
plot_id,batch_id 0 28 miss% 0.05061143891314715
plot_id,batch_id 0 29 miss% 0.04826554328406337
plot_id,batch_id 0 30 miss% 0.07337533945620721
plot_id,batch_id 0 31 miss% 0.0636482682339305
plot_id,batch_id 0 32 miss% 0.06347258568544481
plot_id,batch_id 0 33 miss% 0.054044002155571
plot_id,batch_id 0 34 miss% 0.047172263386592674
plot_id,batch_id 0 35 miss% 0.11622910845954436
plot_id,batch_id 0 36 miss% 0.08123543410872035
plot_id,batch_id 0 37 miss% 0.0630146009183211
plot_id,batch_id 0 38 miss% 0.05578805638482678
plot_id,batch_id 0 39 miss% 0.048932739350364136
plot_id,batch_id 0 40 miss% 0.14274370897471608
plot_id,batch_id 0 41 miss% 0.050982646507366294
plot_id,batch_id 0 42 miss% 0.042412918927376066
plot_id,batch_id 0 43 miss% 0.05934994954469434
plot_id,batch_id 0 44 miss% 0.028075038318977494
plot_id,batch_id 0 45 miss% 0.05685467188199113
plot_id,batch_id 0 46 miss% 0.04469383265942252
plot_id,batch_id 0 47 miss% 0.04707726015635866
plot_id,batch_id 0 48 miss% 0.037021523887100324
plot_id,batch_id 0 49 miss% 0.032241565705006574
plot_id,batch_id 0 50 miss% 0.0483741867721495
plot_id,batch_id 0 51 miss% 0.049367310655697064
plot_id,batch_id 0 52 miss% 0.043779817747381294
plot_id,batch_id 0 53 miss% 0.030202748979348076
plot_id,batch_id 0 54 miss% 0.041197545331271436
plot_id,batch_id 0 55 miss% 0.09116527881192198
plot_id,batch_id 0 56 miss% 0.05721856466786641
plot_id,batch_id 0 57 miss% 0.04688303058381015
plot_id,batch_id 0 58 miss% 0.03395513838632967
plot_id,batch_id 0 59 miss% 0.03984239186593822
plot_id,batch_id 0 60 miss% 0.06462149334441691
plot_id,batch_id 0 61 miss% 0.043489972404315486
plot_id,batch_id 0 62 miss% 0.055743539846437914
plot_id,batch_id 0 63 miss% 0.04932171684549201
plot_id,batch_id 0 64 miss% 0.08344818031229223
plot_id,batch_id 0 65 miss% 0.0858103076937919
plot_id,batch_id 0 66 miss% 0.10622267011527521
plot_id,batch_id 0 67 miss% 0.04600163427656246
plot_id,batch_id 0 68 miss% 0.07989908736873469
plot_id,batch_id 0 69 miss% plot_id,batch_id 0 70 miss% 0.029556048465107357
plot_id,batch_id 0 71 miss% 0.04865205858869681
plot_id,batch_id 0 72 miss% 0.04062956641673013
plot_id,batch_id 0 73 miss% 0.03917160504380488
plot_id,batch_id 0 74 miss% 0.029158018813463065
plot_id,batch_id 0 75 miss% 0.05958353774817569
plot_id,batch_id 0 76 miss% 0.04276446249711248
plot_id,batch_id 0 77 miss% 0.05174986618013023
plot_id,batch_id 0 78 miss% 0.040438483408422585
plot_id,batch_id 0 79 miss% 0.04601487861030361
plot_id,batch_id 0 80 miss% 0.04132117534948772
plot_id,batch_id 0 81 miss% 0.029726814628098213
plot_id,batch_id 0 82 miss% 0.03730167515805815
plot_id,batch_id 0 83 miss% 0.034585944616551366
plot_id,batch_id 0 84 miss% 0.04091235181739114
plot_id,batch_id 0 85 miss% 0.042558869943767
plot_id,batch_id 0 86 miss% 0.029383119360934268
plot_id,batch_id 0 87 miss% 0.04311461441743999
plot_id,batch_id 0 88 miss% 0.04628109162636028
plot_id,batch_id 0 89 miss% 0.04052772201816209
plot_id,batch_id 0 90 miss% 0.07531162511261953
plot_id,batch_id 0 91 miss% 0.04711432510253666
plot_id,batch_id 0 92 miss% 0.05047913255581505
plot_id,batch_id 0 93 miss% 0.046688517610041655
plot_id,batch_id 0 94 miss% 0.056509936082350776
plot_id,batch_id 0 95 miss% 0.07609676813370357
plot_id,batch_id 0 96 miss% 0.04703162583724036
plot_id,batch_id 0 97 miss% 0.04468018429890398
plot_id,batch_id 0 98 miss% 0.039358291542476664
plot_id,batch_id 0 99 miss% 0.03445831031277956
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0423768  0.035259   0.02398468 0.02699404 0.01969505 0.0375425
 0.02818135 0.02514349 0.02471548 0.02562703 0.05252473 0.03443441
 0.02408556 0.03065881 0.04650737 0.03713704 0.03438201 0.03683733
 0.03226253 0.05435682 0.05934311 0.03141153 0.033564   0.03840235
 0.02941046 0.05464523 0.02890267 0.04136174 0.03431707 0.02316075
 0.04304408 0.03266096 0.03697339 0.02941559 0.02703898 0.04963931
 0.06464976 0.03549454 0.03545274 0.03241418 0.05348058 0.03054596
 0.02403181 0.02417523 0.02366784 0.04679334 0.02281232 0.02673063
 0.0315548  0.02827304 0.04425822 0.02718021 0.02939825 0.03276951
 0.02960359 0.05887209 0.03161065 0.03972567 0.02088597 0.03156621
 0.03620778 0.04379228 0.03993751 0.04337378 0.05146624 0.05627967
 0.02527672 0.03463049 0.03992031 0.03152997 0.02955605 0.04865206
 0.04062957 0.03917161 0.02915802 0.05958354 0.04276446 0.05174987
 0.04043848 0.04601488 0.04132118 0.02972681 0.03730168 0.03458594
 0.04091235 0.04255887 0.02938312 0.04311461 0.04628109 0.04052772
 0.07531163 0.04711433 0.05047913 0.04668852 0.05650994 0.07609677
 0.04703163 0.04468018 0.03935829 0.03445831]
for model  19 the mean error 0.03825519777170922
all id 19 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 21 out win 4 err 0.03825519777170922
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  21969
Epoch:0, Train loss:0.537238, valid loss:0.504722
Epoch:1, Train loss:0.360394, valid loss:0.362380
Epoch:2, Train loss:0.347987, valid loss:0.360589
Epoch:3, Train loss:0.346204, valid loss:0.359998
Epoch:4, Train loss:0.345493, valid loss:0.359944
Epoch:5, Train loss:0.344812, valid loss:0.359636
Epoch:6, Train loss:0.344345, valid loss:0.359477
Epoch:7, Train loss:0.344080, valid loss:0.359542
Epoch:8, Train loss:0.343866, valid loss:0.359309
Epoch:9, Train loss:0.343713, valid loss:0.359087
Epoch:10, Train loss:0.343544, valid loss:0.358918
Epoch:11, Train loss:0.342758, valid loss:0.358648
Epoch:12, Train loss:0.342678, valid loss:0.358599
Epoch:13, Train loss:0.342651, valid loss:0.358738
Epoch:14, Train loss:0.342589, valid loss:0.358570
Epoch:15, Train loss:0.342567, valid loss:0.358658
Epoch:16, Train loss:0.342457, valid loss:0.358609
Epoch:17, Train loss:0.342432, valid loss:0.358449
Epoch:18, Train loss:0.342425, valid loss:0.358516
Epoch:19, Train loss:0.342375, valid loss:0.358564
Epoch:20, Train loss:0.342343, valid loss:0.358507
Epoch:21, Train loss:0.341946, valid loss:0.358326
Epoch:22, Train loss:0.341947, valid loss:0.358395
Epoch:23, Train loss:0.341904, valid loss:0.358296
Epoch:24, Train loss:0.341904, valid loss:0.358316
Epoch:25, Train loss:0.341893, valid loss:0.358279
Epoch:26, Train loss:0.341857, valid loss:0.358291
Epoch:27, Train loss:0.341835, valid loss:0.358297
Epoch:28, Train loss:0.341833, valid loss:0.358310
Epoch:29, Train loss:0.341803, valid loss:0.358285
Epoch:30, Train loss:0.341807, valid loss:0.358326
Epoch:31, Train loss:0.341606, valid loss:0.358185
Epoch:32, Train loss:0.341600, valid loss:0.358174
Epoch:33, Train loss:0.341611, valid loss:0.358208
Epoch:34, Train loss:0.341585, valid loss:0.358111
Epoch:35, Train loss:0.341580, valid loss:0.358160
Epoch:36, Train loss:0.341551, valid loss:0.358132
Epoch:37, Train loss:0.341567, valid loss:0.358144
Epoch:38, Train loss:0.341545, valid loss:0.358154
Epoch:39, Train loss:0.341538, valid loss:0.358157
Epoch:40, Train loss:0.341540, valid loss:0.358191
Epoch:41, Train loss:0.341443, valid loss:0.358103
Epoch:42, Train loss:0.341441, valid loss:0.358087
Epoch:43, Train loss:0.341429, valid loss:0.358075
Epoch:44, Train loss:0.341421, valid loss:0.358074
Epoch:45, Train loss:0.341420, valid loss:0.358094
Epoch:46, Train loss:0.341423, valid loss:0.358070
Epoch:47, Train loss:0.341421, valid loss:0.358062
Epoch:48, Train loss:0.341412, valid loss:0.358069
Epoch:49, Train loss:0.341395, valid loss:0.358095
Epoch:50, Train loss:0.341392, valid loss:0.358116
Epoch:51, Train loss:0.341351, valid loss:0.358063
Epoch:52, Train loss:0.341348, valid loss:0.358043
Epoch:53, Train loss:0.341343, valid loss:0.358064
Epoch:54, Train loss:0.341343, valid loss:0.358059
Epoch:55, Train loss:0.341345, valid loss:0.358060
Epoch:56, Train loss:0.341340, valid loss:0.358048
Epoch:57, Train loss:0.341341, valid loss:0.358057
Epoch:58, Train loss:0.341332, valid loss:0.358068
Epoch:59, Train loss:0.341328, valid loss:0.358052
Epoch:60, Train loss:0.341331, valid loss:0.358052
training time 5259.550857782364
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7867255262450434
plot_id,batch_id 0 1 miss% 0.8302926839654665
plot_id,batch_id 0 2 miss% 0.8358990834118585
plot_id,batch_id 0 3 miss% 0.8404953274750878
plot_id,batch_id 0 4 miss% 0.8446698640640918
plot_id,batch_id 0 5 miss% 0.7761002757747232
plot_id,batch_id 0 6 miss% 0.8260985844623484
plot_id,batch_id 0 7 miss% 0.8350722246726701
plot_id,batch_id 0 8 miss% 0.8418462103726876
plot_id,batch_id 0 9 miss% 0.8482200007947441
plot_id,batch_id 0 10 miss% 0.7622167058985991
plot_id,batch_id 0 11 miss% 0.825748990764151
plot_id,batch_id 0 12 miss% 0.8337456441560952
plot_id,batch_id 0 13 miss% 0.842363127910541
plot_id,batch_id 0 14 miss% 0.8428330741672398
plot_id,batch_id 0 15 miss% 0.7747568745301298
plot_id,batch_id 0 16 miss% 0.824780467109261
plot_id,batch_id 0 17 miss% 0.8316761648658415
plot_id,batch_id 0 18 miss% 0.8445784812082002
plot_id,batch_id 0 19 miss% 0.8451095136193445
plot_id,batch_id 0 20 miss% 0.8102544098966539
plot_id,batch_id 0 21 miss% 0.8357695846284614
plot_id,batch_id 0 22 miss% 0.8398797366295867
plot_id,batch_id 0 23 miss% 0.8433863223524198
plot_id,batch_id 0 24 miss% 0.844747902098535
plot_id,batch_id 0 25 miss% 0.8047344135989098
plot_id,batch_id 0 26 miss% 0.8350224191746525
plot_id,batch_id 0 27 miss% 0.8397540674738203
plot_id,batch_id 0 28 miss% 0.8429005033192488
plot_id,batch_id 0 29 miss% 0.8459226192718794
plot_id,batch_id 0 30 miss% 0.7924367110085159
plot_id,batch_id 0 31 miss% 0.8335664966771618
plot_id,batch_id 0 32 miss% 0.8408594236839471
plot_id,batch_id 0 33 miss% 0.8447048662698284
plot_id,batch_id 0 34 miss% 0.8436050119916263
plot_id,batch_id 0 35 miss% 0.7943289344520061
plot_id,batch_id 0 36 miss% 0.836607066569793
plot_id,batch_id 0 37 miss% 0.836366498802737
plot_id,batch_id 0 38 miss% 0.8439354436767045
plot_id,batch_id 0 39 miss% 0.8462039812765453
plot_id,batch_id 0 40 miss% 0.8249229939088594
plot_id,batch_id 0 41 miss% 0.8410037554537352
plot_id,batch_id 0 42 miss% 0.8412713215487145
plot_id,batch_id 0 43 miss% 0.8464845969730338
plot_id,batch_id 0 44 miss% 0.8486930566956064
plot_id,batch_id 0 45 miss% 0.8177921164269076
plot_id,batch_id 0 46 miss% 0.8412171113209063
plot_id,batch_id 0 47 miss% 0.8421460143643995
plot_id,batch_id 0 48 miss% 0.8467262178051923
plot_id,batch_id 0 49 miss% 0.848461233800644
plot_id,batch_id 0 50 miss% 0.8265353101091307
plot_id,batch_id 0 51 miss% 0.839532285312524
plot_id,batch_id 0 52 miss% 0.8422029686778624
plot_id,batch_id 0 53 miss% 0.8452870038396277
plot_id,batch_id 0 54 miss% 0.8481209333748347
plot_id,batch_id 0 55 miss% 0.8282348815075419
plot_id,batch_id 0 56 miss% 0.8396241902990248
plot_id,batch_id 0 57 miss% 0.8433466745730224
plot_id,batch_id 0 58 miss% 0.8471895456605242
plot_id,batch_id 0 59 miss% 0.8455566260329296
plot_id,batch_id 0 60 miss% 0.7189279101030468
plot_id,batch_id 0 61 miss% 0.8105590382571674
plot_id,batch_id 0 62 miss% 0.8251217066271627
plot_id,batch_id 0 63 miss% 0.8321005183620707
plot_id,batch_id 0 64 miss% 0.8369469035883815
plot_id,batch_id 0 65 miss% 0.7213554229677992
plot_id,batch_id 0 66 miss% 0.8021861940964331
plot_id,batch_id 0 67 miss% 0.8136372239691808
plot_id,batch_id 0 68 miss% 0.8304076898626228
plot_id,batch_id 0 69 miss% 0.8302595582896883
plot_id,batch_id 0 70 miss% 0.6850185508522297
plot_id,batch_id 0 71 miss% Launcher: Job 20 completed in 5440 seconds.
Launcher: Task 70 done. Exiting.
0.8100559423547039
plot_id,batch_id 0 72 miss% 0.8124270467722868
plot_id,batch_id 0 73 miss% 0.817449944648087
plot_id,batch_id 0 74 miss% 0.8291802190351174
plot_id,batch_id 0 75 miss% 0.6903683257719628
plot_id,batch_id 0 76 miss% 0.7897932026976663
plot_id,batch_id 0 77 miss% 0.8016390713830339
plot_id,batch_id 0 78 miss% 0.8152464755099746
plot_id,batch_id 0 79 miss% 0.8230131880479385
plot_id,batch_id 0 80 miss% 0.7473675816229914
plot_id,batch_id 0 81 miss% 0.8190983100428267
plot_id,batch_id 0 82 miss% 0.8300845162654696
plot_id,batch_id 0 83 miss% 0.8373626132320997
plot_id,batch_id 0 84 miss% 0.8389945725128092
plot_id,batch_id 0 85 miss% 0.7446896047784557
plot_id,batch_id 0 86 miss% 0.8126995418099767
plot_id,batch_id 0 87 miss% 0.826539629987662
plot_id,batch_id 0 88 miss% 0.8364013214709861
plot_id,batch_id 0 89 miss% 0.8339096044757272
plot_id,batch_id 0 90 miss% 0.7061968932552631
plot_id,batch_id 0 91 miss% 0.8053870263870965
plot_id,batch_id 0 92 miss% 0.8245371501629053
plot_id,batch_id 0 93 miss% 0.8274191536084011
plot_id,batch_id 0 94 miss% 0.8383115639101931
plot_id,batch_id 0 95 miss% 0.7223851877274104
plot_id,batch_id 0 96 miss% 0.8000523804535531
plot_id,batch_id 0 97 miss% 0.8165156419031946
plot_id,batch_id 0 98 miss% 0.8251406334862268
plot_id,batch_id 0 99 miss% 0.8301943907920978
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.78672553 0.83029268 0.83589908 0.84049533 0.84466986 0.77610028
 0.82609858 0.83507222 0.84184621 0.84822    0.76221671 0.82574899
 0.83374564 0.84236313 0.84283307 0.77475687 0.82478047 0.83167616
 0.84457848 0.84510951 0.81025441 0.83576958 0.83987974 0.84338632
 0.8447479  0.80473441 0.83502242 0.83975407 0.8429005  0.84592262
 0.79243671 0.8335665  0.84085942 0.84470487 0.84360501 0.79432893
 0.83660707 0.8363665  0.84393544 0.84620398 0.82492299 0.84100376
 0.84127132 0.8464846  0.84869306 0.81779212 0.84121711 0.84214601
 0.84672622 0.84846123 0.82653531 0.83953229 0.84220297 0.845287
 0.84812093 0.82823488 0.83962419 0.84334667 0.84718955 0.84555663
 0.71892791 0.81055904 0.82512171 0.83210052 0.8369469  0.72135542
 0.80218619 0.81363722 0.83040769 0.83025956 0.68501855 0.81005594
 0.81242705 0.81744994 0.82918022 0.69036833 0.7897932  0.80163907
 0.81524648 0.82301319 0.74736758 0.81909831 0.83008452 0.83736261
 0.83899457 0.7446896  0.81269954 0.82653963 0.83640132 0.8339096
 0.70619689 0.80538703 0.82453715 0.82741915 0.83831156 0.72238519
 0.80005238 0.81651564 0.82514063 0.83019439]
for model  83 the mean error 0.8197154552708809
all id 83 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 25 out win 5 err 0.8197154552708809
Launcher: Job 84 completed in 5440 seconds.
Launcher: Task 98 done. Exiting.
0.07121206589124289
plot_id,batch_id 0 70 miss% 0.061824220500662073
plot_id,batch_id 0 71 miss% 0.08276645272043058
plot_id,batch_id 0 72 miss% 0.044319010945546626
plot_id,batch_id 0 73 miss% 0.0682132718855975
plot_id,batch_id 0 74 miss% 0.08628099962456781
plot_id,batch_id 0 75 miss% 0.06648020909906031
plot_id,batch_id 0 76 miss% 0.07819937234255131
plot_id,batch_id 0 77 miss% 0.059599175842660786
plot_id,batch_id 0 78 miss% 0.08254228288862148
plot_id,batch_id 0 79 miss% 0.08668974497816309
plot_id,batch_id 0 80 miss% 0.0721442928747586
plot_id,batch_id 0 81 miss% 0.038169012133726574
plot_id,batch_id 0 82 miss% 0.050287806633946754
plot_id,batch_id 0 83 miss% 0.062270941860065966
plot_id,batch_id 0 84 miss% 0.06091488562877613
plot_id,batch_id 0 85 miss% 0.06810111460336107
plot_id,batch_id 0 86 miss% 0.0464515486764594
plot_id,batch_id 0 87 miss% 0.05114446115720147
plot_id,batch_id 0 88 miss% 0.06226338688309445
plot_id,batch_id 0 89 miss% 0.06886640374395482
plot_id,batch_id 0 90 miss% 0.0627384208448464
plot_id,batch_id 0 91 miss% 0.05933026691412921
plot_id,batch_id 0 92 miss% 0.08412141412829777
plot_id,batch_id 0 93 miss% 0.10690040449063026
plot_id,batch_id 0 94 miss% 0.060341888578227414
plot_id,batch_id 0 95 miss% 0.05675797692020918
plot_id,batch_id 0 96 miss% 0.07360525115909772
plot_id,batch_id 0 97 miss% 0.10498130888355416
plot_id,batch_id 0 98 miss% 0.07970692051062196
plot_id,batch_id 0 99 miss% 0.07944359145474647
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03992519 0.03134626 0.05728107 0.05389409 0.05861752 0.05136244
 0.06469866 0.0629616  0.06616005 0.05118989 0.06225528 0.04416406
 0.06244501 0.07627339 0.08100416 0.08851165 0.06532741 0.07122336
 0.07411565 0.08296348 0.05988358 0.06324613 0.07139205 0.05342977
 0.05957702 0.10048719 0.06097476 0.0569103  0.05061144 0.04826554
 0.07337534 0.06364827 0.06347259 0.054044   0.04717226 0.11622911
 0.08123543 0.0630146  0.05578806 0.04893274 0.14274371 0.05098265
 0.04241292 0.05934995 0.02807504 0.05685467 0.04469383 0.04707726
 0.03702152 0.03224157 0.04837419 0.04936731 0.04377982 0.03020275
 0.04119755 0.09116528 0.05721856 0.04688303 0.03395514 0.03984239
 0.06462149 0.04348997 0.05574354 0.04932172 0.08344818 0.08581031
 0.10622267 0.04600163 0.07989909 0.07121207 0.06182422 0.08276645
 0.04431901 0.06821327 0.086281   0.06648021 0.07819937 0.05959918
 0.08254228 0.08668974 0.07214429 0.03816901 0.05028781 0.06227094
 0.06091489 0.06810111 0.04645155 0.05114446 0.06226339 0.0688664
 0.06273842 0.05933027 0.08412141 0.1069004  0.06034189 0.05675798
 0.07360525 0.10498131 0.07970692 0.07944359]
for model  137 the mean error 0.06312076242939178
all id 137 hidden_dim 16 learning_rate 0.02 num_layers 3 frames 25 out win 5 err 0.06312076242939178
Launcher: Job 138 completed in 5445 seconds.
Launcher: Task 83 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  61841
Epoch:0, Train loss:0.759093, valid loss:0.763515
Epoch:1, Train loss:0.305390, valid loss:0.006746
Epoch:2, Train loss:0.011512, valid loss:0.005131
Epoch:3, Train loss:0.008963, valid loss:0.004662
Epoch:4, Train loss:0.006921, valid loss:0.003565
Epoch:5, Train loss:0.005374, valid loss:0.003247
Epoch:6, Train loss:0.005015, valid loss:0.002454
Epoch:7, Train loss:0.004508, valid loss:0.003019
Epoch:8, Train loss:0.004312, valid loss:0.002199
Epoch:9, Train loss:0.003924, valid loss:0.002806
Epoch:10, Train loss:0.003917, valid loss:0.002351
Epoch:11, Train loss:0.002692, valid loss:0.001848
Epoch:12, Train loss:0.002582, valid loss:0.001638
Epoch:13, Train loss:0.002624, valid loss:0.001964
Epoch:14, Train loss:0.002542, valid loss:0.001696
Epoch:15, Train loss:0.002529, valid loss:0.001838
Epoch:16, Train loss:0.002437, valid loss:0.001641
Epoch:17, Train loss:0.002440, valid loss:0.001329
Epoch:18, Train loss:0.002402, valid loss:0.001739
Epoch:19, Train loss:0.002468, valid loss:0.001512
Epoch:20, Train loss:0.002305, valid loss:0.001774
Epoch:21, Train loss:0.001715, valid loss:0.001117
Epoch:22, Train loss:0.001720, valid loss:0.001234
Epoch:23, Train loss:0.001716, valid loss:0.001069
Epoch:24, Train loss:0.001714, valid loss:0.001332
Epoch:25, Train loss:0.001673, valid loss:0.001042
Epoch:26, Train loss:0.001715, valid loss:0.001204
Epoch:27, Train loss:0.001658, valid loss:0.001086
Epoch:28, Train loss:0.001622, valid loss:0.001034
Epoch:29, Train loss:0.001671, valid loss:0.001073
Epoch:30, Train loss:0.001613, valid loss:0.001109
Epoch:31, Train loss:0.001331, valid loss:0.001063
Epoch:32, Train loss:0.001318, valid loss:0.000956
Epoch:33, Train loss:0.001330, valid loss:0.000886
Epoch:34, Train loss:0.001344, valid loss:0.000959
Epoch:35, Train loss:0.001325, valid loss:0.000912
Epoch:36, Train loss:0.001284, valid loss:0.000917
Epoch:37, Train loss:0.001286, valid loss:0.000920
Epoch:38, Train loss:0.001294, valid loss:0.000862
Epoch:39, Train loss:0.001274, valid loss:0.000916
Epoch:40, Train loss:0.001261, valid loss:0.000964
Epoch:41, Train loss:0.001145, valid loss:0.000819
Epoch:42, Train loss:0.001140, valid loss:0.000835
Epoch:43, Train loss:0.001139, valid loss:0.000859
Epoch:44, Train loss:0.001146, valid loss:0.000830
Epoch:45, Train loss:0.001130, valid loss:0.000871
Epoch:46, Train loss:0.001127, valid loss:0.000866
Epoch:47, Train loss:0.001128, valid loss:0.000829
Epoch:48, Train loss:0.001116, valid loss:0.000906
Epoch:49, Train loss:0.001130, valid loss:0.000851
Epoch:50, Train loss:0.001120, valid loss:0.000857
Epoch:51, Train loss:0.001056, valid loss:0.000796
Epoch:52, Train loss:0.001055, valid loss:0.000814
Epoch:53, Train loss:0.001052, valid loss:0.000825
Epoch:54, Train loss:0.001056, valid loss:0.000807
Epoch:55, Train loss:0.001048, valid loss:0.000815
Epoch:56, Train loss:0.001051, valid loss:0.000801
Epoch:57, Train loss:0.001043, valid loss:0.000838
Epoch:58, Train loss:0.001045, valid loss:0.000782
Epoch:59, Train loss:0.001044, valid loss:0.000819
Epoch:60, Train loss:0.001041, valid loss:0.000779
training time 5264.140006065369
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.036217511045332655
plot_id,batch_id 0 1 miss% 0.031001669457750517
plot_id,batch_id 0 2 miss% 0.03156049318222576
plot_id,batch_id 0 3 miss% 0.025810487910930567
plot_id,batch_id 0 4 miss% 0.023303752248236345
plot_id,batch_id 0 5 miss% 0.03337842759070257
plot_id,batch_id 0 6 miss% 0.02310242163730523
plot_id,batch_id 0 7 miss% 0.024311693700732835
plot_id,batch_id 0 8 miss% 0.021072958086746148
plot_id,batch_id 0 9 miss% 0.03743900348032328
plot_id,batch_id 0 10 miss% 0.03007910634993596
plot_id,batch_id 0 11 miss% 0.044467175357365836
plot_id,batch_id 0 12 miss% 0.02458217732408886
plot_id,batch_id 0 13 miss% 0.02006031047791063
plot_id,batch_id 0 14 miss% 0.02455311324271174
plot_id,batch_id 0 15 miss% 0.051859010319363116
plot_id,batch_id 0 16 miss% 0.02655185698022646
plot_id,batch_id 0 17 miss% 0.03811306339938253
plot_id,batch_id 0 18 miss% 0.026062429939926152
plot_id,batch_id 0 19 miss% 0.030972809192594607
plot_id,batch_id 0 20 miss% 0.04871141025244935
plot_id,batch_id 0 21 miss% 0.028721992814541703
plot_id,batch_id 0 22 miss% 0.027207886245693605
plot_id,batch_id 0 23 miss% 0.034323367238689055
plot_id,batch_id 0 24 miss% 0.031180073732814187
plot_id,batch_id 0 25 miss% 0.030033447520513488
plot_id,batch_id 0 26 miss% 0.02368073078179426
plot_id,batch_id 0 27 miss% 0.027776622331677794
plot_id,batch_id 0 28 miss% 0.030690012412863867
plot_id,batch_id 0 29 miss% 0.021282147051581875
plot_id,batch_id 0 30 miss% 0.05619451607365191
plot_id,batch_id 0 31 miss% 0.033674232667231276
plot_id,batch_id 0 32 miss% 0.03165397867164588
plot_id,batch_id 0 33 miss% 0.024087713279643545
plot_id,batch_id 0 34 miss% 0.027933176383573358
plot_id,batch_id 0 35 miss% 0.03876548754938822
plot_id,batch_id 0 36 miss% 0.032407060964591165
plot_id,batch_id 0 37 miss% 0.03370592343857725
plot_id,batch_id 0 38 miss% 0.02370531745982613
plot_id,batch_id 0 39 miss% 0.027957011475432236
plot_id,batch_id 0 40 miss% 0.0634799533201647
plot_id,batch_id 0 41 miss% 0.013798134486170302
plot_id,batch_id 0 42 miss% 0.021607758399726253
plot_id,batch_id 0 43 miss% 0.028554970569462577
plot_id,batch_id 0 44 miss% 0.029519383770870895
plot_id,batch_id 0 45 miss% 0.028235212573852857
plot_id,batch_id 0 46 miss% 0.01974183195236993
plot_id,batch_id 0 47 miss% 0.018132456660211436
plot_id,batch_id 0 48 miss% 0.029819091498925997
plot_id,batch_id 0 49 miss% 0.026315260687443286
plot_id,batch_id 0 50 miss% 0.02617793823105177
plot_id,batch_id 0 51 miss% 0.01729372467654159
plot_id,batch_id 0 52 miss% 0.01823828187151543
plot_id,batch_id 0 53 miss% 0.0196961267624366
plot_id,batch_id 0 54 miss% 0.03134342730932665
plot_id,batch_id 0 55 miss% 0.026595922095123393
plot_id,batch_id 0 56 miss% 0.03731852977238964
plot_id,batch_id 0 57 miss% 0.03269160562069945
plot_id,batch_id 0 58 miss% 0.02549377199796695
plot_id,batch_id 0 59 miss% 0.027543168047173917
plot_id,batch_id 0 60 miss% 0.04533943167805118
plot_id,batch_id 0 61 miss% 0.022391952891140137
plot_id,batch_id 0 62 miss% 0.03118985666772112
plot_id,batch_id 0 63 miss% 0.03751761002615103
plot_id,batch_id 0 64 miss% 0.028929034890240964
plot_id,batch_id 0 65 miss% 0.037737124885763966
plot_id,batch_id 0 66 miss% 0.02853881688035372
plot_id,batch_id 0 67 miss% 0.023614314404389155
plot_id,batch_id 0 68 miss% 0.024179946122076218
plot_id,batch_id 0 69 miss% 0.02197765488395299
plot_id,batch_id 0 70 miss% 0.04610196132116041
plot_id,batch_id 0 71 miss% 0.03478381525220851
plot_id,batch_id 0 72 miss% 0.03725896838833226
plot_id,batch_id 0 73 miss% 0.023498502311875172
plot_id,batch_id 0 74 miss% 0.03598597154102418
plot_id,batch_id 0 75 miss% 0.04485826747698713
plot_id,batch_id 0 76 miss% 0.05533520155280165
plot_id,batch_id 0 77 miss% 0.04365697637174526
plot_id,batch_id 0 78 miss% 0.0371001858151298
plot_id,batch_id 0 79 miss% 0.0473104253232693
plot_id,batch_id 0 80 miss% 0.025295885471164837
plot_id,batch_id 0 81 miss% 0.024702398953838828
plot_id,batch_id 0 82 miss% 0.030001195792736794
plot_id,batch_id 0 83 miss% 0.020664419292819414
plot_id,batch_id 0 84 miss% 0.028405542631679357
plot_id,batch_id 0 85 miss% 0.0362147122446858
plot_id,batch_id 0 86 miss% 0.028733001686774385
plot_id,batch_id 0 87 miss% 0.023847542090835
plot_id,batch_id 0 88 miss% 0.027405305569599102
plot_id,batch_id 0 89 miss% 0.03766345982722522
plot_id,batch_id 0 90 miss% 0.037682074335354525
plot_id,batch_id 0 91 miss% 0.03385406381444613
plot_id,batch_id 0 92 miss% 0.03261081901581917
plot_id,batch_id 0 93 miss% 0.027447594206285803
plot_id,batch_id 0 94 miss% 0.037475221859865994
plot_id,batch_id 0 95 miss% 0.04309166294527818
plot_id,batch_id 0 96 miss% 0.031200079116249307
plot_id,batch_id 0 97 miss% 0.05488528558611462
plot_id,batch_id 0 98 miss% 0.030592505930564253
plot_id,batch_id 0 99 miss% 0.030383659767770063
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03621751 0.03100167 0.03156049 0.02581049 0.02330375 0.03337843
 0.02310242 0.02431169 0.02107296 0.037439   0.03007911 0.04446718
 0.02458218 0.02006031 0.02455311 0.05185901 0.02655186 0.03811306
 0.02606243 0.03097281 0.04871141 0.02872199 0.02720789 0.03432337
 0.03118007 0.03003345 0.02368073 0.02777662 0.03069001 0.02128215
 0.05619452 0.03367423 0.03165398 0.02408771 0.02793318 0.03876549
 0.03240706 0.03370592 0.02370532 0.02795701 0.06347995 0.01379813
 0.02160776 0.02855497 0.02951938 0.02823521 0.01974183 0.01813246
 0.02981909 0.02631526 0.02617794 0.01729372 0.01823828 0.01969613
 0.03134343 0.02659592 0.03731853 0.03269161 0.02549377 0.02754317
 0.04533943 0.02239195 0.03118986 0.03751761 0.02892903 0.03773712
 0.02853882 0.02361431 0.02417995 0.02197765 0.04610196 0.03478382
 0.03725897 0.0234985  0.03598597 0.04485827 0.0553352  0.04365698
 0.03710019 0.04731043 0.02529589 0.0247024  0.0300012  0.02066442
 0.02840554 0.03621471 0.028733   0.02384754 0.02740531 0.03766346
 0.03768207 0.03385406 0.03261082 0.02744759 0.03747522 0.04309166
 0.03120008 0.05488529 0.03059251 0.03038366]
for model  39 the mean error 0.03129250578396876
all id 39 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 21 out win 3 err 0.03129250578396876
Launcher: Job 40 completed in 5467 seconds.
Launcher: Task 39 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  21969
Epoch:0, Train loss:0.437137, valid loss:0.400084
Epoch:1, Train loss:0.118088, valid loss:0.003661
Epoch:2, Train loss:0.005863, valid loss:0.003102
Epoch:3, Train loss:0.004651, valid loss:0.002381
Epoch:4, Train loss:0.003980, valid loss:0.002186
Epoch:5, Train loss:0.003615, valid loss:0.001832
Epoch:6, Train loss:0.003283, valid loss:0.001648
Epoch:7, Train loss:0.003072, valid loss:0.001713
Epoch:8, Train loss:0.002900, valid loss:0.001384
Epoch:9, Train loss:0.002752, valid loss:0.001460
Epoch:10, Train loss:0.002698, valid loss:0.001490
Epoch:11, Train loss:0.002085, valid loss:0.001447
Epoch:12, Train loss:0.002072, valid loss:0.001040
Epoch:13, Train loss:0.001994, valid loss:0.001175
Epoch:14, Train loss:0.001990, valid loss:0.001071
Epoch:15, Train loss:0.001909, valid loss:0.001044
Epoch:16, Train loss:0.001906, valid loss:0.001142
Epoch:17, Train loss:0.001873, valid loss:0.000980
Epoch:18, Train loss:0.001846, valid loss:0.000930
Epoch:19, Train loss:0.001828, valid loss:0.001379
Epoch:20, Train loss:0.001817, valid loss:0.000995
Epoch:21, Train loss:0.001542, valid loss:0.000949
Epoch:22, Train loss:0.001533, valid loss:0.000860
Epoch:23, Train loss:0.001533, valid loss:0.000869
Epoch:24, Train loss:0.001495, valid loss:0.000888
Epoch:25, Train loss:0.001514, valid loss:0.000858
Epoch:26, Train loss:0.001493, valid loss:0.000859
Epoch:27, Train loss:0.001481, valid loss:0.000856
Epoch:28, Train loss:0.001479, valid loss:0.000832
Epoch:29, Train loss:0.001450, valid loss:0.000800
Epoch:30, Train loss:0.001455, valid loss:0.000933
Epoch:31, Train loss:0.001319, valid loss:0.000795
Epoch:32, Train loss:0.001302, valid loss:0.000806
Epoch:33, Train loss:0.001317, valid loss:0.000907
Epoch:34, Train loss:0.001308, valid loss:0.000787
Epoch:35, Train loss:0.001294, valid loss:0.000767
Epoch:36, Train loss:0.001304, valid loss:0.000798
Epoch:37, Train loss:0.001272, valid loss:0.000809
Epoch:38, Train loss:0.001285, valid loss:0.000782
Epoch:39, Train loss:0.001271, valid loss:0.000748
Epoch:40, Train loss:0.001289, valid loss:0.000823
Epoch:41, Train loss:0.001197, valid loss:0.000732
Epoch:42, Train loss:0.001189, valid loss:0.000732
Epoch:43, Train loss:0.001198, valid loss:0.000771
Epoch:44, Train loss:0.001192, valid loss:0.000698
Epoch:45, Train loss:0.001188, valid loss:0.000722
Epoch:46, Train loss:0.001181, valid loss:0.000727
Epoch:47, Train loss:0.001188, valid loss:0.000711
Epoch:48, Train loss:0.001176, valid loss:0.000730
Epoch:49, Train loss:0.001168, valid loss:0.000691
Epoch:50, Train loss:0.001180, valid loss:0.000722
Epoch:51, Train loss:0.001138, valid loss:0.000712
Epoch:52, Train loss:0.001134, valid loss:0.000722
Epoch:53, Train loss:0.001134, valid loss:0.000718
Epoch:54, Train loss:0.001130, valid loss:0.000706
Epoch:55, Train loss:0.001131, valid loss:0.000720
Epoch:56, Train loss:0.001127, valid loss:0.000701
Epoch:57, Train loss:0.001132, valid loss:0.000705
Epoch:58, Train loss:0.001126, valid loss:0.000713
Epoch:59, Train loss:0.001127, valid loss:0.000726
Epoch:60, Train loss:0.001126, valid loss:0.000693
training time 5316.565308332443
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.03546341503718464
plot_id,batch_id 0 1 miss% 0.02607807731242288
plot_id,batch_id 0 2 miss% 0.023732514419278452
plot_id,batch_id 0 3 miss% 0.023339939270541607
plot_id,batch_id 0 4 miss% 0.03652748827080395
plot_id,batch_id 0 5 miss% 0.041459888419184646
plot_id,batch_id 0 6 miss% 0.03451100247921676
plot_id,batch_id 0 7 miss% 0.028945290743828157
plot_id,batch_id 0 8 miss% 0.03546625357094693
plot_id,batch_id 0 9 miss% 0.025448894425230142
plot_id,batch_id 0 10 miss% 0.023436136792881694
plot_id,batch_id 0 11 miss% 0.043395154233709765
plot_id,batch_id 0 12 miss% 0.034296672671333066
plot_id,batch_id 0 13 miss% 0.04119925150522013
plot_id,batch_id 0 14 miss% 0.036879291888526046
plot_id,batch_id 0 15 miss% 0.05501520053710868
plot_id,batch_id 0 16 miss% 0.02811136206381407
plot_id,batch_id 0 17 miss% 0.03382402582659338
plot_id,batch_id 0 18 miss% 0.034523225540947874
plot_id,batch_id 0 19 miss% 0.033989219243933895
plot_id,batch_id 0 20 miss% 0.0456925064384523
plot_id,batch_id 0 21 miss% 0.02379085074856318
plot_id,batch_id 0 22 miss% 0.025362202575613207
plot_id,batch_id 0 23 miss% 0.025943381483181975
plot_id,batch_id 0 24 miss% 0.024783325197229727
plot_id,batch_id 0 25 miss% 0.04439895227252217
plot_id,batch_id 0 26 miss% 0.02759143118332354
plot_id,batch_id 0 27 miss% 0.022563918649961878
plot_id,batch_id 0 28 miss% 0.02389072298133173
plot_id,batch_id 0 29 miss% 0.028850541242591465
plot_id,batch_id 0 30 miss% 0.1009787504844971
plot_id,batch_id 0 31 miss% 0.02838380398563881
plot_id,batch_id 0 32 miss% 0.03386860767767537
plot_id,batch_id 0 33 miss% 0.03892972076865005
plot_id,batch_id 0 34 miss% 0.03591243666430391
plot_id,batch_id 0 35 miss% 0.06576394686562645
plot_id,batch_id 0 36 miss% 0.044058037105667344
plot_id,batch_id 0 37 miss% 0.02741190988011044
plot_id,batch_id 0 38 miss% 0.03215613293061482
plot_id,batch_id 0 39 miss% 0.02799018655765469
plot_id,batch_id 0 40 miss% 0.08326647008898458
plot_id,batch_id 0 41 miss% 0.02529830270301875
plot_id,batch_id 0 42 miss% 0.017944722130751167
plot_id,batch_id 0 43 miss% 0.030398225777271394
plot_id,batch_id 0 44 miss% 0.029222170461967064
plot_id,batch_id 0 45 miss% 0.038750981724086184
plot_id,batch_id 0 46 miss% 0.030556735478364395
plot_id,batch_id 0 47 miss% 0.018611744661799857
plot_id,batch_id 0 48 miss% 0.026496508977353197
plot_id,batch_id 0 49 miss% 0.02490347397219747
plot_id,batch_id 0 50 miss% 0.032044157507646095
plot_id,batch_id 0 51 miss% 0.019117614943299277
plot_id,batch_id 0 52 miss% 0.01727252364749654
plot_id,batch_id 0 53 miss% 0.021447916737982502
plot_id,batch_id 0 54 miss% 0.03275114317072217
plot_id,batch_id 0 55 miss% 0.042391746299405675
plot_id,batch_id 0 56 miss% 0.033704151580341395
plot_id,batch_id 0 57 miss% 0.020183521664650438
plot_id,batch_id 0 58 miss% 0.029763207402614567
plot_id,batch_id 0 59 miss% 0.040786287584013634
plot_id,batch_id 0 60 miss% 0.0434652455954807
plot_id,batch_id 0 61 miss% 0.05231281346240475
plot_id,batch_id 0 62 miss% 0.0348937380694535
plot_id,batch_id 0 63 miss% 0.033636333868674345
plot_id,batch_id 0 64 miss% 0.02596200330294163
plot_id,batch_id 0 65 miss% 0.051103204027416485
plot_id,batch_id 0 66 miss% 0.04551789336284331
plot_id,batch_id 0 67 miss% 0.041353761719452084
plot_id,batch_id 0 68 miss% 0.03601978532223461
plot_id,batch_id 0 69 miss% 0.020454018599997636
plot_id,batch_id 0 70 miss% 0.03312082972129338
plot_id,batch_id 0 71 miss% 0.04484200088743101
plot_id,batch_id 0 72 miss% 0.029163247378918587
plot_id,batch_id 0 73 miss% 0.02247335901989814
plot_id,batch_id 0 74 miss% 0.03018506209292853
plot_id,batch_id 0 75 miss% 0.05168021159749087
plot_id,batch_id 0 76 miss% 0.042091569838841776
plot_id,batch_id 0 77 miss% 0.04018366386196036
plot_id,batch_id 0 78 miss% 0.05062060096040063
plot_id,batch_id 0 79 miss% 0.05140758180677578
plot_id,batch_id 0 80 miss% 0.04417077405050861
plot_id,batch_id 0 81 miss% 0.022771877423986883
plot_id,batch_id 0 82 miss% 0.024503429912746493
plot_id,batch_id 0 83 miss% 0.023367906832964948
plot_id,batch_id 0 84 miss% 0.029281077636831687
plot_id,batch_id 0 85 miss% 0.034700118631302276
plot_id,batch_id 0 86 miss% 0.0381963048042913
plot_id,batch_id 0 87 miss% 0.03066970344697042
plot_id,batch_id 0 88 miss% 0.026216718929114716
plot_id,batch_id 0 89 miss% 0.019470153332840967
plot_id,batch_id 0 90 miss% 0.03630435467277399
plot_id,batch_id 0 91 miss% 0.04260632520580143
plot_id,batch_id 0 92 miss% 0.029620814210579823
plot_id,batch_id 0 93 miss% 0.023880613894100632
plot_id,batch_id 0 94 miss% 0.02705363250654419
plot_id,batch_id 0 95 miss% 0.037604983089427446
plot_id,batch_id 0 96 miss% 0.033427598500320004
plot_id,batch_id 0 97 miss% 0.04373250951868946
plot_id,batch_id 0 98 miss% 0.03366895755076471
plot_id,batch_id 0 99 miss% 0.04257653711575582
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03546342 0.02607808 0.02373251 0.02333994 0.03652749 0.04145989
 0.034511   0.02894529 0.03546625 0.02544889 0.02343614 0.04339515
 0.03429667 0.04119925 0.03687929 0.0550152  0.02811136 0.03382403
 0.03452323 0.03398922 0.04569251 0.02379085 0.0253622  0.02594338
 0.02478333 0.04439895 0.02759143 0.02256392 0.02389072 0.02885054
 0.10097875 0.0283838  0.03386861 0.03892972 0.03591244 0.06576395
 0.04405804 0.02741191 0.03215613 0.02799019 0.08326647 0.0252983
 0.01794472 0.03039823 0.02922217 0.03875098 0.03055674 0.01861174
 0.02649651 0.02490347 0.03204416 0.01911761 0.01727252 0.02144792
 0.03275114 0.04239175 0.03370415 0.02018352 0.02976321 0.04078629
 0.04346525 0.05231281 0.03489374 0.03363633 0.025962   0.0511032
 0.04551789 0.04135376 0.03601979 0.02045402 0.03312083 0.044842
 0.02916325 0.02247336 0.03018506 0.05168021 0.04209157 0.04018366
 0.0506206  0.05140758 0.04417077 0.02277188 0.02450343 0.02336791
 0.02928108 0.03470012 0.0381963  0.0306697  0.02621672 0.01947015
 0.03630435 0.04260633 0.02962081 0.02388061 0.02705363 0.03760498
 0.0334276  0.04373251 0.03366896 0.04257654]
for model  163 the mean error 0.034431865922230374
all id 163 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 31 out win 4 err 0.034431865922230374
Launcher: Job 164 completed in 5513 seconds.
Launcher: Task 204 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  61841
Epoch:0, Train loss:0.759093, valid loss:0.763515
Epoch:1, Train loss:0.301843, valid loss:0.008175
Epoch:2, Train loss:0.013254, valid loss:0.005913
Epoch:3, Train loss:0.009325, valid loss:0.004207
Epoch:4, Train loss:0.007198, valid loss:0.003765
Epoch:5, Train loss:0.005785, valid loss:0.003249
Epoch:6, Train loss:0.004935, valid loss:0.002317
Epoch:7, Train loss:0.004491, valid loss:0.002479
Epoch:8, Train loss:0.004199, valid loss:0.003719
Epoch:9, Train loss:0.003816, valid loss:0.002492
Epoch:10, Train loss:0.003723, valid loss:0.001855
Epoch:11, Train loss:0.002707, valid loss:0.002101
Epoch:12, Train loss:0.002649, valid loss:0.001498
Epoch:13, Train loss:0.002660, valid loss:0.001479
Epoch:14, Train loss:0.002509, valid loss:0.001552
Epoch:15, Train loss:0.002490, valid loss:0.001650
Epoch:16, Train loss:0.002464, valid loss:0.001823
Epoch:17, Train loss:0.002415, valid loss:0.001474
Epoch:18, Train loss:0.002358, valid loss:0.001634
Epoch:19, Train loss:0.002298, valid loss:0.001550
Epoch:20, Train loss:0.002272, valid loss:0.001387
Epoch:21, Train loss:0.001811, valid loss:0.001156
Epoch:22, Train loss:0.001761, valid loss:0.001082
Epoch:23, Train loss:0.001752, valid loss:0.001222
Epoch:24, Train loss:0.001759, valid loss:0.001048
Epoch:25, Train loss:0.001747, valid loss:0.001084
Epoch:26, Train loss:0.001744, valid loss:0.001080
Epoch:27, Train loss:0.001717, valid loss:0.001161
Epoch:28, Train loss:0.001682, valid loss:0.001224
Epoch:29, Train loss:0.001678, valid loss:0.001046
Epoch:30, Train loss:0.001649, valid loss:0.001043
Epoch:31, Train loss:0.001446, valid loss:0.000946
Epoch:32, Train loss:0.001432, valid loss:0.000980
Epoch:33, Train loss:0.001424, valid loss:0.000948
Epoch:34, Train loss:0.001425, valid loss:0.000966
Epoch:35, Train loss:0.001412, valid loss:0.000980
Epoch:36, Train loss:0.001416, valid loss:0.001038
Epoch:37, Train loss:0.001390, valid loss:0.000951
Epoch:38, Train loss:0.001404, valid loss:0.000932
Epoch:39, Train loss:0.001371, valid loss:0.000950
Epoch:40, Train loss:0.001376, valid loss:0.000995
Epoch:41, Train loss:0.001269, valid loss:0.000932
Epoch:42, Train loss:0.001263, valid loss:0.000883
Epoch:43, Train loss:0.001262, valid loss:0.000869
Epoch:44, Train loss:0.001260, valid loss:0.000893
Epoch:45, Train loss:0.001260, valid loss:0.000881
Epoch:46, Train loss:0.001256, valid loss:0.000888
Epoch:47, Train loss:0.001250, valid loss:0.000892
Epoch:48, Train loss:0.001241, valid loss:0.000902
Epoch:49, Train loss:0.001237, valid loss:0.000944
Epoch:50, Train loss:0.001238, valid loss:0.000903
Epoch:51, Train loss:0.001190, valid loss:0.000877
Epoch:52, Train loss:0.001189, valid loss:0.000857
Epoch:53, Train loss:0.001184, valid loss:0.000872
Epoch:54, Train loss:0.001188, valid loss:0.000855
Epoch:55, Train loss:0.001180, valid loss:0.000872
Epoch:56, Train loss:0.001179, valid loss:0.000894
Epoch:57, Train loss:0.001177, valid loss:0.000874
Epoch:58, Train loss:0.001174, valid loss:0.000865
Epoch:59, Train loss:0.001173, valid loss:0.000900
Epoch:60, Train loss:0.001172, valid loss:0.000860
training time 5345.622201681137
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.026688256788120444
plot_id,batch_id 0 1 miss% 0.03297192189231137
plot_id,batch_id 0 2 miss% 0.029420332303909363
plot_id,batch_id 0 3 miss% 0.024874013016099893
plot_id,batch_id 0 4 miss% 0.019705296972903814
plot_id,batch_id 0 5 miss% 0.02303209844155899
plot_id,batch_id 0 6 miss% 0.03925621190301307
plot_id,batch_id 0 7 miss% 0.022507302884643074
plot_id,batch_id 0 8 miss% 0.0227167295167348
plot_id,batch_id 0 9 miss% 0.02241242910024099
plot_id,batch_id 0 10 miss% 0.0397127327521184
plot_id,batch_id 0 11 miss% 0.05668824455027319
plot_id,batch_id 0 12 miss% 0.026545269581759366
plot_id,batch_id 0 13 miss% 0.017556493269403485
plot_id,batch_id 0 14 miss% 0.03745970331606109
plot_id,batch_id 0 15 miss% 0.047129699592995546
plot_id,batch_id 0 16 miss% 0.02727535413638342
plot_id,batch_id 0 17 miss% 0.03126950001897658
plot_id,batch_id 0 18 miss% 0.04271698524444188
plot_id,batch_id 0 19 miss% 0.03413679385470852
plot_id,batch_id 0 20 miss% 0.04284340142679076
plot_id,batch_id 0 21 miss% 0.015849790999438625
plot_id,batch_id 0 22 miss% 0.029199585386651978
plot_id,batch_id 0 23 miss% 0.021585645163818452
plot_id,batch_id 0 24 miss% 0.019484626422607294
plot_id,batch_id 0 25 miss% 0.03279976318701027
plot_id,batch_id 0 26 miss% 0.03627626096421292
plot_id,batch_id 0 27 miss% 0.026122582080731618
plot_id,batch_id 0 28 miss% 0.026533395574571305
plot_id,batch_id 0 29 miss% 0.02532982473696767
plot_id,batch_id 0 30 miss% 0.051379266368709646
plot_id,batch_id 0 31 miss% 0.02396705655334289
plot_id,batch_id 0 32 miss% 0.025919183891526226
plot_id,batch_id 0 33 miss% 0.03371073807655992
plot_id,batch_id 0 34 miss% 0.019945886146255528
plot_id,batch_id 0 35 miss% 0.04437353588954375
plot_id,batch_id 0 36 miss% 0.0369039118351268
plot_id,batch_id 0 37 miss% 0.022673065314446624
plot_id,batch_id 0 38 miss% 0.020827064727466656
plot_id,batch_id 0 39 miss% 0.025913158414024837
plot_id,batch_id 0 40 miss% 0.07219228365286469
plot_id,batch_id 0 41 miss% 0.030395378026075417
plot_id,batch_id 0 42 miss% 0.019757783558244906
plot_id,batch_id 0 43 miss% 0.027188359998244524
plot_id,batch_id 0 44 miss% 0.015448529787718858
plot_id,batch_id 0 45 miss% 0.02273735541572667
plot_id,batch_id 0 46 miss% 0.026136111187384025
plot_id,batch_id 0 47 miss% 0.02257297654987928
plot_id,batch_id 0 48 miss% 0.02341303982594195
plot_id,batch_id 0 49 miss% 0.016105708908471992
plot_id,batch_id 0 50 miss% 0.037915710537108356
plot_id,batch_id 0 51 miss% 0.024733970259888553
plot_id,batch_id 0 52 miss% 0.023462049960311177
plot_id,batch_id 0 53 miss% 0.012017528110695052
plot_id,batch_id 0 54 miss% 0.02403700462359901
plot_id,batch_id 0 55 miss% 0.036723077268472536
plot_id,batch_id 0 56 miss% 0.030256369496044492
plot_id,batch_id 0 57 miss% 0.021955550001383498
plot_id,batch_id 0 58 miss% 0.024032283068731317
plot_id,batch_id 0 59 miss% 0.02365156445359344
plot_id,batch_id 0 60 miss% 0.050465102493241376
plot_id,batch_id 0 61 miss% 0.029134075893531557
plot_id,batch_id 0 62 miss% 0.020292616756137964
plot_id,batch_id 0 63 miss% 0.02492607074375178
plot_id,batch_id 0 64 miss% 0.03823545245307376
plot_id,batch_id 0 65 miss% 0.05686111512391557
plot_id,batch_id 0 66 miss% 0.0327378765984232
plot_id,batch_id 0 67 miss% 0.028704108658717992
plot_id,batch_id 0 68 miss% 0.03524396928690478
plot_id,batch_id 0 69 miss% 0.0342247445519399
plot_id,batch_id 0 70 miss% 0.043808040855261306
plot_id,batch_id 0 71 miss% 0.049467814310740536
plot_id,batch_id 0 72 miss% 0.03066194999724212
plot_id,batch_id 0 73 miss% 0.04088867821824
plot_id,batch_id 0 74 miss% 0.035036797265222706
plot_id,batch_id 0 75 miss% 0.051675019696962365
plot_id,batch_id 0 76 miss% 0.041155153215040645
plot_id,batch_id 0 77 miss% 0.036976710601453154
plot_id,batch_id 0 78 miss% 0.03252321207377022
plot_id,batch_id 0 79 miss% 0.04544227339322383
plot_id,batch_id 0 80 miss% 0.04255094637371947
plot_id,batch_id 0 81 miss% 0.027216422604058814
plot_id,batch_id 0 82 miss% 0.023304719346480717
plot_id,batch_id 0 83 miss% 0.029658996309100495
plot_id,batch_id 0 84 miss% 0.028083703674205943
plot_id,batch_id 0 85 miss% 0.044826666245146425
plot_id,batch_id 0 86 miss% 0.02596333612963639
plot_id,batch_id 0 87 miss% 0.026488561067830037
plot_id,batch_id 0 88 miss% 0.0329403638244216
plot_id,batch_id 0 89 miss% 0.02659223399059669
plot_id,batch_id 0 90 miss% 0.03800438523475728
plot_id,batch_id 0 91 miss% 0.03292496035807262
plot_id,batch_id 0 92 miss% 0.037332098266807896
plot_id,batch_id 0 93 miss% 0.027431176965083418
plot_id,batch_id 0 94 miss% 0.025815671692945984
plot_id,batch_id 0 95 miss% 0.035071988549650476
plot_id,batch_id 0 96 miss% 0.03545607430767348
plot_id,batch_id 0 97 miss% 0.05782465984815363
plot_id,batch_id 0 98 miss% 0.023741366661076003
plot_id,batch_id 0 99 miss% 0.024617463050137237
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02668826 0.03297192 0.02942033 0.02487401 0.0197053  0.0230321
 0.03925621 0.0225073  0.02271673 0.02241243 0.03971273 0.05668824
 0.02654527 0.01755649 0.0374597  0.0471297  0.02727535 0.0312695
 0.04271699 0.03413679 0.0428434  0.01584979 0.02919959 0.02158565
 0.01948463 0.03279976 0.03627626 0.02612258 0.0265334  0.02532982
 0.05137927 0.02396706 0.02591918 0.03371074 0.01994589 0.04437354
 0.03690391 0.02267307 0.02082706 0.02591316 0.07219228 0.03039538
 0.01975778 0.02718836 0.01544853 0.02273736 0.02613611 0.02257298
 0.02341304 0.01610571 0.03791571 0.02473397 0.02346205 0.01201753
 0.024037   0.03672308 0.03025637 0.02195555 0.02403228 0.02365156
 0.0504651  0.02913408 0.02029262 0.02492607 0.03823545 0.05686112
 0.03273788 0.02870411 0.03524397 0.03422474 0.04380804 0.04946781
 0.03066195 0.04088868 0.0350368  0.05167502 0.04115515 0.03697671
 0.03252321 0.04544227 0.04255095 0.02721642 0.02330472 0.029659
 0.0280837  0.04482667 0.02596334 0.02648856 0.03294036 0.02659223
 0.03800439 0.03292496 0.0373321  0.02743118 0.02581567 0.03507199
 0.03545607 0.05782466 0.02374137 0.02461746]
for model  12 the mean error 0.0314475232367319
all id 12 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 21 out win 3 err 0.0314475232367319
Launcher: Job 13 completed in 5549 seconds.
Launcher: Task 148 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  35921
Epoch:0, Train loss:0.672969, valid loss:0.663568
Epoch:1, Train loss:0.041675, valid loss:0.012634
Epoch:2, Train loss:0.011172, valid loss:0.004147
Epoch:3, Train loss:0.006994, valid loss:0.003505
Epoch:4, Train loss:0.005861, valid loss:0.002772
Epoch:5, Train loss:0.004683, valid loss:0.002256
Epoch:6, Train loss:0.004086, valid loss:0.001797
Epoch:7, Train loss:0.003753, valid loss:0.001959
Epoch:8, Train loss:0.003542, valid loss:0.001644
Epoch:9, Train loss:0.003415, valid loss:0.001918
Epoch:10, Train loss:0.003199, valid loss:0.001392
Epoch:11, Train loss:0.002272, valid loss:0.001223
Epoch:12, Train loss:0.002329, valid loss:0.001550
Epoch:13, Train loss:0.002270, valid loss:0.001191
Epoch:14, Train loss:0.002243, valid loss:0.001327
Epoch:15, Train loss:0.002230, valid loss:0.001220
Epoch:16, Train loss:0.002088, valid loss:0.001248
Epoch:17, Train loss:0.002125, valid loss:0.001194
Epoch:18, Train loss:0.002098, valid loss:0.001150
Epoch:19, Train loss:0.002013, valid loss:0.001090
Epoch:20, Train loss:0.002038, valid loss:0.001081
Epoch:21, Train loss:0.001562, valid loss:0.000931
Epoch:22, Train loss:0.001603, valid loss:0.001134
Epoch:23, Train loss:0.001578, valid loss:0.000854
Epoch:24, Train loss:0.001540, valid loss:0.000983
Epoch:25, Train loss:0.001553, valid loss:0.000841
Epoch:26, Train loss:0.001539, valid loss:0.000845
Epoch:27, Train loss:0.001529, valid loss:0.000842
Epoch:28, Train loss:0.001482, valid loss:0.001083
Epoch:29, Train loss:0.001480, valid loss:0.000880
Epoch:30, Train loss:0.001475, valid loss:0.000959
Epoch:31, Train loss:0.001273, valid loss:0.000853
Epoch:32, Train loss:0.001247, valid loss:0.000737
Epoch:33, Train loss:0.001245, valid loss:0.000782
Epoch:34, Train loss:0.001242, valid loss:0.000779
Epoch:35, Train loss:0.001237, valid loss:0.000759
Epoch:36, Train loss:0.001233, valid loss:0.000716
Epoch:37, Train loss:0.001229, valid loss:0.000756
Epoch:38, Train loss:0.001209, valid loss:0.000765
Epoch:39, Train loss:0.001213, valid loss:0.000781
Epoch:40, Train loss:0.001215, valid loss:0.000733
Epoch:41, Train loss:0.001098, valid loss:0.000704
Epoch:42, Train loss:0.001101, valid loss:0.000688
Epoch:43, Train loss:0.001088, valid loss:0.000712
Epoch:44, Train loss:0.001092, valid loss:0.000696
Epoch:45, Train loss:0.001085, valid loss:0.000690
Epoch:46, Train loss:0.001087, valid loss:0.000717
Epoch:47, Train loss:0.001088, valid loss:0.000705
Epoch:48, Train loss:0.001083, valid loss:0.000699
Epoch:49, Train loss:0.001084, valid loss:0.000742
Epoch:50, Train loss:0.001066, valid loss:0.000725
Epoch:51, Train loss:0.001021, valid loss:0.000682
Epoch:52, Train loss:0.001016, valid loss:0.000683
Epoch:53, Train loss:0.001019, valid loss:0.000678
Epoch:54, Train loss:0.001012, valid loss:0.000675
Epoch:55, Train loss:0.001016, valid loss:0.000720
Epoch:56, Train loss:0.001014, valid loss:0.000698
Epoch:57, Train loss:0.001011, valid loss:0.000678
Epoch:58, Train loss:0.001006, valid loss:0.000672
Epoch:59, Train loss:0.001004, valid loss:0.000685
Epoch:60, Train loss:0.001004, valid loss:0.000674
training time 5359.145265340805
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.0296106129574513
plot_id,batch_id 0 1 miss% 0.03724336282637394
plot_id,batch_id 0 2 miss% 0.03404744947845392
plot_id,batch_id 0 3 miss% 0.0327313889960851
plot_id,batch_id 0 4 miss% 0.03943762676455373
plot_id,batch_id 0 5 miss% 0.03914506044447965
plot_id,batch_id 0 6 miss% 0.04775452559444111
plot_id,batch_id 0 7 miss% 0.04156843556886241
plot_id,batch_id 0 8 miss% 0.04643492481751219
plot_id,batch_id 0 9 miss% 0.02616095061741832
plot_id,batch_id 0 10 miss% 0.04432497333500411
plot_id,batch_id 0 11 miss% 0.04162008220242126
plot_id,batch_id 0 12 miss% 0.04162558178886471
plot_id,batch_id 0 13 miss% 0.018115900085349655
plot_id,batch_id 0 14 miss% 0.03579044125546064
plot_id,batch_id 0 15 miss% 0.04929028187050185
plot_id,batch_id 0 16 miss% 0.038656581178137674
plot_id,batch_id 0 17 miss% 0.02792130023789242
plot_id,batch_id 0 18 miss% 0.031715795425213376
plot_id,batch_id 0 19 miss% 0.0298668672431481
plot_id,batch_id 0 20 miss% 0.032525146852493844
plot_id,batch_id 0 21 miss% 0.016819157168311987
plot_id,batch_id 0 22 miss% 0.045312585152830355
plot_id,batch_id 0 23 miss% 0.024619476156437576
plot_id,batch_id 0 24 miss% 0.03417812833769698
plot_id,batch_id 0 25 miss% 0.03442389117600526
plot_id,batch_id 0 26 miss% 0.04372135063482056
plot_id,batch_id 0 27 miss% 0.033016995890143015
plot_id,batch_id 0 28 miss% 0.012126670772011794
plot_id,batch_id 0 29 miss% 0.023676632268827776
plot_id,batch_id 0 30 miss% 0.04024942331024813
plot_id,batch_id 0 31 miss% 0.033767974350288325
plot_id,batch_id 0 32 miss% 0.024225735766848035
plot_id,batch_id 0 33 miss% 0.022684799600469263
plot_id,batch_id 0 34 miss% 0.03146358206918044
plot_id,batch_id 0 35 miss% 0.0375913813395055
plot_id,batch_id 0 36 miss% 0.03164970975274189
plot_id,batch_id 0 37 miss% 0.029753439141030947
plot_id,batch_id 0 38 miss% 0.03037136682558406
plot_id,batch_id 0 39 miss% 0.019306306704271834
plot_id,batch_id 0 40 miss% 0.05283570828811698
plot_id,batch_id 0 41 miss% 0.029704388829893784
plot_id,batch_id 0 42 miss% 0.01836689783249959
plot_id,batch_id 0 43 miss% 0.02832502150061137
plot_id,batch_id 0 44 miss% 0.02540840580546438
plot_id,batch_id 0 45 miss% 0.04350473247149047
plot_id,batch_id 0 46 miss% 0.02170076618466387
plot_id,batch_id 0 47 miss% 0.019323013614481903
plot_id,batch_id 0 48 miss% 0.025450889447588235
plot_id,batch_id 0 49 miss% 0.02734290841724156
plot_id,batch_id 0 50 miss% 0.04125402719610527
plot_id,batch_id 0 51 miss% 0.02207372561138086
plot_id,batch_id 0 52 miss% 0.021053369287958826
plot_id,batch_id 0 53 miss% 0.018566223498110428
plot_id,batch_id 0 54 miss% 0.024150842110070903
plot_id,batch_id 0 55 miss% 0.041226663632850706
plot_id,batch_id 0 56 miss% 0.03018074498434235
plot_id,batch_id 0 57 miss% 0.019155651454689997
plot_id,batch_id 0 58 miss% 0.018843173816086443
plot_id,batch_id 0 59 miss% 0.025086077994468627
plot_id,batch_id 0 60 miss% 0.046310545146904726
plot_id,batch_id 0 61 miss% 0.04075802963177537
plot_id,batch_id 0 62 miss% 0.03502717247179601
plot_id,batch_id 0 63 miss% 0.019653132003443836
plot_id,batch_id 0 64 miss% 0.02663961609454826
plot_id,batch_id 0 65 miss% 0.04513451322602598
plot_id,batch_id 0 66 miss% 0.03142175110265257
plot_id,batch_id 0 67 miss% 0.027666047118060963
plot_id,batch_id 0 68 miss% 0.036206540175117305
plot_id,batch_id 0 69 miss% 0.02763023291370573
plot_id,batch_id 0 70 miss% 0.030851286490995666
plot_id,batch_id 0 71 miss% 0.0638880173385836
plot_id,batch_id 0 72 miss% 0.03368942740195649
plot_id,batch_id 0 73 miss% 0.03182078202893037
plot_id,batch_id 0 74 miss% 0.04470026778912134
plot_id,batch_id 0 75 miss% 0.0500632976653805
plot_id,batch_id 0 76 miss% 0.0361846273584877
plot_id,batch_id 0 77 miss% 0.01820279887919113
plot_id,batch_id 0 78 miss% 0.03480836731886001
plot_id,batch_id 0 79 miss% 0.03452007437446758
plot_id,batch_id 0 80 miss% 0.03042125966615166
plot_id,batch_id 0 81 miss% 0.025042843205138863
plot_id,batch_id 0 82 miss% 0.03173959494945002
plot_id,batch_id 0 83 miss% 0.02119170492051183
plot_id,batch_id 0 84 miss% 0.023335428191423962
plot_id,batch_id 0 85 miss% 0.04624328635432017
plot_id,batch_id 0 86 miss% 0.028506017878390397
plot_id,batch_id 0 87 miss% 0.02873741938396599
plot_id,batch_id 0 88 miss% 0.027035577359335154
plot_id,batch_id 0 89 miss% 0.026125310142590355
plot_id,batch_id 0 90 miss% 0.02809975662793623
plot_id,batch_id 0 91 miss% 0.033307767580425106
plot_id,batch_id 0 92 miss% 0.02837946372104838
plot_id,batch_id 0 93 miss% 0.037562358311215946
plot_id,batch_id 0 94 miss% 0.0200075896473337
plot_id,batch_id 0 95 miss% 0.032708466430508336
plot_id,batch_id 0 96 miss% 0.029969870569115018
plot_id,batch_id 0 97 miss% 0.03501311869341448
plot_id,batch_id 0 98 miss% 0.029077171101501327
plot_id,batch_id 0 99 miss% 0.03234885784100737
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02961061 0.03724336 0.03404745 0.03273139 0.03943763 0.03914506
 0.04775453 0.04156844 0.04643492 0.02616095 0.04432497 0.04162008
 0.04162558 0.0181159  0.03579044 0.04929028 0.03865658 0.0279213
 0.0317158  0.02986687 0.03252515 0.01681916 0.04531259 0.02461948
 0.03417813 0.03442389 0.04372135 0.033017   0.01212667 0.02367663
 0.04024942 0.03376797 0.02422574 0.0226848  0.03146358 0.03759138
 0.03164971 0.02975344 0.03037137 0.01930631 0.05283571 0.02970439
 0.0183669  0.02832502 0.02540841 0.04350473 0.02170077 0.01932301
 0.02545089 0.02734291 0.04125403 0.02207373 0.02105337 0.01856622
 0.02415084 0.04122666 0.03018074 0.01915565 0.01884317 0.02508608
 0.04631055 0.04075803 0.03502717 0.01965313 0.02663962 0.04513451
 0.03142175 0.02766605 0.03620654 0.02763023 0.03085129 0.06388802
 0.03368943 0.03182078 0.04470027 0.0500633  0.03618463 0.0182028
 0.03480837 0.03452007 0.03042126 0.02504284 0.03173959 0.0211917
 0.02333543 0.04624329 0.02850602 0.02873742 0.02703558 0.02612531
 0.02809976 0.03330777 0.02837946 0.03756236 0.02000759 0.03270847
 0.02996987 0.03501312 0.02907717 0.03234886]
for model  126 the mean error 0.03198128517040279
all id 126 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 25 out win 3 err 0.03198128517040279
Launcher: Job 127 completed in 5554 seconds.
Launcher: Task 55 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  21969
Epoch:0, Train loss:0.537238, valid loss:0.504722
Epoch:1, Train loss:0.357818, valid loss:0.361965
Epoch:2, Train loss:0.348078, valid loss:0.361064
Epoch:3, Train loss:0.346696, valid loss:0.360833
Epoch:4, Train loss:0.345812, valid loss:0.360130
Epoch:5, Train loss:0.345227, valid loss:0.359447
Epoch:6, Train loss:0.344753, valid loss:0.359876
Epoch:7, Train loss:0.344292, valid loss:0.359411
Epoch:8, Train loss:0.343977, valid loss:0.359317
Epoch:9, Train loss:0.343772, valid loss:0.359039
Epoch:10, Train loss:0.343631, valid loss:0.359111
Epoch:11, Train loss:0.342594, valid loss:0.358449
Epoch:12, Train loss:0.342537, valid loss:0.358378
Epoch:13, Train loss:0.342489, valid loss:0.358569
Epoch:14, Train loss:0.342433, valid loss:0.358575
Epoch:15, Train loss:0.342409, valid loss:0.358373
Epoch:16, Train loss:0.342339, valid loss:0.358289
Epoch:17, Train loss:0.342310, valid loss:0.358402
Epoch:18, Train loss:0.342357, valid loss:0.358613
Epoch:19, Train loss:0.342186, valid loss:0.358416
Epoch:20, Train loss:0.342253, valid loss:0.358281
Epoch:21, Train loss:0.341707, valid loss:0.358180
Epoch:22, Train loss:0.341693, valid loss:0.358098
Epoch:23, Train loss:0.341708, valid loss:0.358305
Epoch:24, Train loss:0.341660, valid loss:0.358148
Epoch:25, Train loss:0.341699, valid loss:0.358069
Epoch:26, Train loss:0.341638, valid loss:0.358031
Epoch:27, Train loss:0.341618, valid loss:0.358155
Epoch:28, Train loss:0.341619, valid loss:0.358051
Epoch:29, Train loss:0.341622, valid loss:0.358045
Epoch:30, Train loss:0.341595, valid loss:0.358075
Epoch:31, Train loss:0.341356, valid loss:0.358002
Epoch:32, Train loss:0.341345, valid loss:0.358069
Epoch:33, Train loss:0.341340, valid loss:0.357946
Epoch:34, Train loss:0.341348, valid loss:0.357939
Epoch:35, Train loss:0.341320, valid loss:0.357945
Epoch:36, Train loss:0.341321, valid loss:0.357913
Epoch:37, Train loss:0.341316, valid loss:0.357965
Epoch:38, Train loss:0.341291, valid loss:0.357934
Epoch:39, Train loss:0.341293, valid loss:0.358051
Epoch:40, Train loss:0.341295, valid loss:0.357985
Epoch:41, Train loss:0.341165, valid loss:0.357924
Epoch:42, Train loss:0.341152, valid loss:0.357910
Epoch:43, Train loss:0.341158, valid loss:0.357924
Epoch:44, Train loss:0.341149, valid loss:0.357910
Epoch:45, Train loss:0.341164, valid loss:0.357915
Epoch:46, Train loss:0.341175, valid loss:0.357894
Epoch:47, Train loss:0.341138, valid loss:0.357897
Epoch:48, Train loss:0.341128, valid loss:0.357938
Epoch:49, Train loss:0.341149, valid loss:0.357872
Epoch:50, Train loss:0.341140, valid loss:0.357891
Epoch:51, Train loss:0.341061, valid loss:0.357898
Epoch:52, Train loss:0.341058, valid loss:0.357877
Epoch:53, Train loss:0.341062, valid loss:0.357894
Epoch:54, Train loss:0.341058, valid loss:0.357864
Epoch:55, Train loss:0.341067, valid loss:0.357875
Epoch:56, Train loss:0.341058, valid loss:0.357871
Epoch:57, Train loss:0.341049, valid loss:0.357867
Epoch:58, Train loss:0.341051, valid loss:0.357863
Epoch:59, Train loss:0.341048, valid loss:0.357861
Epoch:60, Train loss:0.341047, valid loss:0.357870
training time 5383.031973361969
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.783794435026036
plot_id,batch_id 0 1 miss% 0.8323417415983317
plot_id,batch_id 0 2 miss% 0.8349897466279538
plot_id,batch_id 0 3 miss% 0.8409406003266661
plot_id,batch_id 0 4 miss% 0.8445107992550486
plot_id,batch_id 0 5 miss% 0.7768450349587864
plot_id,batch_id 0 6 miss% 0.8268477475967535
plot_id,batch_id 0 7 miss% 0.8363842501023278
plot_id,batch_id 0 8 miss% 0.8409290079832896
plot_id,batch_id 0 9 miss% 0.8478743361573297
plot_id,batch_id 0 10 miss% 0.7653984551701664
plot_id,batch_id 0 11 miss% 0.8232287141829095
plot_id,batch_id 0 12 miss% 0.8331364956610217
plot_id,batch_id 0 13 miss% 0.8404351261319604
plot_id,batch_id 0 14 miss% 0.8416881356445378
plot_id,batch_id 0 15 miss% 0.7669725547537732
plot_id,batch_id 0 16 miss% 0.8207730243400199
plot_id,batch_id 0 17 miss% 0.8369516550222436
plot_id,batch_id 0 18 miss% 0.8426900559484874
plot_id,batch_id 0 19 miss% 0.8471496245968402
plot_id,batch_id 0 20 miss% 0.8082889561950685
plot_id,batch_id 0 21 miss% 0.8374009876442461
plot_id,batch_id 0 22 miss% 0.8432047194432185
plot_id,batch_id 0 23 miss% 0.8441489247069871
plot_id,batch_id 0 24 miss% 0.8460276957675168
plot_id,batch_id 0 25 miss% 0.8049770665135922
plot_id,batch_id 0 26 miss% 0.8354700921751077
plot_id,batch_id 0 27 miss% 0.8390030238796558
plot_id,batch_id 0 28 miss% 0.8430503527767351
plot_id,batch_id 0 29 miss% 0.844084052358116
plot_id,batch_id 0 30 miss% 0.7944214712446704
plot_id,batch_id 0 31 miss% 0.8318871494073367
plot_id,batch_id 0 32 miss% 0.8400971978309519
plot_id,batch_id 0 33 miss% 0.8418362072342418
plot_id,batch_id 0 34 miss% 0.8447391910697495
plot_id,batch_id 0 35 miss% 0.7922507898108925
plot_id,batch_id 0 36 miss% 0.8376895359984673
plot_id,batch_id 0 37 miss% 0.8384248184558573
plot_id,batch_id 0 38 miss% 0.8445213400370096
plot_id,batch_id 0 39 miss% 0.8448942423463346
plot_id,batch_id 0 40 miss% 0.8224855245163905
plot_id,batch_id 0 41 miss% 0.8417792933782876
plot_id,batch_id 0 42 miss% 0.8404667048493499
plot_id,batch_id 0 43 miss% 0.8488117239717872
plot_id,batch_id 0 44 miss% 0.851321840348728
plot_id,batch_id 0 45 miss% 0.8171895097776815
plot_id,batch_id 0 46 miss% 0.8409273949070071
plot_id,batch_id 0 47 miss% 0.8418443248779495
plot_id,batch_id 0 48 miss% 0.846761549647674
plot_id,batch_id 0 49 miss% 0.8511348414876863
plot_id,batch_id 0 50 miss% 0.8251067441086527
plot_id,batch_id 0 51 miss% 0.8394161732332235
plot_id,batch_id 0 52 miss% 0.8415191907669383
plot_id,batch_id 0 53 miss% 0.8462554682301544
plot_id,batch_id 0 54 miss% 0.8481952792080657
plot_id,batch_id 0 55 miss% 0.8180764357393735
plot_id,batch_id 0 56 miss% 0.8408632944180243
plot_id,batch_id 0 57 miss% 0.8437852869995227
plot_id,batch_id 0 58 miss% 0.8467517430696903
plot_id,batch_id 0 59 miss% 0.8444267485715256
plot_id,batch_id 0 60 miss% 0.7206494957470747
plot_id,batch_id 0 61 miss% 0.8130234789662901
plot_id,batch_id 0 62 miss% 0.8255248464817426
plot_id,batch_id 0 63 miss% 0.832432874096712
plot_id,batch_id 0 64 miss% 0.8356728389437177
plot_id,batch_id 0 65 miss% 0.7226348215231572
plot_id,batch_id 0 66 miss% 0.8055699952712008
plot_id,batch_id 0 67 miss% 0.8131179085145541
plot_id,batch_id 0 68 miss% 0.8304268146536592
plot_id,batch_id 0 69 miss% 0.832470506249853
plot_id,batch_id 0 70 miss% 0.681584540902743
plot_id,batch_id 0 71 miss% 0.8068143332768968
plot_id,batch_id 0 72 miss% 0.807371035902705
plot_id,batch_id 0 73 miss% 0.8196434279831719
plot_id,batch_id 0 74 miss% 0.8260134830537375
plot_id,batch_id 0 75 miss% 0.6792826983482279
plot_id,batch_id 0 76 miss% 0.7951234026721739
plot_id,batch_id 0 77 miss% 0.8033879096917665
plot_id,batch_id 0 78 miss% 0.8231064720385401
plot_id,batch_id 0 79 miss% 0.8198123413650195
plot_id,batch_id 0 80 miss% 0.7404760912037034
plot_id,batch_id 0 81 miss% 0.8207916109636737
plot_id,batch_id 0 82 miss% 0.8300686673613563
plot_id,batch_id 0 83 miss% 0.8351350313366406
plot_id,batch_id 0 84 miss% 0.8408259896498134
plot_id,batch_id 0 85 miss% 0.7363531142406506
plot_id,batch_id 0 86 miss% 0.8159728309719415
plot_id,batch_id 0 87 miss% 0.8280775098151285
plot_id,batch_id 0 88 miss% 0.8360104934088027
plot_id,batch_id 0 89 miss% 0.8362517852148682
plot_id,batch_id 0 90 miss% 0.699344327535214
plot_id,batch_id 0 91 miss% 0.8024899896167822
plot_id,batch_id 0 92 miss% 0.8196942203835554
plot_id,batch_id 0 93 miss% 0.8255961502256682
plot_id,batch_id 0 94 miss% 0.8410115134989029
plot_id,batch_id 0 95 miss% 0.7208092251416133
plot_id,batch_id 0 96 miss% 0.7955971834640574
plot_id,batch_id 0 97 miss% 0.8229635388888197
plot_id,batch_id 0 98 miss% 0.8267759871008912
plot_id,batch_id 0 99 miss% 0.8342984041177814
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.78379444 0.83234174 0.83498975 0.8409406  0.8445108  0.77684503
 0.82684775 0.83638425 0.84092901 0.84787434 0.76539846 0.82322871
 0.8331365  0.84043513 0.84168814 0.76697255 0.82077302 0.83695166
 0.84269006 0.84714962 0.80828896 0.83740099 0.84320472 0.84414892
 0.8460277  0.80497707 0.83547009 0.83900302 0.84305035 0.84408405
 0.79442147 0.83188715 0.8400972  0.84183621 0.84473919 0.79225079
 0.83768954 0.83842482 0.84452134 0.84489424 0.82248552 0.84177929
 0.8404667  0.84881172 0.85132184 0.81718951 0.84092739 0.84184432
 0.84676155 0.85113484 0.82510674 0.83941617 0.84151919 0.84625547
 0.84819528 0.81807644 0.84086329 0.84378529 0.84675174 0.84442675
 0.7206495  0.81302348 0.82552485 0.83243287 0.83567284 0.72263482
 0.80557    0.81311791 0.83042681 0.83247051 0.68158454 0.80681433
 0.80737104 0.81964343 0.82601348 0.6792827  0.7951234  0.80338791
 0.82310647 0.81981234 0.74047609 0.82079161 0.83006867 0.83513503
 0.84082599 0.73635311 0.81597283 0.82807751 0.83601049 0.83625179
 0.69934433 0.80248999 0.81969422 0.82559615 0.84101151 0.72080923
 0.79559718 0.82296354 0.82677599 0.8342984 ]
for model  110 the mean error 0.8193962531791076
all id 110 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 25 out win 5 err 0.8193962531791076
Launcher: Job 111 completed in 5562 seconds.
Launcher: Task 76 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  28945
Epoch:0, Train loss:0.621320, valid loss:0.614854
Epoch:1, Train loss:0.065388, valid loss:0.015709
Epoch:2, Train loss:0.021999, valid loss:0.008665
Epoch:3, Train loss:0.013545, valid loss:0.005242
Epoch:4, Train loss:0.010835, valid loss:0.005166
Epoch:5, Train loss:0.009275, valid loss:0.004622
Epoch:6, Train loss:0.008068, valid loss:0.004192
Epoch:7, Train loss:0.007552, valid loss:0.003808
Epoch:8, Train loss:0.006981, valid loss:0.003218
Epoch:9, Train loss:0.006713, valid loss:0.003477
Epoch:10, Train loss:0.006289, valid loss:0.003009
Epoch:11, Train loss:0.004603, valid loss:0.003036
Epoch:12, Train loss:0.004648, valid loss:0.002370
Epoch:13, Train loss:0.004536, valid loss:0.003213
Epoch:14, Train loss:0.004478, valid loss:0.003322
Epoch:15, Train loss:0.004227, valid loss:0.002068
Epoch:16, Train loss:0.004379, valid loss:0.002063
Epoch:17, Train loss:0.004139, valid loss:0.002413
Epoch:18, Train loss:0.004181, valid loss:0.002384
Epoch:19, Train loss:0.004030, valid loss:0.002038
Epoch:20, Train loss:0.003973, valid loss:0.002465
Epoch:21, Train loss:0.003109, valid loss:0.001795
Epoch:22, Train loss:0.003077, valid loss:0.001759
Epoch:23, Train loss:0.003016, valid loss:0.001654
Epoch:24, Train loss:0.003053, valid loss:0.002159
Epoch:25, Train loss:0.003039, valid loss:0.001690
Epoch:26, Train loss:0.002982, valid loss:0.001837
Epoch:27, Train loss:0.002929, valid loss:0.001693
Epoch:28, Train loss:0.002961, valid loss:0.001843
Epoch:29, Train loss:0.002924, valid loss:0.001554
Epoch:30, Train loss:0.002856, valid loss:0.002641
Epoch:31, Train loss:0.002433, valid loss:0.001522
Epoch:32, Train loss:0.002416, valid loss:0.001510
Epoch:33, Train loss:0.002425, valid loss:0.001526
Epoch:34, Train loss:0.002400, valid loss:0.001561
Epoch:35, Train loss:0.002403, valid loss:0.001485
Epoch:36, Train loss:0.002400, valid loss:0.001531
Epoch:37, Train loss:0.002409, valid loss:0.001413
Epoch:38, Train loss:0.002432, valid loss:0.001530
Epoch:39, Train loss:0.002343, valid loss:0.001491
Epoch:40, Train loss:0.002391, valid loss:0.001466
Epoch:41, Train loss:0.002122, valid loss:0.001328
Epoch:42, Train loss:0.002107, valid loss:0.001396
Epoch:43, Train loss:0.002082, valid loss:0.001330
Epoch:44, Train loss:0.002087, valid loss:0.001403
Epoch:45, Train loss:0.002073, valid loss:0.001374
Epoch:46, Train loss:0.002089, valid loss:0.001393
Epoch:47, Train loss:0.002091, valid loss:0.001308
Epoch:48, Train loss:0.002116, valid loss:0.001333
Epoch:49, Train loss:0.002055, valid loss:0.001505
Epoch:50, Train loss:0.002078, valid loss:0.001334
Epoch:51, Train loss:0.001951, valid loss:0.001296
Epoch:52, Train loss:0.001928, valid loss:0.001308
Epoch:53, Train loss:0.001953, valid loss:0.001299
Epoch:54, Train loss:0.001927, valid loss:0.001301
Epoch:55, Train loss:0.001931, valid loss:0.001297
Epoch:56, Train loss:0.001921, valid loss:0.001280
Epoch:57, Train loss:0.001914, valid loss:0.001276
Epoch:58, Train loss:0.001916, valid loss:0.001308
Epoch:59, Train loss:0.001924, valid loss:0.001289
Epoch:60, Train loss:0.001896, valid loss:0.001306
training time 5379.168872833252
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03713063572686896
plot_id,batch_id 0 1 miss% 0.02795471858977162
plot_id,batch_id 0 2 miss% 0.02479436891644572
plot_id,batch_id 0 3 miss% 0.032647964072165706
plot_id,batch_id 0 4 miss% 0.0371465873014977
plot_id,batch_id 0 5 miss% 0.03916357048442333
plot_id,batch_id 0 6 miss% 0.03089424933268569
plot_id,batch_id 0 7 miss% 0.027762141972727618
plot_id,batch_id 0 8 miss% 0.03162417723412494
plot_id,batch_id 0 9 miss% 0.01727895384735291
plot_id,batch_id 0 10 miss% 0.03643096024701024
plot_id,batch_id 0 11 miss% 0.03196245132126981
plot_id,batch_id 0 12 miss% 0.039188033602455145
plot_id,batch_id 0 13 miss% 0.04585112356243891
plot_id,batch_id 0 14 miss% 0.032134346979562335
plot_id,batch_id 0 15 miss% 0.04126583073003983
plot_id,batch_id 0 16 miss% 0.02819562089492361
plot_id,batch_id 0 17 miss% 0.04001785985959517
plot_id,batch_id 0 18 miss% 0.05034330375246961
plot_id,batch_id 0 19 miss% 0.03610758883867565
plot_id,batch_id 0 20 miss% 0.05306798737321389
plot_id,batch_id 0 21 miss% 0.021501325118422066
plot_id,batch_id 0 22 miss% 0.03958624000204833
plot_id,batch_id 0 23 miss% 0.027223181861440834
plot_id,batch_id 0 24 miss% 0.039374576286894135
plot_id,batch_id 0 25 miss% 0.02204766260483843
plot_id,batch_id 0 26 miss% 0.030182227010091586
plot_id,batch_id 0 27 miss% 0.02624750246546462
plot_id,batch_id 0 28 miss% 0.036037159350389016
plot_id,batch_id 0 29 miss% 0.025144729586744203
plot_id,batch_id 0 30 miss% 0.03868731387570229
plot_id,batch_id 0 31 miss% 0.033449280509069576
plot_id,batch_id 0 32 miss% 0.03795300930157118
plot_id,batch_id 0 33 miss% 0.02412621151066149
plot_id,batch_id 0 34 miss% 0.03139068696530511
plot_id,batch_id 0 35 miss% 0.042726657986483584
plot_id,batch_id 0 36 miss% 0.0729632005757015
plot_id,batch_id 0 37 miss% 0.021568144582407908
plot_id,batch_id 0 38 miss% 0.024359083576844017
plot_id,batch_id 0 39 miss% 0.01621934160579528
plot_id,batch_id 0 40 miss% 0.056816194589578456
plot_id,batch_id 0 41 miss% 0.025800388056920618
plot_id,batch_id 0 42 miss% 0.024717450115746793
plot_id,batch_id 0 43 miss% 0.03803987168606543
plot_id,batch_id 0 44 miss% 0.026873417990840028
plot_id,batch_id 0 45 miss% 0.028667765565169125
plot_id,batch_id 0 46 miss% 0.021302300247136176
plot_id,batch_id 0 47 miss% 0.028962913619061018
plot_id,batch_id 0 48 miss% 0.02660852604149209
plot_id,batch_id 0 49 miss% 0.0230699865167005
plot_id,batch_id 0 50 miss% 0.033228128545791165
plot_id,batch_id 0 51 miss% 0.02640863904206497
plot_id,batch_id 0 52 miss% 0.025392679309435978
plot_id,batch_id 0 53 miss% 0.016460612129375458
plot_id,batch_id 0 54 miss% 0.02780181474281668
plot_id,batch_id 0 55 miss% 0.03821090552602858
plot_id,batch_id 0 56 miss% 0.03192194315882328
plot_id,batch_id 0 57 miss% 0.02162814441360963
plot_id,batch_id 0 58 miss% 0.027364833686554006
plot_id,batch_id 0 59 miss% 0.026885221198073427
plot_id,batch_id 0 60 miss% 0.044985977480189204
plot_id,batch_id 0 61 miss% 0.02818723722457196
plot_id,batch_id 0 62 miss% 0.02334331771805513
plot_id,batch_id 0 63 miss% 0.031296183230593064
plot_id,batch_id 0 64 miss% 0.03506609487139363
plot_id,batch_id 0 65 miss% 0.04590000450372361
plot_id,batch_id 0 66 miss% 0.06039481077795866
plot_id,batch_id 0 67 miss% 0.02802751296882498
plot_id,batch_id 0 68 miss% 0.036739947754174405
plot_id,batch_id 0 69 miss% 0.027889677255857767
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  79249
Epoch:0, Train loss:0.608245, valid loss:0.581685
Epoch:1, Train loss:0.050136, valid loss:0.009821
Epoch:2, Train loss:0.012816, valid loss:0.006423
Epoch:3, Train loss:0.009210, valid loss:0.004378
Epoch:4, Train loss:0.007272, valid loss:0.003485
Epoch:5, Train loss:0.006284, valid loss:0.003334
Epoch:6, Train loss:0.005275, valid loss:0.002975
Epoch:7, Train loss:0.004950, valid loss:0.003135
Epoch:8, Train loss:0.004569, valid loss:0.002923
Epoch:9, Train loss:0.004324, valid loss:0.002144
Epoch:10, Train loss:0.003999, valid loss:0.002369
Epoch:11, Train loss:0.002776, valid loss:0.001709
Epoch:12, Train loss:0.002736, valid loss:0.001463
Epoch:13, Train loss:0.002659, valid loss:0.002421
Epoch:14, Train loss:0.002786, valid loss:0.001820
Epoch:15, Train loss:0.002618, valid loss:0.001594
Epoch:16, Train loss:0.002626, valid loss:0.001715
Epoch:17, Train loss:0.002437, valid loss:0.001344
Epoch:18, Train loss:0.002517, valid loss:0.001907
Epoch:19, Train loss:0.002367, valid loss:0.001602
Epoch:20, Train loss:0.002347, valid loss:0.001478
Epoch:21, Train loss:0.001746, valid loss:0.001163
Epoch:22, Train loss:0.001693, valid loss:0.001131
Epoch:23, Train loss:0.001692, valid loss:0.001168
Epoch:24, Train loss:0.001710, valid loss:0.001010
Epoch:25, Train loss:0.001630, valid loss:0.001018
Epoch:26, Train loss:0.001652, valid loss:0.001088
Epoch:27, Train loss:0.001610, valid loss:0.001072
Epoch:28, Train loss:0.001637, valid loss:0.001190
Epoch:29, Train loss:0.001576, valid loss:0.001056
Epoch:30, Train loss:0.001535, valid loss:0.001066
Epoch:31, Train loss:0.001292, valid loss:0.000925
Epoch:32, Train loss:0.001264, valid loss:0.000948
Epoch:33, Train loss:0.001271, valid loss:0.000953
Epoch:34, Train loss:0.001268, valid loss:0.001018
Epoch:35, Train loss:0.001263, valid loss:0.000879
Epoch:36, Train loss:0.001240, valid loss:0.000923
Epoch:37, Train loss:0.001266, valid loss:0.001093
Epoch:38, Train loss:0.001222, valid loss:0.000955
Epoch:39, Train loss:0.001236, valid loss:0.000909
Epoch:40, Train loss:0.001206, valid loss:0.000920
Epoch:41, Train loss:0.001085, valid loss:0.000891
Epoch:42, Train loss:0.001079, valid loss:0.000977
Epoch:43, Train loss:0.001073, valid loss:0.000966
Epoch:44, Train loss:0.001061, valid loss:0.000849
Epoch:45, Train loss:0.001064, valid loss:0.000858
Epoch:46, Train loss:0.001060, valid loss:0.000866
Epoch:47, Train loss:0.001058, valid loss:0.000834
Epoch:48, Train loss:0.001049, valid loss:0.000868
Epoch:49, Train loss:0.001051, valid loss:0.001167
Epoch:50, Train loss:0.001042, valid loss:0.000903
Epoch:51, Train loss:0.000989, valid loss:0.000833
Epoch:52, Train loss:0.000983, valid loss:0.000842
Epoch:53, Train loss:0.000984, valid loss:0.000826
Epoch:54, Train loss:0.000981, valid loss:0.000824
Epoch:55, Train loss:0.000976, valid loss:0.000838
Epoch:56, Train loss:0.000978, valid loss:0.000849
Epoch:57, Train loss:0.000972, valid loss:0.000871
Epoch:58, Train loss:0.000969, valid loss:0.000816
Epoch:59, Train loss:0.000968, valid loss:0.000816
Epoch:60, Train loss:0.000973, valid loss:0.000849
training time 5399.024660348892
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.025284879657621103
plot_id,batch_id 0 1 miss% 0.031125891529865574
plot_id,batch_id 0 2 miss% 0.031430421791695945
plot_id,batch_id 0 3 miss% 0.026378049953636077
plot_id,batch_id 0 4 miss% 0.02233457182098224
plot_id,batch_id 0 5 miss% 0.033932971832268954
plot_id,batch_id 0 6 miss% 0.03521297255514238
plot_id,batch_id 0 7 miss% 0.02460798011705195
plot_id,batch_id 0 8 miss% 0.02675707453314991
plot_id,batch_id 0 9 miss% 0.027907648884516672
plot_id,batch_id 0 10 miss% 0.05518883508519224
plot_id,batch_id 0 11 miss% 0.042273898685625505
plot_id,batch_id 0 12 miss% 0.027985918072958044
plot_id,batch_id 0 13 miss% 0.01973381774759893
plot_id,batch_id 0 14 miss% 0.030238632443152565
plot_id,batch_id 0 15 miss% 0.03134308011526893
plot_id,batch_id 0 16 miss% 0.0324979404858542
plot_id,batch_id 0 17 miss% 0.019758727454696413
plot_id,batch_id 0 18 miss% 0.029361882411432318
plot_id,batch_id 0 19 miss% 0.030010184719653776
plot_id,batch_id 0 20 miss% 0.022409226662845824
plot_id,batch_id 0 21 miss% 0.02189795961830661
plot_id,batch_id 0 22 miss% 0.01976166901210678
plot_id,batch_id 0 23 miss% 0.031117135110115223
plot_id,batch_id 0 24 miss% 0.02372544823488502
plot_id,batch_id 0 25 miss% 0.03600298833515766
plot_id,batch_id 0 26 miss% 0.02613703399556998
plot_id,batch_id 0 27 miss% 0.024573692526540476
plot_id,batch_id 0 28 miss% 0.03163328135856258
plot_id,batch_id 0 29 miss% 0.03368470442711722
plot_id,batch_id 0 30 miss% 0.04645365301085946
plot_id,batch_id 0 31 miss% 0.029464672501091197
plot_id,batch_id 0 32 miss% 0.0314330855055806
plot_id,batch_id 0 33 miss% 0.026537337622027875
plot_id,batch_id 0 34 miss% 0.030406531167421855
plot_id,batch_id 0 35 miss% 0.048024001472006875
plot_id,batch_id 0 36 miss% 0.028299391951651636
plot_id,batch_id 0 37 miss% 0.02285009321657695
plot_id,batch_id 0 38 miss% 0.023599250683263545
plot_id,batch_id 0 39 miss% 0.02119482095607319
plot_id,batch_id 0 40 miss% 0.07264014167577525
plot_id,batch_id 0 41 miss% 0.023006029152745286
plot_id,batch_id 0 42 miss% 0.013598086913600387
plot_id,batch_id 0 43 miss% 0.03905960908632229
plot_id,batch_id 0 44 miss% 0.031838036053247504
plot_id,batch_id 0 45 miss% 0.03389561657460297
plot_id,batch_id 0 46 miss% 0.025086374393784267
plot_id,batch_id 0 47 miss% 0.02567131937311751
plot_id,batch_id 0 48 miss% 0.029688808155923955
plot_id,batch_id 0 49 miss% 0.032499828211190344
plot_id,batch_id 0 50 miss% 0.03217540202016568
plot_id,batch_id 0 51 miss% 0.02357629441347576
plot_id,batch_id 0 52 miss% 0.024137974995028413
plot_id,batch_id 0 53 miss% 0.016155426109129328
plot_id,batch_id 0 54 miss% 0.03174226802731762
plot_id,batch_id 0 55 miss% 0.041605018486242686
plot_id,batch_id 0 56 miss% 0.03307803920483099
plot_id,batch_id 0 57 miss% 0.021807935319516896
plot_id,batch_id 0 58 miss% 0.03111198886046728
plot_id,batch_id 0 59 miss% 0.028824735492681237
plot_id,batch_id 0 60 miss% 0.0410967218883492
plot_id,batch_id 0 61 miss% 0.030211200892424378
plot_id,batch_id 0 62 miss% 0.020334733487994423
plot_id,batch_id 0 63 miss% 0.03220719788574391
plot_id,batch_id 0 64 miss% 0.023861740983499084
plot_id,batch_id 0 65 miss% 0.04005244584593699
plot_id,batch_id 0 66 miss% 0.03470430120000793
plot_id,batch_id 0 67 miss% 0.025252694863456347
plot_id,batch_id 0 68 miss% 0.031981715156824554
plot_id,batch_id 0 69 miss% 0.03533477120489694
plot_id,batch_id 0 70 miss% 0.044794092926909744
plot_id,batch_id 0 71 miss% 0.06632645543834075
plot_id,batch_id 0 72 miss% 0.03989067482325447
plot_id,batch_id 0 73 miss% 0.031718113007868795
plot_id,batch_id 0 74 miss% 0.04221073783760302
plot_id,batch_id 0 75 miss% 0.055027636294306875
plot_id,batch_id 0 76 miss% 0.054327130299702535
plot_id,batch_id 0 77 miss% 0.04288881621966143
plot_id,batch_id 0 78 miss% 0.0394273834196566
plot_id,batch_id 0 79 miss% 0.04961283467714162
plot_id,batch_id 0 80 miss% 0.056295175353709016
plot_id,batch_id 0 81 miss% 0.03210748443652995
plot_id,batch_id 0 82 miss% 0.04368315498621505
plot_id,batch_id 0 83 miss% 0.026592491680885698
plot_id,batch_id 0 84 miss% 0.025284824992243334
plot_id,batch_id 0 85 miss% 0.037725132347710494
plot_id,batch_id 0 86 miss% 0.035026871016270895
plot_id,batch_id 0 87 miss% 0.030628094715641138
plot_id,batch_id 0 88 miss% 0.0265286455675339
plot_id,batch_id 0 89 miss% 0.03585972536478355
plot_id,batch_id 0 90 miss% 0.06468375319983825
plot_id,batch_id 0 91 miss% 0.03875208412317272
plot_id,batch_id 0 92 miss% 0.047665017350022904
plot_id,batch_id 0 93 miss% 0.03808119557192478
plot_id,batch_id 0 94 miss% 0.037010011589064196
plot_id,batch_id 0 95 miss% 0.05669855459951998
plot_id,batch_id 0 96 miss% 0.034455101593936166
plot_id,batch_id 0 97 miss% 0.04714212548920897
plot_id,batch_id 0 98 miss% 0.025824670672151745
plot_id,batch_id 0 99 miss% 0.041038154564311216
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03713064 0.02795472 0.02479437 0.03264796 0.03714659 0.03916357
 0.03089425 0.02776214 0.03162418 0.01727895 0.03643096 0.03196245
 0.03918803 0.04585112 0.03213435 0.04126583 0.02819562 0.04001786
 0.0503433  0.03610759 0.05306799 0.02150133 0.03958624 0.02722318
 0.03937458 0.02204766 0.03018223 0.0262475  0.03603716 0.02514473
 0.03868731 0.03344928 0.03795301 0.02412621 0.03139069 0.04272666
 0.0729632  0.02156814 0.02435908 0.01621934 0.05681619 0.02580039
 0.02471745 0.03803987 0.02687342 0.02866777 0.0213023  0.02896291
 0.02660853 0.02306999 0.03322813 0.02640864 0.02539268 0.01646061
 0.02780181 0.03821091 0.03192194 0.02162814 0.02736483 0.02688522
 0.04498598 0.02818724 0.02334332 0.03129618 0.03506609 0.0459
 0.06039481 0.02802751 0.03673995 0.02788968 0.04479409 0.06632646
 0.03989067 0.03171811 0.04221074 0.05502764 0.05432713 0.04288882
 0.03942738 0.04961283 0.05629518 0.03210748 0.04368315 0.02659249
 0.02528482 0.03772513 0.03502687 0.03062809 0.02652865 0.03585973
 0.06468375 0.03875208 0.04766502 0.0380812  0.03701001 0.05669855
 0.0344551  0.04714213 0.02582467 0.04103815]
for model  38 the mean error 0.03537048653541339
all id 38 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 21 out win 5 err 0.03537048653541339
Launcher: Job 39 completed in 5597 seconds.
Launcher: Task 22 done. Exiting.
plot_id,batch_id 0 70 miss% 0.03598213225316931
plot_id,batch_id 0 71 miss% 0.043552443505294906
plot_id,batch_id 0 72 miss% 0.03816994459226475
plot_id,batch_id 0 73 miss% 0.03245125078698111
plot_id,batch_id 0 74 miss% 0.02363736568783996
plot_id,batch_id 0 75 miss% 0.047076084083505984
plot_id,batch_id 0 76 miss% 0.049293061790951616
plot_id,batch_id 0 77 miss% 0.036072884764342575
plot_id,batch_id 0 78 miss% 0.03727686676666344
plot_id,batch_id 0 79 miss% 0.0485051476474578
plot_id,batch_id 0 80 miss% 0.03847940090996435
plot_id,batch_id 0 81 miss% 0.0324637610794648
plot_id,batch_id 0 82 miss% 0.020712286358640374
plot_id,batch_id 0 83 miss% 0.02928518812770835
plot_id,batch_id 0 84 miss% 0.026559326959556616
plot_id,batch_id 0 85 miss% 0.03483986322664509
plot_id,batch_id 0 86 miss% 0.030229048441689714
plot_id,batch_id 0 87 miss% 0.033824285043227986
plot_id,batch_id 0 88 miss% 0.03506440245975129
plot_id,batch_id 0 89 miss% 0.03600095497693093
plot_id,batch_id 0 90 miss% 0.03979661456237508
plot_id,batch_id 0 91 miss% 0.03204255721494254
plot_id,batch_id 0 92 miss% 0.020447835203895703
plot_id,batch_id 0 93 miss% 0.027401522200219082
plot_id,batch_id 0 94 miss% 0.042233806609141264
plot_id,batch_id 0 95 miss% 0.049660280319531194
plot_id,batch_id 0 96 miss% 0.03404729676177281
plot_id,batch_id 0 97 miss% 0.04752769522694711
plot_id,batch_id 0 98 miss% 0.02744813290559416
plot_id,batch_id 0 99 miss% 0.023559628415210955
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02528488 0.03112589 0.03143042 0.02637805 0.02233457 0.03393297
 0.03521297 0.02460798 0.02675707 0.02790765 0.05518884 0.0422739
 0.02798592 0.01973382 0.03023863 0.03134308 0.03249794 0.01975873
 0.02936188 0.03001018 0.02240923 0.02189796 0.01976167 0.03111714
 0.02372545 0.03600299 0.02613703 0.02457369 0.03163328 0.0336847
 0.04645365 0.02946467 0.03143309 0.02653734 0.03040653 0.048024
 0.02829939 0.02285009 0.02359925 0.02119482 0.07264014 0.02300603
 0.01359809 0.03905961 0.03183804 0.03389562 0.02508637 0.02567132
 0.02968881 0.03249983 0.0321754  0.02357629 0.02413797 0.01615543
 0.03174227 0.04160502 0.03307804 0.02180794 0.03111199 0.02882474
 0.04109672 0.0302112  0.02033473 0.0322072  0.02386174 0.04005245
 0.0347043  0.02525269 0.03198172 0.03533477 0.03598213 0.04355244
 0.03816994 0.03245125 0.02363737 0.04707608 0.04929306 0.03607288
 0.03727687 0.04850515 0.0384794  0.03246376 0.02071229 0.02928519
 0.02655933 0.03483986 0.03022905 0.03382429 0.0350644  0.03600095
 0.03979661 0.03204256 0.02044784 0.02740152 0.04223381 0.04966028
 0.0340473  0.0475277  0.02744813 0.02355963]
for model  33 the mean error 0.03162448882075104
all id 33 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 21 out win 3 err 0.03162448882075104
Launcher: Job 34 completed in 5601 seconds.
Launcher: Task 1 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  28945
Epoch:0, Train loss:0.472961, valid loss:0.467997
Epoch:1, Train loss:0.035151, valid loss:0.006000
Epoch:2, Train loss:0.009409, valid loss:0.004466
Epoch:3, Train loss:0.007331, valid loss:0.003496
Epoch:4, Train loss:0.006386, valid loss:0.003428
Epoch:5, Train loss:0.005328, valid loss:0.003503
Epoch:6, Train loss:0.004782, valid loss:0.003201
Epoch:7, Train loss:0.004576, valid loss:0.002508
Epoch:8, Train loss:0.004191, valid loss:0.002252
Epoch:9, Train loss:0.003990, valid loss:0.002180
Epoch:10, Train loss:0.003879, valid loss:0.002661
Epoch:11, Train loss:0.002846, valid loss:0.001545
Epoch:12, Train loss:0.002896, valid loss:0.001409
Epoch:13, Train loss:0.002787, valid loss:0.001746
Epoch:14, Train loss:0.002796, valid loss:0.001490
Epoch:15, Train loss:0.002742, valid loss:0.001719
Epoch:16, Train loss:0.002634, valid loss:0.001595
Epoch:17, Train loss:0.002587, valid loss:0.001408
Epoch:18, Train loss:0.002609, valid loss:0.001272
Epoch:19, Train loss:0.002558, valid loss:0.001682
Epoch:20, Train loss:0.002462, valid loss:0.001209
Epoch:21, Train loss:0.001977, valid loss:0.001157
Epoch:22, Train loss:0.001976, valid loss:0.001234
Epoch:23, Train loss:0.002005, valid loss:0.001110
Epoch:24, Train loss:0.001929, valid loss:0.001107
Epoch:25, Train loss:0.001973, valid loss:0.001183
Epoch:26, Train loss:0.001948, valid loss:0.001036
Epoch:27, Train loss:0.001968, valid loss:0.001069
Epoch:28, Train loss:0.001886, valid loss:0.001187
Epoch:29, Train loss:0.001880, valid loss:0.001001
Epoch:30, Train loss:0.001866, valid loss:0.001041
Epoch:31, Train loss:0.001608, valid loss:0.000908
Epoch:32, Train loss:0.001602, valid loss:0.000995
Epoch:33, Train loss:0.001623, valid loss:0.000963
Epoch:34, Train loss:0.001583, valid loss:0.000952
Epoch:35, Train loss:0.001586, valid loss:0.000943
Epoch:36, Train loss:0.001565, valid loss:0.001046
Epoch:37, Train loss:0.001577, valid loss:0.000928
Epoch:38, Train loss:0.001582, valid loss:0.000909
Epoch:39, Train loss:0.001557, valid loss:0.000933
Epoch:40, Train loss:0.001557, valid loss:0.001005
Epoch:41, Train loss:0.001419, valid loss:0.000857
Epoch:42, Train loss:0.001414, valid loss:0.000840
Epoch:43, Train loss:0.001409, valid loss:0.000857
Epoch:44, Train loss:0.001417, valid loss:0.000857
Epoch:45, Train loss:0.001403, valid loss:0.000864
Epoch:46, Train loss:0.001396, valid loss:0.000867
Epoch:47, Train loss:0.001395, valid loss:0.000859
Epoch:48, Train loss:0.001401, valid loss:0.000865
Epoch:49, Train loss:0.001390, valid loss:0.000834
Epoch:50, Train loss:0.001378, valid loss:0.000842
Epoch:51, Train loss:0.001325, valid loss:0.000824
Epoch:52, Train loss:0.001321, valid loss:0.000825
Epoch:53, Train loss:0.001324, valid loss:0.000810
Epoch:54, Train loss:0.001321, valid loss:0.000809
Epoch:55, Train loss:0.001320, valid loss:0.000833
Epoch:56, Train loss:0.001329, valid loss:0.000821
Epoch:57, Train loss:0.001317, valid loss:0.000822
Epoch:58, Train loss:0.001313, valid loss:0.000836
Epoch:59, Train loss:0.001306, valid loss:0.000834
Epoch:60, Train loss:0.001307, valid loss:0.000823
training time 5428.180290222168
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.03357649961438923
plot_id,batch_id 0 1 miss% 0.029363205969864387
plot_id,batch_id 0 2 miss% 0.027853422266063414
plot_id,batch_id 0 3 miss% 0.025183822441454624
plot_id,batch_id 0 4 miss% 0.024961939123248938
plot_id,batch_id 0 5 miss% 0.04710836838027741
plot_id,batch_id 0 6 miss% 0.027800657080969862
plot_id,batch_id 0 7 miss% 0.032554401672455005
plot_id,batch_id 0 8 miss% 0.030264096832007804
plot_id,batch_id 0 9 miss% 0.029392152598414458
plot_id,batch_id 0 10 miss% 0.04744023353107701
plot_id,batch_id 0 11 miss% 0.035085071034040474
plot_id,batch_id 0 12 miss% 0.026982208903576967
plot_id,batch_id 0 13 miss% 0.02580758835245026
plot_id,batch_id 0 14 miss% 0.027578477985112756
plot_id,batch_id 0 15 miss% 0.05231672996799979
plot_id,batch_id 0 16 miss% 0.03496294184649131
plot_id,batch_id 0 17 miss% 0.044490239983616106
plot_id,batch_id 0 18 miss% 0.04048659619815598
plot_id,batch_id 0 19 miss% 0.046405915249647045
plot_id,batch_id 0 20 miss% 0.04748825253141604
plot_id,batch_id 0 21 miss% 0.02590399034068132
plot_id,batch_id 0 22 miss% 0.027610023800972546
plot_id,batch_id 0 23 miss% 0.027650413845411195
plot_id,batch_id 0 24 miss% 0.0340246632527123
plot_id,batch_id 0 25 miss% 0.042560972322895414
plot_id,batch_id 0 26 miss% 0.018750556992495313
plot_id,batch_id 0 27 miss% 0.02826394034226734
plot_id,batch_id 0 28 miss% 0.03833674429804791
plot_id,batch_id 0 29 miss% 0.0358687943915669
plot_id,batch_id 0 30 miss% 0.03994067692641596
plot_id,batch_id 0 31 miss% 0.03328337174648734
plot_id,batch_id 0 32 miss% 0.040855136682089815
plot_id,batch_id 0 33 miss% 0.03752526198842279
plot_id,batch_id 0 34 miss% 0.040061502122399166
plot_id,batch_id 0 35 miss% 0.0629338933763766
plot_id,batch_id 0 36 miss% 0.04232282518982837
plot_id,batch_id 0 37 miss% 0.04192601359188138
plot_id,batch_id 0 38 miss% 0.03555377011652993
plot_id,batch_id 0 39 miss% 0.031102593618132248
plot_id,batch_id 0 40 miss% 0.06366564794684913
plot_id,batch_id 0 41 miss% 0.019851468066949138
plot_id,batch_id 0 42 miss% 0.0159407538797038
plot_id,batch_id 0 43 miss% 0.03166801433883434
plot_id,batch_id 0 44 miss% 0.018859223446331022
plot_id,batch_id 0 45 miss% 0.038823104955884244
plot_id,batch_id 0 46 miss% 0.02188159251737575
plot_id,batch_id 0 47 miss% 0.021814096843665007
plot_id,batch_id 0 48 miss% 0.024774253607566892
plot_id,batch_id 0 49 miss% 0.019047537550851753
plot_id,batch_id 0 50 miss% 0.038835439489308225
plot_id,batch_id 0 51 miss% 0.02369496498944735
plot_id,batch_id 0 52 miss% 0.024525712995778822
plot_id,batch_id 0 53 miss% 0.017530302475630904
plot_id,batch_id 0 54 miss% 0.027447096776799643
plot_id,batch_id 0 55 miss% 0.02670886142914404
plot_id,batch_id 0 56 miss% 0.027274391729182598
plot_id,batch_id 0 57 miss% 0.025504126938432264
plot_id,batch_id 0 58 miss% 0.028970741968314978
plot_id,batch_id 0 59 miss% 0.023151326802594945
plot_id,batch_id 0 60 miss% 0.04836734721433612
plot_id,batch_id 0 61 miss% 0.02600165768533466
plot_id,batch_id 0 62 miss% 0.029808862249602455
plot_id,batch_id 0 63 miss% 0.022091687103309384
plot_id,batch_id 0 64 miss% 0.02915501965971802
plot_id,batch_id 0 65 miss% 0.051968133139957184
plot_id,batch_id 0 66 miss% 0.03548386291830711
plot_id,batch_id 0 67 miss% 0.03160283856339095
plot_id,batch_id 0 68 miss% 0.038543710152068714
plot_id,batch_id 0 69 miss% 0.024286990758024134
plot_id,batch_id 0 70 miss% 0.03306568540397374
plot_id,batch_id 0 71 miss% 0.05006110971533781
plot_id,batch_id 0 72 miss% 0.029031313982897097
plot_id,batch_id 0 73 miss% 0.032912801781856196
plot_id,batch_id 0 74 miss% 0.034456884329845776
plot_id,batch_id 0 75 miss% 0.05638062891765676
plot_id,batch_id 0 76 miss% 0.04170920788386648
plot_id,batch_id 0 77 miss% 0.038594410719369386
plot_id,batch_id 0 78 miss% 0.027825625643149277
plot_id,batch_id 0 79 miss% 0.04606076554449204
plot_id,batch_id 0 80 miss% 0.03832594530290206
plot_id,batch_id 0 81 miss% 0.03119615587398101
plot_id,batch_id 0 82 miss% 0.016296968159188453
plot_id,batch_id 0 83 miss% 0.027043802316208867
plot_id,batch_id 0 84 miss% 0.029899920316889254
plot_id,batch_id 0 85 miss% 0.06139342177046823
plot_id,batch_id 0 86 miss% 0.03344406886320826
plot_id,batch_id 0 87 miss% 0.02873180096928772
plot_id,batch_id 0 88 miss% 0.03360189097846195
plot_id,batch_id 0 89 miss% 0.030281602060801778
plot_id,batch_id 0 90 miss% 0.036612703219497805
plot_id,batch_id 0 91 miss% 0.04952304465179654
plot_id,batch_id 0 92 miss% 0.02375307685239861
plot_id,batch_id 0 93 miss% 0.03673475282035992
plot_id,batch_id 0 94 miss% 0.039659968288625586
plot_id,batch_id 0 95 miss% 0.06694281196781461
plot_id,batch_id 0 96 miss% 0.03310606464259411
plot_id,batch_id 0 97 miss% 0.04223112189396745
plot_id,batch_id 0 98 miss% 0.03912134320322647
plot_id,batch_id 0 99 miss% 0.02931562207855354
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0335765  0.02936321 0.02785342 0.02518382 0.02496194 0.04710837
 0.02780066 0.0325544  0.0302641  0.02939215 0.04744023 0.03508507
 0.02698221 0.02580759 0.02757848 0.05231673 0.03496294 0.04449024
 0.0404866  0.04640592 0.04748825 0.02590399 0.02761002 0.02765041
 0.03402466 0.04256097 0.01875056 0.02826394 0.03833674 0.03586879
 0.03994068 0.03328337 0.04085514 0.03752526 0.0400615  0.06293389
 0.04232283 0.04192601 0.03555377 0.03110259 0.06366565 0.01985147
 0.01594075 0.03166801 0.01885922 0.0388231  0.02188159 0.0218141
 0.02477425 0.01904754 0.03883544 0.02369496 0.02452571 0.0175303
 0.0274471  0.02670886 0.02727439 0.02550413 0.02897074 0.02315133
 0.04836735 0.02600166 0.02980886 0.02209169 0.02915502 0.05196813
 0.03548386 0.03160284 0.03854371 0.02428699 0.03306569 0.05006111
 0.02903131 0.0329128  0.03445688 0.05638063 0.04170921 0.03859441
 0.02782563 0.04606077 0.03832595 0.03119616 0.01629697 0.0270438
 0.02989992 0.06139342 0.03344407 0.0287318  0.03360189 0.0302816
 0.0366127  0.04952304 0.02375308 0.03673475 0.03965997 0.06694281
 0.03310606 0.04223112 0.03912134 0.02931562]
for model  118 the mean error 0.03416171254853713
all id 118 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 25 out win 4 err 0.03416171254853713
Launcher: Job 119 completed in 5629 seconds.
Launcher: Task 48 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  46193
Epoch:0, Train loss:0.668440, valid loss:0.637867
Epoch:1, Train loss:0.517830, valid loss:0.522351
Epoch:2, Train loss:0.067282, valid loss:0.006172
Epoch:3, Train loss:0.011050, valid loss:0.006161
Epoch:4, Train loss:0.009241, valid loss:0.004471
Epoch:5, Train loss:0.008022, valid loss:0.004225
Epoch:6, Train loss:0.007291, valid loss:0.003835
Epoch:7, Train loss:0.006542, valid loss:0.003676
Epoch:8, Train loss:0.006065, valid loss:0.003524
Epoch:9, Train loss:0.005487, valid loss:0.002783
Epoch:10, Train loss:0.005430, valid loss:0.003186
Epoch:11, Train loss:0.004058, valid loss:0.002102
Epoch:12, Train loss:0.003880, valid loss:0.002354
Epoch:13, Train loss:0.003849, valid loss:0.002367
Epoch:14, Train loss:0.003700, valid loss:0.002269
Epoch:15, Train loss:0.003677, valid loss:0.001916
Epoch:16, Train loss:0.003576, valid loss:0.002191
Epoch:17, Train loss:0.003430, valid loss:0.002118
Epoch:18, Train loss:0.003413, valid loss:0.002475
Epoch:19, Train loss:0.003402, valid loss:0.002202
Epoch:20, Train loss:0.003380, valid loss:0.001880
Epoch:21, Train loss:0.002752, valid loss:0.001593
Epoch:22, Train loss:0.002618, valid loss:0.001679
Epoch:23, Train loss:0.002697, valid loss:0.001768
Epoch:24, Train loss:0.002583, valid loss:0.001557
Epoch:25, Train loss:0.002637, valid loss:0.001602
Epoch:26, Train loss:0.002564, valid loss:0.001747
Epoch:27, Train loss:0.002576, valid loss:0.001717
Epoch:28, Train loss:0.002531, valid loss:0.001659
Epoch:29, Train loss:0.002521, valid loss:0.001630
Epoch:30, Train loss:0.002447, valid loss:0.001522
Epoch:31, Train loss:0.002147, valid loss:0.001513
Epoch:32, Train loss:0.002139, valid loss:0.001494
Epoch:33, Train loss:0.002150, valid loss:0.001545
Epoch:34, Train loss:0.002125, valid loss:0.001477
Epoch:35, Train loss:0.002136, valid loss:0.001741
Epoch:36, Train loss:0.002097, valid loss:0.001455
Epoch:37, Train loss:0.002081, valid loss:0.001494
Epoch:38, Train loss:0.002091, valid loss:0.001468
Epoch:39, Train loss:0.002076, valid loss:0.001437
Epoch:40, Train loss:0.002079, valid loss:0.001489
Epoch:41, Train loss:0.001907, valid loss:0.001379
Epoch:42, Train loss:0.001897, valid loss:0.001352
Epoch:43, Train loss:0.001900, valid loss:0.001374
Epoch:44, Train loss:0.001893, valid loss:0.001402
Epoch:45, Train loss:0.001903, valid loss:0.001394
Epoch:46, Train loss:0.001872, valid loss:0.001346
Epoch:47, Train loss:0.001875, valid loss:0.001399
Epoch:48, Train loss:0.001883, valid loss:0.001358
Epoch:49, Train loss:0.001849, valid loss:0.001375
Epoch:50, Train loss:0.001858, valid loss:0.001388
Epoch:51, Train loss:0.001787, valid loss:0.001376
Epoch:52, Train loss:0.001773, valid loss:0.001384
Epoch:53, Train loss:0.001774, valid loss:0.001327
Epoch:54, Train loss:0.001778, valid loss:0.001351
Epoch:55, Train loss:0.001769, valid loss:0.001363
Epoch:56, Train loss:0.001771, valid loss:0.001345
Epoch:57, Train loss:0.001759, valid loss:0.001333
Epoch:58, Train loss:0.001757, valid loss:0.001328
Epoch:59, Train loss:0.001759, valid loss:0.001319
Epoch:60, Train loss:0.001757, valid loss:0.001353
training time 5444.247843742371
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.035641722333506716
plot_id,batch_id 0 1 miss% 0.03569615224733235
plot_id,batch_id 0 2 miss% 0.027095640453836292
plot_id,batch_id 0 3 miss% 0.027125027604388008
plot_id,batch_id 0 4 miss% 0.022550258810560088
plot_id,batch_id 0 5 miss% 0.03234994055932375
plot_id,batch_id 0 6 miss% 0.03014269957579592
plot_id,batch_id 0 7 miss% 0.028348153787692502
plot_id,batch_id 0 8 miss% 0.03369771998348325
plot_id,batch_id 0 9 miss% 0.019965777282680917
plot_id,batch_id 0 10 miss% 0.037080131300231925
plot_id,batch_id 0 11 miss% 0.047827068997084665
plot_id,batch_id 0 12 miss% 0.03075855979522265
plot_id,batch_id 0 13 miss% 0.02808837858079413
plot_id,batch_id 0 14 miss% 0.032518347035027304
plot_id,batch_id 0 15 miss% 0.04545992548366847
plot_id,batch_id 0 16 miss% 0.031682605170005665
plot_id,batch_id 0 17 miss% 0.0432260292892615
plot_id,batch_id 0 18 miss% 0.0389145799986626
plot_id,batch_id 0 19 miss% 0.04996157135708581
plot_id,batch_id 0 20 miss% 0.03857522783572199
plot_id,batch_id 0 21 miss% 0.022327529907412075
plot_id,batch_id 0 22 miss% 0.04508304866973107
plot_id,batch_id 0 23 miss% 0.014470712115492099
plot_id,batch_id 0 24 miss% 0.01734706349053976
plot_id,batch_id 0 25 miss% 0.026724860476768007
plot_id,batch_id 0 26 miss% 0.024804707734079928
plot_id,batch_id 0 27 miss% 0.02906886455958908
plot_id,batch_id 0 28 miss% 0.02127438432126403
plot_id,batch_id 0 29 miss% 0.024589895495293362
plot_id,batch_id 0 30 miss% 0.04307447915328713
plot_id,batch_id 0 31 miss% 0.03513293922970114
plot_id,batch_id 0 32 miss% 0.0386790680393317
plot_id,batch_id 0 33 miss% 0.03341787583791125
plot_id,batch_id 0 34 miss% 0.02130738943427358
plot_id,batch_id 0 35 miss% 0.057763774887471045
plot_id,batch_id 0 36 miss% 0.053218239458196814
plot_id,batch_id 0 37 miss% 0.03838849015589193
plot_id,batch_id 0 38 miss% 0.03221272001983239
plot_id,batch_id 0 39 miss% 0.029897846010088134
plot_id,batch_id 0 40 miss% 0.075971512936339
plot_id,batch_id 0 41 miss% 0.02532376472140652
plot_id,batch_id 0 42 miss% 0.03027966491848886
plot_id,batch_id 0 43 miss% 0.027955556329270402
plot_id,batch_id 0 44 miss% 0.021648887632365554
plot_id,batch_id 0 45 miss% 0.02660815189703128
plot_id,batch_id 0 46 miss% 0.02296051934505102
plot_id,batch_id 0 47 miss% 0.025647803338710837
plot_id,batch_id 0 48 miss% 0.020124990381728356
plot_id,batch_id 0 49 miss% 0.018735081859337407
plot_id,batch_id 0 50 miss% 0.03450157409276428
plot_id,batch_id 0 51 miss% 0.03040795142626816
plot_id,batch_id 0 52 miss% 0.023733754197571486
plot_id,batch_id 0 53 miss% 0.025991896088029043
plot_id,batch_id 0 54 miss% 0.031816592934395344
plot_id,batch_id 0 55 miss% 0.06161711127871912
plot_id,batch_id 0 56 miss% 0.022980376851467074
plot_id,batch_id 0 57 miss% 0.020550934202740113
plot_id,batch_id 0 58 miss% 0.015427869057089505
plot_id,batch_id 0 59 miss% 0.019143577065685938
plot_id,batch_id 0 60 miss% 0.030213780878631035
plot_id,batch_id 0 61 miss% 0.03209615401394299
plot_id,batch_id 0 62 miss% 0.0327687692298187
plot_id,batch_id 0 63 miss% 0.02460664750104248
plot_id,batch_id 0 64 miss% 0.03996719633829409
plot_id,batch_id 0 65 miss% 0.03703814320718217
plot_id,batch_id 0 66 miss% 0.06338345932375343
plot_id,batch_id 0 67 miss% 0.03164300461244833
plot_id,batch_id 0 68 miss% 0.038567927703502904
plot_id,batch_id 0 69 miss% 0.02674699947357684
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  28945
Epoch:0, Train loss:0.355650, valid loss:0.353806
Epoch:1, Train loss:0.024645, valid loss:0.004623
Epoch:2, Train loss:0.006676, valid loss:0.002719
Epoch:3, Train loss:0.004433, valid loss:0.002407
Epoch:4, Train loss:0.003821, valid loss:0.001964
Epoch:5, Train loss:0.003381, valid loss:0.001887
Epoch:6, Train loss:0.003083, valid loss:0.001561
Epoch:7, Train loss:0.002693, valid loss:0.001437
Epoch:8, Train loss:0.002480, valid loss:0.001499
Epoch:9, Train loss:0.002310, valid loss:0.001310
Epoch:10, Train loss:0.002149, valid loss:0.001055
Epoch:11, Train loss:0.001718, valid loss:0.001019
Epoch:12, Train loss:0.001689, valid loss:0.000947
Epoch:13, Train loss:0.001660, valid loss:0.000964
Epoch:14, Train loss:0.001622, valid loss:0.001196
Epoch:15, Train loss:0.001592, valid loss:0.000791
Epoch:16, Train loss:0.001562, valid loss:0.000864
Epoch:17, Train loss:0.001494, valid loss:0.000767
Epoch:18, Train loss:0.001545, valid loss:0.000769
Epoch:19, Train loss:0.001460, valid loss:0.000825
Epoch:20, Train loss:0.001452, valid loss:0.000841
Epoch:21, Train loss:0.001236, valid loss:0.000776
Epoch:22, Train loss:0.001213, valid loss:0.000748
Epoch:23, Train loss:0.001216, valid loss:0.000718
Epoch:24, Train loss:0.001190, valid loss:0.000751
Epoch:25, Train loss:0.001199, valid loss:0.000663
Epoch:26, Train loss:0.001170, valid loss:0.000697
Epoch:27, Train loss:0.001167, valid loss:0.000764
Epoch:28, Train loss:0.001162, valid loss:0.000725
Epoch:29, Train loss:0.001147, valid loss:0.000705
Epoch:30, Train loss:0.001137, valid loss:0.000623
Epoch:31, Train loss:0.001026, valid loss:0.000601
Epoch:32, Train loss:0.001019, valid loss:0.000579
Epoch:33, Train loss:0.001013, valid loss:0.000689
Epoch:34, Train loss:0.001010, valid loss:0.000618
Epoch:35, Train loss:0.001002, valid loss:0.000588
Epoch:36, Train loss:0.001005, valid loss:0.000586
Epoch:37, Train loss:0.001000, valid loss:0.000600
Epoch:38, Train loss:0.000991, valid loss:0.000604
Epoch:39, Train loss:0.000995, valid loss:0.000600
Epoch:40, Train loss:0.000976, valid loss:0.000597
Epoch:41, Train loss:0.000923, valid loss:0.000552
Epoch:42, Train loss:0.000924, valid loss:0.000566
Epoch:43, Train loss:0.000923, valid loss:0.000551
Epoch:44, Train loss:0.000925, valid loss:0.000556
Epoch:45, Train loss:0.000918, valid loss:0.000541
Epoch:46, Train loss:0.000911, valid loss:0.000551
Epoch:47, Train loss:0.000914, valid loss:0.000573
Epoch:48, Train loss:0.000915, valid loss:0.000566
Epoch:49, Train loss:0.000908, valid loss:0.000555
Epoch:50, Train loss:0.000909, valid loss:0.000555
Epoch:51, Train loss:0.000881, valid loss:0.000547
Epoch:52, Train loss:0.000879, valid loss:0.000560
Epoch:53, Train loss:0.000877, valid loss:0.000547
Epoch:54, Train loss:0.000877, valid loss:0.000539
Epoch:55, Train loss:0.000875, valid loss:0.000557
Epoch:56, Train loss:0.000875, valid loss:0.000550
Epoch:57, Train loss:0.000873, valid loss:0.000539
Epoch:58, Train loss:0.000872, valid loss:0.000544
Epoch:59, Train loss:0.000871, valid loss:0.000546
Epoch:60, Train loss:0.000871, valid loss:0.000532
training time 5465.153886556625
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.03343835312766963
plot_id,batch_id 0 1 miss% 0.02966538146998305
plot_id,batch_id 0 2 miss% 0.03751452663339975
plot_id,batch_id 0 3 miss% 0.02744899638518024
plot_id,batch_id 0 4 miss% 0.02206390203091821
plot_id,batch_id 0 5 miss% 0.030407163451560523
plot_id,batch_id 0 6 miss% 0.035695539522614374
plot_id,batch_id 0 7 miss% 0.037906703187543155
plot_id,batch_id 0 8 miss% 0.023478000071119755
plot_id,batch_id 0 9 miss% 0.027187002475081306
plot_id,batch_id 0 10 miss% 0.07250250193649992
plot_id,batch_id 0 11 miss% 0.03914676483458345
plot_id,batch_id 0 12 miss% 0.03654498062786009
plot_id,batch_id 0 13 miss% 0.02807130423052223
plot_id,batch_id 0 14 miss% 0.029285899227879282
plot_id,batch_id 0 15 miss% 0.03871236138901618
plot_id,batch_id 0 16 miss% 0.042267852070009805
plot_id,batch_id 0 17 miss% 0.043216998548538985
plot_id,batch_id 0 18 miss% 0.031140904434381975
plot_id,batch_id 0 19 miss% 0.028843933946033174
plot_id,batch_id 0 20 miss% 0.02925539635837022
plot_id,batch_id 0 21 miss% 0.02607206707667974
plot_id,batch_id 0 22 miss% 0.01808950835260493
plot_id,batch_id 0 23 miss% 0.01959645916525251
plot_id,batch_id 0 24 miss% 0.026876572970844004
plot_id,batch_id 0 25 miss% 0.05421265353480352
plot_id,batch_id 0 26 miss% 0.03842218018943838
plot_id,batch_id 0 27 miss% 0.025952921284992467
plot_id,batch_id 0 28 miss% 0.018649299421426624
plot_id,batch_id 0 29 miss% 0.025464957963963888
plot_id,batch_id 0 30 miss% 0.06289303155140331
plot_id,batch_id 0 31 miss% 0.038034742135833424
plot_id,batch_id 0 32 miss% 0.02866555871090308
plot_id,batch_id 0 33 miss% 0.025254372240883817
plot_id,batch_id 0 34 miss% 0.025469671636400296
plot_id,batch_id 0 35 miss% 0.045725416682100416
plot_id,batch_id 0 36 miss% 0.03032893138459136
plot_id,batch_id 0 37 miss% 0.020069273419759334
plot_id,batch_id 0 38 miss% 0.029014025981993617
plot_id,batch_id 0 39 miss% 0.025187259983843076
plot_id,batch_id 0 40 miss% 0.06527659165446423
plot_id,batch_id 0 41 miss% 0.013377105695886456
plot_id,batch_id 0 42 miss% 0.01989643352645711
plot_id,batch_id 0 43 miss% 0.026420709917935083
plot_id,batch_id 0 44 miss% 0.02867234905218726
plot_id,batch_id 0 45 miss% 0.029856007483014004
plot_id,batch_id 0 46 miss% 0.020949932188094083
plot_id,batch_id 0 47 miss% 0.025805131811699765
plot_id,batch_id 0 48 miss% 0.02709764948536676
plot_id,batch_id 0 49 miss% 0.01600176871468327
plot_id,batch_id 0 50 miss% 0.02788942533754173
plot_id,batch_id 0 51 miss% 0.030069020063979282
plot_id,batch_id 0 52 miss% 0.025679864997820302
plot_id,batch_id 0 53 miss% 0.016094326904225766
plot_id,batch_id 0 54 miss% 0.034426799497520984
plot_id,batch_id 0 55 miss% 0.03971015151471315
plot_id,batch_id 0 56 miss% 0.0283837117714278
plot_id,batch_id 0 57 miss% 0.017904030512661706
plot_id,batch_id 0 58 miss% 0.028795770430377475
plot_id,batch_id 0 59 miss% 0.02114636066011622
plot_id,batch_id 0 60 miss% 0.0370954446304881
plot_id,batch_id 0 61 miss% 0.024611508584674816
plot_id,batch_id 0 62 miss% 0.017642678078500472
plot_id,batch_id 0 63 miss% 0.03113114929470059
plot_id,batch_id 0 64 miss% 0.0314729563679079
plot_id,batch_id 0 65 miss% 0.033563925865871654
plot_id,batch_id 0 66 miss% 0.04816224705350852
plot_id,batch_id 0 67 miss% 0.030891170878704067
plot_id,batch_id 0 68 miss% 0.02946301556571808
plot_id,batch_id 0 69 miss% 0.02907375649907224
plot_id,batch_id 0 70 miss% 0.026515290400537152
plot_id,batch_id 0 71 miss% 0.03132779609121125
plot_id,batch_id 0 72 miss% 0.025926500281301133
plot_id,batch_id 0 73 miss% 0.023913248189116734
plot_id,batch_id 0 74 miss% 0.046346757115766614
plot_id,batch_id 0 75 miss% 0.04331008786526466
plot_id,batch_id 0 76 miss% 0.03208942189117819
plot_id,batch_id 0 77 miss% 0.03029201325046305
plot_id,batch_id 0 78 miss% 0.028667864077263096
plot_id,batch_id 0 79 miss% 0.05087509298619821
plot_id,batch_id 0 80 miss% 0.049620740882840644
plot_id,batch_id 0 81 miss% 0.04171321266117554
plot_id,batch_id 0 82 miss% 0.03017661070552063
plot_id,batch_id 0 83 miss% 0.031150016909584157
plot_id,batch_id 0 84 miss% 0.018154004021281017
plot_id,batch_id 0 85 miss% 0.03864394917434117
plot_id,batch_id 0 86 miss% 0.026222256103231485
plot_id,batch_id 0 87 miss% 0.03047723648879597
plot_id,batch_id 0 88 miss% 0.02351521064237385
plot_id,batch_id 0 89 miss% 0.03167479201025193
plot_id,batch_id 0 90 miss% 0.034481373010708985
plot_id,batch_id 0 91 miss% 0.03493875251287337
plot_id,batch_id 0 92 miss% 0.03597735800888127
plot_id,batch_id 0 93 miss% 0.04077707582262347
plot_id,batch_id 0 94 miss% 0.03891212304609461
plot_id,batch_id 0 95 miss% 0.04111831618548596
plot_id,batch_id 0 96 miss% 0.03204275375447563
plot_id,batch_id 0 97 miss% 0.046544067785302956
plot_id,batch_id 0 98 miss% 0.024947446953455354
plot_id,batch_id 0 99 miss% 0.027520302871878413
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03343835 0.02966538 0.03751453 0.027449   0.0220639  0.03040716
 0.03569554 0.0379067  0.023478   0.027187   0.0725025  0.03914676
 0.03654498 0.0280713  0.0292859  0.03871236 0.04226785 0.043217
 0.0311409  0.02884393 0.0292554  0.02607207 0.01808951 0.01959646
 0.02687657 0.05421265 0.03842218 0.02595292 0.0186493  0.02546496
 0.06289303 0.03803474 0.02866556 0.02525437 0.02546967 0.04572542
 0.03032893 0.02006927 0.02901403 0.02518726 0.06527659 0.01337711
 0.01989643 0.02642071 0.02867235 0.02985601 0.02094993 0.02580513
 0.02709765 0.01600177 0.02788943 0.03006902 0.02567986 0.01609433
 0.0344268  0.03971015 0.02838371 0.01790403 0.02879577 0.02114636
 0.03709544 0.02461151 0.01764268 0.03113115 0.03147296 0.03356393
 0.04816225 0.03089117 0.02946302 0.02907376 0.02651529 0.0313278
 0.0259265  0.02391325 0.04634676 0.04331009 0.03208942 0.03029201
 0.02866786 0.05087509 0.04962074 0.04171321 0.03017661 0.03115002
 0.018154   0.03864395 0.02622226 0.03047724 0.02351521 0.03167479
 0.03448137 0.03493875 0.03597736 0.04077708 0.03891212 0.04111832
 0.03204275 0.04654407 0.02494745 0.0275203 ]
for model  171 the mean error 0.03182206035385278
all id 171 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 31 out win 3 err 0.03182206035385278
Launcher: Job 172 completed in 5655 seconds.
Launcher: Task 85 done. Exiting.
plot_id,batch_id 0 70 miss% 0.05004570234763205
plot_id,batch_id 0 71 miss% 0.05035291296851083
plot_id,batch_id 0 72 miss% 0.04526180094885653
plot_id,batch_id 0 73 miss% 0.029569465054806808
plot_id,batch_id 0 74 miss% 0.037016919782335164
plot_id,batch_id 0 75 miss% 0.03900845032666611
plot_id,batch_id 0 76 miss% 0.044009234414160846
plot_id,batch_id 0 77 miss% 0.04240198371151965
plot_id,batch_id 0 78 miss% 0.04536309205459742
plot_id,batch_id 0 79 miss% 0.04963277051824353
plot_id,batch_id 0 80 miss% 0.044526329188375534
plot_id,batch_id 0 81 miss% 0.020801965567100573
plot_id,batch_id 0 82 miss% 0.026948246300199916
plot_id,batch_id 0 83 miss% 0.033267691399390016
plot_id,batch_id 0 84 miss% 0.023905250484207916
plot_id,batch_id 0 85 miss% 0.043398775410678575
plot_id,batch_id 0 86 miss% 0.04242084605415613
plot_id,batch_id 0 87 miss% 0.0391454850171204
plot_id,batch_id 0 88 miss% 0.0431141842597432
plot_id,batch_id 0 89 miss% 0.019173454256733845
plot_id,batch_id 0 90 miss% 0.04363076289250001
plot_id,batch_id 0 91 miss% 0.031935142561265534
plot_id,batch_id 0 92 miss% 0.030894434058986595
plot_id,batch_id 0 93 miss% 0.04243448451489943
plot_id,batch_id 0 94 miss% 0.04053664576513563
plot_id,batch_id 0 95 miss% 0.047867325398618675
plot_id,batch_id 0 96 miss% 0.04491531456957694
plot_id,batch_id 0 97 miss% 0.05693856517041859
plot_id,batch_id 0 98 miss% 0.029399808589917528
plot_id,batch_id 0 99 miss% 0.03559383309016619
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03564172 0.03569615 0.02709564 0.02712503 0.02255026 0.03234994
 0.0301427  0.02834815 0.03369772 0.01996578 0.03708013 0.04782707
 0.03075856 0.02808838 0.03251835 0.04545993 0.03168261 0.04322603
 0.03891458 0.04996157 0.03857523 0.02232753 0.04508305 0.01447071
 0.01734706 0.02672486 0.02480471 0.02906886 0.02127438 0.0245899
 0.04307448 0.03513294 0.03867907 0.03341788 0.02130739 0.05776377
 0.05321824 0.03838849 0.03221272 0.02989785 0.07597151 0.02532376
 0.03027966 0.02795556 0.02164889 0.02660815 0.02296052 0.0256478
 0.02012499 0.01873508 0.03450157 0.03040795 0.02373375 0.0259919
 0.03181659 0.06161711 0.02298038 0.02055093 0.01542787 0.01914358
 0.03021378 0.03209615 0.03276877 0.02460665 0.0399672  0.03703814
 0.06338346 0.031643   0.03856793 0.026747   0.0500457  0.05035291
 0.0452618  0.02956947 0.03701692 0.03900845 0.04400923 0.04240198
 0.04536309 0.04963277 0.04452633 0.02080197 0.02694825 0.03326769
 0.02390525 0.04339878 0.04242085 0.03914549 0.04311418 0.01917345
 0.04363076 0.03193514 0.03089443 0.04243448 0.04053665 0.04786733
 0.04491531 0.05693857 0.02939981 0.03559383]
for model  5 the mean error 0.034374599359906936
all id 5 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 21 out win 5 err 0.034374599359906936
Launcher: Job 6 completed in 5661 seconds.
Launcher: Task 18 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  28945
Epoch:0, Train loss:0.472961, valid loss:0.467997
Epoch:1, Train loss:0.035131, valid loss:0.006353
Epoch:2, Train loss:0.010228, valid loss:0.004047
Epoch:3, Train loss:0.007164, valid loss:0.003543
Epoch:4, Train loss:0.005956, valid loss:0.003661
Epoch:5, Train loss:0.005282, valid loss:0.003716
Epoch:6, Train loss:0.004921, valid loss:0.003488
Epoch:7, Train loss:0.004879, valid loss:0.003212
Epoch:8, Train loss:0.004538, valid loss:0.003256
Epoch:9, Train loss:0.004595, valid loss:0.002369
Epoch:10, Train loss:0.004191, valid loss:0.003212
Epoch:11, Train loss:0.003191, valid loss:0.001852
Epoch:12, Train loss:0.003135, valid loss:0.002232
Epoch:13, Train loss:0.003108, valid loss:0.001862
Epoch:14, Train loss:0.003046, valid loss:0.002015
Epoch:15, Train loss:0.003103, valid loss:0.001998
Epoch:16, Train loss:0.002939, valid loss:0.001788
Epoch:17, Train loss:0.002838, valid loss:0.001557
Epoch:18, Train loss:0.002970, valid loss:0.001672
Epoch:19, Train loss:0.002876, valid loss:0.001963
Epoch:20, Train loss:0.002848, valid loss:0.001523
Epoch:21, Train loss:0.002165, valid loss:0.001424
Epoch:22, Train loss:0.002139, valid loss:0.001282
Epoch:23, Train loss:0.002163, valid loss:0.001432
Epoch:24, Train loss:0.002179, valid loss:0.001273
Epoch:25, Train loss:0.002141, valid loss:0.001307
Epoch:26, Train loss:0.002128, valid loss:0.001558
Epoch:27, Train loss:0.002085, valid loss:0.001335
Epoch:28, Train loss:0.002143, valid loss:0.001288
Epoch:29, Train loss:0.002060, valid loss:0.001336
Epoch:30, Train loss:0.002064, valid loss:0.001292
Epoch:31, Train loss:0.001740, valid loss:0.001127
Epoch:32, Train loss:0.001712, valid loss:0.001058
Epoch:33, Train loss:0.001709, valid loss:0.001062
Epoch:34, Train loss:0.001752, valid loss:0.001133
Epoch:35, Train loss:0.001705, valid loss:0.001094
Epoch:36, Train loss:0.001710, valid loss:0.001530
Epoch:37, Train loss:0.001699, valid loss:0.001177
Epoch:38, Train loss:0.001680, valid loss:0.001148
Epoch:39, Train loss:0.001695, valid loss:0.001084
Epoch:40, Train loss:0.001648, valid loss:0.001026
Epoch:41, Train loss:0.001488, valid loss:0.001005
Epoch:42, Train loss:0.001491, valid loss:0.000968
Epoch:43, Train loss:0.001500, valid loss:0.001011
Epoch:44, Train loss:0.001497, valid loss:0.000972
Epoch:45, Train loss:0.001480, valid loss:0.000949
Epoch:46, Train loss:0.001464, valid loss:0.000978
Epoch:47, Train loss:0.001467, valid loss:0.000996
Epoch:48, Train loss:0.001460, valid loss:0.000973
Epoch:49, Train loss:0.001464, valid loss:0.001016
Epoch:50, Train loss:0.001452, valid loss:0.000969
Epoch:51, Train loss:0.001372, valid loss:0.000954
Epoch:52, Train loss:0.001364, valid loss:0.000956
Epoch:53, Train loss:0.001361, valid loss:0.000955
Epoch:54, Train loss:0.001364, valid loss:0.000937
Epoch:55, Train loss:0.001352, valid loss:0.000928
Epoch:56, Train loss:0.001345, valid loss:0.001018
Epoch:57, Train loss:0.001353, valid loss:0.000938
Epoch:58, Train loss:0.001343, valid loss:0.000957
Epoch:59, Train loss:0.001353, valid loss:0.000916
Epoch:60, Train loss:0.001344, valid loss:0.000941
training time 5489.1126046180725
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.029480257037283828
plot_id,batch_id 0 1 miss% 0.02750608965815734
plot_id,batch_id 0 2 miss% 0.028184046432531785
plot_id,batch_id 0 3 miss% 0.02192693172492079
plot_id,batch_id 0 4 miss% 0.029449729209980294
plot_id,batch_id 0 5 miss% 0.03178480623075552
plot_id,batch_id 0 6 miss% 0.028115846786679718
plot_id,batch_id 0 7 miss% 0.029975452263645383
plot_id,batch_id 0 8 miss% 0.030119565355368715
plot_id,batch_id 0 9 miss% 0.02632325430065512
plot_id,batch_id 0 10 miss% 0.04750855998386318
plot_id,batch_id 0 11 miss% 0.042691912652212664
plot_id,batch_id 0 12 miss% 0.03154831534476495
plot_id,batch_id 0 13 miss% 0.025761357201541116
plot_id,batch_id 0 14 miss% 0.027936446817375556
plot_id,batch_id 0 15 miss% 0.047948829326082534
plot_id,batch_id 0 16 miss% 0.04090267404579597
plot_id,batch_id 0 17 miss% 0.05252742990311611
plot_id,batch_id 0 18 miss% 0.032785454192970846
plot_id,batch_id 0 19 miss% 0.032615233261401165
plot_id,batch_id 0 20 miss% 0.04600090334818698
plot_id,batch_id 0 21 miss% 0.030181921174628458
plot_id,batch_id 0 22 miss% 0.0347068810907107
plot_id,batch_id 0 23 miss% 0.02299867730688635
plot_id,batch_id 0 24 miss% 0.026498929191828047
plot_id,batch_id 0 25 miss% 0.03163941901927554
plot_id,batch_id 0 26 miss% 0.032066222265475626
plot_id,batch_id 0 27 miss% 0.024936887760464148
plot_id,batch_id 0 28 miss% 0.029344174784684155
plot_id,batch_id 0 29 miss% 0.02599071334566884
plot_id,batch_id 0 30 miss% 0.026774251933790662
plot_id,batch_id 0 31 miss% 0.03546587048907061
plot_id,batch_id 0 32 miss% 0.03411166556177147
plot_id,batch_id 0 33 miss% 0.025700205433538754
plot_id,batch_id 0 34 miss% 0.026211079786995727
plot_id,batch_id 0 35 miss% 0.029406559815640113
plot_id,batch_id 0 36 miss% 0.042769834171160086
plot_id,batch_id 0 37 miss% 0.0196771140687999
plot_id,batch_id 0 38 miss% 0.03489162305347967
plot_id,batch_id 0 39 miss% 0.033297861680582796
plot_id,batch_id 0 40 miss% 0.07580215980224408
plot_id,batch_id 0 41 miss% 0.027076642047290985
plot_id,batch_id 0 42 miss% 0.022168672820046188
plot_id,batch_id 0 43 miss% 0.042059828398779926
plot_id,batch_id 0 44 miss% 0.018690804854638886
plot_id,batch_id 0 45 miss% 0.03273194836997366
plot_id,batch_id 0 46 miss% 0.030385547223088172
plot_id,batch_id 0 47 miss% 0.024343328654546238
plot_id,batch_id 0 48 miss% 0.02431374344980137
plot_id,batch_id 0 49 miss% 0.025008460989897032
plot_id,batch_id 0 50 miss% 0.04314623877923355
plot_id,batch_id 0 51 miss% 0.026178753927445862
plot_id,batch_id 0 52 miss% 0.023367855575078724
plot_id,batch_id 0 53 miss% 0.019494202597853668
plot_id,batch_id 0 54 miss% 0.028683360223826074
plot_id,batch_id 0 55 miss% 0.030126694919147996
plot_id,batch_id 0 56 miss% 0.02791500583782872
plot_id,batch_id 0 57 miss% 0.03574244067690193
plot_id,batch_id 0 58 miss% 0.03281389984354013
plot_id,batch_id 0 59 miss% 0.031219973906494173
plot_id,batch_id 0 60 miss% 0.049528449402451866
plot_id,batch_id 0 61 miss% 0.02506570727370667
plot_id,batch_id 0 62 miss% 0.03216677747863026
plot_id,batch_id 0 63 miss% 0.027129643793611796
plot_id,batch_id 0 64 miss% 0.03212238945557445
plot_id,batch_id 0 65 miss% 0.05364676164007959
plot_id,batch_id 0 66 miss% 0.04365466539949693
plot_id,batch_id 0 67 miss% 0.03969655987025919
plot_id,batch_id 0 68 miss% 0.03408047035621594
plot_id,batch_id 0the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  79249
Epoch:0, Train loss:0.608245, valid loss:0.581685
Epoch:1, Train loss:0.045843, valid loss:0.007768
Epoch:2, Train loss:0.011745, valid loss:0.005466
Epoch:3, Train loss:0.008780, valid loss:0.004660
Epoch:4, Train loss:0.007387, valid loss:0.003579
Epoch:5, Train loss:0.006462, valid loss:0.003313
Epoch:6, Train loss:0.005691, valid loss:0.003198
Epoch:7, Train loss:0.005085, valid loss:0.002468
Epoch:8, Train loss:0.004603, valid loss:0.002258
Epoch:9, Train loss:0.004263, valid loss:0.002739
Epoch:10, Train loss:0.003915, valid loss:0.001968
Epoch:11, Train loss:0.002826, valid loss:0.001715
Epoch:12, Train loss:0.002804, valid loss:0.001568
Epoch:13, Train loss:0.002687, valid loss:0.001580
Epoch:14, Train loss:0.002653, valid loss:0.001686
Epoch:15, Train loss:0.002596, valid loss:0.002009
Epoch:16, Train loss:0.002503, valid loss:0.001601
Epoch:17, Train loss:0.002381, valid loss:0.001522
Epoch:18, Train loss:0.002351, valid loss:0.001686
Epoch:19, Train loss:0.002316, valid loss:0.001745
Epoch:20, Train loss:0.002206, valid loss:0.001472
Epoch:21, Train loss:0.001780, valid loss:0.001300
Epoch:22, Train loss:0.001731, valid loss:0.001241
Epoch:23, Train loss:0.001718, valid loss:0.001304
Epoch:24, Train loss:0.001722, valid loss:0.001062
Epoch:25, Train loss:0.001686, valid loss:0.001140
Epoch:26, Train loss:0.001705, valid loss:0.001119
Epoch:27, Train loss:0.001620, valid loss:0.001081
Epoch:28, Train loss:0.001619, valid loss:0.001270
Epoch:29, Train loss:0.001615, valid loss:0.001039
Epoch:30, Train loss:0.001574, valid loss:0.001105
Epoch:31, Train loss:0.001377, valid loss:0.001005
Epoch:32, Train loss:0.001342, valid loss:0.000940
Epoch:33, Train loss:0.001361, valid loss:0.000968
Epoch:34, Train loss:0.001330, valid loss:0.000962
Epoch:35, Train loss:0.001324, valid loss:0.000970
Epoch:36, Train loss:0.001309, valid loss:0.000941
Epoch:37, Train loss:0.001303, valid loss:0.001047
Epoch:38, Train loss:0.001299, valid loss:0.000915
Epoch:39, Train loss:0.001294, valid loss:0.000984
Epoch:40, Train loss:0.001277, valid loss:0.000977
Epoch:41, Train loss:0.001182, valid loss:0.000901
Epoch:42, Train loss:0.001181, valid loss:0.000872
Epoch:43, Train loss:0.001168, valid loss:0.000963
Epoch:44, Train loss:0.001174, valid loss:0.000940
Epoch:45, Train loss:0.001162, valid loss:0.000876
Epoch:46, Train loss:0.001158, valid loss:0.000900
Epoch:47, Train loss:0.001153, valid loss:0.000875
Epoch:48, Train loss:0.001152, valid loss:0.000872
Epoch:49, Train loss:0.001149, valid loss:0.000952
Epoch:50, Train loss:0.001143, valid loss:0.000902
Epoch:51, Train loss:0.001096, valid loss:0.000871
Epoch:52, Train loss:0.001093, valid loss:0.000845
Epoch:53, Train loss:0.001092, valid loss:0.000875
Epoch:54, Train loss:0.001090, valid loss:0.000861
Epoch:55, Train loss:0.001085, valid loss:0.000855
Epoch:56, Train loss:0.001084, valid loss:0.000859
Epoch:57, Train loss:0.001083, valid loss:0.000863
Epoch:58, Train loss:0.001078, valid loss:0.000866
Epoch:59, Train loss:0.001078, valid loss:0.000848
Epoch:60, Train loss:0.001076, valid loss:0.000857
training time 5495.734864234924
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.031978222956246014
plot_id,batch_id 0 1 miss% 0.030549142337753656
plot_id,batch_id 0 2 miss% 0.02961381099645034
plot_id,batch_id 0 3 miss% 0.02467393645493933
plot_id,batch_id 0 4 miss% 0.02405390384419161
plot_id,batch_id 0 5 miss% 0.026774238955811703
plot_id,batch_id 0 6 miss% 0.036375566139738455
plot_id,batch_id 0 7 miss% 0.033669480053645895
plot_id,batch_id 0 8 miss% 0.020000727278586556
plot_id,batch_id 0 9 miss% 0.020635402937534346
plot_id,batch_id 0 10 miss% 0.03835530831219113
plot_id,batch_id 0 11 miss% 0.054687282540190875
plot_id,batch_id 0 12 miss% 0.026513322880995668
plot_id,batch_id 0 13 miss% 0.024803304752904803
plot_id,batch_id 0 14 miss% 0.036169670461582086
plot_id,batch_id 0 15 miss% 0.06141803622263948
plot_id,batch_id 0 16 miss% 0.032864425086752976
plot_id,batch_id 0 17 miss% 0.04214054901573944
plot_id,batch_id 0 18 miss% 0.031810702145736315
plot_id,batch_id 0 19 miss% 0.03524686525065911
plot_id,batch_id 0 20 miss% 0.0314347174226089
plot_id,batch_id 0 21 miss% 0.019969230086910806
plot_id,batch_id 0 22 miss% 0.02689832327569208
plot_id,batch_id 0 23 miss% 0.025153587714429732
plot_id,batch_id 0 24 miss% 0.02625500379445065
plot_id,batch_id 0 25 miss% 0.029786140920445254
plot_id,batch_id 0 26 miss% 0.027802896811618436
plot_id,batch_id 0 27 miss% 0.020582464675949146
plot_id,batch_id 0 28 miss% 0.026200609824729923
plot_id,batch_id 0 29 miss% 0.022002321539869626
plot_id,batch_id 0 30 miss% 0.0596874687569574
plot_id,batch_id 0 31 miss% 0.02229330416823955
plot_id,batch_id 0 32 miss% 0.0261624455568801
plot_id,batch_id 0 33 miss% 0.03257739225134647
plot_id,batch_id 0 34 miss% 0.024152425717068424
plot_id,batch_id 0 35 miss% 0.04983697566521496
plot_id,batch_id 0 36 miss% 0.03718006105980847
plot_id,batch_id 0 37 miss% 0.028615603850383187
plot_id,batch_id 0 38 miss% 0.02444465634447527
plot_id,batch_id 0 39 miss% 0.022005292699297456
plot_id,batch_id 0 40 miss% 0.04390669235236548
plot_id,batch_id 0 41 miss% 0.021693733733492985
plot_id,batch_id 0 42 miss% 0.024233863662247914
plot_id,batch_id 0 43 miss% 0.03956450008323043
plot_id,batch_id 0 44 miss% 0.021498142705383937
plot_id,batch_id 0 45 miss% 0.021894919006489006
plot_id,batch_id 0 46 miss% 0.02824867559106726
plot_id,batch_id 0 47 miss% 0.021316755952415226
plot_id,batch_id 0 48 miss% 0.020123341722805135
plot_id,batch_id 0 49 miss% 0.028083799212455247
plot_id,batch_id 0 50 miss% 0.03244010280844062
plot_id,batch_id 0 51 miss% 0.024930182998356937
plot_id,batch_id 0 52 miss% 0.020732036116835717
plot_id,batch_id 0 53 miss% 0.011563157384506537
plot_id,batch_id 0 54 miss% 0.017752528614276446
plot_id,batch_id 0 55 miss% 0.03610053508064261
plot_id,batch_id 0 56 miss% 0.027352153292974572
plot_id,batch_id 0 57 miss% 0.030026703750975475
plot_id,batch_id 0 58 miss% 0.026361750185834656
plot_id,batch_id 0 59 miss% 0.02214605258817701
plot_id,batch_id 0 60 miss% 0.04577770557337234
plot_id,batch_id 0 61 miss% 0.03876061221705712
plot_id,batch_id 0 62 miss% 0.025484788070179218
plot_id,batch_id 0 63 miss% 0.04683431793899402
plot_id,batch_id 0 64 miss% 0.03566487762857058
plot_id,batch_id 0 65 miss% 0.04710017823351722
plot_id,batch_id 0 66 miss% 0.042778559152071044
plot_id,batch_id 0 67 miss% 0.033280772478431364
plot_id,batch_id 0 68 miss% 0.032593428559978245
plot_id,batch_id 0 69 miss%  69 miss% 0.02492071136188804
plot_id,batch_id 0 70 miss% 0.04115358209510784
plot_id,batch_id 0 71 miss% 0.05983201691424528
plot_id,batch_id 0 72 miss% 0.03245775508780792
plot_id,batch_id 0 73 miss% 0.033592104541930845
plot_id,batch_id 0 74 miss% 0.021226100735029365
plot_id,batch_id 0 75 miss% 0.06430957163809957
plot_id,batch_id 0 76 miss% 0.03804545483152084
plot_id,batch_id 0 77 miss% 0.06006443620924618
plot_id,batch_id 0 78 miss% 0.038040658067649454
plot_id,batch_id 0 79 miss% 0.048120372282780106
plot_id,batch_id 0 80 miss% 0.04737760000694987
plot_id,batch_id 0 81 miss% 0.03222928722133087
plot_id,batch_id 0 82 miss% 0.040330122654266456
plot_id,batch_id 0 83 miss% 0.03150485390359414
plot_id,batch_id 0 84 miss% 0.026891261270875523
plot_id,batch_id 0 85 miss% 0.04959116881190086
plot_id,batch_id 0 86 miss% 0.027406961499517286
plot_id,batch_id 0 87 miss% 0.027468660375729783
plot_id,batch_id 0 88 miss% 0.03479531504243851
plot_id,batch_id 0 89 miss% 0.023060200435298335
plot_id,batch_id 0 90 miss% 0.04124803502587653
plot_id,batch_id 0 91 miss% 0.03909286094399007
plot_id,batch_id 0 92 miss% 0.035974620526323324
plot_id,batch_id 0 93 miss% 0.03340372623035298
plot_id,batch_id 0 94 miss% 0.034642764642166465
plot_id,batch_id 0 95 miss% 0.04669426006143264
plot_id,batch_id 0 96 miss% 0.04892769656631255
plot_id,batch_id 0 97 miss% 0.05295787598299151
plot_id,batch_id 0 98 miss% 0.03830034495371924
plot_id,batch_id 0 99 miss% 0.04383871514470774
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02948026 0.02750609 0.02818405 0.02192693 0.02944973 0.03178481
 0.02811585 0.02997545 0.03011957 0.02632325 0.04750856 0.04269191
 0.03154832 0.02576136 0.02793645 0.04794883 0.04090267 0.05252743
 0.03278545 0.03261523 0.0460009  0.03018192 0.03470688 0.02299868
 0.02649893 0.03163942 0.03206622 0.02493689 0.02934417 0.02599071
 0.02677425 0.03546587 0.03411167 0.02570021 0.02621108 0.02940656
 0.04276983 0.01967711 0.03489162 0.03329786 0.07580216 0.02707664
 0.02216867 0.04205983 0.0186908  0.03273195 0.03038555 0.02434333
 0.02431374 0.02500846 0.04314624 0.02617875 0.02336786 0.0194942
 0.02868336 0.03012669 0.02791501 0.03574244 0.0328139  0.03121997
 0.04952845 0.02506571 0.03216678 0.02712964 0.03212239 0.05364676
 0.04365467 0.03969656 0.03408047 0.02492071 0.04115358 0.05983202
 0.03245776 0.0335921  0.0212261  0.06430957 0.03804545 0.06006444
 0.03804066 0.04812037 0.0473776  0.03222929 0.04033012 0.03150485
 0.02689126 0.04959117 0.02740696 0.02746866 0.03479532 0.0230602
 0.04124804 0.03909286 0.03597462 0.03340373 0.03464276 0.04669426
 0.0489277  0.05295788 0.03830034 0.04383872]
for model  145 the mean error 0.03453623069644506
all id 145 hidden_dim 16 learning_rate 0.02 num_layers 4 frames 25 out win 4 err 0.03453623069644506
Launcher: Job 146 completed in 5690 seconds.
Launcher: Task 108 done. Exiting.
0.024590529075700612
plot_id,batch_id 0 70 miss% 0.055034859540087515
plot_id,batch_id 0 71 miss% 0.05941337423790806
plot_id,batch_id 0 72 miss% 0.035218496058505806
plot_id,batch_id 0 73 miss% 0.04180564048169241
plot_id,batch_id 0 74 miss% 0.03428638854824433
plot_id,batch_id 0 75 miss% 0.031388498098239244
plot_id,batch_id 0 76 miss% 0.041158351268394254
plot_id,batch_id 0 77 miss% 0.028900969103521554
plot_id,batch_id 0 78 miss% 0.029418583721199237
plot_id,batch_id 0 79 miss% 0.0440869808752365
plot_id,batch_id 0 80 miss% 0.027215472130457833
plot_id,batch_id 0 81 miss% 0.022628323937720803
plot_id,batch_id 0 82 miss% 0.03568939927951017
plot_id,batch_id 0 83 miss% 0.0244924960728316
plot_id,batch_id 0 84 miss% 0.02182537930296623
plot_id,batch_id 0 85 miss% 0.05470909791104067
plot_id,batch_id 0 86 miss% 0.020113438461583432
plot_id,batch_id 0 87 miss% 0.043920790631214936
plot_id,batch_id 0 88 miss% 0.027660267043526823
plot_id,batch_id 0 89 miss% 0.025651267972768857
plot_id,batch_id 0 90 miss% 0.05828655627394607
plot_id,batch_id 0 91 miss% 0.03009713346366874
plot_id,batch_id 0 92 miss% 0.024674970179230545
plot_id,batch_id 0 93 miss% 0.03813038741691321
plot_id,batch_id 0 94 miss% 0.041458506639513344
plot_id,batch_id 0 95 miss% 0.05987624241890643
plot_id,batch_id 0 96 miss% 0.03647139147528672
plot_id,batch_id 0 97 miss% 0.044100101172315324
plot_id,batch_id 0 98 miss% 0.030887763333498106
plot_id,batch_id 0 99 miss% 0.051861852914041114
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03197822 0.03054914 0.02961381 0.02467394 0.0240539  0.02677424
 0.03637557 0.03366948 0.02000073 0.0206354  0.03835531 0.05468728
 0.02651332 0.0248033  0.03616967 0.06141804 0.03286443 0.04214055
 0.0318107  0.03524687 0.03143472 0.01996923 0.02689832 0.02515359
 0.026255   0.02978614 0.0278029  0.02058246 0.02620061 0.02200232
 0.05968747 0.0222933  0.02616245 0.03257739 0.02415243 0.04983698
 0.03718006 0.0286156  0.02444466 0.02200529 0.04390669 0.02169373
 0.02423386 0.0395645  0.02149814 0.02189492 0.02824868 0.02131676
 0.02012334 0.0280838  0.0324401  0.02493018 0.02073204 0.01156316
 0.01775253 0.03610054 0.02735215 0.0300267  0.02636175 0.02214605
 0.04577771 0.03876061 0.02548479 0.04683432 0.03566488 0.04710018
 0.04277856 0.03328077 0.03259343 0.02459053 0.05503486 0.05941337
 0.0352185  0.04180564 0.03428639 0.0313885  0.04115835 0.02890097
 0.02941858 0.04408698 0.02721547 0.02262832 0.0356894  0.0244925
 0.02182538 0.0547091  0.02011344 0.04392079 0.02766027 0.02565127
 0.05828656 0.03009713 0.02467497 0.03813039 0.04145851 0.05987624
 0.03647139 0.0441001  0.03088776 0.05186185]
for model  6 the mean error 0.032586731964954804
all id 6 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 21 out win 3 err 0.032586731964954804
Launcher: Job 7 completed in 5699 seconds.
Launcher: Task 51 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  28945
Epoch:0, Train loss:0.355650, valid loss:0.353806
Epoch:1, Train loss:0.020891, valid loss:0.003633
Epoch:2, Train loss:0.005205, valid loss:0.002426
Epoch:3, Train loss:0.003843, valid loss:0.002031
Epoch:4, Train loss:0.003116, valid loss:0.001685
Epoch:5, Train loss:0.002816, valid loss:0.001459
Epoch:6, Train loss:0.002644, valid loss:0.001578
Epoch:7, Train loss:0.002495, valid loss:0.001281
Epoch:8, Train loss:0.002421, valid loss:0.001357
Epoch:9, Train loss:0.002304, valid loss:0.001425
Epoch:10, Train loss:0.002257, valid loss:0.001288
Epoch:11, Train loss:0.001660, valid loss:0.001020
Epoch:12, Train loss:0.001653, valid loss:0.000761
Epoch:13, Train loss:0.001630, valid loss:0.000892
Epoch:14, Train loss:0.001608, valid loss:0.000992
Epoch:15, Train loss:0.001568, valid loss:0.000793
Epoch:16, Train loss:0.001545, valid loss:0.000900
Epoch:17, Train loss:0.001532, valid loss:0.000880
Epoch:18, Train loss:0.001531, valid loss:0.000913
Epoch:19, Train loss:0.001499, valid loss:0.001024
Epoch:20, Train loss:0.001458, valid loss:0.000850
Epoch:21, Train loss:0.001177, valid loss:0.000727
Epoch:22, Train loss:0.001189, valid loss:0.000794
Epoch:23, Train loss:0.001167, valid loss:0.000667
Epoch:24, Train loss:0.001172, valid loss:0.000777
Epoch:25, Train loss:0.001163, valid loss:0.000658
Epoch:26, Train loss:0.001152, valid loss:0.000751
Epoch:27, Train loss:0.001143, valid loss:0.000660
Epoch:28, Train loss:0.001149, valid loss:0.000824
Epoch:29, Train loss:0.001125, valid loss:0.000711
Epoch:30, Train loss:0.001147, valid loss:0.000660
Epoch:31, Train loss:0.000977, valid loss:0.000626
Epoch:32, Train loss:0.000962, valid loss:0.000578
Epoch:33, Train loss:0.000967, valid loss:0.000612
Epoch:34, Train loss:0.000965, valid loss:0.000614
Epoch:35, Train loss:0.000958, valid loss:0.000641
Epoch:36, Train loss:0.000971, valid loss:0.000588
Epoch:37, Train loss:0.000951, valid loss:0.000599
Epoch:38, Train loss:0.000954, valid loss:0.000659
Epoch:39, Train loss:0.000947, valid loss:0.000592
Epoch:40, Train loss:0.000946, valid loss:0.000693
Epoch:41, Train loss:0.000871, valid loss:0.000569
Epoch:42, Train loss:0.000868, valid loss:0.000579
Epoch:43, Train loss:0.000867, valid loss:0.000566
Epoch:44, Train loss:0.000870, valid loss:0.000579
Epoch:45, Train loss:0.000866, valid loss:0.000573
Epoch:46, Train loss:0.000861, valid loss:0.000547
Epoch:47, Train loss:0.000855, valid loss:0.000596
Epoch:48, Train loss:0.000857, valid loss:0.000564
Epoch:49, Train loss:0.000858, valid loss:0.000549
Epoch:50, Train loss:0.000851, valid loss:0.000573
Epoch:51, Train loss:0.000818, valid loss:0.000540
Epoch:52, Train loss:0.000815, valid loss:0.000557
Epoch:53, Train loss:0.000817, valid loss:0.000548
Epoch:54, Train loss:0.000816, valid loss:0.000550
Epoch:55, Train loss:0.000815, valid loss:0.000559
Epoch:56, Train loss:0.000814, valid loss:0.000547
Epoch:57, Train loss:0.000811, valid loss:0.000570
Epoch:58, Train loss:0.000810, valid loss:0.000562
Epoch:59, Train loss:0.000810, valid loss:0.000563
Epoch:60, Train loss:0.000811, valid loss:0.000539
training time 5510.227143526077
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.03633571666951055
plot_id,batch_id 0 1 miss% 0.029812159359560636
plot_id,batch_id 0 2 miss% 0.023871763969583838
plot_id,batch_id 0 3 miss% 0.019384537700105144
plot_id,batch_id 0 4 miss% 0.02064871817636668
plot_id,batch_id 0 5 miss% 0.02915983365486876
plot_id,batch_id 0 6 miss% 0.022613770768546912
plot_id,batch_id 0 7 miss% 0.027297520414916055
plot_id,batch_id 0 8 miss% 0.03032934093358363
plot_id,batch_id 0 9 miss% 0.020527489999271103
plot_id,batch_id 0 10 miss% 0.03386761230506402
plot_id,batch_id 0 11 miss% 0.034730941769281994
plot_id,batch_id 0 12 miss% 0.025296174157278632
plot_id,batch_id 0 13 miss% 0.015856367068574205
plot_id,batch_id 0 14 miss% 0.028590636005733756
plot_id,batch_id 0 15 miss% 0.024062939539702177
plot_id,batch_id 0 16 miss% 0.03483545244143279
plot_id,batch_id 0 17 miss% 0.04684984815228211
plot_id,batch_id 0 18 miss% 0.0312076281443142
plot_id,batch_id 0 19 miss% 0.02517013412999192
plot_id,batch_id 0 20 miss% 0.05293139289374871
plot_id,batch_id 0 21 miss% 0.02714341232356565
plot_id,batch_id 0 22 miss% 0.025030853088052522
plot_id,batch_id 0 23 miss% 0.02039732322978225
plot_id,batch_id 0 24 miss% 0.019762838132514792
plot_id,batch_id 0 25 miss% 0.03732871474055029
plot_id,batch_id 0 26 miss% 0.03985898307790689
plot_id,batch_id 0 27 miss% 0.0306577206902907
plot_id,batch_id 0 28 miss% 0.01939859693258926
plot_id,batch_id 0 29 miss% 0.026190660768595414
plot_id,batch_id 0 30 miss% 0.04649303901686232
plot_id,batch_id 0 31 miss% 0.03458606538901239
plot_id,batch_id 0 32 miss% 0.027542698010517654
plot_id,batch_id 0 33 miss% 0.03181449406257623
plot_id,batch_id 0 34 miss% 0.02893407434960672
plot_id,batch_id 0 35 miss% 0.041691014868407476
plot_id,batch_id 0 36 miss% 0.029128646751381854
plot_id,batch_id 0 37 miss% 0.03220294512008877
plot_id,batch_id 0 38 miss% 0.02747316395684853
plot_id,batch_id 0 39 miss% 0.026136393806929316
plot_id,batch_id 0 40 miss% 0.12862641780838632
plot_id,batch_id 0 41 miss% 0.040202545662876614
plot_id,batch_id 0 42 miss% 0.03890955287565309
plot_id,batch_id 0 43 miss% 0.035355563476575126
plot_id,batch_id 0 44 miss% 0.026451420703985543
plot_id,batch_id 0 45 miss% 0.040594452187076975
plot_id,batch_id 0 46 miss% 0.032456417850075615
plot_id,batch_id 0 47 miss% 0.036187830654905224
plot_id,batch_id 0 48 miss% 0.03140022878437774
plot_id,batch_id 0 49 miss% 0.021736587545911068
plot_id,batch_id 0 50 miss% 0.02658251186189313
plot_id,batch_id 0 51 miss% 0.0328883111038674
plot_id,batch_id 0 52 miss% 0.03495124642780756
plot_id,batch_id 0 53 miss% 0.017986792197166944
plot_id,batch_id 0 54 miss% 0.02617599046076997
plot_id,batch_id 0 55 miss% 0.03197799515106263
plot_id,batch_id 0 56 miss% 0.03096067778705008
plot_id,batch_id 0 57 miss% 0.027276578045733375
plot_id,batch_id 0 58 miss% 0.043643717721636305
plot_id,batch_id 0 59 miss% 0.03683080777975439
plot_id,batch_id 0 60 miss% 0.04719394612695862
plot_id,batch_id 0 61 miss% 0.024355106098275785
plot_id,batch_id 0 62 miss% 0.02069995431253105
plot_id,batch_id 0 63 miss% 0.022814544023867338
plot_id,batch_id 0 64 miss% 0.02564410333276847
plot_id,batch_id 0 65 miss% 0.037105619098627994
plot_id,batch_id 0 66 miss% 0.0270389574793156
plot_id,batch_id 0 67 miss% 0.021189460941171512
plot_id,batch_id 0 68 miss% 0.027496224435288687
plot_id,batch_id 0 69 miss% 0.025657696254924787
plot_id,batch_id 0 70 miss% 0.02488841876647776
plot_id,batch_id 0 71 miss% 0.03830655395556071
plot_id,batch_id 0 72 miss% 0.03606849744363179
plot_id,batch_id 0 73 miss% 0.03375150038609562
plot_id,batch_id 0 74 miss% 0.04227072807046888
plot_id,batch_id 0 75 miss% 0.04546606254940201
plot_id,batch_id 0 76 miss% 0.027630412110932465
plot_id,batch_id 0 77 miss% 0.031746834032802845
plot_id,batch_id 0 78 miss% 0.028039663745586103
plot_id,batch_id 0 79 miss% 0.03298877417636695
plot_id,batch_id 0 80 miss% 0.04873091376186594
plot_id,batch_id 0 81 miss% 0.03064874692772269
plot_id,batch_id 0 82 miss% 0.020562224671825804
plot_id,batch_id 0 83 miss% 0.02934674700721606
plot_id,batch_id 0 84 miss% 0.02816182708784915
plot_id,batch_id 0 85 miss% 0.04976472161030752
plot_id,batch_id 0 86 miss% 0.03250635002980342
plot_id,batch_id 0 87 miss% 0.03107823014694873
plot_id,batch_id 0 88 miss% 0.03198932016384695
plot_id,batch_id 0 89 miss% 0.02839390857029835
plot_id,batch_id 0 90 miss% 0.023481598921220967
plot_id,batch_id 0 91 miss% 0.03908171877154409
plot_id,batch_id 0 92 miss% 0.019290436323620454
plot_id,batch_id 0 93 miss% 0.02732158893919411
plot_id,batch_id 0 94 miss% 0.033747087369348415
plot_id,batch_id 0 95 miss% 0.04984635546301955
plot_id,batch_id 0 96 miss% 0.040138657654384124
plot_id,batch_id 0 97 miss% 0.04706899280241143
plot_id,batch_id 0 98 miss% 0.034352487143560415
plot_id,batch_id 0 99 miss% 0.026980666339400914
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03633572 0.02981216 0.02387176 0.01938454 0.02064872 0.02915983
 0.02261377 0.02729752 0.03032934 0.02052749 0.03386761 0.03473094
 0.02529617 0.01585637 0.02859064 0.02406294 0.03483545 0.04684985
 0.03120763 0.02517013 0.05293139 0.02714341 0.02503085 0.02039732
 0.01976284 0.03732871 0.03985898 0.03065772 0.0193986  0.02619066
 0.04649304 0.03458607 0.0275427  0.03181449 0.02893407 0.04169101
 0.02912865 0.03220295 0.02747316 0.02613639 0.12862642 0.04020255
 0.03890955 0.03535556 0.02645142 0.04059445 0.03245642 0.03618783
 0.03140023 0.02173659 0.02658251 0.03288831 0.03495125 0.01798679
 0.02617599 0.031978   0.03096068 0.02727658 0.04364372 0.03683081
 0.04719395 0.02435511 0.02069995 0.02281454 0.0256441  0.03710562
 0.02703896 0.02118946 0.02749622 0.0256577  0.02488842 0.03830655
 0.0360685  0.0337515  0.04227073 0.04546606 0.02763041 0.03174683
 0.02803966 0.03298877 0.04873091 0.03064875 0.02056222 0.02934675
 0.02816183 0.04976472 0.03250635 0.03107823 0.03198932 0.02839391
 0.0234816  0.03908172 0.01929044 0.02732159 0.03374709 0.04984636
 0.04013866 0.04706899 0.03435249 0.02698067]
for model  198 the mean error 0.03219192899702806
all id 198 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 31 out win 3 err 0.03219192899702806
Launcher: Job 199 completed in 5699 seconds.
Launcher: Task 13 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  79249
Epoch:0, Train loss:0.608245, valid loss:0.581685
Epoch:1, Train loss:0.050590, valid loss:0.007959
Epoch:2, Train loss:0.013067, valid loss:0.005853
Epoch:3, Train loss:0.010103, valid loss:0.005097
Epoch:4, Train loss:0.008416, valid loss:0.003679
Epoch:5, Train loss:0.007296, valid loss:0.004415
Epoch:6, Train loss:0.006011, valid loss:0.003466
Epoch:7, Train loss:0.005953, valid loss:0.003320
Epoch:8, Train loss:0.005211, valid loss:0.003077
Epoch:9, Train loss:0.005213, valid loss:0.002597
Epoch:10, Train loss:0.005046, valid loss:0.003307
Epoch:11, Train loss:0.003315, valid loss:0.001779
Epoch:12, Train loss:0.003105, valid loss:0.001949
Epoch:13, Train loss:0.003248, valid loss:0.002125
Epoch:14, Train loss:0.003090, valid loss:0.002333
Epoch:15, Train loss:0.003078, valid loss:0.002372
Epoch:16, Train loss:0.003121, valid loss:0.001972
Epoch:17, Train loss:0.002967, valid loss:0.001578
Epoch:18, Train loss:0.002995, valid loss:0.002044
Epoch:19, Train loss:0.002761, valid loss:0.001536
Epoch:20, Train loss:0.002843, valid loss:0.002016
Epoch:21, Train loss:0.001941, valid loss:0.001194
Epoch:22, Train loss:0.001907, valid loss:0.001315
Epoch:23, Train loss:0.001919, valid loss:0.001201
Epoch:24, Train loss:0.001914, valid loss:0.001303
Epoch:25, Train loss:0.001873, valid loss:0.001297
Epoch:26, Train loss:0.001847, valid loss:0.001225
Epoch:27, Train loss:0.001813, valid loss:0.001204
Epoch:28, Train loss:0.001859, valid loss:0.001398
Epoch:29, Train loss:0.001761, valid loss:0.001312
Epoch:30, Train loss:0.001757, valid loss:0.001184
Epoch:31, Train loss:0.001396, valid loss:0.001089
Epoch:32, Train loss:0.001353, valid loss:0.000968
Epoch:33, Train loss:0.001360, valid loss:0.000935
Epoch:34, Train loss:0.001337, valid loss:0.000937
Epoch:35, Train loss:0.001367, valid loss:0.000985
Epoch:36, Train loss:0.001303, valid loss:0.000946
Epoch:37, Train loss:0.001347, valid loss:0.001043
Epoch:38, Train loss:0.001330, valid loss:0.001041
Epoch:39, Train loss:0.001320, valid loss:0.001032
Epoch:40, Train loss:0.001293, valid loss:0.000908
Epoch:41, Train loss:0.001109, valid loss:0.000873
Epoch:42, Train loss:0.001108, valid loss:0.000919
Epoch:43, Train loss:0.001103, valid loss:0.000923
Epoch:44, Train loss:0.001103, valid loss:0.000919
Epoch:45, Train loss:0.001096, valid loss:0.000895
Epoch:46, Train loss:0.001101, valid loss:0.000922
Epoch:47, Train loss:0.001079, valid loss:0.000892
Epoch:48, Train loss:0.001086, valid loss:0.000876
Epoch:49, Train loss:0.001081, valid loss:0.000912
Epoch:50, Train loss:0.001074, valid loss:0.000905
Epoch:51, Train loss:0.000997, valid loss:0.000865
Epoch:52, Train loss:0.000994, valid loss:0.000846
Epoch:53, Train loss:0.000989, valid loss:0.000860
Epoch:54, Train loss:0.000991, valid loss:0.000843
Epoch:55, Train loss:0.000986, valid loss:0.000864
Epoch:56, Train loss:0.000987, valid loss:0.000865
Epoch:57, Train loss:0.000980, valid loss:0.000829
Epoch:58, Train loss:0.000976, valid loss:0.000856
Epoch:59, Train loss:0.000975, valid loss:0.000847
Epoch:60, Train loss:0.000981, valid loss:0.000828
training time 5541.745772600174
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.02576465222301099
plot_id,batch_id 0 1 miss% 0.025353638053314018
plot_id,batch_id 0 2 miss% 0.030234208119134638
plot_id,batch_id 0 3 miss% 0.01774403066555226
plot_id,batch_id 0 4 miss% 0.01993506667687609
plot_id,batch_id 0 5 miss% 0.030998578867549816
plot_id,batch_id 0 6 miss% 0.030621480772574892
plot_id,batch_id 0 7 miss% 0.030136840297796057
plot_id,batch_id 0 8 miss% 0.019600879706656325
plot_id,batch_id 0 9 miss% 0.023542136334533913
plot_id,batch_id 0 10 miss% 0.04408085843874048
plot_id,batch_id 0 11 miss% 0.04131559201553079
plot_id,batch_id 0 12 miss% 0.026384224255213754
plot_id,batch_id 0 13 miss% 0.034729895484208635
plot_id,batch_id 0 14 miss% 0.023585731469711775
plot_id,batch_id 0 15 miss% 0.025902764551852895
plot_id,batch_id 0 16 miss% 0.026222407818969836
plot_id,batch_id 0 17 miss% 0.035573594931543356
plot_id,batch_id 0 18 miss% 0.03153563507806003
plot_id,batch_id 0 19 miss% 0.01650853795668752
plot_id,batch_id 0 20 miss% 0.0617551227395094
plot_id,batch_id 0 21 miss% 0.022498812915756346
plot_id,batch_id 0 22 miss% 0.025298058985752724
plot_id,batch_id 0 23 miss% 0.029263549612103142
plot_id,batch_id 0 24 miss% 0.025656327497849334
plot_id,batch_id 0 25 miss% 0.04545731749506551
plot_id,batch_id 0 26 miss% 0.026799998937390365
plot_id,batch_id 0 27 miss% 0.02381523056159618
plot_id,batch_id 0 28 miss% 0.02354258794519727
plot_id,batch_id 0 29 miss% 0.020705652145754486
plot_id,batch_id 0 30 miss% 0.0775999978661565
plot_id,batch_id 0 31 miss% 0.037556587673171514
plot_id,batch_id 0 32 miss% 0.026204617165041886
plot_id,batch_id 0 33 miss% 0.021768891957882383
plot_id,batch_id 0 34 miss% 0.01963130571364084
plot_id,batch_id 0 35 miss% 0.05005934929579845
plot_id,batch_id 0 36 miss% 0.042273770892046515
plot_id,batch_id 0 37 miss% 0.029471207735603664
plot_id,batch_id 0 38 miss% 0.03479839161576731
plot_id,batch_id 0 39 miss% 0.01634955137868932
plot_id,batch_id 0 40 miss% 0.06389170726671656
plot_id,batch_id 0 41 miss% 0.02889925346586454
plot_id,batch_id 0 42 miss% 0.030086891394437217
plot_id,batch_id 0 43 miss% 0.043359730346187765
plot_id,batch_id 0 44 miss% 0.02703433091456765
plot_id,batch_id 0 45 miss% 0.017811276186625546
plot_id,batch_id 0 46 miss% 0.015736468924619357
plot_id,batch_id 0 47 miss% 0.02447721317239848
plot_id,batch_id 0 48 miss% 0.02911363847962538
plot_id,batch_id 0 49 miss% 0.022176085295378862
plot_id,batch_id 0 50 miss% 0.033334756426220705
plot_id,batch_id 0 51 miss% 0.026332090401615903
plot_id,batch_id 0 52 miss% 0.026237234970784784
plot_id,batch_id 0 53 miss% 0.02349214185070072
plot_id,batch_id 0 54 miss% 0.03349909049669764
plot_id,batch_id 0 55 miss% 0.0424766349446095
plot_id,batch_id 0 56 miss% 0.027694276463403406
plot_id,batch_id 0 57 miss% 0.027880344437771293
plot_id,batch_id 0 58 miss% 0.023729516346697847
plot_id,batch_id 0 59 miss% 0.019514101126462475
plot_id,batch_id 0 60 miss% 0.044728472722827685
plot_id,batch_id 0 61 miss% 0.023772033931228422
plot_id,batch_id 0 62 miss% 0.034773958669197066
plot_id,batch_id 0 63 miss% 0.028723106276489694
plot_id,batch_id 0 64 miss% 0.021507114716408156
plot_id,batch_id 0 65 miss% 0.04152648976525292
plot_id,batch_id 0 66 miss% 0.050513484945255704
plot_id,batch_id 0 67 miss% 0.018809358775962754
plot_id,batch_id 0 68 miss% 0.025486406609651235
plot_id,batch_id 0 69 miss% 0.026152553909316974
plot_id,batch_id 0 70 miss% 0.03439103613833012
plot_id,batch_id 0 71 miss% 0.06886907987154328
plot_id,batch_id 0 72 miss% 0.037293126740172836
plot_id,batch_id 0 73 miss% 0.02437173748124262
plot_id,batch_id 0 74 miss% 0.03628819562537025
plot_id,batch_id 0 75 miss% 0.06680626102070282
plot_id,batch_id 0 76 miss% 0.036145404666362235
plot_id,batch_id 0 77 miss% 0.03955617730409933
plot_id,batch_id 0 78 miss% 0.03909359078544793
plot_id,batch_id 0 79 miss% 0.0407843397758719
plot_id,batch_id 0 80 miss% 0.04089834481406715
plot_id,batch_id 0 81 miss% 0.029840857491747155
plot_id,batch_id 0 82 miss% 0.02419090197878064
plot_id,batch_id 0 83 miss% 0.03675945518293201
plot_id,batch_id 0 84 miss% 0.02378145316357021
plot_id,batch_id 0 85 miss% 0.04098841203224709
plot_id,batch_id 0 86 miss% 0.02619061837839029
plot_id,batch_id 0 87 miss% 0.019871660656165115
plot_id,batch_id 0 88 miss% 0.02825104656334286
plot_id,batch_id 0 89 miss% 0.029344002430262178
plot_id,batch_id 0 90 miss% 0.03607521967323099
plot_id,batch_id 0 91 miss% 0.025403249180861055
plot_id,batch_id 0 92 miss% 0.025516395892366036
plot_id,batch_id 0 93 miss% 0.01944489038379035
plot_id,batch_id 0 94 miss% 0.03465858804861735
plot_id,batch_id 0 95 miss% 0.04644290226689967
plot_id,batch_id 0 96 miss% 0.023055881313736447
plot_id,batch_id 0 97 miss% 0.04656519880453985
plot_id,batch_id 0 98 miss% 0.029139632403166908
plot_id,batch_id 0 99 miss% 0.025115297255372745
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02576465 0.02535364 0.03023421 0.01774403 0.01993507 0.03099858
 0.03062148 0.03013684 0.01960088 0.02354214 0.04408086 0.04131559
 0.02638422 0.0347299  0.02358573 0.02590276 0.02622241 0.03557359
 0.03153564 0.01650854 0.06175512 0.02249881 0.02529806 0.02926355
 0.02565633 0.04545732 0.0268     0.02381523 0.02354259 0.02070565
 0.0776     0.03755659 0.02620462 0.02176889 0.01963131 0.05005935
 0.04227377 0.02947121 0.03479839 0.01634955 0.06389171 0.02889925
 0.03008689 0.04335973 0.02703433 0.01781128 0.01573647 0.02447721
 0.02911364 0.02217609 0.03333476 0.02633209 0.02623723 0.02349214
 0.03349909 0.04247663 0.02769428 0.02788034 0.02372952 0.0195141
 0.04472847 0.02377203 0.03477396 0.02872311 0.02150711 0.04152649
 0.05051348 0.01880936 0.02548641 0.02615255 0.03439104 0.06886908
 0.03729313 0.02437174 0.0362882  0.06680626 0.0361454  0.03955618
 0.03909359 0.04078434 0.04089834 0.02984086 0.0241909  0.03675946
 0.02378145 0.04098841 0.02619062 0.01987166 0.02825105 0.029344
 0.03607522 0.02540325 0.0255164  0.01944489 0.03465859 0.0464429
 0.02305588 0.0465652  0.02913963 0.0251153 ]
for model  60 the mean error 0.031581798044034964
all id 60 hidden_dim 32 learning_rate 0.02 num_layers 3 frames 21 out win 3 err 0.031581798044034964
Launcher: Job 61 completed in 5742 seconds.
Launcher: Task 94 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  35921
Epoch:0, Train loss:0.778793, valid loss:0.763398
Epoch:1, Train loss:0.061533, valid loss:0.023370
Epoch:2, Train loss:0.021985, valid loss:0.007136
Epoch:3, Train loss:0.012923, valid loss:0.006235
Epoch:4, Train loss:0.011039, valid loss:0.007410
Epoch:5, Train loss:0.009506, valid loss:0.004799
Epoch:6, Train loss:0.008188, valid loss:0.006025
Epoch:7, Train loss:0.007452, valid loss:0.004455
Epoch:8, Train loss:0.006957, valid loss:0.003769
Epoch:9, Train loss:0.006248, valid loss:0.003760
Epoch:10, Train loss:0.006295, valid loss:0.003376
Epoch:11, Train loss:0.004081, valid loss:0.002320
Epoch:12, Train loss:0.004239, valid loss:0.001923
Epoch:13, Train loss:0.004058, valid loss:0.002193
Epoch:14, Train loss:0.003976, valid loss:0.002380
Epoch:15, Train loss:0.004060, valid loss:0.002157
Epoch:16, Train loss:0.003824, valid loss:0.002183
Epoch:17, Train loss:0.004115, valid loss:0.002037
Epoch:18, Train loss:0.003864, valid loss:0.002487
Epoch:19, Train loss:0.003729, valid loss:0.002306
Epoch:20, Train loss:0.003865, valid loss:0.002240
Epoch:21, Train loss:0.002713, valid loss:0.001828
Epoch:22, Train loss:0.002763, valid loss:0.001648
Epoch:23, Train loss:0.002700, valid loss:0.001481
Epoch:24, Train loss:0.002653, valid loss:0.001560
Epoch:25, Train loss:0.002752, valid loss:0.001607
Epoch:26, Train loss:0.002649, valid loss:0.001594
Epoch:27, Train loss:0.002638, valid loss:0.001704
Epoch:28, Train loss:0.002553, valid loss:0.001576
Epoch:29, Train loss:0.002568, valid loss:0.001855
Epoch:30, Train loss:0.002742, valid loss:0.001453
Epoch:31, Train loss:0.002074, valid loss:0.001359
Epoch:32, Train loss:0.002084, valid loss:0.001415
Epoch:33, Train loss:0.002054, valid loss:0.001358
Epoch:34, Train loss:0.002057, valid loss:0.001304
Epoch:35, Train loss:0.002019, valid loss:0.001371
Epoch:36, Train loss:0.002036, valid loss:0.001346
Epoch:37, Train loss:0.002062, valid loss:0.001297
Epoch:38, Train loss:0.002026, valid loss:0.001363
Epoch:39, Train loss:0.001987, valid loss:0.001456
Epoch:40, Train loss:0.002007, valid loss:0.001364
Epoch:41, Train loss:0.001751, valid loss:0.001184
Epoch:42, Train loss:0.001717, valid loss:0.001189
Epoch:43, Train loss:0.001729, valid loss:0.001214
Epoch:44, Train loss:0.001748, valid loss:0.001339
Epoch:45, Train loss:0.001744, valid loss:0.001334
Epoch:46, Train loss:0.001702, valid loss:0.001333
Epoch:47, Train loss:0.001706, valid loss:0.001337
Epoch:48, Train loss:0.001712, valid loss:0.001243
Epoch:49, Train loss:0.001708, valid loss:0.001175
Epoch:50, Train loss:0.001682, valid loss:0.001284
Epoch:51, Train loss:0.001570, valid loss:0.001169
Epoch:52, Train loss:0.001564, valid loss:0.001153
Epoch:53, Train loss:0.001558, valid loss:0.001170
Epoch:54, Train loss:0.001557, valid loss:0.001157
Epoch:55, Train loss:0.001557, valid loss:0.001165
Epoch:56, Train loss:0.001551, valid loss:0.001123
Epoch:57, Train loss:0.001558, valid loss:0.001163
Epoch:58, Train loss:0.001548, valid loss:0.001139
Epoch:59, Train loss:0.001540, valid loss:0.001170
Epoch:60, Train loss:0.001535, valid loss:0.001167
training time 5545.89021396637
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.023666006191837716
plot_id,batch_id 0 1 miss% 0.01957321592105867
plot_id,batch_id 0 2 miss% 0.02253184141544996
plot_id,batch_id 0 3 miss% 0.0268542220247389
plot_id,batch_id 0 4 miss% 0.024990258663338365
plot_id,batch_id 0 5 miss% 0.0347598881568296
plot_id,batch_id 0 6 miss% 0.022552975087743666
plot_id,batch_id 0 7 miss% 0.027290905421866643
plot_id,batch_id 0 8 miss% 0.029492157413583072
plot_id,batch_id 0 9 miss% 0.02642961530111242
plot_id,batch_id 0 10 miss% 0.033531079807184706
plot_id,batch_id 0 11 miss% 0.0398976977928761
plot_id,batch_id 0 12 miss% 0.031121464206301165
plot_id,batch_id 0 13 miss% 0.015666509693147153
plot_id,batch_id 0 14 miss% 0.028182905907901012
plot_id,batch_id 0 15 miss% 0.04799155186077486
plot_id,batch_id 0 16 miss% 0.030167075175264704
plot_id,batch_id 0 17 miss% 0.03333258588597391
plot_id,batch_id 0 18 miss% 0.044962309239150317
plot_id,batch_id 0 19 miss% 0.02546140870808447
plot_id,batch_id 0 20 miss% 0.06678170475632934
plot_id,batch_id 0 21 miss% 0.03127623811501454
plot_id,batch_id 0 22 miss% 0.027061477258000347
plot_id,batch_id 0 23 miss% 0.028639487958913724
plot_id,batch_id 0 24 miss% 0.018099565000101438
plot_id,batch_id 0 25 miss% 0.02590786495308339
plot_id,batch_id 0 26 miss% 0.02767150587961899
plot_id,batch_id 0 27 miss% 0.023116687479391298
plot_id,batch_id 0 28 miss% 0.03685261875697971
plot_id,batch_id 0 29 miss% 0.01138218896287442
plot_id,batch_id 0 30 miss% 0.03169356593601632
plot_id,batch_id 0 31 miss% 0.03252609921241327
plot_id,batch_id 0 32 miss% 0.026183233244234664
plot_id,batch_id 0 33 miss% 0.028305672651183448
plot_id,batch_id 0 34 miss% 0.021181995105675727
plot_id,batch_id 0 35 miss% 0.048955778686704136
plot_id,batch_id 0 36 miss% 0.03523194349102553
plot_id,batch_id 0 37 miss% 0.04194825725377457
plot_id,batch_id 0 38 miss% 0.030443414688666456
plot_id,batch_id 0 39 miss% 0.0257511211160147
plot_id,batch_id 0 40 miss% 0.07459123540158748
plot_id,batch_id 0 41 miss% 0.019171565592348453
plot_id,batch_id 0 42 miss% 0.019689560021103975
plot_id,batch_id 0 43 miss% 0.02938179796228325
plot_id,batch_id 0 44 miss% 0.016265855012850088
plot_id,batch_id 0 45 miss% 0.027118090221494452
plot_id,batch_id 0 46 miss% 0.03138609114086689
plot_id,batch_id 0 47 miss% 0.016405253671664153
plot_id,batch_id 0 48 miss% 0.012064802016364845
plot_id,batch_id 0 49 miss% 0.022849946834194418
plot_id,batch_id 0 50 miss% 0.03539951168048283
plot_id,batch_id 0 51 miss% 0.022936564043120538
plot_id,batch_id 0 52 miss% 0.02310533784391468
plot_id,batch_id 0 53 miss% 0.016171905123306546
plot_id,batch_id 0 54 miss% 0.03322681129823285
plot_id,batch_id 0 55 miss% 0.03273753976645034
plot_id,batch_id 0 56 miss% 0.024489944967642487
plot_id,batch_id 0 57 miss% 0.032107587558715976
plot_id,batch_id 0 58 miss% 0.026641240684839192
plot_id,batch_id 0 59 miss% 0.02153783629377708
plot_id,batch_id 0 60 miss% 0.03856119696764926
plot_id,batch_id 0 61 miss% 0.028786302393641557
plot_id,batch_id 0 62 miss% 0.02245381543235704
plot_id,batch_id 0 63 miss% 0.03373725804001366
plot_id,batch_id 0 64 miss% 0.0379150737636821
plot_id,batch_id 0 65 miss% 0.05923325415870552
plot_id,batch_id 0 66 miss% 0.06520785800992135
plot_id,batch_id 0 67 miss% 0.02464669356874232
plot_id,batch_id 0 68 miss% 0.025207803275591207
plot_id,batch_id 0 69 miss% 0.03363642021389563
plot_id,batch_id 0 70 miss% 0.03055265746383305
plot_id,batch_id 0 71 miss% 0.045045801579809995
plot_id,batch_id 0 72 miss% 0.04955038014811633
plot_id,batch_id 0 73 miss% 0.02855259711060944
plot_id,batch_id 0 74 miss% 0.05178704802211259
plot_id,batch_id 0 75 miss% 0.043033457008998195
plot_id,batch_id 0 76 miss% 0.045726245050903766
plot_id,batch_id 0 77 miss% 0.0307205461686429
plot_id,batch_id 0 78 miss% 0.03889625825624083
plot_id,batch_id 0 79 miss% 0.03852776500563621
plot_id,batch_id 0 80 miss% 0.03636013927817326
plot_id,batch_id 0 81 miss% 0.026538831764355668
plot_id,batch_id 0 82 miss% 0.03536482037510219
plot_id,batch_id 0 83 miss% 0.029401015512748034
plot_id,batch_id 0 84 miss% 0.03231623397252005
plot_id,batch_id 0 85 miss% 0.033294985237112884
plot_id,batch_id 0 86 miss% 0.024000043720340723
plot_id,batch_id 0 87 miss% 0.029556190585197123
plot_id,batch_id 0 88 miss% 0.04096693597422024
plot_id,batch_id 0 89 miss% 0.0418116535775616
plot_id,batch_id 0 90 miss% 0.030769589038859402
plot_id,batch_id 0 91 miss% 0.028483840306169156
plot_id,batch_id 0 92 miss% 0.038767024484003765
plot_id,batch_id 0 93 miss% 0.02613604498354075
plot_id,batch_id 0 94 miss% 0.053465005167910785
plot_id,batch_id 0 95 miss% 0.05508108375756578
plot_id,batch_id 0 96 miss% 0.03406596683980684
plot_id,batch_id 0 97 miss% 0.060738911246988374
plot_id,batch_id 0 98 miss% 0.02686808701822431
plot_id,batch_id 0 99 miss% 0.04135513193455602
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02366601 0.01957322 0.02253184 0.02685422 0.02499026 0.03475989
 0.02255298 0.02729091 0.02949216 0.02642962 0.03353108 0.0398977
 0.03112146 0.01566651 0.02818291 0.04799155 0.03016708 0.03333259
 0.04496231 0.02546141 0.0667817  0.03127624 0.02706148 0.02863949
 0.01809957 0.02590786 0.02767151 0.02311669 0.03685262 0.01138219
 0.03169357 0.0325261  0.02618323 0.02830567 0.021182   0.04895578
 0.03523194 0.04194826 0.03044341 0.02575112 0.07459124 0.01917157
 0.01968956 0.0293818  0.01626586 0.02711809 0.03138609 0.01640525
 0.0120648  0.02284995 0.03539951 0.02293656 0.02310534 0.01617191
 0.03322681 0.03273754 0.02448994 0.03210759 0.02664124 0.02153784
 0.0385612  0.0287863  0.02245382 0.03373726 0.03791507 0.05923325
 0.06520786 0.02464669 0.0252078  0.03363642 0.03055266 0.0450458
 0.04955038 0.0285526  0.05178705 0.04303346 0.04572625 0.03072055
 0.03889626 0.03852777 0.03636014 0.02653883 0.03536482 0.02940102
 0.03231623 0.03329499 0.02400004 0.02955619 0.04096694 0.04181165
 0.03076959 0.02848384 0.03876702 0.02613604 0.05346501 0.05508108
 0.03406597 0.06073891 0.02686809 0.04135513]
for model  73 the mean error 0.03241864537929534
all id 73 hidden_dim 16 learning_rate 0.02 num_layers 5 frames 21 out win 4 err 0.03241864537929534
Launcher: Job 74 completed in 5752 seconds.
Launcher: Task 91 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  35921
Epoch:0, Train loss:0.672969, valid loss:0.663568
Epoch:1, Train loss:0.047386, valid loss:0.015593
Epoch:2, Train loss:0.016010, valid loss:0.004933
Epoch:3, Train loss:0.007954, valid loss:0.004028
Epoch:4, Train loss:0.006580, valid loss:0.002899
Epoch:5, Train loss:0.005595, valid loss:0.002522
Epoch:6, Train loss:0.004759, valid loss:0.002413
Epoch:7, Train loss:0.004219, valid loss:0.001857
Epoch:8, Train loss:0.003640, valid loss:0.001876
Epoch:9, Train loss:0.003364, valid loss:0.001636
Epoch:10, Train loss:0.003138, valid loss:0.001740
Epoch:11, Train loss:0.002368, valid loss:0.001311
Epoch:12, Train loss:0.002373, valid loss:0.001444
Epoch:13, Train loss:0.002248, valid loss:0.001180
Epoch:14, Train loss:0.002181, valid loss:0.001084
Epoch:15, Train loss:0.002153, valid loss:0.001202
Epoch:16, Train loss:0.002094, valid loss:0.001117
Epoch:17, Train loss:0.002056, valid loss:0.001111
Epoch:18, Train loss:0.002044, valid loss:0.001208
Epoch:19, Train loss:0.001942, valid loss:0.001079
Epoch:20, Train loss:0.001966, valid loss:0.001000
Epoch:21, Train loss:0.001605, valid loss:0.000890
Epoch:22, Train loss:0.001604, valid loss:0.000956
Epoch:23, Train loss:0.001608, valid loss:0.001018
Epoch:24, Train loss:0.001554, valid loss:0.000924
Epoch:25, Train loss:0.001564, valid loss:0.000852
Epoch:26, Train loss:0.001538, valid loss:0.000850
Epoch:27, Train loss:0.001560, valid loss:0.000828
Epoch:28, Train loss:0.001516, valid loss:0.000881
Epoch:29, Train loss:0.001508, valid loss:0.000859
Epoch:30, Train loss:0.001477, valid loss:0.000845
Epoch:31, Train loss:0.001318, valid loss:0.000753
Epoch:32, Train loss:0.001303, valid loss:0.000752
Epoch:33, Train loss:0.001312, valid loss:0.000751
Epoch:34, Train loss:0.001302, valid loss:0.000761
Epoch:35, Train loss:0.001300, valid loss:0.000735
Epoch:36, Train loss:0.001291, valid loss:0.000736
Epoch:37, Train loss:0.001280, valid loss:0.000761
Epoch:38, Train loss:0.001284, valid loss:0.000760
Epoch:39, Train loss:0.001263, valid loss:0.000780
Epoch:40, Train loss:0.001258, valid loss:0.000742
Epoch:41, Train loss:0.001183, valid loss:0.000706
Epoch:42, Train loss:0.001182, valid loss:0.000695
Epoch:43, Train loss:0.001174, valid loss:0.000737
Epoch:44, Train loss:0.001176, valid loss:0.000700
Epoch:45, Train loss:0.001165, valid loss:0.000697
Epoch:46, Train loss:0.001167, valid loss:0.000726
Epoch:47, Train loss:0.001167, valid loss:0.000710
Epoch:48, Train loss:0.001167, valid loss:0.000721
Epoch:49, Train loss:0.001158, valid loss:0.000691
Epoch:50, Train loss:0.001152, valid loss:0.000696
Epoch:51, Train loss:0.001114, valid loss:0.000686
Epoch:52, Train loss:0.001111, valid loss:0.000697
Epoch:53, Train loss:0.001112, valid loss:0.000698
Epoch:54, Train loss:0.001109, valid loss:0.000682
Epoch:55, Train loss:0.001106, valid loss:0.000685
Epoch:56, Train loss:0.001104, valid loss:0.000705
Epoch:57, Train loss:0.001103, valid loss:0.000697
Epoch:58, Train loss:0.001104, valid loss:0.000683
Epoch:59, Train loss:0.001099, valid loss:0.000706
Epoch:60, Train loss:0.001101, valid loss:0.000686
training time 5567.358531713486
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.029450813521740962
plot_id,batch_id 0 1 miss% 0.019947819421710845
plot_id,batch_id 0 2 miss% 0.022332736084924833
plot_id,batch_id 0 3 miss% 0.024996506124669554
plot_id,batch_id 0 4 miss% 0.022226750237750517
plot_id,batch_id 0 5 miss% 0.031750870323586554
plot_id,batch_id 0 6 miss% 0.03109501601058408
plot_id,batch_id 0 7 miss% 0.027202256870439274
plot_id,batch_id 0 8 miss% 0.034196103290414746
plot_id,batch_id 0 9 miss% 0.023901181503547194
plot_id,batch_id 0 10 miss% 0.048821165511849736
plot_id,batch_id 0 11 miss% 0.03607306391241205
plot_id,batch_id 0 12 miss% 0.034498295860408125
plot_id,batch_id 0 13 miss% 0.028659566487094657
plot_id,batch_id 0 14 miss% 0.027324282412543015
plot_id,batch_id 0 15 miss% 0.03948167418793668
plot_id,batch_id 0 16 miss% 0.03483691817687122
plot_id,batch_id 0 17 miss% 0.04917972939895067
plot_id,batch_id 0 18 miss% 0.03379500567285838
plot_id,batch_id 0 19 miss% 0.03359360092228808
plot_id,batch_id 0 20 miss% 0.04376920675005599
plot_id,batch_id 0 21 miss% 0.024633237732239396
plot_id,batch_id 0 22 miss% 0.036539163576356885
plot_id,batch_id 0 23 miss% 0.03105072321069372
plot_id,batch_id 0 24 miss% 0.02238163668033033
plot_id,batch_id 0 25 miss% 0.03591357099500981
plot_id,batch_id 0 26 miss% 0.02793892853412325
plot_id,batch_id 0 27 miss% 0.028312890283302333
plot_id,batch_id 0 28 miss% 0.019223258000969187
plot_id,batch_id 0 29 miss% 0.022254237155823482
plot_id,batch_id 0 30 miss% 0.04248779917524216
plot_id,batch_id 0 31 miss% 0.03399919793275732
plot_id,batch_id 0 32 miss% 0.02927155335567067
plot_id,batch_id 0 33 miss% 0.027460371820782325
plot_id,batch_id 0 34 miss% 0.027549518202348383
plot_id,batch_id 0 35 miss% 0.036395066299723465
plot_id,batch_id 0 36 miss% 0.04194657854141415
plot_id,batch_id 0 37 miss% 0.032063189093047395
plot_id,batch_id 0 38 miss% 0.033249875397898476
plot_id,batch_id 0 39 miss% 0.02990142353741395
plot_id,batch_id 0 40 miss% 0.0725280331840689
plot_id,batch_id 0 41 miss% 0.03240007615635894
plot_id,batch_id 0 42 miss% 0.0314300232573285
plot_id,batch_id 0 43 miss% 0.035153741887521156
plot_id,batch_id 0 44 miss% 0.021577321559161745
plot_id,batch_id 0 45 miss% 0.03768472006209682
plot_id,batch_id 0 46 miss% 0.032316012074188535
plot_id,batch_id 0 47 miss% 0.025391446422233708
plot_id,batch_id 0 48 miss% 0.02831081746953467
plot_id,batch_id 0 49 miss% 0.02145787822085984
plot_id,batch_id 0 50 miss% 0.03571277905163586
plot_id,batch_id 0 51 miss% 0.030849652801461205
plot_id,batch_id 0 52 miss% 0.026059483695441005
plot_id,batch_id 0 53 miss% 0.017266172436882204
plot_id,batch_id 0 54 miss% 0.03367022249419986
plot_id,batch_id 0 55 miss% 0.02946085877062522
plot_id,batch_id 0 56 miss% 0.030659037951826534
plot_id,batch_id 0 57 miss% 0.03405529574344366
plot_id,batch_id 0 58 miss% 0.026768574111403008
plot_id,batch_id 0 59 miss% 0.029333239358175882
plot_id,batch_id 0 60 miss% 0.038204294906487836
plot_id,batch_id 0 61 miss% 0.030591863241942154
plot_id,batch_id 0 62 miss% 0.0234661977759471
plot_id,batch_id 0 63 miss% 0.03648831475310783
plot_id,batch_id 0 64 miss% 0.02613214311216243
plot_id,batch_id 0 65 miss% 0.04820655927246218
plot_id,batch_id 0 66 miss% 0.04581658312751532
plot_id,batch_id 0 67 miss% 0.02840000825347569
plot_id,batch_id 0 68 miss% 0.03425767856527684
plot_id,batch_id 0 69 miss% 0.02356726103685376
plot_id,batch_id 0 70 miss% 0.043456067422217456
plot_id,batch_id 0 71 miss% 0.04616294531144948
plot_id,batch_id 0 72 miss% 0.026882846409018446
plot_id,batch_id 0 73 miss% 0.023031606033289513
plot_id,batch_id 0 74 miss% 0.04736807132937549
plot_id,batch_id 0 75 miss% 0.05312481875102463
plot_id,batch_id 0 76 miss% 0.06584117187931318
plot_id,batch_id 0 77 miss% 0.030640717152888394
plot_id,batch_id 0 78 miss% 0.04093462139435649
plot_id,batch_id 0 79 miss% 0.03873502850979445
plot_id,batch_id 0 80 miss% 0.04735384052705591
plot_id,batch_id 0 81 miss% 0.019277734965146284
plot_id,batch_id 0 82 miss% 0.027489202080700482
plot_id,batch_id 0 83 miss% 0.028268098276731612
plot_id,batch_id 0 84 miss% 0.019111711774218346
plot_id,batch_id 0 85 miss% 0.039354636222827484
plot_id,batch_id 0 86 miss% 0.02863658111517565
plot_id,batch_id 0 87 miss% 0.02357836375512844
plot_id,batch_id 0 88 miss% 0.026900295489156234
plot_id,batch_id 0 89 miss% 0.027423839437972733
plot_id,batch_id 0 90 miss% 0.04719008753900059
plot_id,batch_id 0 91 miss% 0.038208650355176364
plot_id,batch_id 0 92 miss% 0.024450122049746555
plot_id,batch_id 0 93 miss% 0.03581849611652718
plot_id,batch_id 0 94 miss% 0.02858061108629215
plot_id,batch_id 0 95 miss% 0.038556094919594025
plot_id,batch_id 0 96 miss% 0.039175678947228454
plot_id,batch_id 0 97 miss% 0.047122725097711676
plot_id,batch_id 0 98 miss% 0.029083234507273646
plot_id,batch_id 0 99 miss% 0.028228802129660814
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02945081 0.01994782 0.02233274 0.02499651 0.02222675 0.03175087
 0.03109502 0.02720226 0.0341961  0.02390118 0.04882117 0.03607306
 0.0344983  0.02865957 0.02732428 0.03948167 0.03483692 0.04917973
 0.03379501 0.0335936  0.04376921 0.02463324 0.03653916 0.03105072
 0.02238164 0.03591357 0.02793893 0.02831289 0.01922326 0.02225424
 0.0424878  0.0339992  0.02927155 0.02746037 0.02754952 0.03639507
 0.04194658 0.03206319 0.03324988 0.02990142 0.07252803 0.03240008
 0.03143002 0.03515374 0.02157732 0.03768472 0.03231601 0.02539145
 0.02831082 0.02145788 0.03571278 0.03084965 0.02605948 0.01726617
 0.03367022 0.02946086 0.03065904 0.0340553  0.02676857 0.02933324
 0.03820429 0.03059186 0.0234662  0.03648831 0.02613214 0.04820656
 0.04581658 0.02840001 0.03425768 0.02356726 0.04345607 0.04616295
 0.02688285 0.02303161 0.04736807 0.05312482 0.06584117 0.03064072
 0.04093462 0.03873503 0.04735384 0.01927773 0.0274892  0.0282681
 0.01911171 0.03935464 0.02863658 0.02357836 0.0269003  0.02742384
 0.04719009 0.03820865 0.02445012 0.0358185  0.02858061 0.03855609
 0.03917568 0.04712273 0.02908323 0.0282288 ]
for model  99 the mean error 0.032869077735465144
all id 99 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 25 out win 3 err 0.032869077735465144
Launcher: Job 100 completed in 5764 seconds.
Launcher: Task 199 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  46193
Epoch:0, Train loss:0.668440, valid loss:0.637867
Epoch:1, Train loss:0.518179, valid loss:0.525515
Epoch:2, Train loss:0.503468, valid loss:0.520062
Epoch:3, Train loss:0.500460, valid loss:0.519294
Epoch:4, Train loss:0.498578, valid loss:0.518353
Epoch:5, Train loss:0.497533, valid loss:0.517579
Epoch:6, Train loss:0.497138, valid loss:0.517779
Epoch:7, Train loss:0.496859, valid loss:0.517656
Epoch:8, Train loss:0.496642, valid loss:0.518658
Epoch:9, Train loss:0.496257, valid loss:0.517369
Epoch:10, Train loss:0.495995, valid loss:0.516959
Epoch:11, Train loss:0.493990, valid loss:0.516233
Epoch:12, Train loss:0.493967, valid loss:0.516634
Epoch:13, Train loss:0.494056, valid loss:0.516381
Epoch:14, Train loss:0.493848, valid loss:0.516344
Epoch:15, Train loss:0.493869, valid loss:0.516655
Epoch:16, Train loss:0.493934, valid loss:0.516215
Epoch:17, Train loss:0.493795, valid loss:0.516756
Epoch:18, Train loss:0.493657, valid loss:0.516139
Epoch:19, Train loss:0.493675, valid loss:0.516505
Epoch:20, Train loss:0.493593, valid loss:0.516099
Epoch:21, Train loss:0.492707, valid loss:0.515883
Epoch:22, Train loss:0.492732, valid loss:0.515972
Epoch:23, Train loss:0.492619, valid loss:0.516092
Epoch:24, Train loss:0.492667, valid loss:0.515823
Epoch:25, Train loss:0.492595, valid loss:0.515812
Epoch:26, Train loss:0.492575, valid loss:0.515721
Epoch:27, Train loss:0.492631, valid loss:0.515905
Epoch:28, Train loss:0.492560, valid loss:0.515776
Epoch:29, Train loss:0.492552, valid loss:0.516221
Epoch:30, Train loss:0.492665, valid loss:0.515834
Epoch:31, Train loss:0.492055, valid loss:0.515617
Epoch:32, Train loss:0.492015, valid loss:0.515504
Epoch:33, Train loss:0.492035, valid loss:0.515585
Epoch:34, Train loss:0.492034, valid loss:0.515522
Epoch:35, Train loss:0.492020, valid loss:0.515494
Epoch:36, Train loss:0.492007, valid loss:0.515573
Epoch:37, Train loss:0.492006, valid loss:0.515523
Epoch:38, Train loss:0.492024, valid loss:0.515558
Epoch:39, Train loss:0.491980, valid loss:0.515586
Epoch:40, Train loss:0.492011, valid loss:0.515541
Epoch:41, Train loss:0.491759, valid loss:0.515431
Epoch:42, Train loss:0.491750, valid loss:0.515394
Epoch:43, Train loss:0.491727, valid loss:0.515491
Epoch:44, Train loss:0.491751, valid loss:0.515465
Epoch:45, Train loss:0.491779, valid loss:0.515425
Epoch:46, Train loss:0.491734, valid loss:0.515428
Epoch:47, Train loss:0.491736, valid loss:0.515440
Epoch:48, Train loss:0.491704, valid loss:0.515450
Epoch:49, Train loss:0.491720, valid loss:0.515455
Epoch:50, Train loss:0.491708, valid loss:0.515418
Epoch:51, Train loss:0.491611, valid loss:0.515409
Epoch:52, Train loss:0.491590, valid loss:0.515365
Epoch:53, Train loss:0.491619, valid loss:0.515336
Epoch:54, Train loss:0.491586, valid loss:0.515393
Epoch:55, Train loss:0.491599, valid loss:0.515429
Epoch:56, Train loss:0.491585, valid loss:0.515371
Epoch:57, Train loss:0.491583, valid loss:0.515366
Epoch:58, Train loss:0.491579, valid loss:0.515374
Epoch:59, Train loss:0.491583, valid loss:0.515400
Epoch:60, Train loss:0.491575, valid loss:0.515417
training time 5597.002911090851
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.8180756049505751
plot_id,batch_id 0 1 miss% 0.855917540632821
plot_id,batch_id 0 2 miss% 0.8609130555010401
plot_id,batch_id 0 3 miss% 0.8634055594023702
plot_id,batch_id 0 4 miss% 0.8681228751481144
plot_id,batch_id 0 5 miss% 0.812814374068868
plot_id,batch_id 0 6 miss% 0.8557276840904428
plot_id,batch_id 0 7 miss% 0.8621726295984921
plot_id,batch_id 0 8 miss% 0.8643130604037683
plot_id,batch_id 0 9 miss% 0.8667677489781801
plot_id,batch_id 0 10 miss% 0.8066610372051201
plot_id,batch_id 0 11 miss% 0.8491497337667167
plot_id,batch_id 0 12 miss% 0.8603428934773505
plot_id,batch_id 0 13 miss% 0.8619368917502639
plot_id,batch_id 0 14 miss% 0.8674072785357848
plot_id,batch_id 0 15 miss% 0.8075789971583286
plot_id,batch_id 0 16 miss% 0.8490751514212443
plot_id,batch_id 0 17 miss% 0.8590254308881587
plot_id,batch_id 0 18 miss% 0.8650453422801412
plot_id,batch_id 0 19 miss% 0.8636661729486906
plot_id,batch_id 0 20 miss% 0.8469117631366077
plot_id,batch_id 0 21 miss% 0.8617477357747478
plot_id,batch_id 0 22 miss% 0.8647764507928318
plot_id,batch_id 0 23 miss% 0.8686470176355602
plot_id,batch_id 0 24 miss% 0.868873250996335
plot_id,batch_id 0 25 miss% 0.838263152301205
plot_id,batch_id 0 26 miss% 0.8572041379756499
plot_id,batch_id 0 27 miss% 0.864588949305146
plot_id,batch_id 0 28 miss% 0.8665562721228153
plot_id,batch_id 0 29 miss% 0.8684402931992597
plot_id,batch_id 0 30 miss% 0.8220374446428631
plot_id,batch_id 0 31 miss% 0.8591696239559209
plot_id,batch_id 0 32 miss% 0.8615798679458869
plot_id,batch_id 0 33 miss% 0.8643269876614095
plot_id,batch_id 0 34 miss% 0.8642408665622273
plot_id,batch_id 0 35 miss% 0.8252867233971161
plot_id,batch_id 0 36 miss% 0.8614889109530104
plot_id,batch_id 0 37 miss% 0.8629998487973413
plot_id,batch_id 0 38 miss% 0.8663442321088467
plot_id,batch_id 0 39 miss% 0.8655170807423914
plot_id,batch_id 0 40 miss% 0.8580122154022937
plot_id,batch_id 0 41 miss% 0.8632474121243873
plot_id,batch_id 0 42 miss% 0.8645584564838756
plot_id,batch_id 0 43 miss% 0.8672567499449115
plot_id,batch_id 0 44 miss% 0.8687266365247843
plot_id,batch_id 0 45 miss% 0.8492459169484634
plot_id,batch_id 0 46 miss% 0.8626572258476668
plot_id,batch_id 0 47 miss% 0.8660291417363731
plot_id,batch_id 0 48 miss% 0.8706632584856246
plot_id,batch_id 0 49 miss% 0.8682658734302832
plot_id,batch_id 0 50 miss% 0.8544500327421455
plot_id,batch_id 0 51 miss% 0.8625230440130718
plot_id,batch_id 0 52 miss% 0.8657377052762232
plot_id,batch_id 0 53 miss% 0.8681370596835584
plot_id,batch_id 0 54 miss% 0.8686229875254965
plot_id,batch_id 0 55 miss% 0.8489798793246702
plot_id,batch_id 0 56 miss% 0.8660076756744683
plot_id,batch_id 0 57 miss% 0.8657803165777449
plot_id,batch_id 0 58 miss% 0.8692269828108918
plot_id,batch_id 0 59 miss% 0.8681150080941182
plot_id,batch_id 0 60 miss% 0.7661861468057166
plot_id,batch_id 0 61 miss% 0.8447722224316319
plot_id,batch_id 0 62 miss% 0.855711150499631
plot_id,batch_id 0 63 miss% 0.8583793426201772
plot_id,batch_id 0 64 miss% 0.8608518947977081
plot_id,batch_id 0 65 miss% 0.763658081464861
plot_id,batch_id 0 66 miss% 0.8295392208463203
plot_id,batch_id 0 67 miss% 0.8454837323959936
plot_id,batch_id 0 68 miss% 0.856388497382383
plot_id,batch_id 0 69 miss% 0.8581436636986429
plot_id,batch_id 0 70 miss% 0.7260258671803902
plot_id,batch_id 0 71 miss% 0.8318606057682483
plot_id,batch_id 0 72 miss% 0.8415057522899643
plot_id,batch_id 0 73 miss% 0.848452298248916
plot_id,batch_id 0 74 miss% 0.8529991094077954
plot_id,batch_id 0 75 miss% 0.753743713509191
plot_id,batch_id 0 76 miss% 0.8164328366126965
plot_id,batch_id 0 77 miss% 0.8277331359566743
plot_id,batch_id 0 78 miss% 0.842901889428428
plot_id,batch_id 0 79 miss% 0.8522340628541417
plot_id,batch_id 0 80 miss% 0.790128293916152
plot_id,batch_id 0 81 miss% 0.8488986439134357
plot_id,batch_id 0 82 miss% 0.8589405150740042
plot_id,batch_id 0 83 miss% 0.860188676819124
plot_id,batch_id 0 84 miss% 0.8624835362849885
plot_id,batch_id 0 85 miss% 0.7778754678262537
plot_id,batch_id 0 86 miss% 0.8435292153652004
plot_id,batch_id 0 87 miss% 0.8531257711755653
plot_id,batch_id 0 88 miss% 0.8584464641018629
plot_id,batch_id 0 89 miss% 0.8631483027108663
plot_id,batch_id 0 90 miss% 0.7461559417646899
plot_id,batch_id 0 91 miss% 0.8362570272768576
plot_id,batch_id 0 92 miss% 0.8487729208494535
plot_id,batch_id 0 93 miss% 0.8507781417903253
plot_id,batch_id 0 94 miss% 0.8618682754158119
plot_id,batch_id 0 95 miss% 0.7687432265125262
plot_id,batch_id 0 96 miss% 0.8275418195130555
plot_id,batch_id 0 97 miss% 0.8491904068635632
plot_id,batch_id 0 98 miss% 0.8530544554446143
plot_id,batch_id 0 99 miss% 0.8548972243683581
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.8180756  0.85591754 0.86091306 0.86340556 0.86812288 0.81281437
 0.85572768 0.86217263 0.86431306 0.86676775 0.80666104 0.84914973
 0.86034289 0.86193689 0.86740728 0.807579   0.84907515 0.85902543
 0.86504534 0.86366617 0.84691176 0.86174774 0.86477645 0.86864702
 0.86887325 0.83826315 0.85720414 0.86458895 0.86655627 0.86844029
 0.82203744 0.85916962 0.86157987 0.86432699 0.86424087 0.82528672
 0.86148891 0.86299985 0.86634423 0.86551708 0.85801222 0.86324741
 0.86455846 0.86725675 0.86872664 0.84924592 0.86265723 0.86602914
 0.87066326 0.86826587 0.85445003 0.86252304 0.86573771 0.86813706
 0.86862299 0.84897988 0.86600768 0.86578032 0.86922698 0.86811501
 0.76618615 0.84477222 0.85571115 0.85837934 0.86085189 0.76365808
 0.82953922 0.84548373 0.8563885  0.85814366 0.72602587 0.83186061
 0.84150575 0.8484523  0.85299911 0.75374371 0.81643284 0.82773314
 0.84290189 0.85223406 0.79012829 0.84889864 0.85894052 0.86018868
 0.86248354 0.77787547 0.84352922 0.85312577 0.85844646 0.8631483
 0.74615594 0.83625703 0.84877292 0.85077814 0.86186828 0.76874323
 0.82754182 0.84919041 0.85305446 0.85489722]
for model  59 the mean error 0.8470039080205092
all id 59 hidden_dim 24 learning_rate 0.02 num_layers 3 frames 21 out win 5 err 0.8470039080205092
Launcher: Job 60 completed in 5778 seconds.
Launcher: Task 125 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  21969
Epoch:0, Train loss:0.437137, valid loss:0.400084
Epoch:1, Train loss:0.226742, valid loss:0.230147
Epoch:2, Train loss:0.219893, valid loss:0.229906
Epoch:3, Train loss:0.219066, valid loss:0.229447
Epoch:4, Train loss:0.218712, valid loss:0.229347
Epoch:5, Train loss:0.218477, valid loss:0.229163
Epoch:6, Train loss:0.218253, valid loss:0.229226
Epoch:7, Train loss:0.218143, valid loss:0.229761
Epoch:8, Train loss:0.218093, valid loss:0.229286
Epoch:9, Train loss:0.217989, valid loss:0.229278
Epoch:10, Train loss:0.217912, valid loss:0.229122
Epoch:11, Train loss:0.217336, valid loss:0.228654
Epoch:12, Train loss:0.217313, valid loss:0.228623
Epoch:13, Train loss:0.217325, valid loss:0.228801
Epoch:14, Train loss:0.217277, valid loss:0.228697
Epoch:15, Train loss:0.217267, valid loss:0.228844
Epoch:16, Train loss:0.217206, valid loss:0.228533
Epoch:17, Train loss:0.217208, valid loss:0.228744
Epoch:18, Train loss:0.217171, valid loss:0.228613
Epoch:19, Train loss:0.217169, valid loss:0.228848
Epoch:20, Train loss:0.217154, valid loss:0.228692
Epoch:21, Train loss:0.216842, valid loss:0.228482
Epoch:22, Train loss:0.216834, valid loss:0.228483
Epoch:23, Train loss:0.216832, valid loss:0.228446
Epoch:24, Train loss:0.216822, valid loss:0.228396
Epoch:25, Train loss:0.216850, valid loss:0.228395
Epoch:26, Train loss:0.216816, valid loss:0.228392
Epoch:27, Train loss:0.216804, valid loss:0.228376
Epoch:28, Train loss:0.216801, valid loss:0.228500
Epoch:29, Train loss:0.216788, valid loss:0.228455
Epoch:30, Train loss:0.216807, valid loss:0.228321
Epoch:31, Train loss:0.216631, valid loss:0.228333
Epoch:32, Train loss:0.216633, valid loss:0.228376
Epoch:33, Train loss:0.216627, valid loss:0.228309
Epoch:34, Train loss:0.216628, valid loss:0.228393
Epoch:35, Train loss:0.216608, valid loss:0.228294
Epoch:36, Train loss:0.216603, valid loss:0.228374
Epoch:37, Train loss:0.216613, valid loss:0.228380
Epoch:38, Train loss:0.216615, valid loss:0.228409
Epoch:39, Train loss:0.216592, valid loss:0.228388
Epoch:40, Train loss:0.216600, valid loss:0.228393
Epoch:41, Train loss:0.216510, valid loss:0.228240
Epoch:42, Train loss:0.216517, valid loss:0.228273
Epoch:43, Train loss:0.216520, valid loss:0.228279
Epoch:44, Train loss:0.216504, valid loss:0.228275
Epoch:45, Train loss:0.216501, valid loss:0.228286
Epoch:46, Train loss:0.216510, valid loss:0.228285
Epoch:47, Train loss:0.216504, valid loss:0.228285
Epoch:48, Train loss:0.216496, valid loss:0.228289
Epoch:49, Train loss:0.216499, valid loss:0.228312
Epoch:50, Train loss:0.216493, valid loss:0.228284
Epoch:51, Train loss:0.216463, valid loss:0.228253
Epoch:52, Train loss:0.216456, valid loss:0.228261
Epoch:53, Train loss:0.216454, valid loss:0.228268
Epoch:54, Train loss:0.216453, valid loss:0.228247
Epoch:55, Train loss:0.216450, valid loss:0.228258
Epoch:56, Train loss:0.216449, valid loss:0.228254
Epoch:57, Train loss:0.216448, valid loss:0.228253
Epoch:58, Train loss:0.216448, valid loss:0.228250
Epoch:59, Train loss:0.216448, valid loss:0.228242
Epoch:60, Train loss:0.216445, valid loss:0.228251
training time 5644.128319263458
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.6556946428901689
plot_id,batch_id 0 1 miss% 0.7437985886025236
plot_id,batch_id 0 2 miss% 0.7585344350986639
plot_id,batch_id 0 3 miss% 0.7709248733079969
plot_id,batch_id 0 4 miss% 0.7692256482036446
plot_id,batch_id 0 5 miss% 0.664298313845046
plot_id,batch_id 0 6 miss% 0.7410704583122446
plot_id,batch_id 0 7 miss% 0.7550167053980428
plot_id,batch_id 0 8 miss% 0.7660114655826421
plot_id,batch_id 0 9 miss% 0.7743721490539648
plot_id,batch_id 0 10 miss% 0.6328969793288192
plot_id,batch_id 0 11 miss% 0.7428172905462348
plot_id,batch_id 0 12 miss% 0.7525099050726503
plot_id,batch_id 0 13 miss% 0.7625492249225172
plot_id,batch_id 0 14 miss% 0.771592143799737
plot_id,batch_id 0 15 miss% 0.64914284826474
plot_id,batch_id 0 16 miss% 0.7343513554569628
plot_id,batch_id 0 17 miss% 0.7613210354431825
plot_id,batch_id 0 18 miss% 0.7642170843832223
plot_id,batch_id 0 19 miss% 0.7672147980733945
plot_id,batch_id 0 20 miss% 0.7068199520943791
plot_id,batch_id 0 21 miss% 0.7613660004565913
plot_id,batch_id 0 22 miss% 0.7687015716542429
plot_id,batch_id 0 23 miss% 0.7752544333488256
plot_id,batch_id 0 24 miss% 0.7765025687965812
plot_id,batch_id 0 25 miss% 0.6950212166386399
plot_id,batch_id 0 26 miss% 0.7547134438445514
plot_id,batch_id 0 27 miss% 0.7723345330308096
plot_id,batch_id 0 28 miss% 0.7702002144495218
plot_id,batch_id 0 29 miss% 0.7791532940604148
plot_id,batch_id 0 30 miss% 0.7006718819301607
plot_id,batch_id 0 31 miss% 0.7527405016025351
plot_id,batch_id 0 32 miss% 0.7637566412477782
plot_id,batch_id 0 33 miss% 0.7691654553242063
plot_id,batch_id 0 34 miss% 0.7708306182484692
plot_id,batch_id 0 35 miss% 0.692893292327608
plot_id,batch_id 0 36 miss% 0.7551932562296055
plot_id,batch_id 0 37 miss% 0.7666000587474087
plot_id,batch_id 0 38 miss% 0.7717485761528637
plot_id,batch_id 0 39 miss% 0.7720310013197194
plot_id,batch_id 0 40 miss% 0.7335435056665598
plot_id,batch_id 0 41 miss% 0.7681748146521076
plot_id,batch_id 0 42 miss% 0.7689605445591834
plot_id,batch_id 0 43 miss% 0.7765743870710033
plot_id,batch_id 0 44 miss% 0.7811016337993907
plot_id,batch_id 0 45 miss% 0.7551041465720312
plot_id,batch_id 0 46 miss% 0.7679034502570542
plot_id,batch_id 0 47 miss% 0.7698370373590253
plot_id,batch_id 0 48 miss% 0.7754814707548496
plot_id,batch_id 0 49 miss% 0.7793533951654898
plot_id,batch_id 0 50 miss% 0.7412500761181298
plot_id,batch_id 0 51 miss% 0.7632145953500103
plot_id,batch_id 0 52 miss% 0.7702675865427665
plot_id,batch_id 0 53 miss% 0.7749613060358989
plot_id,batch_id 0 54 miss% 0.7815610200609593
plot_id,batch_id 0 55 miss% 0.7354978321051439
plot_id,batch_id 0 56 miss% 0.7631271818117638
plot_id,batch_id 0 57 miss% 0.7716556783361125
plot_id,batch_id 0 58 miss% 0.7786384120531958
plot_id,batch_id 0 59 miss% 0.7807896465516836
plot_id,batch_id 0 60 miss% 0.5717072760127047
plot_id,batch_id 0 61 miss% 0.7085413574869153
plot_id,batch_id 0 62 miss% 0.7316289176259045
plot_id,batch_id 0 63 miss% 0.7541340194548459
plot_id,batch_id 0 64 miss% 0.7595768746777862
plot_id,batch_id 0 65 miss% 0.5612933472199869
plot_id,batch_id 0 66 miss% 0.6977802605339679
plot_id,batch_id 0 67 miss% 0.7222081845151935
plot_id,batch_id 0 68 miss% 0.7486632197904353
plot_id,batch_id 0 69 miss% 0.7503742441750096
plot_id,batch_id 0 70 miss% 0.5300436653983871
plot_id,batch_id 0 71 miss% 0.6972949981582299
plot_id,batch_id 0 72 miss% 0.7161597440515012
plot_id,batch_id 0 73 miss% 0.7385990311026707
plot_id,batch_id 0 74 miss% 0.7500215474676266
plot_id,batch_id 0 75 miss% 0.5150420701136073
plot_id,batch_id 0 76 miss% 0.6576119734734891
plot_id,batch_id 0 77 miss% 0.7082868241019041
plot_id,batch_id 0 78 miss% 0.7353518790707282
plot_id,batch_id 0 79 miss% 0.7384504854851047
plot_id,batch_id 0 80 miss% 0.5935745934449287
plot_id,batch_id 0 81 miss% 0.7284026896499337
plot_id,batch_id 0 82 miss% 0.7435708577247926
plot_id,batch_id 0 83 miss% 0.757411938503939
plot_id,batch_id 0 84 miss% 0.7645047527958279
plot_id,batch_id 0 85 miss% 0.5923336837904408
plot_id,batch_id 0 86 miss% 0.7192850095855271
plot_id,batch_id 0 87 miss% 0.7432327587629451
plot_id,batch_id 0 88 miss% 0.7641247729347337
plot_id,batch_id 0 89 miss% 0.7640816900094282
plot_id,batch_id 0 90 miss% 0.5530528919968654
plot_id,batch_id 0 91 miss% 0.709222947894755
plot_id,batch_id 0 92 miss% 0.7343857775752111
plot_id,batch_id 0 93 miss% 0.7478172622306564
plot_id,batch_id 0 94 miss% 0.7633209293996265
plot_id,batch_id 0 95 miss% 0.5472760058202661
plot_id,batch_id 0 96 miss% 0.6984745642706381
plot_id,batch_id 0 97 miss% 0.7303990734127277
plot_id,batch_id 0 98 miss% 0.7483773345263451
plot_id,batch_id 0 99 miss% 0.7519602628536689
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.65569464 0.74379859 0.75853444 0.77092487 0.76922565 0.66429831
 0.74107046 0.75501671 0.76601147 0.77437215 0.63289698 0.74281729
 0.75250991 0.76254922 0.77159214 0.64914285 0.73435136 0.76132104
 0.76421708 0.7672148  0.70681995 0.761366   0.76870157 0.77525443
 0.77650257 0.69502122 0.75471344 0.77233453 0.77020021 0.77915329
 0.70067188 0.7527405  0.76375664 0.76916546 0.77083062 0.69289329
 0.75519326 0.76660006 0.77174858 0.772031   0.73354351 0.76817481
 0.76896054 0.77657439 0.78110163 0.75510415 0.76790345 0.76983704
 0.77548147 0.7793534  0.74125008 0.7632146  0.77026759 0.77496131
 0.78156102 0.73549783 0.76312718 0.77165568 0.77863841 0.78078965
 0.57170728 0.70854136 0.73162892 0.75413402 0.75957687 0.56129335
 0.69778026 0.72220818 0.74866322 0.75037424 0.53004367 0.697295
 0.71615974 0.73859903 0.75002155 0.51504207 0.65761197 0.70828682
 0.73535188 0.73845049 0.59357459 0.72840269 0.74357086 0.75741194
 0.76450475 0.59233368 0.71928501 0.74323276 0.76412477 0.76408169
 0.55305289 0.70922295 0.73438578 0.74781726 0.76332093 0.54727601
 0.69847456 0.73039907 0.74837733 0.75196026]
for model  190 the mean error 0.7309783589501293
all id 190 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 31 out win 4 err 0.7309783589501293
Launcher: Job 191 completed in 5821 seconds.
Launcher: Task 254 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  28945
Epoch:0, Train loss:0.355650, valid loss:0.353806
Epoch:1, Train loss:0.019368, valid loss:0.003116
Epoch:2, Train loss:0.005179, valid loss:0.002893
Epoch:3, Train loss:0.004367, valid loss:0.002413
Epoch:4, Train loss:0.003786, valid loss:0.002052
Epoch:5, Train loss:0.003378, valid loss:0.001672
Epoch:6, Train loss:0.003097, valid loss:0.001976
Epoch:7, Train loss:0.002969, valid loss:0.001429
Epoch:8, Train loss:0.002800, valid loss:0.001452
Epoch:9, Train loss:0.002725, valid loss:0.001374
Epoch:10, Train loss:0.002553, valid loss:0.001113
Epoch:11, Train loss:0.001765, valid loss:0.000954
Epoch:12, Train loss:0.001784, valid loss:0.000978
Epoch:13, Train loss:0.001761, valid loss:0.000991
Epoch:14, Train loss:0.002004, valid loss:0.001165
Epoch:15, Train loss:0.001702, valid loss:0.000966
Epoch:16, Train loss:0.001708, valid loss:0.001168
Epoch:17, Train loss:0.001664, valid loss:0.000885
Epoch:18, Train loss:0.001680, valid loss:0.001042
Epoch:19, Train loss:0.001627, valid loss:0.000886
Epoch:20, Train loss:0.001639, valid loss:0.000927
Epoch:21, Train loss:0.001267, valid loss:0.000787
Epoch:22, Train loss:0.001255, valid loss:0.000927
Epoch:23, Train loss:0.001246, valid loss:0.000719
Epoch:24, Train loss:0.001233, valid loss:0.000700
Epoch:25, Train loss:0.001218, valid loss:0.000764
Epoch:26, Train loss:0.001235, valid loss:0.000750
Epoch:27, Train loss:0.001207, valid loss:0.000724
Epoch:28, Train loss:0.001224, valid loss:0.000791
Epoch:29, Train loss:0.001195, valid loss:0.000769
Epoch:30, Train loss:0.001166, valid loss:0.000717
Epoch:31, Train loss:0.001008, valid loss:0.000636
Epoch:32, Train loss:0.000989, valid loss:0.000646
Epoch:33, Train loss:0.000980, valid loss:0.000621
Epoch:34, Train loss:0.000989, valid loss:0.000682
Epoch:35, Train loss:0.001012, valid loss:0.000915
Epoch:36, Train loss:0.001078, valid loss:0.000667
Epoch:37, Train loss:0.000984, valid loss:0.000637
Epoch:38, Train loss:0.001045, valid loss:0.000681
Epoch:39, Train loss:0.000988, valid loss:0.000654
Epoch:40, Train loss:0.000968, valid loss:0.000782
Epoch:41, Train loss:0.000873, valid loss:0.000591
Epoch:42, Train loss:0.000868, valid loss:0.000679
Epoch:43, Train loss:0.000873, valid loss:0.000575
Epoch:44, Train loss:0.000867, valid loss:0.000582
Epoch:45, Train loss:0.000878, valid loss:0.000632
Epoch:46, Train loss:0.000859, valid loss:0.000587
Epoch:47, Train loss:0.000854, valid loss:0.000628
Epoch:48, Train loss:0.000851, valid loss:0.000601
Epoch:49, Train loss:0.000860, valid loss:0.000543
Epoch:50, Train loss:0.000844, valid loss:0.000586
Epoch:51, Train loss:0.000798, valid loss:0.000596
Epoch:52, Train loss:0.000797, valid loss:0.000544
Epoch:53, Train loss:0.000800, valid loss:0.000541
Epoch:54, Train loss:0.000793, valid loss:0.000594
Epoch:55, Train loss:0.000796, valid loss:0.000546
Epoch:56, Train loss:0.000791, valid loss:0.000536
Epoch:57, Train loss:0.000791, valid loss:0.000565
Epoch:58, Train loss:0.000790, valid loss:0.000538
Epoch:59, Train loss:0.000788, valid loss:0.000544
Epoch:60, Train loss:0.000789, valid loss:0.000547
training time 5642.305601596832
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.027155129665587058
plot_id,batch_id 0 1 miss% 0.03087062146445179
plot_id,batch_id 0 2 miss% 0.027187255167257927
plot_id,batch_id 0 3 miss% 0.02686108894952259
plot_id,batch_id 0 4 miss% 0.030868117859104805
plot_id,batch_id 0 5 miss% 0.03353185927335779
plot_id,batch_id 0 6 miss% 0.02563069087134448
plot_id,batch_id 0 7 miss% 0.026775365619452155
plot_id,batch_id 0 8 miss% 0.023771517133613144
plot_id,batch_id 0 9 miss% 0.015593749132178461
plot_id,batch_id 0 10 miss% 0.04913199259153415
plot_id,batch_id 0 11 miss% 0.038758118883863775
plot_id,batch_id 0 12 miss% 0.03251506161742514
plot_id,batch_id 0 13 miss% 0.032765616414830584
plot_id,batch_id 0 14 miss% 0.028121068929917593
plot_id,batch_id 0 15 miss% 0.027814125608956155
plot_id,batch_id 0 16 miss% 0.03646816771168451
plot_id,batch_id 0 17 miss% 0.040308380170252046
plot_id,batch_id 0 18 miss% 0.024569503952926428
plot_id,batch_id 0 19 miss% 0.020492280120985258
plot_id,batch_id 0 20 miss% 0.030710055688794816
plot_id,batch_id 0 21 miss% 0.02684028172278311
plot_id,batch_id 0 22 miss% 0.024850422686282975
plot_id,batch_id 0 23 miss% 0.021534983430070598
plot_id,batch_id 0 24 miss% 0.02164161509871348
plot_id,batch_id 0 25 miss% 0.02259423986267483
plot_id,batch_id 0 26 miss% 0.022909578547428457
plot_id,batch_id 0 27 miss% 0.027748933940102662
plot_id,batch_id 0 28 miss% 0.023882884028816203
plot_id,batch_id 0 29 miss% 0.021522929597082713
plot_id,batch_id 0 30 miss% 0.058895375667058096
plot_id,batch_id 0 31 miss% 0.033369230583097244
plot_id,batch_id 0 32 miss% 0.02939403829015772
plot_id,batch_id 0 33 miss% 0.0260399097405081
plot_id,batch_id 0 34 miss% 0.030290759071743726
plot_id,batch_id 0 35 miss% 0.03791905887152566
plot_id,batch_id 0 36 miss% 0.025554808428871045
plot_id,batch_id 0 37 miss% 0.03227357601742985
plot_id,batch_id 0 38 miss% 0.019429801489579766
plot_id,batch_id 0 39 miss% 0.03542031124004545
plot_id,batch_id 0 40 miss% 0.05103152633833964
plot_id,batch_id 0 41 miss% 0.034511894934966925
plot_id,batch_id 0 42 miss% 0.016924580926938045
plot_id,batch_id 0 43 miss% 0.026272646218144804
plot_id,batch_id 0 44 miss% 0.02520240593066292
plot_id,batch_id 0 45 miss% 0.025882090319136113
plot_id,batch_id 0 46 miss% 0.03346894625574984
plot_id,batch_id 0 47 miss% 0.026077312993948165
plot_id,batch_id 0 48 miss% 0.02053130727512254
plot_id,batch_id 0 49 miss% 0.025211367447250953
plot_id,batch_id 0 50 miss% 0.018354649749729373
plot_id,batch_id 0 51 miss% 0.019947781162313313
plot_id,batch_id 0 52 miss% 0.01757285661381898
plot_id,batch_id 0 53 miss% 0.01119011762455939
plot_id,batch_id 0 54 miss% 0.03240636062054208
plot_id,batch_id 0 55 miss% 0.05067013912134174
plot_id,batch_id 0 56 miss% 0.03021715649738254
plot_id,batch_id 0 57 miss% 0.02553972873864781
plot_id,batch_id 0 58 miss% 0.03084702484132856
plot_id,batch_id 0 59 miss% 0.026978740904112158
plot_id,batch_id 0 60 miss% 0.04237066620315416
plot_id,batch_id 0 61 miss% 0.03509116246225726
plot_id,batch_id 0 62 miss% 0.03488494958137609
plot_id,batch_id 0 63 miss% 0.02763492643890072
plot_id,batch_id 0 64 miss% 0.038673775133258245
plot_id,batch_id 0 65 miss% 0.05262846015767353
plot_id,batch_id 0 66 miss% 0.033435197556817745
plot_id,batch_id 0 67 miss% 0.028808840275099737
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  28945
Epoch:0, Train loss:0.472961, valid loss:0.467997
Epoch:1, Train loss:0.044239, valid loss:0.010797
Epoch:2, Train loss:0.012646, valid loss:0.005603
Epoch:3, Train loss:0.008192, valid loss:0.003951
Epoch:4, Train loss:0.006823, valid loss:0.003697
Epoch:5, Train loss:0.006045, valid loss:0.005071
Epoch:6, Train loss:0.005434, valid loss:0.003020
Epoch:7, Train loss:0.004883, valid loss:0.002859
Epoch:8, Train loss:0.004503, valid loss:0.002347
Epoch:9, Train loss:0.004082, valid loss:0.002006
Epoch:10, Train loss:0.003830, valid loss:0.002422
Epoch:11, Train loss:0.003112, valid loss:0.001701
Epoch:12, Train loss:0.002993, valid loss:0.001594
Epoch:13, Train loss:0.002905, valid loss:0.001555
Epoch:14, Train loss:0.002830, valid loss:0.001478
Epoch:15, Train loss:0.002733, valid loss:0.001626
Epoch:16, Train loss:0.002764, valid loss:0.001680
Epoch:17, Train loss:0.002592, valid loss:0.001424
Epoch:18, Train loss:0.002612, valid loss:0.001486
Epoch:19, Train loss:0.002547, valid loss:0.001639
Epoch:20, Train loss:0.002516, valid loss:0.001305
Epoch:21, Train loss:0.002142, valid loss:0.001310
Epoch:22, Train loss:0.002103, valid loss:0.001188
Epoch:23, Train loss:0.002090, valid loss:0.001296
Epoch:24, Train loss:0.002038, valid loss:0.001211
Epoch:25, Train loss:0.002068, valid loss:0.001332
Epoch:26, Train loss:0.002003, valid loss:0.001336
Epoch:27, Train loss:0.001999, valid loss:0.001184
Epoch:28, Train loss:0.001984, valid loss:0.001179
Epoch:29, Train loss:0.001977, valid loss:0.001232
Epoch:30, Train loss:0.001932, valid loss:0.001109
Epoch:31, Train loss:0.001749, valid loss:0.001119
Epoch:32, Train loss:0.001750, valid loss:0.001098
Epoch:33, Train loss:0.001722, valid loss:0.001025
Epoch:34, Train loss:0.001723, valid loss:0.001066
Epoch:35, Train loss:0.001715, valid loss:0.001083
Epoch:36, Train loss:0.001716, valid loss:0.001074
Epoch:37, Train loss:0.001708, valid loss:0.001135
Epoch:38, Train loss:0.001709, valid loss:0.001084
Epoch:39, Train loss:0.001682, valid loss:0.001117
Epoch:40, Train loss:0.001673, valid loss:0.001005
Epoch:41, Train loss:0.001574, valid loss:0.001001
Epoch:42, Train loss:0.001573, valid loss:0.000965
Epoch:43, Train loss:0.001577, valid loss:0.000994
Epoch:44, Train loss:0.001564, valid loss:0.000986
Epoch:45, Train loss:0.001559, valid loss:0.000984
Epoch:46, Train loss:0.001561, valid loss:0.000980
Epoch:47, Train loss:0.001547, valid loss:0.000987
Epoch:48, Train loss:0.001561, valid loss:0.001016
Epoch:49, Train loss:0.001544, valid loss:0.001015
Epoch:50, Train loss:0.001536, valid loss:0.001001
Epoch:51, Train loss:0.001492, valid loss:0.000966
Epoch:52, Train loss:0.001488, valid loss:0.000969
Epoch:53, Train loss:0.001483, valid loss:0.000966
Epoch:54, Train loss:0.001480, valid loss:0.000960
Epoch:55, Train loss:0.001480, valid loss:0.000959
Epoch:56, Train loss:0.001475, valid loss:0.000955
Epoch:57, Train loss:0.001478, valid loss:0.000964
Epoch:58, Train loss:0.001472, valid loss:0.000969
Epoch:59, Train loss:0.001472, valid loss:0.000970
Epoch:60, Train loss:0.001469, valid loss:0.000973
training time 5639.016443014145
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.036174263117207665
plot_id,batch_id 0 1 miss% 0.02008214899138457
plot_id,batch_id 0 2 miss% 0.025806743974467916
plot_id,batch_id 0 3 miss% 0.025066258980912973
plot_id,batch_id 0 4 miss% 0.02751003361749866
plot_id,batch_id 0 5 miss% 0.0432810740935073
plot_id,batch_id 0 6 miss% 0.03139211219825982
plot_id,batch_id 0 7 miss% 0.02090932615907708
plot_id,batch_id 0 8 miss% 0.031181151607837757
plot_id,batch_id 0 9 miss% 0.025816589551116206
plot_id,batch_id 0 10 miss% 0.04631246820457468
plot_id,batch_id 0 11 miss% 0.0561486956680739
plot_id,batch_id 0 12 miss% 0.030602271015258982
plot_id,batch_id 0 13 miss% 0.024619186464891207
plot_id,batch_id 0 14 miss% 0.04560256917970246
plot_id,batch_id 0 15 miss% 0.03849057288615767
plot_id,batch_id 0 16 miss% 0.04224241002891227
plot_id,batch_id 0 17 miss% 0.03314197975994127
plot_id,batch_id 0 18 miss% 0.03893753888123608
plot_id,batch_id 0 19 miss% 0.047034103521209826
plot_id,batch_id 0 20 miss% 0.037027685948868516
plot_id,batch_id 0 21 miss% 0.031139615131049358
plot_id,batch_id 0 22 miss% 0.03348251938142681
plot_id,batch_id 0 23 miss% 0.025745175443977836
plot_id,batch_id 0 24 miss% 0.027780674604475312
plot_id,batch_id 0 25 miss% 0.04236919418103218
plot_id,batch_id 0 26 miss% 0.029325003689074057
plot_id,batch_id 0 27 miss% 0.03208592882530326
plot_id,batch_id 0 28 miss% 0.030594383355550304
plot_id,batch_id 0 29 miss% 0.02621569868044428
plot_id,batch_id 0 30 miss% 0.04605279206764569
plot_id,batch_id 0 31 miss% 0.03723241189343234
plot_id,batch_id 0 32 miss% 0.032933058741442554
plot_id,batch_id 0 33 miss% 0.03102480356053382
plot_id,batch_id 0 34 miss% 0.02644668054352258
plot_id,batch_id 0 35 miss% 0.05064478692562701
plot_id,batch_id 0 36 miss% 0.03297408138139068
plot_id,batch_id 0 37 miss% 0.03451100027720227
plot_id,batch_id 0 38 miss% 0.022760340260888252
plot_id,batch_id 0 39 miss% 0.02276414673326149
plot_id,batch_id 0 40 miss% 0.05833813198478936
plot_id,batch_id 0 41 miss% 0.022139158883178314
plot_id,batch_id 0 42 miss% 0.016053740928825177
plot_id,batch_id 0 43 miss% 0.023739890406586594
plot_id,batch_id 0 44 miss% 0.024768438359959233
plot_id,batch_id 0 45 miss% 0.03276082842897955
plot_id,batch_id 0 46 miss% 0.037049513436828836
plot_id,batch_id 0 47 miss% 0.01835925657928698
plot_id,batch_id 0 48 miss% 0.023679745875204154
plot_id,batch_id 0 49 miss% 0.024115050131957035
plot_id,batch_id 0 50 miss% 0.03103282956824882
plot_id,batch_id 0 51 miss% 0.03796183279012059
plot_id,batch_id 0 52 miss% 0.024179542052375932
plot_id,batch_id 0 53 miss% 0.013501920087000715
plot_id,batch_id 0 54 miss% 0.02653759738649634
plot_id,batch_id 0 55 miss% 0.03209804797650756
plot_id,batch_id 0 56 miss% 0.038854114626278566
plot_id,batch_id 0 57 miss% 0.028469569370306036
plot_id,batch_id 0 58 miss% 0.027396488089468544
plot_id,batch_id 0 59 miss% 0.024149062079686336
plot_id,batch_id 0 60 miss% 0.02881315534047115
plot_id,batch_id 0 61 miss% 0.035158507364795116
plot_id,batch_id 0 62 miss% 0.019381313250816913
plot_id,batch_id 0 63 miss% 0.03602394088806058
plot_id,batch_id 0 64 miss% 0.031033088545898032
plot_id,batch_id 0 65 miss% 0.03825363227701408
plot_id,batch_id 0 66 miss% 0.04257077307969518
plot_id,batch_id 0 67 miss% 0.05038957829356038
plot_id,batch_id 0 68 miss% 0.04092049992510868
plot_id,batch_id 0 69 plot_id,batch_id 0 68 miss% 0.02554002698746515
plot_id,batch_id 0 69 miss% 0.028786336760505467
plot_id,batch_id 0 70 miss% 0.05561352445806487
plot_id,batch_id 0 71 miss% 0.05907851342131089
plot_id,batch_id 0 72 miss% 0.022512078153412544
plot_id,batch_id 0 73 miss% 0.02829762658976998
plot_id,batch_id 0 74 miss% 0.04485595259608277
plot_id,batch_id 0 75 miss% 0.029994890255113173
plot_id,batch_id 0 76 miss% 0.023062875933211158
plot_id,batch_id 0 77 miss% 0.027547653835417078
plot_id,batch_id 0 78 miss% 0.049946475890075996
plot_id,batch_id 0 79 miss% 0.0485981344729522
plot_id,batch_id 0 80 miss% 0.0334552518167873
plot_id,batch_id 0 81 miss% 0.021419357591414764
plot_id,batch_id 0 82 miss% 0.03318480144409981
plot_id,batch_id 0 83 miss% 0.017978835540206444
plot_id,batch_id 0 84 miss% 0.02438820903980042
plot_id,batch_id 0 85 miss% 0.057328898402795245
plot_id,batch_id 0 86 miss% 0.02625899601713803
plot_id,batch_id 0 87 miss% 0.025693105747152285
plot_id,batch_id 0 88 miss% 0.0351109651386541
plot_id,batch_id 0 89 miss% 0.030222196003759902
plot_id,batch_id 0 90 miss% 0.04309345794156102
plot_id,batch_id 0 91 miss% 0.04790761494395087
plot_id,batch_id 0 92 miss% 0.03656381047624118
plot_id,batch_id 0 93 miss% 0.03111355913942644
plot_id,batch_id 0 94 miss% 0.027323748638252005
plot_id,batch_id 0 95 miss% 0.04858832709480288
plot_id,batch_id 0 96 miss% 0.027908959229194164
plot_id,batch_id 0 97 miss% 0.03378976162083403
plot_id,batch_id 0 98 miss% 0.023596785298163836
plot_id,batch_id 0 99 miss% 0.029123390291966336
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02715513 0.03087062 0.02718726 0.02686109 0.03086812 0.03353186
 0.02563069 0.02677537 0.02377152 0.01559375 0.04913199 0.03875812
 0.03251506 0.03276562 0.02812107 0.02781413 0.03646817 0.04030838
 0.0245695  0.02049228 0.03071006 0.02684028 0.02485042 0.02153498
 0.02164162 0.02259424 0.02290958 0.02774893 0.02388288 0.02152293
 0.05889538 0.03336923 0.02939404 0.02603991 0.03029076 0.03791906
 0.02555481 0.03227358 0.0194298  0.03542031 0.05103153 0.03451189
 0.01692458 0.02627265 0.02520241 0.02588209 0.03346895 0.02607731
 0.02053131 0.02521137 0.01835465 0.01994778 0.01757286 0.01119012
 0.03240636 0.05067014 0.03021716 0.02553973 0.03084702 0.02697874
 0.04237067 0.03509116 0.03488495 0.02763493 0.03867378 0.05262846
 0.0334352  0.02880884 0.02554003 0.02878634 0.05561352 0.05907851
 0.02251208 0.02829763 0.04485595 0.02999489 0.02306288 0.02754765
 0.04994648 0.04859813 0.03345525 0.02141936 0.0331848  0.01797884
 0.02438821 0.0573289  0.026259   0.02569311 0.03511097 0.0302222
 0.04309346 0.04790761 0.03656381 0.03111356 0.02732375 0.04858833
 0.02790896 0.03378976 0.02359679 0.02912339]
for model  225 the mean error 0.031122632382331684
all id 225 hidden_dim 16 learning_rate 0.02 num_layers 4 frames 31 out win 3 err 0.031122632382331684
Launcher: Job 226 completed in 5829 seconds.
Launcher: Task 20 done. Exiting.
miss% 0.03888807601382826
plot_id,batch_id 0 70 miss% 0.05463013181172487
plot_id,batch_id 0 71 miss% 0.07634711515455939
plot_id,batch_id 0 72 miss% 0.03496001319337041
plot_id,batch_id 0 73 miss% 0.03962276286321666
plot_id,batch_id 0 74 miss% 0.05441737677528779
plot_id,batch_id 0 75 miss% 0.07001860296326863
plot_id,batch_id 0 76 miss% 0.04615462871664815
plot_id,batch_id 0 77 miss% 0.0386776797895981
plot_id,batch_id 0 78 miss% 0.0402889198616044
plot_id,batch_id 0 79 miss% 0.04867456055460477
plot_id,batch_id 0 80 miss% 0.060297942334415724
plot_id,batch_id 0 81 miss% 0.028471717195129365
plot_id,batch_id 0 82 miss% 0.03742938247231145
plot_id,batch_id 0 83 miss% 0.029695698814354778
plot_id,batch_id 0 84 miss% 0.03380704102424906
plot_id,batch_id 0 85 miss% 0.04565035231008323
plot_id,batch_id 0 86 miss% 0.03317757967552918
plot_id,batch_id 0 87 miss% 0.042645676881323556
plot_id,batch_id 0 88 miss% 0.039036026887123786
plot_id,batch_id 0 89 miss% 0.0426468358203512
plot_id,batch_id 0 90 miss% 0.05089636384660467
plot_id,batch_id 0 91 miss% 0.04732420147942
plot_id,batch_id 0 92 miss% 0.045472055424982526
plot_id,batch_id 0 93 miss% 0.04755228064814711
plot_id,batch_id 0 94 miss% 0.06074071991760561
plot_id,batch_id 0 95 miss% 0.04853995855472314
plot_id,batch_id 0 96 miss% 0.04034941110334244
plot_id,batch_id 0 97 miss% 0.04813133962147779
plot_id,batch_id 0 98 miss% 0.03173541822464154
plot_id,batch_id 0 99 miss% 0.048921725914714796
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03617426 0.02008215 0.02580674 0.02506626 0.02751003 0.04328107
 0.03139211 0.02090933 0.03118115 0.02581659 0.04631247 0.0561487
 0.03060227 0.02461919 0.04560257 0.03849057 0.04224241 0.03314198
 0.03893754 0.0470341  0.03702769 0.03113962 0.03348252 0.02574518
 0.02778067 0.04236919 0.029325   0.03208593 0.03059438 0.0262157
 0.04605279 0.03723241 0.03293306 0.0310248  0.02644668 0.05064479
 0.03297408 0.034511   0.02276034 0.02276415 0.05833813 0.02213916
 0.01605374 0.02373989 0.02476844 0.03276083 0.03704951 0.01835926
 0.02367975 0.02411505 0.03103283 0.03796183 0.02417954 0.01350192
 0.0265376  0.03209805 0.03885411 0.02846957 0.02739649 0.02414906
 0.02881316 0.03515851 0.01938131 0.03602394 0.03103309 0.03825363
 0.04257077 0.05038958 0.0409205  0.03888808 0.05463013 0.07634712
 0.03496001 0.03962276 0.05441738 0.0700186  0.04615463 0.03867768
 0.04028892 0.04867456 0.06029794 0.02847172 0.03742938 0.0296957
 0.03380704 0.04565035 0.03317758 0.04264568 0.03903603 0.04264684
 0.05089636 0.0473242  0.04547206 0.04755228 0.06074072 0.04853996
 0.04034941 0.04813134 0.03173542 0.04892173]
for model  91 the mean error 0.03636392323383124
all id 91 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 25 out win 4 err 0.03636392323383124
Launcher: Job 92 completed in 5840 seconds.
Launcher: Task 35 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  46193
Epoch:0, Train loss:0.533217, valid loss:0.496812
Epoch:1, Train loss:0.356751, valid loss:0.360811
Epoch:2, Train loss:0.344930, valid loss:0.358890
Epoch:3, Train loss:0.343041, valid loss:0.358442
Epoch:4, Train loss:0.342231, valid loss:0.358506
Epoch:5, Train loss:0.341564, valid loss:0.358140
Epoch:6, Train loss:0.341287, valid loss:0.357804
Epoch:7, Train loss:0.340991, valid loss:0.357576
Epoch:8, Train loss:0.340816, valid loss:0.358010
Epoch:9, Train loss:0.340723, valid loss:0.357573
Epoch:10, Train loss:0.340604, valid loss:0.357705
Epoch:11, Train loss:0.339869, valid loss:0.357279
Epoch:12, Train loss:0.339802, valid loss:0.357322
Epoch:13, Train loss:0.339749, valid loss:0.357168
Epoch:14, Train loss:0.339783, valid loss:0.357263
Epoch:15, Train loss:0.339733, valid loss:0.357381
Epoch:16, Train loss:0.339689, valid loss:0.357146
Epoch:17, Train loss:0.339642, valid loss:0.357154
Epoch:18, Train loss:0.339613, valid loss:0.357268
Epoch:19, Train loss:0.339617, valid loss:0.357140
Epoch:20, Train loss:0.339578, valid loss:0.357168
Epoch:21, Train loss:0.339239, valid loss:0.357030
Epoch:22, Train loss:0.339224, valid loss:0.356988
Epoch:23, Train loss:0.339189, valid loss:0.356957
Epoch:24, Train loss:0.339208, valid loss:0.357072
Epoch:25, Train loss:0.339173, valid loss:0.356986
Epoch:26, Train loss:0.339168, valid loss:0.357102
Epoch:27, Train loss:0.339189, valid loss:0.357059
Epoch:28, Train loss:0.339174, valid loss:0.357058
Epoch:29, Train loss:0.339147, valid loss:0.357111
Epoch:30, Train loss:0.339126, valid loss:0.356914
Epoch:31, Train loss:0.338980, valid loss:0.356873
Epoch:32, Train loss:0.338952, valid loss:0.356916
Epoch:33, Train loss:0.338963, valid loss:0.357000
Epoch:34, Train loss:0.338965, valid loss:0.356892
Epoch:35, Train loss:0.338965, valid loss:0.356901
Epoch:36, Train loss:0.338940, valid loss:0.356936
Epoch:37, Train loss:0.338935, valid loss:0.356882
Epoch:38, Train loss:0.338930, valid loss:0.356989
Epoch:39, Train loss:0.338939, valid loss:0.356952
Epoch:40, Train loss:0.338929, valid loss:0.356880
Epoch:41, Train loss:0.338847, valid loss:0.356872
Epoch:42, Train loss:0.338844, valid loss:0.356872
Epoch:43, Train loss:0.338840, valid loss:0.356859
Epoch:44, Train loss:0.338838, valid loss:0.356878
Epoch:45, Train loss:0.338838, valid loss:0.356869
Epoch:46, Train loss:0.338835, valid loss:0.356890
Epoch:47, Train loss:0.338834, valid loss:0.356863
Epoch:48, Train loss:0.338832, valid loss:0.356863
Epoch:49, Train loss:0.338824, valid loss:0.356853
Epoch:50, Train loss:0.338826, valid loss:0.356865
Epoch:51, Train loss:0.338786, valid loss:0.356836
Epoch:52, Train loss:0.338787, valid loss:0.356819
Epoch:53, Train loss:0.338785, valid loss:0.356848
Epoch:54, Train loss:0.338783, valid loss:0.356849
Epoch:55, Train loss:0.338784, valid loss:0.356850
Epoch:56, Train loss:0.338779, valid loss:0.356833
Epoch:57, Train loss:0.338778, valid loss:0.356831
Epoch:58, Train loss:0.338778, valid loss:0.356820
Epoch:59, Train loss:0.338776, valid loss:0.356855
Epoch:60, Train loss:0.338776, valid loss:0.356829
training time 5671.769480228424
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7301971155769861
plot_id,batch_id 0 1 miss% 0.7903238880638491
plot_id,batch_id 0 2 miss% 0.8023673698782566
plot_id,batch_id 0 3 miss% 0.8100194756255035
plot_id,batch_id 0 4 miss% 0.8078757525136324
plot_id,batch_id 0 5 miss% 0.7252018441001041
plot_id,batch_id 0 6 miss% 0.7824141856093345
plot_id,batch_id 0 7 miss% 0.7986094328387069
plot_id,batch_id 0 8 miss% 0.8059140634124218
plot_id,batch_id 0 9 miss% 0.8113270678480081
plot_id,batch_id 0 10 miss% 0.7015920292643119
plot_id,batch_id 0 11 miss% 0.7817545921626594
plot_id,batch_id 0 12 miss% 0.7939712562747737
plot_id,batch_id 0 13 miss% 0.8030070156490678
plot_id,batch_id 0 14 miss% 0.8093525940199756
plot_id,batch_id 0 15 miss% 0.7189226523764566
plot_id,batch_id 0 16 miss% 0.7827008411046602
plot_id,batch_id 0 17 miss% 0.7975211654785036
plot_id,batch_id 0 18 miss% 0.8078966146775632
plot_id,batch_id 0 19 miss% 0.8078736244212207
plot_id,batch_id 0 20 miss% 0.7677654158294147
plot_id,batch_id 0 21 miss% 0.8050580254296248
plot_id,batch_id 0 22 miss% 0.8100574338171157
plot_id,batch_id 0 23 miss% 0.8136459315315959
plot_id,batch_id 0 24 miss% 0.8156454657831422
plot_id,batch_id 0 25 miss% 0.7573865653912707
plot_id,batch_id 0 26 miss% 0.7973466856937548
plot_id,batch_id 0 27 miss% 0.802151155083035
plot_id,batch_id 0 28 miss% 0.8092512892942147
plot_id,batch_id 0 29 miss% 0.8103953400261167
plot_id,batch_id 0 30 miss% 0.756899700969001
plot_id,batch_id 0 31 miss% 0.7925006944169039
plot_id,batch_id 0 32 miss% 0.8041192892045765
plot_id,batch_id 0 33 miss% 0.8069009938078568
plot_id,batch_id 0 34 miss% 0.8086324584818212
plot_id,batch_id 0 35 miss% 0.7551831964838911
plot_id,batch_id 0 36 miss% 0.7969667460617881
plot_id,batch_id 0 37 miss% 0.8024267568364777
plot_id,batch_id 0 38 miss% 0.8088448135803099
plot_id,batch_id 0 39 miss% 0.8101449630749675
plot_id,batch_id 0 40 miss% 0.7825008641169439
plot_id,batch_id 0 41 miss% 0.8061375247373515
plot_id,batch_id 0 42 miss% 0.8114230750419537
plot_id,batch_id 0 43 miss% 0.8150920986703781
plot_id,batch_id 0 44 miss% 0.8188172765642687
plot_id,batch_id 0 45 miss% 0.7797566934655941
plot_id,batch_id 0 46 miss% 0.8060981612433884
plot_id,batch_id 0 47 miss% 0.8126681487458258
plot_id,batch_id 0 48 miss% 0.8127153408630335
plot_id,batch_id 0 49 miss% 0.8179906924335014
plot_id,batch_id 0 50 miss% 0.7891181238877046
plot_id,batch_id 0 51 miss% 0.8042536029521384
plot_id,batch_id 0 52 miss% 0.8090445128886481
plot_id,batch_id 0 53 miss% 0.8136334632936231
plot_id,batch_id 0 54 miss% 0.8160414395541051
plot_id,batch_id 0 55 miss% 0.7652624667418837
plot_id,batch_id 0 56 miss% 0.8046110822608142
plot_id,batch_id 0 57 miss% 0.8086102716831648
plot_id,batch_id 0 58 miss% 0.8135036327041584
plot_id,batch_id 0 59 miss% 0.8175921499355446
plot_id,batch_id 0 60 miss% 0.6516228163448123
plot_id,batch_id 0 61 miss% 0.7483895610233907
plot_id,batch_id 0 62 miss% 0.784974709877539
plot_id,batch_id 0 63 miss% 0.7979964213689057
plot_id,batch_id 0 64 miss% 0.8047005129697306
plot_id,batch_id 0 65 miss% 0.6412941933388607
plot_id,batch_id 0 66 miss% 0.7525052317076788
plot_id,batch_id 0 67 miss% 0.7720054913779835
plot_id,batch_id 0 68 miss% 0.7951496612171759
plot_id,batch_id 0 69 miss% 0.7974152631952046
plot_id,batch_id 0 70 miss% 0.6110163240653099
plot_id,batch_id 0 71 miss% 0.7674977521706469
plot_id,batch_id 0 72 miss% 0.7643475221701759
plot_id,batch_id 0 73 miss% 0.7795261827480571
plot_id,batch_id 0 74 miss% 0.7888441252520112
plot_id,batch_id 0 75 miss% 0.6102259760014188
plot_id,batch_id 0 76 miss% 0.712893060142401
plot_id,batch_id 0 77 miss% 0.75204110464439
plot_id,batch_id 0 78 miss% 0.7778562418449593
plot_id,batch_id 0 79 miss% 0.7850913625237529
plot_id,batch_id 0 80 miss% 0.6710981416055254
plot_id,batch_id 0 81 miss% 0.7800521661099391
plot_id,batch_id 0 82 miss% 0.7914098243297187
plot_id,batch_id 0 83 miss% 0.8001526274504664
plot_id,batch_id 0 84 miss% 0.8048141721480135
plot_id,batch_id 0 85 miss% 0.6686300156903985
plot_id,batch_id 0 86 miss% 0.7705354051392735
plot_id,batch_id 0 87 miss% 0.7902703397563458
plot_id,batch_id 0 88 miss% 0.7983354443041168
plot_id,batch_id 0 89 miss% 0.7997197775516802
plot_id,batch_id 0 90 miss% 0.642310733480868
plot_id,batch_id 0 91 miss% 0.7689051034259088
plot_id,batch_id 0 92 miss% 0.7793309123781601
plot_id,batch_id 0 93 miss% 0.7910140111766927
plot_id,batch_id 0 94 miss% 0.8003143658081829
plot_id,batch_id 0 95 miss% 0.6461756349467116
plot_id,batch_id 0 96 miss% 0.7557895813524936
plot_id,batch_id 0 97 miss% 0.7762201807551916
plot_id,batch_id 0 98 miss% 0.7883899884936753
plot_id,batch_id 0 99 miss% 0.7929948615158975
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73019712 0.79032389 0.80236737 0.81001948 0.80787575 0.72520184
 0.78241419 0.79860943 0.80591406 0.81132707 0.70159203 0.78175459
 0.79397126 0.80300702 0.80935259 0.71892265 0.78270084 0.79752117
 0.80789661 0.80787362 0.76776542 0.80505803 0.81005743 0.81364593
 0.81564547 0.75738657 0.79734669 0.80215116 0.80925129 0.81039534
 0.7568997  0.79250069 0.80411929 0.80690099 0.80863246 0.7551832
 0.79696675 0.80242676 0.80884481 0.81014496 0.78250086 0.80613752
 0.81142308 0.8150921  0.81881728 0.77975669 0.80609816 0.81266815
 0.81271534 0.81799069 0.78911812 0.8042536  0.80904451 0.81363346
 0.81604144 0.76526247 0.80461108 0.80861027 0.81350363 0.81759215
 0.65162282 0.74838956 0.78497471 0.79799642 0.80470051 0.64129419
 0.75250523 0.77200549 0.79514966 0.79741526 0.61101632 0.76749775
 0.76434752 0.77952618 0.78884413 0.61022598 0.71289306 0.7520411
 0.77785624 0.78509136 0.67109814 0.78005217 0.79140982 0.80015263
 0.80481417 0.66863002 0.77053541 0.79027034 0.79833544 0.79971978
 0.64231073 0.7689051  0.77933091 0.79101401 0.80031437 0.64617563
 0.75578958 0.77622018 0.78838999 0.79299486]
for model  85 the mean error 0.7781689292488057
all id 85 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 25 out win 4 err 0.7781689292488057
Launcher: Job 86 completed in 5852 seconds.
Launcher: Task 114 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  46193
Epoch:0, Train loss:0.668440, valid loss:0.637867
Epoch:1, Train loss:0.515069, valid loss:0.520283
Epoch:2, Train loss:0.501510, valid loss:0.519152
Epoch:3, Train loss:0.498398, valid loss:0.518699
Epoch:4, Train loss:0.497093, valid loss:0.517556
Epoch:5, Train loss:0.496558, valid loss:0.517131
Epoch:6, Train loss:0.495847, valid loss:0.517118
Epoch:7, Train loss:0.495519, valid loss:0.516863
Epoch:8, Train loss:0.495296, valid loss:0.517009
Epoch:9, Train loss:0.495117, valid loss:0.516510
Epoch:10, Train loss:0.494824, valid loss:0.517408
Epoch:11, Train loss:0.493516, valid loss:0.516084
Epoch:12, Train loss:0.493565, valid loss:0.516316
Epoch:13, Train loss:0.493464, valid loss:0.516654
Epoch:14, Train loss:0.493369, valid loss:0.516101
Epoch:15, Train loss:0.493247, valid loss:0.516062
Epoch:16, Train loss:0.493277, valid loss:0.515922
Epoch:17, Train loss:0.493221, valid loss:0.515911
Epoch:18, Train loss:0.493298, valid loss:0.516050
Epoch:19, Train loss:0.493160, valid loss:0.515989
Epoch:20, Train loss:0.493030, valid loss:0.515736
Epoch:21, Train loss:0.492385, valid loss:0.515688
Epoch:22, Train loss:0.492395, valid loss:0.515508
Epoch:23, Train loss:0.492345, valid loss:0.515541
Epoch:24, Train loss:0.492350, valid loss:0.515627
Epoch:25, Train loss:0.492404, valid loss:0.515826
Epoch:26, Train loss:0.492343, valid loss:0.515378
Epoch:27, Train loss:0.492330, valid loss:0.515474
Epoch:28, Train loss:0.492277, valid loss:0.515548
Epoch:29, Train loss:0.492271, valid loss:0.515585
Epoch:30, Train loss:0.492345, valid loss:0.515462
Epoch:31, Train loss:0.491947, valid loss:0.515347
Epoch:32, Train loss:0.491918, valid loss:0.515355
Epoch:33, Train loss:0.491915, valid loss:0.515277
Epoch:34, Train loss:0.491916, valid loss:0.515377
Epoch:35, Train loss:0.491911, valid loss:0.515467
Epoch:36, Train loss:0.491890, valid loss:0.515335
Epoch:37, Train loss:0.491924, valid loss:0.515330
Epoch:38, Train loss:0.491917, valid loss:0.515302
Epoch:39, Train loss:0.491857, valid loss:0.515428
Epoch:40, Train loss:0.491881, valid loss:0.515301
Epoch:41, Train loss:0.491702, valid loss:0.515249
Epoch:42, Train loss:0.491691, valid loss:0.515222
Epoch:43, Train loss:0.491696, valid loss:0.515290
Epoch:44, Train loss:0.491706, valid loss:0.515228
Epoch:45, Train loss:0.491694, valid loss:0.515234
Epoch:46, Train loss:0.491684, valid loss:0.515241
Epoch:47, Train loss:0.491728, valid loss:0.515225
Epoch:48, Train loss:0.491676, valid loss:0.515201
Epoch:49, Train loss:0.491650, valid loss:0.515226
Epoch:50, Train loss:0.491676, valid loss:0.515365
Epoch:51, Train loss:0.491610, valid loss:0.515170
Epoch:52, Train loss:0.491586, valid loss:0.515183
Epoch:53, Train loss:0.491614, valid loss:0.515174
Epoch:54, Train loss:0.491578, valid loss:0.515184
Epoch:55, Train loss:0.491583, valid loss:0.515212
Epoch:56, Train loss:0.491583, valid loss:0.515181
Epoch:57, Train loss:0.491581, valid loss:0.515166
Epoch:58, Train loss:0.491593, valid loss:0.515184
Epoch:59, Train loss:0.491584, valid loss:0.515253
Epoch:60, Train loss:0.491565, valid loss:0.515169
training time 5740.5091116428375
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.8175646791477899
plot_id,batch_id 0 1 miss% 0.8536545008773376
plot_id,batch_id 0 2 miss% 0.8628076053995559
plot_id,batch_id 0 3 miss% 0.8654633692377307
plot_id,batch_id 0 4 miss% 0.8690019756336183
plot_id,batch_id 0 5 miss% 0.814466961572531
plot_id,batch_id 0 6 miss% 0.8524826901132119
plot_id,batch_id 0 7 miss% 0.8640483292356512
plot_id,batch_id 0 8 miss% 0.8631439595578831
plot_id,batch_id 0 9 miss% 0.8665380309035127
plot_id,batch_id 0 10 miss% 0.8030862059746111
plot_id,batch_id 0 11 miss% 0.8513261605290322
plot_id,batch_id 0 12 miss% 0.861638876947886
plot_id,batch_id 0 13 miss% 0.8609548750754096
plot_id,batch_id 0 14 miss% 0.8672097075041073
plot_id,batch_id 0 15 miss% 0.8040108760986399
plot_id,batch_id 0 16 miss% 0.8470012973094152
plot_id,batch_id 0 17 miss% 0.8578671550785668
plot_id,batch_id 0 18 miss% 0.8623650648323966
plot_id,batch_id 0 19 miss% 0.8607591330925489
plot_id,batch_id 0 20 miss% 0.8392388736902775
plot_id,batch_id 0 21 miss% 0.8624762402355445
plot_id,batch_id 0 22 miss% 0.8657042379668032
plot_id,batch_id 0 23 miss% 0.8679810718661359
plot_id,batch_id 0 24 miss% 0.8690627153867776
plot_id,batch_id 0 25 miss% 0.8354944961943257
plot_id,batch_id 0 26 miss% 0.8583482021601803
plot_id,batch_id 0 27 miss% 0.8639754114457234
plot_id,batch_id 0 28 miss% 0.8671540577437556
plot_id,batch_id 0 29 miss% 0.8685876679371146
plot_id,batch_id 0 30 miss% 0.8245732045807543
plot_id,batch_id 0 31 miss% 0.857437557818374
plot_id,batch_id 0 32 miss% 0.8621483834787173
plot_id,batch_id 0 33 miss% 0.863056543086493
plot_id,batch_id 0 34 miss% 0.8643684288201188
plot_id,batch_id 0 35 miss% 0.824766341036277
plot_id,batch_id 0 36 miss% 0.8606191160008061
plot_id,batch_id 0 37 miss% 0.8619920035948428
plot_id,batch_id 0 38 miss% 0.8647168187009631
plot_id,batch_id 0 39 miss% 0.8671137218642208
plot_id,batch_id 0 40 miss% 0.8514365978732773
plot_id,batch_id 0 41 miss% 0.8636011125923754
plot_id,batch_id 0 42 miss% 0.8643363614498376
plot_id,batch_id 0 43 miss% 0.8682747594444366
plot_id,batch_id 0 44 miss% 0.869060051752822
plot_id,batch_id 0 45 miss% 0.8469179828602538
plot_id,batch_id 0 46 miss% 0.8638965220164254
plot_id,batch_id 0 47 miss% 0.8655180015380457
plot_id,batch_id 0 48 miss% 0.8700273851032533
plot_id,batch_id 0 49 miss% 0.8688778340257653
plot_id,batch_id 0 50 miss% 0.8502147042002599
plot_id,batch_id 0 51 miss% 0.863076988070763
plot_id,batch_id 0 52 miss% 0.8655101668779867
plot_id,batch_id 0 53 miss% 0.868299416505979
plot_id,batch_id 0 54 miss% 0.8701187096369628
plot_id,batch_id 0 55 miss% 0.8473304544539335
plot_id,batch_id 0 56 miss% 0.8629641875524037
plot_id,batch_id 0 57 miss% 0.8667057043289175
plot_id,batch_id 0 58 miss% 0.8694605307131527
plot_id,batch_id 0 59 miss% 0.8683037419649394
plot_id,batch_id 0 60 miss% 0.7691910673606572
plot_id,batch_id 0 61 miss% 0.8409679982713063
plot_id,batch_id 0 62 miss% 0.8551152977750791
plot_id,batch_id 0 63 miss% 0.8569179237688109
plot_id,batch_id 0 64 miss% 0.8608882498056618
plot_id,batch_id 0 65 miss% 0.7605959822520073
plot_id,batch_id 0 66 miss% 0.8326600568466119
plot_id,batch_id 0 67 miss% 0.8450607283706842
plot_id,batch_id 0 68 miss% 0.8558087936625193
plot_id,batch_id 0 69 miss% 0.8587729382253727
plot_id,batch_id 0 70 miss% 0.723576362056922
plot_id,batch_id 0 71 miss% 0.8354258123704811
plot_id,batch_id 0 72 miss% 0.8409934938308405
plot_id,batch_id 0 73 miss% 0.8480529167152566
plot_id,batch_id 0 74 miss% 0.8560725877919896
plot_id,batch_id 0 75 miss% 0.7532860911022891
plot_id,batch_id 0 76 miss% 0.8181936453824681
plot_id,batch_id 0 77 miss% 0.832845057698372
plot_id,batch_id 0 78 miss% 0.8416781254780992
plot_id,batch_id 0 79 miss% 0.8514493340035137
plot_id,batch_id 0 80 miss% 0.7901971690550249
plot_id,batch_id 0 81 miss% 0.848977035513881
plot_id,batch_id 0 82 miss% 0.8571184755074339
plot_id,batch_id 0 83 miss% 0.8585499903813858
plot_id,batch_id 0 84 miss% 0.8679626176939719
plot_id,batch_id 0 85 miss% 0.7797179792688094
plot_id,batch_id 0 86 miss% 0.8426164399933371
plot_id,batch_id 0 87 miss% 0.8522612318430822
plot_id,batch_id 0 88 miss% 0.861696316839663
plot_id,batch_id 0 89 miss% 0.8624885836939756
plot_id,batch_id 0 90 miss% 0.7524682686664902
plot_id,batch_id 0 91 miss% 0.8326945214831478
plot_id,batch_id 0 92 miss% 0.8480314845156928
plot_id,batch_id 0 93 miss% 0.8511049468116905
plot_id,batch_id 0 94 miss% 0.8618407655189063
plot_id,batch_id 0 95 miss% 0.7624872652146474
plot_id,batch_id 0 96 miss% 0.8274337132909996
plot_id,batch_id 0 97 miss% 0.8468358987086707
plot_id,batch_id 0 98 miss% 0.852182887238558
plot_id,batch_id 0 99 miss% 0.8559163930999542
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.81756468 0.8536545  0.86280761 0.86546337 0.86900198 0.81446696
 0.85248269 0.86404833 0.86314396 0.86653803 0.80308621 0.85132616
 0.86163888 0.86095488 0.86720971 0.80401088 0.8470013  0.85786716
 0.86236506 0.86075913 0.83923887 0.86247624 0.86570424 0.86798107
 0.86906272 0.8354945  0.8583482  0.86397541 0.86715406 0.86858767
 0.8245732  0.85743756 0.86214838 0.86305654 0.86436843 0.82476634
 0.86061912 0.861992   0.86471682 0.86711372 0.8514366  0.86360111
 0.86433636 0.86827476 0.86906005 0.84691798 0.86389652 0.865518
 0.87002739 0.86887783 0.8502147  0.86307699 0.86551017 0.86829942
 0.87011871 0.84733045 0.86296419 0.8667057  0.86946053 0.86830374
 0.76919107 0.840968   0.8551153  0.85691792 0.86088825 0.76059598
 0.83266006 0.84506073 0.85580879 0.85877294 0.72357636 0.83542581
 0.84099349 0.84805292 0.85607259 0.75328609 0.81819365 0.83284506
 0.84167813 0.85144933 0.79019717 0.84897704 0.85711848 0.85854999
 0.86796262 0.77971798 0.84261644 0.85226123 0.86169632 0.86248858
 0.75246827 0.83269452 0.84803148 0.85110495 0.86184077 0.76248727
 0.82743371 0.8468359  0.85218289 0.85591639]
for model  32 the mean error 0.846682722077997
all id 32 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 21 out win 5 err 0.846682722077997
Launcher: Job 33 completed in 5921 seconds.
Launcher: Task 17 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  46193
Epoch:0, Train loss:0.434749, valid loss:0.392697
Epoch:1, Train loss:0.080016, valid loss:0.003669
Epoch:2, Train loss:0.005107, valid loss:0.002370
Epoch:3, Train loss:0.003957, valid loss:0.002223
Epoch:4, Train loss:0.003265, valid loss:0.001770
Epoch:5, Train loss:0.002899, valid loss:0.001427
Epoch:6, Train loss:0.002629, valid loss:0.001415
Epoch:7, Train loss:0.002361, valid loss:0.001409
Epoch:8, Train loss:0.002245, valid loss:0.001417
Epoch:9, Train loss:0.002082, valid loss:0.001119
Epoch:10, Train loss:0.002005, valid loss:0.001240
Epoch:11, Train loss:0.001553, valid loss:0.000981
Epoch:12, Train loss:0.001519, valid loss:0.000860
Epoch:13, Train loss:0.001483, valid loss:0.000768
Epoch:14, Train loss:0.001452, valid loss:0.000861
Epoch:15, Train loss:0.001424, valid loss:0.000856
Epoch:16, Train loss:0.001370, valid loss:0.000819
Epoch:17, Train loss:0.001375, valid loss:0.000862
Epoch:18, Train loss:0.001355, valid loss:0.000727
Epoch:19, Train loss:0.001314, valid loss:0.000832
Epoch:20, Train loss:0.001297, valid loss:0.000777
Epoch:21, Train loss:0.001079, valid loss:0.000769
Epoch:22, Train loss:0.001071, valid loss:0.000667
Epoch:23, Train loss:0.001051, valid loss:0.000648
Epoch:24, Train loss:0.001052, valid loss:0.000839
Epoch:25, Train loss:0.001040, valid loss:0.000626
Epoch:26, Train loss:0.001047, valid loss:0.000686
Epoch:27, Train loss:0.001023, valid loss:0.000654
Epoch:28, Train loss:0.001015, valid loss:0.000651
Epoch:29, Train loss:0.001025, valid loss:0.000670
Epoch:30, Train loss:0.001014, valid loss:0.000692
Epoch:31, Train loss:0.000896, valid loss:0.000626
Epoch:32, Train loss:0.000891, valid loss:0.000601
Epoch:33, Train loss:0.000887, valid loss:0.000595
Epoch:34, Train loss:0.000881, valid loss:0.000623
Epoch:35, Train loss:0.000883, valid loss:0.000598
Epoch:36, Train loss:0.000878, valid loss:0.000613
Epoch:37, Train loss:0.000880, valid loss:0.000620
Epoch:38, Train loss:0.000872, valid loss:0.000613
Epoch:39, Train loss:0.000865, valid loss:0.000610
Epoch:40, Train loss:0.000873, valid loss:0.000600
Epoch:41, Train loss:0.000815, valid loss:0.000568
Epoch:42, Train loss:0.000807, valid loss:0.000562
Epoch:43, Train loss:0.000810, valid loss:0.000580
Epoch:44, Train loss:0.000805, valid loss:0.000591
Epoch:45, Train loss:0.000803, valid loss:0.000576
Epoch:46, Train loss:0.000804, valid loss:0.000573
Epoch:47, Train loss:0.000798, valid loss:0.000588
Epoch:48, Train loss:0.000799, valid loss:0.000562
Epoch:49, Train loss:0.000796, valid loss:0.000565
Epoch:50, Train loss:0.000793, valid loss:0.000578
Epoch:51, Train loss:0.000769, valid loss:0.000558
Epoch:52, Train loss:0.000768, valid loss:0.000567
Epoch:53, Train loss:0.000768, valid loss:0.000559
Epoch:54, Train loss:0.000766, valid loss:0.000560
Epoch:55, Train loss:0.000765, valid loss:0.000564
Epoch:56, Train loss:0.000764, valid loss:0.000564
Epoch:57, Train loss:0.000761, valid loss:0.000558
Epoch:58, Train loss:0.000760, valid loss:0.000550
Epoch:59, Train loss:0.000762, valid loss:0.000563
Epoch:60, Train loss:0.000760, valid loss:0.000559
training time 5738.728640317917
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.015639913032540413
plot_id,batch_id 0 1 miss% 0.019447756797167093
plot_id,batch_id 0 2 miss% 0.02486091367381415
plot_id,batch_id 0 3 miss% 0.025597003731214946
plot_id,batch_id 0 4 miss% 0.026232657697339148
plot_id,batch_id 0 5 miss% 0.03863755559262286
plot_id,batch_id 0 6 miss% 0.023984790242527584
plot_id,batch_id 0 7 miss% 0.036231133255907585
plot_id,batch_id 0 8 miss% 0.02813071207609918
plot_id,batch_id 0 9 miss% 0.028695844781508627
plot_id,batch_id 0 10 miss% 0.04062081177195328
plot_id,batch_id 0 11 miss% 0.045199718657089334
plot_id,batch_id 0 12 miss% 0.022038451080693973
plot_id,batch_id 0 13 miss% 0.03899067420147517
plot_id,batch_id 0 14 miss% 0.02651738749809538
plot_id,batch_id 0 15 miss% 0.03975092499833704
plot_id,batch_id 0 16 miss% 0.028534415562788292
plot_id,batch_id 0 17 miss% 0.027015957582180884
plot_id,batch_id 0 18 miss% 0.03657107034065375
plot_id,batch_id 0 19 miss% 0.033465760704682636
plot_id,batch_id 0 20 miss% 0.03233983098142995
plot_id,batch_id 0 21 miss% 0.019567072418890424
plot_id,batch_id 0 22 miss% 0.020052069902203984
plot_id,batch_id 0 23 miss% 0.0282055208387169
plot_id,batch_id 0 24 miss% 0.01725758607820967
plot_id,batch_id 0 25 miss% 0.033588350375970966
plot_id,batch_id 0 26 miss% 0.02742309722687165
plot_id,batch_id 0 27 miss% 0.03109623957711357
plot_id,batch_id 0 28 miss% 0.019733103486972235
plot_id,batch_id 0 29 miss% 0.020319218152106952
plot_id,batch_id 0 30 miss% 0.029598735621204346
plot_id,batch_id 0 31 miss% 0.03880364158341018
plot_id,batch_id 0 32 miss% 0.016410601035866647
plot_id,batch_id 0 33 miss% 0.027663621051345726
plot_id,batch_id 0 34 miss% 0.02653174194154378
plot_id,batch_id 0 35 miss% 0.050890183311595553
plot_id,batch_id 0 36 miss% 0.02402534206335235
plot_id,batch_id 0 37 miss% 0.03523864071760644
plot_id,batch_id 0 38 miss% 0.028911363910828585
plot_id,batch_id 0 39 miss% 0.023650022284505967
plot_id,batch_id 0 40 miss% 0.08383436060008938
plot_id,batch_id 0 41 miss% 0.034077687190762304
plot_id,batch_id 0 42 miss% 0.015129373002599462
plot_id,batch_id 0 43 miss% 0.025359102462357612
plot_id,batch_id 0 44 miss% 0.024105652309056393
plot_id,batch_id 0 45 miss% 0.0250894916581868
plot_id,batch_id 0 46 miss% 0.021297962804874553
plot_id,batch_id 0 47 miss% 0.018391491439191436
plot_id,batch_id 0 48 miss% 0.01904998198775591
plot_id,batch_id 0 49 miss% 0.023827908188303683
plot_id,batch_id 0 50 miss% 0.04444029281399178
plot_id,batch_id 0 51 miss% 0.03216290837234145
plot_id,batch_id 0 52 miss% 0.01585697843494915
plot_id,batch_id 0 53 miss% 0.010976462412697383
plot_id,batch_id 0 54 miss% 0.032926724883825184
plot_id,batch_id 0 55 miss% 0.04375760320773964
plot_id,batch_id 0 56 miss% 0.03225702519517894
plot_id,batch_id 0 57 miss% 0.025564582913314777
plot_id,batch_id 0 58 miss% 0.02398768663699353
plot_id,batch_id 0 59 miss% 0.029815880912957927
plot_id,batch_id 0 60 miss% 0.038789603040342374
plot_id,batch_id 0 61 miss% 0.04352793967941629
plot_id,batch_id 0 62 miss% 0.03331597534626027
plot_id,batch_id 0 63 miss% 0.029245418777728262
plot_id,batch_id 0 64 miss% 0.023751598652575517
plot_id,batch_id 0 65 miss% 0.05178844219907407
plot_id,batch_id 0 66 miss% 0.05155686437353462
plot_id,batch_id 0 67 miss% 0.031527414925118576
plot_id,batch_id 0 68 miss% 0.03603612555437683
plot_id,batch_id 0 69 miss% 0.02855788952092358
plot_id,batch_id 0 70 miss% 0.04107975957872171
plot_id,batch_id 0 71 miss% 0.03788285464504433
plot_id,batch_id 0 72 miss% 0.03565153029028282
plot_id,batch_id 0 73 miss% 0.02694417736419783
plot_id,batch_id 0 74 miss% 0.03382471178182206
plot_id,batch_id 0 75 miss% 0.024288190258111533
plot_id,batch_id 0 76 miss% 0.03225275839985126
plot_id,batch_id 0 77 miss% 0.03395355525365976
plot_id,batch_id 0 78 miss% 0.031125881532561392
plot_id,batch_id 0 79 miss% 0.03708640110679672
plot_id,batch_id 0 80 miss% 0.04277769032670232
plot_id,batch_id 0 81 miss% 0.042097643370429295
plot_id,batch_id 0 82 miss% 0.03887544387009557
plot_id,batch_id 0 83 miss% 0.023811034616379827
plot_id,batch_id 0 84 miss% 0.024090828449401924
plot_id,batch_id 0 85 miss% 0.057245472190853124
plot_id,batch_id 0 86 miss% 0.03131638023367048
plot_id,batch_id 0 87 miss% 0.02308408441144477
plot_id,batch_id 0 88 miss% 0.033680278851273254
plot_id,batch_id 0 89 miss% 0.0358346299045533
plot_id,batch_id 0 90 miss% 0.05769003878117709
plot_id,batch_id 0 91 miss% 0.03705987596850136
plot_id,batch_id 0 92 miss% 0.028679090836589406
plot_id,batch_id 0 93 miss% 0.02795600158969974
plot_id,batch_id 0 94 miss% 0.03521203436952725
plot_id,batch_id 0 95 miss% 0.044634180471371666
plot_id,batch_id 0 96 miss% 0.028864904903088198
plot_id,batch_id 0 97 miss% 0.04737880518517273
plot_id,batch_id 0 98 miss% 0.017870613459871263
plot_id,batch_id 0 99 miss% 0.030874745547897677
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01563991 0.01944776 0.02486091 0.025597   0.02623266 0.03863756
 0.02398479 0.03623113 0.02813071 0.02869584 0.04062081 0.04519972
 0.02203845 0.03899067 0.02651739 0.03975092 0.02853442 0.02701596
 0.03657107 0.03346576 0.03233983 0.01956707 0.02005207 0.02820552
 0.01725759 0.03358835 0.0274231  0.03109624 0.0197331  0.02031922
 0.02959874 0.03880364 0.0164106  0.02766362 0.02653174 0.05089018
 0.02402534 0.03523864 0.02891136 0.02365002 0.08383436 0.03407769
 0.01512937 0.0253591  0.02410565 0.02508949 0.02129796 0.01839149
 0.01904998 0.02382791 0.04444029 0.03216291 0.01585698 0.01097646
 0.03292672 0.0437576  0.03225703 0.02556458 0.02398769 0.02981588
 0.0387896  0.04352794 0.03331598 0.02924542 0.0237516  0.05178844
 0.05155686 0.03152741 0.03603613 0.02855789 0.04107976 0.03788285
 0.03565153 0.02694418 0.03382471 0.02428819 0.03225276 0.03395356
 0.03112588 0.0370864  0.04277769 0.04209764 0.03887544 0.02381103
 0.02409083 0.05724547 0.03131638 0.02308408 0.03368028 0.03583463
 0.05769004 0.03705988 0.02867909 0.027956   0.03521203 0.04463418
 0.0288649  0.04737881 0.01787061 0.03087475]
for model  165 the mean error 0.03150599462883705
all id 165 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 31 out win 3 err 0.03150599462883705
Launcher: Job 166 completed in 5927 seconds.
Launcher: Task 104 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  35921
Epoch:0, Train loss:0.672969, valid loss:0.663568
Epoch:1, Train loss:0.034279, valid loss:0.005637
Epoch:2, Train loss:0.008817, valid loss:0.004283
Epoch:3, Train loss:0.006477, valid loss:0.003092
Epoch:4, Train loss:0.005506, valid loss:0.003140
Epoch:5, Train loss:0.004932, valid loss:0.002646
Epoch:6, Train loss:0.004532, valid loss:0.002090
Epoch:7, Train loss:0.004305, valid loss:0.002263
Epoch:8, Train loss:0.004280, valid loss:0.001953
Epoch:9, Train loss:0.003997, valid loss:0.002580
Epoch:10, Train loss:0.004108, valid loss:0.002660
Epoch:11, Train loss:0.002660, valid loss:0.001578
Epoch:12, Train loss:0.002744, valid loss:0.001753
Epoch:13, Train loss:0.002662, valid loss:0.001622
Epoch:14, Train loss:0.002616, valid loss:0.001458
Epoch:15, Train loss:0.002492, valid loss:0.001480
Epoch:16, Train loss:0.002541, valid loss:0.001392
Epoch:17, Train loss:0.002445, valid loss:0.001460
Epoch:18, Train loss:0.002506, valid loss:0.001294
Epoch:19, Train loss:0.002441, valid loss:0.001370
Epoch:20, Train loss:0.002400, valid loss:0.001437
Epoch:21, Train loss:0.001782, valid loss:0.001045
Epoch:22, Train loss:0.001738, valid loss:0.000954
Epoch:23, Train loss:0.001781, valid loss:0.001038
Epoch:24, Train loss:0.001761, valid loss:0.001146
Epoch:25, Train loss:0.001785, valid loss:0.000989
Epoch:26, Train loss:0.001741, valid loss:0.000922
Epoch:27, Train loss:0.001717, valid loss:0.001060
Epoch:28, Train loss:0.001751, valid loss:0.001012
Epoch:29, Train loss:0.001681, valid loss:0.001311
Epoch:30, Train loss:0.001656, valid loss:0.001095
Epoch:31, Train loss:0.001392, valid loss:0.000823
Epoch:32, Train loss:0.001350, valid loss:0.000803
Epoch:33, Train loss:0.001390, valid loss:0.000857
Epoch:34, Train loss:0.001363, valid loss:0.000918
Epoch:35, Train loss:0.001345, valid loss:0.000822
Epoch:36, Train loss:0.001356, valid loss:0.000818
Epoch:37, Train loss:0.001334, valid loss:0.000928
Epoch:38, Train loss:0.001357, valid loss:0.000819
Epoch:39, Train loss:0.001336, valid loss:0.000835
Epoch:40, Train loss:0.001332, valid loss:0.001028
Epoch:41, Train loss:0.001183, valid loss:0.000745
Epoch:42, Train loss:0.001166, valid loss:0.000763
Epoch:43, Train loss:0.001175, valid loss:0.000805
Epoch:44, Train loss:0.001180, valid loss:0.000726
Epoch:45, Train loss:0.001167, valid loss:0.000752
Epoch:46, Train loss:0.001172, valid loss:0.000735
Epoch:47, Train loss:0.001172, valid loss:0.000899
Epoch:48, Train loss:0.001176, valid loss:0.000740
Epoch:49, Train loss:0.001145, valid loss:0.000816
Epoch:50, Train loss:0.001142, valid loss:0.000780
Epoch:51, Train loss:0.001082, valid loss:0.000707
Epoch:52, Train loss:0.001073, valid loss:0.000723
Epoch:53, Train loss:0.001078, valid loss:0.000743
Epoch:54, Train loss:0.001072, valid loss:0.000670
Epoch:55, Train loss:0.001070, valid loss:0.000772
Epoch:56, Train loss:0.001064, valid loss:0.000747
Epoch:57, Train loss:0.001064, valid loss:0.000716
Epoch:58, Train loss:0.001066, valid loss:0.000696
Epoch:59, Train loss:0.001064, valid loss:0.000743
Epoch:60, Train loss:0.001063, valid loss:0.000719
training time 5776.4714279174805
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.03700274947175553
plot_id,batch_id 0 1 miss% 0.03441758529578807
plot_id,batch_id 0 2 miss% 0.03396768287127296
plot_id,batch_id 0 3 miss% 0.0344678581625445
plot_id,batch_id 0 4 miss% 0.027802580974340022
plot_id,batch_id 0 5 miss% 0.03851183594472311
plot_id,batch_id 0 6 miss% 0.02780807838393585
plot_id,batch_id 0 7 miss% 0.03973619771719728
plot_id,batch_id 0 8 miss% 0.03787206708945151
plot_id,batch_id 0 9 miss% 0.019980246698025464
plot_id,batch_id 0 10 miss% 0.04883353122475298
plot_id,batch_id 0 11 miss% 0.05520578721439935
plot_id,batch_id 0 12 miss% 0.03555278499202931
plot_id,batch_id 0 13 miss% 0.027266520758516998
plot_id,batch_id 0 14 miss% 0.03733897479938446
plot_id,batch_id 0 15 miss% 0.03167001152636019
plot_id,batch_id 0 16 miss% 0.03157529449411586
plot_id,batch_id 0 17 miss% 0.041962307184481455
plot_id,batch_id 0 18 miss% 0.0363647994995151
plot_id,batch_id 0 19 miss% 0.028683129865059845
plot_id,batch_id 0 20 miss% 0.04226539505305789
plot_id,batch_id 0 21 miss% 0.030161342335809534
plot_id,batch_id 0 22 miss% 0.02929594454165981
plot_id,batch_id 0 23 miss% 0.02664062705620645
plot_id,batch_id 0 24 miss% 0.02642524748324792
plot_id,batch_id 0 25 miss% 0.04822222292020026
plot_id,batch_id 0 26 miss% 0.028749498584565814
plot_id,batch_id 0 27 miss% 0.018335824154590792
plot_id,batch_id 0 28 miss% 0.03283464902227903
plot_id,batch_id 0 29 miss% 0.027906282539208595
plot_id,batch_id 0 30 miss% 0.04461821340361307
plot_id,batch_id 0 31 miss% 0.021685912765459735
plot_id,batch_id 0 32 miss% 0.03307494292323302
plot_id,batch_id 0 33 miss% 0.027608153607311562
plot_id,batch_id 0 34 miss% 0.026522728033277387
plot_id,batch_id 0 35 miss% 0.033697337797258295
plot_id,batch_id 0 36 miss% 0.039463637165904696
plot_id,batch_id 0 37 miss% 0.021044374135164193
plot_id,batch_id 0 38 miss% 0.03676382907121688
plot_id,batch_id 0 39 miss% 0.023121625435595008
plot_id,batch_id 0 40 miss% 0.04568712187225145
plot_id,batch_id 0 41 miss% 0.032537890402279786
plot_id,batch_id 0 42 miss% 0.020017905016687425
plot_id,batch_id 0 43 miss% 0.03874756124453843
plot_id,batch_id 0 44 miss% 0.030644111843411582
plot_id,batch_id 0 45 miss% 0.0348326943014327
plot_id,batch_id 0 46 miss% 0.0415886840627065
plot_id,batch_id 0 47 miss% 0.021779261471911112
plot_id,batch_id 0 48 miss% 0.03311927783843583
plot_id,batch_id 0 49 miss% 0.0317722383606471
plot_id,batch_id 0 50 miss% 0.03781232259858208
plot_id,batch_id 0 51 miss% 0.021581449457528927
plot_id,batch_id 0 52 miss% 0.022023014368787678
plot_id,batch_id 0 53 miss% 0.020137405051145512
plot_id,batch_id 0 54 miss% 0.03266300679206073
plot_id,batch_id 0 55 miss% 0.021826125308499644
plot_id,batch_id 0 56 miss% 0.02484905959010019
plot_id,batch_id 0 57 miss% 0.03566995543065886
plot_id,batch_id 0 58 miss% 0.034044276597908124
plot_id,batch_id 0 59 miss% 0.023921258809540036
plot_id,batch_id 0 60 miss% 0.04982038861844601
plot_id,batch_id 0 61 miss% 0.033413565505817845
plot_id,batch_id 0 62 miss% 0.02496295848010815
plot_id,batch_id 0 63 miss% 0.03393209453334721
plot_id,batch_id 0 64 miss% 0.03252311347790717
plot_id,batch_id 0 65 miss% 0.04243278994443913
plot_id,batch_id 0 66 miss% 0.059832641938095586
plot_id,batch_id 0 67 miss% 0.031804758811975885
plot_id,batch_id 0 68 miss% 0.033753231309358415
plot_id,batch_id 0 69 miss% 0.02320082143478861
plot_id,batch_id 0 70 miss% 0.0353008426956444
plot_id,batch_id 0 71 miss% 0.06724061476788368
plot_id,batch_id 0 72 miss% 0.04555370408940767
plot_id,batch_id 0 73 miss% 0.03121296273744162
plot_id,batch_id 0 74 miss% 0.03229904022199756
plot_id,batch_id 0 75 miss% 0.06169912743952061
plot_id,batch_id 0 76 miss% 0.05335907107111996
plot_id,batch_id 0 77 miss% 0.03843148445566856
plot_id,batch_id 0 78 miss% 0.02954113666662863
plot_id,batch_id 0 79 miss% 0.04127330934931387
plot_id,batch_id 0 80 miss% 0.03740421245475502
plot_id,batch_id 0 81 miss% 0.017631652988791283
plot_id,batch_id 0 82 miss% 0.03458341661760631
plot_id,batch_id 0 83 miss% 0.034741778061032395
plot_id,batch_id 0 84 miss% 0.01901986233672996
plot_id,batch_id 0 85 miss% 0.05031548469309814
plot_id,batch_id 0 86 miss% 0.023851102038982077
plot_id,batch_id 0 87 miss% 0.024545715982570368
plot_id,batch_id 0 88 miss% 0.02606183876372403
plot_id,batch_id 0 89 miss% 0.03973770627092417
plot_id,batch_id 0 90 miss% 0.030012534988702995
plot_id,batch_id 0 91 miss% 0.04686160118445854
plot_id,batch_id 0 92 miss% 0.024382864539311593
plot_id,batch_id 0 93 miss% 0.030432170106685072
plot_id,batch_id 0 94 miss% 0.03734268295259182
plot_id,batch_id 0 95 miss% 0.036894646614556326
plot_id,batch_id 0 96 miss% 0.03650725654666359
plot_id,batch_id 0 97 miss% 0.03528902618410329
plot_id,batch_id 0 98 miss% 0.03378848341653213
plot_id,batch_id 0 99 miss% 0.037386484372412106
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03700275 0.03441759 0.03396768 0.03446786 0.02780258 0.03851184
 0.02780808 0.0397362  0.03787207 0.01998025 0.04883353 0.05520579
 0.03555278 0.02726652 0.03733897 0.03167001 0.03157529 0.04196231
 0.0363648  0.02868313 0.0422654  0.03016134 0.02929594 0.02664063
 0.02642525 0.04822222 0.0287495  0.01833582 0.03283465 0.02790628
 0.04461821 0.02168591 0.03307494 0.02760815 0.02652273 0.03369734
 0.03946364 0.02104437 0.03676383 0.02312163 0.04568712 0.03253789
 0.02001791 0.03874756 0.03064411 0.03483269 0.04158868 0.02177926
 0.03311928 0.03177224 0.03781232 0.02158145 0.02202301 0.02013741
 0.03266301 0.02182613 0.02484906 0.03566996 0.03404428 0.02392126
 0.04982039 0.03341357 0.02496296 0.03393209 0.03252311 0.04243279
 0.05983264 0.03180476 0.03375323 0.02320082 0.03530084 0.06724061
 0.0455537  0.03121296 0.03229904 0.06169913 0.05335907 0.03843148
 0.02954114 0.04127331 0.03740421 0.01763165 0.03458342 0.03474178
 0.01901986 0.05031548 0.0238511  0.02454572 0.02606184 0.03973771
 0.03001253 0.0468616  0.02438286 0.03043217 0.03734268 0.03689465
 0.03650726 0.03528903 0.03378848 0.03738648]
for model  153 the mean error 0.033880906112787657
all id 153 hidden_dim 16 learning_rate 0.02 num_layers 5 frames 25 out win 3 err 0.033880906112787657
Launcher: Job 154 completed in 5969 seconds.
Launcher: Task 60 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  46193
Epoch:0, Train loss:0.434749, valid loss:0.392697
Epoch:1, Train loss:0.225389, valid loss:0.229815
Epoch:2, Train loss:0.218758, valid loss:0.228738
Epoch:3, Train loss:0.217746, valid loss:0.228841
Epoch:4, Train loss:0.217183, valid loss:0.228487
Epoch:5, Train loss:0.216988, valid loss:0.228537
Epoch:6, Train loss:0.216814, valid loss:0.228323
Epoch:7, Train loss:0.216629, valid loss:0.228177
Epoch:8, Train loss:0.216550, valid loss:0.227987
Epoch:9, Train loss:0.216513, valid loss:0.228210
Epoch:10, Train loss:0.216389, valid loss:0.228241
Epoch:11, Train loss:0.215659, valid loss:0.227620
Epoch:12, Train loss:0.215637, valid loss:0.227706
Epoch:13, Train loss:0.215679, valid loss:0.227817
Epoch:14, Train loss:0.215608, valid loss:0.227844
Epoch:15, Train loss:0.215620, valid loss:0.227791
Epoch:16, Train loss:0.215550, valid loss:0.227667
Epoch:17, Train loss:0.215569, valid loss:0.227813
Epoch:18, Train loss:0.215518, valid loss:0.227590
Epoch:19, Train loss:0.215530, valid loss:0.227797
Epoch:20, Train loss:0.215535, valid loss:0.227641
Epoch:21, Train loss:0.215169, valid loss:0.227500
Epoch:22, Train loss:0.215182, valid loss:0.227466
Epoch:23, Train loss:0.215167, valid loss:0.227474
Epoch:24, Train loss:0.215145, valid loss:0.227428
Epoch:25, Train loss:0.215161, valid loss:0.227569
Epoch:26, Train loss:0.215148, valid loss:0.227418
Epoch:27, Train loss:0.215129, valid loss:0.227470
Epoch:28, Train loss:0.215133, valid loss:0.227634
Epoch:29, Train loss:0.215131, valid loss:0.227418
Epoch:30, Train loss:0.215149, valid loss:0.227536
Epoch:31, Train loss:0.214951, valid loss:0.227390
Epoch:32, Train loss:0.214935, valid loss:0.227375
Epoch:33, Train loss:0.214943, valid loss:0.227372
Epoch:34, Train loss:0.214939, valid loss:0.227378
Epoch:35, Train loss:0.214918, valid loss:0.227364
Epoch:36, Train loss:0.214936, valid loss:0.227432
Epoch:37, Train loss:0.214948, valid loss:0.227381
Epoch:38, Train loss:0.214907, valid loss:0.227394
Epoch:39, Train loss:0.214929, valid loss:0.227382
Epoch:40, Train loss:0.214930, valid loss:0.227516
Epoch:41, Train loss:0.214838, valid loss:0.227314
Epoch:42, Train loss:0.214836, valid loss:0.227324
Epoch:43, Train loss:0.214835, valid loss:0.227347
Epoch:44, Train loss:0.214830, valid loss:0.227329
Epoch:45, Train loss:0.214833, valid loss:0.227343
Epoch:46, Train loss:0.214829, valid loss:0.227350
Epoch:47, Train loss:0.214825, valid loss:0.227314
Epoch:48, Train loss:0.214830, valid loss:0.227355
Epoch:49, Train loss:0.214820, valid loss:0.227310
Epoch:50, Train loss:0.214822, valid loss:0.227364
Epoch:51, Train loss:0.214784, valid loss:0.227318
Epoch:52, Train loss:0.214782, valid loss:0.227326
Epoch:53, Train loss:0.214778, valid loss:0.227302
Epoch:54, Train loss:0.214780, valid loss:0.227300
Epoch:55, Train loss:0.214779, valid loss:0.227299
Epoch:56, Train loss:0.214778, valid loss:0.227327
Epoch:57, Train loss:0.214777, valid loss:0.227301
Epoch:58, Train loss:0.214777, valid loss:0.227303
Epoch:59, Train loss:0.214777, valid loss:0.227319
Epoch:60, Train loss:0.214773, valid loss:0.227306
training time 5849.458248376846
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.547604567087042
plot_id,batch_id 0 1 miss% 0.6773007797730474
plot_id,batch_id 0 2 miss% 0.6960513968409935
plot_id,batch_id 0 3 miss% 0.7077229400629842
plot_id,batch_id 0 4 miss% 0.7089023108059428
plot_id,batch_id 0 5 miss% 0.5564484937410226
plot_id,batch_id 0 6 miss% 0.6732013059599055
plot_id,batch_id 0 7 miss% 0.6972134395683268
plot_id,batch_id 0 8 miss% 0.706395204875674
plot_id,batch_id 0 9 miss% 0.7106015986313144
plot_id,batch_id 0 10 miss% 0.5259224270918583
plot_id,batch_id 0 11 miss% 0.66516140347329
plot_id,batch_id 0 12 miss% 0.6908341636929958
plot_id,batch_id 0 13 miss% 0.703716485564508
plot_id,batch_id 0 14 miss% 0.7095013254878436
plot_id,batch_id 0 15 miss% 0.529789662888023
plot_id,batch_id 0 16 miss% 0.6720901387373035
plot_id,batch_id 0 17 miss% 0.6963246766510052
plot_id,batch_id 0 18 miss% 0.7034294966771424
plot_id,batch_id 0 19 miss% 0.7062473707822199
plot_id,batch_id 0 20 miss% 0.6137757906065228
plot_id,batch_id 0 21 miss% 0.6986568598617067
plot_id,batch_id 0 22 miss% 0.7104022597129226
plot_id,batch_id 0 23 miss% 0.718127346888645
plot_id,batch_id 0 24 miss% 0.7194534726222996
plot_id,batch_id 0 25 miss% 0.608548765637651
plot_id,batch_id 0 26 miss% 0.6917772284021404
plot_id,batch_id 0 27 miss% 0.7079862315651508
plot_id,batch_id 0 28 miss% 0.7147718257838067
plot_id,batch_id 0 29 miss% 0.7200200796376051
plot_id,batch_id 0 30 miss% 0.6118051427961836
plot_id,batch_id 0 31 miss% 0.6879491727073537
plot_id,batch_id 0 32 miss% 0.7038116076374804
plot_id,batch_id 0 33 miss% 0.71132853894975
plot_id,batch_id 0 34 miss% 0.714341967111845
plot_id,batch_id 0 35 miss% 0.5878889474775381
plot_id,batch_id 0 36 miss% 0.6917627011241205
plot_id,batch_id 0 37 miss% 0.7013013484440161
plot_id,batch_id 0 38 miss% 0.7220782226675204
plot_id,batch_id 0 39 miss% 0.7141375064488572
plot_id,batch_id 0 40 miss% 0.6525174736828216
plot_id,batch_id 0 41 miss% 0.7100311821352975
plot_id,batch_id 0 42 miss% 0.7136543998469931
plot_id,batch_id 0 43 miss% 0.7242136919359475
plot_id,batch_id 0 44 miss% 0.7280096406322957
plot_id,batch_id 0 45 miss% 0.6544172794026676
plot_id,batch_id 0 46 miss% 0.7126098319278741
plot_id,batch_id 0 47 miss% 0.7147059714498034
plot_id,batch_id 0 48 miss% 0.7202715734469217
plot_id,batch_id 0 49 miss% 0.7267573560754943
plot_id,batch_id 0 50 miss% 0.66805210323176
plot_id,batch_id 0 51 miss% 0.7052058355366998
plot_id,batch_id 0 52 miss% 0.7126598415800789
plot_id,batch_id 0 53 miss% 0.7203956284634434
plot_id,batch_id 0 54 miss% 0.7283087022889217
plot_id,batch_id 0 55 miss% 0.6652959238808317
plot_id,batch_id 0 56 miss% 0.7048760303629189
plot_id,batch_id 0 57 miss% 0.71345788586416
plot_id,batch_id 0 58 miss% 0.720633921914725
plot_id,batch_id 0 59 miss% 0.7180600292711372
plot_id,batch_id 0 60 miss% 0.4448098464019969
plot_id,batch_id 0 61 miss% 0.6165690189763686
plot_id,batch_id 0 62 miss% 0.6637069972380586
plot_id,batch_id 0 63 miss% 0.6825471382216578
plot_id,batch_id 0 64 miss% 0.6877751088878741
plot_id,batch_id 0 65 miss% 0.43946760472376456
plot_id,batch_id 0 66 miss% 0.6039402279738774
plot_id,batch_id 0 67 miss% 0.6438887535556185
plot_id,batch_id 0 68 miss% 0.6863644668237069
plot_id,batch_id 0 69 miss% 0.6873432680293343
plot_id,batch_id 0 70 miss% 0.408661539282168
plot_id,batch_id 0 71 miss% 0.5963232384552598
plot_id,batch_id 0 72 miss% 0.6351402708296436
plot_id,batch_id 0 73 miss% 0.6656033353529864
plot_id,batch_id 0 74 miss% 0.6876857344867303
plot_id,batch_id 0 75 miss% 0.39460819252945456
plot_id,batch_id 0 76 miss% 0.5785952359082522
plot_id,batch_id 0 77 miss% 0.6241173849989623
plot_id,batch_id 0 78 miss% 0.6693008020581781
plot_id,batch_id 0 79 miss% 0.6807904587398799
plot_id,batch_id 0 80 miss% 0.46985020409792344
plot_id,batch_id 0 81 miss% 0.6406043988222191
plot_id,batch_id 0 82 miss% 0.6773143290689569
plot_id,batch_id 0 83 miss% 0.694648543947135
plot_id,batch_id 0 84 miss% 0.7001342931388913
plot_id,batch_id 0 85 miss% 0.46785417022856135
plot_id,batch_id 0 86 miss% 0.6368139308772015
plot_id,batch_id 0 87 miss% 0.6685895130417394
plot_id,batch_id 0 88 miss% 0.6926168383803543
plot_id,batch_id 0 89 miss% 0.6996007876519391
plot_id,batch_id 0 90 miss% 0.44259606110313127
plot_id,batch_id 0 91 miss% 0.6283092584403431
plot_id,batch_id 0 92 miss% 0.6603111064161364
plot_id,batch_id 0 93 miss% 0.687320057375619
plot_id,batch_id 0 94 miss% 0.6946917045496179
plot_id,batch_id 0 95 miss% 0.43435831867535857
plot_id,batch_id 0 96 miss% 0.6117210261603816
plot_id,batch_id 0 97 miss% 0.655980821014753
plot_id,batch_id 0 98 miss% 0.6734069414241416
plot_id,batch_id 0 99 miss% 0.6853198347671667
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.54760457 0.67730078 0.6960514  0.70772294 0.70890231 0.55644849
 0.67320131 0.69721344 0.7063952  0.7106016  0.52592243 0.6651614
 0.69083416 0.70371649 0.70950133 0.52978966 0.67209014 0.69632468
 0.7034295  0.70624737 0.61377579 0.69865686 0.71040226 0.71812735
 0.71945347 0.60854877 0.69177723 0.70798623 0.71477183 0.72002008
 0.61180514 0.68794917 0.70381161 0.71132854 0.71434197 0.58788895
 0.6917627  0.70130135 0.72207822 0.71413751 0.65251747 0.71003118
 0.7136544  0.72421369 0.72800964 0.65441728 0.71260983 0.71470597
 0.72027157 0.72675736 0.6680521  0.70520584 0.71265984 0.72039563
 0.7283087  0.66529592 0.70487603 0.71345789 0.72063392 0.71806003
 0.44480985 0.61656902 0.663707   0.68254714 0.68777511 0.4394676
 0.60394023 0.64388875 0.68636447 0.68734327 0.40866154 0.59632324
 0.63514027 0.66560334 0.68768573 0.39460819 0.57859524 0.62411738
 0.6693008  0.68079046 0.4698502  0.6406044  0.67731433 0.69464854
 0.70013429 0.46785417 0.63681393 0.66858951 0.69261684 0.69960079
 0.44259606 0.62830926 0.66031111 0.68732006 0.6946917  0.43435832
 0.61172103 0.65598082 0.67340694 0.68531983]
for model  219 the mean error 0.657677992700617
all id 219 hidden_dim 24 learning_rate 0.02 num_layers 3 frames 31 out win 3 err 0.657677992700617
Launcher: Job 220 completed in 6025 seconds.
Launcher: Task 87 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  61841
Epoch:0, Train loss:0.598040, valid loss:0.600524
Epoch:1, Train loss:0.321207, valid loss:0.019840
Epoch:2, Train loss:0.014853, valid loss:0.005036
Epoch:3, Train loss:0.008414, valid loss:0.004150
Epoch:4, Train loss:0.007004, valid loss:0.003640
Epoch:5, Train loss:0.006309, valid loss:0.002850
Epoch:6, Train loss:0.005530, valid loss:0.003261
Epoch:7, Train loss:0.004961, valid loss:0.003094
Epoch:8, Train loss:0.004360, valid loss:0.002385
Epoch:9, Train loss:0.004109, valid loss:0.002095
Epoch:10, Train loss:0.004019, valid loss:0.001912
Epoch:11, Train loss:0.002775, valid loss:0.001524
Epoch:12, Train loss:0.002688, valid loss:0.001872
Epoch:13, Train loss:0.002643, valid loss:0.001441
Epoch:14, Train loss:0.002672, valid loss:0.002014
Epoch:15, Train loss:0.002590, valid loss:0.001426
Epoch:16, Train loss:0.002479, valid loss:0.001390
Epoch:17, Train loss:0.002435, valid loss:0.001609
Epoch:18, Train loss:0.002397, valid loss:0.001530
Epoch:19, Train loss:0.002424, valid loss:0.001652
Epoch:20, Train loss:0.002342, valid loss:0.002653
Epoch:21, Train loss:0.001739, valid loss:0.000969
Epoch:22, Train loss:0.001758, valid loss:0.000976
Epoch:23, Train loss:0.001724, valid loss:0.001028
Epoch:24, Train loss:0.001664, valid loss:0.001066
Epoch:25, Train loss:0.001691, valid loss:0.001064
Epoch:26, Train loss:0.001670, valid loss:0.001030
Epoch:27, Train loss:0.001618, valid loss:0.001042
Epoch:28, Train loss:0.001615, valid loss:0.001067
Epoch:29, Train loss:0.001630, valid loss:0.000958
Epoch:30, Train loss:0.001559, valid loss:0.001065
Epoch:31, Train loss:0.001278, valid loss:0.000829
Epoch:32, Train loss:0.001279, valid loss:0.000955
Epoch:33, Train loss:0.001278, valid loss:0.000822
Epoch:34, Train loss:0.001264, valid loss:0.000863
Epoch:35, Train loss:0.001262, valid loss:0.000908
Epoch:36, Train loss:0.001227, valid loss:0.000808
Epoch:37, Train loss:0.001228, valid loss:0.000848
Epoch:38, Train loss:0.001250, valid loss:0.000887
Epoch:39, Train loss:0.001232, valid loss:0.000898
Epoch:40, Train loss:0.001208, valid loss:0.000835
Epoch:41, Train loss:0.001077, valid loss:0.000778
Epoch:42, Train loss:0.001061, valid loss:0.000896
Epoch:43, Train loss:0.001064, valid loss:0.000766
Epoch:44, Train loss:0.001067, valid loss:0.000783
Epoch:45, Train loss:0.001051, valid loss:0.000764
Epoch:46, Train loss:0.001051, valid loss:0.000757
Epoch:47, Train loss:0.001052, valid loss:0.000796
Epoch:48, Train loss:0.001035, valid loss:0.000746
Epoch:49, Train loss:0.001045, valid loss:0.000768
Epoch:50, Train loss:0.001043, valid loss:0.000766
Epoch:51, Train loss:0.000973, valid loss:0.000702
Epoch:52, Train loss:0.000965, valid loss:0.000708
Epoch:53, Train loss:0.000972, valid loss:0.000785
Epoch:54, Train loss:0.000963, valid loss:0.000745
Epoch:55, Train loss:0.000962, valid loss:0.000745
Epoch:56, Train loss:0.000961, valid loss:0.000710
Epoch:57, Train loss:0.000957, valid loss:0.000765
Epoch:58, Train loss:0.000954, valid loss:0.000723
Epoch:59, Train loss:0.000951, valid loss:0.000772
Epoch:60, Train loss:0.000949, valid loss:0.000741
training time 5897.175624370575
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.021277597442186134
plot_id,batch_id 0 1 miss% 0.02441794903435964
plot_id,batch_id 0 2 miss% 0.030431169535980167
plot_id,batch_id 0 3 miss% 0.024603693780857947
plot_id,batch_id 0 4 miss% 0.023675892572378555
plot_id,batch_id 0 5 miss% 0.04056677102742163
plot_id,batch_id 0 6 miss% 0.0248619815154915
plot_id,batch_id 0 7 miss% 0.024166099196321674
plot_id,batch_id 0 8 miss% 0.030615156633314124
plot_id,batch_id 0 9 miss% 0.0241273758810505
plot_id,batch_id 0 10 miss% 0.040740020802488223
plot_id,batch_id 0 11 miss% 0.04592140209414977
plot_id,batch_id 0 12 miss% 0.03494168581345671
plot_id,batch_id 0 13 miss% 0.019846121687620257
plot_id,batch_id 0 14 miss% 0.030639151559517633
plot_id,batch_id 0 15 miss% 0.0314638362118352
plot_id,batch_id 0 16 miss% 0.031903315636709714
plot_id,batch_id 0 17 miss% 0.039277767535794164
plot_id,batch_id 0 18 miss% 0.032586241822937415
plot_id,batch_id 0 19 miss% 0.028817676035305804
plot_id,batch_id 0 20 miss% 0.0370042461613809
plot_id,batch_id 0 21 miss% 0.025435203725541866
plot_id,batch_id 0 22 miss% 0.031132086763210307
plot_id,batch_id 0 23 miss% 0.031313291259611485
plot_id,batch_id 0 24 miss% 0.02181226828080018
plot_id,batch_id 0 25 miss% 0.042578295915180635
plot_id,batch_id 0 26 miss% 0.0377421306247965
plot_id,batch_id 0 27 miss% 0.02810306013190986
plot_id,batch_id 0 28 miss% 0.023458286516686286
plot_id,batch_id 0 29 miss% 0.029706124749094245
plot_id,batch_id 0 30 miss% 0.040798263818365876
plot_id,batch_id 0 31 miss% 0.0298207652361866
plot_id,batch_id 0 32 miss% 0.02471494166621599
plot_id,batch_id 0 33 miss% 0.026890040475722427
plot_id,batch_id 0 34 miss% 0.02918270159492025
plot_id,batch_id 0 35 miss% 0.03947072210644674
plot_id,batch_id 0 36 miss% 0.04288786232383463
plot_id,batch_id 0 37 miss% 0.02881201073989038
plot_id,batch_id 0 38 miss% 0.0378809466940127
plot_id,batch_id 0 39 miss% 0.022077931143665373
plot_id,batch_id 0 40 miss% 0.07630562038133727
plot_id,batch_id 0 41 miss% 0.02281453394464444
plot_id,batch_id 0 42 miss% 0.031865000554878745
plot_id,batch_id 0 43 miss% 0.03388496309414141
plot_id,batch_id 0 44 miss% 0.02717761999954462
plot_id,batch_id 0 45 miss% 0.05134925598259564
plot_id,batch_id 0 46 miss% 0.030797837141497652
plot_id,batch_id 0 47 miss% 0.03152105756697215
plot_id,batch_id 0 48 miss% 0.034947067472267626
plot_id,batch_id 0 49 miss% 0.031135589397011595
plot_id,batch_id 0 50 miss% 0.0330802667848839
plot_id,batch_id 0 51 miss% 0.026220813951545855
plot_id,batch_id 0 52 miss% 0.02685844065720152
plot_id,batch_id 0 53 miss% 0.02990727367320673
plot_id,batch_id 0 54 miss% 0.022123080660014394
plot_id,batch_id 0 55 miss% 0.03637148306240922
plot_id,batch_id 0 56 miss% 0.035791201837577516
plot_id,batch_id 0 57 miss% 0.02468677772090839
plot_id,batch_id 0 58 miss% 0.02508520345100269
plot_id,batch_id 0 59 miss% 0.022837801425072503
plot_id,batch_id 0 60 miss% 0.047456722169754495
plot_id,batch_id 0 61 miss% 0.04171452638789158
plot_id,batch_id 0 62 miss% 0.017397803610208346
plot_id,batch_id 0 63 miss% 0.029666079241214154
plot_id,batch_id 0 64 miss% 0.027475471756696384
plot_id,batch_id 0 65 miss% 0.03618167608975708
plot_id,batch_id 0 66 miss% 0.04688988331110794
plot_id,batch_id 0 67 miss% 0.0332212025904792
plot_id,batch_id 0 68 miss% 0.025727886775227256
plot_id,batch_id 0 69 miss% 0.020340077023297876
plot_id,batch_id 0 70 miss% 0.0344061712031127
plot_id,batch_id 0 71 miss% 0.07701168083372749
plot_id,batch_id 0 72 miss% 0.04886011191992799
plot_id,batch_id 0 73 miss% 0.041717745368174595
plot_id,batch_id 0 74 miss% 0.02929139826521908
plot_id,batch_id 0 75 miss% 0.05256436736428247
plot_id,batch_id 0 76 miss% 0.04469238809221376
plot_id,batch_id 0 77 miss% 0.02630479213018603
plot_id,batch_id 0 78 miss% 0.05874018664171181
plot_id,batch_id 0 79 miss% 0.04701713292966397
plot_id,batch_id 0 80 miss% 0.03298722406033698
plot_id,batch_id 0 81 miss% 0.02415764449338697
plot_id,batch_id 0 82 miss% 0.03781233085889769
plot_id,batch_id 0 83 miss% 0.03205626634230018
plot_id,batch_id 0 84 miss% 0.019355348210342058
plot_id,batch_id 0 85 miss% 0.04693745886056174
plot_id,batch_id 0 86 miss% 0.02118457984727195
plot_id,batch_id 0 87 miss% 0.02248463114941469
plot_id,batch_id 0 88 miss% 0.03434909793463648
plot_id,batch_id 0 89 miss% 0.0270241975702779
plot_id,batch_id 0 90 miss% 0.03950367334142468
plot_id,batch_id 0 91 miss% 0.04100247559687306
plot_id,batch_id 0 92 miss% 0.03212499669811978
plot_id,batch_id 0 93 miss% 0.032839323424183685
plot_id,batch_id 0 94 miss% 0.038601543502697455
plot_id,batch_id 0 95 miss% 0.06277860346942066
plot_id,batch_id 0 96 miss% 0.03733599600040801
plot_id,batch_id 0 97 miss% 0.04409915454631638
plot_id,batch_id 0 98 miss% 0.04463869453307705
plot_id,batch_id 0 99 miss% 0.02791711913010606
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0212776  0.02441795 0.03043117 0.02460369 0.02367589 0.04056677
 0.02486198 0.0241661  0.03061516 0.02412738 0.04074002 0.0459214
 0.03494169 0.01984612 0.03063915 0.03146384 0.03190332 0.03927777
 0.03258624 0.02881768 0.03700425 0.0254352  0.03113209 0.03131329
 0.02181227 0.0425783  0.03774213 0.02810306 0.02345829 0.02970612
 0.04079826 0.02982077 0.02471494 0.02689004 0.0291827  0.03947072
 0.04288786 0.02881201 0.03788095 0.02207793 0.07630562 0.02281453
 0.031865   0.03388496 0.02717762 0.05134926 0.03079784 0.03152106
 0.03494707 0.03113559 0.03308027 0.02622081 0.02685844 0.02990727
 0.02212308 0.03637148 0.0357912  0.02468678 0.0250852  0.0228378
 0.04745672 0.04171453 0.0173978  0.02966608 0.02747547 0.03618168
 0.04688988 0.0332212  0.02572789 0.02034008 0.03440617 0.07701168
 0.04886011 0.04171775 0.0292914  0.05256437 0.04469239 0.02630479
 0.05874019 0.04701713 0.03298722 0.02415764 0.03781233 0.03205627
 0.01935535 0.04693746 0.02118458 0.02248463 0.0343491  0.0270242
 0.03950367 0.04100248 0.032125   0.03283932 0.03860154 0.0627786
 0.037336   0.04409915 0.04463869 0.02791712]
for model  147 the mean error 0.03376330637779272
all id 147 hidden_dim 24 learning_rate 0.02 num_layers 4 frames 25 out win 3 err 0.03376330637779272
Launcher: Job 148 completed in 6091 seconds.
Launcher: Task 34 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  77489
Epoch:0, Train loss:0.665920, valid loss:0.672965
Epoch:1, Train loss:0.064872, valid loss:0.022769
Epoch:2, Train loss:0.027507, valid loss:0.007653
Epoch:3, Train loss:0.012475, valid loss:0.006070
Epoch:4, Train loss:0.009189, valid loss:0.004517
Epoch:5, Train loss:0.007336, valid loss:0.004226
Epoch:6, Train loss:0.006309, valid loss:0.003176
Epoch:7, Train loss:0.005473, valid loss:0.003856
Epoch:8, Train loss:0.004987, valid loss:0.002846
Epoch:9, Train loss:0.004445, valid loss:0.002670
Epoch:10, Train loss:0.004108, valid loss:0.003261
Epoch:11, Train loss:0.002957, valid loss:0.001720
Epoch:12, Train loss:0.002856, valid loss:0.001796
Epoch:13, Train loss:0.002794, valid loss:0.001654
Epoch:14, Train loss:0.002698, valid loss:0.001875
Epoch:15, Train loss:0.002696, valid loss:0.001732
Epoch:16, Train loss:0.002605, valid loss:0.001800
Epoch:17, Train loss:0.002493, valid loss:0.001442
Epoch:18, Train loss:0.002438, valid loss:0.001460
Epoch:19, Train loss:0.002375, valid loss:0.001339
Epoch:20, Train loss:0.002372, valid loss:0.001599
Epoch:21, Train loss:0.001880, valid loss:0.001144
Epoch:22, Train loss:0.001844, valid loss:0.001248
Epoch:23, Train loss:0.001783, valid loss:0.001224
Epoch:24, Train loss:0.001757, valid loss:0.001176
Epoch:25, Train loss:0.001790, valid loss:0.001221
Epoch:26, Train loss:0.001711, valid loss:0.001026
Epoch:27, Train loss:0.001715, valid loss:0.001109
Epoch:28, Train loss:0.001718, valid loss:0.001154
Epoch:29, Train loss:0.001704, valid loss:0.000993
Epoch:30, Train loss:0.001638, valid loss:0.001107
Epoch:31, Train loss:0.001422, valid loss:0.000977
Epoch:32, Train loss:0.001408, valid loss:0.000986
Epoch:33, Train loss:0.001418, valid loss:0.001088
Epoch:34, Train loss:0.001395, valid loss:0.001000
Epoch:35, Train loss:0.001384, valid loss:0.001021
Epoch:36, Train loss:0.001407, valid loss:0.000972
Epoch:37, Train loss:0.001381, valid loss:0.000947
Epoch:38, Train loss:0.001368, valid loss:0.000965
Epoch:39, Train loss:0.001367, valid loss:0.001072
Epoch:40, Train loss:0.001332, valid loss:0.000918
Epoch:41, Train loss:0.001238, valid loss:0.000903
Epoch:42, Train loss:0.001229, valid loss:0.000913
Epoch:43, Train loss:0.001221, valid loss:0.000922
Epoch:44, Train loss:0.001219, valid loss:0.000921
Epoch:45, Train loss:0.001215, valid loss:0.000931
Epoch:46, Train loss:0.001207, valid loss:0.000889
Epoch:47, Train loss:0.001196, valid loss:0.000883
Epoch:48, Train loss:0.001193, valid loss:0.000906
Epoch:49, Train loss:0.001196, valid loss:0.000908
Epoch:50, Train loss:0.001189, valid loss:0.000887
Epoch:51, Train loss:0.001144, valid loss:0.000858
Epoch:52, Train loss:0.001133, valid loss:0.000872
Epoch:53, Train loss:0.001136, valid loss:0.000871
Epoch:54, Train loss:0.001125, valid loss:0.000869
Epoch:55, Train loss:0.001125, valid loss:0.000850
Epoch:56, Train loss:0.001125, valid loss:0.000865
Epoch:57, Train loss:0.001124, valid loss:0.000870
Epoch:58, Train loss:0.001119, valid loss:0.000862
Epoch:59, Train loss:0.001120, valid loss:0.000876
Epoch:60, Train loss:0.001114, valid loss:0.000842
training time 5917.671924352646
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03993761618589831
plot_id,batch_id 0 1 miss% 0.03240350949114883
plot_id,batch_id 0 2 miss% 0.026125099375714063
plot_id,batch_id 0 3 miss% 0.03443022567618496
plot_id,batch_id 0 4 miss% 0.019350967097515187
plot_id,batch_id 0 5 miss% 0.03415531318015255
plot_id,batch_id 0 6 miss% 0.02784759054498591
plot_id,batch_id 0 7 miss% 0.0318000782728268
plot_id,batch_id 0 8 miss% 0.024888798236460785
plot_id,batch_id 0 9 miss% 0.02554559176571846
plot_id,batch_id 0 10 miss% 0.029034098258987135
plot_id,batch_id 0 11 miss% 0.04383884835070817
plot_id,batch_id 0 12 miss% 0.02762891150063068
plot_id,batch_id 0 13 miss% 0.030330006902640653
plot_id,batch_id 0 14 miss% 0.029669457781536547
plot_id,batch_id 0 15 miss% 0.0342925845958929
plot_id,batch_id 0 16 miss% 0.03283525293410179
plot_id,batch_id 0 17 miss% 0.029818056652803824
plot_id,batch_id 0 18 miss% 0.03748299111542858
plot_id,batch_id 0 19 miss% 0.028604507722534606
plot_id,batch_id 0 20 miss% 0.04467820268067738
plot_id,batch_id 0 21 miss% 0.01805732456300529
plot_id,batch_id 0 22 miss% 0.026186229443875466
plot_id,batch_id 0 23 miss% 0.021673423437364444
plot_id,batch_id 0 24 miss% 0.026687195972649784
plot_id,batch_id 0 25 miss% 0.0337824023520587
plot_id,batch_id 0 26 miss% 0.024098314188626156
plot_id,batch_id 0 27 miss% 0.02669867143706094
plot_id,batch_id 0 28 miss% 0.018385037960162148
plot_id,batch_id 0 29 miss% 0.01819199847829585
plot_id,batch_id 0 30 miss% 0.05468316523637068
plot_id,batch_id 0 31 miss% 0.03406864925669601
plot_id,batch_id 0 32 miss% 0.03287242585160635
plot_id,batch_id 0 33 miss% 0.033574431389743344
plot_id,batch_id 0 34 miss% 0.02385231047101537
plot_id,batch_id 0 35 miss% 0.05163745136135174
plot_id,batch_id 0 36 miss% 0.03862527889935901
plot_id,batch_id 0 37 miss% 0.04529097838057065
plot_id,batch_id 0 38 miss% 0.02601895931476234
plot_id,batch_id 0 39 miss% 0.023472785735381287
plot_id,batch_id 0 40 miss% 0.09494105761913842
plot_id,batch_id 0 41 miss% 0.022058800452398275
plot_id,batch_id 0 42 miss% 0.013395484396022542
plot_id,batch_id 0 43 miss% 0.04600960047644843
plot_id,batch_id 0 44 miss% 0.03210865434989319
plot_id,batch_id 0 45 miss% 0.024063140150773297
plot_id,batch_id 0 46 miss% 0.017971027097562268
plot_id,batch_id 0 47 miss% 0.019288502004082022
plot_id,batch_id 0 48 miss% 0.02409329913701554
plot_id,batch_id 0 49 miss% 0.03739731323520462
plot_id,batch_id 0 50 miss% 0.03278702512671053
plot_id,batch_id 0 51 miss% 0.026639940703364885
plot_id,batch_id 0 52 miss% 0.01395654487612061
plot_id,batch_id 0 53 miss% 0.02154447625100039
plot_id,batch_id 0 54 miss% 0.04083328795098949
plot_id,batch_id 0 55 miss% 0.0419908337698716
plot_id,batch_id 0 56 miss% 0.014804867710628788
plot_id,batch_id 0 57 miss% 0.021104662652235318
plot_id,batch_id 0 58 miss% 0.01978330622604024
plot_id,batch_id 0 59 miss% 0.019351907039983043
plot_id,batch_id 0 60 miss% 0.038400681392327524
plot_id,batch_id 0 61 miss% 0.036283801890354885
plot_id,batch_id 0 62 miss% 0.02507748760649983
plot_id,batch_id 0 63 miss% 0.03811275253478797
plot_id,batch_id 0 64 miss% 0.03439584828155018
plot_id,batch_id 0 65 miss% 0.04225724880970436
plot_id,batch_id 0 66 miss% 0.03320403578714348
plot_id,batch_id 0 67 miss% 0.03347139806689311
plot_id,batch_id 0 68 miss% 0.03681265196528091
plot_id,batch_id 0 69 miss% 0.027789753888108656
plot_id,batch_id 0 70 miss% 0.03256952535372499
plot_id,batch_id 0 71 miss% 0.04705154120981843
plot_id,batch_id 0 72 miss% 0.03872739226556393
plot_id,batch_id 0 73 miss% 0.039454394027697806
plot_id,batch_id 0 74 miss% 0.04454668610145303
plot_id,batch_id 0 75 miss% 0.04754733010161797
plot_id,batch_id 0 76 miss% 0.04840925608913797
plot_id,batch_id 0 77 miss% 0.03947589805913001
plot_id,batch_id 0 78 miss% 0.03879079398573182
plot_id,batch_id 0 79 miss% 0.03765224534882349
plot_id,batch_id 0 80 miss% 0.04468830235957184
plot_id,batch_id 0 81 miss% 0.014360621747004707
plot_id,batch_id 0 82 miss% 0.015317191923870753
plot_id,batch_id 0 83 miss% 0.03482441926940221
plot_id,batch_id 0 84 miss% 0.022583881125401287
plot_id,batch_id 0 85 miss% 0.046411968845610684
plot_id,batch_id 0 86 miss% 0.03135693063101901
plot_id,batch_id 0 87 miss% 0.02251215288150869
plot_id,batch_id 0 88 miss% 0.03469424776221812
plot_id,batch_id 0 89 miss% 0.030494207160411294
plot_id,batch_id 0 90 miss% 0.030734300085938953
plot_id,batch_id 0 91 miss% 0.02683150706237435
plot_id,batch_id 0 92 miss% 0.0223926000598637
plot_id,batch_id 0 93 miss% 0.040569963981887884
plot_id,batch_id 0 94 miss% 0.03553755058777048
plot_id,batch_id 0 95 miss% 0.06158254304249492
plot_id,batch_id 0 96 miss% 0.02648094157676903
plot_id,batch_id 0 97 miss% 0.047828008224543285
plot_id,batch_id 0 98 miss% 0.037520453347519224
plot_id,batch_id 0 99 miss% 0.037549743360196564
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03993762 0.03240351 0.0261251  0.03443023 0.01935097 0.03415531
 0.02784759 0.03180008 0.0248888  0.02554559 0.0290341  0.04383885
 0.02762891 0.03033001 0.02966946 0.03429258 0.03283525 0.02981806
 0.03748299 0.02860451 0.0446782  0.01805732 0.02618623 0.02167342
 0.0266872  0.0337824  0.02409831 0.02669867 0.01838504 0.018192
 0.05468317 0.03406865 0.03287243 0.03357443 0.02385231 0.05163745
 0.03862528 0.04529098 0.02601896 0.02347279 0.09494106 0.0220588
 0.01339548 0.0460096  0.03210865 0.02406314 0.01797103 0.0192885
 0.0240933  0.03739731 0.03278703 0.02663994 0.01395654 0.02154448
 0.04083329 0.04199083 0.01480487 0.02110466 0.01978331 0.01935191
 0.03840068 0.0362838  0.02507749 0.03811275 0.03439585 0.04225725
 0.03320404 0.0334714  0.03681265 0.02778975 0.03256953 0.04705154
 0.03872739 0.03945439 0.04454669 0.04754733 0.04840926 0.0394759
 0.03879079 0.03765225 0.0446883  0.01436062 0.01531719 0.03482442
 0.02258388 0.04641197 0.03135693 0.02251215 0.03469425 0.03049421
 0.0307343  0.02683151 0.0223926  0.04056996 0.03553755 0.06158254
 0.02648094 0.04782801 0.03752045 0.03754974]
for model  21 the mean error 0.03250980731078714
all id 21 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 21 out win 3 err 0.03250980731078714
Launcher: Job 22 completed in 6119 seconds.
Launcher: Task 96 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  46193
Epoch:0, Train loss:0.434749, valid loss:0.392697
Epoch:1, Train loss:0.223956, valid loss:0.229409
Epoch:2, Train loss:0.218082, valid loss:0.228771
Epoch:3, Train loss:0.217126, valid loss:0.228795
Epoch:4, Train loss:0.216726, valid loss:0.228402
Epoch:5, Train loss:0.216519, valid loss:0.228241
Epoch:6, Train loss:0.216289, valid loss:0.227834
Epoch:7, Train loss:0.216252, valid loss:0.227989
Epoch:8, Train loss:0.216118, valid loss:0.228293
Epoch:9, Train loss:0.216019, valid loss:0.227997
Epoch:10, Train loss:0.215949, valid loss:0.227999
Epoch:11, Train loss:0.215467, valid loss:0.227562
Epoch:12, Train loss:0.215437, valid loss:0.227635
Epoch:13, Train loss:0.215467, valid loss:0.227986
Epoch:14, Train loss:0.215418, valid loss:0.227564
Epoch:15, Train loss:0.215372, valid loss:0.227626
Epoch:16, Train loss:0.215382, valid loss:0.227512
Epoch:17, Train loss:0.215372, valid loss:0.227562
Epoch:18, Train loss:0.215350, valid loss:0.227506
Epoch:19, Train loss:0.215319, valid loss:0.227560
Epoch:20, Train loss:0.215326, valid loss:0.227586
Epoch:21, Train loss:0.215069, valid loss:0.227431
Epoch:22, Train loss:0.215048, valid loss:0.227455
Epoch:23, Train loss:0.215056, valid loss:0.227517
Epoch:24, Train loss:0.215045, valid loss:0.227440
Epoch:25, Train loss:0.215057, valid loss:0.227379
Epoch:26, Train loss:0.215041, valid loss:0.227418
Epoch:27, Train loss:0.215021, valid loss:0.227448
Epoch:28, Train loss:0.215020, valid loss:0.227493
Epoch:29, Train loss:0.215040, valid loss:0.227429
Epoch:30, Train loss:0.215021, valid loss:0.227423
Epoch:31, Train loss:0.214893, valid loss:0.227382
Epoch:32, Train loss:0.214885, valid loss:0.227352
Epoch:33, Train loss:0.214890, valid loss:0.227355
Epoch:34, Train loss:0.214888, valid loss:0.227368
Epoch:35, Train loss:0.214886, valid loss:0.227353
Epoch:36, Train loss:0.214889, valid loss:0.227354
Epoch:37, Train loss:0.214882, valid loss:0.227337
Epoch:38, Train loss:0.214873, valid loss:0.227315
Epoch:39, Train loss:0.214889, valid loss:0.227322
Epoch:40, Train loss:0.214869, valid loss:0.227356
Epoch:41, Train loss:0.214819, valid loss:0.227305
Epoch:42, Train loss:0.214810, valid loss:0.227304
Epoch:43, Train loss:0.214817, valid loss:0.227314
Epoch:44, Train loss:0.214807, valid loss:0.227320
Epoch:45, Train loss:0.214809, valid loss:0.227302
Epoch:46, Train loss:0.214809, valid loss:0.227328
Epoch:47, Train loss:0.214805, valid loss:0.227287
Epoch:48, Train loss:0.214806, valid loss:0.227294
Epoch:49, Train loss:0.214807, valid loss:0.227296
Epoch:50, Train loss:0.214799, valid loss:0.227316
Epoch:51, Train loss:0.214775, valid loss:0.227302
Epoch:52, Train loss:0.214773, valid loss:0.227290
Epoch:53, Train loss:0.214773, valid loss:0.227305
Epoch:54, Train loss:0.214773, valid loss:0.227298
Epoch:55, Train loss:0.214771, valid loss:0.227301
Epoch:56, Train loss:0.214772, valid loss:0.227297
Epoch:57, Train loss:0.214769, valid loss:0.227298
Epoch:58, Train loss:0.214770, valid loss:0.227300
Epoch:59, Train loss:0.214769, valid loss:0.227301
Epoch:60, Train loss:0.214767, valid loss:0.227288
training time 5970.679928302765
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.548732399204089
plot_id,batch_id 0 1 miss% 0.6815632057958672
plot_id,batch_id 0 2 miss% 0.6954800197032935
plot_id,batch_id 0 3 miss% 0.7105843185268175
plot_id,batch_id 0 4 miss% 0.7096175695205729
plot_id,batch_id 0 5 miss% 0.5527217378682684
plot_id,batch_id 0 6 miss% 0.6715086536021224
plot_id,batch_id 0 7 miss% 0.6992663239512683
plot_id,batch_id 0 8 miss% 0.7035233517166759
plot_id,batch_id 0 9 miss% 0.7134439792967171
plot_id,batch_id 0 10 miss% 0.5357571950325506
plot_id,batch_id 0 11 miss% 0.666798095855211
plot_id,batch_id 0 12 miss% 0.6857363336719747
plot_id,batch_id 0 13 miss% 0.7000317273115799
plot_id,batch_id 0 14 miss% 0.7132143780880261
plot_id,batch_id 0 15 miss% 0.5336040937581658
plot_id,batch_id 0 16 miss% 0.670817525799383
plot_id,batch_id 0 17 miss% 0.701530858988777
plot_id,batch_id 0 18 miss% 0.7034258920964619
plot_id,batch_id 0 19 miss% 0.7099999370362222
plot_id,batch_id 0 20 miss% 0.6171856838106436
plot_id,batch_id 0 21 miss% 0.701176393233512
plot_id,batch_id 0 22 miss% 0.7094218730180657
plot_id,batch_id 0 23 miss% 0.7171752280072171
plot_id,batch_id 0 24 miss% 0.7215689747179148
plot_id,batch_id 0 25 miss% 0.6080525024204778
plot_id,batch_id 0 26 miss% 0.6985407679998712
plot_id,batch_id 0 27 miss% 0.7089462638714498
plot_id,batch_id 0 28 miss% 0.7164307500970394
plot_id,batch_id 0 29 miss% 0.7213384323756211
plot_id,batch_id 0 30 miss% 0.6073684975559173
plot_id,batch_id 0 31 miss% 0.6908807591239118
plot_id,batch_id 0 32 miss% 0.7050743699730685
plot_id,batch_id 0 33 miss% 0.7090834445574512
plot_id,batch_id 0 34 miss% 0.712619974832997
plot_id,batch_id 0 35 miss% 0.591598981926149
plot_id,batch_id 0 36 miss% 0.6909914793171761
plot_id,batch_id 0 37 miss% 0.702466269960249
plot_id,batch_id 0 38 miss% 0.7101644806227971
plot_id,batch_id 0 39 miss% 0.7124982194269283
plot_id,batch_id 0 40 miss% 0.6597041477911791
plot_id,batch_id 0 41 miss% 0.711460497422156
plot_id,batch_id 0 42 miss% 0.7129323266121783
plot_id,batch_id 0 43 miss% 0.72311059856024
plot_id,batch_id 0 44 miss% 0.7283732967141314
plot_id,batch_id 0 45 miss% 0.6541900546242582
plot_id,batch_id 0 46 miss% 0.709678828551562
plot_id,batch_id 0 47 miss% 0.7137973680971132
plot_id,batch_id 0 48 miss% 0.7211662422568951
plot_id,batch_id 0 49 miss% 0.726706116540465
plot_id,batch_id 0 50 miss% 0.6682611675987835
plot_id,batch_id 0 51 miss% 0.7056859425797241
plot_id,batch_id 0 52 miss% 0.7138776393129342
plot_id,batch_id 0 53 miss% 0.7194525579171247
plot_id,batch_id 0 54 miss% 0.7292618546894517
plot_id,batch_id 0 55 miss% 0.67478002386797
plot_id,batch_id 0 56 miss% 0.706723096942818
plot_id,batch_id 0 57 miss% 0.7141303925046133
plot_id,batch_id 0 58 miss% 0.7205544881371241
plot_id,batch_id 0 59 miss% 0.7190228696493615
plot_id,batch_id 0 60 miss% 0.44018592765084624
plot_id,batch_id 0 61 miss% 0.6126303282670456
plot_id,batch_id 0 62 miss% 0.6616844039708956
plot_id,batch_id 0 63 miss% 0.6852872262720978
plot_id,batch_id 0 64 miss% 0.6922106325375097
plot_id,batch_id 0 65 miss% 0.4283946622639796
plot_id,batch_id 0 66 miss% 0.6092976163680544
plot_id,batch_id 0 67 miss% 0.6474858259135393
plot_id,batch_id 0 68 miss% 0.6853739389214919
plot_id,batch_id 0 69 miss% 0.687686246254997
plot_id,batch_id 0 70 miss% 0.4073298660725544
plot_id,batch_id 0 71 miss% 0.5947912465058629
plot_id,batch_id 0 72 miss% 0.6341654754735272
plot_id,batch_id 0 73 miss% 0.6587687272161461
plot_id,batch_id 0 74 miss% 0.6845767824870174
plot_id,batch_id 0 75 miss% 0.3923262395935366
plot_id,batch_id 0 76 miss% 0.5760387686833677
plot_id,batch_id 0 77 miss% 0.6297579345981967
plot_id,batch_id 0 78 miss% 0.6704714627165946
plot_id,batch_id 0 79 miss% 0.6682945908346056
plot_id,batch_id 0 80 miss% 0.4740662770798518
plot_id,batch_id 0 81 miss% 0.6408049104487024
plot_id,batch_id 0 82 miss% 0.6768050426036268
plot_id,batch_id 0 83 miss% 0.695758845655096
plot_id,batch_id 0 84 miss% 0.698259536874655
plot_id,batch_id 0 85 miss% 0.4651888663777668
plot_id,batch_id 0 86 miss% 0.6318303961691814
plot_id,batch_id 0 87 miss% 0.6677965848861673
plot_id,batch_id 0 88 miss% 0.6894139482052897
plot_id,batch_id 0 89 miss% 0.7008067066445629
plot_id,batch_id 0 90 miss% 0.43918535365961836
plot_id,batch_id 0 91 miss% 0.6317386243470424
plot_id,batch_id 0 92 miss% 0.6673951256199084
plot_id,batch_id 0 93 miss% 0.687851793882024
plot_id,batch_id 0 94 miss% 0.6989521293771916
plot_id,batch_id 0 95 miss% 0.4332950706435522
plot_id,batch_id 0 96 miss% 0.6087836158831703
plot_id,batch_id 0 97 miss% 0.654323882172684
plot_id,batch_id 0 98 miss% 0.6710029229603345
plot_id,batch_id 0 99 miss% 0.6853623133929896
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.5487324  0.68156321 0.69548002 0.71058432 0.70961757 0.55272174
 0.67150865 0.69926632 0.70352335 0.71344398 0.5357572  0.6667981
 0.68573633 0.70003173 0.71321438 0.53360409 0.67081753 0.70153086
 0.70342589 0.70999994 0.61718568 0.70117639 0.70942187 0.71717523
 0.72156897 0.6080525  0.69854077 0.70894626 0.71643075 0.72133843
 0.6073685  0.69088076 0.70507437 0.70908344 0.71261997 0.59159898
 0.69099148 0.70246627 0.71016448 0.71249822 0.65970415 0.7114605
 0.71293233 0.7231106  0.7283733  0.65419005 0.70967883 0.71379737
 0.72116624 0.72670612 0.66826117 0.70568594 0.71387764 0.71945256
 0.72926185 0.67478002 0.7067231  0.71413039 0.72055449 0.71902287
 0.44018593 0.61263033 0.6616844  0.68528723 0.69221063 0.42839466
 0.60929762 0.64748583 0.68537394 0.68768625 0.40732987 0.59479125
 0.63416548 0.65876873 0.68457678 0.39232624 0.57603877 0.62975793
 0.67047146 0.66829459 0.47406628 0.64080491 0.67680504 0.69575885
 0.69825954 0.46518887 0.6318304  0.66779658 0.68941395 0.70080671
 0.43918535 0.63173862 0.66739513 0.68785179 0.69895213 0.43329507
 0.60878362 0.65432388 0.67100292 0.68536231]
for model  192 the mean error 0.6577819030352788
all id 192 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 31 out win 3 err 0.6577819030352788
Launcher: Job 193 completed in 6148 seconds.
Launcher: Task 228 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  77489
Epoch:0, Train loss:0.665920, valid loss:0.672965
Epoch:1, Train loss:0.052241, valid loss:0.010534
Epoch:2, Train loss:0.015066, valid loss:0.005918
Epoch:3, Train loss:0.010659, valid loss:0.006494
Epoch:4, Train loss:0.008642, valid loss:0.004527
Epoch:5, Train loss:0.007399, valid loss:0.004111
Epoch:6, Train loss:0.006698, valid loss:0.003875
Epoch:7, Train loss:0.006157, valid loss:0.004052
Epoch:8, Train loss:0.005682, valid loss:0.002683
Epoch:9, Train loss:0.005340, valid loss:0.002824
Epoch:10, Train loss:0.005086, valid loss:0.003551
Epoch:11, Train loss:0.003218, valid loss:0.002114
Epoch:12, Train loss:0.003332, valid loss:0.001949
Epoch:13, Train loss:0.003309, valid loss:0.001928
Epoch:14, Train loss:0.003094, valid loss:0.001768
Epoch:15, Train loss:0.003149, valid loss:0.001596
Epoch:16, Train loss:0.002954, valid loss:0.002044
Epoch:17, Train loss:0.002939, valid loss:0.001968
Epoch:18, Train loss:0.002900, valid loss:0.001747
Epoch:19, Train loss:0.002791, valid loss:0.001695
Epoch:20, Train loss:0.002725, valid loss:0.001994
Epoch:21, Train loss:0.001980, valid loss:0.001245
Epoch:22, Train loss:0.001989, valid loss:0.001432
Epoch:23, Train loss:0.001923, valid loss:0.001429
Epoch:24, Train loss:0.001916, valid loss:0.001235
Epoch:25, Train loss:0.001932, valid loss:0.001203
Epoch:26, Train loss:0.001969, valid loss:0.001080
Epoch:27, Train loss:0.001894, valid loss:0.001262
Epoch:28, Train loss:0.001831, valid loss:0.001332
Epoch:29, Train loss:0.001824, valid loss:0.001239
Epoch:30, Train loss:0.001783, valid loss:0.001310
Epoch:31, Train loss:0.001401, valid loss:0.000974
Epoch:32, Train loss:0.001397, valid loss:0.001009
Epoch:33, Train loss:0.001350, valid loss:0.001003
Epoch:34, Train loss:0.001393, valid loss:0.000939
Epoch:35, Train loss:0.001379, valid loss:0.000929
Epoch:36, Train loss:0.001357, valid loss:0.001190
Epoch:37, Train loss:0.001331, valid loss:0.001010
Epoch:38, Train loss:0.001353, valid loss:0.000957
Epoch:39, Train loss:0.001341, valid loss:0.001178
Epoch:40, Train loss:0.001331, valid loss:0.001001
Epoch:41, Train loss:0.001140, valid loss:0.000901
Epoch:42, Train loss:0.001123, valid loss:0.000879
Epoch:43, Train loss:0.001119, valid loss:0.000874
Epoch:44, Train loss:0.001121, valid loss:0.000841
Epoch:45, Train loss:0.001112, valid loss:0.000917
Epoch:46, Train loss:0.001108, valid loss:0.000859
Epoch:47, Train loss:0.001100, valid loss:0.000859
Epoch:48, Train loss:0.001105, valid loss:0.000852
Epoch:49, Train loss:0.001087, valid loss:0.000852
Epoch:50, Train loss:0.001093, valid loss:0.000836
Epoch:51, Train loss:0.001006, valid loss:0.000821
Epoch:52, Train loss:0.000994, valid loss:0.000782
Epoch:53, Train loss:0.000998, valid loss:0.000799
Epoch:54, Train loss:0.000995, valid loss:0.000800
Epoch:55, Train loss:0.000991, valid loss:0.000793
Epoch:56, Train loss:0.000990, valid loss:0.000839
Epoch:57, Train loss:0.000992, valid loss:0.000807
Epoch:58, Train loss:0.000982, valid loss:0.000789
Epoch:59, Train loss:0.000982, valid loss:0.000819
Epoch:60, Train loss:0.000986, valid loss:0.000786
training time 5962.217768192291
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.02761917522037478
plot_id,batch_id 0 1 miss% 0.019103409061970018
plot_id,batch_id 0 2 miss% 0.03068111204623653
plot_id,batch_id 0 3 miss% 0.024004850228108335
plot_id,batch_id 0 4 miss% 0.04024346182513444
plot_id,batch_id 0 5 miss% 0.046858022879711955
plot_id,batch_id 0 6 miss% 0.02322816480472976
plot_id,batch_id 0 7 miss% 0.017782844923918767
plot_id,batch_id 0 8 miss% 0.02584359427983829
plot_id,batch_id 0 9 miss% 0.02397907885389563
plot_id,batch_id 0 10 miss% 0.0489799376805023
plot_id,batch_id 0 11 miss% 0.032963544311675474
plot_id,batch_id 0 12 miss% 0.029841161503008945
plot_id,batch_id 0 13 miss% 0.026077788028676347
plot_id,batch_id 0 14 miss% 0.02028939897963298
plot_id,batch_id 0 15 miss% 0.046540106680520006
plot_id,batch_id 0 16 miss% 0.03076271142926978
plot_id,batch_id 0 17 miss% 0.03411226727285523
plot_id,batch_id 0 18 miss% 0.030972204975207834
plot_id,batch_id 0 19 miss% 0.032529683940322476
plot_id,batch_id 0 20 miss% 0.039024113819328334
plot_id,batch_id 0 21 miss% 0.026857818852710488
plot_id,batch_id 0 22 miss% 0.026720147024144835
plot_id,batch_id 0 23 miss% 0.016338894133238017
plot_id,batch_id 0 24 miss% 0.02598496755814078
plot_id,batch_id 0 25 miss% 0.04326258120250948
plot_id,batch_id 0 26 miss% 0.02751444437389237
plot_id,batch_id 0 27 miss% 0.01751823884999305
plot_id,batch_id 0 28 miss% 0.021748052664569487
plot_id,batch_id 0 29 miss% 0.0240360025879484
plot_id,batch_id 0 30 miss% 0.04216984038618079
plot_id,batch_id 0 31 miss% 0.027054477887956233
plot_id,batch_id 0 32 miss% 0.022182516231584067
plot_id,batch_id 0 33 miss% 0.020756023968276076
plot_id,batch_id 0 34 miss% 0.019615266487248945
plot_id,batch_id 0 35 miss% 0.04722215355074272
plot_id,batch_id 0 36 miss% 0.038983795351980544
plot_id,batch_id 0 37 miss% 0.023716270652571012
plot_id,batch_id 0 38 miss% 0.020467602239210934
plot_id,batch_id 0 39 miss% 0.012026737588166381
plot_id,batch_id 0 40 miss% 0.0686524799259475
plot_id,batch_id 0 41 miss% 0.02235893812778925
plot_id,batch_id 0 42 miss% 0.020973341329584633
plot_id,batch_id 0 43 miss% 0.035813922369650975
plot_id,batch_id 0 44 miss% 0.02025770053175226
plot_id,batch_id 0 45 miss% 0.04419826013596985
plot_id,batch_id 0 46 miss% 0.02895082947832672
plot_id,batch_id 0 47 miss% 0.016852575478145435
plot_id,batch_id 0 48 miss% 0.024766594250122707
plot_id,batch_id 0 49 miss% 0.04748247329312425
plot_id,batch_id 0 50 miss% 0.027606806489694442
plot_id,batch_id 0 51 miss% 0.01982813452348147
plot_id,batch_id 0 52 miss% 0.022791954090632484
plot_id,batch_id 0 53 miss% 0.01320000421541623
plot_id,batch_id 0 54 miss% 0.027223903263656146
plot_id,batch_id 0 55 miss% 0.025670058166587205
plot_id,batch_id 0 56 miss% 0.026657264945013585
plot_id,batch_id 0 57 miss% 0.024872491268194002
plot_id,batch_id 0 58 miss% 0.024149093974060974
plot_id,batch_id 0 59 miss% 0.020307756653445336
plot_id,batch_id 0 60 miss% 0.05040894878215586
plot_id,batch_id 0 61 miss% 0.03112311577618282
plot_id,batch_id 0 62 miss% 0.029139498723518887
plot_id,batch_id 0 63 miss% 0.02824782259996594
plot_id,batch_id 0 64 miss% 0.03343768030962976
plot_id,batch_id 0 65 miss% 0.05628570031827947
plot_id,batch_id 0 66 miss% 0.042644626638055745
plot_id,batch_id 0 67 miss% 0.020777167214888093
plot_id,batch_id 0 68 miss% 0.019480815190597255
plot_id,batch_id 0 69 miss% 0.02394922738163998
plot_id,batch_id 0 70 miss% 0.03243829994176143
plot_id,batch_id 0 71 miss% 0.05233717644914539
plot_id,batch_id 0 72 miss% 0.025337842875516325
plot_id,batch_id 0 73 miss% 0.03355011638147175
plot_id,batch_id 0 74 miss% 0.027986091516593623
plot_id,batch_id 0 75 miss% 0.02746387502730284
plot_id,batch_id 0 76 miss% 0.052274529439759465
plot_id,batch_id 0 77 miss% 0.032404048150451836
plot_id,batch_id 0 78 miss% 0.029048836786315032
plot_id,batch_id 0 79 miss% 0.054171709174194206
plot_id,batch_id 0 80 miss% 0.04382940715251656
plot_id,batch_id 0 81 miss% 0.029136933144638808
plot_id,batch_id 0 82 miss% 0.01621241398119896
plot_id,batch_id 0 83 miss% 0.028170117589726682
plot_id,batch_id 0 84 miss% 0.02768761673238225
plot_id,batch_id 0 85 miss% 0.046531660237289156
plot_id,batch_id 0 86 miss% 0.025633826928138405
plot_id,batch_id 0 87 miss% 0.02276189719756462
plot_id,batch_id 0 88 miss% 0.02244369634151581
plot_id,batch_id 0 89 miss% 0.019997548440459402
plot_id,batch_id 0 90 miss% 0.04984711127089724
plot_id,batch_id 0 91 miss% 0.03169044750079995
plot_id,batch_id 0 92 miss% 0.019811236911687884
plot_id,batch_id 0 93 miss% 0.025706140210880645
plot_id,batch_id 0 94 miss% 0.039771618291717596
plot_id,batch_id 0 95 miss% 0.05116528090119621
plot_id,batch_id 0 96 miss% 0.026314547529609638
plot_id,batch_id 0 97 miss% 0.04785247151619844
plot_id,batch_id 0 98 miss% 0.03010611529427184
plot_id,batch_id 0 99 miss% 0.02559914830572317
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02761918 0.01910341 0.03068111 0.02400485 0.04024346 0.04685802
 0.02322816 0.01778284 0.02584359 0.02397908 0.04897994 0.03296354
 0.02984116 0.02607779 0.0202894  0.04654011 0.03076271 0.03411227
 0.0309722  0.03252968 0.03902411 0.02685782 0.02672015 0.01633889
 0.02598497 0.04326258 0.02751444 0.01751824 0.02174805 0.024036
 0.04216984 0.02705448 0.02218252 0.02075602 0.01961527 0.04722215
 0.0389838  0.02371627 0.0204676  0.01202674 0.06865248 0.02235894
 0.02097334 0.03581392 0.0202577  0.04419826 0.02895083 0.01685258
 0.02476659 0.04748247 0.02760681 0.01982813 0.02279195 0.0132
 0.0272239  0.02567006 0.02665726 0.02487249 0.02414909 0.02030776
 0.05040895 0.03112312 0.0291395  0.02824782 0.03343768 0.0562857
 0.04264463 0.02077717 0.01948082 0.02394923 0.0324383  0.05233718
 0.02533784 0.03355012 0.02798609 0.02746388 0.05227453 0.03240405
 0.02904884 0.05417171 0.04382941 0.02913693 0.01621241 0.02817012
 0.02768762 0.04653166 0.02563383 0.0227619  0.0224437  0.01999755
 0.04984711 0.03169045 0.01981124 0.02570614 0.03977162 0.05116528
 0.02631455 0.04785247 0.03010612 0.02559915]
for model  75 the mean error 0.030510034110024172
all id 75 hidden_dim 24 learning_rate 0.02 num_layers 5 frames 21 out win 3 err 0.030510034110024172
Launcher: Job 76 completed in 6162 seconds.
Launcher: Task 72 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  61841
Epoch:0, Train loss:0.598040, valid loss:0.600524
Epoch:1, Train loss:0.227028, valid loss:0.007073
Epoch:2, Train loss:0.015034, valid loss:0.005326
Epoch:3, Train loss:0.010858, valid loss:0.003410
Epoch:4, Train loss:0.005762, valid loss:0.003104
Epoch:5, Train loss:0.004848, valid loss:0.002530
Epoch:6, Train loss:0.004131, valid loss:0.002293
Epoch:7, Train loss:0.003806, valid loss:0.002181
Epoch:8, Train loss:0.003563, valid loss:0.002184
Epoch:9, Train loss:0.003346, valid loss:0.001618
Epoch:10, Train loss:0.003288, valid loss:0.001598
Epoch:11, Train loss:0.002322, valid loss:0.001386
Epoch:12, Train loss:0.002263, valid loss:0.001695
Epoch:13, Train loss:0.002257, valid loss:0.001398
Epoch:14, Train loss:0.002231, valid loss:0.001207
Epoch:15, Train loss:0.002153, valid loss:0.001283
Epoch:16, Train loss:0.002121, valid loss:0.001405
Epoch:17, Train loss:0.002050, valid loss:0.001292
Epoch:18, Train loss:0.002003, valid loss:0.001463
Epoch:19, Train loss:0.002003, valid loss:0.001404
Epoch:20, Train loss:0.001953, valid loss:0.001555
Epoch:21, Train loss:0.001502, valid loss:0.001013
Epoch:22, Train loss:0.001486, valid loss:0.001057
Epoch:23, Train loss:0.001482, valid loss:0.000992
Epoch:24, Train loss:0.001467, valid loss:0.001001
Epoch:25, Train loss:0.001444, valid loss:0.001032
Epoch:26, Train loss:0.001419, valid loss:0.001134
Epoch:27, Train loss:0.001439, valid loss:0.000972
Epoch:28, Train loss:0.001417, valid loss:0.000987
Epoch:29, Train loss:0.001404, valid loss:0.001030
Epoch:30, Train loss:0.001414, valid loss:0.000985
Epoch:31, Train loss:0.001160, valid loss:0.000838
Epoch:32, Train loss:0.001151, valid loss:0.000813
Epoch:33, Train loss:0.001155, valid loss:0.000827
Epoch:34, Train loss:0.001144, valid loss:0.000906
Epoch:35, Train loss:0.001140, valid loss:0.000942
Epoch:36, Train loss:0.001147, valid loss:0.000916
Epoch:37, Train loss:0.001119, valid loss:0.000845
Epoch:38, Train loss:0.001123, valid loss:0.000835
Epoch:39, Train loss:0.001110, valid loss:0.000881
Epoch:40, Train loss:0.001115, valid loss:0.000834
Epoch:41, Train loss:0.001005, valid loss:0.000788
Epoch:42, Train loss:0.000991, valid loss:0.000791
Epoch:43, Train loss:0.001001, valid loss:0.000816
Epoch:44, Train loss:0.000990, valid loss:0.000828
Epoch:45, Train loss:0.000987, valid loss:0.000783
Epoch:46, Train loss:0.000985, valid loss:0.000785
Epoch:47, Train loss:0.000977, valid loss:0.000880
Epoch:48, Train loss:0.000976, valid loss:0.000801
Epoch:49, Train loss:0.000977, valid loss:0.000785
Epoch:50, Train loss:0.000973, valid loss:0.000797
Epoch:51, Train loss:0.000922, valid loss:0.000754
Epoch:52, Train loss:0.000919, valid loss:0.000757
Epoch:53, Train loss:0.000923, valid loss:0.000789
Epoch:54, Train loss:0.000920, valid loss:0.000758
Epoch:55, Train loss:0.000918, valid loss:0.000773
Epoch:56, Train loss:0.000918, valid loss:0.000776
Epoch:57, Train loss:0.000909, valid loss:0.000758
Epoch:58, Train loss:0.000907, valid loss:0.000779
Epoch:59, Train loss:0.000907, valid loss:0.000787
Epoch:60, Train loss:0.000904, valid loss:0.000758
training time 6006.123031377792
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.030148575895487466
plot_id,batch_id 0 1 miss% 0.033171251926422805
plot_id,batch_id 0 2 miss% 0.02754986875481797
plot_id,batch_id 0 3 miss% 0.01916194430289035
plot_id,batch_id 0 4 miss% 0.02031693206467977
plot_id,batch_id 0 5 miss% 0.028154487640207188
plot_id,batch_id 0 6 miss% 0.03488867341133861
plot_id,batch_id 0 7 miss% 0.04126187591068555
plot_id,batch_id 0 8 miss% 0.023829090442323047
plot_id,batch_id 0 9 miss% 0.027820452090230617
plot_id,batch_id 0 10 miss% 0.026752345208883196
plot_id,batch_id 0 11 miss% 0.04962459095422306
plot_id,batch_id 0 12 miss% 0.026336393723039888
plot_id,batch_id 0 13 miss% 0.02339014762410168
plot_id,batch_id 0 14 miss% 0.03175239941554466
plot_id,batch_id 0 15 miss% 0.0433604462313792
plot_id,batch_id 0 16 miss% 0.03811240389384727
plot_id,batch_id 0 17 miss% 0.05144321689907272
plot_id,batch_id 0 18 miss% 0.027355152491706402
plot_id,batch_id 0 19 miss% 0.024425561642457183
plot_id,batch_id 0 20 miss% 0.04275511543693867
plot_id,batch_id 0 21 miss% 0.02961047765531782
plot_id,batch_id 0 22 miss% 0.03559457350401925
plot_id,batch_id 0 23 miss% 0.0245270518210663
plot_id,batch_id 0 24 miss% 0.01982914943654812
plot_id,batch_id 0 25 miss% 0.03931985755330774
plot_id,batch_id 0 26 miss% 0.02314303351576743
plot_id,batch_id 0 27 miss% 0.027156217787031648
plot_id,batch_id 0 28 miss% 0.01729820990649467
plot_id,batch_id 0 29 miss% 0.023391534310766564
plot_id,batch_id 0 30 miss% 0.033687867135394484
plot_id,batch_id 0 31 miss% 0.03749167914510225
plot_id,batch_id 0 32 miss% 0.028763332309532156
plot_id,batch_id 0 33 miss% 0.020042490392265223
plot_id,batch_id 0 34 miss% 0.024634357701157274
plot_id,batch_id 0 35 miss% 0.03715441284263621
plot_id,batch_id 0 36 miss% 0.023623959817019712
plot_id,batch_id 0 37 miss% 0.028171888230186088
plot_id,batch_id 0 38 miss% 0.027418575226451677
plot_id,batch_id 0 39 miss% 0.013062698217699767
plot_id,batch_id 0 40 miss% 0.07276250786382384
plot_id,batch_id 0 41 miss% 0.0235290360357972
plot_id,batch_id 0 42 miss% 0.018196206048649417
plot_id,batch_id 0 43 miss% 0.030908635844373127
plot_id,batch_id 0 44 miss% 0.02646599991200682
plot_id,batch_id 0 45 miss% 0.03452297318298505
plot_id,batch_id 0 46 miss% 0.0325287182437437
plot_id,batch_id 0 47 miss% 0.017328617155266526
plot_id,batch_id 0 48 miss% 0.023719795340613824
plot_id,batch_id 0 49 miss% 0.028859915661838507
plot_id,batch_id 0 50 miss% 0.023999443013246576
plot_id,batch_id 0 51 miss% 0.02864315325385823
plot_id,batch_id 0 52 miss% 0.02475474570086787
plot_id,batch_id 0 53 miss% 0.022253770246918478
plot_id,batch_id 0 54 miss% 0.032805868189583025
plot_id,batch_id 0 55 miss% 0.022898450252446086
plot_id,batch_id 0 56 miss% 0.025339457162413686
plot_id,batch_id 0 57 miss% 0.0292800080961435
plot_id,batch_id 0 58 miss% 0.018497800654728606
plot_id,batch_id 0 59 miss% 0.02350032568356211
plot_id,batch_id 0 60 miss% 0.03513298474147338
plot_id,batch_id 0 61 miss% 0.03118153695717663
plot_id,batch_id 0 62 miss% 0.024385989256006355
plot_id,batch_id 0 63 miss% 0.037182909584555814
plot_id,batch_id 0 64 miss% 0.031695240999646254
plot_id,batch_id 0 65 miss% 0.03903360114083632
plot_id,batch_id 0 66 miss% 0.04266978155907826
plot_id,batch_id 0 67 miss% 0.028810438249517272
plot_id,batch_id 0 68 miss% 0.031014826273235905
plot_id,batch_id 0 69 miss% 0.02393529352292825
plot_id,batch_id 0 70 miss% 0.02603795178289912
plot_id,batch_id 0 71 miss% 0.04365615084233898
plot_id,batch_id 0 72 miss% 0.035238768549062396
plot_id,batch_id 0 73 miss% 0.027852765020436012
plot_id,batch_id 0 74 miss% 0.035176144129135535
plot_id,batch_id 0 75 miss% 0.03936998215224944
plot_id,batch_id 0 76 miss% 0.03445680310571059
plot_id,batch_id 0 77 miss% 0.029270790295675784
plot_id,batch_id 0 78 miss% 0.036334608374778005
plot_id,batch_id 0 79 miss% 0.042339484438185714
plot_id,batch_id 0 80 miss% 0.03786467029646465
plot_id,batch_id 0 81 miss% 0.019308835744406835
plot_id,batch_id 0 82 miss% 0.02163115658899759
plot_id,batch_id 0 83 miss% 0.02941196109927181
plot_id,batch_id 0 84 miss% 0.02921340566975525
plot_id,batch_id 0 85 miss% 0.045352613656955504
plot_id,batch_id 0 86 miss% 0.02307843337733238
plot_id,batch_id 0 87 miss% 0.02436758646242252
plot_id,batch_id 0 88 miss% 0.03404107212989595
plot_id,batch_id 0 89 miss% 0.022907824615343898
plot_id,batch_id 0 90 miss% 0.0436408980530789
plot_id,batch_id 0 91 miss% 0.036745832073106124
plot_id,batch_id 0 92 miss% 0.01787184395069299
plot_id,batch_id 0 93 miss% 0.03031778892675576
plot_id,batch_id 0 94 miss% 0.03246089543008645
plot_id,batch_id 0 95 miss% 0.04340122032423147
plot_id,batch_id 0 96 miss% 0.029305117039477732
plot_id,batch_id 0 97 miss% 0.029970591008516397
plot_id,batch_id 0 98 miss% 0.03226902362741298
plot_id,batch_id 0 99 miss% 0.03651830514743257
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03014858 0.03317125 0.02754987 0.01916194 0.02031693 0.02815449
 0.03488867 0.04126188 0.02382909 0.02782045 0.02675235 0.04962459
 0.02633639 0.02339015 0.0317524  0.04336045 0.0381124  0.05144322
 0.02735515 0.02442556 0.04275512 0.02961048 0.03559457 0.02452705
 0.01982915 0.03931986 0.02314303 0.02715622 0.01729821 0.02339153
 0.03368787 0.03749168 0.02876333 0.02004249 0.02463436 0.03715441
 0.02362396 0.02817189 0.02741858 0.0130627  0.07276251 0.02352904
 0.01819621 0.03090864 0.026466   0.03452297 0.03252872 0.01732862
 0.0237198  0.02885992 0.02399944 0.02864315 0.02475475 0.02225377
 0.03280587 0.02289845 0.02533946 0.02928001 0.0184978  0.02350033
 0.03513298 0.03118154 0.02438599 0.03718291 0.03169524 0.0390336
 0.04266978 0.02881044 0.03101483 0.02393529 0.02603795 0.04365615
 0.03523877 0.02785277 0.03517614 0.03936998 0.0344568  0.02927079
 0.03633461 0.04233948 0.03786467 0.01930884 0.02163116 0.02941196
 0.02921341 0.04535261 0.02307843 0.02436759 0.03404107 0.02290782
 0.0436409  0.03674583 0.01787184 0.03031779 0.0324609  0.04340122
 0.02930512 0.02997059 0.03226902 0.03651831]
for model  120 the mean error 0.030407828462074713
all id 120 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 25 out win 3 err 0.030407828462074713
Launcher: Job 121 completed in 6199 seconds.
Launcher: Task 59 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  21969
Epoch:0, Train loss:0.437137, valid loss:0.400084
Epoch:1, Train loss:0.226146, valid loss:0.230160
Epoch:2, Train loss:0.220658, valid loss:0.230153
Epoch:3, Train loss:0.219702, valid loss:0.229431
Epoch:4, Train loss:0.219208, valid loss:0.229409
Epoch:5, Train loss:0.218965, valid loss:0.229408
Epoch:6, Train loss:0.218819, valid loss:0.229732
Epoch:7, Train loss:0.218722, valid loss:0.229803
Epoch:8, Train loss:0.218721, valid loss:0.228952
Epoch:9, Train loss:0.218552, valid loss:0.229549
Epoch:10, Train loss:0.218524, valid loss:0.229189
Epoch:11, Train loss:0.217666, valid loss:0.228806
Epoch:12, Train loss:0.217644, valid loss:0.229053
Epoch:13, Train loss:0.217618, valid loss:0.228897
Epoch:14, Train loss:0.217594, valid loss:0.228839
Epoch:15, Train loss:0.217551, valid loss:0.228921
Epoch:16, Train loss:0.217530, valid loss:0.228939
Epoch:17, Train loss:0.217503, valid loss:0.228743
Epoch:18, Train loss:0.217486, valid loss:0.228794
Epoch:19, Train loss:0.217445, valid loss:0.229088
Epoch:20, Train loss:0.217509, valid loss:0.228902
Epoch:21, Train loss:0.217073, valid loss:0.228559
Epoch:22, Train loss:0.217056, valid loss:0.228739
Epoch:23, Train loss:0.217074, valid loss:0.228570
Epoch:24, Train loss:0.217026, valid loss:0.228542
Epoch:25, Train loss:0.217034, valid loss:0.228653
Epoch:26, Train loss:0.217012, valid loss:0.228557
Epoch:27, Train loss:0.217046, valid loss:0.228495
Epoch:28, Train loss:0.217039, valid loss:0.228524
Epoch:29, Train loss:0.216994, valid loss:0.228538
Epoch:30, Train loss:0.217003, valid loss:0.228551
Epoch:31, Train loss:0.216780, valid loss:0.228491
Epoch:32, Train loss:0.216771, valid loss:0.228709
Epoch:33, Train loss:0.216777, valid loss:0.228387
Epoch:34, Train loss:0.216767, valid loss:0.228488
Epoch:35, Train loss:0.216776, valid loss:0.228453
Epoch:36, Train loss:0.216763, valid loss:0.228444
Epoch:37, Train loss:0.216752, valid loss:0.228470
Epoch:38, Train loss:0.216758, valid loss:0.228359
Epoch:39, Train loss:0.216750, valid loss:0.228421
Epoch:40, Train loss:0.216759, valid loss:0.228403
Epoch:41, Train loss:0.216626, valid loss:0.228348
Epoch:42, Train loss:0.216628, valid loss:0.228340
Epoch:43, Train loss:0.216617, valid loss:0.228407
Epoch:44, Train loss:0.216618, valid loss:0.228383
Epoch:45, Train loss:0.216615, valid loss:0.228338
Epoch:46, Train loss:0.216613, valid loss:0.228333
Epoch:47, Train loss:0.216617, valid loss:0.228334
Epoch:48, Train loss:0.216605, valid loss:0.228334
Epoch:49, Train loss:0.216607, valid loss:0.228358
Epoch:50, Train loss:0.216610, valid loss:0.228343
Epoch:51, Train loss:0.216539, valid loss:0.228308
Epoch:52, Train loss:0.216542, valid loss:0.228288
Epoch:53, Train loss:0.216539, valid loss:0.228328
Epoch:54, Train loss:0.216537, valid loss:0.228294
Epoch:55, Train loss:0.216539, valid loss:0.228293
Epoch:56, Train loss:0.216534, valid loss:0.228324
Epoch:57, Train loss:0.216531, valid loss:0.228352
Epoch:58, Train loss:0.216529, valid loss:0.228306
Epoch:59, Train loss:0.216534, valid loss:0.228283
Epoch:60, Train loss:0.216528, valid loss:0.228310
training time 6043.541679859161
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.6533576079949411
plot_id,batch_id 0 1 miss% 0.745423244945624
plot_id,batch_id 0 2 miss% 0.7597195832781548
plot_id,batch_id 0 3 miss% 0.7704484658121096
plot_id,batch_id 0 4 miss% 0.7684983606260521
plot_id,batch_id 0 5 miss% 0.6650673074972302
plot_id,batch_id 0 6 miss% 0.7428790271688932
plot_id,batch_id 0 7 miss% 0.7562117364195503
plot_id,batch_id 0 8 miss% 0.767260956188398
plot_id,batch_id 0 9 miss% 0.7714393220133798
plot_id,batch_id 0 10 miss% 0.6345957864674847
plot_id,batch_id 0 11 miss% 0.746853107684495
plot_id,batch_id 0 12 miss% 0.7559546671861375
plot_id,batch_id 0 13 miss% 0.7656606049685216
plot_id,batch_id 0 14 miss% 0.770793445950583
plot_id,batch_id 0 15 miss% 0.6537351632109395
plot_id,batch_id 0 16 miss% 0.7371056918875559
plot_id,batch_id 0 17 miss% 0.7586460745389583
plot_id,batch_id 0 18 miss% 0.7639158331814586
plot_id,batch_id 0 19 miss% 0.7676656477347528
plot_id,batch_id 0 20 miss% 0.7082442149333829
plot_id,batch_id 0 21 miss% 0.7629074619848237
plot_id,batch_id 0 22 miss% 0.7689932477158277
plot_id,batch_id 0 23 miss% 0.7748312113335293
plot_id,batch_id 0 24 miss% 0.7766582291391123
plot_id,batch_id 0 25 miss% 0.6991283321898525
plot_id,batch_id 0 26 miss% 0.7593859892513563
plot_id,batch_id 0 27 miss% 0.7686108520305324
plot_id,batch_id 0 28 miss% 0.7702809336526492
plot_id,batch_id 0 29 miss% 0.7766734603734861
plot_id,batch_id 0 30 miss% 0.696476934513016
plot_id,batch_id 0 31 miss% 0.7579309752246448
plot_id,batch_id 0 32 miss% 0.7649944981368555
plot_id,batch_id 0 33 miss% 0.7690019494878357
plot_id,batch_id 0 34 miss% 0.7697266687816842
plot_id,batch_id 0 35 miss% 0.6942864128644745
plot_id,batch_id 0 36 miss% 0.7666046457316578
plot_id,batch_id 0 37 miss% 0.763423986646828
plot_id,batch_id 0 38 miss% 0.773784662739944
plot_id,batch_id 0 39 miss% 0.7739247786117087
plot_id,batch_id 0 40 miss% 0.7425808985302607
plot_id,batch_id 0 41 miss% 0.7685079925741675
plot_id,batch_id 0 42 miss% 0.7730559251646913
plot_id,batch_id 0 43 miss% 0.7774215295451867
plot_id,batch_id 0 44 miss% 0.7813351011398835
plot_id,batch_id 0 45 miss% 0.7474814236246771
plot_id,batch_id 0 46 miss% 0.7682516477852467
plot_id,batch_id 0 47 miss% 0.7731205935426427
plot_id,batch_id 0 48 miss% 0.7762387840161477
plot_id,batch_id 0 49 miss% 0.781005148965397
plot_id,batch_id 0 50 miss% 0.7414499523983435
plot_id,batch_id 0 51 miss% 0.7668088783258502
plot_id,batch_id 0 52 miss% 0.771819348486421
plot_id,batch_id 0 53 miss% 0.7743570846724983
plot_id,batch_id 0 54 miss% 0.7824952941630616
plot_id,batch_id 0 55 miss% 0.7382110709808387
plot_id,batch_id 0 56 miss% 0.7654242509200636
plot_id,batch_id 0 57 miss% 0.7695666501422864
plot_id,batch_id 0 58 miss% 0.7766935188457856
plot_id,batch_id 0 59 miss% 0.7821784031961944
plot_id,batch_id 0 60 miss% 0.5736354819347053
plot_id,batch_id 0 61 miss% 0.7117399003383744
plot_id,batch_id 0 62 miss% 0.7312090151029745
plot_id,batch_id 0 63 miss% 0.7611036215799212
plot_id,batch_id 0 64 miss% 0.7598859557796718
plot_id,batch_id 0 65 miss% 0.5670138424244902
plot_id,batch_id 0 66 miss% 0.7018728489834732
plot_id,batch_id 0 67 miss% 0.723084078896464
plot_id,batch_id 0 68 miss% 0.7497731057041352
plot_id,batch_id 0 69 miss% 0.7522192967490348
plot_id,batch_id 0 70 miss% 0.5353395667033723
plot_id,batch_id 0 71 miss% 0.6991310891431334
plot_id,batch_id 0 72 miss% 0.7197629734327207
plot_id,batch_id 0 73 miss% 0.7442723789635403
plot_id,batch_id 0 74 miss% 0.7505057134077852
plot_id,batch_id 0 75 miss% 0.5257876616689549
plot_id,batch_id 0 76 miss% 0.6510878958103807
plot_id,batch_id 0 77 miss% 0.7012185299996304
plot_id,batch_id 0 78 miss% 0.7312970568288678
plot_id,batch_id 0 79 miss% 0.7458718266513356
plot_id,batch_id 0 80 miss% 0.5922778651537555
plot_id,batch_id 0 81 miss% 0.7261250719708856
plot_id,batch_id 0 82 miss% 0.744225184985006
plot_id,batch_id 0 83 miss% 0.759210979225524
plot_id,batch_id 0 84 miss% 0.7665216085950896
plot_id,batch_id 0 85 miss% 0.5967389419955109
plot_id,batch_id 0 86 miss% 0.7152238130325091
plot_id,batch_id 0 87 miss% 0.7422315128426076
plot_id,batch_id 0 88 miss% 0.7583585568198324
plot_id,batch_id 0 89 miss% 0.7683550234680688
plot_id,batch_id 0 90 miss% 0.5549407436692153
plot_id,batch_id 0 91 miss% 0.7160796087391363
plot_id,batch_id 0 92 miss% 0.7372654537151316
plot_id,batch_id 0 93 miss% 0.750016282433776
plot_id,batch_id 0 94 miss% 0.7613234852378699
plot_id,batch_id 0 95 miss% 0.5620618108626547
plot_id,batch_id 0 96 miss% 0.7021741461971261
plot_id,batch_id 0 97 miss% 0.7309672188432509
plot_id,batch_id 0 98 miss% 0.745132426873482
plot_id,batch_id 0 99 miss% 0.754056096573608
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.65335761 0.74542324 0.75971958 0.77044847 0.76849836 0.66506731
 0.74287903 0.75621174 0.76726096 0.77143932 0.63459579 0.74685311
 0.75595467 0.7656606  0.77079345 0.65373516 0.73710569 0.75864607
 0.76391583 0.76766565 0.70824421 0.76290746 0.76899325 0.77483121
 0.77665823 0.69912833 0.75938599 0.76861085 0.77028093 0.77667346
 0.69647693 0.75793098 0.7649945  0.76900195 0.76972667 0.69428641
 0.76660465 0.76342399 0.77378466 0.77392478 0.7425809  0.76850799
 0.77305593 0.77742153 0.7813351  0.74748142 0.76825165 0.77312059
 0.77623878 0.78100515 0.74144995 0.76680888 0.77181935 0.77435708
 0.78249529 0.73821107 0.76542425 0.76956665 0.77669352 0.7821784
 0.57363548 0.7117399  0.73120902 0.76110362 0.75988596 0.56701384
 0.70187285 0.72308408 0.74977311 0.7522193  0.53533957 0.69913109
 0.71976297 0.74427238 0.75050571 0.52578766 0.6510879  0.70121853
 0.73129706 0.74587183 0.59227787 0.72612507 0.74422518 0.75921098
 0.76652161 0.59673894 0.71522381 0.74223151 0.75835856 0.76835502
 0.55494074 0.71607961 0.73726545 0.75001628 0.76132349 0.56206181
 0.70217415 0.73096722 0.74513243 0.7540561 ]
for model  217 the mean error 0.7322820224845507
all id 217 hidden_dim 16 learning_rate 0.02 num_layers 3 frames 31 out win 4 err 0.7322820224845507
Launcher: Job 218 completed in 6220 seconds.
Launcher: Task 208 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  79249
Epoch:0, Train loss:0.598719, valid loss:0.574885
Epoch:1, Train loss:0.047481, valid loss:0.010166
Epoch:2, Train loss:0.013439, valid loss:0.006462
Epoch:3, Train loss:0.010254, valid loss:0.004668
Epoch:4, Train loss:0.008983, valid loss:0.004945
Epoch:5, Train loss:0.007704, valid loss:0.004453
Epoch:6, Train loss:0.006725, valid loss:0.003576
Epoch:7, Train loss:0.006100, valid loss:0.003019
Epoch:8, Train loss:0.005606, valid loss:0.003130
Epoch:9, Train loss:0.005065, valid loss:0.003124
Epoch:10, Train loss:0.004643, valid loss:0.002507
Epoch:11, Train loss:0.003464, valid loss:0.002333
Epoch:12, Train loss:0.003470, valid loss:0.001976
Epoch:13, Train loss:0.003272, valid loss:0.002065
Epoch:14, Train loss:0.003247, valid loss:0.002236
Epoch:15, Train loss:0.003083, valid loss:0.002073
Epoch:16, Train loss:0.003242, valid loss:0.001741
Epoch:17, Train loss:0.002995, valid loss:0.001749
Epoch:18, Train loss:0.002839, valid loss:0.001775
Epoch:19, Train loss:0.002788, valid loss:0.001732
Epoch:20, Train loss:0.002826, valid loss:0.001756
Epoch:21, Train loss:0.002176, valid loss:0.001543
Epoch:22, Train loss:0.002114, valid loss:0.001526
Epoch:23, Train loss:0.002121, valid loss:0.001479
Epoch:24, Train loss:0.002078, valid loss:0.001498
Epoch:25, Train loss:0.002068, valid loss:0.001463
Epoch:26, Train loss:0.002042, valid loss:0.001485
Epoch:27, Train loss:0.002026, valid loss:0.001426
Epoch:28, Train loss:0.002018, valid loss:0.001550
Epoch:29, Train loss:0.001978, valid loss:0.001860
Epoch:30, Train loss:0.001971, valid loss:0.001490
Epoch:31, Train loss:0.001653, valid loss:0.001330
Epoch:32, Train loss:0.001621, valid loss:0.001387
Epoch:33, Train loss:0.001629, valid loss:0.001345
Epoch:34, Train loss:0.001629, valid loss:0.001226
Epoch:35, Train loss:0.001613, valid loss:0.001298
Epoch:36, Train loss:0.001599, valid loss:0.001237
Epoch:37, Train loss:0.001615, valid loss:0.001401
Epoch:38, Train loss:0.001607, valid loss:0.001347
Epoch:39, Train loss:0.001555, valid loss:0.001358
Epoch:40, Train loss:0.001565, valid loss:0.001312
Epoch:41, Train loss:0.001425, valid loss:0.001246
Epoch:42, Train loss:0.001412, valid loss:0.001210
Epoch:43, Train loss:0.001417, valid loss:0.001251
Epoch:44, Train loss:0.001411, valid loss:0.001267
Epoch:45, Train loss:0.001396, valid loss:0.001264
Epoch:46, Train loss:0.001398, valid loss:0.001230
Epoch:47, Train loss:0.001397, valid loss:0.001289
Epoch:48, Train loss:0.001384, valid loss:0.001243
Epoch:49, Train loss:0.001384, valid loss:0.001240
Epoch:50, Train loss:0.001382, valid loss:0.001233
Epoch:51, Train loss:0.001314, valid loss:0.001242
Epoch:52, Train loss:0.001303, valid loss:0.001214
Epoch:53, Train loss:0.001302, valid loss:0.001206
Epoch:54, Train loss:0.001299, valid loss:0.001219
Epoch:55, Train loss:0.001300, valid loss:0.001216
Epoch:56, Train loss:0.001297, valid loss:0.001221
Epoch:57, Train loss:0.001293, valid loss:0.001239
Epoch:58, Train loss:0.001290, valid loss:0.001250
Epoch:59, Train loss:0.001286, valid loss:0.001231
Epoch:60, Train loss:0.001287, valid loss:0.001233
training time 6032.523799657822
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.0320242194700358
plot_id,batch_id 0 1 miss% 0.022480833487605274
plot_id,batch_id 0 2 miss% 0.023007717289928634
plot_id,batch_id 0 3 miss% 0.029113826159262755
plot_id,batch_id 0 4 miss% 0.029739162649563528
plot_id,batch_id 0 5 miss% 0.03875388238297968
plot_id,batch_id 0 6 miss% 0.028960372115017844
plot_id,batch_id 0 7 miss% 0.03470585759936898
plot_id,batch_id 0 8 miss% 0.028180886583671762
plot_id,batch_id 0 9 miss% 0.02189983610304201
plot_id,batch_id 0 10 miss% 0.03176765010933753
plot_id,batch_id 0 11 miss% 0.03938962791047866
plot_id,batch_id 0 12 miss% 0.025717260459681573
plot_id,batch_id 0 13 miss% 0.04085250374409286
plot_id,batch_id 0 14 miss% 0.047195414125668356
plot_id,batch_id 0 15 miss% 0.055441956145818726
plot_id,batch_id 0 16 miss% 0.032340880130562
plot_id,batch_id 0 17 miss% 0.04076130432141615
plot_id,batch_id 0 18 miss% 0.04197194326722195
plot_id,batch_id 0 19 miss% 0.03720437830036506
plot_id,batch_id 0 20 miss% 0.0584028812219737
plot_id,batch_id 0 21 miss% 0.02277891388476089
plot_id,batch_id 0 22 miss% 0.034585965943434493
plot_id,batch_id 0 23 miss% 0.023297861894344685
plot_id,batch_id 0 24 miss% 0.025907027596021807
plot_id,batch_id 0 25 miss% 0.04063446849331822
plot_id,batch_id 0 26 miss% 0.02454206412612527
plot_id,batch_id 0 27 miss% 0.023269309353056444
plot_id,batch_id 0 28 miss% 0.03010376522368033
plot_id,batch_id 0 29 miss% 0.02805388772213391
plot_id,batch_id 0 30 miss% 0.04270340911613602
plot_id,batch_id 0 31 miss% 0.039127599262945234
plot_id,batch_id 0 32 miss% 0.02917305282389809
plot_id,batch_id 0 33 miss% 0.03057154563904166
plot_id,batch_id 0 34 miss% 0.025459654182017226
plot_id,batch_id 0 35 miss% 0.0405353536994757
plot_id,batch_id 0 36 miss% 0.04697666993909664
plot_id,batch_id 0 37 miss% 0.04024403543779568
plot_id,batch_id 0 38 miss% 0.03001233876084236
plot_id,batch_id 0 39 miss% 0.02070220860732545
plot_id,batch_id 0 40 miss% 0.08275831782788241
plot_id,batch_id 0 41 miss% 0.027775726520832995
plot_id,batch_id 0 42 miss% 0.01711925558810302
plot_id,batch_id 0 43 miss% 0.031506324188209166
plot_id,batch_id 0 44 miss% 0.020385587128934533
plot_id,batch_id 0 45 miss% 0.01867505357717781
plot_id,batch_id 0 46 miss% 0.01766182590106805
plot_id,batch_id 0 47 miss% 0.02513698696243395
plot_id,batch_id 0 48 miss% 0.02667997622202108
plot_id,batch_id 0 49 miss% 0.020574592148962876
plot_id,batch_id 0 50 miss% 0.03598836523205888
plot_id,batch_id 0 51 miss% 0.020656056914765783
plot_id,batch_id 0 52 miss% 0.02596887991755774
plot_id,batch_id 0 53 miss% 0.01705021512908181
plot_id,batch_id 0 54 miss% 0.02966869721226789
plot_id,batch_id 0 55 miss% 0.031347513923306224
plot_id,batch_id 0 56 miss% 0.03253142129628644
plot_id,batch_id 0 57 miss% 0.03330469067959736
plot_id,batch_id 0 58 miss% 0.030234656462652228
plot_id,batch_id 0 59 miss% 0.021448266029660903
plot_id,batch_id 0 60 miss% 0.0273591279403334
plot_id,batch_id 0 61 miss% 0.037719757767086505
plot_id,batch_id 0 62 miss% 0.026279094592398404
plot_id,batch_id 0 63 miss% 0.03249404850224076
plot_id,batch_id 0 64 miss% 0.03557766910403968
plot_id,batch_id 0 65 miss% 0.04232649245292886
plot_id,batch_id 0 66 miss% 0.02368154735839693
plot_id,batch_id 0 67 miss% 0.0336273438713905
plot_id,batch_id 0 68 miss% 0.03505283264240713
plot_id,batch_id 0 69 miss% 0.021948596749392636
plot_id,batch_id 0 70 miss% 0.06551408499433391
plot_id,batch_id 0 71 miss% 0.06421744214143368
plot_id,batch_id 0 72 miss% 0.035602238842963
plot_id,batch_id 0 73 miss% 0.03198492641844714
plot_id,batch_id 0 74 miss% 0.0465344068461889
plot_id,batch_id 0 75 miss% 0.05943535026560925
plot_id,batch_id 0 76 miss% 0.04503944089771035
plot_id,batch_id 0 77 miss% 0.02658705238312425
plot_id,batch_id 0 78 miss% 0.04069552900364936
plot_id,batch_id 0 79 miss% 0.04681031009184168
plot_id,batch_id 0 80 miss% 0.05711677340702856
plot_id,batch_id 0 81 miss% 0.025610833320916557
plot_id,batch_id 0 82 miss% 0.0340385960921299
plot_id,batch_id 0 83 miss% 0.03290981067626747
plot_id,batch_id 0 84 miss% 0.03566675055310408
plot_id,batch_id 0 85 miss% 0.0367751558519922
plot_id,batch_id 0 86 miss% 0.02759096343582455
plot_id,batch_id 0 87 miss% 0.0306277762621035
plot_id,batch_id 0 88 miss% 0.04373287750117874
plot_id,batch_id 0 89 miss% 0.031014822606257833
plot_id,batch_id 0 90 miss% 0.04377509580384922
plot_id,batch_id 0 91 miss% 0.03209595659529662
plot_id,batch_id 0 92 miss% 0.029958082892610982
plot_id,batch_id 0 93 miss% 0.031977740022343853
plot_id,batch_id 0 94 miss% 0.026814306276562887
plot_id,batch_id 0 95 miss% 0.05381363873770412
plot_id,batch_id 0 96 miss% 0.05124541806319312
plot_id,batch_id 0 97 miss% 0.04512971556857824
plot_id,batch_id 0 98 miss% 0.0326719916741147
plot_id,batch_id 0 99 miss% 0.03311880282165245
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03202422 0.02248083 0.02300772 0.02911383 0.02973916 0.03875388
 0.02896037 0.03470586 0.02818089 0.02189984 0.03176765 0.03938963
 0.02571726 0.0408525  0.04719541 0.05544196 0.03234088 0.0407613
 0.04197194 0.03720438 0.05840288 0.02277891 0.03458597 0.02329786
 0.02590703 0.04063447 0.02454206 0.02326931 0.03010377 0.02805389
 0.04270341 0.0391276  0.02917305 0.03057155 0.02545965 0.04053535
 0.04697667 0.04024404 0.03001234 0.02070221 0.08275832 0.02777573
 0.01711926 0.03150632 0.02038559 0.01867505 0.01766183 0.02513699
 0.02667998 0.02057459 0.03598837 0.02065606 0.02596888 0.01705022
 0.0296687  0.03134751 0.03253142 0.03330469 0.03023466 0.02144827
 0.02735913 0.03771976 0.02627909 0.03249405 0.03557767 0.04232649
 0.02368155 0.03362734 0.03505283 0.0219486  0.06551408 0.06421744
 0.03560224 0.03198493 0.04653441 0.05943535 0.04503944 0.02658705
 0.04069553 0.04681031 0.05711677 0.02561083 0.0340386  0.03290981
 0.03566675 0.03677516 0.02759096 0.03062778 0.04373288 0.03101482
 0.0437751  0.03209596 0.02995808 0.03197774 0.02681431 0.05381364
 0.05124542 0.04512972 0.03267199 0.0331188 ]
for model  7 the mean error 0.03419236335174029
all id 7 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 21 out win 4 err 0.03419236335174029
Launcher: Job 8 completed in 6241 seconds.
Launcher: Task 36 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  46193
Epoch:0, Train loss:0.533217, valid loss:0.496812
Epoch:1, Train loss:0.354504, valid loss:0.360037
Epoch:2, Train loss:0.345076, valid loss:0.359187
Epoch:3, Train loss:0.343458, valid loss:0.358539
Epoch:4, Train loss:0.342586, valid loss:0.358129
Epoch:5, Train loss:0.341848, valid loss:0.357920
Epoch:6, Train loss:0.341569, valid loss:0.357792
Epoch:7, Train loss:0.341326, valid loss:0.358709
Epoch:8, Train loss:0.341108, valid loss:0.357968
Epoch:9, Train loss:0.340910, valid loss:0.358101
Epoch:10, Train loss:0.340803, valid loss:0.357823
Epoch:11, Train loss:0.339881, valid loss:0.357439
Epoch:12, Train loss:0.339856, valid loss:0.357350
Epoch:13, Train loss:0.339841, valid loss:0.357423
Epoch:14, Train loss:0.339849, valid loss:0.357505
Epoch:15, Train loss:0.339841, valid loss:0.357334
Epoch:16, Train loss:0.339798, valid loss:0.357328
Epoch:17, Train loss:0.339734, valid loss:0.357128
Epoch:18, Train loss:0.339666, valid loss:0.357328
Epoch:19, Train loss:0.339673, valid loss:0.357354
Epoch:20, Train loss:0.339727, valid loss:0.357368
Epoch:21, Train loss:0.339220, valid loss:0.356947
Epoch:22, Train loss:0.339221, valid loss:0.357056
Epoch:23, Train loss:0.339220, valid loss:0.357149
Epoch:24, Train loss:0.339196, valid loss:0.357015
Epoch:25, Train loss:0.339203, valid loss:0.356962
Epoch:26, Train loss:0.339222, valid loss:0.356934
Epoch:27, Train loss:0.339171, valid loss:0.357053
Epoch:28, Train loss:0.339147, valid loss:0.356949
Epoch:29, Train loss:0.339146, valid loss:0.357077
Epoch:30, Train loss:0.339132, valid loss:0.356921
Epoch:31, Train loss:0.338902, valid loss:0.356880
Epoch:32, Train loss:0.338905, valid loss:0.357068
Epoch:33, Train loss:0.338919, valid loss:0.356877
Epoch:34, Train loss:0.338920, valid loss:0.356840
Epoch:35, Train loss:0.338904, valid loss:0.356856
Epoch:36, Train loss:0.338889, valid loss:0.357004
Epoch:37, Train loss:0.338901, valid loss:0.356839
Epoch:38, Train loss:0.338867, valid loss:0.356898
Epoch:39, Train loss:0.338895, valid loss:0.356856
Epoch:40, Train loss:0.338876, valid loss:0.356867
Epoch:41, Train loss:0.338771, valid loss:0.356784
Epoch:42, Train loss:0.338761, valid loss:0.356828
Epoch:43, Train loss:0.338766, valid loss:0.356809
Epoch:44, Train loss:0.338758, valid loss:0.356797
Epoch:45, Train loss:0.338758, valid loss:0.356804
Epoch:46, Train loss:0.338753, valid loss:0.356826
Epoch:47, Train loss:0.338755, valid loss:0.356794
Epoch:48, Train loss:0.338751, valid loss:0.356791
Epoch:49, Train loss:0.338744, valid loss:0.356807
Epoch:50, Train loss:0.338747, valid loss:0.356777
Epoch:51, Train loss:0.338693, valid loss:0.356775
Epoch:52, Train loss:0.338688, valid loss:0.356758
Epoch:53, Train loss:0.338691, valid loss:0.356783
Epoch:54, Train loss:0.338684, valid loss:0.356785
Epoch:55, Train loss:0.338688, valid loss:0.356789
Epoch:56, Train loss:0.338684, valid loss:0.356759
Epoch:57, Train loss:0.338684, valid loss:0.356816
Epoch:58, Train loss:0.338682, valid loss:0.356772
Epoch:59, Train loss:0.338679, valid loss:0.356786
Epoch:60, Train loss:0.338682, valid loss:0.356747
training time 6072.067463874817
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7288172659968125
plot_id,batch_id 0 1 miss% 0.7920922310085075
plot_id,batch_id 0 2 miss% 0.8003902526128448
plot_id,batch_id 0 3 miss% 0.8071296584552138
plot_id,batch_id 0 4 miss% 0.8073808631353471
plot_id,batch_id 0 5 miss% 0.7265936246893936
plot_id,batch_id 0 6 miss% 0.7839642081967939
plot_id,batch_id 0 7 miss% 0.7973308516034708
plot_id,batch_id 0 8 miss% 0.8038471110317346
plot_id,batch_id 0 9 miss% 0.8100615395907373
plot_id,batch_id 0 10 miss% 0.7026486312903996
plot_id,batch_id 0 11 miss% 0.786558202854295
plot_id,batch_id 0 12 miss% 0.7955938903508393
plot_id,batch_id 0 13 miss% 0.802931110138123
plot_id,batch_id 0 14 miss% 0.8097872297859833
plot_id,batch_id 0 15 miss% 0.7179158763155976
plot_id,batch_id 0 16 miss% 0.7804089999260048
plot_id,batch_id 0 17 miss% 0.7970307003878062
plot_id,batch_id 0 18 miss% 0.8043753591485172
plot_id,batch_id 0 19 miss% 0.8046156513671336
plot_id,batch_id 0 20 miss% 0.776244283776395
plot_id,batch_id 0 21 miss% 0.8038819870155963
plot_id,batch_id 0 22 miss% 0.8087210331310865
plot_id,batch_id 0 23 miss% 0.8127283121072044
plot_id,batch_id 0 24 miss% 0.8125748153297182
plot_id,batch_id 0 25 miss% 0.7506534487954737
plot_id,batch_id 0 26 miss% 0.7962097245866353
plot_id,batch_id 0 27 miss% 0.804007282806885
plot_id,batch_id 0 28 miss% 0.8080930649135197
plot_id,batch_id 0 29 miss% 0.810036327330575
plot_id,batch_id 0 30 miss% 0.7579351957291397
plot_id,batch_id 0 31 miss% 0.7931389124438716
plot_id,batch_id 0 32 miss% 0.803039818395631
plot_id,batch_id 0 33 miss% 0.80752955390008
plot_id,batch_id 0 34 miss% 0.8097893254922659
plot_id,batch_id 0 35 miss% 0.7474168475066054
plot_id,batch_id 0 36 miss% 0.798633329092592
plot_id,batch_id 0 37 miss% 0.8018542933503819
plot_id,batch_id 0 38 miss% 0.8075351881098327
plot_id,batch_id 0 39 miss% 0.810854757281463
plot_id,batch_id 0 40 miss% 0.7982303967575466
plot_id,batch_id 0 41 miss% 0.806548669610047
plot_id,batch_id 0 42 miss% 0.8094329402085824
plot_id,batch_id 0 43 miss% 0.8141746592041884
plot_id,batch_id 0 44 miss% 0.8169902468300178
plot_id,batch_id 0 45 miss% 0.7751349563973199
plot_id,batch_id 0 46 miss% 0.8052761051073616
plot_id,batch_id 0 47 miss% 0.8120039335033646
plot_id,batch_id 0 48 miss% 0.8129348833730786
plot_id,batch_id 0 49 miss% 0.8166768525895794
plot_id,batch_id 0 50 miss% 0.7849984503995456
plot_id,batch_id 0 51 miss% 0.8043055906350519
plot_id,batch_id 0 52 miss% 0.8081908912593213
plot_id,batch_id 0 53 miss% 0.8131890545357131
plot_id,batch_id 0 54 miss% 0.8179848873995321
plot_id,batch_id 0 55 miss% 0.7741692428730763
plot_id,batch_id 0 56 miss% 0.8055539236961337
plot_id,batch_id 0 57 miss% 0.8096764179317643
plot_id,batch_id 0 58 miss% 0.8138601262023388
plot_id,batch_id 0 59 miss% 0.8176655293868217
plot_id,batch_id 0 60 miss% 0.6458007204430595
plot_id,batch_id 0 61 miss% 0.750777677127205
plot_id,batch_id 0 62 miss% 0.7841451551524703
plot_id,batch_id 0 63 miss% 0.7957080838346139
plot_id,batch_id 0 64 miss% 0.8045790342149699
plot_id,batch_id 0 65 miss% 0.6353715454086529
plot_id,batch_id 0 66 miss% 0.7526496737161339
plot_id,batch_id 0 67 miss% 0.776281399663573
plot_id,batch_id 0 68 miss% 0.7973850116535606
plot_id,batch_id 0 69 miss% 0.7971282196877431
plot_id,batch_id 0 70 miss% 0.6130832885076936
plot_id,batch_id 0 71 miss% 0.7637539142472615
plot_id,batch_id 0 72 miss% 0.765994472415988
plot_id,batch_id 0 73 miss% 0.7817850326061929
plot_id,batch_id 0 74 miss% 0.7880323811659292
plot_id,batch_id 0 75 miss% 0.6011290452758844
plot_id,batch_id 0 76 miss% 0.7148119682223956
plot_id,batch_id 0 77 miss% 0.7574446393631883
plot_id,batch_id 0 78 miss% 0.7797320035478726
plot_id,batch_id 0 79 miss% 0.7901697218642868
plot_id,batch_id 0 80 miss% 0.6722896116437137
plot_id,batch_id 0 81 miss% 0.7785259355058081
plot_id,batch_id 0 82 miss% 0.7919566000194992
plot_id,batch_id 0 83 miss% 0.800946336853194
plot_id,batch_id 0 84 miss% 0.7999690201858809
plot_id,batch_id 0 85 miss% 0.6685047240152681
plot_id,batch_id 0 86 miss% 0.7684410406435489
plot_id,batch_id 0 87 miss% 0.7913578899993723
plot_id,batch_id 0 88 miss% 0.7962164773568401
plot_id,batch_id 0 89 miss% 0.7989113098309598
plot_id,batch_id 0 90 miss% 0.6479917122508155
plot_id,batch_id 0 91 miss% 0.7651567127657329
plot_id,batch_id 0 92 miss% 0.7784730113660475
plot_id,batch_id 0 93 miss% 0.7907745077738644
plot_id,batch_id 0 94 miss% 0.7994751734846477
plot_id,batch_id 0 95 miss% 0.6450291682794053
plot_id,batch_id 0 96 miss% 0.7482191656886612
plot_id,batch_id 0 97 miss% 0.7775814715222739
plot_id,batch_id 0 98 miss% 0.7851247646800362
plot_id,batch_id 0 99 miss% 0.7933557075041148
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.72881727 0.79209223 0.80039025 0.80712966 0.80738086 0.72659362
 0.78396421 0.79733085 0.80384711 0.81006154 0.70264863 0.7865582
 0.79559389 0.80293111 0.80978723 0.71791588 0.780409   0.7970307
 0.80437536 0.80461565 0.77624428 0.80388199 0.80872103 0.81272831
 0.81257482 0.75065345 0.79620972 0.80400728 0.80809306 0.81003633
 0.7579352  0.79313891 0.80303982 0.80752955 0.80978933 0.74741685
 0.79863333 0.80185429 0.80753519 0.81085476 0.7982304  0.80654867
 0.80943294 0.81417466 0.81699025 0.77513496 0.80527611 0.81200393
 0.81293488 0.81667685 0.78499845 0.80430559 0.80819089 0.81318905
 0.81798489 0.77416924 0.80555392 0.80967642 0.81386013 0.81766553
 0.64580072 0.75077768 0.78414516 0.79570808 0.80457903 0.63537155
 0.75264967 0.7762814  0.79738501 0.79712822 0.61308329 0.76375391
 0.76599447 0.78178503 0.78803238 0.60112905 0.71481197 0.75744464
 0.779732   0.79016972 0.67228961 0.77852594 0.7919566  0.80094634
 0.79996902 0.66850472 0.76844104 0.79135789 0.79621648 0.79891131
 0.64799171 0.76515671 0.77847301 0.79077451 0.79947517 0.64502917
 0.74821917 0.77758147 0.78512476 0.79335571]
for model  112 the mean error 0.7779141184636926
all id 112 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 25 out win 4 err 0.7779141184636926
Launcher: Job 113 completed in 6250 seconds.
Launcher: Task 128 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  61841
Epoch:0, Train loss:0.598040, valid loss:0.600524
Epoch:1, Train loss:0.203241, valid loss:0.005628
Epoch:2, Train loss:0.007955, valid loss:0.003834
Epoch:3, Train loss:0.005529, valid loss:0.002554
Epoch:4, Train loss:0.004386, valid loss:0.002635
Epoch:5, Train loss:0.003743, valid loss:0.001932
Epoch:6, Train loss:0.003359, valid loss:0.001992
Epoch:7, Train loss:0.003154, valid loss:0.002145
Epoch:8, Train loss:0.002936, valid loss:0.001459
Epoch:9, Train loss:0.002726, valid loss:0.001798
Epoch:10, Train loss:0.002633, valid loss:0.001206
Epoch:11, Train loss:0.001908, valid loss:0.001168
Epoch:12, Train loss:0.001923, valid loss:0.001239
Epoch:13, Train loss:0.001844, valid loss:0.001057
Epoch:14, Train loss:0.001869, valid loss:0.000980
Epoch:15, Train loss:0.001817, valid loss:0.001279
Epoch:16, Train loss:0.001745, valid loss:0.001009
Epoch:17, Train loss:0.001756, valid loss:0.001092
Epoch:18, Train loss:0.001712, valid loss:0.001022
Epoch:19, Train loss:0.001732, valid loss:0.001185
Epoch:20, Train loss:0.001662, valid loss:0.001052
Epoch:21, Train loss:0.001305, valid loss:0.000798
Epoch:22, Train loss:0.001321, valid loss:0.000824
Epoch:23, Train loss:0.001293, valid loss:0.000793
Epoch:24, Train loss:0.001276, valid loss:0.000995
Epoch:25, Train loss:0.001276, valid loss:0.000750
Epoch:26, Train loss:0.001255, valid loss:0.000834
Epoch:27, Train loss:0.001249, valid loss:0.000795
Epoch:28, Train loss:0.001248, valid loss:0.000773
Epoch:29, Train loss:0.001230, valid loss:0.000740
Epoch:30, Train loss:0.001204, valid loss:0.000798
Epoch:31, Train loss:0.001063, valid loss:0.000729
Epoch:32, Train loss:0.001046, valid loss:0.000713
Epoch:33, Train loss:0.001041, valid loss:0.000700
Epoch:34, Train loss:0.001047, valid loss:0.000703
Epoch:35, Train loss:0.001039, valid loss:0.000693
Epoch:36, Train loss:0.001021, valid loss:0.000740
Epoch:37, Train loss:0.001027, valid loss:0.000754
Epoch:38, Train loss:0.001037, valid loss:0.000719
Epoch:39, Train loss:0.001005, valid loss:0.000746
Epoch:40, Train loss:0.001008, valid loss:0.000668
Epoch:41, Train loss:0.000937, valid loss:0.000667
Epoch:42, Train loss:0.000927, valid loss:0.000719
Epoch:43, Train loss:0.000930, valid loss:0.000671
Epoch:44, Train loss:0.000927, valid loss:0.000670
Epoch:45, Train loss:0.000921, valid loss:0.000660
Epoch:46, Train loss:0.000918, valid loss:0.000689
Epoch:47, Train loss:0.000920, valid loss:0.000825
Epoch:48, Train loss:0.000920, valid loss:0.000658
Epoch:49, Train loss:0.000907, valid loss:0.000647
Epoch:50, Train loss:0.000908, valid loss:0.000685
Epoch:51, Train loss:0.000873, valid loss:0.000634
Epoch:52, Train loss:0.000872, valid loss:0.000643
Epoch:53, Train loss:0.000873, valid loss:0.000671
Epoch:54, Train loss:0.000869, valid loss:0.000647
Epoch:55, Train loss:0.000869, valid loss:0.000635
Epoch:56, Train loss:0.000866, valid loss:0.000641
Epoch:57, Train loss:0.000864, valid loss:0.000648
Epoch:58, Train loss:0.000863, valid loss:0.000682
Epoch:59, Train loss:0.000862, valid loss:0.000665
Epoch:60, Train loss:0.000862, valid loss:0.000630
training time 6105.165135383606
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.014727190021732072
plot_id,batch_id 0 1 miss% 0.02592117338932141
plot_id,batch_id 0 2 miss% 0.025679602200067955
plot_id,batch_id 0 3 miss% 0.025961050907641246
plot_id,batch_id 0 4 miss% 0.029465583390042543
plot_id,batch_id 0 5 miss% 0.024431624859430866
plot_id,batch_id 0 6 miss% 0.030241117060774755
plot_id,batch_id 0 7 miss% 0.026304072825829656
plot_id,batch_id 0 8 miss% 0.01773107867550021
plot_id,batch_id 0 9 miss% 0.016518352079154974
plot_id,batch_id 0 10 miss% 0.03435191331526372
plot_id,batch_id 0 11 miss% 0.04773178572169902
plot_id,batch_id 0 12 miss% 0.027878952734749114
plot_id,batch_id 0 13 miss% 0.02353450731430538
plot_id,batch_id 0 14 miss% 0.028062263461366945
plot_id,batch_id 0 15 miss% 0.030307095831508295
plot_id,batch_id 0 16 miss% 0.02513114279571678
plot_id,batch_id 0 17 miss% 0.03455250432224974
plot_id,batch_id 0 18 miss% 0.02398691962775657
plot_id,batch_id 0 19 miss% 0.031831237785786226
plot_id,batch_id 0 20 miss% 0.039840016182609454
plot_id,batch_id 0 21 miss% 0.03238122035970474
plot_id,batch_id 0 22 miss% 0.02982848914114897
plot_id,batch_id 0 23 miss% 0.02528233192730862
plot_id,batch_id 0 24 miss% 0.0251860836653775
plot_id,batch_id 0 25 miss% 0.041463935871945634
plot_id,batch_id 0 26 miss% 0.03546823846961313
plot_id,batch_id 0 27 miss% 0.027425514890494867
plot_id,batch_id 0 28 miss% 0.027441089963394342
plot_id,batch_id 0 29 miss% 0.01755403736974357
plot_id,batch_id 0 30 miss% 0.042499878908099246
plot_id,batch_id 0 31 miss% 0.02556799767074855
plot_id,batch_id 0 32 miss% 0.021212022341636404
plot_id,batch_id 0 33 miss% 0.035456447995785574
plot_id,batch_id 0 34 miss% 0.026055750875978095
plot_id,batch_id 0 35 miss% 0.030283360389221078
plot_id,batch_id 0 36 miss% 0.030355364448767082
plot_id,batch_id 0 37 miss% 0.03607479818306403
plot_id,batch_id 0 38 miss% 0.023730173836648234
plot_id,batch_id 0 39 miss% 0.015943450092978352
plot_id,batch_id 0 40 miss% 0.09900104071084076
plot_id,batch_id 0 41 miss% 0.019119072903398232
plot_id,batch_id 0 42 miss% 0.011646230801211794
plot_id,batch_id 0 43 miss% 0.019361819736343728
plot_id,batch_id 0 44 miss% 0.02065996816760145
plot_id,batch_id 0 45 miss% 0.032684027390469834
plot_id,batch_id 0 46 miss% 0.028505677186472966
plot_id,batch_id 0 47 miss% 0.020555912122265773
plot_id,batch_id 0 48 miss% 0.01565584613230647
plot_id,batch_id 0 49 miss% 0.021267412613703775
plot_id,batch_id 0 50 miss% 0.02774596859890792
plot_id,batch_id 0 51 miss% 0.030604952525252475
plot_id,batch_id 0 52 miss% 0.024451127037132335
plot_id,batch_id 0 53 miss% 0.012707542630357739
plot_id,batch_id 0 54 miss% 0.02919587880836653
plot_id,batch_id 0 55 miss% 0.03304786612790598
plot_id,batch_id 0 56 miss% 0.029638338897080004
plot_id,batch_id 0 57 miss% 0.01917301809119029
plot_id,batch_id 0 58 miss% 0.02092302819899625
plot_id,batch_id 0 59 miss% 0.022023264860529187
plot_id,batch_id 0 60 miss% 0.03098573685940782
plot_id,batch_id 0 61 miss% 0.04483829881034221
plot_id,batch_id 0 62 miss% 0.03149268949422866
plot_id,batch_id 0 63 miss% 0.026672911488507842
plot_id,batch_id 0 64 miss% 0.025658476248347686
plot_id,batch_id 0 65 miss% 0.03196058782915466
plot_id,batch_id 0 66 miss% 0.03875479769436942
plot_id,batch_id 0 67 miss% 0.027141373673638655
plot_id,batch_id 0 68 miss% 0.03389634256308341
plot_id,batch_id 0 69 miss% 0.02380355220532263
plot_id,batch_id 0 70 miss% 0.03496052662950179
plot_id,batch_id 0 71 miss% 0.06132718159421378
plot_id,batch_id 0 72 miss% 0.030302595294926276
plot_id,batch_id 0 73 miss% 0.03197357341808017
plot_id,batch_id 0 74 miss% 0.026651307769049788
plot_id,batch_id 0 75 miss% 0.028594644144615405
plot_id,batch_id 0 76 miss% 0.03345744556851196
plot_id,batch_id 0 77 miss% 0.03116075895291769
plot_id,batch_id 0 78 miss% 0.039364028721123774
plot_id,batch_id 0 79 miss% 0.03450389760084922
plot_id,batch_id 0 80 miss% 0.035968662727414016
plot_id,batch_id 0 81 miss% 0.026225834458087682
plot_id,batch_id 0 82 miss% 0.02656638660870404
plot_id,batch_id 0 83 miss% 0.024175726994072798
plot_id,batch_id 0 84 miss% 0.015980059974632897
plot_id,batch_id 0 85 miss% 0.054165709409565346
plot_id,batch_id 0 86 miss% 0.023563065114335535
plot_id,batch_id 0 87 miss% 0.030760256387523565
plot_id,batch_id 0 88 miss% 0.03342260972855128
plot_id,batch_id 0 89 miss% 0.02674586791426197
plot_id,batch_id 0 90 miss% 0.035778831669171854
plot_id,batch_id 0 91 miss% 0.0322961359343196
plot_id,batch_id 0 92 miss% 0.030036275613285035
plot_id,batch_id 0 93 miss% 0.04767984909878448
plot_id,batch_id 0 94 miss% 0.022990726872673367
plot_id,batch_id 0 95 miss% 0.04185113400425289
plot_id,batch_id 0 96 miss% 0.03847844997038168
plot_id,batch_id 0 97 miss% 0.05200614623504177
plot_id,batch_id 0 98 miss% 0.023941843035341224
plot_id,batch_id 0 99 miss% 0.03375307760778285
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01472719 0.02592117 0.0256796  0.02596105 0.02946558 0.02443162
 0.03024112 0.02630407 0.01773108 0.01651835 0.03435191 0.04773179
 0.02787895 0.02353451 0.02806226 0.0303071  0.02513114 0.0345525
 0.02398692 0.03183124 0.03984002 0.03238122 0.02982849 0.02528233
 0.02518608 0.04146394 0.03546824 0.02742551 0.02744109 0.01755404
 0.04249988 0.025568   0.02121202 0.03545645 0.02605575 0.03028336
 0.03035536 0.0360748  0.02373017 0.01594345 0.09900104 0.01911907
 0.01164623 0.01936182 0.02065997 0.03268403 0.02850568 0.02055591
 0.01565585 0.02126741 0.02774597 0.03060495 0.02445113 0.01270754
 0.02919588 0.03304787 0.02963834 0.01917302 0.02092303 0.02202326
 0.03098574 0.0448383  0.03149269 0.02667291 0.02565848 0.03196059
 0.0387548  0.02714137 0.03389634 0.02380355 0.03496053 0.06132718
 0.0303026  0.03197357 0.02665131 0.02859464 0.03345745 0.03116076
 0.03936403 0.0345039  0.03596866 0.02622583 0.02656639 0.02417573
 0.01598006 0.05416571 0.02356307 0.03076026 0.03342261 0.02674587
 0.03577883 0.03229614 0.03003628 0.04767985 0.02299073 0.04185113
 0.03847845 0.05200615 0.02394184 0.03375308]
for model  93 the mean error 0.02995254740362875
all id 93 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 25 out win 3 err 0.02995254740362875
Launcher: Job 94 completed in 6300 seconds.
Launcher: Task 53 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  28945
Epoch:0, Train loss:0.621320, valid loss:0.614854
Epoch:1, Train loss:0.069791, valid loss:0.023131
Epoch:2, Train loss:0.026084, valid loss:0.008808
Epoch:3, Train loss:0.014320, valid loss:0.006235
Epoch:4, Train loss:0.012347, valid loss:0.006414
Epoch:5, Train loss:0.010688, valid loss:0.005167
Epoch:6, Train loss:0.008851, valid loss:0.005215
Epoch:7, Train loss:0.008039, valid loss:0.004166
Epoch:8, Train loss:0.007393, valid loss:0.003142
Epoch:9, Train loss:0.006646, valid loss:0.003436
Epoch:10, Train loss:0.006351, valid loss:0.003952
Epoch:11, Train loss:0.004896, valid loss:0.002796
Epoch:12, Train loss:0.004851, valid loss:0.002691
Epoch:13, Train loss:0.004734, valid loss:0.002501
Epoch:14, Train loss:0.004654, valid loss:0.002937
Epoch:15, Train loss:0.004445, valid loss:0.002725
Epoch:16, Train loss:0.004333, valid loss:0.002343
Epoch:17, Train loss:0.004248, valid loss:0.002437
Epoch:18, Train loss:0.004192, valid loss:0.002269
Epoch:19, Train loss:0.004055, valid loss:0.002167
Epoch:20, Train loss:0.003940, valid loss:0.002369
Epoch:21, Train loss:0.003319, valid loss:0.002063
Epoch:22, Train loss:0.003264, valid loss:0.001888
Epoch:23, Train loss:0.003332, valid loss:0.001796
Epoch:24, Train loss:0.003206, valid loss:0.002031
Epoch:25, Train loss:0.003209, valid loss:0.001807
Epoch:26, Train loss:0.003142, valid loss:0.002063
Epoch:27, Train loss:0.003130, valid loss:0.001853
Epoch:28, Train loss:0.003156, valid loss:0.001913
Epoch:29, Train loss:0.003125, valid loss:0.001854
Epoch:30, Train loss:0.003051, valid loss:0.001877
Epoch:31, Train loss:0.002745, valid loss:0.001676
Epoch:32, Train loss:0.002714, valid loss:0.001730
Epoch:33, Train loss:0.002691, valid loss:0.001686
Epoch:34, Train loss:0.002702, valid loss:0.001734
Epoch:35, Train loss:0.002708, valid loss:0.001721
Epoch:36, Train loss:0.002673, valid loss:0.001659
Epoch:37, Train loss:0.002666, valid loss:0.001581
Epoch:38, Train loss:0.002666, valid loss:0.001632
Epoch:39, Train loss:0.002643, valid loss:0.001614
Epoch:40, Train loss:0.002604, valid loss:0.001665
Epoch:41, Train loss:0.002472, valid loss:0.001625
Epoch:42, Train loss:0.002471, valid loss:0.001619
Epoch:43, Train loss:0.002472, valid loss:0.001583
Epoch:44, Train loss:0.002442, valid loss:0.001617
Epoch:45, Train loss:0.002448, valid loss:0.001547
Epoch:46, Train loss:0.002439, valid loss:0.001629
Epoch:47, Train loss:0.002441, valid loss:0.001564
Epoch:48, Train loss:0.002416, valid loss:0.001582
Epoch:49, Train loss:0.002424, valid loss:0.001624
Epoch:50, Train loss:0.002401, valid loss:0.001608
Epoch:51, Train loss:0.002345, valid loss:0.001554
Epoch:52, Train loss:0.002331, valid loss:0.001535
Epoch:53, Train loss:0.002335, valid loss:0.001535
Epoch:54, Train loss:0.002329, valid loss:0.001539
Epoch:55, Train loss:0.002318, valid loss:0.001540
Epoch:56, Train loss:0.002328, valid loss:0.001559
Epoch:57, Train loss:0.002313, valid loss:0.001561
Epoch:58, Train loss:0.002321, valid loss:0.001580
Epoch:59, Train loss:0.002308, valid loss:0.001542
Epoch:60, Train loss:0.002305, valid loss:0.001563
training time 6137.793471097946
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.04000075386136502
plot_id,batch_id 0 1 miss% 0.03886586680665685
plot_id,batch_id 0 2 miss% 0.034683666962185944
plot_id,batch_id 0 3 miss% 0.030622566200480622
plot_id,batch_id 0 4 miss% 0.02732999426199939
plot_id,batch_id 0 5 miss% 0.04799541186047634
plot_id,batch_id 0 6 miss% 0.03414371118587633
plot_id,batch_id 0 7 miss% 0.04666317917760646
plot_id,batch_id 0 8 miss% 0.029421691188083163
plot_id,batch_id 0 9 miss% 0.022616111064904434
plot_id,batch_id 0 10 miss% 0.048897333515603915
plot_id,batch_id 0 11 miss% 0.05634768320397883
plot_id,batch_id 0 12 miss% 0.03237718071074388
plot_id,batch_id 0 13 miss% 0.05243559722632503
plot_id,batch_id 0 14 miss% 0.04234792197263723
plot_id,batch_id 0 15 miss% 0.06096057089157538
plot_id,batch_id 0 16 miss% 0.03085629902083721
plot_id,batch_id 0 17 miss% 0.036821708806332024
plot_id,batch_id 0 18 miss% 0.0357498726070079
plot_id,batch_id 0 19 miss% 0.049891475490032894
plot_id,batch_id 0 20 miss% 0.062457205143260114
plot_id,batch_id 0 21 miss% 0.03636196012450575
plot_id,batch_id 0 22 miss% 0.01825050961285524
plot_id,batch_id 0 23 miss% 0.01870676684602134
plot_id,batch_id 0 24 miss% 0.020910513454743627
plot_id,batch_id 0 25 miss% 0.044598184975012896
plot_id,batch_id 0 26 miss% 0.04001470690889676
plot_id,batch_id 0 27 miss% 0.019040727725153386
plot_id,batch_id 0 28 miss% 0.02088692341641746
plot_id,batch_id 0 29 miss% 0.019441714818840595
plot_id,batch_id 0 30 miss% 0.05001685765472176
plot_id,batch_id 0 31 miss% 0.03789345249417519
plot_id,batch_id 0 32 miss% 0.048186474868975704
plot_id,batch_id 0 33 miss% 0.031790540572116624
plot_id,batch_id 0 34 miss% 0.02565844148906435
plot_id,batch_id 0 35 miss% 0.05617427805766151
plot_id,batch_id 0 36 miss% 0.09319413153619113
plot_id,batch_id 0 37 miss% 0.04531099651164173
plot_id,batch_id 0 38 miss% 0.03117320632104271
plot_id,batch_id 0 39 miss% 0.02107343844389029
plot_id,batch_id 0 40 miss% 0.074339122931599
plot_id,batch_id 0 41 miss% 0.030104004869435496
plot_id,batch_id 0 42 miss% 0.0247544716417378
plot_id,batch_id 0 43 miss% 0.02463755002637004
plot_id,batch_id 0 44 miss% 0.02034581563511717
plot_id,batch_id 0 45 miss% 0.025565598085093743
plot_id,batch_id 0 46 miss% 0.03071548552099454
plot_id,batch_id 0 47 miss% 0.02661646951776725
plot_id,batch_id 0 48 miss% 0.01923168032177419
plot_id,batch_id 0 49 miss% 0.015705150359227166
plot_id,batch_id 0 50 miss% 0.044789332237312865
plot_id,batch_id 0 51 miss% 0.03870796767964425
plot_id,batch_id 0 52 miss% 0.03038876729442498
plot_id,batch_id 0 53 miss% 0.019146293423311325
plot_id,batch_id 0 54 miss% 0.028119896822885265
plot_id,batch_id 0 55 miss% 0.03756449430214136
plot_id,batch_id 0 56 miss% 0.028315840580170677
plot_id,batch_id 0 57 miss% 0.027108243473563786
plot_id,batch_id 0 58 miss% 0.019234601832994653
plot_id,batch_id 0 59 miss% 0.03028917958174483
plot_id,batch_id 0 60 miss% 0.045250499176092425
plot_id,batch_id 0 61 miss% 0.03067057345087924
plot_id,batch_id 0 62 miss% 0.027677325997758764
plot_id,batch_id 0 63 miss% 0.03132816496086004
plot_id,batch_id 0 64 miss% 0.03740189439610626
plot_id,batch_id 0 65 miss% 0.05778150017519258
plot_id,batch_id 0 66 miss% 0.0788392106585368
plot_id,batch_id 0 67 miss% 0.032900031417362086
plot_id,batch_id 0 68 miss% 0.027864475798235748
plot_id,batch_id 0 69 miss% 0.03708744634118544
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  61841
Epoch:0, Train loss:0.734007, valid loss:0.732855
Epoch:1, Train loss:0.193352, valid loss:0.010631
Epoch:2, Train loss:0.016156, valid loss:0.008175
Epoch:3, Train loss:0.012106, valid loss:0.005338
Epoch:4, Train loss:0.010298, valid loss:0.004812
Epoch:5, Train loss:0.008894, valid loss:0.005105
Epoch:6, Train loss:0.007790, valid loss:0.005586
Epoch:7, Train loss:0.006991, valid loss:0.003661
Epoch:8, Train loss:0.006382, valid loss:0.003602
Epoch:9, Train loss:0.006047, valid loss:0.003235
Epoch:10, Train loss:0.005453, valid loss:0.002807
Epoch:11, Train loss:0.003798, valid loss:0.002196
Epoch:12, Train loss:0.003824, valid loss:0.002305
Epoch:13, Train loss:0.003661, valid loss:0.001970
Epoch:14, Train loss:0.003683, valid loss:0.002116
Epoch:15, Train loss:0.003485, valid loss:0.001877
Epoch:16, Train loss:0.003521, valid loss:0.001902
Epoch:17, Train loss:0.003347, valid loss:0.001996
Epoch:18, Train loss:0.003316, valid loss:0.001558
Epoch:19, Train loss:0.003215, valid loss:0.001996
Epoch:20, Train loss:0.003320, valid loss:0.001651
Epoch:21, Train loss:0.002360, valid loss:0.001421
Epoch:22, Train loss:0.002398, valid loss:0.001427
Epoch:23, Train loss:0.002418, valid loss:0.001383
Epoch:24, Train loss:0.002347, valid loss:0.001488
Epoch:25, Train loss:0.002338, valid loss:0.001401
Epoch:26, Train loss:0.002258, valid loss:0.001442
Epoch:27, Train loss:0.002257, valid loss:0.001768
Epoch:28, Train loss:0.002210, valid loss:0.001431
Epoch:29, Train loss:0.002254, valid loss:0.001365
Epoch:30, Train loss:0.002136, valid loss:0.001365
Epoch:31, Train loss:0.001757, valid loss:0.001416
Epoch:32, Train loss:0.001793, valid loss:0.001324
Epoch:33, Train loss:0.001763, valid loss:0.001192
Epoch:34, Train loss:0.001768, valid loss:0.001242
Epoch:35, Train loss:0.001753, valid loss:0.001264
Epoch:36, Train loss:0.001768, valid loss:0.001211
Epoch:37, Train loss:0.001738, valid loss:0.001247
Epoch:38, Train loss:0.001701, valid loss:0.001353
Epoch:39, Train loss:0.001700, valid loss:0.001197
Epoch:40, Train loss:0.001667, valid loss:0.001199
Epoch:41, Train loss:0.001483, valid loss:0.001125
Epoch:42, Train loss:0.001494, valid loss:0.001108
Epoch:43, Train loss:0.001535, valid loss:0.001123
Epoch:44, Train loss:0.001457, valid loss:0.001104
Epoch:45, Train loss:0.001442, valid loss:0.001148
Epoch:46, Train loss:0.001467, valid loss:0.001149
Epoch:47, Train loss:0.001463, valid loss:0.001073
Epoch:48, Train loss:0.001445, valid loss:0.001223
Epoch:49, Train loss:0.001444, valid loss:0.001142
Epoch:50, Train loss:0.001422, valid loss:0.001147
Epoch:51, Train loss:0.001347, valid loss:0.001086
Epoch:52, Train loss:0.001336, valid loss:0.001115
Epoch:53, Train loss:0.001343, valid loss:0.001131
Epoch:54, Train loss:0.001339, valid loss:0.001085
Epoch:55, Train loss:0.001330, valid loss:0.001058
Epoch:56, Train loss:0.001321, valid loss:0.001064
Epoch:57, Train loss:0.001318, valid loss:0.001054
Epoch:58, Train loss:0.001323, valid loss:0.001105
Epoch:59, Train loss:0.001361, valid loss:0.001067
Epoch:60, Train loss:0.001312, valid loss:0.001075
training time 6152.641214132309
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.02194140366452994
plot_id,batch_id 0 1 miss% 0.02112734852590242
plot_id,batch_id 0 2 miss% 0.031249283269755474
plot_id,batch_id 0 3 miss% 0.03036659353969211
plot_id,batch_id 0 4 miss% 0.03344442855616221
plot_id,batch_id 0 5 miss% 0.03538027507756394
plot_id,batch_id 0 6 miss% 0.02623516576817641
plot_id,batch_id 0 7 miss% 0.03152615578282134
plot_id,batch_id 0 8 miss% 0.02558241958419266
plot_id,batch_id 0 9 miss% 0.02789561544544257
plot_id,batch_id 0 10 miss% 0.026531248687941753
plot_id,batch_id 0 11 miss% 0.04263350288397085
plot_id,batch_id 0 12 miss% 0.038205577631847516
plot_id,batch_id 0 13 miss% 0.03403084166317142
plot_id,batch_id 0 14 miss% 0.03281310423310517
plot_id,batch_id 0 15 miss% 0.051005424197760434
plot_id,batch_id 0 16 miss% 0.027739956135831294
plot_id,batch_id 0 17 miss% 0.03722751793701599
plot_id,batch_id 0 18 miss% 0.02669670858127262
plot_id,batch_id 0 19 miss% 0.032680378227867325
plot_id,batch_id 0 20 miss% 0.045081111896688175
plot_id,batch_id 0 21 miss% 0.02429898203275241
plot_id,batch_id 0 22 miss% 0.027431055332745487
plot_id,batch_id 0 23 miss% 0.027920519838592672
plot_id,batch_id 0 24 miss% 0.026483787821211392
plot_id,batch_id 0 25 miss% 0.03946362970545768
plot_id,batch_id 0 26 miss% 0.031729403254854474
plot_id,batch_id 0 27 miss% 0.025171464119444483
plot_id,batch_id 0 28 miss% 0.017923772857254622
plot_id,batch_id 0 29 miss% 0.03338841376415385
plot_id,batch_id 0 30 miss% 0.048551784490213974
plot_id,batch_id 0 31 miss% 0.03185757504352792
plot_id,batch_id 0 32 miss% 0.027495432096841067
plot_id,batch_id 0 33 miss% 0.029848242198088303
plot_id,batch_id 0 34 miss% 0.02487813445550305
plot_id,batch_id 0 35 miss% 0.05533856291543881
plot_id,batch_id 0 36 miss% 0.03810944016356103
plot_id,batch_id 0 37 miss% 0.03275894898431816
plot_id,batch_id 0 38 miss% 0.024723561833581355
plot_id,batch_id 0 39 miss% 0.029678828608021752
plot_id,batch_id 0 40 miss% 0.066839383043805
plot_id,batch_id 0 41 miss% 0.01945344112766332
plot_id,batch_id 0 42 miss% 0.015719197994185714
plot_id,batch_id 0 43 miss% 0.0287545045476591
plot_id,batch_id 0 44 miss% 0.01879095695953737
plot_id,batch_id 0 45 miss% 0.023580547999932377
plot_id,batch_id 0 46 miss% 0.02720895749641083
plot_id,batch_id 0 47 miss% 0.022146456651395495
plot_id,batch_id 0 48 miss% 0.02307254516689205
plot_id,batch_id 0 49 miss% 0.022424455935075683
plot_id,batch_id 0 50 miss% 0.028808086329897228
plot_id,batch_id 0 51 miss% 0.01642163123439263
plot_id,batch_id 0 52 miss% 0.03417775765115369
plot_id,batch_id 0 53 miss% 0.01578194302302751
plot_id,batch_id 0 54 miss% 0.020836024410837005
plot_id,batch_id 0 55 miss% 0.032671431491159766
plot_id,batch_id 0 56 miss% 0.025990869889215916
plot_id,batch_id 0 57 miss% 0.03511545519088478
plot_id,batch_id 0 58 miss% 0.04128085451508584
plot_id,batch_id 0 59 miss% 0.019137955546946275
plot_id,batch_id 0 60 miss% 0.0368762848750868
plot_id,batch_id 0 61 miss% 0.021312937025361463
plot_id,batch_id 0 62 miss% 0.03737486378284392
plot_id,batch_id 0 63 miss% 0.03465363036416485
plot_id,batch_id 0 64 miss% 0.04110236577947294
plot_id,batch_id 0 65 miss% 0.04804161000620871
plot_id,batch_id 0 66 miss% 0.03251746549183572
plot_id,batch_id 0 67 miss% 0.022189015508603936
plot_id,batch_id 0 68 miss% 0.028605322097503515
plot_id,batch_id 0 69 miss% 0.02224048574722886
plot_id,batch_id 0 70 miss% 0.0354326807193863
plot_id,batch_id 0 71 miss% 0.03332742659615965
plot_id,batch_id 0 72 miss% 0.03882643400816745
plot_id,batch_id 0 73 miss% 0.04202847844907447
plot_id,batch_id 0 74 miss% 0.04679334925049181
plot_id,batch_id 0 75 miss% 0.04506859897024349
plot_id,batch_id 0 76 miss% 0.04319035863529007
plot_id,batch_id 0 77 miss% 0.03491533806307438
plot_id,batch_id 0 78 miss% 0.06065725708798151
plot_id,batch_id 0 79 miss% 0.06099047121154137
plot_id,batch_id 0 80 miss% 0.05154614874961305
plot_id,batch_id 0 81 miss% 0.028015824987412053
plot_id,batch_id 0 82 miss% 0.030690253752443455
plot_id,batch_id 0 83 miss% 0.06671405374663407
plot_id,batch_id 0 84 miss% 0.03853992750528677
plot_id,batch_id 0 85 miss% 0.059799020273960224
plot_id,batch_id 0 86 miss% 0.055791786022106715
plot_id,batch_id 0 87 miss% 0.04284298440950857
plot_id,batch_id 0 88 miss% 0.06464396149280272
plot_id,batch_id 0 89 miss% 0.03140975307908261
plot_id,batch_id 0 90 miss% 0.07336629980362101
plot_id,batch_id 0 91 miss% 0.03812948812196842
plot_id,batch_id 0 92 miss% 0.04919573202864577
plot_id,batch_id 0 93 miss% 0.0410679999705843
plot_id,batch_id 0 94 miss% 0.04053355329101332
plot_id,batch_id 0 95 miss% 0.05530227597360432
plot_id,batch_id 0 96 miss% 0.028065556495637224
plot_id,batch_id 0 97 miss% 0.07056812694816232
plot_id,batch_id 0 98 miss% 0.039942734026178324
plot_id,batch_id 0 99 miss% 0.05873640820736329
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04000075 0.03886587 0.03468367 0.03062257 0.02732999 0.04799541
 0.03414371 0.04666318 0.02942169 0.02261611 0.04889733 0.05634768
 0.03237718 0.0524356  0.04234792 0.06096057 0.0308563  0.03682171
 0.03574987 0.04989148 0.06245721 0.03636196 0.01825051 0.01870677
 0.02091051 0.04459818 0.04001471 0.01904073 0.02088692 0.01944171
 0.05001686 0.03789345 0.04818647 0.03179054 0.02565844 0.05617428
 0.09319413 0.045311   0.03117321 0.02107344 0.07433912 0.030104
 0.02475447 0.02463755 0.02034582 0.0255656  0.03071549 0.02661647
 0.01923168 0.01570515 0.04478933 0.03870797 0.03038877 0.01914629
 0.0281199  0.03756449 0.02831584 0.02710824 0.0192346  0.03028918
 0.0452505  0.03067057 0.02767733 0.03132816 0.03740189 0.0577815
 0.07883921 0.03290003 0.02786448 0.03708745 0.03543268 0.03332743
 0.03882643 0.04202848 0.04679335 0.0450686  0.04319036 0.03491534
 0.06065726 0.06099047 0.05154615 0.02801582 0.03069025 0.06671405
 0.03853993 0.05979902 0.05579179 0.04284298 0.06464396 0.03140975
 0.0733663  0.03812949 0.04919573 0.041068   0.04053355 0.05530228
 0.02806556 0.07056813 0.03994273 0.05873641]
for model  11 the mean error 0.039487829973764564
all id 11 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 21 out win 5 err 0.039487829973764564
Launcher: Job 12 completed in 6356 seconds.
Launcher: Task 157 done. Exiting.
plot_id,batch_id 0 70 miss% 0.03905329681533927
plot_id,batch_id 0 71 miss% 0.03841871175147941
plot_id,batch_id 0 72 miss% 0.043295616050369606
plot_id,batch_id 0 73 miss% 0.028358776158801596
plot_id,batch_id 0 74 miss% 0.043866573376426896
plot_id,batch_id 0 75 miss% 0.03829307173074416
plot_id,batch_id 0 76 miss% 0.0585088306603603
plot_id,batch_id 0 77 miss% 0.04643435903378269
plot_id,batch_id 0 78 miss% 0.05341210274604243
plot_id,batch_id 0 79 miss% 0.040098201414886486
plot_id,batch_id 0 80 miss% 0.04595517623715865
plot_id,batch_id 0 81 miss% 0.029504789894291426
plot_id,batch_id 0 82 miss% 0.03321008878672236
plot_id,batch_id 0 83 miss% 0.0317320014277435
plot_id,batch_id 0 84 miss% 0.04057978968800827
plot_id,batch_id 0 85 miss% 0.053345529992433006
plot_id,batch_id 0 86 miss% 0.02830719668011645
plot_id,batch_id 0 87 miss% 0.03588027876326511
plot_id,batch_id 0 88 miss% 0.047135113568001104
plot_id,batch_id 0 89 miss% 0.029862718659702137
plot_id,batch_id 0 90 miss% 0.03306138550350805
plot_id,batch_id 0 91 miss% 0.049875345299669645
plot_id,batch_id 0 92 miss% 0.03806404455289506
plot_id,batch_id 0 93 miss% 0.03244641504857534
plot_id,batch_id 0 94 miss% 0.03647198202088618
plot_id,batch_id 0 95 miss% 0.05718352653020651
plot_id,batch_id 0 96 miss% 0.0486175593303866
plot_id,batch_id 0 97 miss% 0.05149223333715383
plot_id,batch_id 0 98 miss% 0.03251497369095593
plot_id,batch_id 0 99 miss% 0.03276630388872305
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0219414  0.02112735 0.03124928 0.03036659 0.03344443 0.03538028
 0.02623517 0.03152616 0.02558242 0.02789562 0.02653125 0.0426335
 0.03820558 0.03403084 0.0328131  0.05100542 0.02773996 0.03722752
 0.02669671 0.03268038 0.04508111 0.02429898 0.02743106 0.02792052
 0.02648379 0.03946363 0.0317294  0.02517146 0.01792377 0.03338841
 0.04855178 0.03185758 0.02749543 0.02984824 0.02487813 0.05533856
 0.03810944 0.03275895 0.02472356 0.02967883 0.06683938 0.01945344
 0.0157192  0.0287545  0.01879096 0.02358055 0.02720896 0.02214646
 0.02307255 0.02242446 0.02880809 0.01642163 0.03417776 0.01578194
 0.02083602 0.03267143 0.02599087 0.03511546 0.04128085 0.01913796
 0.03687628 0.02131294 0.03737486 0.03465363 0.04110237 0.04804161
 0.03251747 0.02218902 0.02860532 0.02224049 0.0390533  0.03841871
 0.04329562 0.02835878 0.04386657 0.03829307 0.05850883 0.04643436
 0.0534121  0.0400982  0.04595518 0.02950479 0.03321009 0.031732
 0.04057979 0.05334553 0.0283072  0.03588028 0.04713511 0.02986272
 0.03306139 0.04987535 0.03806404 0.03244642 0.03647198 0.05718353
 0.04861756 0.05149223 0.03251497 0.0327663 ]
for model  40 the mean error 0.033573180303283774
all id 40 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 21 out win 4 err 0.033573180303283774
Launcher: Job 41 completed in 6360 seconds.
Launcher: Task 42 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  28945
Epoch:0, Train loss:0.462343, valid loss:0.455370
Epoch:1, Train loss:0.036147, valid loss:0.008663
Epoch:2, Train loss:0.011694, valid loss:0.004351
Epoch:3, Train loss:0.008440, valid loss:0.004235
Epoch:4, Train loss:0.007481, valid loss:0.003954
Epoch:5, Train loss:0.006587, valid loss:0.003425
Epoch:6, Train loss:0.006038, valid loss:0.003446
Epoch:7, Train loss:0.005650, valid loss:0.002767
Epoch:8, Train loss:0.005489, valid loss:0.002445
Epoch:9, Train loss:0.005116, valid loss:0.002823
Epoch:10, Train loss:0.005147, valid loss:0.002653
Epoch:11, Train loss:0.003568, valid loss:0.001996
Epoch:12, Train loss:0.003473, valid loss:0.002700
Epoch:13, Train loss:0.003533, valid loss:0.001845
Epoch:14, Train loss:0.003431, valid loss:0.002048
Epoch:15, Train loss:0.003501, valid loss:0.002546
Epoch:16, Train loss:0.003379, valid loss:0.001689
Epoch:17, Train loss:0.003292, valid loss:0.002019
Epoch:18, Train loss:0.003266, valid loss:0.002099
Epoch:19, Train loss:0.003206, valid loss:0.002459
Epoch:20, Train loss:0.003342, valid loss:0.001675
Epoch:21, Train loss:0.002484, valid loss:0.001760
Epoch:22, Train loss:0.002494, valid loss:0.001407
Epoch:23, Train loss:0.002464, valid loss:0.001768
Epoch:24, Train loss:0.002479, valid loss:0.001501
Epoch:25, Train loss:0.002505, valid loss:0.001439
Epoch:26, Train loss:0.002406, valid loss:0.001911
Epoch:27, Train loss:0.002417, valid loss:0.001483
Epoch:28, Train loss:0.002369, valid loss:0.001450
Epoch:29, Train loss:0.002398, valid loss:0.001464
Epoch:30, Train loss:0.002394, valid loss:0.001651
Epoch:31, Train loss:0.002023, valid loss:0.001266
Epoch:32, Train loss:0.002002, valid loss:0.001261
Epoch:33, Train loss:0.001989, valid loss:0.001372
Epoch:34, Train loss:0.002022, valid loss:0.001331
Epoch:35, Train loss:0.001999, valid loss:0.001218
Epoch:36, Train loss:0.001980, valid loss:0.001256
Epoch:37, Train loss:0.001966, valid loss:0.001241
Epoch:38, Train loss:0.001969, valid loss:0.001210
Epoch:39, Train loss:0.001940, valid loss:0.001389
Epoch:40, Train loss:0.001946, valid loss:0.001241
Epoch:41, Train loss:0.001765, valid loss:0.001172
Epoch:42, Train loss:0.001743, valid loss:0.001260
Epoch:43, Train loss:0.001741, valid loss:0.001195
Epoch:44, Train loss:0.001727, valid loss:0.001238
Epoch:45, Train loss:0.001742, valid loss:0.001143
Epoch:46, Train loss:0.001722, valid loss:0.001168
Epoch:47, Train loss:0.001734, valid loss:0.001130
Epoch:48, Train loss:0.001736, valid loss:0.001193
Epoch:49, Train loss:0.001699, valid loss:0.001110
Epoch:50, Train loss:0.001703, valid loss:0.001126
Epoch:51, Train loss:0.001612, valid loss:0.001110
Epoch:52, Train loss:0.001600, valid loss:0.001137
Epoch:53, Train loss:0.001596, valid loss:0.001282
Epoch:54, Train loss:0.001607, valid loss:0.001125
Epoch:55, Train loss:0.001590, valid loss:0.001165
Epoch:56, Train loss:0.001593, valid loss:0.001124
Epoch:57, Train loss:0.001593, valid loss:0.001080
Epoch:58, Train loss:0.001577, valid loss:0.001139
Epoch:59, Train loss:0.001590, valid loss:0.001069
Epoch:60, Train loss:0.001592, valid loss:0.001137
training time 6162.220386505127
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.03221634736277313
plot_id,batch_id 0 1 miss% 0.027428033217838876
plot_id,batch_id 0 2 miss% 0.03548140590310611
plot_id,batch_id 0 3 miss% 0.026275003338423045
plot_id,batch_id 0 4 miss% 0.03038150477687249
plot_id,batch_id 0 5 miss% 0.03482601619483403
plot_id,batch_id 0 6 miss% 0.029459994870360755
plot_id,batch_id 0 7 miss% 0.031121500104628868
plot_id,batch_id 0 8 miss% 0.036582179413712716
plot_id,batch_id 0 9 miss% 0.027961285634859702
plot_id,batch_id 0 10 miss% 0.04719138505074762
plot_id,batch_id 0 11 miss% 0.04321774273157966
plot_id,batch_id 0 12 miss% 0.027363529246399756
plot_id,batch_id 0 13 miss% 0.03077246912527633
plot_id,batch_id 0 14 miss% 0.04084124573816408
plot_id,batch_id 0 15 miss% 0.06417845432671254
plot_id,batch_id 0 16 miss% 0.04363185101400329
plot_id,batch_id 0 17 miss% 0.03024505391729577
plot_id,batch_id 0 18 miss% 0.035352529871105334
plot_id,batch_id 0 19 miss% 0.033394158617290955
plot_id,batch_id 0 20 miss% 0.05020871716069213
plot_id,batch_id 0 21 miss% 0.030957796379940058
plot_id,batch_id 0 22 miss% 0.025183567278310753
plot_id,batch_id 0 23 miss% 0.02844533202765205
plot_id,batch_id 0 24 miss% 0.02674488511263509
plot_id,batch_id 0 25 miss% 0.027701134994651867
plot_id,batch_id 0 26 miss% 0.028335281910206456
plot_id,batch_id 0 27 miss% 0.027686025697603797
plot_id,batch_id 0 28 miss% 0.03985035826670545
plot_id,batch_id 0 29 miss% 0.03869395859150209
plot_id,batch_id 0 30 miss% 0.034945111971865124
plot_id,batch_id 0 31 miss% 0.043223503140335066
plot_id,batch_id 0 32 miss% 0.03707844318488211
plot_id,batch_id 0 33 miss% 0.03687868573892119
plot_id,batch_id 0 34 miss% 0.03253322084744649
plot_id,batch_id 0 35 miss% 0.04839101737029183
plot_id,batch_id 0 36 miss% 0.04880843301109653
plot_id,batch_id 0 37 miss% 0.03979158400840653
plot_id,batch_id 0 38 miss% 0.03551780908489252
plot_id,batch_id 0 39 miss% 0.04052202152117645
plot_id,batch_id 0 40 miss% 0.07276808466518563
plot_id,batch_id 0 41 miss% 0.028428171255957257
plot_id,batch_id 0 42 miss% 0.027035607766583973
plot_id,batch_id 0 43 miss% 0.022149955144579833
plot_id,batch_id 0 44 miss% 0.028244978511194077
plot_id,batch_id 0 45 miss% 0.03813775644305009
plot_id,batch_id 0 46 miss% 0.0222629216163454
plot_id,batch_id 0 47 miss% 0.02620639031269902
plot_id,batch_id 0 48 miss% 0.03520477382355103
plot_id,batch_id 0 49 miss% 0.02508458995825808
plot_id,batch_id 0 50 miss% 0.020964442034382927
plot_id,batch_id 0 51 miss% 0.022147675274622067
plot_id,batch_id 0 52 miss% 0.026718525420089626
plot_id,batch_id 0 53 miss% 0.015408474237419005
plot_id,batch_id 0 54 miss% 0.047152192275966745
plot_id,batch_id 0 55 miss% 0.046239285907506165
plot_id,batch_id 0 56 miss% 0.03421326995578875
plot_id,batch_id 0 57 miss% 0.04260484600782031
plot_id,batch_id 0 58 miss% 0.043759965033198886
plot_id,batch_id 0 59 miss% 0.035874171568004175
plot_id,batch_id 0 60 miss% 0.05590447784946708
plot_id,batch_id 0 61 miss% 0.04216062347611039
plot_id,batch_id 0 62 miss% 0.03688607775124624
plot_id,batch_id 0 63 miss% 0.023442968902592502
plot_id,batch_id 0 64 miss% 0.028376893994630236
plot_id,batch_id 0 65 miss% 0.07117091722063577
plot_id,batch_id 0 66 miss% 0.05612058304296662
plot_id,batch_id 0 67 miss% 0.02974041828540392
plot_id,batch_id 0 68 miss% 0.046559404445556854
plot_id,batch_id 0 69 miss% 0.033363789450344905
plot_id,batch_id 0 70 miss% 0.05777238823867722
plot_id,batch_id 0 71 miss% 0.033325872500256495
plot_id,batch_id 0 72 miss% 0.05212972382996919
plot_id,batch_id 0 73 miss% 0.0374617119345875
plot_id,batch_id 0 74 miss% 0.0516322531012581
plot_id,batch_id 0 75 miss% 0.03774680900718045
plot_id,batch_id 0 76 miss% 0.038626078761947534
plot_id,batch_id 0 77 miss% 0.05442375388318995
plot_id,batch_id 0 78 miss% 0.042976519455408806
plot_id,batch_id 0 79 miss% 0.03630871393304364
plot_id,batch_id 0 80 miss% 0.03297759408079162
plot_id,batch_id 0 81 miss% 0.03388408927298212
plot_id,batch_id 0 82 miss% 0.036150426343506646
plot_id,batch_id 0 83 miss% 0.03578658197989232
plot_id,batch_id 0 84 miss% 0.03213081063075202
plot_id,batch_id 0 85 miss% 0.03403059056526042
plot_id,batch_id 0 86 miss% 0.031681338391809116
plot_id,batch_id 0 87 miss% 0.034103091382047215
plot_id,batch_id 0 88 miss% 0.03598859012015398
plot_id,batch_id 0 89 miss% 0.035757341492287216
plot_id,batch_id 0 90 miss% 0.05368891659734058
plot_id,batch_id 0 91 miss% 0.03186355226101398
plot_id,batch_id 0 92 miss% 0.0401414801974168
plot_id,batch_id 0 93 miss% 0.05130047525686958
plot_id,batch_id 0 94 miss% 0.03116528522797565
plot_id,batch_id 0 95 miss% 0.0497707667201204
plot_id,batch_id 0 96 miss% 0.038480247443850105
plot_id,batch_id 0 97 miss% 0.06904938009172171
plot_id,batch_id 0 98 miss% 0.03400982261757805
plot_id,batch_id 0 99 miss% 0.02873762227580492
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03221635 0.02742803 0.03548141 0.026275   0.0303815  0.03482602
 0.02945999 0.0311215  0.03658218 0.02796129 0.04719139 0.04321774
 0.02736353 0.03077247 0.04084125 0.06417845 0.04363185 0.03024505
 0.03535253 0.03339416 0.05020872 0.0309578  0.02518357 0.02844533
 0.02674489 0.02770113 0.02833528 0.02768603 0.03985036 0.03869396
 0.03494511 0.0432235  0.03707844 0.03687869 0.03253322 0.04839102
 0.04880843 0.03979158 0.03551781 0.04052202 0.07276808 0.02842817
 0.02703561 0.02214996 0.02824498 0.03813776 0.02226292 0.02620639
 0.03520477 0.02508459 0.02096444 0.02214768 0.02671853 0.01540847
 0.04715219 0.04623929 0.03421327 0.04260485 0.04375997 0.03587417
 0.05590448 0.04216062 0.03688608 0.02344297 0.02837689 0.07117092
 0.05612058 0.02974042 0.0465594  0.03336379 0.05777239 0.03332587
 0.05212972 0.03746171 0.05163225 0.03774681 0.03862608 0.05442375
 0.04297652 0.03630871 0.03297759 0.03388409 0.03615043 0.03578658
 0.03213081 0.03403059 0.03168134 0.03410309 0.03598859 0.03575734
 0.05368892 0.03186355 0.04014148 0.05130048 0.03116529 0.04977077
 0.03848025 0.06904938 0.03400982 0.02873762]
for model  146 the mean error 0.037268526369810505
all id 146 hidden_dim 16 learning_rate 0.02 num_layers 4 frames 25 out win 5 err 0.037268526369810505
Launcher: Job 147 completed in 6367 seconds.
Launcher: Task 7 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  79249
Epoch:0, Train loss:0.448520, valid loss:0.420811
Epoch:1, Train loss:0.028565, valid loss:0.004958
Epoch:2, Train loss:0.007555, valid loss:0.004413
Epoch:3, Train loss:0.005349, valid loss:0.002873
Epoch:4, Train loss:0.004367, valid loss:0.002690
Epoch:5, Train loss:0.003779, valid loss:0.002244
Epoch:6, Train loss:0.003447, valid loss:0.002198
Epoch:7, Train loss:0.003220, valid loss:0.002206
Epoch:8, Train loss:0.003144, valid loss:0.001945
Epoch:9, Train loss:0.002852, valid loss:0.001570
Epoch:10, Train loss:0.002902, valid loss:0.001884
Epoch:11, Train loss:0.001940, valid loss:0.001152
Epoch:12, Train loss:0.001957, valid loss:0.001197
Epoch:13, Train loss:0.001897, valid loss:0.001103
Epoch:14, Train loss:0.001925, valid loss:0.001154
Epoch:15, Train loss:0.001870, valid loss:0.001053
Epoch:16, Train loss:0.001760, valid loss:0.001212
Epoch:17, Train loss:0.001783, valid loss:0.001166
Epoch:18, Train loss:0.001714, valid loss:0.001000
Epoch:19, Train loss:0.001712, valid loss:0.001180
Epoch:20, Train loss:0.001645, valid loss:0.000867
Epoch:21, Train loss:0.001223, valid loss:0.000863
Epoch:22, Train loss:0.001274, valid loss:0.000927
Epoch:23, Train loss:0.001209, valid loss:0.000868
Epoch:24, Train loss:0.001220, valid loss:0.000750
Epoch:25, Train loss:0.001175, valid loss:0.000748
Epoch:26, Train loss:0.001234, valid loss:0.000725
Epoch:27, Train loss:0.001176, valid loss:0.000784
Epoch:28, Train loss:0.001212, valid loss:0.000757
Epoch:29, Train loss:0.001144, valid loss:0.000871
Epoch:30, Train loss:0.001188, valid loss:0.000748
Epoch:31, Train loss:0.000941, valid loss:0.000641
Epoch:32, Train loss:0.000938, valid loss:0.000712
Epoch:33, Train loss:0.000938, valid loss:0.000654
Epoch:34, Train loss:0.000937, valid loss:0.000680
Epoch:35, Train loss:0.000932, valid loss:0.000658
Epoch:36, Train loss:0.000917, valid loss:0.000622
Epoch:37, Train loss:0.000909, valid loss:0.000631
Epoch:38, Train loss:0.000906, valid loss:0.000641
Epoch:39, Train loss:0.000922, valid loss:0.000611
Epoch:40, Train loss:0.000896, valid loss:0.000702
Epoch:41, Train loss:0.000803, valid loss:0.000595
Epoch:42, Train loss:0.000798, valid loss:0.000614
Epoch:43, Train loss:0.000803, valid loss:0.000604
Epoch:44, Train loss:0.000796, valid loss:0.000634
Epoch:45, Train loss:0.000794, valid loss:0.000567
Epoch:46, Train loss:0.000796, valid loss:0.000630
Epoch:47, Train loss:0.000792, valid loss:0.000691
Epoch:48, Train loss:0.000800, valid loss:0.000587
Epoch:49, Train loss:0.000776, valid loss:0.000605
Epoch:50, Train loss:0.000776, valid loss:0.000604
Epoch:51, Train loss:0.000737, valid loss:0.000570
Epoch:52, Train loss:0.000739, valid loss:0.000593
Epoch:53, Train loss:0.000736, valid loss:0.000579
Epoch:54, Train loss:0.000738, valid loss:0.000611
Epoch:55, Train loss:0.000730, valid loss:0.000578
Epoch:56, Train loss:0.000730, valid loss:0.000580
Epoch:57, Train loss:0.000731, valid loss:0.000596
Epoch:58, Train loss:0.000727, valid loss:0.000566
Epoch:59, Train loss:0.000725, valid loss:0.000553
Epoch:60, Train loss:0.000724, valid loss:0.000581
training time 6215.688304424286
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.03648871089386325
plot_id,batch_id 0 1 miss% 0.022074365875925855
plot_id,batch_id 0 2 miss% 0.03194126031057223
plot_id,batch_id 0 3 miss% 0.024516818293578994
plot_id,batch_id 0 4 miss% 0.025933218130814686
plot_id,batch_id 0 5 miss% 0.03341015276303758
plot_id,batch_id 0 6 miss% 0.02105679423923651
plot_id,batch_id 0 7 miss% 0.028425781476532633
plot_id,batch_id 0 8 miss% 0.024550965417763353
plot_id,batch_id 0 9 miss% 0.014048516235190352
plot_id,batch_id 0 10 miss% 0.03486084191826615
plot_id,batch_id 0 11 miss% 0.03196031242221541
plot_id,batch_id 0 12 miss% 0.028986029820161657
plot_id,batch_id 0 13 miss% 0.021129821796390884
plot_id,batch_id 0 14 miss% 0.022547303224622346
plot_id,batch_id 0 15 miss% 0.036184765844223685
plot_id,batch_id 0 16 miss% 0.023586805980429646
plot_id,batch_id 0 17 miss% 0.03280660070110114
plot_id,batch_id 0 18 miss% 0.03950275976498241
plot_id,batch_id 0 19 miss% 0.02503897830641741
plot_id,batch_id 0 20 miss% 0.03047832607303797
plot_id,batch_id 0 21 miss% 0.030595635830270695
plot_id,batch_id 0 22 miss% 0.02886282828420272
plot_id,batch_id 0 23 miss% 0.032239406436685214
plot_id,batch_id 0 24 miss% 0.02458760364685297
plot_id,batch_id 0 25 miss% 0.037556586680215505
plot_id,batch_id 0 26 miss% 0.028531527785330926
plot_id,batch_id 0 27 miss% 0.026263877424460975
plot_id,batch_id 0 28 miss% 0.031813878252986094
plot_id,batch_id 0 29 miss% 0.024658490571567656
plot_id,batch_id 0 30 miss% 0.0461985619176329
plot_id,batch_id 0 31 miss% 0.030902506546820863
plot_id,batch_id 0 32 miss% 0.021901025884931372
plot_id,batch_id 0 33 miss% 0.02908936066334367
plot_id,batch_id 0 34 miss% 0.025271233716922137
plot_id,batch_id 0 35 miss% 0.04516937265304307
plot_id,batch_id 0 36 miss% 0.035942351703224094
plot_id,batch_id 0 37 miss% 0.019982942285670234
plot_id,batch_id 0 38 miss% 0.02532899060091913
plot_id,batch_id 0 39 miss% 0.02856434055790484
plot_id,batch_id 0 40 miss% 0.06141511574691422
plot_id,batch_id 0 41 miss% 0.024231852590221998
plot_id,batch_id 0 42 miss% 0.012966058169992854
plot_id,batch_id 0 43 miss% 0.019377850563934016
plot_id,batch_id 0 44 miss% 0.015024755333338143
plot_id,batch_id 0 45 miss% 0.03401714859195275
plot_id,batch_id 0 46 miss% 0.03221104696459313
plot_id,batch_id 0 47 miss% 0.023565573479607978
plot_id,batch_id 0 48 miss% 0.017841487686797195
plot_id,batch_id 0 49 miss% 0.014930561301529905
plot_id,batch_id 0 50 miss% 0.029469072976634873
plot_id,batch_id 0 51 miss% 0.02932742231798724
plot_id,batch_id 0 52 miss% 0.017519203003009094
plot_id,batch_id 0 53 miss% 0.011473420809302045
plot_id,batch_id 0 54 miss% 0.024176685335026985
plot_id,batch_id 0 55 miss% 0.04914941622826445
plot_id,batch_id 0 56 miss% 0.025749977113813317
plot_id,batch_id 0 57 miss% 0.02457884623274205
plot_id,batch_id 0 58 miss% 0.018131249815271182
plot_id,batch_id 0 59 miss% 0.02438129383057294
plot_id,batch_id 0 60 miss% 0.030357576885812327
plot_id,batch_id 0 61 miss% 0.03676263448739529
plot_id,batch_id 0 62 miss% 0.01823551056741725
plot_id,batch_id 0 63 miss% 0.023306744409415964
plot_id,batch_id 0 64 miss% 0.04193662793911203
plot_id,batch_id 0 65 miss% 0.03939233365979759
plot_id,batch_id 0 66 miss% 0.03659551529131929
plot_id,batch_id 0 67 miss% 0.02063861360317183
plot_id,batch_id 0 68 miss% 0.034634020014509194
plot_id,batch_id the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  77489
Epoch:0, Train loss:0.665920, valid loss:0.672965
Epoch:1, Train loss:0.054488, valid loss:0.011505
Epoch:2, Train loss:0.013493, valid loss:0.005586
Epoch:3, Train loss:0.008427, valid loss:0.004359
Epoch:4, Train loss:0.006584, valid loss:0.004242
Epoch:5, Train loss:0.005555, valid loss:0.003514
Epoch:6, Train loss:0.004951, valid loss:0.002601
Epoch:7, Train loss:0.004622, valid loss:0.005709
Epoch:8, Train loss:0.004278, valid loss:0.002294
Epoch:9, Train loss:0.004046, valid loss:0.002159
Epoch:10, Train loss:0.003823, valid loss:0.003294
Epoch:11, Train loss:0.002604, valid loss:0.001409
Epoch:12, Train loss:0.002513, valid loss:0.001314
Epoch:13, Train loss:0.002508, valid loss:0.001204
Epoch:14, Train loss:0.002470, valid loss:0.001747
Epoch:15, Train loss:0.002465, valid loss:0.001346
Epoch:16, Train loss:0.002431, valid loss:0.001283
Epoch:17, Train loss:0.002336, valid loss:0.001351
Epoch:18, Train loss:0.002296, valid loss:0.001354
Epoch:19, Train loss:0.002281, valid loss:0.001383
Epoch:20, Train loss:0.002178, valid loss:0.001456
Epoch:21, Train loss:0.001626, valid loss:0.001214
Epoch:22, Train loss:0.001556, valid loss:0.001203
Epoch:23, Train loss:0.001598, valid loss:0.001103
Epoch:24, Train loss:0.001557, valid loss:0.001086
Epoch:25, Train loss:0.001585, valid loss:0.001083
Epoch:26, Train loss:0.001561, valid loss:0.001034
Epoch:27, Train loss:0.001513, valid loss:0.000955
Epoch:28, Train loss:0.001539, valid loss:0.000953
Epoch:29, Train loss:0.001511, valid loss:0.000940
Epoch:30, Train loss:0.001433, valid loss:0.000998
Epoch:31, Train loss:0.001182, valid loss:0.000868
Epoch:32, Train loss:0.001165, valid loss:0.000830
Epoch:33, Train loss:0.001180, valid loss:0.000867
Epoch:34, Train loss:0.001175, valid loss:0.000921
Epoch:35, Train loss:0.001188, valid loss:0.000895
Epoch:36, Train loss:0.001138, valid loss:0.001069
Epoch:37, Train loss:0.001137, valid loss:0.000848
Epoch:38, Train loss:0.001121, valid loss:0.000910
Epoch:39, Train loss:0.001161, valid loss:0.000891
Epoch:40, Train loss:0.001109, valid loss:0.000929
Epoch:41, Train loss:0.000986, valid loss:0.000805
Epoch:42, Train loss:0.000984, valid loss:0.000805
Epoch:43, Train loss:0.000979, valid loss:0.000819
Epoch:44, Train loss:0.000992, valid loss:0.000914
Epoch:45, Train loss:0.000979, valid loss:0.000813
Epoch:46, Train loss:0.000981, valid loss:0.000834
Epoch:47, Train loss:0.000968, valid loss:0.000772
Epoch:48, Train loss:0.000962, valid loss:0.000861
Epoch:49, Train loss:0.000954, valid loss:0.000806
Epoch:50, Train loss:0.000950, valid loss:0.000800
Epoch:51, Train loss:0.000901, valid loss:0.000768
Epoch:52, Train loss:0.000897, valid loss:0.000759
Epoch:53, Train loss:0.000895, valid loss:0.000776
Epoch:54, Train loss:0.000889, valid loss:0.000773
Epoch:55, Train loss:0.000890, valid loss:0.000761
Epoch:56, Train loss:0.000886, valid loss:0.000770
Epoch:57, Train loss:0.000884, valid loss:0.000800
Epoch:58, Train loss:0.000882, valid loss:0.000786
Epoch:59, Train loss:0.000885, valid loss:0.000792
Epoch:60, Train loss:0.000882, valid loss:0.000772
training time 6213.969818353653
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.027605138275101904
plot_id,batch_id 0 1 miss% 0.030024248998757285
plot_id,batch_id 0 2 miss% 0.02703527682258312
plot_id,batch_id 0 3 miss% 0.022523470891758424
plot_id,batch_id 0 4 miss% 0.028911793187079794
plot_id,batch_id 0 5 miss% 0.02787645935244325
plot_id,batch_id 0 6 miss% 0.02956870730942634
plot_id,batch_id 0 7 miss% 0.020574119479801027
plot_id,batch_id 0 8 miss% 0.0317391697910692
plot_id,batch_id 0 9 miss% 0.02115020155265506
plot_id,batch_id 0 10 miss% 0.03646277427583618
plot_id,batch_id 0 11 miss% 0.03927935892668118
plot_id,batch_id 0 12 miss% 0.022411807382604336
plot_id,batch_id 0 13 miss% 0.015910243309753522
plot_id,batch_id 0 14 miss% 0.022049490702495216
plot_id,batch_id 0 15 miss% 0.04306114804494673
plot_id,batch_id 0 16 miss% 0.0245150447995701
plot_id,batch_id 0 17 miss% 0.03957774598651265
plot_id,batch_id 0 18 miss% 0.03174375665026793
plot_id,batch_id 0 19 miss% 0.032551637467748064
plot_id,batch_id 0 20 miss% 0.05218918364831794
plot_id,batch_id 0 21 miss% 0.015014586737061398
plot_id,batch_id 0 22 miss% 0.029154498248947718
plot_id,batch_id 0 23 miss% 0.035155073791706244
plot_id,batch_id 0 24 miss% 0.033803016119387574
plot_id,batch_id 0 25 miss% 0.033086077662761615
plot_id,batch_id 0 26 miss% 0.03871091561128237
plot_id,batch_id 0 27 miss% 0.02319050033682416
plot_id,batch_id 0 28 miss% 0.02919653695482297
plot_id,batch_id 0 29 miss% 0.030076206875609815
plot_id,batch_id 0 30 miss% 0.03158603862923766
plot_id,batch_id 0 31 miss% 0.021416710348559475
plot_id,batch_id 0 32 miss% 0.023137934513216007
plot_id,batch_id 0 33 miss% 0.025071204968456308
plot_id,batch_id 0 34 miss% 0.03261750742719095
plot_id,batch_id 0 35 miss% 0.04053740210157991
plot_id,batch_id 0 36 miss% 0.03442839600011627
plot_id,batch_id 0 37 miss% 0.03208554921971373
plot_id,batch_id 0 38 miss% 0.023814654916488355
plot_id,batch_id 0 39 miss% 0.019855628433530472
plot_id,batch_id 0 40 miss% 0.041561215700427624
plot_id,batch_id 0 41 miss% 0.02367381787740079
plot_id,batch_id 0 42 miss% 0.018173910498974814
plot_id,batch_id 0 43 miss% 0.04407218715469219
plot_id,batch_id 0 44 miss% 0.03487099718078997
plot_id,batch_id 0 45 miss% 0.03226071858616894
plot_id,batch_id 0 46 miss% 0.030216183966075623
plot_id,batch_id 0 47 miss% 0.029107905086665296
plot_id,batch_id 0 48 miss% 0.029532258016864904
plot_id,batch_id 0 49 miss% 0.02805370600961637
plot_id,batch_id 0 50 miss% 0.0381144847011286
plot_id,batch_id 0 51 miss% 0.02508582741284904
plot_id,batch_id 0 52 miss% 0.020703594589537026
plot_id,batch_id 0 53 miss% 0.018316091333695095
plot_id,batch_id 0 54 miss% 0.033006749008451367
plot_id,batch_id 0 55 miss% 0.035926498971266534
plot_id,batch_id 0 56 miss% 0.03363590398529066
plot_id,batch_id 0 57 miss% 0.025024607229318905
plot_id,batch_id 0 58 miss% 0.03057506820849675
plot_id,batch_id 0 59 miss% 0.035165383216876456
plot_id,batch_id 0 60 miss% 0.058289955949155155
plot_id,batch_id 0 61 miss% 0.02745428656742908
plot_id,batch_id 0 62 miss% 0.029483275023498942
plot_id,batch_id 0 63 miss% 0.022332945533838987
plot_id,batch_id 0 64 miss% 0.04612937590983347
plot_id,batch_id 0 65 miss% 0.053736847233081954
plot_id,batch_id 0 66 miss% 0.04088142747424827
plot_id,batch_id 0 67 miss% 0.02188817681666015
plot_id,batch_id 0 68 miss% 0.023892237585269507
plot_id,batch_id 0 69 miss% 0.02546137767822947
0 69 miss% 0.02191886182829863
plot_id,batch_id 0 70 miss% 0.03948573115977207
plot_id,batch_id 0 71 miss% 0.06260520559135309
plot_id,batch_id 0 72 miss% 0.03323924386822545
plot_id,batch_id 0 73 miss% 0.031868301628042384
plot_id,batch_id 0 74 miss% 0.023534535299115287
plot_id,batch_id 0 75 miss% 0.024918883166253205
plot_id,batch_id 0 76 miss% 0.03145429550149693
plot_id,batch_id 0 77 miss% 0.03913851024031639
plot_id,batch_id 0 78 miss% 0.037746524952068226
plot_id,batch_id 0 79 miss% 0.030600675284232708
plot_id,batch_id 0 80 miss% 0.05453623457956543
plot_id,batch_id 0 81 miss% 0.02823682643871479
plot_id,batch_id 0 82 miss% 0.026090994079694303
plot_id,batch_id 0 83 miss% 0.021055287966990688
plot_id,batch_id 0 84 miss% 0.020201011930672558
plot_id,batch_id 0 85 miss% 0.06824611678121632
plot_id,batch_id 0 86 miss% 0.024040879711262668
plot_id,batch_id 0 87 miss% 0.036787984859870046
plot_id,batch_id 0 88 miss% 0.030838221705763096
plot_id,batch_id 0 89 miss% 0.025224853039685823
plot_id,batch_id 0 90 miss% 0.023102387581687735
plot_id,batch_id 0 91 miss% 0.03187930704920445
plot_id,batch_id 0 92 miss% 0.03314935612910232
plot_id,batch_id 0 93 miss% 0.021041062254640343
plot_id,batch_id 0 94 miss% 0.04236484483025963
plot_id,batch_id 0 95 miss% 0.03450236122630956
plot_id,batch_id 0 96 miss% 0.031092333818587194
plot_id,batch_id 0 97 miss% 0.04095032822315807
plot_id,batch_id 0 98 miss% 0.029606608421802693
plot_id,batch_id 0 99 miss% 0.036215317845289
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03648871 0.02207437 0.03194126 0.02451682 0.02593322 0.03341015
 0.02105679 0.02842578 0.02455097 0.01404852 0.03486084 0.03196031
 0.02898603 0.02112982 0.0225473  0.03618477 0.02358681 0.0328066
 0.03950276 0.02503898 0.03047833 0.03059564 0.02886283 0.03223941
 0.0245876  0.03755659 0.02853153 0.02626388 0.03181388 0.02465849
 0.04619856 0.03090251 0.02190103 0.02908936 0.02527123 0.04516937
 0.03594235 0.01998294 0.02532899 0.02856434 0.06141512 0.02423185
 0.01296606 0.01937785 0.01502476 0.03401715 0.03221105 0.02356557
 0.01784149 0.01493056 0.02946907 0.02932742 0.0175192  0.01147342
 0.02417669 0.04914942 0.02574998 0.02457885 0.01813125 0.02438129
 0.03035758 0.03676263 0.01823551 0.02330674 0.04193663 0.03939233
 0.03659552 0.02063861 0.03463402 0.02191886 0.03948573 0.06260521
 0.03323924 0.0318683  0.02353454 0.02491888 0.0314543  0.03913851
 0.03774652 0.03060068 0.05453623 0.02823683 0.02609099 0.02105529
 0.02020101 0.06824612 0.02404088 0.03678798 0.03083822 0.02522485
 0.02310239 0.03187931 0.03314936 0.02104106 0.04236484 0.03450236
 0.03109233 0.04095033 0.02960661 0.03621532]
for model  114 the mean error 0.02990060352873457
all id 114 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 25 out win 3 err 0.02990060352873457
Launcher: Job 115 completed in 6409 seconds.
Launcher: Task 6 done. Exiting.
plot_id,batch_id 0 70 miss% 0.042654841313797535
plot_id,batch_id 0 71 miss% 0.043292509151091424
plot_id,batch_id 0 72 miss% 0.04054415503762561
plot_id,batch_id 0 73 miss% 0.035946216113068266
plot_id,batch_id 0 74 miss% 0.02835151210789599
plot_id,batch_id 0 75 miss% 0.028946392320797317
plot_id,batch_id 0 76 miss% 0.0487986413087319
plot_id,batch_id 0 77 miss% 0.02499079404493568
plot_id,batch_id 0 78 miss% 0.028093262015875243
plot_id,batch_id 0 79 miss% 0.03959118998913277
plot_id,batch_id 0 80 miss% 0.033262067943278795
plot_id,batch_id 0 81 miss% 0.02575987096161804
plot_id,batch_id 0 82 miss% 0.02677548494438454
plot_id,batch_id 0 83 miss% 0.023743371301401556
plot_id,batch_id 0 84 miss% 0.030966245190016506
plot_id,batch_id 0 85 miss% 0.03693280589269035
plot_id,batch_id 0 86 miss% 0.01862813393503991
plot_id,batch_id 0 87 miss% 0.023153561638931203
plot_id,batch_id 0 88 miss% 0.03849575309084185
plot_id,batch_id 0 89 miss% 0.025317085845774034
plot_id,batch_id 0 90 miss% 0.02936107182773438
plot_id,batch_id 0 91 miss% 0.027611292222294578
plot_id,batch_id 0 92 miss% 0.03031875416923323
plot_id,batch_id 0 93 miss% 0.02688672523607664
plot_id,batch_id 0 94 miss% 0.038443192302176674
plot_id,batch_id 0 95 miss% 0.04041427185722093
plot_id,batch_id 0 96 miss% 0.033549577613239165
plot_id,batch_id 0 97 miss% 0.0472691819609818
plot_id,batch_id 0 98 miss% 0.0281327147318887
plot_id,batch_id 0 99 miss% 0.03652962807856123
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02760514 0.03002425 0.02703528 0.02252347 0.02891179 0.02787646
 0.02956871 0.02057412 0.03173917 0.0211502  0.03646277 0.03927936
 0.02241181 0.01591024 0.02204949 0.04306115 0.02451504 0.03957775
 0.03174376 0.03255164 0.05218918 0.01501459 0.0291545  0.03515507
 0.03380302 0.03308608 0.03871092 0.0231905  0.02919654 0.03007621
 0.03158604 0.02141671 0.02313793 0.0250712  0.03261751 0.0405374
 0.0344284  0.03208555 0.02381465 0.01985563 0.04156122 0.02367382
 0.01817391 0.04407219 0.034871   0.03226072 0.03021618 0.02910791
 0.02953226 0.02805371 0.03811448 0.02508583 0.02070359 0.01831609
 0.03300675 0.0359265  0.0336359  0.02502461 0.03057507 0.03516538
 0.05828996 0.02745429 0.02948328 0.02233295 0.04612938 0.05373685
 0.04088143 0.02188818 0.02389224 0.02546138 0.04265484 0.04329251
 0.04054416 0.03594622 0.02835151 0.02894639 0.04879864 0.02499079
 0.02809326 0.03959119 0.03326207 0.02575987 0.02677548 0.02374337
 0.03096625 0.03693281 0.01862813 0.02315356 0.03849575 0.02531709
 0.02936107 0.02761129 0.03031875 0.02688673 0.03844319 0.04041427
 0.03354958 0.04726918 0.02813271 0.03652963]
for model  48 the mean error 0.0311808653440407
all id 48 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 21 out win 3 err 0.0311808653440407
Launcher: Job 49 completed in 6414 seconds.
Launcher: Task 95 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  107025
Epoch:0, Train loss:0.594995, valid loss:0.592053
Epoch:1, Train loss:0.052088, valid loss:0.008635
Epoch:2, Train loss:0.013752, valid loss:0.006032
Epoch:3, Train loss:0.009301, valid loss:0.004329
Epoch:4, Train loss:0.007478, valid loss:0.003358
Epoch:5, Train loss:0.006154, valid loss:0.003164
Epoch:6, Train loss:0.005249, valid loss:0.002663
Epoch:7, Train loss:0.004784, valid loss:0.003006
Epoch:8, Train loss:0.004349, valid loss:0.002982
Epoch:9, Train loss:0.004016, valid loss:0.002356
Epoch:10, Train loss:0.003702, valid loss:0.002131
Epoch:11, Train loss:0.002630, valid loss:0.001528
Epoch:12, Train loss:0.002559, valid loss:0.001644
Epoch:13, Train loss:0.002562, valid loss:0.001366
Epoch:14, Train loss:0.002459, valid loss:0.001589
Epoch:15, Train loss:0.002349, valid loss:0.001658
Epoch:16, Train loss:0.002283, valid loss:0.001480
Epoch:17, Train loss:0.002241, valid loss:0.001588
Epoch:18, Train loss:0.002228, valid loss:0.001546
Epoch:19, Train loss:0.002084, valid loss:0.001532
Epoch:20, Train loss:0.002115, valid loss:0.001302
Epoch:21, Train loss:0.001602, valid loss:0.001094
Epoch:22, Train loss:0.001561, valid loss:0.001076
Epoch:23, Train loss:0.001566, valid loss:0.001129
Epoch:24, Train loss:0.001531, valid loss:0.001322
Epoch:25, Train loss:0.001512, valid loss:0.001144
Epoch:26, Train loss:0.001489, valid loss:0.001121
Epoch:27, Train loss:0.001469, valid loss:0.001275
Epoch:28, Train loss:0.001445, valid loss:0.001076
Epoch:29, Train loss:0.001449, valid loss:0.001056
Epoch:30, Train loss:0.001405, valid loss:0.000964
Epoch:31, Train loss:0.001174, valid loss:0.000924
Epoch:32, Train loss:0.001179, valid loss:0.000902
Epoch:33, Train loss:0.001176, valid loss:0.000935
Epoch:34, Train loss:0.001143, valid loss:0.000954
Epoch:35, Train loss:0.001153, valid loss:0.001044
Epoch:36, Train loss:0.001136, valid loss:0.000962
Epoch:37, Train loss:0.001125, valid loss:0.000906
Epoch:38, Train loss:0.001111, valid loss:0.000942
Epoch:39, Train loss:0.001112, valid loss:0.000893
Epoch:40, Train loss:0.001134, valid loss:0.000923
Epoch:41, Train loss:0.001002, valid loss:0.000853
Epoch:42, Train loss:0.000996, valid loss:0.000859
Epoch:43, Train loss:0.000985, valid loss:0.000846
Epoch:44, Train loss:0.000988, valid loss:0.000888
Epoch:45, Train loss:0.000987, valid loss:0.000898
Epoch:46, Train loss:0.000980, valid loss:0.000859
Epoch:47, Train loss:0.000975, valid loss:0.000896
Epoch:48, Train loss:0.000969, valid loss:0.000843
Epoch:49, Train loss:0.000965, valid loss:0.000894
Epoch:50, Train loss:0.000965, valid loss:0.000834
Epoch:51, Train loss:0.000914, valid loss:0.000832
Epoch:52, Train loss:0.000911, valid loss:0.000847
Epoch:53, Train loss:0.000910, valid loss:0.000831
Epoch:54, Train loss:0.000906, valid loss:0.000836
Epoch:55, Train loss:0.000908, valid loss:0.000817
Epoch:56, Train loss:0.000903, valid loss:0.000857
Epoch:57, Train loss:0.000904, valid loss:0.000825
Epoch:58, Train loss:0.000902, valid loss:0.000853
Epoch:59, Train loss:0.000903, valid loss:0.000863
Epoch:60, Train loss:0.000896, valid loss:0.000854
training time 6239.046476840973
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.016336913011903254
plot_id,batch_id 0 1 miss% 0.030744472901963978
plot_id,batch_id 0 2 miss% 0.020848305889577944
plot_id,batch_id 0 3 miss% 0.025280431840070717
plot_id,batch_id 0 4 miss% 0.025944461015589677
plot_id,batch_id 0 5 miss% 0.03464449736671608
plot_id,batch_id 0 6 miss% 0.020513352916419623
plot_id,batch_id 0 7 miss% 0.021463938672360097
plot_id,batch_id 0 8 miss% 0.023267257205419976
plot_id,batch_id 0 9 miss% 0.028156240080149126
plot_id,batch_id 0 10 miss% 0.03648728511849357
plot_id,batch_id 0 11 miss% 0.04439828052981855
plot_id,batch_id 0 12 miss% 0.015157590872670339
plot_id,batch_id 0 13 miss% 0.026149712290588076
plot_id,batch_id 0 14 miss% 0.028793205580195116
plot_id,batch_id 0 15 miss% 0.05262899437685664
plot_id,batch_id 0 16 miss% 0.026992406273582008
plot_id,batch_id 0 17 miss% 0.03461123691869584
plot_id,batch_id 0 18 miss% 0.02350675870003542
plot_id,batch_id 0 19 miss% 0.03564475983127626
plot_id,batch_id 0 20 miss% 0.034719840680738336
plot_id,batch_id 0 21 miss% 0.011290992213692749
plot_id,batch_id 0 22 miss% 0.024857106744472873
plot_id,batch_id 0 23 miss% 0.024759763664206413
plot_id,batch_id 0 24 miss% 0.028137092515244235
plot_id,batch_id 0 25 miss% 0.027606147888627985
plot_id,batch_id 0 26 miss% 0.0417572023097144
plot_id,batch_id 0 27 miss% 0.02129923041715882
plot_id,batch_id 0 28 miss% 0.024695084682602095
plot_id,batch_id 0 29 miss% 0.024289319136620937
plot_id,batch_id 0 30 miss% 0.061500219567676824
plot_id,batch_id 0 31 miss% 0.022237175810790516
plot_id,batch_id 0 32 miss% 0.023521393336234868
plot_id,batch_id 0 33 miss% 0.02709537923817237
plot_id,batch_id 0 34 miss% 0.024048826485100684
plot_id,batch_id 0 35 miss% 0.04213906976031631
plot_id,batch_id 0 36 miss% 0.04479601177242356
plot_id,batch_id 0 37 miss% 0.02771850855989357
plot_id,batch_id 0 38 miss% 0.027731469299031395
plot_id,batch_id 0 39 miss% 0.02204304326658313
plot_id,batch_id 0 40 miss% 0.07754106461962788
plot_id,batch_id 0 41 miss% 0.019357104480055513
plot_id,batch_id 0 42 miss% 0.011171635201597257
plot_id,batch_id 0 43 miss% 0.029059846973006377
plot_id,batch_id 0 44 miss% 0.011871366149810223
plot_id,batch_id 0 45 miss% 0.023879907828108855
plot_id,batch_id 0 46 miss% 0.023227440613487114
plot_id,batch_id 0 47 miss% 0.018123522160690585
plot_id,batch_id 0 48 miss% 0.020197210473299025
plot_id,batch_id 0 49 miss% 0.019296286715525822
plot_id,batch_id 0 50 miss% 0.03752876136307749
plot_id,batch_id 0 51 miss% 0.01937239772843544
plot_id,batch_id 0 52 miss% 0.02475207556779694
plot_id,batch_id 0 53 miss% 0.010959407507182635
plot_id,batch_id 0 54 miss% 0.037203412073803774
plot_id,batch_id 0 55 miss% 0.04050997592287165
plot_id,batch_id 0 56 miss% 0.03495385038946275
plot_id,batch_id 0 57 miss% 0.023850066111738826
plot_id,batch_id 0 58 miss% 0.02467040173095095
plot_id,batch_id 0 59 miss% 0.02117070551971137
plot_id,batch_id 0 60 miss% 0.04956324782994928
plot_id,batch_id 0 61 miss% 0.037409640242516994
plot_id,batch_id 0 62 miss% 0.026372611284947438
plot_id,batch_id 0 63 miss% 0.032601930495128294
plot_id,batch_id 0 64 miss% 0.02688405792404704
plot_id,batch_id 0 65 miss% 0.048590033058035026
plot_id,batch_id 0 66 miss% 0.05054920192724021
plot_id,batch_id 0 67 miss% 0.029700589777736403
plot_id,batch_id 0 68 miss% 0.027845005063131235
plot_id,batch_id 0 69 miss% the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  21969
Epoch:0, Train loss:0.404525, valid loss:0.371485
Epoch:1, Train loss:0.229976, valid loss:0.231488
Epoch:2, Train loss:0.222450, valid loss:0.231128
Epoch:3, Train loss:0.221373, valid loss:0.230668
Epoch:4, Train loss:0.220738, valid loss:0.229940
Epoch:5, Train loss:0.220458, valid loss:0.229953
Epoch:6, Train loss:0.220234, valid loss:0.230115
Epoch:7, Train loss:0.220110, valid loss:0.229926
Epoch:8, Train loss:0.219933, valid loss:0.230382
Epoch:9, Train loss:0.219854, valid loss:0.231051
Epoch:10, Train loss:0.219807, valid loss:0.230006
Epoch:11, Train loss:0.218902, valid loss:0.229746
Epoch:12, Train loss:0.218891, valid loss:0.229683
Epoch:13, Train loss:0.218876, valid loss:0.229682
Epoch:14, Train loss:0.218868, valid loss:0.229379
Epoch:15, Train loss:0.218812, valid loss:0.229537
Epoch:16, Train loss:0.218753, valid loss:0.229339
Epoch:17, Train loss:0.218777, valid loss:0.229289
Epoch:18, Train loss:0.218708, valid loss:0.229422
Epoch:19, Train loss:0.218741, valid loss:0.229619
Epoch:20, Train loss:0.218693, valid loss:0.229352
Epoch:21, Train loss:0.218264, valid loss:0.229222
Epoch:22, Train loss:0.218274, valid loss:0.229107
Epoch:23, Train loss:0.218271, valid loss:0.229081
Epoch:24, Train loss:0.218275, valid loss:0.229337
Epoch:25, Train loss:0.218239, valid loss:0.229113
Epoch:26, Train loss:0.218264, valid loss:0.229144
Epoch:27, Train loss:0.218226, valid loss:0.229334
Epoch:28, Train loss:0.218192, valid loss:0.229183
Epoch:29, Train loss:0.218241, valid loss:0.229329
Epoch:30, Train loss:0.218212, valid loss:0.229171
Epoch:31, Train loss:0.217954, valid loss:0.228956
Epoch:32, Train loss:0.217946, valid loss:0.229125
Epoch:33, Train loss:0.217958, valid loss:0.229013
Epoch:34, Train loss:0.217971, valid loss:0.229007
Epoch:35, Train loss:0.217947, valid loss:0.228970
Epoch:36, Train loss:0.217944, valid loss:0.229029
Epoch:37, Train loss:0.217960, valid loss:0.228936
Epoch:38, Train loss:0.217936, valid loss:0.228979
Epoch:39, Train loss:0.217940, valid loss:0.229017
Epoch:40, Train loss:0.217932, valid loss:0.228949
Epoch:41, Train loss:0.217792, valid loss:0.228961
Epoch:42, Train loss:0.217802, valid loss:0.228883
Epoch:43, Train loss:0.217790, valid loss:0.228945
Epoch:44, Train loss:0.217792, valid loss:0.228895
Epoch:45, Train loss:0.217800, valid loss:0.228880
Epoch:46, Train loss:0.217770, valid loss:0.228877
Epoch:47, Train loss:0.217786, valid loss:0.228867
Epoch:48, Train loss:0.217784, valid loss:0.228903
Epoch:49, Train loss:0.217772, valid loss:0.228943
Epoch:50, Train loss:0.217781, valid loss:0.228926
Epoch:51, Train loss:0.217704, valid loss:0.228842
Epoch:52, Train loss:0.217696, valid loss:0.228903
Epoch:53, Train loss:0.217708, valid loss:0.228972
Epoch:54, Train loss:0.217699, valid loss:0.228856
Epoch:55, Train loss:0.217700, valid loss:0.228885
Epoch:56, Train loss:0.217695, valid loss:0.228871
Epoch:57, Train loss:0.217696, valid loss:0.228871
Epoch:58, Train loss:0.217698, valid loss:0.228866
Epoch:59, Train loss:0.217693, valid loss:0.228875
Epoch:60, Train loss:0.217691, valid loss:0.228845
training time 6253.835678100586
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.7302992106522446
plot_id,batch_id 0 1 miss% 0.7893557772224203
plot_id,batch_id 0 2 miss% 0.8006996839178917
plot_id,batch_id 0 3 miss% 0.8098503443078214
plot_id,batch_id 0 4 miss% 0.8077064078572921
plot_id,batch_id 0 5 miss% 0.7288988916367771
plot_id,batch_id 0 6 miss% 0.7834001111670293
plot_id,batch_id 0 7 miss% 0.7979688035983518
plot_id,batch_id 0 8 miss% 0.8042533414946352
plot_id,batch_id 0 9 miss% 0.8087206129407236
plot_id,batch_id 0 10 miss% 0.7052466255828402
plot_id,batch_id 0 11 miss% 0.7859575333116445
plot_id,batch_id 0 12 miss% 0.7974675941546792
plot_id,batch_id 0 13 miss% 0.8030108523325473
plot_id,batch_id 0 14 miss% 0.8076412061127127
plot_id,batch_id 0 15 miss% 0.7147789110544582
plot_id,batch_id 0 16 miss% 0.7773060846729489
plot_id,batch_id 0 17 miss% 0.7967218711630221
plot_id,batch_id 0 18 miss% 0.8041865146470061
plot_id,batch_id 0 19 miss% 0.8039664569016026
plot_id,batch_id 0 20 miss% 0.7662786318963996
plot_id,batch_id 0 21 miss% 0.8040098751261988
plot_id,batch_id 0 22 miss% 0.8116954098948083
plot_id,batch_id 0 23 miss% 0.8149477194599746
plot_id,batch_id 0 24 miss% 0.8147418305233456
plot_id,batch_id 0 25 miss% 0.7578476916120578
plot_id,batch_id 0 26 miss% 0.798447688990867
plot_id,batch_id 0 27 miss% 0.8042785969546714
plot_id,batch_id 0 28 miss% 0.8078752636921841
plot_id,batch_id 0 29 miss% 0.8093482966862232
plot_id,batch_id 0 30 miss% 0.7593460379776821
plot_id,batch_id 0 31 miss% 0.7926911040642982
plot_id,batch_id 0 32 miss% 0.8030152685923182
plot_id,batch_id 0 33 miss% 0.8066295523516096
plot_id,batch_id 0 34 miss% 0.8082546287268024
plot_id,batch_id 0 35 miss% 0.7498685170740111
plot_id,batch_id 0 36 miss% 0.8011205859237003
plot_id,batch_id 0 37 miss% 0.8034595126740665
plot_id,batch_id 0 38 miss% 0.8098995839633881
plot_id,batch_id 0 39 miss% 0.8112935762790351
plot_id,batch_id 0 40 miss% 0.7859460803477478
plot_id,batch_id 0 41 miss% 0.8059493766568683
plot_id,batch_id 0 42 miss% 0.8119846776413182
plot_id,batch_id 0 43 miss% 0.8150845155845068
plot_id,batch_id 0 44 miss% 0.8200829548218999
plot_id,batch_id 0 45 miss% 0.7775698224118642
plot_id,batch_id 0 46 miss% 0.8091804012336388
plot_id,batch_id 0 47 miss% 0.8126625334120282
plot_id,batch_id 0 48 miss% 0.8136229552978479
plot_id,batch_id 0 49 miss% 0.819840632756591
plot_id,batch_id 0 50 miss% 0.784595252066902
plot_id,batch_id 0 51 miss% 0.8020940733790503
plot_id,batch_id 0 52 miss% 0.8084278600498843
plot_id,batch_id 0 53 miss% 0.8121960184566025
plot_id,batch_id 0 54 miss% 0.819831974561169
plot_id,batch_id 0 55 miss% 0.7655885187786967
plot_id,batch_id 0 56 miss% 0.8040625421815012
plot_id,batch_id 0 57 miss% 0.8102376419692183
plot_id,batch_id 0 58 miss% 0.8141713398853151
plot_id,batch_id 0 59 miss% 0.8187728260393289
plot_id,batch_id 0 60 miss% 0.6520400229767327
plot_id,batch_id 0 61 miss% 0.7504704965170376
plot_id,batch_id 0 62 miss% 0.7857379310469734
plot_id,batch_id 0 63 miss% 0.7968276229573258
plot_id,batch_id 0 64 miss% 0.8057432697938206
plot_id,batch_id 0 65 miss% 0.6413857580394994
plot_id,batch_id 0 66 miss% 0.748529590022785
plot_id,batch_id 0 67 miss% 0.771417771570413
plot_id,batch_id 0 68 miss% 0.7956834711538814
plot_id,batch_id 0 69 miss% 0.794643657503443
plot_id,batch_id 0 70 miss% 0.6140569826670761
plot_id,batch_id 0 71 miss% 0.7579680551668948
plot_id,batch_id 0 72 miss% 0.7640495409892935
plot_id,batch_id 0 73 miss% 0.7792542874963692
plot_id,batch_id 0 74 miss% 0.791498821666715
plot_id,batch_id 0 75 miss% 0.6128205998454735
plot_id,batch_id 0 76 miss% 0.7228287726608095
plot_id,batch_id 0 77 miss% 0.7538557235780304
plot_id,batch_id 0 78 miss% 0.7828088630472897
plot_id,batch_id 0 79 miss% 0.7846540972338532
plot_id,batch_id 0 80 miss% 0.6727409229213007
plot_id,batch_id 0 81 miss% 0.7776978107503644
plot_id,batch_id 0 82 miss% 0.7925233075485473
plot_id,batch_id 0 83 miss% 0.8003131381170661
plot_id,batch_id 0 84 miss% 0.8002083256930255
plot_id,batch_id 0 85 miss% 0.6721701113104306
plot_id,batch_id 0 86 miss% 0.7701339535670004
plot_id,batch_id 0 87 miss% 0.7909431505637058
plot_id,batch_id 0 88 miss% 0.7972751790720584
plot_id,batch_id 0 89 miss% 0.799455280018381
plot_id,batch_id 0 90 miss% 0.6368230541467785
plot_id,batch_id 0 91 miss% 0.7656377302883945
plot_id,batch_id 0 92 miss% 0.7787612005022837
plot_id,batch_id 0 93 miss% 0.7861390671432352
plot_id,batch_id 0 94 miss% 0.798528470217148
plot_id,batch_id 0 95 miss% 0.6411313897816269
plot_id,batch_id 0 96 miss% 0.7499695179666708
plot_id,batch_id 0 97 miss% 0.7779482926902647
plot_id,batch_id 0 98 miss% 0.7860095883711649
plot_id,batch_id 0 99 miss% 0.795795465607957
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73029921 0.78935578 0.80069968 0.80985034 0.80770641 0.72889889
 0.78340011 0.7979688  0.80425334 0.80872061 0.70524663 0.78595753
 0.79746759 0.80301085 0.80764121 0.71477891 0.77730608 0.79672187
 0.80418651 0.80396646 0.76627863 0.80400988 0.81169541 0.81494772
 0.81474183 0.75784769 0.79844769 0.8042786  0.80787526 0.8093483
 0.75934604 0.7926911  0.80301527 0.80662955 0.80825463 0.74986852
 0.80112059 0.80345951 0.80989958 0.81129358 0.78594608 0.80594938
 0.81198468 0.81508452 0.82008295 0.77756982 0.8091804  0.81266253
 0.81362296 0.81984063 0.78459525 0.80209407 0.80842786 0.81219602
 0.81983197 0.76558852 0.80406254 0.81023764 0.81417134 0.81877283
 0.65204002 0.7504705  0.78573793 0.79682762 0.80574327 0.64138576
 0.74852959 0.77141777 0.79568347 0.79464366 0.61405698 0.75796806
 0.76404954 0.77925429 0.79149882 0.6128206  0.72282877 0.75385572
 0.78280886 0.7846541  0.67274092 0.77769781 0.79252331 0.80031314
 0.80020833 0.67217011 0.77013395 0.79094315 0.79727518 0.79945528
 0.63682305 0.76563773 0.7787612  0.78613907 0.79852847 0.64113139
 0.74996952 0.77794829 0.78600959 0.79579547]
for model  218 the mean error 0.7780086849693539
all id 218 hidden_dim 16 learning_rate 0.02 num_layers 3 frames 31 out win 5 err 0.7780086849693539
Launcher: Job 219 completed in 6431 seconds.
Launcher: Task 229 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  79249
Epoch:0, Train loss:0.448520, valid loss:0.420811
Epoch:1, Train loss:0.033118, valid loss:0.005775
Epoch:2, Train loss:0.008669, valid loss:0.003758
Epoch:3, Train loss:0.006360, valid loss:0.004067
Epoch:4, Train loss:0.005136, valid loss:0.002689
Epoch:5, Train loss:0.004304, valid loss:0.002978
Epoch:6, Train loss:0.003812, valid loss:0.002070
Epoch:7, Train loss:0.003417, valid loss:0.001819
Epoch:8, Train loss:0.003084, valid loss:0.001551
Epoch:9, Train loss:0.002852, valid loss:0.001486
Epoch:10, Train loss:0.002691, valid loss:0.001592
Epoch:11, Train loss:0.002005, valid loss:0.001108
Epoch:12, Train loss:0.001969, valid loss:0.001384
Epoch:13, Train loss:0.001900, valid loss:0.001182
Epoch:14, Train loss:0.001853, valid loss:0.001163
Epoch:15, Train loss:0.001819, valid loss:0.001013
Epoch:16, Train loss:0.001782, valid loss:0.001123
Epoch:17, Train loss:0.001731, valid loss:0.001021
Epoch:18, Train loss:0.001729, valid loss:0.001129
Epoch:19, Train loss:0.001645, valid loss:0.001023
Epoch:20, Train loss:0.001575, valid loss:0.000920
Epoch:21, Train loss:0.001286, valid loss:0.000886
Epoch:22, Train loss:0.001266, valid loss:0.000903
Epoch:23, Train loss:0.001266, valid loss:0.000866
Epoch:24, Train loss:0.001233, valid loss:0.000813
Epoch:25, Train loss:0.001241, valid loss:0.000806
Epoch:26, Train loss:0.001221, valid loss:0.000777
Epoch:27, Train loss:0.001211, valid loss:0.000903
Epoch:28, Train loss:0.001220, valid loss:0.000765
Epoch:29, Train loss:0.001191, valid loss:0.000853
Epoch:30, Train loss:0.001163, valid loss:0.000808
Epoch:31, Train loss:0.001014, valid loss:0.000754
Epoch:32, Train loss:0.001010, valid loss:0.000758
Epoch:33, Train loss:0.000996, valid loss:0.000733
Epoch:34, Train loss:0.000990, valid loss:0.000738
Epoch:35, Train loss:0.000991, valid loss:0.000727
Epoch:36, Train loss:0.000975, valid loss:0.000731
Epoch:37, Train loss:0.000986, valid loss:0.000732
Epoch:38, Train loss:0.000968, valid loss:0.000712
Epoch:39, Train loss:0.000986, valid loss:0.000738
Epoch:40, Train loss:0.000959, valid loss:0.000717
Epoch:41, Train loss:0.000891, valid loss:0.000684
Epoch:42, Train loss:0.000888, valid loss:0.000706
Epoch:43, Train loss:0.000883, valid loss:0.000698
Epoch:44, Train loss:0.000884, valid loss:0.000704
Epoch:45, Train loss:0.000881, valid loss:0.000675
Epoch:46, Train loss:0.000877, valid loss:0.000694
Epoch:47, Train loss:0.000876, valid loss:0.000699
Epoch:48, Train loss:0.000872, valid loss:0.000697
Epoch:49, Train loss:0.000868, valid loss:0.000701
Epoch:50, Train loss:0.000863, valid loss:0.000684
Epoch:51, Train loss:0.000833, valid loss:0.000683
Epoch:52, Train loss:0.000831, valid loss:0.000688
Epoch:53, Train loss:0.000830, valid loss:0.000678
Epoch:54, Train loss:0.000826, valid loss:0.000679
Epoch:55, Train loss:0.000827, valid loss:0.000681
Epoch:56, Train loss:0.000826, valid loss:0.000693
Epoch:57, Train loss:0.000823, valid loss:0.000670
Epoch:58, Train loss:0.000822, valid loss:0.000675
Epoch:59, Train loss:0.000826, valid loss:0.000663
Epoch:60, Train loss:0.000821, valid loss:0.000668
training time 6246.660947799683
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.030800186047767283
plot_id,batch_id 0 1 miss% 0.029166681284460063
plot_id,batch_id 0 2 miss% 0.02450030802269493
plot_id,batch_id 0 3 miss% 0.020697936954830295
plot_id,batch_id 0 4 miss% 0.024334401464019734
plot_id,batch_id 0 5 miss% 0.030972712591167524
plot_id,batch_id 0 6 miss% 0.029295110978872416
plot_id,batch_id 0 7 miss% 0.022015545580571904
plot_id,batch_id 0 8 miss% 0.02962946588894136
plot_id,batch_id 0 9 miss% 0.018829412119590175
plot_id,batch_id 0 10 miss% 0.04640032969100198
plot_id,batch_id 0 11 miss% 0.03725521459354909
plot_id,batch_id 0 12 miss% 0.02839279010684368
plot_id,batch_id 0 13 miss% 0.028021752034528263
plot_id,batch_id 0 14 miss% 0.031220659864479857
plot_id,batch_id 0 15 miss% 0.031004535727970118
plot_id,batch_id 0 16 miss% 0.03932046285194925
plot_id,batch_id 0 17 miss% 0.040412297850145835
plot_id,batch_id 0 18 miss% 0.03232066124015933
plot_id,batch_id 0 19 miss% 0.033482851941020034
plot_id,batch_id 0 20 miss% 0.048390768832203314
plot_id,batch_id 0 21 miss% 0.01430719623326043
plot_id,batch_id 0 22 miss% 0.032439507740890335
plot_id,batch_id 0 23 miss% 0.028088944895157508
plot_id,batch_id 0 24 miss% 0.020744613172641566
plot_id,batch_id 0 25 miss% 0.023720233494139552
plot_id,batch_id 0 26 miss% 0.03135106161604632
plot_id,batch_id 0 27 miss% 0.020116548935517186
plot_id,batch_id 0 28 miss% 0.020310307053966522
plot_id,batch_id 0 29 miss% 0.02624309250971546
plot_id,batch_id 0 30 miss% 0.05702174881688054
plot_id,batch_id 0 31 miss% 0.022017046040693468
plot_id,batch_id 0 32 miss% 0.02904721455506205
plot_id,batch_id 0 33 miss% 0.02747399680356066
plot_id,batch_id 0 34 miss% 0.02359719039308776
plot_id,batch_id 0 35 miss% 0.036218802307030534
plot_id,batch_id 0 36 miss% 0.029762758792596083
plot_id,batch_id 0 37 miss% 0.04790138240686473
plot_id,batch_id 0 38 miss% 0.015217414542432058
plot_id,batch_id 0 39 miss% 0.01878489856557081
plot_id,batch_id 0 40 miss% 0.06655987825504539
plot_id,batch_id 0 41 miss% 0.022083416540538622
plot_id,batch_id 0 42 miss% 0.020403809432649023
plot_id,batch_id 0 43 miss% 0.025999110596279608
plot_id,batch_id 0 44 miss% 0.02080365878131373
plot_id,batch_id 0 45 miss% 0.042332571664512784
plot_id,batch_id 0 46 miss% 0.01851974549055354
plot_id,batch_id 0 47 miss% 0.021488836773375028
plot_id,batch_id 0 48 miss% 0.019050258273735892
plot_id,batch_id 0 49 miss% 0.015646902553342838
plot_id,batch_id 0 50 miss% 0.024820202294743264
plot_id,batch_id 0 51 miss% 0.019282546364171572
plot_id,batch_id 0 52 miss% 0.01771117448696013
plot_id,batch_id 0 53 miss% 0.021316943121351222
plot_id,batch_id 0 54 miss% 0.02611879602949317
plot_id,batch_id 0 55 miss% 0.03028838049919414
plot_id,batch_id 0 56 miss% 0.02188103585431642
plot_id,batch_id 0 57 miss% 0.025283628258878397
plot_id,batch_id 0 58 miss% 0.02327004682936668
plot_id,batch_id 0 59 miss% 0.024728477164818876
plot_id,batch_id 0 60 miss% 0.04556495741003787
plot_id,batch_id 0 61 miss% 0.03566370484908802
plot_id,batch_id 0 62 miss% 0.03077167334890828
plot_id,batch_id 0 63 miss% 0.041648381695606175
plot_id,batch_id 0 64 miss% 0.02989083027400558
plot_id,batch_id 0 65 miss% 0.035166729312448505
plot_id,batch_id 0 66 miss% 0.04916635360761707
plot_id,batch_id 0 67 miss% 0.03546605204782353
plot_id,batch_id 0 68 miss% 0.027056608208498888
0.035847026956424784
plot_id,batch_id 0 70 miss% 0.028302911414113806
plot_id,batch_id 0 71 miss% 0.05519082187690401
plot_id,batch_id 0 72 miss% 0.032637500119163144
plot_id,batch_id 0 73 miss% 0.03145976518957808
plot_id,batch_id 0 74 miss% 0.04690085158245823
plot_id,batch_id 0 75 miss% 0.03955638391592236
plot_id,batch_id 0 76 miss% 0.05508567554869008
plot_id,batch_id 0 77 miss% 0.027888705826912843
plot_id,batch_id 0 78 miss% 0.037869407712960323
plot_id,batch_id 0 79 miss% 0.04494286678392703
plot_id,batch_id 0 80 miss% 0.038510507484764686
plot_id,batch_id 0 81 miss% 0.024526029144824876
plot_id,batch_id 0 82 miss% 0.019338719409220825
plot_id,batch_id 0 83 miss% 0.03097439100472825
plot_id,batch_id 0 84 miss% 0.026419284255890702
plot_id,batch_id 0 85 miss% 0.03355735256585796
plot_id,batch_id 0 86 miss% 0.020406848721843156
plot_id,batch_id 0 87 miss% 0.025917440879850477
plot_id,batch_id 0 88 miss% 0.046427602228133254
plot_id,batch_id 0 89 miss% 0.023852210399897815
plot_id,batch_id 0 90 miss% 0.040134732510862005
plot_id,batch_id 0 91 miss% 0.027222651438095925
plot_id,batch_id 0 92 miss% 0.024589063483065925
plot_id,batch_id 0 93 miss% 0.023745312663428622
plot_id,batch_id 0 94 miss% 0.03391508684899895
plot_id,batch_id 0 95 miss% 0.053991103261573165
plot_id,batch_id 0 96 miss% 0.03300517207897568
plot_id,batch_id 0 97 miss% 0.05997853574973983
plot_id,batch_id 0 98 miss% 0.029219240576140254
plot_id,batch_id 0 99 miss% 0.03454524635614128
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01633691 0.03074447 0.02084831 0.02528043 0.02594446 0.0346445
 0.02051335 0.02146394 0.02326726 0.02815624 0.03648729 0.04439828
 0.01515759 0.02614971 0.02879321 0.05262899 0.02699241 0.03461124
 0.02350676 0.03564476 0.03471984 0.01129099 0.02485711 0.02475976
 0.02813709 0.02760615 0.0417572  0.02129923 0.02469508 0.02428932
 0.06150022 0.02223718 0.02352139 0.02709538 0.02404883 0.04213907
 0.04479601 0.02771851 0.02773147 0.02204304 0.07754106 0.0193571
 0.01117164 0.02905985 0.01187137 0.02387991 0.02322744 0.01812352
 0.02019721 0.01929629 0.03752876 0.0193724  0.02475208 0.01095941
 0.03720341 0.04050998 0.03495385 0.02385007 0.0246704  0.02117071
 0.04956325 0.03740964 0.02637261 0.03260193 0.02688406 0.04859003
 0.0505492  0.02970059 0.02784501 0.03584703 0.02830291 0.05519082
 0.0326375  0.03145977 0.04690085 0.03955638 0.05508568 0.02788871
 0.03786941 0.04494287 0.03851051 0.02452603 0.01933872 0.03097439
 0.02641928 0.03355735 0.02040685 0.02591744 0.0464276  0.02385221
 0.04013473 0.02722265 0.02458906 0.02374531 0.03391509 0.0539911
 0.03300517 0.05997854 0.02921924 0.03454525]
for model  15 the mean error 0.03101984183463745
all id 15 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 21 out win 3 err 0.03101984183463745
Launcher: Job 16 completed in 6441 seconds.
Launcher: Task 89 done. Exiting.
plot_id,batch_id 0 69 miss% 0.02656561310090806
plot_id,batch_id 0 70 miss% 0.03263976388890829
plot_id,batch_id 0 71 miss% 0.06097149349302881
plot_id,batch_id 0 72 miss% 0.025185957533999482
plot_id,batch_id 0 73 miss% 0.025894077049295444
plot_id,batch_id 0 74 miss% 0.026293584898867687
plot_id,batch_id 0 75 miss% 0.05170500443322234
plot_id,batch_id 0 76 miss% 0.04027973490812921
plot_id,batch_id 0 77 miss% 0.02857751010267861
plot_id,batch_id 0 78 miss% 0.038479928733435705
plot_id,batch_id 0 79 miss% 0.0402012948769879
plot_id,batch_id 0 80 miss% 0.049104270206272005
plot_id,batch_id 0 81 miss% 0.017272877601767832
plot_id,batch_id 0 82 miss% 0.031049377294098002
plot_id,batch_id 0 83 miss% 0.025773336899613798
plot_id,batch_id 0 84 miss% 0.020584968929398206
plot_id,batch_id 0 85 miss% 0.047472900144568037
plot_id,batch_id 0 86 miss% 0.03074382938068472
plot_id,batch_id 0 87 miss% 0.02273186845532561
plot_id,batch_id 0 88 miss% 0.023155532636425943
plot_id,batch_id 0 89 miss% 0.020016311701357287
plot_id,batch_id 0 90 miss% 0.03994246715519884
plot_id,batch_id 0 91 miss% 0.03997148025540928
plot_id,batch_id 0 92 miss% 0.028124649235168386
plot_id,batch_id 0 93 miss% 0.026730238523508733
plot_id,batch_id 0 94 miss% 0.03469756395463304
plot_id,batch_id 0 95 miss% 0.04836630438587675
plot_id,batch_id 0 96 miss% 0.02957264020403861
plot_id,batch_id 0 97 miss% 0.04588368148361488
plot_id,batch_id 0 98 miss% 0.0315269921131044
plot_id,batch_id 0 99 miss% 0.030799852002887724
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03080019 0.02916668 0.02450031 0.02069794 0.0243344  0.03097271
 0.02929511 0.02201555 0.02962947 0.01882941 0.04640033 0.03725521
 0.02839279 0.02802175 0.03122066 0.03100454 0.03932046 0.0404123
 0.03232066 0.03348285 0.04839077 0.0143072  0.03243951 0.02808894
 0.02074461 0.02372023 0.03135106 0.02011655 0.02031031 0.02624309
 0.05702175 0.02201705 0.02904721 0.027474   0.02359719 0.0362188
 0.02976276 0.04790138 0.01521741 0.0187849  0.06655988 0.02208342
 0.02040381 0.02599911 0.02080366 0.04233257 0.01851975 0.02148884
 0.01905026 0.0156469  0.0248202  0.01928255 0.01771117 0.02131694
 0.0261188  0.03028838 0.02188104 0.02528363 0.02327005 0.02472848
 0.04556496 0.0356637  0.03077167 0.04164838 0.02989083 0.03516673
 0.04916635 0.03546605 0.02705661 0.02656561 0.03263976 0.06097149
 0.02518596 0.02589408 0.02629358 0.051705   0.04027973 0.02857751
 0.03847993 0.04020129 0.04910427 0.01727288 0.03104938 0.02577334
 0.02058497 0.0474729  0.03074383 0.02273187 0.02315553 0.02001631
 0.03994247 0.03997148 0.02812465 0.02673024 0.03469756 0.0483663
 0.02957264 0.04588368 0.03152699 0.03079985]
for model  87 the mean error 0.030591278601429676
all id 87 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 25 out win 3 err 0.030591278601429676
Launcher: Job 88 completed in 6442 seconds.
Launcher: Task 0 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  28945
Epoch:0, Train loss:0.462343, valid loss:0.455370
Epoch:1, Train loss:0.045614, valid loss:0.011660
Epoch:2, Train loss:0.013788, valid loss:0.005157
Epoch:3, Train loss:0.009674, valid loss:0.005276
Epoch:4, Train loss:0.008250, valid loss:0.004293
Epoch:5, Train loss:0.007355, valid loss:0.003571
Epoch:6, Train loss:0.006613, valid loss:0.003155
Epoch:7, Train loss:0.005723, valid loss:0.002546
Epoch:8, Train loss:0.004959, valid loss:0.002283
Epoch:9, Train loss:0.004574, valid loss:0.002293
Epoch:10, Train loss:0.004314, valid loss:0.002280
Epoch:11, Train loss:0.003361, valid loss:0.001874
Epoch:12, Train loss:0.003281, valid loss:0.001826
Epoch:13, Train loss:0.003222, valid loss:0.001671
Epoch:14, Train loss:0.003147, valid loss:0.001858
Epoch:15, Train loss:0.003052, valid loss:0.001803
Epoch:16, Train loss:0.003014, valid loss:0.001855
Epoch:17, Train loss:0.002957, valid loss:0.001583
Epoch:18, Train loss:0.002999, valid loss:0.001540
Epoch:19, Train loss:0.002888, valid loss:0.001424
Epoch:20, Train loss:0.002779, valid loss:0.001428
Epoch:21, Train loss:0.002357, valid loss:0.001310
Epoch:22, Train loss:0.002311, valid loss:0.001266
Epoch:23, Train loss:0.002346, valid loss:0.001318
Epoch:24, Train loss:0.002320, valid loss:0.001279
Epoch:25, Train loss:0.002246, valid loss:0.001333
Epoch:26, Train loss:0.002254, valid loss:0.001321
Epoch:27, Train loss:0.002243, valid loss:0.001306
Epoch:28, Train loss:0.002232, valid loss:0.001252
Epoch:29, Train loss:0.002177, valid loss:0.001348
Epoch:30, Train loss:0.002186, valid loss:0.001366
Epoch:31, Train loss:0.001963, valid loss:0.001132
Epoch:32, Train loss:0.001948, valid loss:0.001172
Epoch:33, Train loss:0.001953, valid loss:0.001131
Epoch:34, Train loss:0.001933, valid loss:0.001178
Epoch:35, Train loss:0.001920, valid loss:0.001145
Epoch:36, Train loss:0.001916, valid loss:0.001133
Epoch:37, Train loss:0.001905, valid loss:0.001140
Epoch:38, Train loss:0.001896, valid loss:0.001072
Epoch:39, Train loss:0.001871, valid loss:0.001091
Epoch:40, Train loss:0.001871, valid loss:0.001082
Epoch:41, Train loss:0.001761, valid loss:0.001053
Epoch:42, Train loss:0.001765, valid loss:0.001058
Epoch:43, Train loss:0.001748, valid loss:0.001101
Epoch:44, Train loss:0.001747, valid loss:0.001024
Epoch:45, Train loss:0.001735, valid loss:0.001041
Epoch:46, Train loss:0.001736, valid loss:0.001018
Epoch:47, Train loss:0.001733, valid loss:0.001027
Epoch:48, Train loss:0.001728, valid loss:0.001036
Epoch:49, Train loss:0.001720, valid loss:0.001011
Epoch:50, Train loss:0.001715, valid loss:0.001051
Epoch:51, Train loss:0.001657, valid loss:0.001016
Epoch:52, Train loss:0.001654, valid loss:0.000988
Epoch:53, Train loss:0.001655, valid loss:0.000989
Epoch:54, Train loss:0.001650, valid loss:0.001012
Epoch:55, Train loss:0.001646, valid loss:0.001022
Epoch:56, Train loss:0.001647, valid loss:0.001013
Epoch:57, Train loss:0.001639, valid loss:0.000996
Epoch:58, Train loss:0.001644, valid loss:0.001022
Epoch:59, Train loss:0.001632, valid loss:0.000999
Epoch:60, Train loss:0.001630, valid loss:0.000983
training time 6254.586541652679
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.033700633656625706
plot_id,batch_id 0 1 miss% 0.025194697528286965
plot_id,batch_id 0 2 miss% 0.020074663902669113
plot_id,batch_id 0 3 miss% 0.017796438292277592
plot_id,batch_id 0 4 miss% 0.028029689097743414
plot_id,batch_id 0 5 miss% 0.03025072398586441
plot_id,batch_id 0 6 miss% 0.023203269189718132
plot_id,batch_id 0 7 miss% 0.02670087414558705
plot_id,batch_id 0 8 miss% 0.03057023928134282
plot_id,batch_id 0 9 miss% 0.03155424633028683
plot_id,batch_id 0 10 miss% 0.05668920860396568
plot_id,batch_id 0 11 miss% 0.03436964888346791
plot_id,batch_id 0 12 miss% 0.03338176089025459
plot_id,batch_id 0 13 miss% 0.03794149670408563
plot_id,batch_id 0 14 miss% 0.042243376641334596
plot_id,batch_id 0 15 miss% 0.03558116929909529
plot_id,batch_id 0 16 miss% 0.02459856730194089
plot_id,batch_id 0 17 miss% 0.04659165407067754
plot_id,batch_id 0 18 miss% 0.029044864335815514
plot_id,batch_id 0 19 miss% 0.0429139875970773
plot_id,batch_id 0 20 miss% 0.06608792957181887
plot_id,batch_id 0 21 miss% 0.018343938238385243
plot_id,batch_id 0 22 miss% 0.039951548422267794
plot_id,batch_id 0 23 miss% 0.02382927644124368
plot_id,batch_id 0 24 miss% 0.035415633465613765
plot_id,batch_id 0 25 miss% 0.03515092944651316
plot_id,batch_id 0 26 miss% 0.02967611044009914
plot_id,batch_id 0 27 miss% 0.03713705255120609
plot_id,batch_id 0 28 miss% 0.028056773578256356
plot_id,batch_id 0 29 miss% 0.03703220351737273
plot_id,batch_id 0 30 miss% 0.04393561091674949
plot_id,batch_id 0 31 miss% 0.03431619700462224
plot_id,batch_id 0 32 miss% 0.04734644485135411
plot_id,batch_id 0 33 miss% 0.04513344661542965
plot_id,batch_id 0 34 miss% 0.04366015057997742
plot_id,batch_id 0 35 miss% 0.03744336305192948
plot_id,batch_id 0 36 miss% 0.04697236774125953
plot_id,batch_id 0 37 miss% 0.03171242226149765
plot_id,batch_id 0 38 miss% 0.02618759539725092
plot_id,batch_id 0 39 miss% 0.022298738841032874
plot_id,batch_id 0 40 miss% 0.04508162697876934
plot_id,batch_id 0 41 miss% 0.03685371232231516
plot_id,batch_id 0 42 miss% 0.03186770489448701
plot_id,batch_id 0 43 miss% 0.04726854810920222
plot_id,batch_id 0 44 miss% 0.033815894929037874
plot_id,batch_id 0 45 miss% 0.022085136885506645
plot_id,batch_id 0 46 miss% 0.031210762582763278
plot_id,batch_id 0 47 miss% 0.038120689502157516
plot_id,batch_id 0 48 miss% 0.031529817553347196
plot_id,batch_id 0 49 miss% 0.02818073540610293
plot_id,batch_id 0 50 miss% 0.03841509799813605
plot_id,batch_id 0 51 miss% 0.03952106087640251
plot_id,batch_id 0 52 miss% 0.03424834157483708
plot_id,batch_id 0 53 miss% 0.0322432986333712
plot_id,batch_id 0 54 miss% 0.03619257434675206
plot_id,batch_id 0 55 miss% 0.04778908793519963
plot_id,batch_id 0 56 miss% 0.04893879771830241
plot_id,batch_id 0 57 miss% 0.031234630641252533
plot_id,batch_id 0 58 miss% 0.034202547011956484
plot_id,batch_id 0 59 miss% 0.039054924127625595
plot_id,batch_id 0 60 miss% 0.04639913464972472
plot_id,batch_id 0 61 miss% 0.03739570590359256
plot_id,batch_id 0 62 miss% 0.020445791312850976
plot_id,batch_id 0 63 miss% 0.04565822071164346
plot_id,batch_id 0 64 miss% 0.04135883853418953
plot_id,batch_id 0 65 miss% 0.05078595990850051
plot_id,batch_id 0 66 miss% 0.06088370256045749
plot_id,batch_id 0 67 miss% 0.03986347524388763
plot_id,batch_id 0 68 miss% 0.04774936714779289
plot_id,batch_id 0 69 miss% 0.03798559607131538
plot_id,batch_id 0 70 miss% 0.056119881029216105
plot_id,batch_id 0 71 miss% 0.04029960451528358
plot_id,batch_id 0 72 miss% 0.031862241604835716
plot_id,batch_id 0 73 miss% 0.03128499458474698
plot_id,batch_id 0 74 miss% 0.05009978814349358
plot_id,batch_id 0 75 miss% 0.0590539283840963
plot_id,batch_id 0 76 miss% 0.03196082186843241
plot_id,batch_id 0 77 miss% 0.024002865103237153
plot_id,batch_id 0 78 miss% 0.03749168411111973
plot_id,batch_id 0 79 miss% 0.04467931915232205
plot_id,batch_id 0 80 miss% 0.047822876332307744
plot_id,batch_id 0 81 miss% 0.029806644248538683
plot_id,batch_id 0 82 miss% 0.03275611583709809
plot_id,batch_id 0 83 miss% 0.032111756005369245
plot_id,batch_id 0 84 miss% 0.03372496200569971
plot_id,batch_id 0 85 miss% 0.07460915518456125
plot_id,batch_id 0 86 miss% 0.036234618036822326
plot_id,batch_id 0 87 miss% 0.03666103488973598
plot_id,batch_id 0 88 miss% 0.03729859271403249
plot_id,batch_id 0 89 miss% 0.044457852660458534
plot_id,batch_id 0 90 miss% 0.04073139658175628
plot_id,batch_id 0 91 miss% 0.04500627531995218
plot_id,batch_id 0 92 miss% 0.03564197443695833
plot_id,batch_id 0 93 miss% 0.031027014063025684
plot_id,batch_id 0 94 miss% 0.029153906210022496
plot_id,batch_id 0 95 miss% 0.03852736716985637
plot_id,batch_id 0 96 miss% 0.04116424283445114
plot_id,batch_id 0 97 miss% 0.043598780696948614
plot_id,batch_id 0 98 miss% 0.045110905191685494
plot_id,batch_id 0 99 miss% 0.04621976336802659
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03370063 0.0251947  0.02007466 0.01779644 0.02802969 0.03025072
 0.02320327 0.02670087 0.03057024 0.03155425 0.05668921 0.03436965
 0.03338176 0.0379415  0.04224338 0.03558117 0.02459857 0.04659165
 0.02904486 0.04291399 0.06608793 0.01834394 0.03995155 0.02382928
 0.03541563 0.03515093 0.02967611 0.03713705 0.02805677 0.0370322
 0.04393561 0.0343162  0.04734644 0.04513345 0.04366015 0.03744336
 0.04697237 0.03171242 0.0261876  0.02229874 0.04508163 0.03685371
 0.0318677  0.04726855 0.03381589 0.02208514 0.03121076 0.03812069
 0.03152982 0.02818074 0.0384151  0.03952106 0.03424834 0.0322433
 0.03619257 0.04778909 0.0489388  0.03123463 0.03420255 0.03905492
 0.04639913 0.03739571 0.02044579 0.04565822 0.04135884 0.05078596
 0.0608837  0.03986348 0.04774937 0.0379856  0.05611988 0.0402996
 0.03186224 0.03128499 0.05009979 0.05905393 0.03196082 0.02400287
 0.03749168 0.04467932 0.04782288 0.02980664 0.03275612 0.03211176
 0.03372496 0.07460916 0.03623462 0.03666103 0.03729859 0.04445785
 0.0407314  0.04500628 0.03564197 0.03102701 0.02915391 0.03852737
 0.04116424 0.04359878 0.04511091 0.04621976]
for model  92 the mean error 0.037450200870275674
all id 92 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 25 out win 5 err 0.037450200870275674
Launcher: Job 93 completed in 6461 seconds.
Launcher: Task 24 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  35921
Epoch:0, Train loss:0.736710, valid loss:0.719174
Epoch:1, Train loss:0.073067, valid loss:0.025174
Epoch:2, Train loss:0.037750, valid loss:0.009167
Epoch:3, Train loss:0.018340, valid loss:0.007298
Epoch:4, Train loss:0.014987, valid loss:0.006782
Epoch:5, Train loss:0.012740, valid loss:0.005742
Epoch:6, Train loss:0.011158, valid loss:0.005081
Epoch:7, Train loss:0.009846, valid loss:0.005706
Epoch:8, Train loss:0.009030, valid loss:0.005828
Epoch:9, Train loss:0.008189, valid loss:0.004275
Epoch:10, Train loss:0.007790, valid loss:0.004163
Epoch:11, Train loss:0.005983, valid loss:0.003101
Epoch:12, Train loss:0.005758, valid loss:0.003007
Epoch:13, Train loss:0.005444, valid loss:0.002933
Epoch:14, Train loss:0.005291, valid loss:0.003093
Epoch:15, Train loss:0.005138, valid loss:0.002742
Epoch:16, Train loss:0.004886, valid loss:0.003070
Epoch:17, Train loss:0.004794, valid loss:0.002673
Epoch:18, Train loss:0.004686, valid loss:0.002674
Epoch:19, Train loss:0.004552, valid loss:0.002434
Epoch:20, Train loss:0.004454, valid loss:0.002642
Epoch:21, Train loss:0.003736, valid loss:0.002190
Epoch:22, Train loss:0.003676, valid loss:0.002249
Epoch:23, Train loss:0.003635, valid loss:0.002359
Epoch:24, Train loss:0.003604, valid loss:0.002132
Epoch:25, Train loss:0.003525, valid loss:0.002266
Epoch:26, Train loss:0.003462, valid loss:0.002057
Epoch:27, Train loss:0.003471, valid loss:0.002108
Epoch:28, Train loss:0.003404, valid loss:0.002107
Epoch:29, Train loss:0.003335, valid loss:0.002161
Epoch:30, Train loss:0.003295, valid loss:0.002079
Epoch:31, Train loss:0.002961, valid loss:0.001920
Epoch:32, Train loss:0.002945, valid loss:0.001921
Epoch:33, Train loss:0.002909, valid loss:0.001970
Epoch:34, Train loss:0.002898, valid loss:0.001877
Epoch:35, Train loss:0.002902, valid loss:0.001926
Epoch:36, Train loss:0.002871, valid loss:0.001879
Epoch:37, Train loss:0.002829, valid loss:0.001993
Epoch:38, Train loss:0.002819, valid loss:0.001932
Epoch:39, Train loss:0.002830, valid loss:0.001867
Epoch:40, Train loss:0.002800, valid loss:0.001833
Epoch:41, Train loss:0.002617, valid loss:0.001823
Epoch:42, Train loss:0.002599, valid loss:0.001826
Epoch:43, Train loss:0.002598, valid loss:0.001771
Epoch:44, Train loss:0.002589, valid loss:0.001755
Epoch:45, Train loss:0.002586, valid loss:0.001759
Epoch:46, Train loss:0.002568, valid loss:0.001801
Epoch:47, Train loss:0.002570, valid loss:0.001801
Epoch:48, Train loss:0.002573, valid loss:0.001790
Epoch:49, Train loss:0.002537, valid loss:0.001795
Epoch:50, Train loss:0.002534, valid loss:0.001745
Epoch:51, Train loss:0.002449, valid loss:0.001759
Epoch:52, Train loss:0.002448, valid loss:0.001719
Epoch:53, Train loss:0.002442, valid loss:0.001714
Epoch:54, Train loss:0.002434, valid loss:0.001750
Epoch:55, Train loss:0.002436, valid loss:0.001777
Epoch:56, Train loss:0.002422, valid loss:0.001752
Epoch:57, Train loss:0.002423, valid loss:0.001766
Epoch:58, Train loss:0.002416, valid loss:0.001733
Epoch:59, Train loss:0.002406, valid loss:0.001775
Epoch:60, Train loss:0.002409, valid loss:0.001753
training time 6263.666149377823
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.033740187894800606
plot_id,batch_id 0 1 miss% 0.01888051449296446
plot_id,batch_id 0 2 miss% 0.027028025390756206
plot_id,batch_id 0 3 miss% 0.027291258140878672
plot_id,batch_id 0 4 miss% 0.03722125875150098
plot_id,batch_id 0 5 miss% 0.026647393067192823
plot_id,batch_id 0 6 miss% 0.035355268630437614
plot_id,batch_id 0 7 miss% 0.027123982912186363
plot_id,batch_id 0 8 miss% 0.03493735835963429
plot_id,batch_id 0 9 miss% 0.021766654512796562
plot_id,batch_id 0 10 miss% 0.04607476161788108
plot_id,batch_id 0 11 miss% 0.0600365439630901
plot_id,batch_id 0 12 miss% 0.020929538291253096
plot_id,batch_id 0 13 miss% 0.04103962384002634
plot_id,batch_id 0 14 miss% 0.04457253629412288
plot_id,batch_id 0 15 miss% 0.04612922079099738
plot_id,batch_id 0 16 miss% 0.03872517990963052
plot_id,batch_id 0 17 miss% 0.03200616596827305
plot_id,batch_id 0 18 miss% 0.03843663973802953
plot_id,batch_id 0 19 miss% 0.05176533334986004
plot_id,batch_id 0 20 miss% 0.04333267859365134
plot_id,batch_id 0 21 miss% 0.03789497003129587
plot_id,batch_id 0 22 miss% 0.024139515594735376
plot_id,batch_id 0 23 miss% 0.01979127572530324
plot_id,batch_id 0 24 miss% 0.024750198980364888
plot_id,batch_id 0 25 miss% 0.0389367891115601
plot_id,batch_id 0 26 miss% 0.027992381104703187
plot_id,batch_id 0 27 miss% 0.032592888431701675
plot_id,batch_id 0 28 miss% 0.017723574380087294
plot_id,batch_id 0 29 miss% 0.024703586419608774
plot_id,batch_id 0 30 miss% 0.048661709674508946
plot_id,batch_id 0 31 miss% 0.047881730711779584
plot_id,batch_id 0 32 miss% 0.03676500832776116
plot_id,batch_id 0 33 miss% 0.037634727223521056
plot_id,batch_id 0 34 miss% 0.042930398674670606
plot_id,batch_id 0 35 miss% 0.05022983101120803
plot_id,batch_id 0 36 miss% 0.06234111035749829
plot_id,batch_id 0 37 miss% 0.023069639953575846
plot_id,batch_id 0 38 miss% 0.019718031351383253
plot_id,batch_id 0 39 miss% 0.015783769189416912
plot_id,batch_id 0 40 miss% 0.06682736748865276
plot_id,batch_id 0 41 miss% 0.021353521463915857
plot_id,batch_id 0 42 miss% 0.023473938415723924
plot_id,batch_id 0 43 miss% 0.02512591510044854
plot_id,batch_id 0 44 miss% 0.023819165733524324
plot_id,batch_id 0 45 miss% 0.02372176426091434
plot_id,batch_id 0 46 miss% 0.01702485494962422
plot_id,batch_id 0 47 miss% 0.0239769021275534
plot_id,batch_id 0 48 miss% 0.019467667344487193
plot_id,batch_id 0 49 miss% 0.026337445226411765
plot_id,batch_id 0 50 miss% 0.036381761457384185
plot_id,batch_id 0 51 miss% 0.036118464011647734
plot_id,batch_id 0 52 miss% 0.027764932572370914
plot_id,batch_id 0 53 miss% 0.027164481628519276
plot_id,batch_id 0 54 miss% 0.02306436832566656
plot_id,batch_id 0 55 miss% 0.021572581987745133
plot_id,batch_id 0 56 miss% 0.039281634267610804
plot_id,batch_id 0 57 miss% 0.026029562966220905
plot_id,batch_id 0 58 miss% 0.025323461642418724
plot_id,batch_id 0 59 miss% 0.021978341366979348
plot_id,batch_id 0 60 miss% 0.029448931805117404
plot_id,batch_id 0 61 miss% 0.028085627586229366
plot_id,batch_id 0 62 miss% 0.021441766427294055
plot_id,batch_id 0 63 miss% 0.038267522231927996
plot_id,batch_id 0 64 miss% 0.05139348483536737
plot_id,batch_id 0 65 miss% 0.03286124136584868
plot_id,batch_id 0 66 miss% 0.05291094805798758
plot_id,batch_id 0 67 miss% 0.03971789103339301
plot_id,batch_id 0 68 miss% 0.04290791549672012
plot_id,batch_id 0 69 miss% the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  79249
Epoch:0, Train loss:0.598719, valid loss:0.574885
Epoch:1, Train loss:0.048056, valid loss:0.008788
Epoch:2, Train loss:0.013521, valid loss:0.006050
Epoch:3, Train loss:0.009661, valid loss:0.004158
Epoch:4, Train loss:0.007584, valid loss:0.003361
Epoch:5, Train loss:0.006604, valid loss:0.003878
Epoch:6, Train loss:0.006042, valid loss:0.003619
Epoch:7, Train loss:0.005381, valid loss:0.003719
Epoch:8, Train loss:0.005292, valid loss:0.003631
Epoch:9, Train loss:0.004858, valid loss:0.002936
Epoch:10, Train loss:0.004521, valid loss:0.002754
Epoch:11, Train loss:0.003179, valid loss:0.001999
Epoch:12, Train loss:0.003195, valid loss:0.001816
Epoch:13, Train loss:0.003094, valid loss:0.001980
Epoch:14, Train loss:0.003048, valid loss:0.002054
Epoch:15, Train loss:0.003029, valid loss:0.001829
Epoch:16, Train loss:0.002893, valid loss:0.001903
Epoch:17, Train loss:0.002915, valid loss:0.001719
Epoch:18, Train loss:0.002804, valid loss:0.001742
Epoch:19, Train loss:0.002787, valid loss:0.001554
Epoch:20, Train loss:0.002715, valid loss:0.002219
Epoch:21, Train loss:0.002012, valid loss:0.001434
Epoch:22, Train loss:0.001936, valid loss:0.001284
Epoch:23, Train loss:0.001972, valid loss:0.001359
Epoch:24, Train loss:0.001941, valid loss:0.001380
Epoch:25, Train loss:0.001941, valid loss:0.001297
Epoch:26, Train loss:0.001879, valid loss:0.001522
Epoch:27, Train loss:0.001879, valid loss:0.001397
Epoch:28, Train loss:0.001923, valid loss:0.001357
Epoch:29, Train loss:0.001850, valid loss:0.001309
Epoch:30, Train loss:0.001834, valid loss:0.001372
Epoch:31, Train loss:0.001498, valid loss:0.001124
Epoch:32, Train loss:0.001447, valid loss:0.001154
Epoch:33, Train loss:0.001458, valid loss:0.001159
Epoch:34, Train loss:0.001494, valid loss:0.001074
Epoch:35, Train loss:0.001427, valid loss:0.001113
Epoch:36, Train loss:0.001424, valid loss:0.001100
Epoch:37, Train loss:0.001434, valid loss:0.001097
Epoch:38, Train loss:0.001397, valid loss:0.001165
Epoch:39, Train loss:0.001429, valid loss:0.001109
Epoch:40, Train loss:0.001406, valid loss:0.001069
Epoch:41, Train loss:0.001219, valid loss:0.001019
Epoch:42, Train loss:0.001216, valid loss:0.001005
Epoch:43, Train loss:0.001205, valid loss:0.001050
Epoch:44, Train loss:0.001212, valid loss:0.001014
Epoch:45, Train loss:0.001202, valid loss:0.001092
Epoch:46, Train loss:0.001228, valid loss:0.000989
Epoch:47, Train loss:0.001199, valid loss:0.001014
Epoch:48, Train loss:0.001175, valid loss:0.001000
Epoch:49, Train loss:0.001182, valid loss:0.001013
Epoch:50, Train loss:0.001185, valid loss:0.000967
Epoch:51, Train loss:0.001101, valid loss:0.000973
Epoch:52, Train loss:0.001096, valid loss:0.000972
Epoch:53, Train loss:0.001101, valid loss:0.000964
Epoch:54, Train loss:0.001094, valid loss:0.000967
Epoch:55, Train loss:0.001094, valid loss:0.000946
Epoch:56, Train loss:0.001092, valid loss:0.000985
Epoch:57, Train loss:0.001085, valid loss:0.000954
Epoch:58, Train loss:0.001085, valid loss:0.000960
Epoch:59, Train loss:0.001085, valid loss:0.000948
Epoch:60, Train loss:0.001075, valid loss:0.000967
training time 6278.26490855217
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.025469908579289116
plot_id,batch_id 0 1 miss% 0.02416605310549103
plot_id,batch_id 0 2 miss% 0.01784335830879946
plot_id,batch_id 0 3 miss% 0.032079126856860804
plot_id,batch_id 0 4 miss% 0.017766638949226466
plot_id,batch_id 0 5 miss% 0.027699686286787886
plot_id,batch_id 0 6 miss% 0.022930815087885106
plot_id,batch_id 0 7 miss% 0.027647374597565923
plot_id,batch_id 0 8 miss% 0.023668374956219207
plot_id,batch_id 0 9 miss% 0.02402273100894266
plot_id,batch_id 0 10 miss% 0.029179278915778505
plot_id,batch_id 0 11 miss% 0.059360409139493545
plot_id,batch_id 0 12 miss% 0.02647063236527682
plot_id,batch_id 0 13 miss% 0.025547781151042254
plot_id,batch_id 0 14 miss% 0.031117855302435767
plot_id,batch_id 0 15 miss% 0.03442482617698974
plot_id,batch_id 0 16 miss% 0.03335733013218329
plot_id,batch_id 0 17 miss% 0.034320187006461826
plot_id,batch_id 0 18 miss% 0.03569731995200503
plot_id,batch_id 0 19 miss% 0.027619063816998295
plot_id,batch_id 0 20 miss% 0.04874811167937345
plot_id,batch_id 0 21 miss% 0.023216945761306763
plot_id,batch_id 0 22 miss% 0.020848646808861983
plot_id,batch_id 0 23 miss% 0.02428332349595557
plot_id,batch_id 0 24 miss% 0.026740743117419634
plot_id,batch_id 0 25 miss% 0.03183210490537313
plot_id,batch_id 0 26 miss% 0.016227961345062
plot_id,batch_id 0 27 miss% 0.017699176068140417
plot_id,batch_id 0 28 miss% 0.02448484276628708
plot_id,batch_id 0 29 miss% 0.023565455457810452
plot_id,batch_id 0 30 miss% 0.046473322294808314
plot_id,batch_id 0 31 miss% 0.03626039593581472
plot_id,batch_id 0 32 miss% 0.030929898943056065
plot_id,batch_id 0 33 miss% 0.02615911771156309
plot_id,batch_id 0 34 miss% 0.02374606021743227
plot_id,batch_id 0 35 miss% 0.03987564412804939
plot_id,batch_id 0 36 miss% 0.03722769967991781
plot_id,batch_id 0 37 miss% 0.023039579492645113
plot_id,batch_id 0 38 miss% 0.016793069455571838
plot_id,batch_id 0 39 miss% 0.02056508145675325
plot_id,batch_id 0 40 miss% 0.0610411851330399
plot_id,batch_id 0 41 miss% 0.0247600288804952
plot_id,batch_id 0 42 miss% 0.01795121582717813
plot_id,batch_id 0 43 miss% 0.02961009836269581
plot_id,batch_id 0 44 miss% 0.022351041438661755
plot_id,batch_id 0 45 miss% 0.023252585829050463
plot_id,batch_id 0 46 miss% 0.01399489411331108
plot_id,batch_id 0 47 miss% 0.023585256149560307
plot_id,batch_id 0 48 miss% 0.022683462943613508
plot_id,batch_id 0 49 miss% 0.0198124678525317
plot_id,batch_id 0 50 miss% 0.021171493712762817
plot_id,batch_id 0 51 miss% 0.022604205914729333
plot_id,batch_id 0 52 miss% 0.026020179180693766
plot_id,batch_id 0 53 miss% 0.02059632430740986
plot_id,batch_id 0 54 miss% 0.02646152010572618
plot_id,batch_id 0 55 miss% 0.02768694305327343
plot_id,batch_id 0 56 miss% 0.03142888287870757
plot_id,batch_id 0 57 miss% 0.024512743571341016
plot_id,batch_id 0 58 miss% 0.02184753821279305
plot_id,batch_id 0 59 miss% 0.023918741153881935
plot_id,batch_id 0 60 miss% 0.04753502602140709
plot_id,batch_id 0 61 miss% 0.027335700890644025
plot_id,batch_id 0 62 miss% 0.02757038509872275
plot_id,batch_id 0 63 miss% 0.03202217646974112
plot_id,batch_id 0 64 miss% 0.03320958371040711
plot_id,batch_id 0 65 miss% 0.03154303494240233
plot_id,batch_id 0 66 miss% 0.03774300773993306
plot_id,batch_id 0 67 miss% 0.029844513362636275
plot_id,batch_id 0 68 miss% 0.03335581654815074
plot_id,batch_id 0 69 miss% 0.023194889932528943
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  35921
Epoch:0, Train loss:0.778793, valid loss:0.763398
Epoch:1, Train loss:0.061503, valid loss:0.011907
Epoch:2, Train loss:0.015381, valid loss:0.006352
Epoch:3, Train loss:0.011109, valid loss:0.006968
Epoch:4, Train loss:0.009146, valid loss:0.005199
Epoch:5, Train loss:0.008075, valid loss:0.004547
Epoch:6, Train loss:0.006953, valid loss:0.004077
Epoch:7, Train loss:0.006308, valid loss:0.003266
Epoch:8, Train loss:0.005892, valid loss:0.003435
Epoch:9, Train loss:0.005409, valid loss:0.002693
Epoch:10, Train loss:0.005329, valid loss:0.002899
Epoch:11, Train loss:0.003901, valid loss:0.002160
Epoch:12, Train loss:0.003845, valid loss:0.001963
Epoch:13, Train loss:0.003637, valid loss:0.002142
Epoch:14, Train loss:0.003655, valid loss:0.002054
Epoch:15, Train loss:0.003509, valid loss:0.002009
Epoch:16, Train loss:0.003464, valid loss:0.002607
Epoch:17, Train loss:0.003435, valid loss:0.002144
Epoch:18, Train loss:0.003469, valid loss:0.002522
Epoch:19, Train loss:0.003219, valid loss:0.001952
Epoch:20, Train loss:0.003273, valid loss:0.001717
Epoch:21, Train loss:0.002554, valid loss:0.001509
Epoch:22, Train loss:0.002567, valid loss:0.001427
Epoch:23, Train loss:0.002553, valid loss:0.001459
Epoch:24, Train loss:0.002522, valid loss:0.001622
Epoch:25, Train loss:0.002532, valid loss:0.001710
Epoch:26, Train loss:0.002512, valid loss:0.001454
Epoch:27, Train loss:0.002456, valid loss:0.001511
Epoch:28, Train loss:0.002417, valid loss:0.001851
Epoch:29, Train loss:0.002493, valid loss:0.001609
Epoch:30, Train loss:0.002414, valid loss:0.001651
Epoch:31, Train loss:0.002045, valid loss:0.001347
Epoch:32, Train loss:0.002036, valid loss:0.001376
Epoch:33, Train loss:0.002015, valid loss:0.001345
Epoch:34, Train loss:0.002008, valid loss:0.001256
Epoch:35, Train loss:0.002037, valid loss:0.001402
Epoch:36, Train loss:0.002000, valid loss:0.001283
Epoch:37, Train loss:0.002018, valid loss:0.001344
Epoch:38, Train loss:0.001993, valid loss:0.001253
Epoch:39, Train loss:0.001968, valid loss:0.001281
Epoch:40, Train loss:0.001948, valid loss:0.001372
Epoch:41, Train loss:0.001792, valid loss:0.001197
Epoch:42, Train loss:0.001782, valid loss:0.001268
Epoch:43, Train loss:0.001772, valid loss:0.001209
Epoch:44, Train loss:0.001777, valid loss:0.001223
Epoch:45, Train loss:0.001776, valid loss:0.001202
Epoch:46, Train loss:0.001752, valid loss:0.001226
Epoch:47, Train loss:0.001753, valid loss:0.001180
Epoch:48, Train loss:0.001747, valid loss:0.001259
Epoch:49, Train loss:0.001738, valid loss:0.001168
Epoch:50, Train loss:0.001728, valid loss:0.001195
Epoch:51, Train loss:0.001648, valid loss:0.001146
Epoch:52, Train loss:0.001645, valid loss:0.001208
Epoch:53, Train loss:0.001635, valid loss:0.001157
Epoch:54, Train loss:0.001644, valid loss:0.001110
Epoch:55, Train loss:0.001633, valid loss:0.001152
Epoch:56, Train loss:0.001634, valid loss:0.001136
Epoch:57, Train loss:0.001625, valid loss:0.001132
Epoch:58, Train loss:0.001619, valid loss:0.001178
Epoch:59, Train loss:0.001624, valid loss:0.001155
Epoch:60, Train loss:0.001620, valid loss:0.001124
training time 6277.769158840179
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.030444661685694503
plot_id,batch_id 0 1 miss% 0.022649872243228775
plot_id,batch_id 0 2 miss% 0.02770757227469515
plot_id,batch_id 0 3 miss% 0.022377162517936144
plot_id,batch_id 0 4 miss% 0.019157065084732066
plot_id,batch_id 0 5 miss% 0.03303709304239889
plot_id,batch_id 0 6 miss% 0.028023647868521514
plot_id,batch_id 0 7 miss% 0.026083167702428637
plot_id,batch_id 0 8 miss% 0.03112408669924266
plot_id,batch_id 0 9 miss% 0.022312302817102888
plot_id,batch_id 0 10 miss% 0.042295683491910374
plot_id,batch_id 0 11 miss% 0.060117493514035884
plot_id,batch_id 0 12 miss% 0.03357260283655442
plot_id,batch_id 0 13 miss% 0.02956257660340828
plot_id,batch_id 0 14 miss% 0.028237530906964535
plot_id,batch_id 0 15 miss% 0.0598492013537097
plot_id,batch_id 0 16 miss% 0.02611555599889094
plot_id,batch_id 0 17 miss% 0.041568252507812216
plot_id,batch_id 0 18 miss% 0.035003381299123976
plot_id,batch_id 0 19 miss% 0.04182949645857933
plot_id,batch_id 0 20 miss% 0.05851984496575486
plot_id,batch_id 0 21 miss% 0.02621394572403263
plot_id,batch_id 0 22 miss% 0.026687577070475935
plot_id,batch_id 0 23 miss% 0.015965856382658884
plot_id,batch_id 0 24 miss% 0.02532827751930926
plot_id,batch_id 0 25 miss% 0.03706768646247607
plot_id,batch_id 0 26 miss% 0.028062442036072728
plot_id,batch_id 0 27 miss% 0.02399746006103951
plot_id,batch_id 0 28 miss% 0.021539581558210175
plot_id,batch_id 0 29 miss% 0.01902451439200581
plot_id,batch_id 0 30 miss% 0.04309025920702045
plot_id,batch_id 0 31 miss% 0.023261994509067987
plot_id,batch_id 0 32 miss% 0.028466282188819695
plot_id,batch_id 0 33 miss% 0.028765535539907787
plot_id,batch_id 0 34 miss% 0.025514495514663314
plot_id,batch_id 0 35 miss% 0.053515137361644934
plot_id,batch_id 0 36 miss% 0.04423716712462599
plot_id,batch_id 0 37 miss% 0.03557818700822753
plot_id,batch_id 0 38 miss% 0.028819997519479456
plot_id,batch_id 0 39 miss% 0.032160504581175184
plot_id,batch_id 0 40 miss% 0.060324271812850966
plot_id,batch_id 0 41 miss% 0.02801107670198687
plot_id,batch_id 0 42 miss% 0.02033022278058674
plot_id,batch_id 0 43 miss% 0.03495537234911988
plot_id,batch_id 0 44 miss% 0.02673893908688676
plot_id,batch_id 0 45 miss% 0.03411272335824472
plot_id,batch_id 0 46 miss% 0.03448629879549084
plot_id,batch_id 0 47 miss% 0.029652720259364065
plot_id,batch_id 0 48 miss% 0.022073997344061406
plot_id,batch_id 0 49 miss% 0.02923965074898992
plot_id,batch_id 0 50 miss% 0.03605399441801381
plot_id,batch_id 0 51 miss% 0.028675261098744683
plot_id,batch_id 0 52 miss% 0.03255501556768202
plot_id,batch_id 0 53 miss% 0.022137381533562876
plot_id,batch_id 0 54 miss% 0.03510457456581421
plot_id,batch_id 0 55 miss% 0.029366915468841938
plot_id,batch_id 0 56 miss% 0.04194260781272544
plot_id,batch_id 0 57 miss% 0.032707094682523696
plot_id,batch_id 0 58 miss% 0.026747091236447498
plot_id,batch_id 0 59 miss% 0.022513656455337733
plot_id,batch_id 0 60 miss% 0.04163894854013779
plot_id,batch_id 0 61 miss% 0.03424499630500576
plot_id,batch_id 0 62 miss% 0.03997485530381927
plot_id,batch_id 0 63 miss% 0.030433497549357695
plot_id,batch_id 0 64 miss% 0.03882341072055076
plot_id,batch_id 0 65 miss% 0.07223800249106163
plot_id,batch_id 0 66 miss% 0.03020728643819192
plot_id,batch_id 0 67 miss% 0.03760814186494953
plot_id,batch_id 0 68 miss% 0.02304253275352853
plot_id,batch_id 0 69 miss% 0.044778640987345866
0.03740703010403489
plot_id,batch_id 0 70 miss% 0.07134562890310098
plot_id,batch_id 0 71 miss% 0.048640467799122844
plot_id,batch_id 0 72 miss% 0.037478196017328214
plot_id,batch_id 0 73 miss% 0.03907050819811329
plot_id,batch_id 0 74 miss% 0.05580680250714069
plot_id,batch_id 0 75 miss% 0.05227851544212139
plot_id,batch_id 0 76 miss% 0.03367361317312634
plot_id,batch_id 0 77 miss% 0.02637075005255473
plot_id,batch_id 0 78 miss% 0.05338192386582683
plot_id,batch_id 0 79 miss% 0.03604145395126638
plot_id,batch_id 0 80 miss% 0.07513490139506582
plot_id,batch_id 0 81 miss% 0.034906248913491
plot_id,batch_id 0 82 miss% 0.03580988514355562
plot_id,batch_id 0 83 miss% 0.045428929171743876
plot_id,batch_id 0 84 miss% 0.042467091365137724
plot_id,batch_id 0 85 miss% 0.029972972027988733
plot_id,batch_id 0 86 miss% 0.028336258660672905
plot_id,batch_id 0 87 miss% 0.0396441416513849
plot_id,batch_id 0 88 miss% 0.05577681654414072
plot_id,batch_id 0 89 miss% 0.03968098969322055
plot_id,batch_id 0 90 miss% 0.052517255215055215
plot_id,batch_id 0 91 miss% 0.03162369247779579
plot_id,batch_id 0 92 miss% 0.060103987908426096
plot_id,batch_id 0 93 miss% 0.02996203506885925
plot_id,batch_id 0 94 miss% 0.05227359178476075
plot_id,batch_id 0 95 miss% 0.0391837894732859
plot_id,batch_id 0 96 miss% 0.03632727385761479
plot_id,batch_id 0 97 miss% 0.042484209289553564
plot_id,batch_id 0 98 miss% 0.05127461936381219
plot_id,batch_id 0 99 miss% 0.0502406813319559
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03374019 0.01888051 0.02702803 0.02729126 0.03722126 0.02664739
 0.03535527 0.02712398 0.03493736 0.02176665 0.04607476 0.06003654
 0.02092954 0.04103962 0.04457254 0.04612922 0.03872518 0.03200617
 0.03843664 0.05176533 0.04333268 0.03789497 0.02413952 0.01979128
 0.0247502  0.03893679 0.02799238 0.03259289 0.01772357 0.02470359
 0.04866171 0.04788173 0.03676501 0.03763473 0.0429304  0.05022983
 0.06234111 0.02306964 0.01971803 0.01578377 0.06682737 0.02135352
 0.02347394 0.02512592 0.02381917 0.02372176 0.01702485 0.0239769
 0.01946767 0.02633745 0.03638176 0.03611846 0.02776493 0.02716448
 0.02306437 0.02157258 0.03928163 0.02602956 0.02532346 0.02197834
 0.02944893 0.02808563 0.02144177 0.03826752 0.05139348 0.03286124
 0.05291095 0.03971789 0.04290792 0.03740703 0.07134563 0.04864047
 0.0374782  0.03907051 0.0558068  0.05227852 0.03367361 0.02637075
 0.05338192 0.03604145 0.0751349  0.03490625 0.03580989 0.04542893
 0.04246709 0.02997297 0.02833626 0.03964414 0.05577682 0.03968099
 0.05251726 0.03162369 0.06010399 0.02996204 0.05227359 0.03918379
 0.03632727 0.04248421 0.05127462 0.05024068]
for model  20 the mean error 0.03644068982293611
all id 20 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 21 out win 5 err 0.03644068982293611
Launcher: Job 21 completed in 6480 seconds.
Launcher: Task 93 done. Exiting.
plot_id,batch_id 0 70 miss% 0.03260077831137358
plot_id,batch_id 0 71 miss% 0.039169487770965426
plot_id,batch_id 0 72 miss% 0.029769327563579938
plot_id,batch_id 0 73 miss% 0.02541072708476069
plot_id,batch_id 0 74 miss% 0.02663714770429909
plot_id,batch_id 0 75 miss% 0.03766884166373325
plot_id,batch_id 0 76 miss% 0.048388713384406104
plot_id,batch_id 0 77 miss% 0.040967640489362175
plot_id,batch_id 0 78 miss% 0.027377226176446712
plot_id,batch_id 0 79 miss% 0.03943119987909635
plot_id,batch_id 0 80 miss% 0.04304170851943489
plot_id,batch_id 0 81 miss% 0.019862193059761134
plot_id,batch_id 0 82 miss% 0.024569788801051703
plot_id,batch_id 0 83 miss% 0.03214908653761854
plot_id,batch_id 0 84 miss% 0.038248942398882636
plot_id,batch_id 0 85 miss% 0.04252892263108632
plot_id,batch_id 0 86 miss% 0.033186144659545745
plot_id,batch_id 0 87 miss% 0.037923455382227744
plot_id,batch_id 0 88 miss% 0.03591430690561688
plot_id,batch_id 0 89 miss% 0.035947280569895046
plot_id,batch_id 0 90 miss% 0.04054493299535794
plot_id,batch_id 0 91 miss% 0.02656847505934709
plot_id,batch_id 0 92 miss% 0.0357081724733475
plot_id,batch_id 0 93 miss% 0.03183037979681126
plot_id,batch_id 0 94 miss% 0.032388592700481805
plot_id,batch_id 0 95 miss% 0.052190342377159034
plot_id,batch_id 0 96 miss% 0.03389554830611966
plot_id,batch_id 0 97 miss% 0.07863704328518038
plot_id,batch_id 0 98 miss% 0.02725416226956441
plot_id,batch_id 0 99 miss% 0.0306344670953473
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02546991 0.02416605 0.01784336 0.03207913 0.01776664 0.02769969
 0.02293082 0.02764737 0.02366837 0.02402273 0.02917928 0.05936041
 0.02647063 0.02554778 0.03111786 0.03442483 0.03335733 0.03432019
 0.03569732 0.02761906 0.04874811 0.02321695 0.02084865 0.02428332
 0.02674074 0.0318321  0.01622796 0.01769918 0.02448484 0.02356546
 0.04647332 0.0362604  0.0309299  0.02615912 0.02374606 0.03987564
 0.0372277  0.02303958 0.01679307 0.02056508 0.06104119 0.02476003
 0.01795122 0.0296101  0.02235104 0.02325259 0.01399489 0.02358526
 0.02268346 0.01981247 0.02117149 0.02260421 0.02602018 0.02059632
 0.02646152 0.02768694 0.03142888 0.02451274 0.02184754 0.02391874
 0.04753503 0.0273357  0.02757039 0.03202218 0.03320958 0.03154303
 0.03774301 0.02984451 0.03335582 0.02319489 0.03260078 0.03916949
 0.02976933 0.02541073 0.02663715 0.03766884 0.04838871 0.04096764
 0.02737723 0.0394312  0.04304171 0.01986219 0.02456979 0.03214909
 0.03824894 0.04252892 0.03318614 0.03792346 0.03591431 0.03594728
 0.04054493 0.02656848 0.03570817 0.03183038 0.03238859 0.05219034
 0.03389555 0.07863704 0.02725416 0.03063447]
for model  34 the mean error 0.03048195911606826
all id 34 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 21 out win 4 err 0.03048195911606826
Launcher: Job 35 completed in 6486 seconds.
Launcher: Task 2 done. Exiting.
plot_id,batch_id 0 70 miss% 0.02831583372136004
plot_id,batch_id 0 71 miss% 0.046060156074891424
plot_id,batch_id 0 72 miss% 0.028409479933166944
plot_id,batch_id 0 73 miss% 0.030260406530238763
plot_id,batch_id 0 74 miss% 0.04818522070618304
plot_id,batch_id 0 75 miss% 0.04483640670240868
plot_id,batch_id 0 76 miss% 0.04057077108529425
plot_id,batch_id 0 77 miss% 0.04006229524258579
plot_id,batch_id 0 78 miss% 0.03925088251179597
plot_id,batch_id 0 79 miss% 0.032715698849238514
plot_id,batch_id 0 80 miss% 0.02649193442283124
plot_id,batch_id 0 81 miss% 0.03333413320926728
plot_id,batch_id 0 82 miss% 0.030249448774250808
plot_id,batch_id 0 83 miss% 0.025947443647656332
plot_id,batch_id 0 84 miss% 0.028354903419487376
plot_id,batch_id 0 85 miss% 0.04613104616830197
plot_id,batch_id 0 86 miss% 0.03846227997055862
plot_id,batch_id 0 87 miss% 0.027875146417234157
plot_id,batch_id 0 88 miss% 0.03357743946269949
plot_id,batch_id 0 89 miss% 0.03756164631691139
plot_id,batch_id 0 90 miss% 0.04898127756420007
plot_id,batch_id 0 91 miss% 0.03878415204610151
plot_id,batch_id 0 92 miss% 0.03256350594748128
plot_id,batch_id 0 93 miss% 0.042742583140247835
plot_id,batch_id 0 94 miss% 0.02637327470646662
plot_id,batch_id 0 95 miss% 0.06491166089610577
plot_id,batch_id 0 96 miss% 0.045696063173550884
plot_id,batch_id 0 97 miss% 0.051849543737212844
plot_id,batch_id 0 98 miss% 0.03168649318727511
plot_id,batch_id 0 99 miss% 0.029877900279462407
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03044466 0.02264987 0.02770757 0.02237716 0.01915707 0.03303709
 0.02802365 0.02608317 0.03112409 0.0223123  0.04229568 0.06011749
 0.0335726  0.02956258 0.02823753 0.0598492  0.02611556 0.04156825
 0.03500338 0.0418295  0.05851984 0.02621395 0.02668758 0.01596586
 0.02532828 0.03706769 0.02806244 0.02399746 0.02153958 0.01902451
 0.04309026 0.02326199 0.02846628 0.02876554 0.0255145  0.05351514
 0.04423717 0.03557819 0.02882    0.0321605  0.06032427 0.02801108
 0.02033022 0.03495537 0.02673894 0.03411272 0.0344863  0.02965272
 0.022074   0.02923965 0.03605399 0.02867526 0.03255502 0.02213738
 0.03510457 0.02936692 0.04194261 0.03270709 0.02674709 0.02251366
 0.04163895 0.034245   0.03997486 0.0304335  0.03882341 0.072238
 0.03020729 0.03760814 0.02304253 0.04477864 0.02831583 0.04606016
 0.02840948 0.03026041 0.04818522 0.04483641 0.04057077 0.0400623
 0.03925088 0.0327157  0.02649193 0.03333413 0.03024945 0.02594744
 0.0283549  0.04613105 0.03846228 0.02787515 0.03357744 0.03756165
 0.04898128 0.03878415 0.03256351 0.04274258 0.02637327 0.06491166
 0.04569606 0.05184954 0.03168649 0.0298779 ]
for model  46 the mean error 0.0342772336050933
all id 46 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 21 out win 4 err 0.0342772336050933
Launcher: Job 47 completed in 6487 seconds.
Launcher: Task 155 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  35921
Epoch:0, Train loss:0.620031, valid loss:0.603796
Epoch:1, Train loss:0.037978, valid loss:0.005606
Epoch:2, Train loss:0.009791, valid loss:0.004589
Epoch:3, Train loss:0.007218, valid loss:0.003230
Epoch:4, Train loss:0.005904, valid loss:0.002600
Epoch:5, Train loss:0.005116, valid loss:0.003131
Epoch:6, Train loss:0.004730, valid loss:0.002153
Epoch:7, Train loss:0.004349, valid loss:0.002496
Epoch:8, Train loss:0.004267, valid loss:0.002733
Epoch:9, Train loss:0.004000, valid loss:0.001964
Epoch:10, Train loss:0.004123, valid loss:0.002609
Epoch:11, Train loss:0.002888, valid loss:0.001457
Epoch:12, Train loss:0.002827, valid loss:0.002165
Epoch:13, Train loss:0.002801, valid loss:0.001563
Epoch:14, Train loss:0.002774, valid loss:0.001370
Epoch:15, Train loss:0.002705, valid loss:0.001960
Epoch:16, Train loss:0.002707, valid loss:0.001793
Epoch:17, Train loss:0.002713, valid loss:0.001548
Epoch:18, Train loss:0.002593, valid loss:0.001382
Epoch:19, Train loss:0.002605, valid loss:0.001737
Epoch:20, Train loss:0.002561, valid loss:0.001552
Epoch:21, Train loss:0.001987, valid loss:0.001172
Epoch:22, Train loss:0.001995, valid loss:0.001170
Epoch:23, Train loss:0.001987, valid loss:0.001233
Epoch:24, Train loss:0.001986, valid loss:0.001189
Epoch:25, Train loss:0.001966, valid loss:0.001257
Epoch:26, Train loss:0.001931, valid loss:0.001177
Epoch:27, Train loss:0.001939, valid loss:0.001105
Epoch:28, Train loss:0.001884, valid loss:0.001098
Epoch:29, Train loss:0.001866, valid loss:0.001094
Epoch:30, Train loss:0.001870, valid loss:0.001236
Epoch:31, Train loss:0.001612, valid loss:0.001075
Epoch:32, Train loss:0.001622, valid loss:0.001076
Epoch:33, Train loss:0.001601, valid loss:0.001160
Epoch:34, Train loss:0.001607, valid loss:0.001085
Epoch:35, Train loss:0.001589, valid loss:0.000970
Epoch:36, Train loss:0.001583, valid loss:0.000975
Epoch:37, Train loss:0.001582, valid loss:0.001050
Epoch:38, Train loss:0.001550, valid loss:0.001089
Epoch:39, Train loss:0.001555, valid loss:0.000995
Epoch:40, Train loss:0.001544, valid loss:0.001060
Epoch:41, Train loss:0.001418, valid loss:0.000974
Epoch:42, Train loss:0.001413, valid loss:0.000944
Epoch:43, Train loss:0.001405, valid loss:0.000974
Epoch:44, Train loss:0.001404, valid loss:0.000944
Epoch:45, Train loss:0.001419, valid loss:0.000950
Epoch:46, Train loss:0.001395, valid loss:0.000934
Epoch:47, Train loss:0.001385, valid loss:0.000943
Epoch:48, Train loss:0.001386, valid loss:0.000968
Epoch:49, Train loss:0.001382, valid loss:0.000984
Epoch:50, Train loss:0.001377, valid loss:0.000917
Epoch:51, Train loss:0.001311, valid loss:0.000921
Epoch:52, Train loss:0.001304, valid loss:0.000912
Epoch:53, Train loss:0.001306, valid loss:0.000949
Epoch:54, Train loss:0.001305, valid loss:0.000911
Epoch:55, Train loss:0.001303, valid loss:0.000897
Epoch:56, Train loss:0.001301, valid loss:0.000901
Epoch:57, Train loss:0.001298, valid loss:0.000905
Epoch:58, Train loss:0.001291, valid loss:0.000904
Epoch:59, Train loss:0.001293, valid loss:0.000897
Epoch:60, Train loss:0.001291, valid loss:0.000893
training time 6313.563867092133
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.024776569614109695
plot_id,batch_id 0 1 miss% 0.028385123587063122
plot_id,batch_id 0 2 miss% 0.02278476145202251
plot_id,batch_id 0 3 miss% 0.02595477863283131
plot_id,batch_id 0 4 miss% 0.03167717401246714
plot_id,batch_id 0 5 miss% 0.03988966516490296
plot_id,batch_id 0 6 miss% 0.03885953159656067
plot_id,batch_id 0 7 miss% 0.03099482258394994
plot_id,batch_id 0 8 miss% 0.03963394652412715
plot_id,batch_id 0 9 miss% 0.030034199099252416
plot_id,batch_id 0 10 miss% 0.058256761536616894
plot_id,batch_id 0 11 miss% 0.0388942645716589
plot_id,batch_id 0 12 miss% 0.025409650638742134
plot_id,batch_id 0 13 miss% 0.01944051659804018
plot_id,batch_id 0 14 miss% 0.04486362664228308
plot_id,batch_id 0 15 miss% 0.04621323054301807
plot_id,batch_id 0 16 miss% 0.04024925025127301
plot_id,batch_id 0 17 miss% 0.049000254857058716
plot_id,batch_id 0 18 miss% 0.03624143624585705
plot_id,batch_id 0 19 miss% 0.04634706019883736
plot_id,batch_id 0 20 miss% 0.09947839551431394
plot_id,batch_id 0 21 miss% 0.02381298211502836
plot_id,batch_id 0 22 miss% 0.03540003494154092
plot_id,batch_id 0 23 miss% 0.0329099926586635
plot_id,batch_id 0 24 miss% 0.04381569953223069
plot_id,batch_id 0 25 miss% 0.034707805564767887
plot_id,batch_id 0 26 miss% 0.027502192653730453
plot_id,batch_id 0 27 miss% 0.026564533732691353
plot_id,batch_id 0 28 miss% 0.02555434944353932
plot_id,batch_id 0 29 miss% 0.032908319056774625
plot_id,batch_id 0 30 miss% 0.036895163859216006
plot_id,batch_id 0 31 miss% 0.03139735771797426
plot_id,batch_id 0 32 miss% 0.03809697009836982
plot_id,batch_id 0 33 miss% 0.027995378650468975
plot_id,batch_id 0 34 miss% 0.034305632321448606
plot_id,batch_id 0 35 miss% 0.046825267302327216
plot_id,batch_id 0 36 miss% 0.044032221553487606
plot_id,batch_id 0 37 miss% 0.029376918543104074
plot_id,batch_id 0 38 miss% 0.024988188875289547
plot_id,batch_id 0 39 miss% 0.01900738394982202
plot_id,batch_id 0 40 miss% 0.05103708453720933
plot_id,batch_id 0 41 miss% 0.019443597390343594
plot_id,batch_id 0 42 miss% 0.013138503862918321
plot_id,batch_id 0 43 miss% 0.03714045440047032
plot_id,batch_id 0 44 miss% 0.03265491601136859
plot_id,batch_id 0 45 miss% 0.05002321778027975
plot_id,batch_id 0 46 miss% 0.03335669785485285
plot_id,batch_id 0 47 miss% 0.02993743488932992
plot_id,batch_id 0 48 miss% 0.028909448122124575
plot_id,batch_id 0 49 miss% 0.02688522837695807
plot_id,batch_id 0 50 miss% 0.03171901405671676
plot_id,batch_id 0 51 miss% 0.022934918519208942
plot_id,batch_id 0 52 miss% 0.02128276248239539
plot_id,batch_id 0 53 miss% 0.015885445166102137
plot_id,batch_id 0 54 miss% 0.02615153035801144
plot_id,batch_id 0 55 miss% 0.04167562334919922
plot_id,batch_id 0 56 miss% 0.016130029317317933
plot_id,batch_id 0 57 miss% 0.019133307126190054
plot_id,batch_id 0 58 miss% 0.024017825892801124
plot_id,batch_id 0 59 miss% 0.028434748453969396
plot_id,batch_id 0 60 miss% 0.042779240349632805
plot_id,batch_id 0 61 miss% 0.024933581258241348
plot_id,batch_id 0 62 miss% 0.03386760870024716
plot_id,batch_id 0 63 miss% 0.02615575746080444
plot_id,batch_id 0 64 miss% 0.027841312601377455
plot_id,batch_id 0 65 miss% 0.035115538279656654
plot_id,batch_id 0 66 miss% 0.03769679020619399
plot_id,batch_id 0 67 miss% 0.0324431286376478
plot_id,batch_id 0 68 miss% 0.025610607667078577
plot_id,batch_id 0 69 miss%the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  79249
Epoch:0, Train loss:0.598719, valid loss:0.574885
Epoch:1, Train loss:0.065611, valid loss:0.010603
Epoch:2, Train loss:0.016765, valid loss:0.006968
Epoch:3, Train loss:0.013113, valid loss:0.005714
Epoch:4, Train loss:0.011625, valid loss:0.004633
Epoch:5, Train loss:0.009306, valid loss:0.006317
Epoch:6, Train loss:0.008389, valid loss:0.004957
Epoch:7, Train loss:0.008054, valid loss:0.004075
Epoch:8, Train loss:0.007012, valid loss:0.003552
Epoch:9, Train loss:0.006931, valid loss:0.003712
Epoch:10, Train loss:0.006317, valid loss:0.003342
Epoch:11, Train loss:0.004101, valid loss:0.002466
Epoch:12, Train loss:0.004027, valid loss:0.001913
Epoch:13, Train loss:0.004040, valid loss:0.002057
Epoch:14, Train loss:0.004042, valid loss:0.002308
Epoch:15, Train loss:0.003803, valid loss:0.004215
Epoch:16, Train loss:0.003873, valid loss:0.002688
Epoch:17, Train loss:0.003813, valid loss:0.001743
Epoch:18, Train loss:0.003517, valid loss:0.002923
Epoch:19, Train loss:0.003854, valid loss:0.001582
Epoch:20, Train loss:0.003515, valid loss:0.002539
Epoch:21, Train loss:0.002409, valid loss:0.001467
Epoch:22, Train loss:0.002354, valid loss:0.001450
Epoch:23, Train loss:0.002384, valid loss:0.001497
Epoch:24, Train loss:0.002359, valid loss:0.001399
Epoch:25, Train loss:0.002331, valid loss:0.001306
Epoch:26, Train loss:0.002299, valid loss:0.001314
Epoch:27, Train loss:0.002205, valid loss:0.001589
Epoch:28, Train loss:0.002300, valid loss:0.001778
Epoch:29, Train loss:0.002206, valid loss:0.001662
Epoch:30, Train loss:0.002247, valid loss:0.001435
Epoch:31, Train loss:0.001690, valid loss:0.001188
Epoch:32, Train loss:0.001629, valid loss:0.001163
Epoch:33, Train loss:0.001618, valid loss:0.001249
Epoch:34, Train loss:0.001684, valid loss:0.001092
Epoch:35, Train loss:0.001615, valid loss:0.001116
Epoch:36, Train loss:0.001621, valid loss:0.001062
Epoch:37, Train loss:0.001661, valid loss:0.001097
Epoch:38, Train loss:0.001585, valid loss:0.001157
Epoch:39, Train loss:0.001592, valid loss:0.001065
Epoch:40, Train loss:0.001578, valid loss:0.001195
Epoch:41, Train loss:0.001326, valid loss:0.001009
Epoch:42, Train loss:0.001322, valid loss:0.000989
Epoch:43, Train loss:0.001320, valid loss:0.000982
Epoch:44, Train loss:0.001308, valid loss:0.000996
Epoch:45, Train loss:0.001302, valid loss:0.001054
Epoch:46, Train loss:0.001315, valid loss:0.000977
Epoch:47, Train loss:0.001290, valid loss:0.000999
Epoch:48, Train loss:0.001287, valid loss:0.000997
Epoch:49, Train loss:0.001288, valid loss:0.000969
Epoch:50, Train loss:0.001256, valid loss:0.001016
Epoch:51, Train loss:0.001163, valid loss:0.000936
Epoch:52, Train loss:0.001147, valid loss:0.000925
Epoch:53, Train loss:0.001155, valid loss:0.000985
Epoch:54, Train loss:0.001149, valid loss:0.000933
Epoch:55, Train loss:0.001144, valid loss:0.000999
Epoch:56, Train loss:0.001154, valid loss:0.000981
Epoch:57, Train loss:0.001131, valid loss:0.000927
Epoch:58, Train loss:0.001127, valid loss:0.000948
Epoch:59, Train loss:0.001140, valid loss:0.000925
Epoch:60, Train loss:0.001128, valid loss:0.000940
training time 6315.914641618729
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03268120873330322
plot_id,batch_id 0 1 miss% 0.020308749132190048
plot_id,batch_id 0 2 miss% 0.018661709463905767
plot_id,batch_id 0 3 miss% 0.021288537137868775
plot_id,batch_id 0 4 miss% 0.020867056255039144
plot_id,batch_id 0 5 miss% 0.04436843680252343
plot_id,batch_id 0 6 miss% 0.031328006668534907
plot_id,batch_id 0 7 miss% 0.02014629839257341
plot_id,batch_id 0 8 miss% 0.020371483990160952
plot_id,batch_id 0 9 miss% 0.04244784015297174
plot_id,batch_id 0 10 miss% 0.038757242638538414
plot_id,batch_id 0 11 miss% 0.04325070662382155
plot_id,batch_id 0 12 miss% 0.025359092949727973
plot_id,batch_id 0 13 miss% 0.02857460717573241
plot_id,batch_id 0 14 miss% 0.030833031019787478
plot_id,batch_id 0 15 miss% 0.028770807776806538
plot_id,batch_id 0 16 miss% 0.03488708144685863
plot_id,batch_id 0 17 miss% 0.040424023935954526
plot_id,batch_id 0 18 miss% 0.028573375775073066
plot_id,batch_id 0 19 miss% 0.034277011201556
plot_id,batch_id 0 20 miss% 0.08472924953013489
plot_id,batch_id 0 21 miss% 0.017025774374583146
plot_id,batch_id 0 22 miss% 0.021932917835105953
plot_id,batch_id 0 23 miss% 0.026248558300405636
plot_id,batch_id 0 24 miss% 0.02350096521211204
plot_id,batch_id 0 25 miss% 0.02575529966573947
plot_id,batch_id 0 26 miss% 0.023319111241149604
plot_id,batch_id 0 27 miss% 0.02343403112988808
plot_id,batch_id 0 28 miss% 0.023886368105507993
plot_id,batch_id 0 29 miss% 0.021525378686594644
plot_id,batch_id 0 30 miss% 0.037916457762268096
plot_id,batch_id 0 31 miss% 0.022925413884725035
plot_id,batch_id 0 32 miss% 0.026904771123449186
plot_id,batch_id 0 33 miss% 0.023119690935206876
plot_id,batch_id 0 34 miss% 0.018407323095215047
plot_id,batch_id 0 35 miss% 0.03996056371711917
plot_id,batch_id 0 36 miss% 0.04657269413788873
plot_id,batch_id 0 37 miss% 0.013819414511406055
plot_id,batch_id 0 38 miss% 0.031101368213898542
plot_id,batch_id 0 39 miss% 0.023545631062421482
plot_id,batch_id 0 40 miss% 0.07073113564166068
plot_id,batch_id 0 41 miss% 0.01886900025616852
plot_id,batch_id 0 42 miss% 0.01456120812404829
plot_id,batch_id 0 43 miss% 0.024066188311177527
plot_id,batch_id 0 44 miss% 0.015515609121415806
plot_id,batch_id 0 45 miss% 0.023681623681241184
plot_id,batch_id 0 46 miss% 0.02540739713425157
plot_id,batch_id 0 47 miss% 0.012896640282729622
plot_id,batch_id 0 48 miss% 0.022147111647913723
plot_id,batch_id 0 49 miss% 0.02631421425386495
plot_id,batch_id 0 50 miss% 0.039923858006240306
plot_id,batch_id 0 51 miss% 0.019987037266779185
plot_id,batch_id 0 52 miss% 0.019234138956617717
plot_id,batch_id 0 53 miss% 0.013058991607906523
plot_id,batch_id 0 54 miss% 0.02253921642616754
plot_id,batch_id 0 55 miss% 0.027043312144167823
plot_id,batch_id 0 56 miss% 0.019459642571555604
plot_id,batch_id 0 57 miss% 0.017437359200716974
plot_id,batch_id 0 58 miss% 0.01865207569542029
plot_id,batch_id 0 59 miss% 0.02077290582388332
plot_id,batch_id 0 60 miss% 0.038106576234822526
plot_id,batch_id 0 61 miss% 0.02758087660231759
plot_id,batch_id 0 62 miss% 0.0369005584851586
plot_id,batch_id 0 63 miss% 0.0355797159552856
plot_id,batch_id 0 64 miss% 0.026790890691250752
plot_id,batch_id 0 65 miss% 0.04042423202963652
plot_id,batch_id 0 66 miss% 0.030276143152192906
plot_id,batch_id 0 67 miss% 0.01966702878538631
plot_id,batch_id 0 68 miss% 0.03488350428019558
plot_id,batch_id 0 69 miss%  0.033183136993719484
plot_id,batch_id 0 70 miss% 0.037068922915414575
plot_id,batch_id 0 71 miss% 0.04795516695888259
plot_id,batch_id 0 72 miss% 0.032052790974794944
plot_id,batch_id 0 73 miss% 0.038083566555756905
plot_id,batch_id 0 74 miss% 0.03767905887104825
plot_id,batch_id 0 75 miss% 0.04369956760517068
plot_id,batch_id 0 76 miss% 0.06068354039474817
plot_id,batch_id 0 77 miss% 0.0507280507109541
plot_id,batch_id 0 78 miss% 0.04647546627228763
plot_id,batch_id 0 79 miss% 0.04295734283337869
plot_id,batch_id 0 80 miss% 0.06642403958638894
plot_id,batch_id 0 81 miss% 0.028174856602438073
plot_id,batch_id 0 82 miss% 0.01994850167899989
plot_id,batch_id 0 83 miss% 0.039515264579005345
plot_id,batch_id 0 84 miss% 0.030531376186200918
plot_id,batch_id 0 85 miss% 0.03413837192088959
plot_id,batch_id 0 86 miss% 0.021053795945114074
plot_id,batch_id 0 87 miss% 0.04131351595793405
plot_id,batch_id 0 88 miss% 0.042809771758337764
plot_id,batch_id 0 89 miss% 0.037308870412333765
plot_id,batch_id 0 90 miss% 0.044209391889378495
plot_id,batch_id 0 91 miss% 0.03308776941626826
plot_id,batch_id 0 92 miss% 0.03430888820684351
plot_id,batch_id 0 93 miss% 0.028296608939679148
plot_id,batch_id 0 94 miss% 0.036588992783107586
plot_id,batch_id 0 95 miss% 0.04991152374675048
plot_id,batch_id 0 96 miss% 0.039782097662163375
plot_id,batch_id 0 97 miss% 0.060382495085178116
plot_id,batch_id 0 98 miss% 0.053633960348001476
plot_id,batch_id 0 99 miss% 0.03355731255181341
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02477657 0.02838512 0.02278476 0.02595478 0.03167717 0.03988967
 0.03885953 0.03099482 0.03963395 0.0300342  0.05825676 0.03889426
 0.02540965 0.01944052 0.04486363 0.04621323 0.04024925 0.04900025
 0.03624144 0.04634706 0.0994784  0.02381298 0.03540003 0.03290999
 0.0438157  0.03470781 0.02750219 0.02656453 0.02555435 0.03290832
 0.03689516 0.03139736 0.03809697 0.02799538 0.03430563 0.04682527
 0.04403222 0.02937692 0.02498819 0.01900738 0.05103708 0.0194436
 0.0131385  0.03714045 0.03265492 0.05002322 0.0333567  0.02993743
 0.02890945 0.02688523 0.03171901 0.02293492 0.02128276 0.01588545
 0.02615153 0.04167562 0.01613003 0.01913331 0.02401783 0.02843475
 0.04277924 0.02493358 0.03386761 0.02615576 0.02784131 0.03511554
 0.03769679 0.03244313 0.02561061 0.03318314 0.03706892 0.04795517
 0.03205279 0.03808357 0.03767906 0.04369957 0.06068354 0.05072805
 0.04647547 0.04295734 0.06642404 0.02817486 0.0199485  0.03951526
 0.03053138 0.03413837 0.0210538  0.04131352 0.04280977 0.03730887
 0.04420939 0.03308777 0.03430889 0.02829661 0.03658899 0.04991152
 0.0397821  0.0603825  0.05363396 0.03355731]
for model  127 the mean error 0.03535360781887091
all id 127 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 25 out win 4 err 0.03535360781887091
Launcher: Job 128 completed in 6514 seconds.
Launcher: Task 64 done. Exiting.
0.02466611954899714
plot_id,batch_id 0 70 miss% 0.03693929241998783
plot_id,batch_id 0 71 miss% 0.05013799448223294
plot_id,batch_id 0 72 miss% 0.03269323991251212
plot_id,batch_id 0 73 miss% 0.0355638364631634
plot_id,batch_id 0 74 miss% 0.03131065497553748
plot_id,batch_id 0 75 miss% 0.043744671041838656
plot_id,batch_id 0 76 miss% 0.04527108511013096
plot_id,batch_id 0 77 miss% 0.023121532109391232
plot_id,batch_id 0 78 miss% 0.03708038631174939
plot_id,batch_id 0 79 miss% 0.04488368280254196
plot_id,batch_id 0 80 miss% 0.05731414838139657
plot_id,batch_id 0 81 miss% 0.032588483846919376
plot_id,batch_id 0 82 miss% 0.02090706289530287
plot_id,batch_id 0 83 miss% 0.019216945272719166
plot_id,batch_id 0 84 miss% 0.036872602811503896
plot_id,batch_id 0 85 miss% 0.04542524801146194
plot_id,batch_id 0 86 miss% 0.04011216886400807
plot_id,batch_id 0 87 miss% 0.039875832722950574
plot_id,batch_id 0 88 miss% 0.02820354198085733
plot_id,batch_id 0 89 miss% 0.021448150135486545
plot_id,batch_id 0 90 miss% 0.03573716044814845
plot_id,batch_id 0 91 miss% 0.03575183955409075
plot_id,batch_id 0 92 miss% 0.04112595640843898
plot_id,batch_id 0 93 miss% 0.027069668357160678
plot_id,batch_id 0 94 miss% 0.02974991859754144
plot_id,batch_id 0 95 miss% 0.0363522007660808
plot_id,batch_id 0 96 miss% 0.0355597667339213
plot_id,batch_id 0 97 miss% 0.04066685287461245
plot_id,batch_id 0 98 miss% 0.02251034827357345
plot_id,batch_id 0 99 miss% 0.030167046567823822
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03268121 0.02030875 0.01866171 0.02128854 0.02086706 0.04436844
 0.03132801 0.0201463  0.02037148 0.04244784 0.03875724 0.04325071
 0.02535909 0.02857461 0.03083303 0.02877081 0.03488708 0.04042402
 0.02857338 0.03427701 0.08472925 0.01702577 0.02193292 0.02624856
 0.02350097 0.0257553  0.02331911 0.02343403 0.02388637 0.02152538
 0.03791646 0.02292541 0.02690477 0.02311969 0.01840732 0.03996056
 0.04657269 0.01381941 0.03110137 0.02354563 0.07073114 0.018869
 0.01456121 0.02406619 0.01551561 0.02368162 0.0254074  0.01289664
 0.02214711 0.02631421 0.03992386 0.01998704 0.01923414 0.01305899
 0.02253922 0.02704331 0.01945964 0.01743736 0.01865208 0.02077291
 0.03810658 0.02758088 0.03690056 0.03557972 0.02679089 0.04042423
 0.03027614 0.01966703 0.0348835  0.02466612 0.03693929 0.05013799
 0.03269324 0.03556384 0.03131065 0.04374467 0.04527109 0.02312153
 0.03708039 0.04488368 0.05731415 0.03258848 0.02090706 0.01921695
 0.0368726  0.04542525 0.04011217 0.03987583 0.02820354 0.02144815
 0.03573716 0.03575184 0.04112596 0.02706967 0.02974992 0.0363522
 0.03555977 0.04066685 0.02251035 0.03016705]
for model  61 the mean error 0.030263829208500032
all id 61 hidden_dim 32 learning_rate 0.02 num_layers 3 frames 21 out win 4 err 0.030263829208500032
Launcher: Job 62 completed in 6523 seconds.
Launcher: Task 82 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  46193
Epoch:0, Train loss:0.533217, valid loss:0.496812
Epoch:1, Train loss:0.356640, valid loss:0.361166
Epoch:2, Train loss:0.345355, valid loss:0.360056
Epoch:3, Train loss:0.343667, valid loss:0.358772
Epoch:4, Train loss:0.343066, valid loss:0.358595
Epoch:5, Train loss:0.342370, valid loss:0.358779
Epoch:6, Train loss:0.342251, valid loss:0.358532
Epoch:7, Train loss:0.341948, valid loss:0.358545
Epoch:8, Train loss:0.341633, valid loss:0.358474
Epoch:9, Train loss:0.341567, valid loss:0.357937
Epoch:10, Train loss:0.341532, valid loss:0.357852
Epoch:11, Train loss:0.340276, valid loss:0.357520
Epoch:12, Train loss:0.340161, valid loss:0.357473
Epoch:13, Train loss:0.340131, valid loss:0.357249
Epoch:14, Train loss:0.340101, valid loss:0.357315
Epoch:15, Train loss:0.340114, valid loss:0.357293
Epoch:16, Train loss:0.340123, valid loss:0.357381
Epoch:17, Train loss:0.340042, valid loss:0.357587
Epoch:18, Train loss:0.339975, valid loss:0.357403
Epoch:19, Train loss:0.339981, valid loss:0.357300
Epoch:20, Train loss:0.340042, valid loss:0.357221
Epoch:21, Train loss:0.339331, valid loss:0.357014
Epoch:22, Train loss:0.339342, valid loss:0.357095
Epoch:23, Train loss:0.339332, valid loss:0.357287
Epoch:24, Train loss:0.339383, valid loss:0.356996
Epoch:25, Train loss:0.339326, valid loss:0.357089
Epoch:26, Train loss:0.339309, valid loss:0.357620
Epoch:27, Train loss:0.339325, valid loss:0.357139
Epoch:28, Train loss:0.339305, valid loss:0.357023
Epoch:29, Train loss:0.339272, valid loss:0.357187
Epoch:30, Train loss:0.339303, valid loss:0.357029
Epoch:31, Train loss:0.338995, valid loss:0.356941
Epoch:32, Train loss:0.338948, valid loss:0.356950
Epoch:33, Train loss:0.338958, valid loss:0.357081
Epoch:34, Train loss:0.338957, valid loss:0.356822
Epoch:35, Train loss:0.338984, valid loss:0.356998
Epoch:36, Train loss:0.338932, valid loss:0.357081
Epoch:37, Train loss:0.338964, valid loss:0.356874
Epoch:38, Train loss:0.338927, valid loss:0.356926
Epoch:39, Train loss:0.338945, valid loss:0.356915
Epoch:40, Train loss:0.338932, valid loss:0.356905
Epoch:41, Train loss:0.338785, valid loss:0.356822
Epoch:42, Train loss:0.338763, valid loss:0.356814
Epoch:43, Train loss:0.338763, valid loss:0.356837
Epoch:44, Train loss:0.338774, valid loss:0.356807
Epoch:45, Train loss:0.338769, valid loss:0.356810
Epoch:46, Train loss:0.338760, valid loss:0.356842
Epoch:47, Train loss:0.338780, valid loss:0.356821
Epoch:48, Train loss:0.338761, valid loss:0.356788
Epoch:49, Train loss:0.338741, valid loss:0.356895
Epoch:50, Train loss:0.338755, valid loss:0.356796
Epoch:51, Train loss:0.338682, valid loss:0.356760
Epoch:52, Train loss:0.338670, valid loss:0.356751
Epoch:53, Train loss:0.338692, valid loss:0.356758
Epoch:54, Train loss:0.338674, valid loss:0.356762
Epoch:55, Train loss:0.338665, valid loss:0.356772
Epoch:56, Train loss:0.338675, valid loss:0.356767
Epoch:57, Train loss:0.338665, valid loss:0.356765
Epoch:58, Train loss:0.338664, valid loss:0.356777
Epoch:59, Train loss:0.338666, valid loss:0.356764
Epoch:60, Train loss:0.338658, valid loss:0.356766
training time 6370.229686975479
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7306941383994785
plot_id,batch_id 0 1 miss% 0.7906059962401077
plot_id,batch_id 0 2 miss% 0.8002997401414966
plot_id,batch_id 0 3 miss% 0.8103378779708318
plot_id,batch_id 0 4 miss% 0.808196547959513
plot_id,batch_id 0 5 miss% 0.7260346899369837
plot_id,batch_id 0 6 miss% 0.7841609670706273
plot_id,batch_id 0 7 miss% 0.7988549870516088
plot_id,batch_id 0 8 miss% 0.8064712050044683
plot_id,batch_id 0 9 miss% 0.8113153108557528
plot_id,batch_id 0 10 miss% 0.6997603490528982
plot_id,batch_id 0 11 miss% 0.7821125029807019
plot_id,batch_id 0 12 miss% 0.7963040123121807
plot_id,batch_id 0 13 miss% 0.8031220929219508
plot_id,batch_id 0 14 miss% 0.8112011442421833
plot_id,batch_id 0 15 miss% 0.7118385967412268
plot_id,batch_id 0 16 miss% 0.7820363223917797
plot_id,batch_id 0 17 miss% 0.7961482264984613
plot_id,batch_id 0 18 miss% 0.80552690744756
plot_id,batch_id 0 19 miss% 0.8051889584735751
plot_id,batch_id 0 20 miss% 0.7651601459223999
plot_id,batch_id 0 21 miss% 0.803013751773813
plot_id,batch_id 0 22 miss% 0.8087101293634271
plot_id,batch_id 0 23 miss% 0.8128351149113656
plot_id,batch_id 0 24 miss% 0.8147427601580755
plot_id,batch_id 0 25 miss% 0.754869194736998
plot_id,batch_id 0 26 miss% 0.799780779945537
plot_id,batch_id 0 27 miss% 0.8033741323664794
plot_id,batch_id 0 28 miss% 0.8079816854267888
plot_id,batch_id 0 29 miss% 0.8084250440068154
plot_id,batch_id 0 30 miss% 0.7620598752602178
plot_id,batch_id 0 31 miss% 0.7961009417196668
plot_id,batch_id 0 32 miss% 0.8040071436107431
plot_id,batch_id 0 33 miss% 0.8072179994939322
plot_id,batch_id 0 34 miss% 0.8081606128468944
plot_id,batch_id 0 35 miss% 0.7521626754967402
plot_id,batch_id 0 36 miss% 0.8005987458929923
plot_id,batch_id 0 37 miss% 0.8029728763419498
plot_id,batch_id 0 38 miss% 0.8085487986544234
plot_id,batch_id 0 39 miss% 0.8107942404521422
plot_id,batch_id 0 40 miss% 0.7955946998970594
plot_id,batch_id 0 41 miss% 0.8049125343846171
plot_id,batch_id 0 42 miss% 0.8099957200175737
plot_id,batch_id 0 43 miss% 0.8122117318951398
plot_id,batch_id 0 44 miss% 0.8159836275187772
plot_id,batch_id 0 45 miss% 0.7803460384832731
plot_id,batch_id 0 46 miss% 0.8052831180279793
plot_id,batch_id 0 47 miss% 0.8121947402349432
plot_id,batch_id 0 48 miss% 0.8136875765296804
plot_id,batch_id 0 49 miss% 0.8159910191656122
plot_id,batch_id 0 50 miss% 0.785418587574292
plot_id,batch_id 0 51 miss% 0.8033152204215994
plot_id,batch_id 0 52 miss% 0.808894287502449
plot_id,batch_id 0 53 miss% 0.811603188072213
plot_id,batch_id 0 54 miss% 0.8180076338046312
plot_id,batch_id 0 55 miss% 0.7632770685074519
plot_id,batch_id 0 56 miss% 0.8020354936002908
plot_id,batch_id 0 57 miss% 0.8100345071962194
plot_id,batch_id 0 58 miss% 0.8129689228038459
plot_id,batch_id 0 59 miss% 0.8165526887129234
plot_id,batch_id 0 60 miss% 0.6466289523613765
plot_id,batch_id 0 61 miss% 0.7506729874943875
plot_id,batch_id 0 62 miss% 0.7866552436911604
plot_id,batch_id 0 63 miss% 0.7976601563310113
plot_id,batch_id 0 64 miss% 0.8015533867039997
plot_id,batch_id 0 65 miss% 0.6412913907883134
plot_id,batch_id 0 66 miss% 0.7492829928781656
plot_id,batch_id 0 67 miss% 0.7720670674634225
plot_id,batch_id 0 68 miss% 0.7946960419661737
plot_id,batch_id 0 69 miss% 0.7953491695102708
plot_id,batch_id 0 70 miss% 0.6069506290571925
plot_id,batch_id 0 71 miss% 0.7620980488717441
plot_id,batch_id 0 72 miss% 0.7631682212590398
plot_id,batch_id 0 73 miss% 0.7790028308478515
plot_id,batch_id 0 74 miss% 0.7908674573953893
plot_id,batch_id 0 75 miss% 0.6106304134510664
plot_id,batch_id 0 76 miss% 0.7092242263711951
plot_id,batch_id 0 77 miss% 0.7531107368838103
plot_id,batch_id 0 78 miss% 0.777116494669963
plot_id,batch_id 0 79 miss% 0.7853439132967458
plot_id,batch_id 0 80 miss% 0.6729426388267373
plot_id,batch_id 0 81 miss% 0.777284585684484
plot_id,batch_id 0 82 miss% 0.7918841565149423
plot_id,batch_id 0 83 miss% 0.8006684483073316
plot_id,batch_id 0 84 miss% 0.8014754054445282
plot_id,batch_id 0 85 miss% 0.6679435673210682
plot_id,batch_id 0 86 miss% 0.7684528385148597
plot_id,batch_id 0 87 miss% 0.7872088783076293
plot_id,batch_id 0 88 miss% 0.7995580236716794
plot_id,batch_id 0 89 miss% 0.8003378083094786
plot_id,batch_id 0 90 miss% 0.6430750625827708
plot_id,batch_id 0 91 miss% 0.7671222196967578
plot_id,batch_id 0 92 miss% 0.7776828362065659
plot_id,batch_id 0 93 miss% 0.7894339078760362
plot_id,batch_id 0 94 miss% 0.8007049564797016
plot_id,batch_id 0 95 miss% 0.6449623905065281
plot_id,batch_id 0 96 miss% 0.7527620032315364
plot_id,batch_id 0 97 miss% 0.7735808498538446
plot_id,batch_id 0 98 miss% 0.7879960525211027
plot_id,batch_id 0 99 miss% 0.7959753032180785
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73069414 0.790606   0.80029974 0.81033788 0.80819655 0.72603469
 0.78416097 0.79885499 0.80647121 0.81131531 0.69976035 0.7821125
 0.79630401 0.80312209 0.81120114 0.7118386  0.78203632 0.79614823
 0.80552691 0.80518896 0.76516015 0.80301375 0.80871013 0.81283511
 0.81474276 0.75486919 0.79978078 0.80337413 0.80798169 0.80842504
 0.76205988 0.79610094 0.80400714 0.807218   0.80816061 0.75216268
 0.80059875 0.80297288 0.8085488  0.81079424 0.7955947  0.80491253
 0.80999572 0.81221173 0.81598363 0.78034604 0.80528312 0.81219474
 0.81368758 0.81599102 0.78541859 0.80331522 0.80889429 0.81160319
 0.81800763 0.76327707 0.80203549 0.81003451 0.81296892 0.81655269
 0.64662895 0.75067299 0.78665524 0.79766016 0.80155339 0.64129139
 0.74928299 0.77206707 0.79469604 0.79534917 0.60695063 0.76209805
 0.76316822 0.77900283 0.79086746 0.61063041 0.70922423 0.75311074
 0.77711649 0.78534391 0.67294264 0.77728459 0.79188416 0.80066845
 0.80147541 0.66794357 0.76845284 0.78720888 0.79955802 0.80033781
 0.64307506 0.76712222 0.77768284 0.78943391 0.80070496 0.64496239
 0.752762   0.77358085 0.78799605 0.7959753 ]
for model  139 the mean error 0.7776445792079332
all id 139 hidden_dim 24 learning_rate 0.02 num_layers 3 frames 25 out win 4 err 0.7776445792079332
Launcher: Job 140 completed in 6549 seconds.
Launcher: Task 239 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  28945
Epoch:0, Train loss:0.621320, valid loss:0.614854
Epoch:1, Train loss:0.053314, valid loss:0.010210
Epoch:2, Train loss:0.015266, valid loss:0.007394
Epoch:3, Train loss:0.011890, valid loss:0.006800
Epoch:4, Train loss:0.010836, valid loss:0.005945
Epoch:5, Train loss:0.009839, valid loss:0.006689
Epoch:6, Train loss:0.008819, valid loss:0.004910
Epoch:7, Train loss:0.008383, valid loss:0.004739
Epoch:8, Train loss:0.008266, valid loss:0.003409
Epoch:9, Train loss:0.007354, valid loss:0.003681
Epoch:10, Train loss:0.007145, valid loss:0.003398
Epoch:11, Train loss:0.005066, valid loss:0.002723
Epoch:12, Train loss:0.005100, valid loss:0.003907
Epoch:13, Train loss:0.005084, valid loss:0.002502
Epoch:14, Train loss:0.004802, valid loss:0.004182
Epoch:15, Train loss:0.005193, valid loss:0.002305
Epoch:16, Train loss:0.005729, valid loss:0.002895
Epoch:17, Train loss:0.004704, valid loss:0.002414
Epoch:18, Train loss:0.004487, valid loss:0.002365
Epoch:19, Train loss:0.004582, valid loss:0.002763
Epoch:20, Train loss:0.004646, valid loss:0.002163
Epoch:21, Train loss:0.003369, valid loss:0.001761
Epoch:22, Train loss:0.003413, valid loss:0.001859
Epoch:23, Train loss:0.003323, valid loss:0.001684
Epoch:24, Train loss:0.003325, valid loss:0.002497
Epoch:25, Train loss:0.003246, valid loss:0.001934
Epoch:26, Train loss:0.003302, valid loss:0.002055
Epoch:27, Train loss:0.003393, valid loss:0.001850
Epoch:28, Train loss:0.003195, valid loss:0.001944
Epoch:29, Train loss:0.003239, valid loss:0.001952
Epoch:30, Train loss:0.003152, valid loss:0.001934
Epoch:31, Train loss:0.002615, valid loss:0.001515
Epoch:32, Train loss:0.002553, valid loss:0.001790
Epoch:33, Train loss:0.002584, valid loss:0.001545
Epoch:34, Train loss:0.002604, valid loss:0.001695
Epoch:35, Train loss:0.002622, valid loss:0.001675
Epoch:36, Train loss:0.002560, valid loss:0.001625
Epoch:37, Train loss:0.002525, valid loss:0.001703
Epoch:38, Train loss:0.002560, valid loss:0.001565
Epoch:39, Train loss:0.002544, valid loss:0.001595
Epoch:40, Train loss:0.002596, valid loss:0.001715
Epoch:41, Train loss:0.002220, valid loss:0.001422
Epoch:42, Train loss:0.002215, valid loss:0.001551
Epoch:43, Train loss:0.002302, valid loss:0.001449
Epoch:44, Train loss:0.002193, valid loss:0.001459
Epoch:45, Train loss:0.002190, valid loss:0.001449
Epoch:46, Train loss:0.002199, valid loss:0.001554
Epoch:47, Train loss:0.002193, valid loss:0.001483
Epoch:48, Train loss:0.002188, valid loss:0.001447
Epoch:49, Train loss:0.002209, valid loss:0.001388
Epoch:50, Train loss:0.002154, valid loss:0.001588
Epoch:51, Train loss:0.002081, valid loss:0.001398
Epoch:52, Train loss:0.002010, valid loss:0.001395
Epoch:53, Train loss:0.002004, valid loss:0.001369
Epoch:54, Train loss:0.002002, valid loss:0.001356
Epoch:55, Train loss:0.002020, valid loss:0.001356
Epoch:56, Train loss:0.002011, valid loss:0.001393
Epoch:57, Train loss:0.002001, valid loss:0.001388
Epoch:58, Train loss:0.002001, valid loss:0.001365
Epoch:59, Train loss:0.002001, valid loss:0.001349
Epoch:60, Train loss:0.001993, valid loss:0.001336
training time 6414.523092031479
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.04662131625453511
plot_id,batch_id 0 1 miss% 0.02705641331465891
plot_id,batch_id 0 2 miss% 0.025460092481223395
plot_id,batch_id 0 3 miss% 0.019646250766147455
plot_id,batch_id 0 4 miss% 0.017697688647640198
plot_id,batch_id 0 5 miss% 0.03431548377850345
plot_id,batch_id 0 6 miss% 0.029225497720352746
plot_id,batch_id 0 7 miss% 0.031763032478549176
plot_id,batch_id 0 8 miss% 0.029677783596806624
plot_id,batch_id 0 9 miss% 0.026082068331108207
plot_id,batch_id 0 10 miss% 0.06485625428392292
plot_id,batch_id 0 11 miss% 0.04358404839410296
plot_id,batch_id 0 12 miss% 0.02394768188146607
plot_id,batch_id 0 13 miss% 0.031727333408372776
plot_id,batch_id 0 14 miss% 0.03565485100786091
plot_id,batch_id 0 15 miss% 0.06184831180727937
plot_id,batch_id 0 16 miss% 0.023112554253218454
plot_id,batch_id 0 17 miss% 0.07803436222551333
plot_id,batch_id 0 18 miss% 0.03189366369695794
plot_id,batch_id 0 19 miss% 0.04621361591065339
plot_id,batch_id 0 20 miss% 0.03925577411373168
plot_id,batch_id 0 21 miss% 0.02233363262137298
plot_id,batch_id 0 22 miss% 0.02774437159308937
plot_id,batch_id 0 23 miss% 0.027768461635695067
plot_id,batch_id 0 24 miss% 0.026467896767231408
plot_id,batch_id 0 25 miss% 0.04825686342734461
plot_id,batch_id 0 26 miss% 0.01655617274926575
plot_id,batch_id 0 27 miss% 0.02224985098400786
plot_id,batch_id 0 28 miss% 0.02973168175136574
plot_id,batch_id 0 29 miss% 0.009774105531602266
plot_id,batch_id 0 30 miss% 0.04578747507264049
plot_id,batch_id 0 31 miss% 0.03051512475481722
plot_id,batch_id 0 32 miss% 0.04003352307617413
plot_id,batch_id 0 33 miss% 0.0270853903468708
plot_id,batch_id 0 34 miss% 0.019723420122978313
plot_id,batch_id 0 35 miss% 0.03723632891759566
plot_id,batch_id 0 36 miss% 0.04537640477431733
plot_id,batch_id 0 37 miss% 0.03380978639554535
plot_id,batch_id 0 38 miss% 0.027796854337810716
plot_id,batch_id 0 39 miss% 0.027956228815198082
plot_id,batch_id 0 40 miss% 0.0641243941303547
plot_id,batch_id 0 41 miss% 0.019048678992409637
plot_id,batch_id 0 42 miss% 0.02173694507981433
plot_id,batch_id 0 43 miss% 0.025961619053146513
plot_id,batch_id 0 44 miss% 0.023010531506272535
plot_id,batch_id 0 45 miss% 0.03463953596659149
plot_id,batch_id 0 46 miss% 0.01784430811955583
plot_id,batch_id 0 47 miss% 0.01770902742225787
plot_id,batch_id 0 48 miss% 0.021895193468302945
plot_id,batch_id 0 49 miss% 0.020596921195667077
plot_id,batch_id 0 50 miss% 0.027884427575218113
plot_id,batch_id 0 51 miss% 0.01792317661585624
plot_id,batch_id 0 52 miss% 0.016717657238984922
plot_id,batch_id 0 53 miss% 0.021423653155555576
plot_id,batch_id 0 54 miss% 0.02250974852984963
plot_id,batch_id 0 55 miss% 0.03243641934932294
plot_id,batch_id 0 56 miss% 0.022976631592617015
plot_id,batch_id 0 57 miss% 0.019347590778100634
plot_id,batch_id 0 58 miss% 0.02627190009579957
plot_id,batch_id 0 59 miss% 0.01281914680080392
plot_id,batch_id 0 60 miss% 0.04523395470503129
plot_id,batch_id 0 61 miss% 0.027725168267172785
plot_id,batch_id 0 62 miss% 0.03378062740507901
plot_id,batch_id 0 63 miss% 0.042987779017476864
plot_id,batch_id 0 64 miss% 0.037850838557374034
plot_id,batch_id 0 65 miss% 0.06253038855534841
plot_id,batch_id 0 66 miss% 0.05971556584234289
plot_id,batch_id 0 67 miss% 0.028500743479474195
plot_id,batch_id 0 68 miss% 0.0394627659380624
plot_id,batch_id 0 69 miss% 0.02358681076141866
plot_id,batch_id 0 70 miss% 0.06459962004950451
plot_id,batch_id 0 71 miss% 0.05923701240581111
plot_id,batch_id 0 72 miss% 0.045927502942945925
plot_id,batch_id 0 73 miss% 0.023049553305853954
plot_id,batch_id 0 74 miss% 0.0489553348925706
plot_id,batch_id 0 75 miss% 0.05004024334548882
plot_id,batch_id 0 76 miss% 0.039221960106848856
plot_id,batch_id 0 77 miss% 0.040619896442427014
plot_id,batch_id 0 78 miss% 0.034701258711245894
plot_id,batch_id 0 79 miss% 0.03902070773488745
plot_id,batch_id 0 80 miss% 0.07021811496901388
plot_id,batch_id 0 81 miss% 0.02924393372188578
plot_id,batch_id 0 82 miss% 0.026549889043793254
plot_id,batch_id 0 83 miss% 0.04283614313153836
plot_id,batch_id 0 84 miss% 0.0360006764200154
plot_id,batch_id 0 85 miss% 0.04611432103880668
plot_id,batch_id 0 86 miss% 0.03812598813715416
plot_id,batch_id 0 87 miss% 0.02935445152866939
plot_id,batch_id 0 88 miss% 0.05332393900120886
plot_id,batch_id 0 89 miss% 0.031756573117587485
plot_id,batch_id 0 90 miss% 0.06983622340866061
plot_id,batch_id 0 91 miss% 0.021987121783293477
plot_id,batch_id 0 92 miss% 0.039390911344774564
plot_id,batch_id 0 93 miss% 0.03428353691982899
plot_id,batch_id 0 94 miss% 0.054659890712174336
plot_id,batch_id 0 95 miss% 0.05716475574843822
plot_id,batch_id 0 96 miss% 0.030566106051880752
plot_id,batch_id 0 97 miss% 0.04404649181371683
plot_id,batch_id 0 98 miss% 0.04252233055013169
plot_id,batch_id 0 99 miss% 0.03280797288491005
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04662132 0.02705641 0.02546009 0.01964625 0.01769769 0.03431548
 0.0292255  0.03176303 0.02967778 0.02608207 0.06485625 0.04358405
 0.02394768 0.03172733 0.03565485 0.06184831 0.02311255 0.07803436
 0.03189366 0.04621362 0.03925577 0.02233363 0.02774437 0.02776846
 0.0264679  0.04825686 0.01655617 0.02224985 0.02973168 0.00977411
 0.04578748 0.03051512 0.04003352 0.02708539 0.01972342 0.03723633
 0.0453764  0.03380979 0.02779685 0.02795623 0.06412439 0.01904868
 0.02173695 0.02596162 0.02301053 0.03463954 0.01784431 0.01770903
 0.02189519 0.02059692 0.02788443 0.01792318 0.01671766 0.02142365
 0.02250975 0.03243642 0.02297663 0.01934759 0.0262719  0.01281915
 0.04523395 0.02772517 0.03378063 0.04298778 0.03785084 0.06253039
 0.05971557 0.02850074 0.03946277 0.02358681 0.06459962 0.05923701
 0.0459275  0.02304955 0.04895533 0.05004024 0.03922196 0.0406199
 0.03470126 0.03902071 0.07021811 0.02924393 0.02654989 0.04283614
 0.03600068 0.04611432 0.03812599 0.02935445 0.05332394 0.03175657
 0.06983622 0.02198712 0.03939091 0.03428354 0.05465989 0.05716476
 0.03056611 0.04404649 0.04252233 0.03280797]
for model  65 the mean error 0.03498292262487857
all id 65 hidden_dim 16 learning_rate 0.02 num_layers 4 frames 21 out win 5 err 0.03498292262487857
Launcher: Job 66 completed in 6631 seconds.
Launcher: Task 156 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  28945
Epoch:0, Train loss:0.343389, valid loss:0.337637
Epoch:1, Train loss:0.026003, valid loss:0.004896
Epoch:2, Train loss:0.006678, valid loss:0.003012
Epoch:3, Train loss:0.004852, valid loss:0.002793
Epoch:4, Train loss:0.004175, valid loss:0.002397
Epoch:5, Train loss:0.003614, valid loss:0.001920
Epoch:6, Train loss:0.003219, valid loss:0.001882
Epoch:7, Train loss:0.003009, valid loss:0.001350
Epoch:8, Train loss:0.002798, valid loss:0.001335
Epoch:9, Train loss:0.002611, valid loss:0.001390
Epoch:10, Train loss:0.002504, valid loss:0.001239
Epoch:11, Train loss:0.001970, valid loss:0.001086
Epoch:12, Train loss:0.001911, valid loss:0.000990
Epoch:13, Train loss:0.001874, valid loss:0.001112
Epoch:14, Train loss:0.001840, valid loss:0.001197
Epoch:15, Train loss:0.001814, valid loss:0.000945
Epoch:16, Train loss:0.001772, valid loss:0.001018
Epoch:17, Train loss:0.001718, valid loss:0.000991
Epoch:18, Train loss:0.001693, valid loss:0.000906
Epoch:19, Train loss:0.001671, valid loss:0.001121
Epoch:20, Train loss:0.001682, valid loss:0.001025
Epoch:21, Train loss:0.001392, valid loss:0.000853
Epoch:22, Train loss:0.001376, valid loss:0.000768
Epoch:23, Train loss:0.001367, valid loss:0.000796
Epoch:24, Train loss:0.001352, valid loss:0.000849
Epoch:25, Train loss:0.001351, valid loss:0.000736
Epoch:26, Train loss:0.001353, valid loss:0.000811
Epoch:27, Train loss:0.001348, valid loss:0.000878
Epoch:28, Train loss:0.001316, valid loss:0.000833
Epoch:29, Train loss:0.001316, valid loss:0.000803
Epoch:30, Train loss:0.001306, valid loss:0.000768
Epoch:31, Train loss:0.001170, valid loss:0.000745
Epoch:32, Train loss:0.001164, valid loss:0.000684
Epoch:33, Train loss:0.001151, valid loss:0.000736
Epoch:34, Train loss:0.001164, valid loss:0.000720
Epoch:35, Train loss:0.001139, valid loss:0.000689
Epoch:36, Train loss:0.001143, valid loss:0.000685
Epoch:37, Train loss:0.001139, valid loss:0.000660
Epoch:38, Train loss:0.001138, valid loss:0.000738
Epoch:39, Train loss:0.001138, valid loss:0.000793
Epoch:40, Train loss:0.001128, valid loss:0.000678
Epoch:41, Train loss:0.001065, valid loss:0.000645
Epoch:42, Train loss:0.001050, valid loss:0.000667
Epoch:43, Train loss:0.001048, valid loss:0.000666
Epoch:44, Train loss:0.001045, valid loss:0.000673
Epoch:45, Train loss:0.001049, valid loss:0.000661
Epoch:46, Train loss:0.001045, valid loss:0.000677
Epoch:47, Train loss:0.001049, valid loss:0.000656
Epoch:48, Train loss:0.001039, valid loss:0.000635
Epoch:49, Train loss:0.001037, valid loss:0.000628
Epoch:50, Train loss:0.001031, valid loss:0.000651
Epoch:51, Train loss:0.001001, valid loss:0.000621
Epoch:52, Train loss:0.001004, valid loss:0.000632
Epoch:53, Train loss:0.000995, valid loss:0.000623
Epoch:54, Train loss:0.000997, valid loss:0.000627
Epoch:55, Train loss:0.000992, valid loss:0.000629
Epoch:56, Train loss:0.000994, valid loss:0.000627
Epoch:57, Train loss:0.000994, valid loss:0.000622
Epoch:58, Train loss:0.000991, valid loss:0.000656
Epoch:59, Train loss:0.000992, valid loss:0.000617
Epoch:60, Train loss:0.000991, valid loss:0.000628
training time 6473.065586090088
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.02943817569854018
plot_id,batch_id 0 1 miss% 0.029686342375745547
plot_id,batch_id 0 2 miss% 0.029252284138186146
plot_id,batch_id 0 3 miss% 0.025569184478523473
plot_id,batch_id 0 4 miss% 0.03289325220179118
plot_id,batch_id 0 5 miss% 0.024732692426832185
plot_id,batch_id 0 6 miss% 0.03869919899490367
plot_id,batch_id 0 7 miss% 0.028969828873768644
plot_id,batch_id 0 8 miss% 0.03232215306797385
plot_id,batch_id 0 9 miss% 0.025093029481556856
plot_id,batch_id 0 10 miss% 0.03618948524798697
plot_id,batch_id 0 11 miss% 0.05787169319675615
plot_id,batch_id 0 12 miss% 0.03096629343567099
plot_id,batch_id 0 13 miss% 0.030429453774364814
plot_id,batch_id 0 14 miss% 0.031413004134065815
plot_id,batch_id 0 15 miss% 0.05520924924303672
plot_id,batch_id 0 16 miss% 0.04225575142071196
plot_id,batch_id 0 17 miss% 0.05242348869254257
plot_id,batch_id 0 18 miss% 0.028020939360730645
plot_id,batch_id 0 19 miss% 0.03207849442322364
plot_id,batch_id 0 20 miss% 0.056464188498954536
plot_id,batch_id 0 21 miss% 0.0254945308144637
plot_id,batch_id 0 22 miss% 0.022026884918679143
plot_id,batch_id 0 23 miss% 0.022663984826561796
plot_id,batch_id 0 24 miss% 0.0325937601364854
plot_id,batch_id 0 25 miss% 0.04257202607610483
plot_id,batch_id 0 26 miss% 0.04035825880667621
plot_id,batch_id 0 27 miss% 0.030404751413508322
plot_id,batch_id 0 28 miss% 0.021351143454781712
plot_id,batch_id 0 29 miss% 0.02615028623181485
plot_id,batch_id 0 30 miss% 0.05560929625303168
plot_id,batch_id 0 31 miss% 0.03723893629966814
plot_id,batch_id 0 32 miss% 0.023570070654640063
plot_id,batch_id 0 33 miss% 0.025484830493426373
plot_id,batch_id 0 34 miss% 0.02264565066551673
plot_id,batch_id 0 35 miss% 0.051430458633043644
plot_id,batch_id 0 36 miss% 0.03721326540705796
plot_id,batch_id 0 37 miss% 0.026442043308476337
plot_id,batch_id 0 38 miss% 0.03066320501610113
plot_id,batch_id 0 39 miss% 0.03589121557004562
plot_id,batch_id 0 40 miss% 0.10596206042255228
plot_id,batch_id 0 41 miss% 0.026806160863504677
plot_id,batch_id 0 42 miss% 0.017679315710016175
plot_id,batch_id 0 43 miss% 0.02433248382531033
plot_id,batch_id 0 44 miss% 0.0272201370484101
plot_id,batch_id 0 45 miss% 0.04217009327746333
plot_id,batch_id 0 46 miss% 0.026491130321890127
plot_id,batch_id 0 47 miss% 0.022514941463457845
plot_id,batch_id 0 48 miss% 0.02134368267349055
plot_id,batch_id 0 49 miss% 0.029493349012863657
plot_id,batch_id 0 50 miss% 0.027984394058100246
plot_id,batch_id 0 51 miss% 0.029823740921594906
plot_id,batch_id 0 52 miss% 0.02706652942390991
plot_id,batch_id 0 53 miss% 0.015518747252557828
plot_id,batch_id 0 54 miss% 0.04458953792447749
plot_id,batch_id 0 55 miss% 0.05178636330024474
plot_id,batch_id 0 56 miss% 0.025090585082502108
plot_id,batch_id 0 57 miss% 0.025378749675492217
plot_id,batch_id 0 58 miss% 0.027644180617331548
plot_id,batch_id 0 59 miss% 0.029978726847421637
plot_id,batch_id 0 60 miss% 0.05046621592851015
plot_id,batch_id 0 61 miss% 0.045207242599768105
plot_id,batch_id 0 62 miss% 0.03246605507486809
plot_id,batch_id 0 63 miss% 0.039511366003404685
plot_id,batch_id 0 64 miss% 0.03473764091817411
plot_id,batch_id 0 65 miss% 0.0484436311109364
plot_id,batch_id 0 66 miss% 0.023947507181970622
plot_id,batch_id 0 67 miss% 0.03588883923739832
plot_id,batch_id 0 68 miss% 0.044121457153066194
plot_id,batch_id 0 69 miss% 0.025073634440091384
plot_id,batch_id 0 70 miss% 0.03926700323297127
plot_id,batch_id 0 71 miss% 0.04090214593425502
plot_id,batch_id 0 72 miss% 0.03769296687155866
plot_id,batch_id 0 73 miss% 0.034068000116705
plot_id,batch_id 0 74 miss% 0.038303934637165936
plot_id,batch_id 0 75 miss% 0.04527558882792174
plot_id,batch_id 0 76 miss% 0.05487207430573181
plot_id,batch_id 0 77 miss% 0.0315468243006056
plot_id,batch_id 0 78 miss% 0.03852596993481176
plot_id,batch_id 0 79 miss% 0.04693596361678362
plot_id,batch_id 0 80 miss% 0.05326321712256013
plot_id,batch_id 0 81 miss% 0.020331884072677397
plot_id,batch_id 0 82 miss% 0.03163319570689711
plot_id,batch_id 0 83 miss% 0.030256983783442395
plot_id,batch_id 0 84 miss% 0.027830324768206585
plot_id,batch_id 0 85 miss% 0.060999520057823775
plot_id,batch_id 0 86 miss% 0.02754386348279692
plot_id,batch_id 0 87 miss% 0.039822888476655965
plot_id,batch_id 0 88 miss% 0.03322030210135179
plot_id,batch_id 0 89 miss% 0.02669574864268273
plot_id,batch_id 0 90 miss% 0.0503211472937175
plot_id,batch_id 0 91 miss% 0.025630777637116707
plot_id,batch_id 0 92 miss% 0.0326937220946212
plot_id,batch_id 0 93 miss% 0.023938928596806536
plot_id,batch_id 0 94 miss% 0.030759088398056307
plot_id,batch_id 0 95 miss% 0.03862081500454492
plot_id,batch_id 0 96 miss% 0.030950799364991736
plot_id,batch_id 0 97 miss% 0.032876248730917663
plot_id,batch_id 0 98 miss% 0.029906098217423703
plot_id,batch_id 0 99 miss% 0.032426767073149614
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02943818 0.02968634 0.02925228 0.02556918 0.03289325 0.02473269
 0.0386992  0.02896983 0.03232215 0.02509303 0.03618949 0.05787169
 0.03096629 0.03042945 0.031413   0.05520925 0.04225575 0.05242349
 0.02802094 0.03207849 0.05646419 0.02549453 0.02202688 0.02266398
 0.03259376 0.04257203 0.04035826 0.03040475 0.02135114 0.02615029
 0.0556093  0.03723894 0.02357007 0.02548483 0.02264565 0.05143046
 0.03721327 0.02644204 0.03066321 0.03589122 0.10596206 0.02680616
 0.01767932 0.02433248 0.02722014 0.04217009 0.02649113 0.02251494
 0.02134368 0.02949335 0.02798439 0.02982374 0.02706653 0.01551875
 0.04458954 0.05178636 0.02509059 0.02537875 0.02764418 0.02997873
 0.05046622 0.04520724 0.03246606 0.03951137 0.03473764 0.04844363
 0.02394751 0.03588884 0.04412146 0.02507363 0.039267   0.04090215
 0.03769297 0.034068   0.03830393 0.04527559 0.05487207 0.03154682
 0.03852597 0.04693596 0.05326322 0.02033188 0.0316332  0.03025698
 0.02783032 0.06099952 0.02754386 0.03982289 0.0332203  0.02669575
 0.05032115 0.02563078 0.03269372 0.02393893 0.03075909 0.03862082
 0.0309508  0.03287625 0.0299061  0.03242677]
for model  172 the mean error 0.03475634073921681
all id 172 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 31 out win 4 err 0.03475634073921681
Launcher: Job 173 completed in 6665 seconds.
Launcher: Task 14 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  46193
Epoch:0, Train loss:0.507051, valid loss:0.475426
Epoch:1, Train loss:0.357946, valid loss:0.361502
Epoch:2, Train loss:0.347253, valid loss:0.359951
Epoch:3, Train loss:0.345519, valid loss:0.361016
Epoch:4, Train loss:0.344631, valid loss:0.359692
Epoch:5, Train loss:0.344024, valid loss:0.359566
Epoch:6, Train loss:0.343781, valid loss:0.359477
Epoch:7, Train loss:0.343453, valid loss:0.359091
Epoch:8, Train loss:0.343246, valid loss:0.358790
Epoch:9, Train loss:0.343096, valid loss:0.359056
Epoch:10, Train loss:0.343079, valid loss:0.358565
Epoch:11, Train loss:0.342003, valid loss:0.358243
Epoch:12, Train loss:0.342093, valid loss:0.358218
Epoch:13, Train loss:0.342010, valid loss:0.358318
Epoch:14, Train loss:0.341924, valid loss:0.358166
Epoch:15, Train loss:0.341990, valid loss:0.358451
Epoch:16, Train loss:0.341853, valid loss:0.358487
Epoch:17, Train loss:0.341886, valid loss:0.358051
Epoch:18, Train loss:0.341814, valid loss:0.358307
Epoch:19, Train loss:0.341823, valid loss:0.358221
Epoch:20, Train loss:0.341731, valid loss:0.358248
Epoch:21, Train loss:0.341275, valid loss:0.357911
Epoch:22, Train loss:0.341308, valid loss:0.357997
Epoch:23, Train loss:0.341245, valid loss:0.357964
Epoch:24, Train loss:0.341283, valid loss:0.358031
Epoch:25, Train loss:0.341243, valid loss:0.357928
Epoch:26, Train loss:0.341243, valid loss:0.358072
Epoch:27, Train loss:0.341216, valid loss:0.357995
Epoch:28, Train loss:0.341195, valid loss:0.357997
Epoch:29, Train loss:0.341182, valid loss:0.358044
Epoch:30, Train loss:0.341227, valid loss:0.358008
Epoch:31, Train loss:0.340936, valid loss:0.357940
Epoch:32, Train loss:0.340918, valid loss:0.357872
Epoch:33, Train loss:0.340931, valid loss:0.357860
Epoch:34, Train loss:0.340922, valid loss:0.357919
Epoch:35, Train loss:0.340910, valid loss:0.357898
Epoch:36, Train loss:0.340914, valid loss:0.357944
Epoch:37, Train loss:0.340915, valid loss:0.357864
Epoch:38, Train loss:0.340898, valid loss:0.357885
Epoch:39, Train loss:0.340890, valid loss:0.357864
Epoch:40, Train loss:0.340906, valid loss:0.357975
Epoch:41, Train loss:0.340759, valid loss:0.357853
Epoch:42, Train loss:0.340761, valid loss:0.357846
Epoch:43, Train loss:0.340749, valid loss:0.357881
Epoch:44, Train loss:0.340748, valid loss:0.357892
Epoch:45, Train loss:0.340758, valid loss:0.357837
Epoch:46, Train loss:0.340746, valid loss:0.357815
Epoch:47, Train loss:0.340738, valid loss:0.357805
Epoch:48, Train loss:0.340750, valid loss:0.357798
Epoch:49, Train loss:0.340741, valid loss:0.357820
Epoch:50, Train loss:0.340739, valid loss:0.357833
Epoch:51, Train loss:0.340679, valid loss:0.357814
Epoch:52, Train loss:0.340673, valid loss:0.357801
Epoch:53, Train loss:0.340668, valid loss:0.357797
Epoch:54, Train loss:0.340669, valid loss:0.357801
Epoch:55, Train loss:0.340671, valid loss:0.357813
Epoch:56, Train loss:0.340669, valid loss:0.357797
Epoch:57, Train loss:0.340665, valid loss:0.357839
Epoch:58, Train loss:0.340661, valid loss:0.357795
Epoch:59, Train loss:0.340660, valid loss:0.357783
Epoch:60, Train loss:0.340662, valid loss:0.357802
training time 6533.326885700226
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7829387137907579
plot_id,batch_id 0 1 miss% 0.8327861446008515
plot_id,batch_id 0 2 miss% 0.8338325311768846
plot_id,batch_id 0 3 miss% 0.839476096380869
plot_id,batch_id 0 4 miss% 0.8430710111070464
plot_id,batch_id 0 5 miss% 0.7804546540699959
plot_id,batch_id 0 6 miss% 0.8234694566240313
plot_id,batch_id 0 7 miss% 0.8346098002245201
plot_id,batch_id 0 8 miss% 0.8401027388319632
plot_id,batch_id 0 9 miss% 0.8467042305978241
plot_id,batch_id 0 10 miss% 0.7659518716243583
plot_id,batch_id 0 11 miss% 0.824962150490566
plot_id,batch_id 0 12 miss% 0.8327048306308616
plot_id,batch_id 0 13 miss% 0.8389306385399963
plot_id,batch_id 0 14 miss% 0.8409378370847231
plot_id,batch_id 0 15 miss% 0.771011412651432
plot_id,batch_id 0 16 miss% 0.8246613617145562
plot_id,batch_id 0 17 miss% 0.8313111359798458
plot_id,batch_id 0 18 miss% 0.8411071324565816
plot_id,batch_id 0 19 miss% 0.8410350132338362
plot_id,batch_id 0 20 miss% 0.8177031136135756
plot_id,batch_id 0 21 miss% 0.8357280589355267
plot_id,batch_id 0 22 miss% 0.8422946292895543
plot_id,batch_id 0 23 miss% 0.8433442930160596
plot_id,batch_id 0 24 miss% 0.8443316933072752
plot_id,batch_id 0 25 miss% 0.8100285136761647
plot_id,batch_id 0 26 miss% 0.83444832657423
plot_id,batch_id 0 27 miss% 0.8382653121272686
plot_id,batch_id 0 28 miss% 0.8418152253078539
plot_id,batch_id 0 29 miss% 0.8438184514593561
plot_id,batch_id 0 30 miss% 0.7894362840731658
plot_id,batch_id 0 31 miss% 0.8314845585997213
plot_id,batch_id 0 32 miss% 0.8391521612446029
plot_id,batch_id 0 33 miss% 0.8418084467333506
plot_id,batch_id 0 34 miss% 0.8440306726983121
plot_id,batch_id 0 35 miss% 0.7955480789787098
plot_id,batch_id 0 36 miss% 0.838420042803567
plot_id,batch_id 0 37 miss% 0.8351577010417585
plot_id,batch_id 0 38 miss% 0.8440669083555133
plot_id,batch_id 0 39 miss% 0.84522738372755
plot_id,batch_id 0 40 miss% 0.8217977611187419
plot_id,batch_id 0 41 miss% 0.8405928503611771
plot_id,batch_id 0 42 miss% 0.8401320692230172
plot_id,batch_id 0 43 miss% 0.8463199211686767
plot_id,batch_id 0 44 miss% 0.849551553039009
plot_id,batch_id 0 45 miss% 0.8195762769198249
plot_id,batch_id 0 46 miss% 0.8400476111944415
plot_id,batch_id 0 47 miss% 0.8410301053449522
plot_id,batch_id 0 48 miss% 0.8443727542725503
plot_id,batch_id 0 49 miss% 0.8493769709723321
plot_id,batch_id 0 50 miss% 0.8279362303099903
plot_id,batch_id 0 51 miss% 0.8370283390824153
plot_id,batch_id 0 52 miss% 0.8416976870424486
plot_id,batch_id 0 53 miss% 0.8446747470691399
plot_id,batch_id 0 54 miss% 0.8465853247440344
plot_id,batch_id 0 55 miss% 0.8174088881339963
plot_id,batch_id 0 56 miss% 0.8401413734792722
plot_id,batch_id 0 57 miss% 0.8414464939810521
plot_id,batch_id 0 58 miss% 0.8441475490742975
plot_id,batch_id 0 59 miss% 0.843904526765812
plot_id,batch_id 0 60 miss% 0.7157615901496753
plot_id,batch_id 0 61 miss% 0.811637350190553
plot_id,batch_id 0 62 miss% 0.828650937242565
plot_id,batch_id 0 63 miss% 0.8322315553458837
plot_id,batch_id 0 64 miss% 0.8400833046685624
plot_id,batch_id 0 65 miss% 0.7191370355890664
plot_id,batch_id 0 66 miss% 0.8000485717645445
plot_id,batch_id 0 67 miss% 0.8143023959449673
plot_id,batch_id 0 68 miss% 0.8300872305254707
plot_id,batch_id 0 69 miss% 0.8311234821738225
plot_id,batch_id 0 70 miss% 0.6762924681504712
plot_id,batch_id 0 71 miss% 0.810376661686815
plot_id,batch_id 0 72 miss% 0.8076292877842043
plot_id,batch_id 0 73 miss% 0.8182884947498802
plot_id,batch_id 0 74 miss% 0.8275744835682304
plot_id,batch_id 0 75 miss% 0.6839731764127479
plot_id,batch_id 0 76 miss% 0.796193501328773
plot_id,batch_id 0 77 miss% 0.8022865311673801
plot_id,batch_id 0 78 miss% 0.8200908080036771
plot_id,batch_id 0 79 miss% 0.8221960538274687
plot_id,batch_id 0 80 miss% 0.7439241042306497
plot_id,batch_id 0 81 miss% 0.8196025409656115
plot_id,batch_id 0 82 miss% 0.8294872832452158
plot_id,batch_id 0 83 miss% 0.8352516234668046
plot_id,batch_id 0 84 miss% 0.8390328719374232
plot_id,batch_id 0 85 miss% 0.7354921947174518
plot_id,batch_id 0 86 miss% 0.8111282916493404
plot_id,batch_id 0 87 miss% 0.8292711850112604
plot_id,batch_id 0 88 miss% 0.8358162722122112
plot_id,batch_id 0 89 miss% 0.8345156713192042
plot_id,batch_id 0 90 miss% 0.6976248449084341
plot_id,batch_id 0 91 miss% 0.8035265667281902
plot_id,batch_id 0 92 miss% 0.8196572737163527
plot_id,batch_id 0 93 miss% 0.8246901894068184
plot_id,batch_id 0 94 miss% 0.8390778913004519
plot_id,batch_id 0 95 miss% 0.7197532436561107
plot_id,batch_id 0 96 miss% 0.7954774357912096
plot_id,batch_id 0 97 miss% 0.8174579140125867
plot_id,batch_id 0 98 miss% 0.8246007378882741
plot_id,batch_id 0 99 miss% 0.8315620553988969
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.78293871 0.83278614 0.83383253 0.8394761  0.84307101 0.78045465
 0.82346946 0.8346098  0.84010274 0.84670423 0.76595187 0.82496215
 0.83270483 0.83893064 0.84093784 0.77101141 0.82466136 0.83131114
 0.84110713 0.84103501 0.81770311 0.83572806 0.84229463 0.84334429
 0.84433169 0.81002851 0.83444833 0.83826531 0.84181523 0.84381845
 0.78943628 0.83148456 0.83915216 0.84180845 0.84403067 0.79554808
 0.83842004 0.8351577  0.84406691 0.84522738 0.82179776 0.84059285
 0.84013207 0.84631992 0.84955155 0.81957628 0.84004761 0.84103011
 0.84437275 0.84937697 0.82793623 0.83702834 0.84169769 0.84467475
 0.84658532 0.81740889 0.84014137 0.84144649 0.84414755 0.84390453
 0.71576159 0.81163735 0.82865094 0.83223156 0.8400833  0.71913704
 0.80004857 0.8143024  0.83008723 0.83112348 0.67629247 0.81037666
 0.80762929 0.81828849 0.82757448 0.68397318 0.7961935  0.80228653
 0.82009081 0.82219605 0.7439241  0.81960254 0.82948728 0.83525162
 0.83903287 0.73549219 0.81112829 0.82927119 0.83581627 0.83451567
 0.69762484 0.80352657 0.81965727 0.82469019 0.83907789 0.71975324
 0.79547744 0.81745791 0.82460074 0.83156206]
for model  113 the mean error 0.8187885276250906
all id 113 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 25 out win 5 err 0.8187885276250906
Launcher: Job 114 completed in 6710 seconds.
Launcher: Task 33 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  28945
Epoch:0, Train loss:0.343389, valid loss:0.337637
Epoch:1, Train loss:0.020026, valid loss:0.004439
Epoch:2, Train loss:0.005874, valid loss:0.002643
Epoch:3, Train loss:0.004483, valid loss:0.002091
Epoch:4, Train loss:0.003883, valid loss:0.002115
Epoch:5, Train loss:0.003436, valid loss:0.002403
Epoch:6, Train loss:0.003108, valid loss:0.001648
Epoch:7, Train loss:0.002951, valid loss:0.001619
Epoch:8, Train loss:0.002832, valid loss:0.001554
Epoch:9, Train loss:0.002726, valid loss:0.001868
Epoch:10, Train loss:0.002624, valid loss:0.001610
Epoch:11, Train loss:0.001973, valid loss:0.001161
Epoch:12, Train loss:0.001950, valid loss:0.001175
Epoch:13, Train loss:0.001896, valid loss:0.001044
Epoch:14, Train loss:0.001859, valid loss:0.001041
Epoch:15, Train loss:0.001875, valid loss:0.000996
Epoch:16, Train loss:0.001787, valid loss:0.001047
Epoch:17, Train loss:0.001809, valid loss:0.001183
Epoch:18, Train loss:0.001779, valid loss:0.001126
Epoch:19, Train loss:0.001750, valid loss:0.001190
Epoch:20, Train loss:0.001717, valid loss:0.001116
Epoch:21, Train loss:0.001408, valid loss:0.000762
Epoch:22, Train loss:0.001371, valid loss:0.000789
Epoch:23, Train loss:0.001360, valid loss:0.000887
Epoch:24, Train loss:0.001392, valid loss:0.000758
Epoch:25, Train loss:0.001340, valid loss:0.000757
Epoch:26, Train loss:0.001337, valid loss:0.000838
Epoch:27, Train loss:0.001347, valid loss:0.000905
Epoch:28, Train loss:0.001325, valid loss:0.000872
Epoch:29, Train loss:0.001325, valid loss:0.000762
Epoch:30, Train loss:0.001311, valid loss:0.000869
Epoch:31, Train loss:0.001118, valid loss:0.000707
Epoch:32, Train loss:0.001131, valid loss:0.000678
Epoch:33, Train loss:0.001130, valid loss:0.000724
Epoch:34, Train loss:0.001125, valid loss:0.000695
Epoch:35, Train loss:0.001124, valid loss:0.000668
Epoch:36, Train loss:0.001107, valid loss:0.000764
Epoch:37, Train loss:0.001081, valid loss:0.000674
Epoch:38, Train loss:0.001120, valid loss:0.000650
Epoch:39, Train loss:0.001089, valid loss:0.000717
Epoch:40, Train loss:0.001081, valid loss:0.000696
Epoch:41, Train loss:0.000996, valid loss:0.000635
Epoch:42, Train loss:0.000986, valid loss:0.000637
Epoch:43, Train loss:0.000985, valid loss:0.000632
Epoch:44, Train loss:0.000985, valid loss:0.000639
Epoch:45, Train loss:0.000981, valid loss:0.000654
Epoch:46, Train loss:0.000973, valid loss:0.000653
Epoch:47, Train loss:0.000980, valid loss:0.000629
Epoch:48, Train loss:0.000967, valid loss:0.000664
Epoch:49, Train loss:0.000967, valid loss:0.000614
Epoch:50, Train loss:0.000974, valid loss:0.000611
Epoch:51, Train loss:0.000927, valid loss:0.000632
Epoch:52, Train loss:0.000915, valid loss:0.000605
Epoch:53, Train loss:0.000917, valid loss:0.000587
Epoch:54, Train loss:0.000914, valid loss:0.000614
Epoch:55, Train loss:0.000909, valid loss:0.000632
Epoch:56, Train loss:0.000915, valid loss:0.000620
Epoch:57, Train loss:0.000910, valid loss:0.000628
Epoch:58, Train loss:0.000913, valid loss:0.000662
Epoch:59, Train loss:0.000909, valid loss:0.000628
Epoch:60, Train loss:0.000906, valid loss:0.000624
training time 6524.775655031204
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.015765163124233574
plot_id,batch_id 0 1 miss% 0.023883194414176172
plot_id,batch_id 0 2 miss% 0.026911523435873306
plot_id,batch_id 0 3 miss% 0.03383470663357805
plot_id,batch_id 0 4 miss% 0.028402116230951447
plot_id,batch_id 0 5 miss% 0.042870601375533675
plot_id,batch_id 0 6 miss% 0.03657595416143582
plot_id,batch_id 0 7 miss% 0.026900678143714992
plot_id,batch_id 0 8 miss% 0.04078649436448002
plot_id,batch_id 0 9 miss% 0.027718798256728138
plot_id,batch_id 0 10 miss% 0.04595365943285766
plot_id,batch_id 0 11 miss% 0.03964871817908397
plot_id,batch_id 0 12 miss% 0.03332210323622616
plot_id,batch_id 0 13 miss% 0.028795908431777182
plot_id,batch_id 0 14 miss% 0.033155009757188575
plot_id,batch_id 0 15 miss% 0.053193439317991115
plot_id,batch_id 0 16 miss% 0.023708440920625535
plot_id,batch_id 0 17 miss% 0.04161511910183114
plot_id,batch_id 0 18 miss% 0.03990773791559528
plot_id,batch_id 0 19 miss% 0.039720347582725135
plot_id,batch_id 0 20 miss% 0.03890374099630035
plot_id,batch_id 0 21 miss% 0.02490925570745697
plot_id,batch_id 0 22 miss% 0.032394661235315446
plot_id,batch_id 0 23 miss% 0.025568118545044254
plot_id,batch_id 0 24 miss% 0.022695584883436183
plot_id,batch_id 0 25 miss% 0.036670904808327444
plot_id,batch_id 0 26 miss% 0.03579034031733618
plot_id,batch_id 0 27 miss% 0.023895052384115368
plot_id,batch_id 0 28 miss% 0.026256144617947133
plot_id,batch_id 0 29 miss% 0.026563163094003524
plot_id,batch_id 0 30 miss% 0.04895252859644292
plot_id,batch_id 0 31 miss% 0.03632301695796047
plot_id,batch_id 0 32 miss% 0.033808435152860854
plot_id,batch_id 0 33 miss% 0.03961692707582153
plot_id,batch_id 0 34 miss% 0.03440393098701028
plot_id,batch_id 0 35 miss% 0.03857698215834982
plot_id,batch_id 0 36 miss% 0.04486508675421571
plot_id,batch_id 0 37 miss% 0.028372951110300818
plot_id,batch_id 0 38 miss% 0.02814562660034485
plot_id,batch_id 0 39 miss% 0.03040170476141608
plot_id,batch_id 0 40 miss% 0.06768795134696297
plot_id,batch_id 0 41 miss% 0.04190958979349667
plot_id,batch_id 0 42 miss% 0.02587068138695822
plot_id,batch_id 0 43 miss% 0.04010000429002412
plot_id,batch_id 0 44 miss% 0.028686245341714482
plot_id,batch_id 0 45 miss% 0.028548655873317823
plot_id,batch_id 0 46 miss% 0.025823296034958445
plot_id,batch_id 0 47 miss% 0.02340170060222325
plot_id,batch_id 0 48 miss% 0.02690279020735642
plot_id,batch_id 0 49 miss% 0.0184210102483144
plot_id,batch_id 0 50 miss% 0.027488259836718952
plot_id,batch_id 0 51 miss% 0.030094344826578627
plot_id,batch_id 0 52 miss% 0.02069374794259097
plot_id,batch_id 0 53 miss% 0.017309207480526004
plot_id,batch_id 0 54 miss% 0.043884273603468076
plot_id,batch_id 0 55 miss% 0.04136173795445106
plot_id,batch_id 0 56 miss% 0.03162243398502778
plot_id,batch_id 0 57 miss% 0.03760398420601612
plot_id,batch_id 0 58 miss% 0.021408869867782666
plot_id,batch_id 0 59 miss% 0.02691449828315883
plot_id,batch_id 0 60 miss% 0.038081352215484476
plot_id,batch_id 0 61 miss% 0.026073642732062204
plot_id,batch_id 0 62 miss% 0.03834568181565417
plot_id,batch_id 0 63 miss% 0.028629338152820562
plot_id,batch_id 0 64 miss% 0.024092935124839645
plot_id,batch_id 0 65 miss% 0.039192055315719274
plot_id,batch_id 0 66 miss% 0.04175811866171738
plot_id,batch_id 0 67 miss% 0.030666923915662454
plot_id,batch_id 0 68 miss% 0.03163034969173841
plot_id,batch_id 0 69 miss% 0.028315203042329307
plot_id,batch_id 0 70 miss% 0.028606148627687938
plot_id,batch_id 0 71 miss% 0.042064666824506776
plot_id,batch_id 0 72 miss% 0.02727560920603752
plot_id,batch_id 0 73 miss% 0.0426884389602766
plot_id,batch_id 0 74 miss% 0.04063288553161948
plot_id,batch_id 0 75 miss% 0.05166739945464044
plot_id,batch_id 0 76 miss% 0.04062261842865978
plot_id,batch_id 0 77 miss% 0.03147813734931642
plot_id,batch_id 0 78 miss% 0.03727900175108118
plot_id,batch_id 0 79 miss% 0.043514475559693955
plot_id,batch_id 0 80 miss% 0.05234619030155323
plot_id,batch_id 0 81 miss% 0.02577540333223731
plot_id,batch_id 0 82 miss% 0.03570110191653777
plot_id,batch_id 0 83 miss% 0.033953956952788446
plot_id,batch_id 0 84 miss% 0.020707866719904048
plot_id,batch_id 0 85 miss% 0.05493171578498745
plot_id,batch_id 0 86 miss% 0.02390410475542788
plot_id,batch_id 0 87 miss% 0.02186907970066558
plot_id,batch_id 0 88 miss% 0.03275492085403319
plot_id,batch_id 0 89 miss% 0.03688674094597559
plot_id,batch_id 0 90 miss% 0.03679192295972984
plot_id,batch_id 0 91 miss% 0.02488666663161276
plot_id,batch_id 0 92 miss% 0.04039189552080262
plot_id,batch_id 0 93 miss% 0.029440283785376324
plot_id,batch_id 0 94 miss% 0.031212445489503755
plot_id,batch_id 0 95 miss% 0.05155605916974042
plot_id,batch_id 0 96 miss% 0.037252144125916514
plot_id,batch_id 0 97 miss% 0.05639610851190664
plot_id,batch_id 0 98 miss% 0.02261922240135861
plot_id,batch_id 0 99 miss% 0.04347504460352172
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01576516 0.02388319 0.02691152 0.03383471 0.02840212 0.0428706
 0.03657595 0.02690068 0.04078649 0.0277188  0.04595366 0.03964872
 0.0333221  0.02879591 0.03315501 0.05319344 0.02370844 0.04161512
 0.03990774 0.03972035 0.03890374 0.02490926 0.03239466 0.02556812
 0.02269558 0.0366709  0.03579034 0.02389505 0.02625614 0.02656316
 0.04895253 0.03632302 0.03380844 0.03961693 0.03440393 0.03857698
 0.04486509 0.02837295 0.02814563 0.0304017  0.06768795 0.04190959
 0.02587068 0.0401     0.02868625 0.02854866 0.0258233  0.0234017
 0.02690279 0.01842101 0.02748826 0.03009434 0.02069375 0.01730921
 0.04388427 0.04136174 0.03162243 0.03760398 0.02140887 0.0269145
 0.03808135 0.02607364 0.03834568 0.02862934 0.02409294 0.03919206
 0.04175812 0.03066692 0.03163035 0.0283152  0.02860615 0.04206467
 0.02727561 0.04268844 0.04063289 0.0516674  0.04062262 0.03147814
 0.037279   0.04351448 0.05234619 0.0257754  0.0357011  0.03395396
 0.02070787 0.05493172 0.0239041  0.02186908 0.03275492 0.03688674
 0.03679192 0.02488667 0.0403919  0.02944028 0.03121245 0.05155606
 0.03725214 0.05639611 0.02261922 0.04347504]
for model  199 the mean error 0.033909850106973866
all id 199 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 31 out win 4 err 0.033909850106973866
Launcher: Job 200 completed in 6717 seconds.
Launcher: Task 56 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  28945
Epoch:0, Train loss:0.462343, valid loss:0.455370
Epoch:1, Train loss:0.039155, valid loss:0.007717
Epoch:2, Train loss:0.011234, valid loss:0.004485
Epoch:3, Train loss:0.008445, valid loss:0.004884
Epoch:4, Train loss:0.006925, valid loss:0.003353
Epoch:5, Train loss:0.005994, valid loss:0.003086
Epoch:6, Train loss:0.005420, valid loss:0.003476
Epoch:7, Train loss:0.004910, valid loss:0.002752
Epoch:8, Train loss:0.004727, valid loss:0.002103
Epoch:9, Train loss:0.004598, valid loss:0.002698
Epoch:10, Train loss:0.004407, valid loss:0.002639
Epoch:11, Train loss:0.003349, valid loss:0.001732
Epoch:12, Train loss:0.003265, valid loss:0.002238
Epoch:13, Train loss:0.003153, valid loss:0.001827
Epoch:14, Train loss:0.003140, valid loss:0.001910
Epoch:15, Train loss:0.003084, valid loss:0.002026
Epoch:16, Train loss:0.003010, valid loss:0.001528
Epoch:17, Train loss:0.002999, valid loss:0.001690
Epoch:18, Train loss:0.002916, valid loss:0.001761
Epoch:19, Train loss:0.002979, valid loss:0.001859
Epoch:20, Train loss:0.002846, valid loss:0.001871
Epoch:21, Train loss:0.002285, valid loss:0.001227
Epoch:22, Train loss:0.002298, valid loss:0.001363
Epoch:23, Train loss:0.002252, valid loss:0.001456
Epoch:24, Train loss:0.002244, valid loss:0.001317
Epoch:25, Train loss:0.002285, valid loss:0.001499
Epoch:26, Train loss:0.002185, valid loss:0.001315
Epoch:27, Train loss:0.002214, valid loss:0.001195
Epoch:28, Train loss:0.002163, valid loss:0.001226
Epoch:29, Train loss:0.002149, valid loss:0.001206
Epoch:30, Train loss:0.002173, valid loss:0.001396
Epoch:31, Train loss:0.001858, valid loss:0.001077
Epoch:32, Train loss:0.001866, valid loss:0.001127
Epoch:33, Train loss:0.001848, valid loss:0.001108
Epoch:34, Train loss:0.001869, valid loss:0.001090
Epoch:35, Train loss:0.001849, valid loss:0.001218
Epoch:36, Train loss:0.001827, valid loss:0.001108
Epoch:37, Train loss:0.001828, valid loss:0.001100
Epoch:38, Train loss:0.001792, valid loss:0.001174
Epoch:39, Train loss:0.001805, valid loss:0.001299
Epoch:40, Train loss:0.001787, valid loss:0.001057
Epoch:41, Train loss:0.001643, valid loss:0.001085
Epoch:42, Train loss:0.001643, valid loss:0.001026
Epoch:43, Train loss:0.001634, valid loss:0.001016
Epoch:44, Train loss:0.001636, valid loss:0.001006
Epoch:45, Train loss:0.001622, valid loss:0.001020
Epoch:46, Train loss:0.001624, valid loss:0.001010
Epoch:47, Train loss:0.001621, valid loss:0.001009
Epoch:48, Train loss:0.001608, valid loss:0.001077
Epoch:49, Train loss:0.001614, valid loss:0.001081
Epoch:50, Train loss:0.001613, valid loss:0.001047
Epoch:51, Train loss:0.001529, valid loss:0.000994
Epoch:52, Train loss:0.001522, valid loss:0.000971
Epoch:53, Train loss:0.001516, valid loss:0.000984
Epoch:54, Train loss:0.001517, valid loss:0.000977
Epoch:55, Train loss:0.001522, valid loss:0.000963
Epoch:56, Train loss:0.001516, valid loss:0.000980
Epoch:57, Train loss:0.001509, valid loss:0.001018
Epoch:58, Train loss:0.001510, valid loss:0.000975
Epoch:59, Train loss:0.001509, valid loss:0.000995
Epoch:60, Train loss:0.001509, valid loss:0.000951
training time 6530.915928602219
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.051318726643767534
plot_id,batch_id 0 1 miss% 0.026313837304525474
plot_id,batch_id 0 2 miss% 0.029515144326123153
plot_id,batch_id 0 3 miss% 0.02393753651423088
plot_id,batch_id 0 4 miss% 0.029681624041351166
plot_id,batch_id 0 5 miss% 0.051456663489363695
plot_id,batch_id 0 6 miss% 0.03687366504778609
plot_id,batch_id 0 7 miss% 0.03977880015253188
plot_id,batch_id 0 8 miss% 0.03190044559441034
plot_id,batch_id 0 9 miss% 0.019209387356453413
plot_id,batch_id 0 10 miss% 0.05517254720544151
plot_id,batch_id 0 11 miss% 0.051717257519604834
plot_id,batch_id 0 12 miss% 0.03628894698564375
plot_id,batch_id 0 13 miss% 0.022298852031120173
plot_id,batch_id 0 14 miss% 0.0349057221156875
plot_id,batch_id 0 15 miss% 0.042166639695482566
plot_id,batch_id 0 16 miss% 0.04104964123322958
plot_id,batch_id 0 17 miss% 0.04885220476435433
plot_id,batch_id 0 18 miss% 0.033041097919788306
plot_id,batch_id 0 19 miss% 0.046385031478866724
plot_id,batch_id 0 20 miss% 0.03489250278300679
plot_id,batch_id 0 21 miss% 0.028035756868896786
plot_id,batch_id 0 22 miss% 0.03983394647777541
plot_id,batch_id 0 23 miss% 0.02640957589141807
plot_id,batch_id 0 24 miss% 0.032478961565807686
plot_id,batch_id 0 25 miss% 0.02416748186397643
plot_id,batch_id 0 26 miss% 0.029483971382415576
plot_id,batch_id 0 27 miss% 0.02292757930511404
plot_id,batch_id 0 28 miss% 0.026378446617631356
plot_id,batch_id 0 29 miss% 0.03747739324826548
plot_id,batch_id 0 30 miss% 0.046223869152347934
plot_id,batch_id 0 31 miss% 0.027181092914690244
plot_id,batch_id 0 32 miss% 0.033902698552255625
plot_id,batch_id 0 33 miss% 0.03109802664987649
plot_id,batch_id 0 34 miss% 0.02576824558052827
plot_id,batch_id 0 35 miss% 0.0325729875101085
plot_id,batch_id 0 36 miss% 0.03868188795636865
plot_id,batch_id 0 37 miss% 0.033071344695965864
plot_id,batch_id 0 38 miss% 0.0322979865002332
plot_id,batch_id 0 39 miss% 0.0257921693767244
plot_id,batch_id 0 40 miss% 0.05549103639233492
plot_id,batch_id 0 41 miss% 0.024540687371643204
plot_id,batch_id 0 42 miss% 0.02770922218626487
plot_id,batch_id 0 43 miss% 0.04336384156158077
plot_id,batch_id 0 44 miss% 0.03132033144565498
plot_id,batch_id 0 45 miss% 0.03747600135907671
plot_id,batch_id 0 46 miss% 0.02607639086463028
plot_id,batch_id 0 47 miss% 0.02881182290414113
plot_id,batch_id 0 48 miss% 0.026404866481519355
plot_id,batch_id 0 49 miss% 0.02446856173022275
plot_id,batch_id 0 50 miss% 0.034655684182545984
plot_id,batch_id 0 51 miss% 0.020673850964695352
plot_id,batch_id 0 52 miss% 0.023335879112925825
plot_id,batch_id 0 53 miss% 0.02509864469147542
plot_id,batch_id 0 54 miss% 0.0322353859796683
plot_id,batch_id 0 55 miss% 0.03529996284665988
plot_id,batch_id 0 56 miss% 0.02821356670256102
plot_id,batch_id 0 57 miss% 0.020714363670326974
plot_id,batch_id 0 58 miss% 0.025322861985511314
plot_id,batch_id 0 59 miss% 0.03244066981816576
plot_id,batch_id 0 60 miss% 0.03581594821909608
plot_id,batch_id 0 61 miss% 0.02917086977072155
plot_id,batch_id 0 62 miss% 0.0200959866224829
plot_id,batch_id 0 63 miss% 0.040415278615268564
plot_id,batch_id 0 64 miss% 0.027766620062269633
plot_id,batch_id 0 65 miss% 0.04450716040682195
plot_id,batch_id 0 66 miss% 0.056034141522741276
plot_id,batch_id 0 67 miss% 0.037755794916421434
plot_id,batch_id 0 68 miss% 0.021557890296289885
plot_id,batch_id 0 69 miss% 0.028434945867601893
plot_id,batch_id 0 70 miss% 0.06418013505174888
plot_id,batch_id 0 71 miss% 0.03525544135039658
plot_id,batch_id 0 72 miss% 0.05329292415226415
plot_id,batch_id 0 73 miss% 0.0505419241127586
plot_id,batch_id 0 74 miss% 0.02690331292200502
plot_id,batch_id 0 75 miss% 0.0541363624325617
plot_id,batch_id 0 76 miss% 0.04957476097425677
plot_id,batch_id 0 77 miss% 0.03174692346710528
plot_id,batch_id 0 78 miss% 0.03253969674429489
plot_id,batch_id 0 79 miss% 0.03764984173522429
plot_id,batch_id 0 80 miss% 0.09508513312261983
plot_id,batch_id 0 81 miss% 0.05177009627024757
plot_id,batch_id 0 82 miss% 0.03506988170955864
plot_id,batch_id 0 83 miss% 0.02922499908118247
plot_id,batch_id 0 84 miss% 0.026627950803304982
plot_id,batch_id 0 85 miss% 0.06553961073878127
plot_id,batch_id 0 86 miss% 0.038714269900798126
plot_id,batch_id 0 87 miss% 0.028838967672117796
plot_id,batch_id 0 88 miss% 0.04099800039069109
plot_id,batch_id 0 89 miss% 0.03466607770985966
plot_id,batch_id 0 90 miss% 0.03649297275728713
plot_id,batch_id 0 91 miss% 0.054829997865387746
plot_id,batch_id 0 92 miss% 0.039571717733366915
plot_id,batch_id 0 93 miss% 0.03584303973153562
plot_id,batch_id 0 94 miss% 0.04086045411415201
plot_id,batch_id 0 95 miss% 0.054905147687155854
plot_id,batch_id 0 96 miss% 0.04188119902190129
plot_id,batch_id 0 97 miss% 0.05598798771993037
plot_id,batch_id 0 98 miss% 0.03686588602546836
plot_id,batch_id 0 99 miss% 0.04567376507466482
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.05131873 0.02631384 0.02951514 0.02393754 0.02968162 0.05145666
 0.03687367 0.0397788  0.03190045 0.01920939 0.05517255 0.05171726
 0.03628895 0.02229885 0.03490572 0.04216664 0.04104964 0.0488522
 0.0330411  0.04638503 0.0348925  0.02803576 0.03983395 0.02640958
 0.03247896 0.02416748 0.02948397 0.02292758 0.02637845 0.03747739
 0.04622387 0.02718109 0.0339027  0.03109803 0.02576825 0.03257299
 0.03868189 0.03307134 0.03229799 0.02579217 0.05549104 0.02454069
 0.02770922 0.04336384 0.03132033 0.037476   0.02607639 0.02881182
 0.02640487 0.02446856 0.03465568 0.02067385 0.02333588 0.02509864
 0.03223539 0.03529996 0.02821357 0.02071436 0.02532286 0.03244067
 0.03581595 0.02917087 0.02009599 0.04041528 0.02776662 0.04450716
 0.05603414 0.03775579 0.02155789 0.02843495 0.06418014 0.03525544
 0.05329292 0.05054192 0.02690331 0.05413636 0.04957476 0.03174692
 0.0325397  0.03764984 0.09508513 0.0517701  0.03506988 0.029225
 0.02662795 0.06553961 0.03871427 0.02883897 0.040998   0.03466608
 0.03649297 0.05483    0.03957172 0.03584304 0.04086045 0.05490515
 0.0418812  0.05598799 0.03686589 0.04567377]
for model  119 the mean error 0.036510164429371175
all id 119 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 25 out win 5 err 0.036510164429371175
Launcher: Job 120 completed in 6737 seconds.
Launcher: Task 37 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  28945
Epoch:0, Train loss:0.343389, valid loss:0.337637
Epoch:1, Train loss:0.019448, valid loss:0.004616
Epoch:2, Train loss:0.005878, valid loss:0.002444
Epoch:3, Train loss:0.004700, valid loss:0.002423
Epoch:4, Train loss:0.004030, valid loss:0.001997
Epoch:5, Train loss:0.003667, valid loss:0.002075
Epoch:6, Train loss:0.003521, valid loss:0.001782
Epoch:7, Train loss:0.003368, valid loss:0.001535
Epoch:8, Train loss:0.003136, valid loss:0.001716
Epoch:9, Train loss:0.003080, valid loss:0.001427
Epoch:10, Train loss:0.002966, valid loss:0.001734
Epoch:11, Train loss:0.002047, valid loss:0.001282
Epoch:12, Train loss:0.002073, valid loss:0.001153
Epoch:13, Train loss:0.002046, valid loss:0.001442
Epoch:14, Train loss:0.002062, valid loss:0.001341
Epoch:15, Train loss:0.002043, valid loss:0.001234
Epoch:16, Train loss:0.001981, valid loss:0.001069
Epoch:17, Train loss:0.001922, valid loss:0.001017
Epoch:18, Train loss:0.001921, valid loss:0.000930
Epoch:19, Train loss:0.001898, valid loss:0.001128
Epoch:20, Train loss:0.001874, valid loss:0.001404
Epoch:21, Train loss:0.001459, valid loss:0.000854
Epoch:22, Train loss:0.001424, valid loss:0.000848
Epoch:23, Train loss:0.001441, valid loss:0.000918
Epoch:24, Train loss:0.001437, valid loss:0.000760
Epoch:25, Train loss:0.001401, valid loss:0.000745
Epoch:26, Train loss:0.001431, valid loss:0.000776
Epoch:27, Train loss:0.001405, valid loss:0.000818
Epoch:28, Train loss:0.001390, valid loss:0.000814
Epoch:29, Train loss:0.001356, valid loss:0.000808
Epoch:30, Train loss:0.001388, valid loss:0.000925
Epoch:31, Train loss:0.001138, valid loss:0.000690
Epoch:32, Train loss:0.001139, valid loss:0.000690
Epoch:33, Train loss:0.001128, valid loss:0.000713
Epoch:34, Train loss:0.001132, valid loss:0.000686
Epoch:35, Train loss:0.001118, valid loss:0.000719
Epoch:36, Train loss:0.001130, valid loss:0.000708
Epoch:37, Train loss:0.001114, valid loss:0.000676
Epoch:38, Train loss:0.001131, valid loss:0.000650
Epoch:39, Train loss:0.001118, valid loss:0.000866
Epoch:40, Train loss:0.001083, valid loss:0.000740
Epoch:41, Train loss:0.000981, valid loss:0.000646
Epoch:42, Train loss:0.000979, valid loss:0.000661
Epoch:43, Train loss:0.000975, valid loss:0.000637
Epoch:44, Train loss:0.000964, valid loss:0.000682
Epoch:45, Train loss:0.000968, valid loss:0.000685
Epoch:46, Train loss:0.000964, valid loss:0.000702
Epoch:47, Train loss:0.000966, valid loss:0.000687
Epoch:48, Train loss:0.000968, valid loss:0.000672
Epoch:49, Train loss:0.000964, valid loss:0.000647
Epoch:50, Train loss:0.000963, valid loss:0.000670
Epoch:51, Train loss:0.000895, valid loss:0.000605
Epoch:52, Train loss:0.000895, valid loss:0.000648
Epoch:53, Train loss:0.000890, valid loss:0.000651
Epoch:54, Train loss:0.000893, valid loss:0.000624
Epoch:55, Train loss:0.000888, valid loss:0.000606
Epoch:56, Train loss:0.000893, valid loss:0.000628
Epoch:57, Train loss:0.000885, valid loss:0.000636
Epoch:58, Train loss:0.000885, valid loss:0.000700
Epoch:59, Train loss:0.000898, valid loss:0.000621
Epoch:60, Train loss:0.000882, valid loss:0.000620
training time 6653.381427288055
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.031147021367462682
plot_id,batch_id 0 1 miss% 0.020173743378858446
plot_id,batch_id 0 2 miss% 0.027323662502074346
plot_id,batch_id 0 3 miss% 0.02823510912707431
plot_id,batch_id 0 4 miss% 0.025196766064017023
plot_id,batch_id 0 5 miss% 0.0291099504881055
plot_id,batch_id 0 6 miss% 0.03173479119318449
plot_id,batch_id 0 7 miss% 0.017282326706834644
plot_id,batch_id 0 8 miss% 0.022729400455344703
plot_id,batch_id 0 9 miss% 0.01933472378515609
plot_id,batch_id 0 10 miss% 0.04590230583289948
plot_id,batch_id 0 11 miss% 0.04950624382079935
plot_id,batch_id 0 12 miss% 0.026579773183141732
plot_id,batch_id 0 13 miss% 0.022306226108133435
plot_id,batch_id 0 14 miss% 0.025466749290415466
plot_id,batch_id 0 15 miss% 0.042951635539764686
plot_id,batch_id 0 16 miss% 0.0352320347159622
plot_id,batch_id 0 17 miss% 0.038700471089900816
plot_id,batch_id 0 18 miss% 0.02705476821334128
plot_id,batch_id 0 19 miss% 0.029326581550935298
plot_id,batch_id 0 20 miss% 0.06455156012015369
plot_id,batch_id 0 21 miss% 0.027296351202074947
plot_id,batch_id 0 22 miss% 0.03005674506671939
plot_id,batch_id 0 23 miss% 0.029713180216988634
plot_id,batch_id 0 24 miss% 0.02401677785230993
plot_id,batch_id 0 25 miss% 0.033596005654739844
plot_id,batch_id 0 26 miss% 0.02316108786122592
plot_id,batch_id 0 27 miss% 0.022407458872655082
plot_id,batch_id 0 28 miss% 0.018680738941228726
plot_id,batch_id 0 29 miss% 0.034470023519679684
plot_id,batch_id 0 30 miss% 0.050506352446336104
plot_id,batch_id 0 31 miss% 0.028635123677802427
plot_id,batch_id 0 32 miss% 0.024522637083143475
plot_id,batch_id 0 33 miss% 0.02284015396122613
plot_id,batch_id 0 34 miss% 0.016020295534142432
plot_id,batch_id 0 35 miss% 0.04323281378357518
plot_id,batch_id 0 36 miss% 0.04025507964606019
plot_id,batch_id 0 37 miss% 0.01688880561917334
plot_id,batch_id 0 38 miss% 0.02626702483858018
plot_id,batch_id 0 39 miss% 0.01747812234344148
plot_id,batch_id 0 40 miss% 0.08486749757400093
plot_id,batch_id 0 41 miss% 0.014899270187675997
plot_id,batch_id 0 42 miss% 0.01428945948113443
plot_id,batch_id 0 43 miss% 0.015255520195327027
plot_id,batch_id 0 44 miss% 0.024692238362061532
plot_id,batch_id 0 45 miss% 0.03012298005631844
plot_id,batch_id 0 46 miss% 0.022714741803377257
plot_id,batch_id 0 47 miss% 0.02059306666727005
plot_id,batch_id 0 48 miss% 0.019811024171975916
plot_id,batch_id 0 49 miss% 0.022679552329803385
plot_id,batch_id 0 50 miss% 0.032814034127862365
plot_id,batch_id 0 51 miss% 0.017300070446684055
plot_id,batch_id 0 52 miss% 0.021928987646678454
plot_id,batch_id 0 53 miss% 0.02697483184650144
plot_id,batch_id 0 54 miss% 0.031793066214574235
plot_id,batch_id 0 55 miss% 0.04668907017284544
plot_id,batch_id 0 56 miss% 0.03117405457881935
plot_id,batch_id 0 57 miss% 0.02002499886257231
plot_id,batch_id 0 58 miss% 0.02431734453790981
plot_id,batch_id 0 59 miss% 0.058602949609738296
plot_id,batch_id 0 60 miss% 0.059672601744272406
plot_id,batch_id 0 61 miss% 0.04843922147849827
plot_id,batch_id 0 62 miss% 0.02051808597028648
plot_id,batch_id 0 63 miss% 0.03139195747821029
plot_id,batch_id 0 64 miss% 0.024391898329387502
plot_id,batch_id 0 65 miss% 0.054607888337984384
plot_id,batch_id 0 66 miss% 0.05352585972424747
plot_id,batch_id 0 67 miss% 0.02846043338214832
plot_id,batch_id 0 68 miss% 0.031330801080835954
plot_id,batch_id 0 69 miss% 0.025968295684828238
plot_id,batch_id 0 70 miss% 0.028005578330264305
plot_id,batch_id 0 71 miss% 0.05416168325157531
plot_id,batch_id 0 72 miss% 0.04166695703253584
plot_id,batch_id 0 73 miss% 0.031628522026698996
plot_id,batch_id 0 74 miss% 0.03139394721881119
plot_id,batch_id 0 75 miss% 0.02804914715927625
plot_id,batch_id 0 76 miss% 0.02957315406732597
plot_id,batch_id 0 77 miss% 0.024302136453478666
plot_id,batch_id 0 78 miss% 0.021337632278221816
plot_id,batch_id 0 79 miss% 0.039318046783254505
plot_id,batch_id 0 80 miss% 0.027035925713245923
plot_id,batch_id 0 81 miss% 0.02776091465951246
plot_id,batch_id 0 82 miss% 0.031918807973259376
plot_id,batch_id 0 83 miss% 0.02313251036301597
plot_id,batch_id 0 84 miss% 0.023471177456366978
plot_id,batch_id 0 85 miss% 0.04400691737644423
plot_id,batch_id 0 86 miss% 0.037913776009487886
plot_id,batch_id 0 87 miss% 0.034834298054937304
plot_id,batch_id 0 88 miss% 0.029060018131984156
plot_id,batch_id 0 89 miss% 0.031130096792473794
plot_id,batch_id 0 90 miss% 0.04738037153555061
plot_id,batch_id 0 91 miss% 0.03735453955426356
plot_id,batch_id 0 92 miss% 0.034502917979954265
plot_id,batch_id 0 93 miss% 0.031213736221121884
plot_id,batch_id 0 94 miss% 0.031535156684784564
plot_id,batch_id 0 95 miss% 0.04920926842286066
plot_id,batch_id 0 96 miss% 0.04752079540176183
plot_id,batch_id 0 97 miss% 0.041133214164568575
plot_id,batch_id 0 98 miss% 0.01948611752772344
plot_id,batch_id 0 99 miss% 0.030153709631191395
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03114702 0.02017374 0.02732366 0.02823511 0.02519677 0.02910995
 0.03173479 0.01728233 0.0227294  0.01933472 0.04590231 0.04950624
 0.02657977 0.02230623 0.02546675 0.04295164 0.03523203 0.03870047
 0.02705477 0.02932658 0.06455156 0.02729635 0.03005675 0.02971318
 0.02401678 0.03359601 0.02316109 0.02240746 0.01868074 0.03447002
 0.05050635 0.02863512 0.02452264 0.02284015 0.0160203  0.04323281
 0.04025508 0.01688881 0.02626702 0.01747812 0.0848675  0.01489927
 0.01428946 0.01525552 0.02469224 0.03012298 0.02271474 0.02059307
 0.01981102 0.02267955 0.03281403 0.01730007 0.02192899 0.02697483
 0.03179307 0.04668907 0.03117405 0.020025   0.02431734 0.05860295
 0.0596726  0.04843922 0.02051809 0.03139196 0.0243919  0.05460789
 0.05352586 0.02846043 0.0313308  0.0259683  0.02800558 0.05416168
 0.04166696 0.03162852 0.03139395 0.02804915 0.02957315 0.02430214
 0.02133763 0.03931805 0.02703593 0.02776091 0.03191881 0.02313251
 0.02347118 0.04400692 0.03791378 0.0348343  0.02906002 0.0311301
 0.04738037 0.03735454 0.03450292 0.03121374 0.03153516 0.04920927
 0.0475208  0.04113321 0.01948612 0.03015371]
for model  226 the mean error 0.03158933498994445
all id 226 hidden_dim 16 learning_rate 0.02 num_layers 4 frames 31 out win 4 err 0.03158933498994445
Launcher: Job 227 completed in 6844 seconds.
Launcher: Task 12 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  61841
Epoch:0, Train loss:0.734007, valid loss:0.732855
Epoch:1, Train loss:0.285687, valid loss:0.008636
Epoch:2, Train loss:0.014816, valid loss:0.006118
Epoch:3, Train loss:0.010976, valid loss:0.004436
Epoch:4, Train loss:0.009087, valid loss:0.004563
Epoch:5, Train loss:0.007926, valid loss:0.004055
Epoch:6, Train loss:0.007042, valid loss:0.003566
Epoch:7, Train loss:0.006103, valid loss:0.003694
Epoch:8, Train loss:0.006006, valid loss:0.003101
Epoch:9, Train loss:0.005203, valid loss:0.002717
Epoch:10, Train loss:0.005078, valid loss:0.002830
Epoch:11, Train loss:0.003768, valid loss:0.002040
Epoch:12, Train loss:0.003641, valid loss:0.002602
Epoch:13, Train loss:0.003687, valid loss:0.002120
Epoch:14, Train loss:0.003486, valid loss:0.002021
Epoch:15, Train loss:0.003343, valid loss:0.001765
Epoch:16, Train loss:0.003319, valid loss:0.002339
Epoch:17, Train loss:0.003387, valid loss:0.001799
Epoch:18, Train loss:0.003153, valid loss:0.001817
Epoch:19, Train loss:0.003192, valid loss:0.001704
Epoch:20, Train loss:0.003116, valid loss:0.001684
Epoch:21, Train loss:0.002465, valid loss:0.001732
Epoch:22, Train loss:0.002475, valid loss:0.001483
Epoch:23, Train loss:0.002438, valid loss:0.001488
Epoch:24, Train loss:0.002376, valid loss:0.001476
Epoch:25, Train loss:0.002409, valid loss:0.001488
Epoch:26, Train loss:0.002312, valid loss:0.001448
Epoch:27, Train loss:0.002270, valid loss:0.001502
Epoch:28, Train loss:0.002326, valid loss:0.001516
Epoch:29, Train loss:0.002262, valid loss:0.001371
Epoch:30, Train loss:0.002222, valid loss:0.001456
Epoch:31, Train loss:0.001925, valid loss:0.001305
Epoch:32, Train loss:0.001904, valid loss:0.001246
Epoch:33, Train loss:0.001882, valid loss:0.001235
Epoch:34, Train loss:0.001870, valid loss:0.001268
Epoch:35, Train loss:0.001876, valid loss:0.001227
Epoch:36, Train loss:0.001877, valid loss:0.001252
Epoch:37, Train loss:0.001838, valid loss:0.001259
Epoch:38, Train loss:0.001859, valid loss:0.001337
Epoch:39, Train loss:0.001815, valid loss:0.001225
Epoch:40, Train loss:0.001824, valid loss:0.001264
Epoch:41, Train loss:0.001664, valid loss:0.001192
Epoch:42, Train loss:0.001652, valid loss:0.001158
Epoch:43, Train loss:0.001640, valid loss:0.001144
Epoch:44, Train loss:0.001647, valid loss:0.001168
Epoch:45, Train loss:0.001636, valid loss:0.001187
Epoch:46, Train loss:0.001630, valid loss:0.001193
Epoch:47, Train loss:0.001628, valid loss:0.001148
Epoch:48, Train loss:0.001624, valid loss:0.001271
Epoch:49, Train loss:0.001614, valid loss:0.001150
Epoch:50, Train loss:0.001602, valid loss:0.001146
Epoch:51, Train loss:0.001541, valid loss:0.001144
Epoch:52, Train loss:0.001535, valid loss:0.001119
Epoch:53, Train loss:0.001530, valid loss:0.001113
Epoch:54, Train loss:0.001528, valid loss:0.001105
Epoch:55, Train loss:0.001523, valid loss:0.001116
Epoch:56, Train loss:0.001522, valid loss:0.001132
Epoch:57, Train loss:0.001516, valid loss:0.001118
Epoch:58, Train loss:0.001518, valid loss:0.001113
Epoch:59, Train loss:0.001509, valid loss:0.001112
Epoch:60, Train loss:0.001510, valid loss:0.001115
training time 6663.575569629669
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03284236517173086
plot_id,batch_id 0 1 miss% 0.02747700779780451
plot_id,batch_id 0 2 miss% 0.025200309283897524
plot_id,batch_id 0 3 miss% 0.02909486349743821
plot_id,batch_id 0 4 miss% 0.030237578145892743
plot_id,batch_id 0 5 miss% 0.03385854763980529
plot_id,batch_id 0 6 miss% 0.0271574359526691
plot_id,batch_id 0 7 miss% 0.03174614634029321
plot_id,batch_id 0 8 miss% 0.0389690411419547
plot_id,batch_id 0 9 miss% 0.028880598403142915
plot_id,batch_id 0 10 miss% 0.043671044755618345
plot_id,batch_id 0 11 miss% 0.04588620333475593
plot_id,batch_id 0 12 miss% 0.03673271310458216
plot_id,batch_id 0 13 miss% 0.04048129155527152
plot_id,batch_id 0 14 miss% 0.03836062383710959
plot_id,batch_id 0 15 miss% 0.03407718810152358
plot_id,batch_id 0 16 miss% 0.039917034995051934
plot_id,batch_id 0 17 miss% 0.035800902598665814
plot_id,batch_id 0 18 miss% 0.04546511076138859
plot_id,batch_id 0 19 miss% 0.033279668597486003
plot_id,batch_id 0 20 miss% 0.09135584498218022
plot_id,batch_id 0 21 miss% 0.03216149478105667
plot_id,batch_id 0 22 miss% 0.029830681767058696
plot_id,batch_id 0 23 miss% 0.028306888482426247
plot_id,batch_id 0 24 miss% 0.03662698965851065
plot_id,batch_id 0 25 miss% 0.04186265641519487
plot_id,batch_id 0 26 miss% 0.02243659896075772
plot_id,batch_id 0 27 miss% 0.022550337595420737
plot_id,batch_id 0 28 miss% 0.02021051159407065
plot_id,batch_id 0 29 miss% 0.024903355453830087
plot_id,batch_id 0 30 miss% 0.02783257005123709
plot_id,batch_id 0 31 miss% 0.03842865787815102
plot_id,batch_id 0 32 miss% 0.04164209363072231
plot_id,batch_id 0 33 miss% 0.029572470226597928
plot_id,batch_id 0 34 miss% 0.030762018726209085
plot_id,batch_id 0 35 miss% 0.043316744899727665
plot_id,batch_id 0 36 miss% 0.045999592777466104
plot_id,batch_id 0 37 miss% 0.027842910919286368
plot_id,batch_id 0 38 miss% 0.019455741148871098
plot_id,batch_id 0 39 miss% 0.026383324625806524
plot_id,batch_id 0 40 miss% 0.045773859309983934
plot_id,batch_id 0 41 miss% 0.02739719056071864
plot_id,batch_id 0 42 miss% 0.022618366597658042
plot_id,batch_id 0 43 miss% 0.032397493663322806
plot_id,batch_id 0 44 miss% 0.024237484967781955
plot_id,batch_id 0 45 miss% 0.033765586659062555
plot_id,batch_id 0 46 miss% 0.04003544524884553
plot_id,batch_id 0 47 miss% 0.020513170947344537
plot_id,batch_id 0 48 miss% 0.021801116666914568
plot_id,batch_id 0 49 miss% 0.026757450213285933
plot_id,batch_id 0 50 miss% 0.041095397373820015
plot_id,batch_id 0 51 miss% 0.04041289918903111
plot_id,batch_id 0 52 miss% 0.017333635495143194
plot_id,batch_id 0 53 miss% 0.021387597681255557
plot_id,batch_id 0 54 miss% 0.026486475596091432
plot_id,batch_id 0 55 miss% 0.04825843369054448
plot_id,batch_id 0 56 miss% 0.02541519984349009
plot_id,batch_id 0 57 miss% 0.022274166909373377
plot_id,batch_id 0 58 miss% 0.022330646238799084
plot_id,batch_id 0 59 miss% 0.023082154683192815
plot_id,batch_id 0 60 miss% 0.039827742025681574
plot_id,batch_id 0 61 miss% 0.04552805480961967
plot_id,batch_id 0 62 miss% 0.042252999021197335
plot_id,batch_id 0 63 miss% 0.03546952357940043
plot_id,batch_id 0 64 miss% 0.04305645257811175
plot_id,batch_id 0 65 miss% 0.06272659363162751
plot_id,batch_id 0 66 miss% 0.039248747331043075
plot_id,batch_id 0 67 miss% 0.03370236749663929
plot_id,batch_id 0 68 miss% 0.03013037500539693
plot_id,batch_id 0 69 miss% 0.029618855637200233the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  35921
Epoch:0, Train loss:0.736710, valid loss:0.719174
Epoch:1, Train loss:0.069817, valid loss:0.024567
Epoch:2, Train loss:0.036880, valid loss:0.009275
Epoch:3, Train loss:0.016236, valid loss:0.005682
Epoch:4, Train loss:0.012398, valid loss:0.006839
Epoch:5, Train loss:0.010695, valid loss:0.005100
Epoch:6, Train loss:0.009640, valid loss:0.004479
Epoch:7, Train loss:0.008697, valid loss:0.005503
Epoch:8, Train loss:0.008024, valid loss:0.004825
Epoch:9, Train loss:0.007450, valid loss:0.003156
Epoch:10, Train loss:0.006608, valid loss:0.003400
Epoch:11, Train loss:0.005151, valid loss:0.002603
Epoch:12, Train loss:0.004910, valid loss:0.002342
Epoch:13, Train loss:0.004924, valid loss:0.002359
Epoch:14, Train loss:0.004643, valid loss:0.002422
Epoch:15, Train loss:0.004638, valid loss:0.002566
Epoch:16, Train loss:0.004658, valid loss:0.002395
Epoch:17, Train loss:0.004357, valid loss:0.002220
Epoch:18, Train loss:0.004354, valid loss:0.002975
Epoch:19, Train loss:0.004410, valid loss:0.001999
Epoch:20, Train loss:0.004490, valid loss:0.002388
Epoch:21, Train loss:0.003317, valid loss:0.001780
Epoch:22, Train loss:0.003297, valid loss:0.001803
Epoch:23, Train loss:0.003253, valid loss:0.002035
Epoch:24, Train loss:0.003236, valid loss:0.002149
Epoch:25, Train loss:0.003206, valid loss:0.001710
Epoch:26, Train loss:0.003191, valid loss:0.001844
Epoch:27, Train loss:0.003190, valid loss:0.001744
Epoch:28, Train loss:0.003165, valid loss:0.001758
Epoch:29, Train loss:0.003076, valid loss:0.002062
Epoch:30, Train loss:0.003063, valid loss:0.001718
Epoch:31, Train loss:0.002653, valid loss:0.001527
Epoch:32, Train loss:0.002609, valid loss:0.001640
Epoch:33, Train loss:0.002571, valid loss:0.001545
Epoch:34, Train loss:0.002567, valid loss:0.001542
Epoch:35, Train loss:0.002555, valid loss:0.001578
Epoch:36, Train loss:0.002534, valid loss:0.001878
Epoch:37, Train loss:0.002586, valid loss:0.001532
Epoch:38, Train loss:0.002538, valid loss:0.001789
Epoch:39, Train loss:0.002493, valid loss:0.001811
Epoch:40, Train loss:0.002532, valid loss:0.001533
Epoch:41, Train loss:0.002257, valid loss:0.001372
Epoch:42, Train loss:0.002251, valid loss:0.001509
Epoch:43, Train loss:0.002256, valid loss:0.001377
Epoch:44, Train loss:0.002251, valid loss:0.001359
Epoch:45, Train loss:0.002222, valid loss:0.001407
Epoch:46, Train loss:0.002217, valid loss:0.001369
Epoch:47, Train loss:0.002209, valid loss:0.001420
Epoch:48, Train loss:0.002228, valid loss:0.001409
Epoch:49, Train loss:0.002222, valid loss:0.001390
Epoch:50, Train loss:0.002180, valid loss:0.001370
Epoch:51, Train loss:0.002072, valid loss:0.001322
Epoch:52, Train loss:0.002076, valid loss:0.001342
Epoch:53, Train loss:0.002070, valid loss:0.001329
Epoch:54, Train loss:0.002056, valid loss:0.001306
Epoch:55, Train loss:0.002057, valid loss:0.001324
Epoch:56, Train loss:0.002047, valid loss:0.001317
Epoch:57, Train loss:0.002060, valid loss:0.001342
Epoch:58, Train loss:0.002054, valid loss:0.001352
Epoch:59, Train loss:0.002038, valid loss:0.001411
Epoch:60, Train loss:0.002033, valid loss:0.001377
training time 6663.9813940525055
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.04258413464920421
plot_id,batch_id 0 1 miss% 0.02863017930936315
plot_id,batch_id 0 2 miss% 0.0303053077671732
plot_id,batch_id 0 3 miss% 0.028952394707503746
plot_id,batch_id 0 4 miss% 0.023270533435356545
plot_id,batch_id 0 5 miss% 0.03434644471709512
plot_id,batch_id 0 6 miss% 0.029520447360368837
plot_id,batch_id 0 7 miss% 0.029832201100151147
plot_id,batch_id 0 8 miss% 0.033513468758835936
plot_id,batch_id 0 9 miss% 0.02785025668899164
plot_id,batch_id 0 10 miss% 0.047555492258591237
plot_id,batch_id 0 11 miss% 0.04950069120684705
plot_id,batch_id 0 12 miss% 0.01850401881317262
plot_id,batch_id 0 13 miss% 0.034587563742504886
plot_id,batch_id 0 14 miss% 0.04318921731454906
plot_id,batch_id 0 15 miss% 0.04961856650953731
plot_id,batch_id 0 16 miss% 0.04020589954549351
plot_id,batch_id 0 17 miss% 0.046518125424544
plot_id,batch_id 0 18 miss% 0.04495415180737098
plot_id,batch_id 0 19 miss% 0.046544106569215656
plot_id,batch_id 0 20 miss% 0.05311679146360965
plot_id,batch_id 0 21 miss% 0.019806021092749412
plot_id,batch_id 0 22 miss% 0.03829095411586146
plot_id,batch_id 0 23 miss% 0.0177794612171807
plot_id,batch_id 0 24 miss% 0.026392393174209494
plot_id,batch_id 0 25 miss% 0.04737862861047362
plot_id,batch_id 0 26 miss% 0.02910062044100223
plot_id,batch_id 0 27 miss% 0.031890520428533145
plot_id,batch_id 0 28 miss% 0.023252148592227267
plot_id,batch_id 0 29 miss% 0.013757835131133243
plot_id,batch_id 0 30 miss% 0.03951666508264727
plot_id,batch_id 0 31 miss% 0.03652254339059639
plot_id,batch_id 0 32 miss% 0.03152030079448188
plot_id,batch_id 0 33 miss% 0.02925565055784884
plot_id,batch_id 0 34 miss% 0.021492820121822513
plot_id,batch_id 0 35 miss% 0.07343417013587858
plot_id,batch_id 0 36 miss% 0.055783295328152675
plot_id,batch_id 0 37 miss% 0.0502946391450691
plot_id,batch_id 0 38 miss% 0.023775813814898485
plot_id,batch_id 0 39 miss% 0.01777496140445541
plot_id,batch_id 0 40 miss% 0.05028993388392263
plot_id,batch_id 0 41 miss% 0.0193552545492445
plot_id,batch_id 0 42 miss% 0.02601953873095977
plot_id,batch_id 0 43 miss% 0.030607296820403578
plot_id,batch_id 0 44 miss% 0.016741547873554665
plot_id,batch_id 0 45 miss% 0.024458472018382187
plot_id,batch_id 0 46 miss% 0.03507156026541153
plot_id,batch_id 0 47 miss% 0.015672129996724085
plot_id,batch_id 0 48 miss% 0.022767627490270632
plot_id,batch_id 0 49 miss% 0.02461133768876462
plot_id,batch_id 0 50 miss% 0.056848368610711
plot_id,batch_id 0 51 miss% 0.02811149145012901
plot_id,batch_id 0 52 miss% 0.021937652315980747
plot_id,batch_id 0 53 miss% 0.01645132037381855
plot_id,batch_id 0 54 miss% 0.023299046790289896
plot_id,batch_id 0 55 miss% 0.04808550333396604
plot_id,batch_id 0 56 miss% 0.040282851599192954
plot_id,batch_id 0 57 miss% 0.01836034475856427
plot_id,batch_id 0 58 miss% 0.02159468737401378
plot_id,batch_id 0 59 miss% 0.026205991908705692
plot_id,batch_id 0 60 miss% 0.04954208786083941
plot_id,batch_id 0 61 miss% 0.025116773356141063
plot_id,batch_id 0 62 miss% 0.03123629137538747
plot_id,batch_id 0 63 miss% 0.042366612522673916
plot_id,batch_id 0 64 miss% 0.05067551251040268
plot_id,batch_id 0 65 miss% 0.0425371875654617
plot_id,batch_id 0 66 miss% 0.06843106390096772
plot_id,batch_id 0 67 miss% 0.024361664912890494
plot_id,batch_id 0 68 miss% 0.038169787928187204
plot_id,batch_id 0 69 miss% 0.0391367289890421

plot_id,batch_id 0 70 miss% 0.042436220688220797
plot_id,batch_id 0 71 miss% 0.03846957833223134
plot_id,batch_id 0 72 miss% 0.04530292763134254
plot_id,batch_id 0 73 miss% 0.04056038224475808
plot_id,batch_id 0 74 miss% 0.03919872437440098
plot_id,batch_id 0 75 miss% 0.04390187318166574
plot_id,batch_id 0 76 miss% 0.047943222674963634
plot_id,batch_id 0 77 miss% 0.04565026973882319
plot_id,batch_id 0 78 miss% 0.0290802689092767
plot_id,batch_id 0 79 miss% 0.023275940342416288
plot_id,batch_id 0 80 miss% 0.04549335842800664
plot_id,batch_id 0 81 miss% 0.03450816767994957
plot_id,batch_id 0 82 miss% 0.036392270259363604
plot_id,batch_id 0 83 miss% 0.032031035441935064
plot_id,batch_id 0 84 miss% 0.03458197390100672
plot_id,batch_id 0 85 miss% 0.03603058398849943
plot_id,batch_id 0 86 miss% 0.044515355481268844
plot_id,batch_id 0 87 miss% 0.04260894141768932
plot_id,batch_id 0 88 miss% 0.04705935420366464
plot_id,batch_id 0 89 miss% 0.03150877970697232
plot_id,batch_id 0 90 miss% 0.04578539872748699
plot_id,batch_id 0 91 miss% 0.061398350539676905
plot_id,batch_id 0 92 miss% 0.033634549313125504
plot_id,batch_id 0 93 miss% 0.036972874949888136
plot_id,batch_id 0 94 miss% 0.04521759464316209
plot_id,batch_id 0 95 miss% 0.05862954962913438
plot_id,batch_id 0 96 miss% 0.03711341928240429
plot_id,batch_id 0 97 miss% 0.03903264290045832
plot_id,batch_id 0 98 miss% 0.03825869538976005
plot_id,batch_id 0 99 miss% 0.03307485209625545
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03284237 0.02747701 0.02520031 0.02909486 0.03023758 0.03385855
 0.02715744 0.03174615 0.03896904 0.0288806  0.04367104 0.0458862
 0.03673271 0.04048129 0.03836062 0.03407719 0.03991703 0.0358009
 0.04546511 0.03327967 0.09135584 0.03216149 0.02983068 0.02830689
 0.03662699 0.04186266 0.0224366  0.02255034 0.02021051 0.02490336
 0.02783257 0.03842866 0.04164209 0.02957247 0.03076202 0.04331674
 0.04599959 0.02784291 0.01945574 0.02638332 0.04577386 0.02739719
 0.02261837 0.03239749 0.02423748 0.03376559 0.04003545 0.02051317
 0.02180112 0.02675745 0.0410954  0.0404129  0.01733364 0.0213876
 0.02648648 0.04825843 0.0254152  0.02227417 0.02233065 0.02308215
 0.03982774 0.04552805 0.042253   0.03546952 0.04305645 0.06272659
 0.03924875 0.03370237 0.03013038 0.02961886 0.04243622 0.03846958
 0.04530293 0.04056038 0.03919872 0.04390187 0.04794322 0.04565027
 0.02908027 0.02327594 0.04549336 0.03450817 0.03639227 0.03203104
 0.03458197 0.03603058 0.04451536 0.04260894 0.04705935 0.03150878
 0.0457854  0.06139835 0.03363455 0.03697287 0.04521759 0.05862955
 0.03711342 0.03903264 0.0382587  0.03307485]
for model  13 the mean error 0.035752197983400485
all id 13 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 21 out win 4 err 0.035752197983400485
Launcher: Job 14 completed in 6874 seconds.
Launcher: Task 201 done. Exiting.
plot_id,batch_id 0 70 miss% 0.04350673430416725
plot_id,batch_id 0 71 miss% 0.062491588484532644
plot_id,batch_id 0 72 miss% 0.04552416565525054
plot_id,batch_id 0 73 miss% 0.039532873989612964
plot_id,batch_id 0 74 miss% 0.041184138387508784
plot_id,batch_id 0 75 miss% 0.057087440753215875
plot_id,batch_id 0 76 miss% 0.05019070573234331
plot_id,batch_id 0 77 miss% 0.04091688838474196
plot_id,batch_id 0 78 miss% 0.04322214250610793
plot_id,batch_id 0 79 miss% 0.044047666596697815
plot_id,batch_id 0 80 miss% 0.05270037578512376
plot_id,batch_id 0 81 miss% 0.03314426500433233
plot_id,batch_id 0 82 miss% 0.06291459335527472
plot_id,batch_id 0 83 miss% 0.03526526342045127
plot_id,batch_id 0 84 miss% 0.03069864823031243
plot_id,batch_id 0 85 miss% 0.04743487835117575
plot_id,batch_id 0 86 miss% 0.03759908507029023
plot_id,batch_id 0 87 miss% 0.046280179186014576
plot_id,batch_id 0 88 miss% 0.044951824627345156
plot_id,batch_id 0 89 miss% 0.04770660704538614
plot_id,batch_id 0 90 miss% 0.04433803917100399
plot_id,batch_id 0 91 miss% 0.0455367320991334
plot_id,batch_id 0 92 miss% 0.0345373710801966
plot_id,batch_id 0 93 miss% 0.03532693880647757
plot_id,batch_id 0 94 miss% 0.06071506808859525
plot_id,batch_id 0 95 miss% 0.04430477273830473
plot_id,batch_id 0 96 miss% 0.045212292569292524
plot_id,batch_id 0 97 miss% 0.055701772286069845
plot_id,batch_id 0 98 miss% 0.04910154597076677
plot_id,batch_id 0 99 miss% 0.026021147540576283
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04258413 0.02863018 0.03030531 0.02895239 0.02327053 0.03434644
 0.02952045 0.0298322  0.03351347 0.02785026 0.04755549 0.04950069
 0.01850402 0.03458756 0.04318922 0.04961857 0.0402059  0.04651813
 0.04495415 0.04654411 0.05311679 0.01980602 0.03829095 0.01777946
 0.02639239 0.04737863 0.02910062 0.03189052 0.02325215 0.01375784
 0.03951667 0.03652254 0.0315203  0.02925565 0.02149282 0.07343417
 0.0557833  0.05029464 0.02377581 0.01777496 0.05028993 0.01935525
 0.02601954 0.0306073  0.01674155 0.02445847 0.03507156 0.01567213
 0.02276763 0.02461134 0.05684837 0.02811149 0.02193765 0.01645132
 0.02329905 0.0480855  0.04028285 0.01836034 0.02159469 0.02620599
 0.04954209 0.02511677 0.03123629 0.04236661 0.05067551 0.04253719
 0.06843106 0.02436166 0.03816979 0.03913673 0.04350673 0.06249159
 0.04552417 0.03953287 0.04118414 0.05708744 0.05019071 0.04091689
 0.04322214 0.04404767 0.05270038 0.03314427 0.06291459 0.03526526
 0.03069865 0.04743488 0.03759909 0.04628018 0.04495182 0.04770661
 0.04433804 0.04553673 0.03453737 0.03532694 0.06071507 0.04430477
 0.04521229 0.05570177 0.04910155 0.02602115]
for model  47 the mean error 0.037356608497040075
all id 47 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 21 out win 5 err 0.037356608497040075
Launcher: Job 48 completed in 6882 seconds.
Launcher: Task 205 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  61841
Epoch:0, Train loss:0.710423, valid loss:0.705107
Epoch:1, Train loss:0.517683, valid loss:0.521553
Epoch:2, Train loss:0.502496, valid loss:0.520157
Epoch:3, Train loss:0.499613, valid loss:0.519556
Epoch:4, Train loss:0.497887, valid loss:0.517552
Epoch:5, Train loss:0.496797, valid loss:0.517757
Epoch:6, Train loss:0.496176, valid loss:0.517897
Epoch:7, Train loss:0.495951, valid loss:0.517522
Epoch:8, Train loss:0.495202, valid loss:0.517191
Epoch:9, Train loss:0.495316, valid loss:0.516543
Epoch:10, Train loss:0.495081, valid loss:0.517232
Epoch:11, Train loss:0.493382, valid loss:0.516600
Epoch:12, Train loss:0.493279, valid loss:0.516176
Epoch:13, Train loss:0.493421, valid loss:0.515955
Epoch:14, Train loss:0.493273, valid loss:0.515944
Epoch:15, Train loss:0.493189, valid loss:0.515941
Epoch:16, Train loss:0.493207, valid loss:0.516108
Epoch:17, Train loss:0.493143, valid loss:0.516137
Epoch:18, Train loss:0.493055, valid loss:0.515933
Epoch:19, Train loss:0.493074, valid loss:0.515723
Epoch:20, Train loss:0.492962, valid loss:0.515850
Epoch:21, Train loss:0.492287, valid loss:0.515804
Epoch:22, Train loss:0.492243, valid loss:0.515631
Epoch:23, Train loss:0.492355, valid loss:0.515502
Epoch:24, Train loss:0.492175, valid loss:0.515566
Epoch:25, Train loss:0.492178, valid loss:0.515511
Epoch:26, Train loss:0.492205, valid loss:0.515813
Epoch:27, Train loss:0.492156, valid loss:0.515421
Epoch:28, Train loss:0.492136, valid loss:0.515658
Epoch:29, Train loss:0.492279, valid loss:0.515591
Epoch:30, Train loss:0.492053, valid loss:0.515853
Epoch:31, Train loss:0.491785, valid loss:0.515416
Epoch:32, Train loss:0.491740, valid loss:0.515472
Epoch:33, Train loss:0.491781, valid loss:0.515423
Epoch:34, Train loss:0.491729, valid loss:0.515412
Epoch:35, Train loss:0.491765, valid loss:0.515365
Epoch:36, Train loss:0.491744, valid loss:0.515428
Epoch:37, Train loss:0.491771, valid loss:0.515447
Epoch:38, Train loss:0.491704, valid loss:0.515503
Epoch:39, Train loss:0.491700, valid loss:0.515427
Epoch:40, Train loss:0.491688, valid loss:0.515368
Epoch:41, Train loss:0.491526, valid loss:0.515361
Epoch:42, Train loss:0.491522, valid loss:0.515388
Epoch:43, Train loss:0.491525, valid loss:0.515343
Epoch:44, Train loss:0.491522, valid loss:0.515360
Epoch:45, Train loss:0.491513, valid loss:0.515408
Epoch:46, Train loss:0.491501, valid loss:0.515366
Epoch:47, Train loss:0.491503, valid loss:0.515399
Epoch:48, Train loss:0.491532, valid loss:0.515365
Epoch:49, Train loss:0.491486, valid loss:0.515353
Epoch:50, Train loss:0.491501, valid loss:0.515338
Epoch:51, Train loss:0.491430, valid loss:0.515310
Epoch:52, Train loss:0.491414, valid loss:0.515338
Epoch:53, Train loss:0.491409, valid loss:0.515353
Epoch:54, Train loss:0.491397, valid loss:0.515324
Epoch:55, Train loss:0.491414, valid loss:0.515306
Epoch:56, Train loss:0.491394, valid loss:0.515320
Epoch:57, Train loss:0.491403, valid loss:0.515311
Epoch:58, Train loss:0.491398, valid loss:0.515329
Epoch:59, Train loss:0.491395, valid loss:0.515388
Epoch:60, Train loss:0.491395, valid loss:0.515334
training time 6705.236276388168
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.8182933826911308
plot_id,batch_id 0 1 miss% 0.8550247541026794
plot_id,batch_id 0 2 miss% 0.8609295724984211
plot_id,batch_id 0 3 miss% 0.8653105079725503
plot_id,batch_id 0 4 miss% 0.870201026118771
plot_id,batch_id 0 5 miss% 0.8123312126403737
plot_id,batch_id 0 6 miss% 0.8531490075313289
plot_id,batch_id 0 7 miss% 0.8622531474819711
plot_id,batch_id 0 8 miss% 0.8641114692534777
plot_id,batch_id 0 9 miss% 0.8672992363630054
plot_id,batch_id 0 10 miss% 0.8054543285093623
plot_id,batch_id 0 11 miss% 0.8509346726020366
plot_id,batch_id 0 12 miss% 0.860590408256751
plot_id,batch_id 0 13 miss% 0.8620571831871005
plot_id,batch_id 0 14 miss% 0.8681082829901189
plot_id,batch_id 0 15 miss% 0.8027885150390464
plot_id,batch_id 0 16 miss% 0.8474833502956499
plot_id,batch_id 0 17 miss% 0.8616737413867939
plot_id,batch_id 0 18 miss% 0.8639200364126269
plot_id,batch_id 0 19 miss% 0.8650990814432756
plot_id,batch_id 0 20 miss% 0.837797974321971
plot_id,batch_id 0 21 miss% 0.8624414865442561
plot_id,batch_id 0 22 miss% 0.8665724152787112
plot_id,batch_id 0 23 miss% 0.8678237591680809
plot_id,batch_id 0 24 miss% 0.8699390189188652
plot_id,batch_id 0 25 miss% 0.8366186973796139
plot_id,batch_id 0 26 miss% 0.8579173802594842
plot_id,batch_id 0 27 miss% 0.8653350840057239
plot_id,batch_id 0 28 miss% 0.8668560394969848
plot_id,batch_id 0 29 miss% 0.8689598741591539
plot_id,batch_id 0 30 miss% 0.8226393207629003
plot_id,batch_id 0 31 miss% 0.8589650613504567
plot_id,batch_id 0 32 miss% 0.8619230585676744
plot_id,batch_id 0 33 miss% 0.8636591434257526
plot_id,batch_id 0 34 miss% 0.8656667358733195
plot_id,batch_id 0 35 miss% 0.8255121172484594
plot_id,batch_id 0 36 miss% 0.8586933954968675
plot_id,batch_id 0 37 miss% 0.861869957081374
plot_id,batch_id 0 38 miss% 0.8655259326748702
plot_id,batch_id 0 39 miss% 0.8649088767942096
plot_id,batch_id 0 40 miss% 0.8503411630740687
plot_id,batch_id 0 41 miss% 0.8624998597938696
plot_id,batch_id 0 42 miss% 0.8644426108928579
plot_id,batch_id 0 43 miss% 0.8693108871378855
plot_id,batch_id 0 44 miss% 0.8686999248054972
plot_id,batch_id 0 45 miss% 0.8473161880465859
plot_id,batch_id 0 46 miss% 0.862330907546584
plot_id,batch_id 0 47 miss% 0.8657073225254093
plot_id,batch_id 0 48 miss% 0.8704525004816179
plot_id,batch_id 0 49 miss% 0.8678559293680533
plot_id,batch_id 0 50 miss% 0.8484438569097035
plot_id,batch_id 0 51 miss% 0.8623395712594923
plot_id,batch_id 0 52 miss% 0.8652333948207149
plot_id,batch_id 0 53 miss% 0.8684020413020621
plot_id,batch_id 0 54 miss% 0.8699526445057203
plot_id,batch_id 0 55 miss% 0.8504632713515317
plot_id,batch_id 0 56 miss% 0.8642171103555626
plot_id,batch_id 0 57 miss% 0.867014094440038
plot_id,batch_id 0 58 miss% 0.8697486017192785
plot_id,batch_id 0 59 miss% 0.8683754015986905
plot_id,batch_id 0 60 miss% 0.7669604091791685
plot_id,batch_id 0 61 miss% 0.8404953699315323
plot_id,batch_id 0 62 miss% 0.85426831989628
plot_id,batch_id 0 63 miss% 0.8576241668306822
plot_id,batch_id 0 64 miss% 0.8614567556447865
plot_id,batch_id 0 65 miss% 0.7650701049237442
plot_id,batch_id 0 66 miss% 0.8316981548931378
plot_id,batch_id 0 67 miss% 0.8430825027704877
plot_id,batch_id 0 68 miss% 0.8585548255506971
plot_id,batch_id 0 69 miss% 0.856885266974148
plot_id,batch_id 0 70 miss% 0.720729146678551
plot_id,batch_id 0 71 miss% 0.8354090207836499
plot_id,batch_id 0 72 miss% 0.8418515820289332
plot_id,batch_id 0 73 miss% 0.850001025453657
plot_id,batch_id 0 74 miss% 0.85765199912637
plot_id,batch_id 0 75 miss% 0.7424704195440751
plot_id,batch_id 0 76 miss% 0.821055230034733
plot_id,batch_id 0 77 miss% 0.8315140461931865
plot_id,batch_id 0 78 miss% 0.842671452721023
plot_id,batch_id 0 79 miss% 0.8546609775868079
plot_id,batch_id 0 80 miss% 0.7879290912594102
plot_id,batch_id 0 81 miss% 0.8496669739245104
plot_id,batch_id 0 82 miss% 0.8568337417865898
plot_id,batch_id 0 83 miss% 0.8606779863970755
plot_id,batch_id 0 84 miss% 0.8639532453650411
plot_id,batch_id 0 85 miss% 0.7786637986680188
plot_id,batch_id 0 86 miss% 0.8421898009515462
plot_id,batch_id 0 87 miss% 0.8516109097218261
plot_id,batch_id 0 88 miss% 0.8604396169043872
plot_id,batch_id 0 89 miss% 0.8608704321697898
plot_id,batch_id 0 90 miss% 0.7482082985555234
plot_id,batch_id 0 91 miss% 0.8362594249890969
plot_id,batch_id 0 92 miss% 0.8455860621971129
plot_id,batch_id 0 93 miss% 0.850748739923616
plot_id,batch_id 0 94 miss% 0.8630749170908276
plot_id,batch_id 0 95 miss% 0.7660027465190992
plot_id,batch_id 0 96 miss% 0.8299287132046237
plot_id,batch_id 0 97 miss% 0.8461429503490762
plot_id,batch_id 0 98 miss% 0.8509759150464405
plot_id,batch_id 0 99 miss% 0.8553794430387288
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.81829338 0.85502475 0.86092957 0.86531051 0.87020103 0.81233121
 0.85314901 0.86225315 0.86411147 0.86729924 0.80545433 0.85093467
 0.86059041 0.86205718 0.86810828 0.80278852 0.84748335 0.86167374
 0.86392004 0.86509908 0.83779797 0.86244149 0.86657242 0.86782376
 0.86993902 0.8366187  0.85791738 0.86533508 0.86685604 0.86895987
 0.82263932 0.85896506 0.86192306 0.86365914 0.86566674 0.82551212
 0.8586934  0.86186996 0.86552593 0.86490888 0.85034116 0.86249986
 0.86444261 0.86931089 0.86869992 0.84731619 0.86233091 0.86570732
 0.8704525  0.86785593 0.84844386 0.86233957 0.86523339 0.86840204
 0.86995264 0.85046327 0.86421711 0.86701409 0.8697486  0.8683754
 0.76696041 0.84049537 0.85426832 0.85762417 0.86145676 0.7650701
 0.83169815 0.8430825  0.85855483 0.85688527 0.72072915 0.83540902
 0.84185158 0.85000103 0.857652   0.74247042 0.82105523 0.83151405
 0.84267145 0.85466098 0.78792909 0.84966697 0.85683374 0.86067799
 0.86395325 0.7786638  0.8421898  0.85161091 0.86043962 0.86087043
 0.7482083  0.83625942 0.84558606 0.85074874 0.86307492 0.76600275
 0.82992871 0.84614295 0.85097592 0.85537944]
for model  41 the mean error 0.8466903911232841
all id 41 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 21 out win 5 err 0.8466903911232841
Launcher: Job 42 completed in 6885 seconds.
Launcher: Task 47 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  61841
Epoch:0, Train loss:0.465810, valid loss:0.466722
Epoch:1, Train loss:0.082295, valid loss:0.003185
Epoch:2, Train loss:0.004903, valid loss:0.002383
Epoch:3, Train loss:0.003433, valid loss:0.001636
Epoch:4, Train loss:0.002940, valid loss:0.001371
Epoch:5, Train loss:0.002542, valid loss:0.001389
Epoch:6, Train loss:0.002407, valid loss:0.001388
Epoch:7, Train loss:0.002272, valid loss:0.001422
Epoch:8, Train loss:0.002189, valid loss:0.001637
Epoch:9, Train loss:0.002091, valid loss:0.001318
Epoch:10, Train loss:0.001965, valid loss:0.001190
Epoch:11, Train loss:0.001401, valid loss:0.001025
Epoch:12, Train loss:0.001378, valid loss:0.000787
Epoch:13, Train loss:0.001343, valid loss:0.000859
Epoch:14, Train loss:0.001368, valid loss:0.000915
Epoch:15, Train loss:0.001305, valid loss:0.000943
Epoch:16, Train loss:0.001302, valid loss:0.000934
Epoch:17, Train loss:0.001276, valid loss:0.001021
Epoch:18, Train loss:0.001214, valid loss:0.000780
Epoch:19, Train loss:0.001241, valid loss:0.000852
Epoch:20, Train loss:0.001215, valid loss:0.000951
Epoch:21, Train loss:0.000940, valid loss:0.000656
Epoch:22, Train loss:0.000921, valid loss:0.000618
Epoch:23, Train loss:0.000913, valid loss:0.000686
Epoch:24, Train loss:0.000915, valid loss:0.000744
Epoch:25, Train loss:0.000916, valid loss:0.000702
Epoch:26, Train loss:0.000876, valid loss:0.000623
Epoch:27, Train loss:0.000895, valid loss:0.000626
Epoch:28, Train loss:0.000879, valid loss:0.000716
Epoch:29, Train loss:0.000884, valid loss:0.000575
Epoch:30, Train loss:0.000881, valid loss:0.000960
Epoch:31, Train loss:0.000737, valid loss:0.000575
Epoch:32, Train loss:0.000724, valid loss:0.000575
Epoch:33, Train loss:0.000741, valid loss:0.000711
Epoch:34, Train loss:0.000739, valid loss:0.000557
Epoch:35, Train loss:0.000721, valid loss:0.000537
Epoch:36, Train loss:0.000719, valid loss:0.000557
Epoch:37, Train loss:0.000723, valid loss:0.000540
Epoch:38, Train loss:0.000715, valid loss:0.000560
Epoch:39, Train loss:0.000714, valid loss:0.000553
Epoch:40, Train loss:0.000702, valid loss:0.000531
Epoch:41, Train loss:0.000640, valid loss:0.000520
Epoch:42, Train loss:0.000638, valid loss:0.000577
Epoch:43, Train loss:0.000644, valid loss:0.000537
Epoch:44, Train loss:0.000634, valid loss:0.000531
Epoch:45, Train loss:0.000639, valid loss:0.000532
Epoch:46, Train loss:0.000637, valid loss:0.000504
Epoch:47, Train loss:0.000631, valid loss:0.000521
Epoch:48, Train loss:0.000629, valid loss:0.000548
Epoch:49, Train loss:0.000620, valid loss:0.000519
Epoch:50, Train loss:0.000629, valid loss:0.000518
Epoch:51, Train loss:0.000596, valid loss:0.000504
Epoch:52, Train loss:0.000596, valid loss:0.000491
Epoch:53, Train loss:0.000594, valid loss:0.000510
Epoch:54, Train loss:0.000593, valid loss:0.000515
Epoch:55, Train loss:0.000595, valid loss:0.000501
Epoch:56, Train loss:0.000593, valid loss:0.000497
Epoch:57, Train loss:0.000590, valid loss:0.000502
Epoch:58, Train loss:0.000589, valid loss:0.000492
Epoch:59, Train loss:0.000588, valid loss:0.000502
Epoch:60, Train loss:0.000593, valid loss:0.000503
training time 6698.617599725723
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.014058005787743409
plot_id,batch_id 0 1 miss% 0.02967805428937574
plot_id,batch_id 0 2 miss% 0.024762053193648055
plot_id,batch_id 0 3 miss% 0.027469249434858324
plot_id,batch_id 0 4 miss% 0.02401400462183296
plot_id,batch_id 0 5 miss% 0.039511987611418324
plot_id,batch_id 0 6 miss% 0.02760637762912003
plot_id,batch_id 0 7 miss% 0.018893785284180262
plot_id,batch_id 0 8 miss% 0.024472895400519147
plot_id,batch_id 0 9 miss% 0.019104541524272257
plot_id,batch_id 0 10 miss% 0.039497634881770285
plot_id,batch_id 0 11 miss% 0.03847056787278281
plot_id,batch_id 0 12 miss% 0.019200222064022096
plot_id,batch_id 0 13 miss% 0.01591574177012701
plot_id,batch_id 0 14 miss% 0.018754104904927744
plot_id,batch_id 0 15 miss% 0.026310777565251943
plot_id,batch_id 0 16 miss% 0.030361012547272446
plot_id,batch_id 0 17 miss% 0.03809526103837306
plot_id,batch_id 0 18 miss% 0.025146196384237495
plot_id,batch_id 0 19 miss% 0.03409912362405607
plot_id,batch_id 0 20 miss% 0.021078398443965182
plot_id,batch_id 0 21 miss% 0.01694030847562005
plot_id,batch_id 0 22 miss% 0.032510952132725826
plot_id,batch_id 0 23 miss% 0.021155751147058074
plot_id,batch_id 0 24 miss% 0.03158437374642707
plot_id,batch_id 0 25 miss% 0.051334878772032695
plot_id,batch_id 0 26 miss% 0.027910329366672853
plot_id,batch_id 0 27 miss% 0.017072020007352307
plot_id,batch_id 0 28 miss% 0.025545679799464406
plot_id,batch_id 0 29 miss% 0.025943507736346277
plot_id,batch_id 0 30 miss% 0.03867934164753796
plot_id,batch_id 0 31 miss% 0.034015701291869774
plot_id,batch_id 0 32 miss% 0.018512410936386586
plot_id,batch_id 0 33 miss% 0.02406467963421181
plot_id,batch_id 0 34 miss% 0.026855793386975357
plot_id,batch_id 0 35 miss% 0.029593735074550532
plot_id,batch_id 0 36 miss% 0.03294965406315478
plot_id,batch_id 0 37 miss% 0.021168846808547817
plot_id,batch_id 0 38 miss% 0.019652978510357983
plot_id,batch_id 0 39 miss% 0.01612735542887111
plot_id,batch_id 0 40 miss% 0.07642878558136829
plot_id,batch_id 0 41 miss% 0.02553494824419624
plot_id,batch_id 0 42 miss% 0.02849535545049165
plot_id,batch_id 0 43 miss% 0.023386530674128637
plot_id,batch_id 0 44 miss% 0.02360620253029203
plot_id,batch_id 0 45 miss% 0.019517708733576233
plot_id,batch_id 0 46 miss% 0.025590750433106633
plot_id,batch_id 0 47 miss% 0.02872964815659201
plot_id,batch_id 0 48 miss% 0.019990194238789772
plot_id,batch_id 0 49 miss% 0.024916106658499866
plot_id,batch_id 0 50 miss% 0.025351926717561958
plot_id,batch_id 0 51 miss% 0.038275437631174906
plot_id,batch_id 0 52 miss% 0.026072190151019057
plot_id,batch_id 0 53 miss% 0.016656159179263084
plot_id,batch_id 0 54 miss% 0.03167047006299122
plot_id,batch_id 0 55 miss% 0.03818484452497442
plot_id,batch_id 0 56 miss% 0.029730810425769767
plot_id,batch_id 0 57 miss% 0.02715884300834237
plot_id,batch_id 0 58 miss% 0.022045879340320207
plot_id,batch_id 0 59 miss% 0.025172852384100462
plot_id,batch_id 0 60 miss% 0.03802055506665307
plot_id,batch_id 0 61 miss% 0.021699235737795808
plot_id,batch_id 0 62 miss% 0.0286791828188901
plot_id,batch_id 0 63 miss% 0.027875935367238314
plot_id,batch_id 0 64 miss% 0.0265705471942593
plot_id,batch_id 0 65 miss% 0.032913994491556006
plot_id,batch_id 0 66 miss% 0.037007545065298346
plot_id,batch_id 0 67 miss% 0.032719491999420106
plot_id,batch_id 0 68 miss% 0.019408173898190294
plot_id,batch_id 0 69 miss% 0.026446425127623466
plot_id,batch_id 0 70 miss% 0.04515168362345225
plot_id,batch_id 0 71 miss% 0.045916050704023804
plot_id,batch_id 0 72 miss% 0.031289775634056105
plot_id,batch_id 0 73 miss% 0.03411759918299994
plot_id,batch_id 0 74 miss% 0.030291303432894205
plot_id,batch_id 0 75 miss% 0.043816105691639566
plot_id,batch_id 0 76 miss% 0.03433924649784293
plot_id,batch_id 0 77 miss% 0.028209655182593878
plot_id,batch_id 0 78 miss% 0.03807131001196519
plot_id,batch_id 0 79 miss% 0.05219630542144484
plot_id,batch_id 0 80 miss% 0.043340355706630196
plot_id,batch_id 0 81 miss% 0.025165404802658588
plot_id,batch_id 0 82 miss% 0.03158264039404097
plot_id,batch_id 0 83 miss% 0.03136560305325472
plot_id,batch_id 0 84 miss% 0.02452848391439875
plot_id,batch_id 0 85 miss% 0.04550803486475243
plot_id,batch_id 0 86 miss% 0.021466727144741935
plot_id,batch_id 0 87 miss% 0.03713827696788793
plot_id,batch_id 0 88 miss% 0.03884604769179808
plot_id,batch_id 0 89 miss% 0.019156550336540536
plot_id,batch_id 0 90 miss% 0.03834627479289628
plot_id,batch_id 0 91 miss% 0.03355896189763086
plot_id,batch_id 0 92 miss% 0.03416436302614632
plot_id,batch_id 0 93 miss% 0.028259642664348935
plot_id,batch_id 0 94 miss% 0.038965414314014825
plot_id,batch_id 0 95 miss% 0.0341083740323836
plot_id,batch_id 0 96 miss% 0.03548129343046825
plot_id,batch_id 0 97 miss% 0.054809588468122596
plot_id,batch_id 0 98 miss% 0.02907339467827955
plot_id,batch_id 0 99 miss% 0.029428609316356703
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01405801 0.02967805 0.02476205 0.02746925 0.024014   0.03951199
 0.02760638 0.01889379 0.0244729  0.01910454 0.03949763 0.03847057
 0.01920022 0.01591574 0.0187541  0.02631078 0.03036101 0.03809526
 0.0251462  0.03409912 0.0210784  0.01694031 0.03251095 0.02115575
 0.03158437 0.05133488 0.02791033 0.01707202 0.02554568 0.02594351
 0.03867934 0.0340157  0.01851241 0.02406468 0.02685579 0.02959374
 0.03294965 0.02116885 0.01965298 0.01612736 0.07642879 0.02553495
 0.02849536 0.02338653 0.0236062  0.01951771 0.02559075 0.02872965
 0.01999019 0.02491611 0.02535193 0.03827544 0.02607219 0.01665616
 0.03167047 0.03818484 0.02973081 0.02715884 0.02204588 0.02517285
 0.03802056 0.02169924 0.02867918 0.02787594 0.02657055 0.03291399
 0.03700755 0.03271949 0.01940817 0.02644643 0.04515168 0.04591605
 0.03128978 0.0341176  0.0302913  0.04381611 0.03433925 0.02820966
 0.03807131 0.05219631 0.04334036 0.0251654  0.03158264 0.0313656
 0.02452848 0.04550803 0.02146673 0.03713828 0.03884605 0.01915655
 0.03834627 0.03355896 0.03416436 0.02825964 0.03896541 0.03410837
 0.03548129 0.05480959 0.02907339 0.02942861]
for model  201 the mean error 0.029936681016136668
all id 201 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 31 out win 3 err 0.029936681016136668
Launcher: Job 202 completed in 6885 seconds.
Launcher: Task 38 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  35921
Epoch:0, Train loss:0.543107, valid loss:0.533067
Epoch:1, Train loss:0.021277, valid loss:0.003969
Epoch:2, Train loss:0.005539, valid loss:0.002538
Epoch:3, Train loss:0.004187, valid loss:0.001790
Epoch:4, Train loss:0.003640, valid loss:0.001602
Epoch:5, Train loss:0.003322, valid loss:0.001579
Epoch:6, Train loss:0.003159, valid loss:0.001763
Epoch:7, Train loss:0.003068, valid loss:0.001651
Epoch:8, Train loss:0.002896, valid loss:0.001949
Epoch:9, Train loss:0.002813, valid loss:0.001635
Epoch:10, Train loss:0.002771, valid loss:0.001950
Epoch:11, Train loss:0.001829, valid loss:0.001045
Epoch:12, Train loss:0.001842, valid loss:0.001053
Epoch:13, Train loss:0.001801, valid loss:0.000908
Epoch:14, Train loss:0.001778, valid loss:0.001148
Epoch:15, Train loss:0.001736, valid loss:0.000974
Epoch:16, Train loss:0.001742, valid loss:0.001197
Epoch:17, Train loss:0.001701, valid loss:0.001388
Epoch:18, Train loss:0.001725, valid loss:0.001233
Epoch:19, Train loss:0.001684, valid loss:0.000922
Epoch:20, Train loss:0.001693, valid loss:0.001152
Epoch:21, Train loss:0.001269, valid loss:0.000858
Epoch:22, Train loss:0.001249, valid loss:0.000729
Epoch:23, Train loss:0.001263, valid loss:0.000753
Epoch:24, Train loss:0.001239, valid loss:0.000859
Epoch:25, Train loss:0.001241, valid loss:0.000728
Epoch:26, Train loss:0.001222, valid loss:0.000768
Epoch:27, Train loss:0.001230, valid loss:0.001043
Epoch:28, Train loss:0.001222, valid loss:0.000718
Epoch:29, Train loss:0.001195, valid loss:0.000689
Epoch:30, Train loss:0.001207, valid loss:0.000741
Epoch:31, Train loss:0.000988, valid loss:0.000632
Epoch:32, Train loss:0.000978, valid loss:0.000703
Epoch:33, Train loss:0.000995, valid loss:0.000673
Epoch:34, Train loss:0.000982, valid loss:0.000700
Epoch:35, Train loss:0.000986, valid loss:0.000631
Epoch:36, Train loss:0.000973, valid loss:0.000767
Epoch:37, Train loss:0.000965, valid loss:0.000644
Epoch:38, Train loss:0.000974, valid loss:0.000689
Epoch:39, Train loss:0.000957, valid loss:0.000651
Epoch:40, Train loss:0.000959, valid loss:0.000673
Epoch:41, Train loss:0.000842, valid loss:0.000578
Epoch:42, Train loss:0.000846, valid loss:0.000659
Epoch:43, Train loss:0.000844, valid loss:0.000641
Epoch:44, Train loss:0.000843, valid loss:0.000621
Epoch:45, Train loss:0.000834, valid loss:0.000619
Epoch:46, Train loss:0.000835, valid loss:0.000624
Epoch:47, Train loss:0.000835, valid loss:0.000585
Epoch:48, Train loss:0.000831, valid loss:0.000591
Epoch:49, Train loss:0.000836, valid loss:0.000606
Epoch:50, Train loss:0.000829, valid loss:0.000622
Epoch:51, Train loss:0.000777, valid loss:0.000572
Epoch:52, Train loss:0.000774, valid loss:0.000602
Epoch:53, Train loss:0.000771, valid loss:0.000581
Epoch:54, Train loss:0.000772, valid loss:0.000575
Epoch:55, Train loss:0.000767, valid loss:0.000566
Epoch:56, Train loss:0.000768, valid loss:0.000590
Epoch:57, Train loss:0.000765, valid loss:0.000591
Epoch:58, Train loss:0.000765, valid loss:0.000595
Epoch:59, Train loss:0.000765, valid loss:0.000555
Epoch:60, Train loss:0.000767, valid loss:0.000579
training time 6717.1509890556335
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.013194547940414997
plot_id,batch_id 0 1 miss% 0.028194209080955955
plot_id,batch_id 0 2 miss% 0.018939812901286657
plot_id,batch_id 0 3 miss% 0.02704554904913295
plot_id,batch_id 0 4 miss% 0.02689342312092768
plot_id,batch_id 0 5 miss% 0.04044199463122425
plot_id,batch_id 0 6 miss% 0.036275474104571964
plot_id,batch_id 0 7 miss% 0.032627804322835706
plot_id,batch_id 0 8 miss% 0.017896960544887607
plot_id,batch_id 0 9 miss% 0.023063959251955642
plot_id,batch_id 0 10 miss% 0.030445585703954448
plot_id,batch_id 0 11 miss% 0.0385653799491097
plot_id,batch_id 0 12 miss% 0.038436969792212276
plot_id,batch_id 0 13 miss% 0.030360736486534066
plot_id,batch_id 0 14 miss% 0.036591703513570724
plot_id,batch_id 0 15 miss% 0.04044725376516274
plot_id,batch_id 0 16 miss% 0.03673736516614467
plot_id,batch_id 0 17 miss% 0.04941365860253321
plot_id,batch_id 0 18 miss% 0.034581600728130955
plot_id,batch_id 0 19 miss% 0.02075660466926299
plot_id,batch_id 0 20 miss% 0.04677961979746563
plot_id,batch_id 0 21 miss% 0.027130230447145104
plot_id,batch_id 0 22 miss% 0.02725285302542727
plot_id,batch_id 0 23 miss% 0.021508768463230455
plot_id,batch_id 0 24 miss% 0.033585197936406
plot_id,batch_id 0 25 miss% 0.04023831060573357
plot_id,batch_id 0 26 miss% 0.03319200917103137
plot_id,batch_id 0 27 miss% 0.027686464670496094
plot_id,batch_id 0 28 miss% 0.02032877253784045
plot_id,batch_id 0 29 miss% 0.025016961031250562
plot_id,batch_id 0 30 miss% 0.04689449432413021
plot_id,batch_id 0 31 miss% 0.024162857182594394
plot_id,batch_id 0 32 miss% 0.022748457268192183
plot_id,batch_id 0 33 miss% 0.017818463652877255
plot_id,batch_id 0 34 miss% 0.018964145103578276
plot_id,batch_id 0 35 miss% 0.04103463262037093
plot_id,batch_id 0 36 miss% 0.027029134500925023
plot_id,batch_id 0 37 miss% 0.0218076790694771
plot_id,batch_id 0 38 miss% 0.023474608590693603
plot_id,batch_id 0 39 miss% 0.03044657681641661
plot_id,batch_id 0 40 miss% 0.049568624459597435
plot_id,batch_id 0 41 miss% 0.017621480347207258
plot_id,batch_id 0 42 miss% 0.014129909762564934
plot_id,batch_id 0 43 miss% 0.05070438234677275
plot_id,batch_id 0 44 miss% 0.024303397728797006
plot_id,batch_id 0 45 miss% 0.0380827319461355
plot_id,batch_id 0 46 miss% 0.020113914194953172
plot_id,batch_id 0 47 miss% 0.02316022506078557
plot_id,batch_id 0 48 miss% 0.027034821279210693
plot_id,batch_id 0 49 miss% 0.029678686992118213
plot_id,batch_id 0 50 miss% 0.043770630326809466
plot_id,batch_id 0 51 miss% 0.031026316904180222
plot_id,batch_id 0 52 miss% 0.022581041544212473
plot_id,batch_id 0 53 miss% 0.0107720394001687
plot_id,batch_id 0 54 miss% 0.030993185536378115
plot_id,batch_id 0 55 miss% 0.030038417246061735
plot_id,batch_id 0 56 miss% 0.019064788715383307
plot_id,batch_id 0 57 miss% 0.025271525098458775
plot_id,batch_id 0 58 miss% 0.022291377333792883
plot_id,batch_id 0 59 miss% 0.027625323292302585
plot_id,batch_id 0 60 miss% 0.04017845388383586
plot_id,batch_id 0 61 miss% 0.0314191801008291
plot_id,batch_id 0 62 miss% 0.027825195991597603
plot_id,batch_id 0 63 miss% 0.03696043139897653
plot_id,batch_id 0 64 miss% 0.03554029281662991
plot_id,batch_id 0 65 miss% 0.04507917734611171
plot_id,batch_id 0 66 miss% 0.033128581463055896
plot_id,batch_id 0 67 miss% 0.03212154945391999
plot_id,batch_id 0 68 miss% 0.051636562785082524
plot_id,batch_id 0 69 miss% 0.03722681338615484
plot_id,batch_id 0 70 miss% 0.04652652695494909
plot_id,batch_id 0 71 miss% 0.0509003286843493
plot_id,batch_id 0 72 miss% 0.03622274025296294
plot_id,batch_id 0 73 miss% 0.05172956101445819
plot_id,batch_id 0 74 miss% 0.05962529108976904
plot_id,batch_id 0 75 miss% 0.053215712554482
plot_id,batch_id 0 76 miss% 0.04252041817755422
plot_id,batch_id 0 77 miss% 0.03765415767981756
plot_id,batch_id 0 78 miss% 0.053039477632221776
plot_id,batch_id 0 79 miss% 0.06830592928606113
plot_id,batch_id 0 80 miss% 0.040869612939515715
plot_id,batch_id 0 81 miss% 0.03588950347177941
plot_id,batch_id 0 82 miss% 0.022769401646086585
plot_id,batch_id 0 83 miss% 0.03550541860997752
plot_id,batch_id 0 84 miss% 0.0156543587434538
plot_id,batch_id 0 85 miss% 0.05033133657194343
plot_id,batch_id 0 86 miss% 0.02423169882564593
plot_id,batch_id 0 87 miss% 0.032324870221224676
plot_id,batch_id 0 88 miss% 0.029521816697969088
plot_id,batch_id 0 89 miss% 0.03338072564353028
plot_id,batch_id 0 90 miss% 0.03821190073558305
plot_id,batch_id 0 91 miss% 0.037910535123505625
plot_id,batch_id 0 92 miss% 0.038438617173257895
plot_id,batch_id 0 93 miss% 0.049036585823823045
plot_id,batch_id 0 94 miss% 0.03045742340697553
plot_id,batch_id 0 95 miss% 0.030785239852986077
plot_id,batch_id 0 96 miss% 0.0317162747448952
plot_id,batch_id 0 97 miss% 0.045125140692175464
plot_id,batch_id 0 98 miss% 0.033305489553973444
plot_id,batch_id 0 99 miss% 0.04102533935482378
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01319455 0.02819421 0.01893981 0.02704555 0.02689342 0.04044199
 0.03627547 0.0326278  0.01789696 0.02306396 0.03044559 0.03856538
 0.03843697 0.03036074 0.0365917  0.04044725 0.03673737 0.04941366
 0.0345816  0.0207566  0.04677962 0.02713023 0.02725285 0.02150877
 0.0335852  0.04023831 0.03319201 0.02768646 0.02032877 0.02501696
 0.04689449 0.02416286 0.02274846 0.01781846 0.01896415 0.04103463
 0.02702913 0.02180768 0.02347461 0.03044658 0.04956862 0.01762148
 0.01412991 0.05070438 0.0243034  0.03808273 0.02011391 0.02316023
 0.02703482 0.02967869 0.04377063 0.03102632 0.02258104 0.01077204
 0.03099319 0.03003842 0.01906479 0.02527153 0.02229138 0.02762532
 0.04017845 0.03141918 0.0278252  0.03696043 0.03554029 0.04507918
 0.03312858 0.03212155 0.05163656 0.03722681 0.04652653 0.05090033
 0.03622274 0.05172956 0.05962529 0.05321571 0.04252042 0.03765416
 0.05303948 0.06830593 0.04086961 0.0358895  0.0227694  0.03550542
 0.01565436 0.05033134 0.0242317  0.03232487 0.02952182 0.03338073
 0.0382119  0.03791054 0.03843862 0.04903659 0.03045742 0.03078524
 0.03171627 0.04512514 0.03330549 0.04102534]
for model  234 the mean error 0.03323191321441926
all id 234 hidden_dim 16 learning_rate 0.02 num_layers 5 frames 31 out win 3 err 0.03323191321441926
Launcher: Job 235 completed in 6904 seconds.
Launcher: Task 171 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  61841
Epoch:0, Train loss:0.465810, valid loss:0.466722
Epoch:1, Train loss:0.226911, valid loss:0.229701
Epoch:2, Train loss:0.218461, valid loss:0.229182
Epoch:3, Train loss:0.144081, valid loss:0.002986
Epoch:4, Train loss:0.003569, valid loss:0.001895
Epoch:5, Train loss:0.002939, valid loss:0.001788
Epoch:6, Train loss:0.002776, valid loss:0.001508
Epoch:7, Train loss:0.002609, valid loss:0.001150
Epoch:8, Train loss:0.002523, valid loss:0.001444
Epoch:9, Train loss:0.002494, valid loss:0.001311
Epoch:10, Train loss:0.002388, valid loss:0.001535
Epoch:11, Train loss:0.001645, valid loss:0.001060
Epoch:12, Train loss:0.001624, valid loss:0.001052
Epoch:13, Train loss:0.001635, valid loss:0.001258
Epoch:14, Train loss:0.001607, valid loss:0.001033
Epoch:15, Train loss:0.001545, valid loss:0.000990
Epoch:16, Train loss:0.001532, valid loss:0.001117
Epoch:17, Train loss:0.001558, valid loss:0.001226
Epoch:18, Train loss:0.001480, valid loss:0.001135
Epoch:19, Train loss:0.001489, valid loss:0.001070
Epoch:20, Train loss:0.001440, valid loss:0.000877
Epoch:21, Train loss:0.001098, valid loss:0.000745
Epoch:22, Train loss:0.001062, valid loss:0.000665
Epoch:23, Train loss:0.001093, valid loss:0.000717
Epoch:24, Train loss:0.001057, valid loss:0.000841
Epoch:25, Train loss:0.001076, valid loss:0.000882
Epoch:26, Train loss:0.001060, valid loss:0.000769
Epoch:27, Train loss:0.001031, valid loss:0.000783
Epoch:28, Train loss:0.001031, valid loss:0.000831
Epoch:29, Train loss:0.001060, valid loss:0.000657
Epoch:30, Train loss:0.001029, valid loss:0.000868
Epoch:31, Train loss:0.000823, valid loss:0.000635
Epoch:32, Train loss:0.000824, valid loss:0.000619
Epoch:33, Train loss:0.000823, valid loss:0.000663
Epoch:34, Train loss:0.000827, valid loss:0.000690
Epoch:35, Train loss:0.000817, valid loss:0.000599
Epoch:36, Train loss:0.000811, valid loss:0.000620
Epoch:37, Train loss:0.000803, valid loss:0.000574
Epoch:38, Train loss:0.000797, valid loss:0.000610
Epoch:39, Train loss:0.000796, valid loss:0.000568
Epoch:40, Train loss:0.000836, valid loss:0.000597
Epoch:41, Train loss:0.000695, valid loss:0.000576
Epoch:42, Train loss:0.000706, valid loss:0.000550
Epoch:43, Train loss:0.000693, valid loss:0.000567
Epoch:44, Train loss:0.000697, valid loss:0.000603
Epoch:45, Train loss:0.000687, valid loss:0.000550
Epoch:46, Train loss:0.000697, valid loss:0.000519
Epoch:47, Train loss:0.000682, valid loss:0.000537
Epoch:48, Train loss:0.000689, valid loss:0.000540
Epoch:49, Train loss:0.000688, valid loss:0.000599
Epoch:50, Train loss:0.000681, valid loss:0.000536
Epoch:51, Train loss:0.000631, valid loss:0.000538
Epoch:52, Train loss:0.000631, valid loss:0.000540
Epoch:53, Train loss:0.000632, valid loss:0.000548
Epoch:54, Train loss:0.000627, valid loss:0.000567
Epoch:55, Train loss:0.000638, valid loss:0.000537
Epoch:56, Train loss:0.000624, valid loss:0.000523
Epoch:57, Train loss:0.000628, valid loss:0.000530
Epoch:58, Train loss:0.000625, valid loss:0.000574
Epoch:59, Train loss:0.000624, valid loss:0.000529
Epoch:60, Train loss:0.000629, valid loss:0.000525
training time 6752.256258487701
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.02783949720013934
plot_id,batch_id 0 1 miss% 0.03152840204523483
plot_id,batch_id 0 2 miss% 0.030307040840340048
plot_id,batch_id 0 3 miss% 0.037299839365757904
plot_id,batch_id 0 4 miss% 0.0204777388228235
plot_id,batch_id 0 5 miss% 0.039331959201017445
plot_id,batch_id 0 6 miss% 0.032535976187104826
plot_id,batch_id 0 7 miss% 0.031045042521801132
plot_id,batch_id 0 8 miss% 0.018549070299983424
plot_id,batch_id 0 9 miss% 0.0380412405471212
plot_id,batch_id 0 10 miss% 0.05734557954089359
plot_id,batch_id 0 11 miss% 0.026037900282686058
plot_id,batch_id 0 12 miss% 0.02843248239234559
plot_id,batch_id 0 13 miss% 0.024839000051734977
plot_id,batch_id 0 14 miss% 0.0350908585556849
plot_id,batch_id 0 15 miss% 0.04416889972951913
plot_id,batch_id 0 16 miss% 0.0399228733654573
plot_id,batch_id 0 17 miss% 0.03193700804317172
plot_id,batch_id 0 18 miss% 0.030159829301553358
plot_id,batch_id 0 19 miss% 0.034797536802408276
plot_id,batch_id 0 20 miss% 0.050280184768604644
plot_id,batch_id 0 21 miss% 0.02978558713626974
plot_id,batch_id 0 22 miss% 0.0285230371712531
plot_id,batch_id 0 23 miss% 0.024465503429879357
plot_id,batch_id 0 24 miss% 0.026793835958062347
plot_id,batch_id 0 25 miss% 0.033699584952200996
plot_id,batch_id 0 26 miss% 0.034403221313375004
plot_id,batch_id 0 27 miss% 0.03945363745107513
plot_id,batch_id 0 28 miss% 0.02104029122037701
plot_id,batch_id 0 29 miss% 0.02658328689370649
plot_id,batch_id 0 30 miss% 0.05157517036639816
plot_id,batch_id 0 31 miss% 0.04608238178521221
plot_id,batch_id 0 32 miss% 0.031923333518836125
plot_id,batch_id 0 33 miss% 0.019986682678910024
plot_id,batch_id 0 34 miss% 0.02857792366317461
plot_id,batch_id 0 35 miss% 0.027469324886458195
plot_id,batch_id 0 36 miss% 0.033969989401792446
plot_id,batch_id 0 37 miss% 0.024019600274976398
plot_id,batch_id 0 38 miss% 0.020837022893228894
plot_id,batch_id 0 39 miss% 0.02845600469738958
plot_id,batch_id 0 40 miss% 0.06632054243371635
plot_id,batch_id 0 41 miss% 0.024188998390595927
plot_id,batch_id 0 42 miss% 0.015421321899680663
plot_id,batch_id 0 43 miss% 0.031146580057883084
plot_id,batch_id 0 44 miss% 0.02407640999223623
plot_id,batch_id 0 45 miss% 0.044568995899159844
plot_id,batch_id 0 46 miss% 0.032892264199972196
plot_id,batch_id 0 47 miss% 0.018890700699641345
plot_id,batch_id 0 48 miss% 0.0215194054431999
plot_id,batch_id 0 49 miss% 0.0187874270517458
plot_id,batch_id 0 50 miss% 0.045470187328255696
plot_id,batch_id 0 51 miss% 0.03446628010713256
plot_id,batch_id 0 52 miss% 0.01930117423116267
plot_id,batch_id 0 53 miss% 0.018057000072633084
plot_id,batch_id 0 54 miss% 0.028340685461422137
plot_id,batch_id 0 55 miss% 0.03666849440965453
plot_id,batch_id 0 56 miss% 0.01883777537513098
plot_id,batch_id 0 57 miss% 0.020548284768618086
plot_id,batch_id 0 58 miss% 0.026059055374231158
plot_id,batch_id 0 59 miss% 0.024288221078708976
plot_id,batch_id 0 60 miss% 0.056308877892640344
plot_id,batch_id 0 61 miss% 0.03435229680618811
plot_id,batch_id 0 62 miss% 0.029767357556242335
plot_id,batch_id 0 63 miss% 0.029444832831812408
plot_id,batch_id 0 64 miss% 0.031154713234553724
plot_id,batch_id 0 65 miss% 0.04157008647444668
plot_id,batch_id 0 66 miss% 0.03608642134056052
plot_id,batch_id 0 67 miss% 0.020408267191702
plot_id,batch_idthe mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  46193
Epoch:0, Train loss:0.400516, valid loss:0.363185
Epoch:1, Train loss:0.227346, valid loss:0.230485
Epoch:2, Train loss:0.220600, valid loss:0.229999
Epoch:3, Train loss:0.219685, valid loss:0.229710
Epoch:4, Train loss:0.219212, valid loss:0.229595
Epoch:5, Train loss:0.218949, valid loss:0.229137
Epoch:6, Train loss:0.218619, valid loss:0.229460
Epoch:7, Train loss:0.218532, valid loss:0.229763
Epoch:8, Train loss:0.218350, valid loss:0.229530
Epoch:9, Train loss:0.218230, valid loss:0.229354
Epoch:10, Train loss:0.218076, valid loss:0.229510
Epoch:11, Train loss:0.217333, valid loss:0.228626
Epoch:12, Train loss:0.217310, valid loss:0.228569
Epoch:13, Train loss:0.217263, valid loss:0.228784
Epoch:14, Train loss:0.217275, valid loss:0.228846
Epoch:15, Train loss:0.217226, valid loss:0.228553
Epoch:16, Train loss:0.217232, valid loss:0.228528
Epoch:17, Train loss:0.217187, valid loss:0.228780
Epoch:18, Train loss:0.217154, valid loss:0.228512
Epoch:19, Train loss:0.217198, valid loss:0.228484
Epoch:20, Train loss:0.217183, valid loss:0.228746
Epoch:21, Train loss:0.216738, valid loss:0.228539
Epoch:22, Train loss:0.216745, valid loss:0.228472
Epoch:23, Train loss:0.216758, valid loss:0.228548
Epoch:24, Train loss:0.216734, valid loss:0.228300
Epoch:25, Train loss:0.216711, valid loss:0.228359
Epoch:26, Train loss:0.216801, valid loss:0.229002
Epoch:27, Train loss:0.216754, valid loss:0.228458
Epoch:28, Train loss:0.216656, valid loss:0.228380
Epoch:29, Train loss:0.216685, valid loss:0.228447
Epoch:30, Train loss:0.216679, valid loss:0.228360
Epoch:31, Train loss:0.216462, valid loss:0.228272
Epoch:32, Train loss:0.216494, valid loss:0.228263
Epoch:33, Train loss:0.216473, valid loss:0.228229
Epoch:34, Train loss:0.216467, valid loss:0.228318
Epoch:35, Train loss:0.216474, valid loss:0.228246
Epoch:36, Train loss:0.216453, valid loss:0.228299
Epoch:37, Train loss:0.216446, valid loss:0.228287
Epoch:38, Train loss:0.216448, valid loss:0.228233
Epoch:39, Train loss:0.216443, valid loss:0.228172
Epoch:40, Train loss:0.216444, valid loss:0.228287
Epoch:41, Train loss:0.216328, valid loss:0.228218
Epoch:42, Train loss:0.216326, valid loss:0.228162
Epoch:43, Train loss:0.216321, valid loss:0.228235
Epoch:44, Train loss:0.216339, valid loss:0.228267
Epoch:45, Train loss:0.216322, valid loss:0.228220
Epoch:46, Train loss:0.216325, valid loss:0.228201
Epoch:47, Train loss:0.216313, valid loss:0.228203
Epoch:48, Train loss:0.216318, valid loss:0.228189
Epoch:49, Train loss:0.216304, valid loss:0.228184
Epoch:50, Train loss:0.216306, valid loss:0.228180
Epoch:51, Train loss:0.216254, valid loss:0.228172
Epoch:52, Train loss:0.216251, valid loss:0.228161
Epoch:53, Train loss:0.216255, valid loss:0.228153
Epoch:54, Train loss:0.216249, valid loss:0.228164
Epoch:55, Train loss:0.216250, valid loss:0.228140
Epoch:56, Train loss:0.216247, valid loss:0.228156
Epoch:57, Train loss:0.216243, valid loss:0.228148
Epoch:58, Train loss:0.216243, valid loss:0.228157
Epoch:59, Train loss:0.216245, valid loss:0.228154
Epoch:60, Train loss:0.216249, valid loss:0.228175
training time 6763.3123779296875
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.6599868262609897
plot_id,batch_id 0 1 miss% 0.7455279289850207
plot_id,batch_id 0 2 miss% 0.7590823968329413
plot_id,batch_id 0 3 miss% 0.7695509789530816
plot_id,batch_id 0 4 miss% 0.7687010681356806
plot_id,batch_id 0 5 miss% 0.6632529520289109
plot_id,batch_id 0 6 miss% 0.7440051279164941
plot_id,batch_id 0 7 miss% 0.7570080423573752
plot_id,batch_id 0 8 miss% 0.7675553485607671
plot_id,batch_id 0 9 miss% 0.7704754188635332
plot_id,batch_id 0 10 miss% 0.633722062659884
plot_id,batch_id 0 11 miss% 0.7426860754899651
plot_id,batch_id 0 12 miss% 0.7529451294226585
plot_id,batch_id 0 13 miss% 0.7623391031393655
plot_id,batch_id 0 14 miss% 0.7705907642082291
plot_id,batch_id 0 15 miss% 0.6517917995603889
plot_id,batch_id 0 16 miss% 0.7394471033865463
plot_id,batch_id 0 17 miss% 0.7613580293399561
plot_id,batch_id 0 18 miss% 0.7642168870155139
plot_id,batch_id 0 19 miss% 0.7662621462483873
plot_id,batch_id 0 20 miss% 0.7118737970294311
plot_id,batch_id 0 21 miss% 0.7617778362602787
plot_id,batch_id 0 22 miss% 0.7716393221261705
plot_id,batch_id 0 23 miss% 0.7748551820262058
plot_id,batch_id 0 24 miss% 0.7773129579427795
plot_id,batch_id 0 25 miss% 0.6939476173670346
plot_id,batch_id 0 26 miss% 0.7579833747635876
plot_id,batch_id 0 27 miss% 0.768145030184595
plot_id,batch_id 0 28 miss% 0.7685265561166094
plot_id,batch_id 0 29 miss% 0.7772491143994803
plot_id,batch_id 0 30 miss% 0.6982820357279288
plot_id,batch_id 0 31 miss% 0.7536254161213327
plot_id,batch_id 0 32 miss% 0.7636801217430691
plot_id,batch_id 0 33 miss% 0.7687011319194509
plot_id,batch_id 0 34 miss% 0.7704116820975031
plot_id,batch_id 0 35 miss% 0.6896790415690606
plot_id,batch_id 0 36 miss% 0.7602920166303558
plot_id,batch_id 0 37 miss% 0.7662408340349898
plot_id,batch_id 0 38 miss% 0.7726215360315313
plot_id,batch_id 0 39 miss% 0.7730052223979639
plot_id,batch_id 0 40 miss% 0.733326043143927
plot_id,batch_id 0 41 miss% 0.7673977495189074
plot_id,batch_id 0 42 miss% 0.7709140753713914
plot_id,batch_id 0 43 miss% 0.7781356872225415
plot_id,batch_id 0 44 miss% 0.7813615732508754
plot_id,batch_id 0 45 miss% 0.7298571986112057
plot_id,batch_id 0 46 miss% 0.7709793023976855
plot_id,batch_id 0 47 miss% 0.772257340719435
plot_id,batch_id 0 48 miss% 0.7770251802656454
plot_id,batch_id 0 49 miss% 0.7798977172885383
plot_id,batch_id 0 50 miss% 0.7410243923702694
plot_id,batch_id 0 51 miss% 0.7636670038105258
plot_id,batch_id 0 52 miss% 0.7705790209825701
plot_id,batch_id 0 53 miss% 0.7762032611993837
plot_id,batch_id 0 54 miss% 0.7834086735902772
plot_id,batch_id 0 55 miss% 0.7433038659156834
plot_id,batch_id 0 56 miss% 0.7664165009756885
plot_id,batch_id 0 57 miss% 0.7720444476770721
plot_id,batch_id 0 58 miss% 0.776698009386631
plot_id,batch_id 0 59 miss% 0.7813884606020728
plot_id,batch_id 0 60 miss% 0.5699346790270788
plot_id,batch_id 0 61 miss% 0.7053663379393498
plot_id,batch_id 0 62 miss% 0.7341212020280935
plot_id,batch_id 0 63 miss% 0.7568926676170863
plot_id,batch_id 0 64 miss% 0.7597867331094059
plot_id,batch_id 0 65 miss% 0.5623746651759077
plot_id,batch_id 0 66 miss% 0.7006690893785839
plot_id,batch_id 0 67 miss% 0.7257869591167373
plot_id,batch_id 0 68 miss% 0.7499863187780593
plot_id,batch_id 0 69 miss% 0.7490888134578284
plot_id,batch_id 0 70 miss% 0.5289965890448194
plot_id,batch_id 0 71 miss% 0.6991755739267174
plot_id,batch_id 0 72 miss% 0.7162998568326199
plot_id,batch_id 0 73 miss% 0.7383213325356792
plot_id,batch_id 0 74 miss% 0.7438713734539867
plot_id,batch_id 0 75 miss% 0.5143652457128639
plot_id,batch_id 0 76 miss% 0.6502937786935159
plot_id,batch_id 0 77 miss% 0.6979355160288149
plot_id,batch_id 0 78 miss% 0.7344920350302662
plot_id,batch_id 0 79 miss% 0.7411612285663778
plot_id,batch_id 0 80 miss% 0.5948746817726084
plot_id,batch_id 0 81 miss% 0.7237137455613906
plot_id,batch_id 0 82 miss% 0.7443796172570998
plot_id,batch_id 0 83 miss% 0.7573692324064507
plot_id,batch_id 0 84 miss% 0.7637738627299154
plot_id,batch_id 0 85 miss% 0.5935275376457175
plot_id,batch_id 0 86 miss% 0.7173328199498995
plot_id,batch_id 0 87 miss% 0.7400796143238044
plot_id,batch_id 0 88 miss% 0.7575882605909405
plot_id,batch_id 0 89 miss% 0.7632350998463947
plot_id,batch_id 0 90 miss% 0.5558946159507445
plot_id,batch_id 0 91 miss% 0.7148042707977905
plot_id,batch_id 0 92 miss% 0.7354649043187754
plot_id,batch_id 0 93 miss% 0.7446771661553142
plot_id,batch_id 0 94 miss% 0.7593621601260748
plot_id,batch_id 0 95 miss% 0.5485365585607919
plot_id,batch_id 0 96 miss% 0.6987526018048825
plot_id,batch_id 0 97 miss% 0.7293407968349027
plot_id,batch_id 0 98 miss% 0.7440026522117402
plot_id,batch_id 0 99 miss% 0.7543988974157798
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.65998683 0.74552793 0.7590824  0.76955098 0.76870107 0.66325295
 0.74400513 0.75700804 0.76755535 0.77047542 0.63372206 0.74268608
 0.75294513 0.7623391  0.77059076 0.6517918  0.7394471  0.76135803
 0.76421689 0.76626215 0.7118738  0.76177784 0.77163932 0.77485518
 0.77731296 0.69394762 0.75798337 0.76814503 0.76852656 0.77724911
 0.69828204 0.75362542 0.76368012 0.76870113 0.77041168 0.68967904
 0.76029202 0.76624083 0.77262154 0.77300522 0.73332604 0.76739775
 0.77091408 0.77813569 0.78136157 0.7298572  0.7709793  0.77225734
 0.77702518 0.77989772 0.74102439 0.763667   0.77057902 0.77620326
 0.78340867 0.74330387 0.7664165  0.77204445 0.77669801 0.78138846
 0.56993468 0.70536634 0.7341212  0.75689267 0.75978673 0.56237467
 0.70066909 0.72578696 0.74998632 0.74908881 0.52899659 0.69917557
 0.71629986 0.73832133 0.74387137 0.51436525 0.65029378 0.69793552
 0.73449204 0.74116123 0.59487468 0.72371375 0.74437962 0.75736923
 0.76377386 0.59352754 0.71733282 0.74007961 0.75758826 0.7632351
 0.55589462 0.71480427 0.7354649  0.74467717 0.75936216 0.54853656
 0.6987526  0.7293408  0.74400265 0.7543989 ]
for model  220 the mean error 0.7308626961187019
all id 220 hidden_dim 24 learning_rate 0.02 num_layers 3 frames 31 out win 4 err 0.7308626961187019
Launcher: Job 221 completed in 6937 seconds.
Launcher: Task 118 done. Exiting.
 0 68 miss% 0.03402388330460239
plot_id,batch_id 0 69 miss% 0.019325812812079693
plot_id,batch_id 0 70 miss% 0.029163214269508683
plot_id,batch_id 0 71 miss% 0.02776338674999535
plot_id,batch_id 0 72 miss% 0.02986724301141738
plot_id,batch_id 0 73 miss% 0.031065264171801273
plot_id,batch_id 0 74 miss% 0.03416269349534886
plot_id,batch_id 0 75 miss% 0.047249570363051795
plot_id,batch_id 0 76 miss% 0.04520889692915866
plot_id,batch_id 0 77 miss% 0.03135815614287629
plot_id,batch_id 0 78 miss% 0.039412519114407595
plot_id,batch_id 0 79 miss% 0.04955502534180947
plot_id,batch_id 0 80 miss% 0.033621798385016444
plot_id,batch_id 0 81 miss% 0.026667392553316405
plot_id,batch_id 0 82 miss% 0.020613035207726726
plot_id,batch_id 0 83 miss% 0.026649353675897686
plot_id,batch_id 0 84 miss% 0.018778451612456677
plot_id,batch_id 0 85 miss% 0.042291528760578886
plot_id,batch_id 0 86 miss% 0.020831468180138847
plot_id,batch_id 0 87 miss% 0.02753661783444274
plot_id,batch_id 0 88 miss% 0.03351230765795991
plot_id,batch_id 0 89 miss% 0.019698004188518914
plot_id,batch_id 0 90 miss% 0.03422742206835844
plot_id,batch_id 0 91 miss% 0.05207698789231299
plot_id,batch_id 0 92 miss% 0.02996564305956073
plot_id,batch_id 0 93 miss% 0.029822911688828196
plot_id,batch_id 0 94 miss% 0.031318209511236574
plot_id,batch_id 0 95 miss% 0.03984989385541112
plot_id,batch_id 0 96 miss% 0.034050576307689095
plot_id,batch_id 0 97 miss% 0.04602383088367175
plot_id,batch_id 0 98 miss% 0.026581783328916534
plot_id,batch_id 0 99 miss% 0.02944039921990454
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0278395  0.0315284  0.03030704 0.03729984 0.02047774 0.03933196
 0.03253598 0.03104504 0.01854907 0.03804124 0.05734558 0.0260379
 0.02843248 0.024839   0.03509086 0.0441689  0.03992287 0.03193701
 0.03015983 0.03479754 0.05028018 0.02978559 0.02852304 0.0244655
 0.02679384 0.03369958 0.03440322 0.03945364 0.02104029 0.02658329
 0.05157517 0.04608238 0.03192333 0.01998668 0.02857792 0.02746932
 0.03396999 0.0240196  0.02083702 0.028456   0.06632054 0.024189
 0.01542132 0.03114658 0.02407641 0.044569   0.03289226 0.0188907
 0.02151941 0.01878743 0.04547019 0.03446628 0.01930117 0.018057
 0.02834069 0.03666849 0.01883778 0.02054828 0.02605906 0.02428822
 0.05630888 0.0343523  0.02976736 0.02944483 0.03115471 0.04157009
 0.03608642 0.02040827 0.03402388 0.01932581 0.02916321 0.02776339
 0.02986724 0.03106526 0.03416269 0.04724957 0.0452089  0.03135816
 0.03941252 0.04955503 0.0336218  0.02666739 0.02061304 0.02664935
 0.01877845 0.04229153 0.02083147 0.02753662 0.03351231 0.019698
 0.03422742 0.05207699 0.02996564 0.02982291 0.03131821 0.03984989
 0.03405058 0.04602383 0.02658178 0.0294404 ]
for model  228 the mean error 0.03178269318738887
all id 228 hidden_dim 24 learning_rate 0.02 num_layers 4 frames 31 out win 3 err 0.03178269318738887
Launcher: Job 229 completed in 6937 seconds.
Launcher: Task 58 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  79249
Epoch:0, Train loss:0.590469, valid loss:0.568793
Epoch:1, Train loss:0.064313, valid loss:0.010007
Epoch:2, Train loss:0.016149, valid loss:0.006597
Epoch:3, Train loss:0.012379, valid loss:0.006054
Epoch:4, Train loss:0.011268, valid loss:0.006116
Epoch:5, Train loss:0.009585, valid loss:0.005431
Epoch:6, Train loss:0.008403, valid loss:0.004026
Epoch:7, Train loss:0.007896, valid loss:0.003648
Epoch:8, Train loss:0.007286, valid loss:0.003271
Epoch:9, Train loss:0.006773, valid loss:0.004487
Epoch:10, Train loss:0.006587, valid loss:0.004252
Epoch:11, Train loss:0.004522, valid loss:0.002301
Epoch:12, Train loss:0.004551, valid loss:0.002689
Epoch:13, Train loss:0.004306, valid loss:0.002429
Epoch:14, Train loss:0.004480, valid loss:0.002107
Epoch:15, Train loss:0.004247, valid loss:0.002361
Epoch:16, Train loss:0.004130, valid loss:0.002156
Epoch:17, Train loss:0.004140, valid loss:0.002121
Epoch:18, Train loss:0.004031, valid loss:0.002212
Epoch:19, Train loss:0.003823, valid loss:0.002308
Epoch:20, Train loss:0.003868, valid loss:0.001930
Epoch:21, Train loss:0.002725, valid loss:0.001691
Epoch:22, Train loss:0.002577, valid loss:0.001465
Epoch:23, Train loss:0.002661, valid loss:0.001710
Epoch:24, Train loss:0.002714, valid loss:0.001648
Epoch:25, Train loss:0.002488, valid loss:0.001824
Epoch:26, Train loss:0.002491, valid loss:0.001595
Epoch:27, Train loss:0.002693, valid loss:0.001963
Epoch:28, Train loss:0.002501, valid loss:0.001568
Epoch:29, Train loss:0.002382, valid loss:0.001828
Epoch:30, Train loss:0.002469, valid loss:0.001631
Epoch:31, Train loss:0.001846, valid loss:0.001340
Epoch:32, Train loss:0.001849, valid loss:0.001380
Epoch:33, Train loss:0.001842, valid loss:0.001358
Epoch:34, Train loss:0.001834, valid loss:0.001304
Epoch:35, Train loss:0.001843, valid loss:0.001214
Epoch:36, Train loss:0.001817, valid loss:0.001366
Epoch:37, Train loss:0.001784, valid loss:0.001257
Epoch:38, Train loss:0.001749, valid loss:0.001424
Epoch:39, Train loss:0.001780, valid loss:0.001433
Epoch:40, Train loss:0.001725, valid loss:0.001273
Epoch:41, Train loss:0.001467, valid loss:0.001245
Epoch:42, Train loss:0.001447, valid loss:0.001213
Epoch:43, Train loss:0.001473, valid loss:0.001222
Epoch:44, Train loss:0.001447, valid loss:0.001158
Epoch:45, Train loss:0.001465, valid loss:0.001240
Epoch:46, Train loss:0.001439, valid loss:0.001233
Epoch:47, Train loss:0.001409, valid loss:0.001250
Epoch:48, Train loss:0.001435, valid loss:0.001222
Epoch:49, Train loss:0.001443, valid loss:0.001179
Epoch:50, Train loss:0.001415, valid loss:0.001134
Epoch:51, Train loss:0.001268, valid loss:0.001217
Epoch:52, Train loss:0.001262, valid loss:0.001108
Epoch:53, Train loss:0.001265, valid loss:0.001224
Epoch:54, Train loss:0.001265, valid loss:0.001168
Epoch:55, Train loss:0.001261, valid loss:0.001162
Epoch:56, Train loss:0.001269, valid loss:0.001105
Epoch:57, Train loss:0.001257, valid loss:0.001206
Epoch:58, Train loss:0.001243, valid loss:0.001145
Epoch:59, Train loss:0.001248, valid loss:0.001169
Epoch:60, Train loss:0.001249, valid loss:0.001260
training time 6782.849841117859
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03315580074490877
plot_id,batch_id 0 1 miss% 0.0271218217040735
plot_id,batch_id 0 2 miss% 0.027278431926251807
plot_id,batch_id 0 3 miss% 0.02490358408161389
plot_id,batch_id 0 4 miss% 0.024794954753192208
plot_id,batch_id 0 5 miss% 0.03434829813083166
plot_id,batch_id 0 6 miss% 0.030596816352817052
plot_id,batch_id 0 7 miss% 0.03447303048994656
plot_id,batch_id 0 8 miss% 0.03748621419604084
plot_id,batch_id 0 9 miss% 0.020287707484666843
plot_id,batch_id 0 10 miss% 0.04570558436980696
plot_id,batch_id 0 11 miss% 0.030509568527709304
plot_id,batch_id 0 12 miss% 0.02117234604310753
plot_id,batch_id 0 13 miss% 0.03349779367873297
plot_id,batch_id 0 14 miss% 0.024869060621953102
plot_id,batch_id 0 15 miss% 0.04454210598780148
plot_id,batch_id 0 16 miss% 0.03390920882615187
plot_id,batch_id 0 17 miss% 0.04345006376342944
plot_id,batch_id 0 18 miss% 0.0338709549288405
plot_id,batch_id 0 19 miss% 0.03710455084348104
plot_id,batch_id 0 20 miss% 0.09872925688253675
plot_id,batch_id 0 21 miss% 0.022207367990169976
plot_id,batch_id 0 22 miss% 0.022464066731368047
plot_id,batch_id 0 23 miss% 0.022974169819064597
plot_id,batch_id 0 24 miss% 0.028863470388390707
plot_id,batch_id 0 25 miss% 0.028604490755866206
plot_id,batch_id 0 26 miss% 0.023091809250335594
plot_id,batch_id 0 27 miss% 0.028453322134774924
plot_id,batch_id 0 28 miss% 0.021161095275444332
plot_id,batch_id 0 29 miss% 0.02424788279740384
plot_id,batch_id 0 30 miss% 0.04174192796320676
plot_id,batch_id 0 31 miss% 0.04871303586766824
plot_id,batch_id 0 32 miss% 0.037381980315697144
plot_id,batch_id 0 33 miss% 0.023471489025529896
plot_id,batch_id 0 34 miss% 0.023199352061497213
plot_id,batch_id 0 35 miss% 0.03480175805163144
plot_id,batch_id 0 36 miss% 0.056569495617714856
plot_id,batch_id 0 37 miss% 0.02427741026829288
plot_id,batch_id 0 38 miss% 0.026446146444546556
plot_id,batch_id 0 39 miss% 0.020542928885620263
plot_id,batch_id 0 40 miss% 0.06997124897166165
plot_id,batch_id 0 41 miss% 0.02490822221058881
plot_id,batch_id 0 42 miss% 0.02316343750949555
plot_id,batch_id 0 43 miss% 0.02395956511138844
plot_id,batch_id 0 44 miss% 0.022207221297110787
plot_id,batch_id 0 45 miss% 0.03271845698870304
plot_id,batch_id 0 46 miss% 0.0236149779525794
plot_id,batch_id 0 47 miss% 0.02186746993355875
plot_id,batch_id 0 48 miss% 0.017324658235502742
plot_id,batch_id 0 49 miss% 0.021821899422920725
plot_id,batch_id 0 50 miss% 0.03234268484112262
plot_id,batch_id 0 51 miss% 0.02150733289858223
plot_id,batch_id 0 52 miss% 0.020327801965154733
plot_id,batch_id 0 53 miss% 0.016814245114596768
plot_id,batch_id 0 54 miss% 0.024531843121583033
plot_id,batch_id 0 55 miss% 0.03259057359072967
plot_id,batch_id 0 56 miss% 0.024208592077563675
plot_id,batch_id 0 57 miss% 0.017688309482537336
plot_id,batch_id 0 58 miss% 0.01857629556615427
plot_id,batch_id 0 59 miss% 0.0193601651378125
plot_id,batch_id 0 60 miss% 0.037257982960571874
plot_id,batch_id 0 61 miss% 0.024231219669454022
plot_id,batch_id 0 62 miss% 0.02066690248999868
plot_id,batch_id 0 63 miss% 0.030573313620029366
plot_id,batch_id 0 64 miss% 0.03260424292633935
plot_id,batch_id 0 65 miss% 0.03512310882608496
plot_id,batch_id 0 66 miss% 0.0898730609191175
plot_id,batch_id 0 67 miss% 0.030609942804991113
plot_id,batch_id 0 68 miss% 0.03472681511231632
plot_id,batch_id 0 69 miss% 0.03678966726575764
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  61841
Epoch:0, Train loss:0.572246, valid loss:0.569650
Epoch:1, Train loss:0.281206, valid loss:0.006560
Epoch:2, Train loss:0.008470, valid loss:0.003931
Epoch:3, Train loss:0.006433, valid loss:0.003552
Epoch:4, Train loss:0.005423, valid loss:0.003153
Epoch:5, Train loss:0.004647, valid loss:0.002161
Epoch:6, Train loss:0.004123, valid loss:0.002405
Epoch:7, Train loss:0.003815, valid loss:0.002098
Epoch:8, Train loss:0.003402, valid loss:0.002801
Epoch:9, Train loss:0.003321, valid loss:0.001858
Epoch:10, Train loss:0.003126, valid loss:0.001875
Epoch:11, Train loss:0.002344, valid loss:0.001437
Epoch:12, Train loss:0.002312, valid loss:0.001266
Epoch:13, Train loss:0.002268, valid loss:0.001314
Epoch:14, Train loss:0.002222, valid loss:0.001204
Epoch:15, Train loss:0.002147, valid loss:0.001594
Epoch:16, Train loss:0.002104, valid loss:0.001277
Epoch:17, Train loss:0.002080, valid loss:0.001337
Epoch:18, Train loss:0.002019, valid loss:0.001148
Epoch:19, Train loss:0.001986, valid loss:0.001196
Epoch:20, Train loss:0.001985, valid loss:0.001515
Epoch:21, Train loss:0.001552, valid loss:0.000879
Epoch:22, Train loss:0.001548, valid loss:0.001395
Epoch:23, Train loss:0.001570, valid loss:0.000975
Epoch:24, Train loss:0.001485, valid loss:0.000877
Epoch:25, Train loss:0.001498, valid loss:0.000878
Epoch:26, Train loss:0.001490, valid loss:0.000923
Epoch:27, Train loss:0.001472, valid loss:0.001004
Epoch:28, Train loss:0.001438, valid loss:0.000953
Epoch:29, Train loss:0.001479, valid loss:0.001048
Epoch:30, Train loss:0.001514, valid loss:0.000976
Epoch:31, Train loss:0.001219, valid loss:0.000814
Epoch:32, Train loss:0.001224, valid loss:0.000884
Epoch:33, Train loss:0.001219, valid loss:0.000840
Epoch:34, Train loss:0.001216, valid loss:0.000845
Epoch:35, Train loss:0.001185, valid loss:0.000871
Epoch:36, Train loss:0.001196, valid loss:0.000822
Epoch:37, Train loss:0.001202, valid loss:0.000859
Epoch:38, Train loss:0.001172, valid loss:0.000835
Epoch:39, Train loss:0.001194, valid loss:0.000837
Epoch:40, Train loss:0.001155, valid loss:0.000816
Epoch:41, Train loss:0.001074, valid loss:0.000790
Epoch:42, Train loss:0.001064, valid loss:0.000771
Epoch:43, Train loss:0.001057, valid loss:0.000813
Epoch:44, Train loss:0.001062, valid loss:0.000766
Epoch:45, Train loss:0.001054, valid loss:0.000768
Epoch:46, Train loss:0.001047, valid loss:0.000769
Epoch:47, Train loss:0.001050, valid loss:0.000777
Epoch:48, Train loss:0.001051, valid loss:0.000797
Epoch:49, Train loss:0.001040, valid loss:0.000776
Epoch:50, Train loss:0.001030, valid loss:0.000788
Epoch:51, Train loss:0.000988, valid loss:0.000746
Epoch:52, Train loss:0.000987, valid loss:0.000745
Epoch:53, Train loss:0.000982, valid loss:0.000757
Epoch:54, Train loss:0.000981, valid loss:0.000751
Epoch:55, Train loss:0.000982, valid loss:0.000764
Epoch:56, Train loss:0.000983, valid loss:0.000749
Epoch:57, Train loss:0.000977, valid loss:0.000770
Epoch:58, Train loss:0.000974, valid loss:0.000728
Epoch:59, Train loss:0.000978, valid loss:0.000741
Epoch:60, Train loss:0.000974, valid loss:0.000756
training time 6794.99788069725
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.026844766847095945
plot_id,batch_id 0 1 miss% 0.02924220201331745
plot_id,batch_id 0 2 miss% 0.019921299689149248
plot_id,batch_id 0 3 miss% 0.034837906454640015
plot_id,batch_id 0 4 miss% 0.017691208753927698
plot_id,batch_id 0 5 miss% 0.03500381293978896
plot_id,batch_id 0 6 miss% 0.025645213096301585
plot_id,batch_id 0 7 miss% 0.02999615413588914
plot_id,batch_id 0 8 miss% 0.017136464744263776
plot_id,batch_id 0 9 miss% 0.02756287024104088
plot_id,batch_id 0 10 miss% 0.056077456740497786
plot_id,batch_id 0 11 miss% 0.038207137814111974
plot_id,batch_id 0 12 miss% 0.02862315820893788
plot_id,batch_id 0 13 miss% 0.028922287271155762
plot_id,batch_id 0 14 miss% 0.03330317775326634
plot_id,batch_id 0 15 miss% 0.03909597404630117
plot_id,batch_id 0 16 miss% 0.023981939199782015
plot_id,batch_id 0 17 miss% 0.03249445678082665
plot_id,batch_id 0 18 miss% 0.03221718080346369
plot_id,batch_id 0 19 miss% 0.03813198214417546
plot_id,batch_id 0 20 miss% 0.05572657329756783
plot_id,batch_id 0 21 miss% 0.022197434927285646
plot_id,batch_id 0 22 miss% 0.024407462075287118
plot_id,batch_id 0 23 miss% 0.028881094511157085
plot_id,batch_id 0 24 miss% 0.025101895835792528
plot_id,batch_id 0 25 miss% 0.03757267212717401
plot_id,batch_id 0 26 miss% 0.0264706976035744
plot_id,batch_id 0 27 miss% 0.023679741277852943
plot_id,batch_id 0 28 miss% 0.02815860423184099
plot_id,batch_id 0 29 miss% 0.028916819524977398
plot_id,batch_id 0 30 miss% 0.037752124017015096
plot_id,batch_id 0 31 miss% 0.0330495392176617
plot_id,batch_id 0 32 miss% 0.027192411713631993
plot_id,batch_id 0 33 miss% 0.03298041044442878
plot_id,batch_id 0 34 miss% 0.022859646135457268
plot_id,batch_id 0 35 miss% 0.031973262021130384
plot_id,batch_id 0 36 miss% 0.046024838234703346
plot_id,batch_id 0 37 miss% 0.03389036372699671
plot_id,batch_id 0 38 miss% 0.018735051364028758
plot_id,batch_id 0 39 miss% 0.022615228815458688
plot_id,batch_id 0 40 miss% 0.0920000008980075
plot_id,batch_id 0 41 miss% 0.020414577626112138
plot_id,batch_id 0 42 miss% 0.017223083073964952
plot_id,batch_id 0 43 miss% 0.024494308298926744
plot_id,batch_id 0 44 miss% 0.026508329893334057
plot_id,batch_id 0 45 miss% 0.02002639347323228
plot_id,batch_id 0 46 miss% 0.024546203656985153
plot_id,batch_id 0 47 miss% 0.03504946544684639
plot_id,batch_id 0 48 miss% 0.03685413598772032
plot_id,batch_id 0 49 miss% 0.02535075281399224
plot_id,batch_id 0 50 miss% 0.03196833571910424
plot_id,batch_id 0 51 miss% 0.03349563675640023
plot_id,batch_id 0 52 miss% 0.031317133648043755
plot_id,batch_id 0 53 miss% 0.01467202977093543
plot_id,batch_id 0 54 miss% 0.028447280165120672
plot_id,batch_id 0 55 miss% 0.04550229579049212
plot_id,batch_id 0 56 miss% 0.029171925732571165
plot_id,batch_id 0 57 miss% 0.028793979073088227
plot_id,batch_id 0 58 miss% 0.024471920384482967
plot_id,batch_id 0 59 miss% 0.02993478621619107
plot_id,batch_id 0 60 miss% 0.02319635293471587
plot_id,batch_id 0 61 miss% 0.027187552357593625
plot_id,batch_id 0 62 miss% 0.02794596952559183
plot_id,batch_id 0 63 miss% 0.0355786426738934
plot_id,batch_id 0 64 miss% 0.05331875182195225
plot_id,batch_id 0 65 miss% 0.0367647091300682
plot_id,batch_id 0 66 miss% 0.038045438302738266
plot_id,batch_id 0 67 miss% 0.0299223037502953
plot_id,batch_id 0 68 miss% 0.033893503568365534
plot_id,batch_id 0 69 miss% 0.0198345855658791
plot_id,batch_id 0 70 miss% 0.05177596566031831
plot_id,batch_id 0 71 miss% 0.03592482501318571
plot_id,batch_id 0 72 miss% 0.032757575943504626
plot_id,batch_id 0 73 miss% 0.02413703743656252
plot_id,batch_id 0 74 miss% 0.03195445440852992
plot_id,batch_id 0 75 miss% 0.060247392461205355
plot_id,batch_id 0 76 miss% 0.04123047177156053
plot_id,batch_id 0 77 miss% 0.029104389924014935
plot_id,batch_id 0 78 miss% 0.04869416424893477
plot_id,batch_id 0 79 miss% 0.045757107974167203
plot_id,batch_id 0 80 miss% 0.03989631149064358
plot_id,batch_id 0 81 miss% 0.028549780752240853
plot_id,batch_id 0 82 miss% 0.02436474584485786
plot_id,batch_id 0 83 miss% 0.02995113072325067
plot_id,batch_id 0 84 miss% 0.031751846653505354
plot_id,batch_id 0 85 miss% 0.029218423781241076
plot_id,batch_id 0 86 miss% 0.03884718692304157
plot_id,batch_id 0 87 miss% 0.025182744199863268
plot_id,batch_id 0 88 miss% 0.039610975916035907
plot_id,batch_id 0 89 miss% 0.02795929395431311
plot_id,batch_id 0 90 miss% 0.03599394637587614
plot_id,batch_id 0 91 miss% 0.033244859171839565
plot_id,batch_id 0 92 miss% 0.027940751087621493
plot_id,batch_id 0 93 miss% 0.02402615124236404
plot_id,batch_id 0 94 miss% 0.034956667987160905
plot_id,batch_id 0 95 miss% 0.04527516314807441
plot_id,batch_id 0 96 miss% 0.028623225679824834
plot_id,batch_id 0 97 miss% 0.04812845036335416
plot_id,batch_id 0 98 miss% 0.03486298072170847
plot_id,batch_id 0 99 miss% 0.02530617406015986
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02684477 0.0292422  0.0199213  0.03483791 0.01769121 0.03500381
 0.02564521 0.02999615 0.01713646 0.02756287 0.05607746 0.03820714
 0.02862316 0.02892229 0.03330318 0.03909597 0.02398194 0.03249446
 0.03221718 0.03813198 0.05572657 0.02219743 0.02440746 0.02888109
 0.0251019  0.03757267 0.0264707  0.02367974 0.0281586  0.02891682
 0.03775212 0.03304954 0.02719241 0.03298041 0.02285965 0.03197326
 0.04602484 0.03389036 0.01873505 0.02261523 0.092      0.02041458
 0.01722308 0.02449431 0.02650833 0.02002639 0.0245462  0.03504947
 0.03685414 0.02535075 0.03196834 0.03349564 0.03131713 0.01467203
 0.02844728 0.0455023  0.02917193 0.02879398 0.02447192 0.02993479
 0.02319635 0.02718755 0.02794597 0.03557864 0.05331875 0.03676471
 0.03804544 0.0299223  0.0338935  0.01983459 0.05177597 0.03592483
 0.03275758 0.02413704 0.03195445 0.06024739 0.04123047 0.02910439
 0.04869416 0.04575711 0.03989631 0.02854978 0.02436475 0.02995113
 0.03175185 0.02921842 0.03884719 0.02518274 0.03961098 0.02795929
 0.03599395 0.03324486 0.02794075 0.02402615 0.03495667 0.04527516
 0.02862323 0.04812845 0.03486298 0.02530617]
for model  94 the mean error 0.032243270997565625
all id 94 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 25 out win 4 err 0.032243270997565625
Launcher: Job 95 completed in 6994 seconds.
Launcher: Task 40 done. Exiting.
plot_id,batch_id 0 70 miss% 0.034772733537113396
plot_id,batch_id 0 71 miss% 0.03778695144180533
plot_id,batch_id 0 72 miss% 0.029569122726168403
plot_id,batch_id 0 73 miss% 0.042350101312017975
plot_id,batch_id 0 74 miss% 0.04294480577982419
plot_id,batch_id 0 75 miss% 0.06354298169657893
plot_id,batch_id 0 76 miss% 0.05399944876890464
plot_id,batch_id 0 77 miss% 0.034335362217911024
plot_id,batch_id 0 78 miss% 0.03737184551104475
plot_id,batch_id 0 79 miss% 0.036397093762020084
plot_id,batch_id 0 80 miss% 0.07226484427597969
plot_id,batch_id 0 81 miss% 0.027623719994555533
plot_id,batch_id 0 82 miss% 0.03159646639970042
plot_id,batch_id 0 83 miss% 0.03476100621634258
plot_id,batch_id 0 84 miss% 0.025830252847373968
plot_id,batch_id 0 85 miss% 0.05652140366399523
plot_id,batch_id 0 86 miss% 0.05285105976823898
plot_id,batch_id 0 87 miss% 0.044317855568739274
plot_id,batch_id 0 88 miss% 0.05485875188508293
plot_id,batch_id 0 89 miss% 0.028219991203653615
plot_id,batch_id 0 90 miss% 0.037787701951662936
plot_id,batch_id 0 91 miss% 0.022558446054770618
plot_id,batch_id 0 92 miss% 0.04710081173577102
plot_id,batch_id 0 93 miss% 0.03930145919671768
plot_id,batch_id 0 94 miss% 0.03270642316983424
plot_id,batch_id 0 95 miss% 0.04952212492103436
plot_id,batch_id 0 96 miss% 0.04737257237798849
plot_id,batch_id 0 97 miss% 0.05133023295995722
plot_id,batch_id 0 98 miss% 0.025356319283976865
plot_id,batch_id 0 99 miss% 0.031160109033953533
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0331558  0.02712182 0.02727843 0.02490358 0.02479495 0.0343483
 0.03059682 0.03447303 0.03748621 0.02028771 0.04570558 0.03050957
 0.02117235 0.03349779 0.02486906 0.04454211 0.03390921 0.04345006
 0.03387095 0.03710455 0.09872926 0.02220737 0.02246407 0.02297417
 0.02886347 0.02860449 0.02309181 0.02845332 0.0211611  0.02424788
 0.04174193 0.04871304 0.03738198 0.02347149 0.02319935 0.03480176
 0.0565695  0.02427741 0.02644615 0.02054293 0.06997125 0.02490822
 0.02316344 0.02395957 0.02220722 0.03271846 0.02361498 0.02186747
 0.01732466 0.0218219  0.03234268 0.02150733 0.0203278  0.01681425
 0.02453184 0.03259057 0.02420859 0.01768831 0.0185763  0.01936017
 0.03725798 0.02423122 0.0206669  0.03057331 0.03260424 0.03512311
 0.08987306 0.03060994 0.03472682 0.03678967 0.03477273 0.03778695
 0.02956912 0.0423501  0.04294481 0.06354298 0.05399945 0.03433536
 0.03737185 0.03639709 0.07226484 0.02762372 0.03159647 0.03476101
 0.02583025 0.0565214  0.05285106 0.04431786 0.05485875 0.02821999
 0.0377877  0.02255845 0.04710081 0.03930146 0.03270642 0.04952212
 0.04737257 0.05133023 0.02535632 0.03116011]
for model  62 the mean error 0.03409093611268844
all id 62 hidden_dim 32 learning_rate 0.02 num_layers 3 frames 21 out win 5 err 0.03409093611268844
Launcher: Job 63 completed in 6997 seconds.
Launcher: Task 23 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  77489
Epoch:0, Train loss:0.507052, valid loss:0.511720
Epoch:1, Train loss:0.040706, valid loss:0.009963
Epoch:2, Train loss:0.011104, valid loss:0.004209
Epoch:3, Train loss:0.006358, valid loss:0.003035
Epoch:4, Train loss:0.004893, valid loss:0.002198
Epoch:5, Train loss:0.004085, valid loss:0.002245
Epoch:6, Train loss:0.003694, valid loss:0.001838
Epoch:7, Train loss:0.003300, valid loss:0.001932
Epoch:8, Train loss:0.003197, valid loss:0.001486
Epoch:9, Train loss:0.003070, valid loss:0.001404
Epoch:10, Train loss:0.002899, valid loss:0.001727
Epoch:11, Train loss:0.001989, valid loss:0.001185
Epoch:12, Train loss:0.002051, valid loss:0.001171
Epoch:13, Train loss:0.002004, valid loss:0.000998
Epoch:14, Train loss:0.001954, valid loss:0.001236
Epoch:15, Train loss:0.001868, valid loss:0.001129
Epoch:16, Train loss:0.001888, valid loss:0.001110
Epoch:17, Train loss:0.001878, valid loss:0.001067
Epoch:18, Train loss:0.001794, valid loss:0.001137
Epoch:19, Train loss:0.001852, valid loss:0.001111
Epoch:20, Train loss:0.001712, valid loss:0.000969
Epoch:21, Train loss:0.001292, valid loss:0.000814
Epoch:22, Train loss:0.001296, valid loss:0.000761
Epoch:23, Train loss:0.001289, valid loss:0.000921
Epoch:24, Train loss:0.001261, valid loss:0.000853
Epoch:25, Train loss:0.001271, valid loss:0.000812
Epoch:26, Train loss:0.001226, valid loss:0.000822
Epoch:27, Train loss:0.001215, valid loss:0.000931
Epoch:28, Train loss:0.001222, valid loss:0.000706
Epoch:29, Train loss:0.001210, valid loss:0.000788
Epoch:30, Train loss:0.001168, valid loss:0.000778
Epoch:31, Train loss:0.000951, valid loss:0.000644
Epoch:32, Train loss:0.000957, valid loss:0.000719
Epoch:33, Train loss:0.000936, valid loss:0.000653
Epoch:34, Train loss:0.000960, valid loss:0.000748
Epoch:35, Train loss:0.000927, valid loss:0.000717
Epoch:36, Train loss:0.000944, valid loss:0.000647
Epoch:37, Train loss:0.000923, valid loss:0.000663
Epoch:38, Train loss:0.000919, valid loss:0.000746
Epoch:39, Train loss:0.000921, valid loss:0.000661
Epoch:40, Train loss:0.000925, valid loss:0.000672
Epoch:41, Train loss:0.000813, valid loss:0.000610
Epoch:42, Train loss:0.000807, valid loss:0.000598
Epoch:43, Train loss:0.000821, valid loss:0.000605
Epoch:44, Train loss:0.000810, valid loss:0.000650
Epoch:45, Train loss:0.000796, valid loss:0.000604
Epoch:46, Train loss:0.000787, valid loss:0.000601
Epoch:47, Train loss:0.000798, valid loss:0.000626
Epoch:48, Train loss:0.000810, valid loss:0.000577
Epoch:49, Train loss:0.000790, valid loss:0.000692
Epoch:50, Train loss:0.000787, valid loss:0.000607
Epoch:51, Train loss:0.000738, valid loss:0.000582
Epoch:52, Train loss:0.000738, valid loss:0.000602
Epoch:53, Train loss:0.000739, valid loss:0.000601
Epoch:54, Train loss:0.000733, valid loss:0.000584
Epoch:55, Train loss:0.000732, valid loss:0.000575
Epoch:56, Train loss:0.000727, valid loss:0.000579
Epoch:57, Train loss:0.000730, valid loss:0.000584
Epoch:58, Train loss:0.000727, valid loss:0.000577
Epoch:59, Train loss:0.000727, valid loss:0.000586
Epoch:60, Train loss:0.000726, valid loss:0.000592
training time 6809.025787353516
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.04137109152172387
plot_id,batch_id 0 1 miss% 0.026073654308431417
plot_id,batch_id 0 2 miss% 0.029813661035486633
plot_id,batch_id 0 3 miss% 0.024882391302167035
plot_id,batch_id 0 4 miss% 0.03553934866232363
plot_id,batch_id 0 5 miss% 0.03479719228771025
plot_id,batch_id 0 6 miss% 0.0326464782346669
plot_id,batch_id 0 7 miss% 0.036358585243457336
plot_id,batch_id 0 8 miss% 0.03173231865370244
plot_id,batch_id 0 9 miss% 0.0213713707038922
plot_id,batch_id 0 10 miss% 0.04723693628131402
plot_id,batch_id 0 11 miss% 0.04434100924395843
plot_id,batch_id 0 12 miss% 0.034877506807318145
plot_id,batch_id 0 13 miss% 0.026108614264552216
plot_id,batch_id 0 14 miss% 0.03129460040042511
plot_id,batch_id 0 15 miss% 0.03935350825254727
plot_id,batch_id 0 16 miss% 0.0414594264068189
plot_id,batch_id 0 17 miss% 0.04353862133855749
plot_id,batch_id 0 18 miss% 0.038076913466099614
plot_id,batch_id 0 19 miss% 0.025413361828524723
plot_id,batch_id 0 20 miss% 0.03735701001818166
plot_id,batch_id 0 21 miss% 0.03335795461272118
plot_id,batch_id 0 22 miss% 0.017923893495708602
plot_id,batch_id 0 23 miss% 0.01922847349502674
plot_id,batch_id 0 24 miss% 0.020118199931810375
plot_id,batch_id 0 25 miss% 0.030595889042687445
plot_id,batch_id 0 26 miss% 0.028614136064583528
plot_id,batch_id 0 27 miss% 0.03173918463054427
plot_id,batch_id 0 28 miss% 0.023453615036606836
plot_id,batch_id 0 29 miss% 0.022571868306841134
plot_id,batch_id 0 30 miss% 0.03775711729704626
plot_id,batch_id 0 31 miss% 0.032504523745341664
plot_id,batch_id 0 32 miss% 0.0223081751566518
plot_id,batch_id 0 33 miss% 0.03436063410659867
plot_id,batch_id 0 34 miss% 0.03382099839494613
plot_id,batch_id 0 35 miss% 0.04704507374779518
plot_id,batch_id 0 36 miss% 0.039616256682986954
plot_id,batch_id 0 37 miss% 0.03310534591881123
plot_id,batch_id 0 38 miss% 0.02115355584076996
plot_id,batch_id 0 39 miss% 0.02241448185666615
plot_id,batch_id 0 40 miss% 0.05559466797023064
plot_id,batch_id 0 41 miss% 0.024739280435101946
plot_id,batch_id 0 42 miss% 0.018444908227070945
plot_id,batch_id 0 43 miss% 0.031676910037219966
plot_id,batch_id 0 44 miss% 0.023552515186358466
plot_id,batch_id 0 45 miss% 0.04194874202266601
plot_id,batch_id 0 46 miss% 0.027553421629588678
plot_id,batch_id 0 47 miss% 0.024497960506754977
plot_id,batch_id 0 48 miss% 0.028254202603381073
plot_id,batch_id 0 49 miss% 0.020550205931173834
plot_id,batch_id 0 50 miss% 0.030089194750858184
plot_id,batch_id 0 51 miss% 0.02683367871576144
plot_id,batch_id 0 52 miss% 0.020175243972555715
plot_id,batch_id 0 53 miss% 0.017562285615456332
plot_id,batch_id 0 54 miss% 0.0310703230990674
plot_id,batch_id 0 55 miss% 0.02155795932426212
plot_id,batch_id 0 56 miss% 0.021850149651802014
plot_id,batch_id 0 57 miss% 0.01576012925700665
plot_id,batch_id 0 58 miss% 0.02326734167833645
plot_id,batch_id 0 59 miss% 0.028377864591177537
plot_id,batch_id 0 60 miss% 0.05732886700364237
plot_id,batch_id 0 61 miss% 0.04530749030437893
plot_id,batch_id 0 62 miss% 0.025649569181248784
plot_id,batch_id 0 63 miss% 0.04063173626069601
plot_id,batch_id 0 64 miss% 0.04384838263206284
plot_id,batch_id 0 65 miss% 0.05263269462718778
plot_id,batch_id 0 66 miss% 0.04556294794593695
plot_id,batch_id 0 67 miss% 0.031118667315813368
plot_id,batch_id 0 68 miss% 0.03463826949215567
plot_id,batch_id 0 69 miss% the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  61841
Epoch:0, Train loss:0.734007, valid loss:0.732855
Epoch:1, Train loss:0.260336, valid loss:0.009823
Epoch:2, Train loss:0.016421, valid loss:0.005969
Epoch:3, Train loss:0.011599, valid loss:0.006078
Epoch:4, Train loss:0.010352, valid loss:0.004993
Epoch:5, Train loss:0.008679, valid loss:0.004495
Epoch:6, Train loss:0.007799, valid loss:0.004862
Epoch:7, Train loss:0.007058, valid loss:0.004347
Epoch:8, Train loss:0.006798, valid loss:0.004136
Epoch:9, Train loss:0.006340, valid loss:0.003990
Epoch:10, Train loss:0.005959, valid loss:0.003168
Epoch:11, Train loss:0.004165, valid loss:0.002604
Epoch:12, Train loss:0.004081, valid loss:0.002166
Epoch:13, Train loss:0.004092, valid loss:0.002994
Epoch:14, Train loss:0.003984, valid loss:0.002237
Epoch:15, Train loss:0.003845, valid loss:0.001953
Epoch:16, Train loss:0.003919, valid loss:0.002127
Epoch:17, Train loss:0.003576, valid loss:0.002327
Epoch:18, Train loss:0.003553, valid loss:0.002012
Epoch:19, Train loss:0.003502, valid loss:0.002221
Epoch:20, Train loss:0.003384, valid loss:0.001821
Epoch:21, Train loss:0.002493, valid loss:0.001609
Epoch:22, Train loss:0.002508, valid loss:0.001807
Epoch:23, Train loss:0.002473, valid loss:0.001572
Epoch:24, Train loss:0.002473, valid loss:0.001708
Epoch:25, Train loss:0.002408, valid loss:0.001447
Epoch:26, Train loss:0.002387, valid loss:0.001896
Epoch:27, Train loss:0.002414, valid loss:0.001705
Epoch:28, Train loss:0.002665, valid loss:0.001649
Epoch:29, Train loss:0.002264, valid loss:0.001754
Epoch:30, Train loss:0.002340, valid loss:0.003245
Epoch:31, Train loss:0.001939, valid loss:0.001495
Epoch:32, Train loss:0.001833, valid loss:0.001473
Epoch:33, Train loss:0.001832, valid loss:0.001231
Epoch:34, Train loss:0.001833, valid loss:0.001309
Epoch:35, Train loss:0.001823, valid loss:0.001327
Epoch:36, Train loss:0.001829, valid loss:0.001860
Epoch:37, Train loss:0.001807, valid loss:0.001256
Epoch:38, Train loss:0.001781, valid loss:0.001375
Epoch:39, Train loss:0.001762, valid loss:0.001271
Epoch:40, Train loss:0.001743, valid loss:0.001221
Epoch:41, Train loss:0.001549, valid loss:0.001306
Epoch:42, Train loss:0.001552, valid loss:0.001229
Epoch:43, Train loss:0.001510, valid loss:0.001171
Epoch:44, Train loss:0.001516, valid loss:0.001241
Epoch:45, Train loss:0.001526, valid loss:0.001246
Epoch:46, Train loss:0.001517, valid loss:0.001254
Epoch:47, Train loss:0.001506, valid loss:0.001164
Epoch:48, Train loss:0.001502, valid loss:0.001358
Epoch:49, Train loss:0.001498, valid loss:0.001220
Epoch:50, Train loss:0.001489, valid loss:0.001240
Epoch:51, Train loss:0.001383, valid loss:0.001211
Epoch:52, Train loss:0.001381, valid loss:0.001154
Epoch:53, Train loss:0.001372, valid loss:0.001176
Epoch:54, Train loss:0.001373, valid loss:0.001182
Epoch:55, Train loss:0.001368, valid loss:0.001156
Epoch:56, Train loss:0.001367, valid loss:0.001173
Epoch:57, Train loss:0.001358, valid loss:0.001155
Epoch:58, Train loss:0.001365, valid loss:0.001200
Epoch:59, Train loss:0.001362, valid loss:0.001155
Epoch:60, Train loss:0.001350, valid loss:0.001180
training time 6803.318641901016
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03939010104498251
plot_id,batch_id 0 1 miss% 0.026680348956589797
plot_id,batch_id 0 2 miss% 0.024170884347909965
plot_id,batch_id 0 3 miss% 0.027157470938282677
plot_id,batch_id 0 4 miss% 0.02811732040622377
plot_id,batch_id 0 5 miss% 0.05899862906229539
plot_id,batch_id 0 6 miss% 0.025755994950458654
plot_id,batch_id 0 7 miss% 0.03288930302707314
plot_id,batch_id 0 8 miss% 0.023650456151656587
plot_id,batch_id 0 9 miss% 0.021809268757764358
plot_id,batch_id 0 10 miss% 0.042564922917313466
plot_id,batch_id 0 11 miss% 0.044636166175589465
plot_id,batch_id 0 12 miss% 0.0325658395447809
plot_id,batch_id 0 13 miss% 0.028713470359385256
plot_id,batch_id 0 14 miss% 0.03508721040270556
plot_id,batch_id 0 15 miss% 0.04053526065780153
plot_id,batch_id 0 16 miss% 0.020916076403134844
plot_id,batch_id 0 17 miss% 0.035147680862681434
plot_id,batch_id 0 18 miss% 0.03363442815718203
plot_id,batch_id 0 19 miss% 0.037673907297344746
plot_id,batch_id 0 20 miss% 0.04276644905731648
plot_id,batch_id 0 21 miss% 0.031496230289033256
plot_id,batch_id 0 22 miss% 0.03476601837452445
plot_id,batch_id 0 23 miss% 0.031255074369142925
plot_id,batch_id 0 24 miss% 0.03529689017469245
plot_id,batch_id 0 25 miss% 0.03763104300701532
plot_id,batch_id 0 26 miss% 0.026218229407193747
plot_id,batch_id 0 27 miss% 0.02429476673974766
plot_id,batch_id 0 28 miss% 0.023159221063112523
plot_id,batch_id 0 29 miss% 0.024728226415761875
plot_id,batch_id 0 30 miss% 0.04561543865874604
plot_id,batch_id 0 31 miss% 0.0318056835482087
plot_id,batch_id 0 32 miss% 0.0350093258056391
plot_id,batch_id 0 33 miss% 0.02611578541864861
plot_id,batch_id 0 34 miss% 0.033811209057928025
plot_id,batch_id 0 35 miss% 0.031023184218586602
plot_id,batch_id 0 36 miss% 0.04182730617728919
plot_id,batch_id 0 37 miss% 0.030534800356550672
plot_id,batch_id 0 38 miss% 0.028711580103260602
plot_id,batch_id 0 39 miss% 0.018387578267131607
plot_id,batch_id 0 40 miss% 0.05744127197484029
plot_id,batch_id 0 41 miss% 0.017672315811667293
plot_id,batch_id 0 42 miss% 0.015612261183028135
plot_id,batch_id 0 43 miss% 0.03796332272146818
plot_id,batch_id 0 44 miss% 0.016711939807325396
plot_id,batch_id 0 45 miss% 0.02284512203682883
plot_id,batch_id 0 46 miss% 0.020696586632048625
plot_id,batch_id 0 47 miss% 0.021408040591562306
plot_id,batch_id 0 48 miss% 0.01924496659467155
plot_id,batch_id 0 49 miss% 0.022904921024375913
plot_id,batch_id 0 50 miss% 0.02852545903809212
plot_id,batch_id 0 51 miss% 0.030011162698052166
plot_id,batch_id 0 52 miss% 0.02293198672366279
plot_id,batch_id 0 53 miss% 0.014863323663825772
plot_id,batch_id 0 54 miss% 0.02472708501051636
plot_id,batch_id 0 55 miss% 0.03414330008124339
plot_id,batch_id 0 56 miss% 0.03220540443162649
plot_id,batch_id 0 57 miss% 0.028513879640509766
plot_id,batch_id 0 58 miss% 0.020840269101975898
plot_id,batch_id 0 59 miss% 0.024167625876783002
plot_id,batch_id 0 60 miss% 0.04236492133647026
plot_id,batch_id 0 61 miss% 0.031612051482423104
plot_id,batch_id 0 62 miss% 0.02379374624061178
plot_id,batch_id 0 63 miss% 0.029255157501672416
plot_id,batch_id 0 64 miss% 0.036182347452096426
plot_id,batch_id 0 65 miss% 0.04658227722791297
plot_id,batch_id 0 66 miss% 0.044973820758348194
plot_id,batch_id 0 67 miss% 0.030380361950420307
plot_id,batch_id 0 68 miss% 0.0468410024972844
plot_id,batch_id 0 69 miss% 0.029886966239873445
0.014294744916586555
plot_id,batch_id 0 70 miss% 0.057897905679093684
plot_id,batch_id 0 71 miss% 0.07098657132209486
plot_id,batch_id 0 72 miss% 0.03693066566637584
plot_id,batch_id 0 73 miss% 0.03752282099107095
plot_id,batch_id 0 74 miss% 0.031015589592580443
plot_id,batch_id 0 75 miss% 0.06300411423593534
plot_id,batch_id 0 76 miss% 0.04024685835640853
plot_id,batch_id 0 77 miss% 0.03790316445635286
plot_id,batch_id 0 78 miss% 0.05206319204872784
plot_id,batch_id 0 79 miss% 0.04800581119897204
plot_id,batch_id 0 80 miss% 0.05160292681234465
plot_id,batch_id 0 81 miss% 0.023757853061811607
plot_id,batch_id 0 82 miss% 0.03557769314667625
plot_id,batch_id 0 83 miss% 0.03568901980319927
plot_id,batch_id 0 84 miss% 0.02086274226316303
plot_id,batch_id 0 85 miss% 0.04635418656564814
plot_id,batch_id 0 86 miss% 0.030660142360582312
plot_id,batch_id 0 87 miss% 0.031211777342266267
plot_id,batch_id 0 88 miss% 0.030986562287989915
plot_id,batch_id 0 89 miss% 0.02839459529773819
plot_id,batch_id 0 90 miss% 0.0607257184151743
plot_id,batch_id 0 91 miss% 0.03890739685899587
plot_id,batch_id 0 92 miss% 0.02510218160551266
plot_id,batch_id 0 93 miss% 0.036483073213058895
plot_id,batch_id 0 94 miss% 0.031759447577277264
plot_id,batch_id 0 95 miss% 0.04965232892005905
plot_id,batch_id 0 96 miss% 0.04083894672175232
plot_id,batch_id 0 97 miss% 0.03924425937895292
plot_id,batch_id 0 98 miss% 0.03464227632767647
plot_id,batch_id 0 99 miss% 0.027441651650024095
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04137109 0.02607365 0.02981366 0.02488239 0.03553935 0.03479719
 0.03264648 0.03635859 0.03173232 0.02137137 0.04723694 0.04434101
 0.03487751 0.02610861 0.0312946  0.03935351 0.04145943 0.04353862
 0.03807691 0.02541336 0.03735701 0.03335795 0.01792389 0.01922847
 0.0201182  0.03059589 0.02861414 0.03173918 0.02345362 0.02257187
 0.03775712 0.03250452 0.02230818 0.03436063 0.033821   0.04704507
 0.03961626 0.03310535 0.02115356 0.02241448 0.05559467 0.02473928
 0.01844491 0.03167691 0.02355252 0.04194874 0.02755342 0.02449796
 0.0282542  0.02055021 0.03008919 0.02683368 0.02017524 0.01756229
 0.03107032 0.02155796 0.02185015 0.01576013 0.02326734 0.02837786
 0.05732887 0.04530749 0.02564957 0.04063174 0.04384838 0.05263269
 0.04556295 0.03111867 0.03463827 0.01429474 0.05789791 0.07098657
 0.03693067 0.03752282 0.03101559 0.06300411 0.04024686 0.03790316
 0.05206319 0.04800581 0.05160293 0.02375785 0.03557769 0.03568902
 0.02086274 0.04635419 0.03066014 0.03121178 0.03098656 0.0283946
 0.06072572 0.0389074  0.02510218 0.03648307 0.03175945 0.04965233
 0.04083895 0.03924426 0.03464228 0.02744165]
for model  129 the mean error 0.03395174805669059
all id 129 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 25 out win 3 err 0.03395174805669059
Launcher: Job 130 completed in 7002 seconds.
Launcher: Task 65 done. Exiting.
plot_id,batch_id 0 70 miss% 0.03405767523968052
plot_id,batch_id 0 71 miss% 0.05697579267505877
plot_id,batch_id 0 72 miss% 0.05210506620909547
plot_id,batch_id 0 73 miss% 0.036895518173149816
plot_id,batch_id 0 74 miss% 0.03660142729131831
plot_id,batch_id 0 75 miss% 0.058345167375427574
plot_id,batch_id 0 76 miss% 0.044877354325760614
plot_id,batch_id 0 77 miss% 0.038728649981151435
plot_id,batch_id 0 78 miss% 0.035174514944021136
plot_id,batch_id 0 79 miss% 0.0628780578060181
plot_id,batch_id 0 80 miss% 0.05174249287018143
plot_id,batch_id 0 81 miss% 0.022980757711992605
plot_id,batch_id 0 82 miss% 0.021813603142973474
plot_id,batch_id 0 83 miss% 0.038800439624644585
plot_id,batch_id 0 84 miss% 0.02879796132037459
plot_id,batch_id 0 85 miss% 0.05711485163381397
plot_id,batch_id 0 86 miss% 0.02673708264066932
plot_id,batch_id 0 87 miss% 0.028989259359567303
plot_id,batch_id 0 88 miss% 0.03896575374496573
plot_id,batch_id 0 89 miss% 0.031014453528105928
plot_id,batch_id 0 90 miss% 0.0377306852536742
plot_id,batch_id 0 91 miss% 0.03865099183858284
plot_id,batch_id 0 92 miss% 0.04773431775398229
plot_id,batch_id 0 93 miss% 0.037463492116376375
plot_id,batch_id 0 94 miss% 0.04361771025910167
plot_id,batch_id 0 95 miss% 0.06521830828900897
plot_id,batch_id 0 96 miss% 0.03613257982925606
plot_id,batch_id 0 97 miss% 0.03824733404839811
plot_id,batch_id 0 98 miss% 0.03376404462532028
plot_id,batch_id 0 99 miss% 0.029468460415640857
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0393901  0.02668035 0.02417088 0.02715747 0.02811732 0.05899863
 0.02575599 0.0328893  0.02365046 0.02180927 0.04256492 0.04463617
 0.03256584 0.02871347 0.03508721 0.04053526 0.02091608 0.03514768
 0.03363443 0.03767391 0.04276645 0.03149623 0.03476602 0.03125507
 0.03529689 0.03763104 0.02621823 0.02429477 0.02315922 0.02472823
 0.04561544 0.03180568 0.03500933 0.02611579 0.03381121 0.03102318
 0.04182731 0.0305348  0.02871158 0.01838758 0.05744127 0.01767232
 0.01561226 0.03796332 0.01671194 0.02284512 0.02069659 0.02140804
 0.01924497 0.02290492 0.02852546 0.03001116 0.02293199 0.01486332
 0.02472709 0.0341433  0.0322054  0.02851388 0.02084027 0.02416763
 0.04236492 0.03161205 0.02379375 0.02925516 0.03618235 0.04658228
 0.04497382 0.03038036 0.046841   0.02988697 0.03405768 0.05697579
 0.05210507 0.03689552 0.03660143 0.05834517 0.04487735 0.03872865
 0.03517451 0.06287806 0.05174249 0.02298076 0.0218136  0.03880044
 0.02879796 0.05711485 0.02673708 0.02898926 0.03896575 0.03101445
 0.03773069 0.03865099 0.04773432 0.03746349 0.04361771 0.06521831
 0.03613258 0.03824733 0.03376404 0.02946846]
for model  67 the mean error 0.03381475482289216
all id 67 hidden_dim 24 learning_rate 0.02 num_layers 4 frames 21 out win 4 err 0.03381475482289216
Launcher: Job 68 completed in 7012 seconds.
Launcher: Task 166 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  35921
Epoch:0, Train loss:0.620031, valid loss:0.603796
Epoch:1, Train loss:0.041957, valid loss:0.009946
Epoch:2, Train loss:0.012165, valid loss:0.005202
Epoch:3, Train loss:0.008283, valid loss:0.003970
Epoch:4, Train loss:0.006796, valid loss:0.003061
Epoch:5, Train loss:0.005745, valid loss:0.003488
Epoch:6, Train loss:0.005616, valid loss:0.003063
Epoch:7, Train loss:0.005006, valid loss:0.003218
Epoch:8, Train loss:0.005030, valid loss:0.003009
Epoch:9, Train loss:0.004760, valid loss:0.002685
Epoch:10, Train loss:0.004616, valid loss:0.002746
Epoch:11, Train loss:0.003133, valid loss:0.001608
Epoch:12, Train loss:0.002943, valid loss:0.001663
Epoch:13, Train loss:0.003025, valid loss:0.002092
Epoch:14, Train loss:0.003041, valid loss:0.001688
Epoch:15, Train loss:0.002885, valid loss:0.001741
Epoch:16, Train loss:0.002914, valid loss:0.001548
Epoch:17, Train loss:0.002835, valid loss:0.001664
Epoch:18, Train loss:0.002781, valid loss:0.001805
Epoch:19, Train loss:0.002747, valid loss:0.001549
Epoch:20, Train loss:0.002761, valid loss:0.002327
Epoch:21, Train loss:0.002035, valid loss:0.001240
Epoch:22, Train loss:0.002054, valid loss:0.001186
Epoch:23, Train loss:0.002019, valid loss:0.001302
Epoch:24, Train loss:0.001983, valid loss:0.001112
Epoch:25, Train loss:0.002034, valid loss:0.001145
Epoch:26, Train loss:0.002068, valid loss:0.001173
Epoch:27, Train loss:0.001936, valid loss:0.001190
Epoch:28, Train loss:0.001968, valid loss:0.001236
Epoch:29, Train loss:0.001932, valid loss:0.001234
Epoch:30, Train loss:0.001956, valid loss:0.001120
Epoch:31, Train loss:0.001595, valid loss:0.000966
Epoch:32, Train loss:0.001608, valid loss:0.001089
Epoch:33, Train loss:0.001586, valid loss:0.001367
Epoch:34, Train loss:0.001575, valid loss:0.000982
Epoch:35, Train loss:0.001567, valid loss:0.000951
Epoch:36, Train loss:0.001537, valid loss:0.001024
Epoch:37, Train loss:0.001583, valid loss:0.000995
Epoch:38, Train loss:0.001535, valid loss:0.001095
Epoch:39, Train loss:0.001525, valid loss:0.000885
Epoch:40, Train loss:0.001535, valid loss:0.001000
Epoch:41, Train loss:0.001361, valid loss:0.001100
Epoch:42, Train loss:0.001351, valid loss:0.000920
Epoch:43, Train loss:0.001337, valid loss:0.001010
Epoch:44, Train loss:0.001351, valid loss:0.000869
Epoch:45, Train loss:0.001337, valid loss:0.000901
Epoch:46, Train loss:0.001351, valid loss:0.001028
Epoch:47, Train loss:0.001319, valid loss:0.000872
Epoch:48, Train loss:0.001329, valid loss:0.000829
Epoch:49, Train loss:0.001310, valid loss:0.001096
Epoch:50, Train loss:0.001321, valid loss:0.000911
Epoch:51, Train loss:0.001226, valid loss:0.000826
Epoch:52, Train loss:0.001222, valid loss:0.000823
Epoch:53, Train loss:0.001209, valid loss:0.000895
Epoch:54, Train loss:0.001217, valid loss:0.000856
Epoch:55, Train loss:0.001212, valid loss:0.000822
Epoch:56, Train loss:0.001212, valid loss:0.000896
Epoch:57, Train loss:0.001209, valid loss:0.000825
Epoch:58, Train loss:0.001212, valid loss:0.000917
Epoch:59, Train loss:0.001209, valid loss:0.000828
Epoch:60, Train loss:0.001195, valid loss:0.000880
training time 6829.741450548172
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.03587477540107298
plot_id,batch_id 0 1 miss% 0.028143008445401227
plot_id,batch_id 0 2 miss% 0.027984977836213613
plot_id,batch_id 0 3 miss% 0.027870838708176894
plot_id,batch_id 0 4 miss% 0.03747266694641883
plot_id,batch_id 0 5 miss% 0.0372191774903752
plot_id,batch_id 0 6 miss% 0.034562990306091154
plot_id,batch_id 0 7 miss% 0.02481859169608032
plot_id,batch_id 0 8 miss% 0.03558032245066524
plot_id,batch_id 0 9 miss% 0.024679763083239457
plot_id,batch_id 0 10 miss% 0.050894517365846485
plot_id,batch_id 0 11 miss% 0.04632910392480545
plot_id,batch_id 0 12 miss% 0.03201484443988747
plot_id,batch_id 0 13 miss% 0.025088264050355486
plot_id,batch_id 0 14 miss% 0.026628977281585118
plot_id,batch_id 0 15 miss% 0.03853150346923368
plot_id,batch_id 0 16 miss% 0.037883754450360504
plot_id,batch_id 0 17 miss% 0.037168633093700436
plot_id,batch_id 0 18 miss% 0.028173948010629186
plot_id,batch_id 0 19 miss% 0.03780707127306871
plot_id,batch_id 0 20 miss% 0.03898516718524891
plot_id,batch_id 0 21 miss% 0.015439160132902424
plot_id,batch_id 0 22 miss% 0.029293822617430795
plot_id,batch_id 0 23 miss% 0.026533328906304096
plot_id,batch_id 0 24 miss% 0.025844423811533252
plot_id,batch_id 0 25 miss% 0.048069295235319755
plot_id,batch_id 0 26 miss% 0.02390163480278174
plot_id,batch_id 0 27 miss% 0.032581311568175314
plot_id,batch_id 0 28 miss% 0.02952999157623722
plot_id,batch_id 0 29 miss% 0.0328593780326442
plot_id,batch_id 0 30 miss% 0.04587016173064855
plot_id,batch_id 0 31 miss% 0.05015639973919452
plot_id,batch_id 0 32 miss% 0.03233925821040508
plot_id,batch_id 0 33 miss% 0.02784184055074031
plot_id,batch_id 0 34 miss% 0.031086429700049965
plot_id,batch_id 0 35 miss% 0.061179971190882816
plot_id,batch_id 0 36 miss% 0.042671808095795793
plot_id,batch_id 0 37 miss% 0.027656949916723017
plot_id,batch_id 0 38 miss% 0.03362823676946539
plot_id,batch_id 0 39 miss% 0.023073308046783498
plot_id,batch_id 0 40 miss% 0.07627037832072071
plot_id,batch_id 0 41 miss% 0.031197524852490784
plot_id,batch_id 0 42 miss% 0.025309301033097326
plot_id,batch_id 0 43 miss% 0.02615863752573027
plot_id,batch_id 0 44 miss% 0.02220535673029541
plot_id,batch_id 0 45 miss% 0.023808498438869435
plot_id,batch_id 0 46 miss% 0.030346837234227052
plot_id,batch_id 0 47 miss% 0.0237330779233574
plot_id,batch_id 0 48 miss% 0.026447551492719924
plot_id,batch_id 0 49 miss% 0.027833415620819823
plot_id,batch_id 0 50 miss% 0.030621381409474527
plot_id,batch_id 0 51 miss% 0.029072624533540214
plot_id,batch_id 0 52 miss% 0.023516199123952534
plot_id,batch_id 0 53 miss% 0.018167988813971102
plot_id,batch_id 0 54 miss% 0.031091856738518508
plot_id,batch_id 0 55 miss% 0.04877322173959276
plot_id,batch_id 0 56 miss% 0.029657907770823985
plot_id,batch_id 0 57 miss% 0.028205974593890657
plot_id,batch_id 0 58 miss% 0.028544571718685515
plot_id,batch_id 0 59 miss% 0.030809372783701012
plot_id,batch_id 0 60 miss% 0.03734834183955062
plot_id,batch_id 0 61 miss% 0.0408596395074365
plot_id,batch_id 0 62 miss% 0.041521835562747716
plot_id,batch_id 0 63 miss% 0.03653757381927849
plot_id,batch_id 0 64 miss% 0.030228039327517876
plot_id,batch_id 0 65 miss% 0.04887644198409052
plot_id,batch_id 0 66 miss% 0.0670132680213062
plot_id,batch_id 0 67 miss% 0.03656598169728071
plot_id,batch_id 0 68 miss% 0.03228837949263075
plot_id,batch_id 0 69the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  61841
Epoch:0, Train loss:0.572246, valid loss:0.569650
Epoch:1, Train loss:0.357611, valid loss:0.362045
Epoch:2, Train loss:0.346203, valid loss:0.359564
Epoch:3, Train loss:0.344147, valid loss:0.359632
Epoch:4, Train loss:0.343305, valid loss:0.358490
Epoch:5, Train loss:0.342446, valid loss:0.358550
Epoch:6, Train loss:0.342164, valid loss:0.358459
Epoch:7, Train loss:0.341916, valid loss:0.358256
Epoch:8, Train loss:0.341819, valid loss:0.359142
Epoch:9, Train loss:0.341590, valid loss:0.357686
Epoch:10, Train loss:0.341416, valid loss:0.357787
Epoch:11, Train loss:0.340206, valid loss:0.357530
Epoch:12, Train loss:0.340281, valid loss:0.357224
Epoch:13, Train loss:0.340196, valid loss:0.357333
Epoch:14, Train loss:0.340115, valid loss:0.358205
Epoch:15, Train loss:0.340033, valid loss:0.357478
Epoch:16, Train loss:0.340003, valid loss:0.357306
Epoch:17, Train loss:0.339999, valid loss:0.357732
Epoch:18, Train loss:0.339972, valid loss:0.357224
Epoch:19, Train loss:0.339868, valid loss:0.357130
Epoch:20, Train loss:0.339911, valid loss:0.357328
Epoch:21, Train loss:0.339293, valid loss:0.357210
Epoch:22, Train loss:0.339279, valid loss:0.357123
Epoch:23, Train loss:0.339310, valid loss:0.357219
Epoch:24, Train loss:0.339235, valid loss:0.356979
Epoch:25, Train loss:0.339277, valid loss:0.356996
Epoch:26, Train loss:0.339234, valid loss:0.357113
Epoch:27, Train loss:0.339231, valid loss:0.357400
Epoch:28, Train loss:0.339193, valid loss:0.357281
Epoch:29, Train loss:0.339217, valid loss:0.357109
Epoch:30, Train loss:0.339208, valid loss:0.357186
Epoch:31, Train loss:0.338882, valid loss:0.356889
Epoch:32, Train loss:0.338905, valid loss:0.356881
Epoch:33, Train loss:0.338900, valid loss:0.356855
Epoch:34, Train loss:0.338878, valid loss:0.356924
Epoch:35, Train loss:0.338873, valid loss:0.356929
Epoch:36, Train loss:0.338893, valid loss:0.356942
Epoch:37, Train loss:0.338858, valid loss:0.356833
Epoch:38, Train loss:0.338873, valid loss:0.356868
Epoch:39, Train loss:0.338856, valid loss:0.356961
Epoch:40, Train loss:0.338820, valid loss:0.356968
Epoch:41, Train loss:0.338711, valid loss:0.356824
Epoch:42, Train loss:0.338689, valid loss:0.356827
Epoch:43, Train loss:0.338706, valid loss:0.356854
Epoch:44, Train loss:0.338708, valid loss:0.356885
Epoch:45, Train loss:0.338686, valid loss:0.356871
Epoch:46, Train loss:0.338680, valid loss:0.356839
Epoch:47, Train loss:0.338681, valid loss:0.356798
Epoch:48, Train loss:0.338685, valid loss:0.356811
Epoch:49, Train loss:0.338663, valid loss:0.356773
Epoch:50, Train loss:0.338666, valid loss:0.356957
Epoch:51, Train loss:0.338608, valid loss:0.356792
Epoch:52, Train loss:0.338595, valid loss:0.356819
Epoch:53, Train loss:0.338600, valid loss:0.356814
Epoch:54, Train loss:0.338595, valid loss:0.356804
Epoch:55, Train loss:0.338596, valid loss:0.356821
Epoch:56, Train loss:0.338589, valid loss:0.356813
Epoch:57, Train loss:0.338583, valid loss:0.356803
Epoch:58, Train loss:0.338596, valid loss:0.356790
Epoch:59, Train loss:0.338585, valid loss:0.356787
Epoch:60, Train loss:0.338580, valid loss:0.356831
training time 6854.114293813705
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.732058413043154
plot_id,batch_id 0 1 miss% 0.7906443568029842
plot_id,batch_id 0 2 miss% 0.8043004110840171
plot_id,batch_id 0 3 miss% 0.8110407483761025
plot_id,batch_id 0 4 miss% 0.8091349417157758
plot_id,batch_id 0 5 miss% 0.7271693102879635
plot_id,batch_id 0 6 miss% 0.7846277255982866
plot_id,batch_id 0 7 miss% 0.7989102642340016
plot_id,batch_id 0 8 miss% 0.8066133385521935
plot_id,batch_id 0 9 miss% 0.8122349812556864
plot_id,batch_id 0 10 miss% 0.7091021563204508
plot_id,batch_id 0 11 miss% 0.7847774765871208
plot_id,batch_id 0 12 miss% 0.7979303005313689
plot_id,batch_id 0 13 miss% 0.8038242384380929
plot_id,batch_id 0 14 miss% 0.8109885263109882
plot_id,batch_id 0 15 miss% 0.7157807795768937
plot_id,batch_id 0 16 miss% 0.7801817910423182
plot_id,batch_id 0 17 miss% 0.7950657008853282
plot_id,batch_id 0 18 miss% 0.8042547238584197
plot_id,batch_id 0 19 miss% 0.8083543689539562
plot_id,batch_id 0 20 miss% 0.7712175722948517
plot_id,batch_id 0 21 miss% 0.809022010953105
plot_id,batch_id 0 22 miss% 0.8121593852492399
plot_id,batch_id 0 23 miss% 0.8135685604575421
plot_id,batch_id 0 24 miss% 0.8163165790944891
plot_id,batch_id 0 25 miss% 0.756809460315913
plot_id,batch_id 0 26 miss% 0.7982190596722751
plot_id,batch_id 0 27 miss% 0.8044281607985654
plot_id,batch_id 0 28 miss% 0.8075273423487416
plot_id,batch_id 0 29 miss% 0.8087656618505139
plot_id,batch_id 0 30 miss% 0.758854235965322
plot_id,batch_id 0 31 miss% 0.7944837192983577
plot_id,batch_id 0 32 miss% 0.8059409528865062
plot_id,batch_id 0 33 miss% 0.8090485336408221
plot_id,batch_id 0 34 miss% 0.8100719897685832
plot_id,batch_id 0 35 miss% 0.7526808706732442
plot_id,batch_id 0 36 miss% 0.7996040461923791
plot_id,batch_id 0 37 miss% 0.8078810919358249
plot_id,batch_id 0 38 miss% 0.8104100088544238
plot_id,batch_id 0 39 miss% 0.8112268878546132
plot_id,batch_id 0 40 miss% 0.7805629452509802
plot_id,batch_id 0 41 miss% 0.805289399774369
plot_id,batch_id 0 42 miss% 0.8105602455642351
plot_id,batch_id 0 43 miss% 0.8148149475981623
plot_id,batch_id 0 44 miss% 0.818054755306623
plot_id,batch_id 0 45 miss% 0.7789044880451403
plot_id,batch_id 0 46 miss% 0.8055306643476624
plot_id,batch_id 0 47 miss% 0.81137670520421
plot_id,batch_id 0 48 miss% 0.8116177773585656
plot_id,batch_id 0 49 miss% 0.815800880131445
plot_id,batch_id 0 50 miss% 0.7861744075350785
plot_id,batch_id 0 51 miss% 0.8024902171812976
plot_id,batch_id 0 52 miss% 0.8076742548712849
plot_id,batch_id 0 53 miss% 0.8128925950638297
plot_id,batch_id 0 54 miss% 0.8182337293526065
plot_id,batch_id 0 55 miss% 0.7713346728132165
plot_id,batch_id 0 56 miss% 0.8058627027405488
plot_id,batch_id 0 57 miss% 0.8087476454529057
plot_id,batch_id 0 58 miss% 0.8133566077911596
plot_id,batch_id 0 59 miss% 0.8167930186307791
plot_id,batch_id 0 60 miss% 0.6542599373845002
plot_id,batch_id 0 61 miss% 0.7480922586071277
plot_id,batch_id 0 62 miss% 0.7869719236970096
plot_id,batch_id 0 63 miss% 0.7954979898115512
plot_id,batch_id 0 64 miss% 0.8031793089874382
plot_id,batch_id 0 65 miss% 0.6438350231785309
plot_id,batch_id 0 66 miss% 0.7544689820068002
plot_id,batch_id 0 67 miss% 0.7719989753812451
plot_id,batch_id 0 68 miss% 0.7972794910658849
plot_id,batch_id 0 69 miss% 0.7935690836739745
plot_id,batch_id 0 70 miss% 0.6171472016203697
plot_id,batch_id 0 71 miss%  miss% 0.03840536022219498
plot_id,batch_id 0 70 miss% 0.0434741709152625
plot_id,batch_id 0 71 miss% 0.0461725320875285
plot_id,batch_id 0 72 miss% 0.042619391881879
plot_id,batch_id 0 73 miss% 0.0440692823216739
plot_id,batch_id 0 74 miss% 0.026316189214677745
plot_id,batch_id 0 75 miss% 0.046050667496964036
plot_id,batch_id 0 76 miss% 0.05634375818650845
plot_id,batch_id 0 77 miss% 0.052447540980832145
plot_id,batch_id 0 78 miss% 0.05502293292604609
plot_id,batch_id 0 79 miss% 0.03751994144460408
plot_id,batch_id 0 80 miss% 0.07162021745109275
plot_id,batch_id 0 81 miss% 0.030914514198376687
plot_id,batch_id 0 82 miss% 0.02802106171517673
plot_id,batch_id 0 83 miss% 0.030271704601381847
plot_id,batch_id 0 84 miss% 0.03530409886285488
plot_id,batch_id 0 85 miss% 0.05949654792391758
plot_id,batch_id 0 86 miss% 0.04273791582281555
plot_id,batch_id 0 87 miss% 0.03606650121635262
plot_id,batch_id 0 88 miss% 0.058416016893033336
plot_id,batch_id 0 89 miss% 0.029450403101958685
plot_id,batch_id 0 90 miss% 0.057706041014629846
plot_id,batch_id 0 91 miss% 0.04642933997216228
plot_id,batch_id 0 92 miss% 0.040765490563945336
plot_id,batch_id 0 93 miss% 0.02889570165075452
plot_id,batch_id 0 94 miss% 0.03751140613065077
plot_id,batch_id 0 95 miss% 0.06682534410735347
plot_id,batch_id 0 96 miss% 0.03269869783299846
plot_id,batch_id 0 97 miss% 0.04387687918819053
plot_id,batch_id 0 98 miss% 0.048376732251251615
plot_id,batch_id 0 99 miss% 0.03968943068541859
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03587478 0.02814301 0.02798498 0.02787084 0.03747267 0.03721918
 0.03456299 0.02481859 0.03558032 0.02467976 0.05089452 0.0463291
 0.03201484 0.02508826 0.02662898 0.0385315  0.03788375 0.03716863
 0.02817395 0.03780707 0.03898517 0.01543916 0.02929382 0.02653333
 0.02584442 0.0480693  0.02390163 0.03258131 0.02952999 0.03285938
 0.04587016 0.0501564  0.03233926 0.02784184 0.03108643 0.06117997
 0.04267181 0.02765695 0.03362824 0.02307331 0.07627038 0.03119752
 0.0253093  0.02615864 0.02220536 0.0238085  0.03034684 0.02373308
 0.02644755 0.02783342 0.03062138 0.02907262 0.0235162  0.01816799
 0.03109186 0.04877322 0.02965791 0.02820597 0.02854457 0.03080937
 0.03734834 0.04085964 0.04152184 0.03653757 0.03022804 0.04887644
 0.06701327 0.03656598 0.03228838 0.03840536 0.04347417 0.04617253
 0.04261939 0.04406928 0.02631619 0.04605067 0.05634376 0.05244754
 0.05502293 0.03751994 0.07162022 0.03091451 0.02802106 0.0302717
 0.0353041  0.05949655 0.04273792 0.0360665  0.05841602 0.0294504
 0.05770604 0.04642934 0.04076549 0.0288957  0.03751141 0.06682534
 0.0326987  0.04387688 0.04837673 0.03968943]
for model  154 the mean error 0.036897966000552805
all id 154 hidden_dim 16 learning_rate 0.02 num_layers 5 frames 25 out win 4 err 0.036897966000552805
Launcher: Job 155 completed in 7030 seconds.
Launcher: Task 161 done. Exiting.
0.7709303954406824
plot_id,batch_id 0 72 miss% 0.7623068609385698
plot_id,batch_id 0 73 miss% 0.779534859045115
plot_id,batch_id 0 74 miss% 0.7894474917443213
plot_id,batch_id 0 75 miss% 0.6085286277359511
plot_id,batch_id 0 76 miss% 0.7141202395818472
plot_id,batch_id 0 77 miss% 0.7544682664840188
plot_id,batch_id 0 78 miss% 0.7772557062026649
plot_id,batch_id 0 79 miss% 0.7902179188101833
plot_id,batch_id 0 80 miss% 0.6760107588684087
plot_id,batch_id 0 81 miss% 0.7804326111718682
plot_id,batch_id 0 82 miss% 0.793724531912265
plot_id,batch_id 0 83 miss% 0.8029760003803683
plot_id,batch_id 0 84 miss% 0.8018939794988715
plot_id,batch_id 0 85 miss% 0.6690500545083696
plot_id,batch_id 0 86 miss% 0.7715792258519609
plot_id,batch_id 0 87 miss% 0.7917833356416273
plot_id,batch_id 0 88 miss% 0.7971901076232604
plot_id,batch_id 0 89 miss% 0.7991676043436446
plot_id,batch_id 0 90 miss% 0.6397962252349023
plot_id,batch_id 0 91 miss% 0.7667626756914008
plot_id,batch_id 0 92 miss% 0.7757144285913139
plot_id,batch_id 0 93 miss% 0.7885222909095111
plot_id,batch_id 0 94 miss% 0.8000851111704953
plot_id,batch_id 0 95 miss% 0.6389452136887347
plot_id,batch_id 0 96 miss% 0.7531635366781066
plot_id,batch_id 0 97 miss% 0.7761820790834988
plot_id,batch_id 0 98 miss% 0.7862358009708403
plot_id,batch_id 0 99 miss% 0.7974350514122794
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73205841 0.79064436 0.80430041 0.81104075 0.80913494 0.72716931
 0.78462773 0.79891026 0.80661334 0.81223498 0.70910216 0.78477748
 0.7979303  0.80382424 0.81098853 0.71578078 0.78018179 0.7950657
 0.80425472 0.80835437 0.77121757 0.80902201 0.81215939 0.81356856
 0.81631658 0.75680946 0.79821906 0.80442816 0.80752734 0.80876566
 0.75885424 0.79448372 0.80594095 0.80904853 0.81007199 0.75268087
 0.79960405 0.80788109 0.81041001 0.81122689 0.78056295 0.8052894
 0.81056025 0.81481495 0.81805476 0.77890449 0.80553066 0.81137671
 0.81161778 0.81580088 0.78617441 0.80249022 0.80767425 0.8128926
 0.81823373 0.77133467 0.8058627  0.80874765 0.81335661 0.81679302
 0.65425994 0.74809226 0.78697192 0.79549799 0.80317931 0.64383502
 0.75446898 0.77199898 0.79727949 0.79356908 0.6171472  0.7709304
 0.76230686 0.77953486 0.78944749 0.60852863 0.71412024 0.75446827
 0.77725571 0.79021792 0.67601076 0.78043261 0.79372453 0.802976
 0.80189398 0.66905005 0.77157923 0.79178334 0.79719011 0.7991676
 0.63979623 0.76676268 0.77571443 0.78852229 0.80008511 0.63894521
 0.75316354 0.77618208 0.7862358  0.79743505]
for model  148 the mean error 0.7786106450820403
all id 148 hidden_dim 24 learning_rate 0.02 num_layers 4 frames 25 out win 4 err 0.7786106450820403
Launcher: Job 149 completed in 7030 seconds.
Launcher: Task 43 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  79249
Epoch:0, Train loss:0.448520, valid loss:0.420811
Epoch:1, Train loss:0.035530, valid loss:0.006025
Epoch:2, Train loss:0.009474, valid loss:0.004664
Epoch:3, Train loss:0.007316, valid loss:0.003968
Epoch:4, Train loss:0.005945, valid loss:0.003212
Epoch:5, Train loss:0.005126, valid loss:0.002995
Epoch:6, Train loss:0.004569, valid loss:0.002362
Epoch:7, Train loss:0.004150, valid loss:0.003358
Epoch:8, Train loss:0.003936, valid loss:0.001878
Epoch:9, Train loss:0.003700, valid loss:0.002591
Epoch:10, Train loss:0.003684, valid loss:0.002136
Epoch:11, Train loss:0.002367, valid loss:0.001372
Epoch:12, Train loss:0.002380, valid loss:0.001717
Epoch:13, Train loss:0.002326, valid loss:0.001336
Epoch:14, Train loss:0.002360, valid loss:0.001238
Epoch:15, Train loss:0.002268, valid loss:0.001316
Epoch:16, Train loss:0.002251, valid loss:0.001137
Epoch:17, Train loss:0.002111, valid loss:0.001184
Epoch:18, Train loss:0.002107, valid loss:0.001227
Epoch:19, Train loss:0.002064, valid loss:0.001405
Epoch:20, Train loss:0.002051, valid loss:0.001675
Epoch:21, Train loss:0.001448, valid loss:0.000919
Epoch:22, Train loss:0.001444, valid loss:0.000800
Epoch:23, Train loss:0.001451, valid loss:0.001053
Epoch:24, Train loss:0.001398, valid loss:0.001063
Epoch:25, Train loss:0.001424, valid loss:0.000936
Epoch:26, Train loss:0.001397, valid loss:0.000783
Epoch:27, Train loss:0.001384, valid loss:0.000911
Epoch:28, Train loss:0.001372, valid loss:0.001015
Epoch:29, Train loss:0.001325, valid loss:0.000875
Epoch:30, Train loss:0.001360, valid loss:0.001140
Epoch:31, Train loss:0.001067, valid loss:0.000840
Epoch:32, Train loss:0.001036, valid loss:0.000741
Epoch:33, Train loss:0.001052, valid loss:0.000805
Epoch:34, Train loss:0.001050, valid loss:0.000918
Epoch:35, Train loss:0.001045, valid loss:0.000703
Epoch:36, Train loss:0.001018, valid loss:0.000744
Epoch:37, Train loss:0.001014, valid loss:0.000732
Epoch:38, Train loss:0.001021, valid loss:0.000774
Epoch:39, Train loss:0.001006, valid loss:0.000746
Epoch:40, Train loss:0.001007, valid loss:0.000829
Epoch:41, Train loss:0.000875, valid loss:0.000712
Epoch:42, Train loss:0.000864, valid loss:0.000687
Epoch:43, Train loss:0.000864, valid loss:0.000716
Epoch:44, Train loss:0.000864, valid loss:0.000687
Epoch:45, Train loss:0.000862, valid loss:0.000706
Epoch:46, Train loss:0.000859, valid loss:0.000671
Epoch:47, Train loss:0.000853, valid loss:0.000732
Epoch:48, Train loss:0.000857, valid loss:0.000768
Epoch:49, Train loss:0.000839, valid loss:0.000696
Epoch:50, Train loss:0.000840, valid loss:0.000731
Epoch:51, Train loss:0.000784, valid loss:0.000677
Epoch:52, Train loss:0.000781, valid loss:0.000688
Epoch:53, Train loss:0.000780, valid loss:0.000675
Epoch:54, Train loss:0.000777, valid loss:0.000685
Epoch:55, Train loss:0.000778, valid loss:0.000714
Epoch:56, Train loss:0.000773, valid loss:0.000656
Epoch:57, Train loss:0.000772, valid loss:0.000714
Epoch:58, Train loss:0.000772, valid loss:0.000674
Epoch:59, Train loss:0.000769, valid loss:0.000677
Epoch:60, Train loss:0.000771, valid loss:0.000663
training time 6858.101830482483
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02882338600711461
plot_id,batch_id 0 1 miss% 0.024655981958052586
plot_id,batch_id 0 2 miss% 0.023036680129082235
plot_id,batch_id 0 3 miss% 0.02090916834666238
plot_id,batch_id 0 4 miss% 0.029627320209140186
plot_id,batch_id 0 5 miss% 0.0380026184199904
plot_id,batch_id 0 6 miss% 0.024413396081776412
plot_id,batch_id 0 7 miss% 0.02879067947152651
plot_id,batch_id 0 8 miss% 0.02333591787916882
plot_id,batch_id 0 9 miss% 0.02501479035137185
plot_id,batch_id 0 10 miss% 0.050825176944929544
plot_id,batch_id 0 11 miss% 0.039946988935077324
plot_id,batch_id 0 12 miss% 0.026108584078009863
plot_id,batch_id 0 13 miss% 0.019908693258620182
plot_id,batch_id 0 14 miss% 0.022343109494996568
plot_id,batch_id 0 15 miss% 0.04857607708254051
plot_id,batch_id 0 16 miss% 0.03747017866332956
plot_id,batch_id 0 17 miss% 0.03960170468808763
plot_id,batch_id 0 18 miss% 0.03432766705406464
plot_id,batch_id 0 19 miss% 0.03153559369339685
plot_id,batch_id 0 20 miss% 0.046614212189618934
plot_id,batch_id 0 21 miss% 0.030937771567581304
plot_id,batch_id 0 22 miss% 0.03113030305949182
plot_id,batch_id 0 23 miss% 0.023539013648235376
plot_id,batch_id 0 24 miss% 0.029221122945411324
plot_id,batch_id 0 25 miss% 0.04438059024247964
plot_id,batch_id 0 26 miss% 0.028373157021292863
plot_id,batch_id 0 27 miss% 0.022812495999966376
plot_id,batch_id 0 28 miss% 0.026895068577135858
plot_id,batch_id 0 29 miss% 0.029006200215235756
plot_id,batch_id 0 30 miss% 0.0465684414546866
plot_id,batch_id 0 31 miss% 0.032499021902988756
plot_id,batch_id 0 32 miss% 0.035809507276254864
plot_id,batch_id 0 33 miss% 0.028156507480156967
plot_id,batch_id 0 34 miss% 0.022471539905432013
plot_id,batch_id 0 35 miss% 0.04923811235581531
plot_id,batch_id 0 36 miss% 0.03772460434195036
plot_id,batch_id 0 37 miss% 0.016879481517064397
plot_id,batch_id 0 38 miss% 0.022115458742213833
plot_id,batch_id 0 39 miss% 0.02018686379990205
plot_id,batch_id 0 40 miss% 0.07798484213401297
plot_id,batch_id 0 41 miss% 0.027874417641366486
plot_id,batch_id 0 42 miss% 0.02652660055106004
plot_id,batch_id 0 43 miss% 0.026530777378061653
plot_id,batch_id 0 44 miss% 0.025654370708150156
plot_id,batch_id 0 45 miss% 0.02623910264311291
plot_id,batch_id 0 46 miss% 0.02339292938223017
plot_id,batch_id 0 47 miss% 0.02759183828141125
plot_id,batch_id 0 48 miss% 0.021223583512688167
plot_id,batch_id 0 49 miss% 0.02344707414135569
plot_id,batch_id 0 50 miss% 0.031899711809026024
plot_id,batch_id 0 51 miss% 0.031243433492575443
plot_id,batch_id 0 52 miss% 0.03257629774429687
plot_id,batch_id 0 53 miss% 0.024765654723021908
plot_id,batch_id 0 54 miss% 0.024497015807223608
plot_id,batch_id 0 55 miss% 0.024644684472457153
plot_id,batch_id 0 56 miss% 0.018870698777771887
plot_id,batch_id 0 57 miss% 0.02741172137936007
plot_id,batch_id 0 58 miss% 0.023229516839165673
plot_id,batch_id 0 59 miss% 0.03475196270320557
plot_id,batch_id 0 60 miss% 0.03496288702362189
plot_id,batch_id 0 61 miss% 0.034552613390518135
plot_id,batch_id 0 62 miss% 0.02550626279488781
plot_id,batch_id 0 63 miss% 0.02539671301674842
plot_id,batch_id 0 64 miss% 0.03411435159848197
plot_id,batch_id 0 65 miss% 0.02909694179497399
plot_id,batch_id 0 66 miss% 0.0356457307584171
plot_id,batch_id 0 67 miss% 0.0292149047290399
plot_id,batch_id 0 68 miss% 0.033046175265547824
plot_id,batch_id 0 69 miss% 0.02192221429087935
plot_id,batch_id 0 70 miss% 0.04071047804993701
plot_id,batch_id 0 71 miss% 0.053184524123199725
plot_id,batch_id 0 72 miss% 0.03652966370570701
plot_id,batch_id 0 73 miss% 0.021537800360008564
plot_id,batch_id 0 74 miss% 0.02595006767169897
plot_id,batch_id 0 75 miss% 0.04093038137611832
plot_id,batch_id 0 76 miss% 0.034462914779913166
plot_id,batch_id 0 77 miss% 0.03618900365778132
plot_id,batch_id 0 78 miss% 0.027982839233985445
plot_id,batch_id 0 79 miss% 0.04627388759318737
plot_id,batch_id 0 80 miss% 0.04167907014580012
plot_id,batch_id 0 81 miss% 0.017192952537840166
plot_id,batch_id 0 82 miss% 0.038046240129031246
plot_id,batch_id 0 83 miss% 0.02755181982690558
plot_id,batch_id 0 84 miss% 0.03528852080462033
plot_id,batch_id 0 85 miss% 0.04573867749447379
plot_id,batch_id 0 86 miss% 0.02010574043687227
plot_id,batch_id 0 87 miss% 0.034211327703236906
plot_id,batch_id 0 88 miss% 0.02843810971486329
plot_id,batch_id 0 89 miss% 0.02243267471795395
plot_id,batch_id 0 90 miss% 0.03419349644405549
plot_id,batch_id 0 91 miss% 0.034405487012165135
plot_id,batch_id 0 92 miss% 0.025559073531661695
plot_id,batch_id 0 93 miss% 0.02644404473233185
plot_id,batch_id 0 94 miss% 0.03559054493431392
plot_id,batch_id 0 95 miss% 0.03885116043001294
plot_id,batch_id 0 96 miss% 0.021369524913900748
plot_id,batch_id 0 97 miss% 0.035402524912271686
plot_id,batch_id 0 98 miss% 0.027917246520744943
plot_id,batch_id 0 99 miss% 0.03101719409612178
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02882339 0.02465598 0.02303668 0.02090917 0.02962732 0.03800262
 0.0244134  0.02879068 0.02333592 0.02501479 0.05082518 0.03994699
 0.02610858 0.01990869 0.02234311 0.04857608 0.03747018 0.0396017
 0.03432767 0.03153559 0.04661421 0.03093777 0.0311303  0.02353901
 0.02922112 0.04438059 0.02837316 0.0228125  0.02689507 0.0290062
 0.04656844 0.03249902 0.03580951 0.02815651 0.02247154 0.04923811
 0.0377246  0.01687948 0.02211546 0.02018686 0.07798484 0.02787442
 0.0265266  0.02653078 0.02565437 0.0262391  0.02339293 0.02759184
 0.02122358 0.02344707 0.03189971 0.03124343 0.0325763  0.02476565
 0.02449702 0.02464468 0.0188707  0.02741172 0.02322952 0.03475196
 0.03496289 0.03455261 0.02550626 0.02539671 0.03411435 0.02909694
 0.03564573 0.0292149  0.03304618 0.02192221 0.04071048 0.05318452
 0.03652966 0.0215378  0.02595007 0.04093038 0.03446291 0.036189
 0.02798284 0.04627389 0.04167907 0.01719295 0.03804624 0.02755182
 0.03528852 0.04573868 0.02010574 0.03421133 0.02843811 0.02243267
 0.0341935  0.03440549 0.02555907 0.02644404 0.03559054 0.03885116
 0.02136952 0.03540252 0.02791725 0.03101719]
for model  141 the mean error 0.03110815207365238
all id 141 hidden_dim 32 learning_rate 0.02 num_layers 3 frames 25 out win 3 err 0.03110815207365238
Launcher: Job 142 completed in 7053 seconds.
Launcher: Task 249 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  79249
Epoch:0, Train loss:0.590469, valid loss:0.568793
Epoch:1, Train loss:0.048881, valid loss:0.009726
Epoch:2, Train loss:0.014480, valid loss:0.007127
Epoch:3, Train loss:0.011036, valid loss:0.004561
Epoch:4, Train loss:0.009274, valid loss:0.005898
Epoch:5, Train loss:0.008225, valid loss:0.004899
Epoch:6, Train loss:0.007034, valid loss:0.003503
Epoch:7, Train loss:0.006774, valid loss:0.004112
Epoch:8, Train loss:0.006077, valid loss:0.004644
Epoch:9, Train loss:0.005770, valid loss:0.003241
Epoch:10, Train loss:0.005465, valid loss:0.003324
Epoch:11, Train loss:0.003767, valid loss:0.002530
Epoch:12, Train loss:0.003644, valid loss:0.002286
Epoch:13, Train loss:0.003769, valid loss:0.002230
Epoch:14, Train loss:0.003719, valid loss:0.002338
Epoch:15, Train loss:0.003575, valid loss:0.002124
Epoch:16, Train loss:0.003827, valid loss:0.002024
Epoch:17, Train loss:0.003412, valid loss:0.002126
Epoch:18, Train loss:0.003323, valid loss:0.001945
Epoch:19, Train loss:0.003313, valid loss:0.001952
Epoch:20, Train loss:0.003152, valid loss:0.002138
Epoch:21, Train loss:0.002350, valid loss:0.001621
Epoch:22, Train loss:0.002360, valid loss:0.001625
Epoch:23, Train loss:0.002308, valid loss:0.001755
Epoch:24, Train loss:0.002344, valid loss:0.001465
Epoch:25, Train loss:0.002263, valid loss:0.001616
Epoch:26, Train loss:0.002337, valid loss:0.001569
Epoch:27, Train loss:0.002188, valid loss:0.001718
Epoch:28, Train loss:0.002210, valid loss:0.001702
Epoch:29, Train loss:0.002233, valid loss:0.001697
Epoch:30, Train loss:0.002189, valid loss:0.001579
Epoch:31, Train loss:0.001715, valid loss:0.001474
Epoch:32, Train loss:0.001754, valid loss:0.001376
Epoch:33, Train loss:0.001736, valid loss:0.001440
Epoch:34, Train loss:0.001727, valid loss:0.001456
Epoch:35, Train loss:0.001715, valid loss:0.001447
Epoch:36, Train loss:0.001699, valid loss:0.001451
Epoch:37, Train loss:0.001669, valid loss:0.001502
Epoch:38, Train loss:0.001657, valid loss:0.001423
Epoch:39, Train loss:0.001644, valid loss:0.001378
Epoch:40, Train loss:0.001622, valid loss:0.001399
Epoch:41, Train loss:0.001439, valid loss:0.001374
Epoch:42, Train loss:0.001427, valid loss:0.001261
Epoch:43, Train loss:0.001438, valid loss:0.001405
Epoch:44, Train loss:0.001411, valid loss:0.001270
Epoch:45, Train loss:0.001426, valid loss:0.001335
Epoch:46, Train loss:0.001436, valid loss:0.001401
Epoch:47, Train loss:0.001425, valid loss:0.001296
Epoch:48, Train loss:0.001419, valid loss:0.001300
Epoch:49, Train loss:0.001406, valid loss:0.001282
Epoch:50, Train loss:0.001387, valid loss:0.001295
Epoch:51, Train loss:0.001294, valid loss:0.001294
Epoch:52, Train loss:0.001276, valid loss:0.001228
Epoch:53, Train loss:0.001273, valid loss:0.001282
Epoch:54, Train loss:0.001286, valid loss:0.001278
Epoch:55, Train loss:0.001264, valid loss:0.001241
Epoch:56, Train loss:0.001276, valid loss:0.001242
Epoch:57, Train loss:0.001269, valid loss:0.001352
Epoch:58, Train loss:0.001273, valid loss:0.001286
Epoch:59, Train loss:0.001262, valid loss:0.001245
Epoch:60, Train loss:0.001260, valid loss:0.001288
training time 6909.058583021164
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.0429804050643055
plot_id,batch_id 0 1 miss% 0.027936855683931776
plot_id,batch_id 0 2 miss% 0.03616295816030814
plot_id,batch_id 0 3 miss% 0.031569735353242666
plot_id,batch_id 0 4 miss% 0.02939846658899311
plot_id,batch_id 0 5 miss% 0.03408303553687603
plot_id,batch_id 0 6 miss% 0.034928197412667325
plot_id,batch_id 0 7 miss% 0.03405006035945296
plot_id,batch_id 0 8 miss% 0.031101748203039736
plot_id,batch_id 0 9 miss% 0.019734532625903967
plot_id,batch_id 0 10 miss% 0.04995294226128238
plot_id,batch_id 0 11 miss% 0.03569661350467473
plot_id,batch_id 0 12 miss% 0.03351262933743664
plot_id,batch_id 0 13 miss% 0.03233767833186464
plot_id,batch_id 0 14 miss% 0.028823378464012377
plot_id,batch_id 0 15 miss% 0.04959943784480688
plot_id,batch_id 0 16 miss% 0.02638071358363973
plot_id,batch_id 0 17 miss% 0.05570121832633403
plot_id,batch_id 0 18 miss% 0.03685853625253022
plot_id,batch_id 0 19 miss% 0.047427676246078465
plot_id,batch_id 0 20 miss% 0.1091751321328055
plot_id,batch_id 0 21 miss% 0.02746885689080279
plot_id,batch_id 0 22 miss% 0.03464301134197849
plot_id,batch_id 0 23 miss% 0.033042016223970856
plot_id,batch_id 0 24 miss% 0.03470138527098803
plot_id,batch_id 0 25 miss% 0.03330389966637166
plot_id,batch_id 0 26 miss% 0.03364023472249461
plot_id,batch_id 0 27 miss% 0.03544086804077807
plot_id,batch_id 0 28 miss% 0.026436327270789647
plot_id,batch_id 0 29 miss% 0.030090692533043226
plot_id,batch_id 0 30 miss% 0.04133944733508799
plot_id,batch_id 0 31 miss% 0.054261445978876915
plot_id,batch_id 0 32 miss% 0.036355643389806284
plot_id,batch_id 0 33 miss% 0.03472871683589984
plot_id,batch_id 0 34 miss% 0.032570302251592756
plot_id,batch_id 0 35 miss% 0.03676522656871863
plot_id,batch_id 0 36 miss% 0.07361622429019757
plot_id,batch_id 0 37 miss% 0.021437917283808797
plot_id,batch_id 0 38 miss% 0.02196914660179634
plot_id,batch_id 0 39 miss% 0.025670910680834425
plot_id,batch_id 0 40 miss% 0.09120768736888941
plot_id,batch_id 0 41 miss% 0.019260450703027305
plot_id,batch_id 0 42 miss% 0.022819710081933533
plot_id,batch_id 0 43 miss% 0.025414660500059878
plot_id,batch_id 0 44 miss% 0.02327864847224332
plot_id,batch_id 0 45 miss% 0.042535547540466394
plot_id,batch_id 0 46 miss% 0.02357935638419991
plot_id,batch_id 0 47 miss% 0.019844622604198323
plot_id,batch_id 0 48 miss% 0.027718178989079845
plot_id,batch_id 0 49 miss% 0.015854267554524456
plot_id,batch_id 0 50 miss% 0.03815832004444046
plot_id,batch_id 0 51 miss% 0.029943379580370466
plot_id,batch_id 0 52 miss% 0.025698812089612742
plot_id,batch_id 0 53 miss% 0.027367965447295754
plot_id,batch_id 0 54 miss% 0.029057645605390134
plot_id,batch_id 0 55 miss% 0.038090515884682624
plot_id,batch_id 0 56 miss% 0.029266446232150495
plot_id,batch_id 0 57 miss% 0.027596802545537688
plot_id,batch_id 0 58 miss% 0.03559917490128141
plot_id,batch_id 0 59 miss% 0.029625229891707294
plot_id,batch_id 0 60 miss% 0.03747979203005286
plot_id,batch_id 0 61 miss% 0.029075323398989418
plot_id,batch_id 0 62 miss% 0.041571033731513544
plot_id,batch_id 0 63 miss% 0.03189739385088138
plot_id,batch_id 0 64 miss% 0.04139090492990148
plot_id,batch_id 0 65 miss% 0.038649974568535285
plot_id,batch_id 0 66 miss% 0.06873127468980375
plot_id,batch_id 0 67 miss% 0.03740552163897613
plot_id,batch_id 0 68 miss% 0.031120851747677647
plot_id,batch_id 0 69 miss% 0.03413886299008572
plot_id,batch_id 0 70 miss% 0.04168828062681521
plot_id,batch_id 0 71 miss% 0.049620478790202885
plot_id,batch_id 0 72 miss% 0.02903375159456113
plot_id,batch_id 0 73 miss% 0.04375201116150301
plot_id,batch_id 0 74 miss% 0.03311296211990836
plot_id,batch_id 0 75 miss% 0.05457870005152659
plot_id,batch_id 0 76 miss% 0.04081744447850008
plot_id,batch_id 0 77 miss% 0.033655156892789945
plot_id,batch_id 0 78 miss% 0.04478582407821114
plot_id,batch_id 0 79 miss% 0.04862945489186027
plot_id,batch_id 0 80 miss% 0.02356537986672323
plot_id,batch_id 0 81 miss% 0.033419723106521894
plot_id,batch_id 0 82 miss% 0.027327076638597517
plot_id,batch_id 0 83 miss% 0.03968466840432445
plot_id,batch_id 0 84 miss% 0.033432192216969615
plot_id,batch_id 0 85 miss% 0.04337304451004605
plot_id,batch_id 0 86 miss% 0.022949588368212086
plot_id,batch_id 0 87 miss% 0.03695574125643261
plot_id,batch_id 0 88 miss% 0.034590232146175494
plot_id,batch_id 0 89 miss% 0.0327643675290435
plot_id,batch_id 0 90 miss% 0.03360982547291883
plot_id,batch_id 0 91 miss% 0.029592479097648314
plot_id,batch_id 0 92 miss% 0.027867836833270487
plot_id,batch_id 0 93 miss% 0.04994127400389319
plot_id,batch_id 0 94 miss% 0.03239784808841788
plot_id,batch_id 0 95 miss% 0.0410980376654521
plot_id,batch_id 0 96 miss% 0.0368477493892747
plot_id,batch_id 0 97 miss% 0.05429010071370367
plot_id,batch_id 0 98 miss% 0.03403720392670762
plot_id,batch_id 0 99 miss% 0.03428671907564608
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04298041 0.02793686 0.03616296 0.03156974 0.02939847 0.03408304
 0.0349282  0.03405006 0.03110175 0.01973453 0.04995294 0.03569661
 0.03351263 0.03233768 0.02882338 0.04959944 0.02638071 0.05570122
 0.03685854 0.04742768 0.10917513 0.02746886 0.03464301 0.03304202
 0.03470139 0.0333039  0.03364023 0.03544087 0.02643633 0.03009069
 0.04133945 0.05426145 0.03635564 0.03472872 0.0325703  0.03676523
 0.07361622 0.02143792 0.02196915 0.02567091 0.09120769 0.01926045
 0.02281971 0.02541466 0.02327865 0.04253555 0.02357936 0.01984462
 0.02771818 0.01585427 0.03815832 0.02994338 0.02569881 0.02736797
 0.02905765 0.03809052 0.02926645 0.0275968  0.03559917 0.02962523
 0.03747979 0.02907532 0.04157103 0.03189739 0.0413909  0.03864997
 0.06873127 0.03740552 0.03112085 0.03413886 0.04168828 0.04962048
 0.02903375 0.04375201 0.03311296 0.0545787  0.04081744 0.03365516
 0.04478582 0.04862945 0.02356538 0.03341972 0.02732708 0.03968467
 0.03343219 0.04337304 0.02294959 0.03695574 0.03459023 0.03276437
 0.03360983 0.02959248 0.02786784 0.04994127 0.03239785 0.04109804
 0.03684775 0.0542901  0.0340372  0.03428672]
for model  35 the mean error 0.0362997773344539
all id 35 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 21 out win 5 err 0.0362997773344539
Launcher: Job 36 completed in 7123 seconds.
Launcher: Task 11 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  79249
Epoch:0, Train loss:0.317563, valid loss:0.289009
Epoch:1, Train loss:0.014883, valid loss:0.003390
Epoch:2, Train loss:0.004362, valid loss:0.002190
Epoch:3, Train loss:0.003538, valid loss:0.001919
Epoch:4, Train loss:0.002932, valid loss:0.001526
Epoch:5, Train loss:0.002617, valid loss:0.001582
Epoch:6, Train loss:0.002405, valid loss:0.001269
Epoch:7, Train loss:0.002253, valid loss:0.001478
Epoch:8, Train loss:0.002139, valid loss:0.001212
Epoch:9, Train loss:0.002066, valid loss:0.001359
Epoch:10, Train loss:0.001976, valid loss:0.001207
Epoch:11, Train loss:0.001349, valid loss:0.000793
Epoch:12, Train loss:0.001326, valid loss:0.000999
Epoch:13, Train loss:0.001305, valid loss:0.000742
Epoch:14, Train loss:0.001268, valid loss:0.000741
Epoch:15, Train loss:0.001227, valid loss:0.000686
Epoch:16, Train loss:0.001250, valid loss:0.000873
Epoch:17, Train loss:0.001240, valid loss:0.000615
Epoch:18, Train loss:0.001191, valid loss:0.000750
Epoch:19, Train loss:0.001156, valid loss:0.000700
Epoch:20, Train loss:0.001139, valid loss:0.000771
Epoch:21, Train loss:0.000877, valid loss:0.000538
Epoch:22, Train loss:0.000865, valid loss:0.000615
Epoch:23, Train loss:0.000857, valid loss:0.000773
Epoch:24, Train loss:0.000857, valid loss:0.000496
Epoch:25, Train loss:0.000834, valid loss:0.000588
Epoch:26, Train loss:0.000837, valid loss:0.000551
Epoch:27, Train loss:0.000849, valid loss:0.000538
Epoch:28, Train loss:0.000805, valid loss:0.000562
Epoch:29, Train loss:0.000810, valid loss:0.000551
Epoch:30, Train loss:0.000813, valid loss:0.000560
Epoch:31, Train loss:0.000671, valid loss:0.000497
Epoch:32, Train loss:0.000667, valid loss:0.000476
Epoch:33, Train loss:0.000671, valid loss:0.000504
Epoch:34, Train loss:0.000653, valid loss:0.000558
Epoch:35, Train loss:0.000658, valid loss:0.000515
Epoch:36, Train loss:0.000655, valid loss:0.000536
Epoch:37, Train loss:0.000655, valid loss:0.000487
Epoch:38, Train loss:0.000657, valid loss:0.000484
Epoch:39, Train loss:0.000647, valid loss:0.000529
Epoch:40, Train loss:0.000650, valid loss:0.000475
Epoch:41, Train loss:0.000580, valid loss:0.000476
Epoch:42, Train loss:0.000577, valid loss:0.000494
Epoch:43, Train loss:0.000576, valid loss:0.000468
Epoch:44, Train loss:0.000577, valid loss:0.000481
Epoch:45, Train loss:0.000573, valid loss:0.000488
Epoch:46, Train loss:0.000567, valid loss:0.000492
Epoch:47, Train loss:0.000567, valid loss:0.000475
Epoch:48, Train loss:0.000566, valid loss:0.000491
Epoch:49, Train loss:0.000571, valid loss:0.000515
Epoch:50, Train loss:0.000561, valid loss:0.000481
Epoch:51, Train loss:0.000536, valid loss:0.000474
Epoch:52, Train loss:0.000533, valid loss:0.000465
Epoch:53, Train loss:0.000532, valid loss:0.000465
Epoch:54, Train loss:0.000531, valid loss:0.000471
Epoch:55, Train loss:0.000531, valid loss:0.000461
Epoch:56, Train loss:0.000529, valid loss:0.000479
Epoch:57, Train loss:0.000528, valid loss:0.000452
Epoch:58, Train loss:0.000528, valid loss:0.000462
Epoch:59, Train loss:0.000527, valid loss:0.000485
Epoch:60, Train loss:0.000526, valid loss:0.000472
training time 6948.263015031815
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.0444273004766864
plot_id,batch_id 0 1 miss% 0.02762233729851997
plot_id,batch_id 0 2 miss% 0.021260405458070168
plot_id,batch_id 0 3 miss% 0.018506930874160332
plot_id,batch_id 0 4 miss% 0.02362262294705526
plot_id,batch_id 0 5 miss% 0.028966381918829824
plot_id,batch_id 0 6 miss% 0.02189699961466618
plot_id,batch_id 0 7 miss% 0.027204587490631146
plot_id,batch_id 0 8 miss% 0.017776656823470956
plot_id,batch_id 0 9 miss% 0.017615983211617524
plot_id,batch_id 0 10 miss% 0.024826953776868055
plot_id,batch_id 0 11 miss% 0.03287584987982349
plot_id,batch_id 0 12 miss% 0.02443626832670962
plot_id,batch_id 0 13 miss% 0.02330554317879044
plot_id,batch_id 0 14 miss% 0.026523324937987876
plot_id,batch_id 0 15 miss% 0.03889662838853542
plot_id,batch_id 0 16 miss% 0.035364908294121526
plot_id,batch_id 0 17 miss% 0.04127767552068975
plot_id,batch_id 0 18 miss% 0.03150968778942998
plot_id,batch_id 0 19 miss% 0.03514690101060372
plot_id,batch_id 0 20 miss% 0.022429186070135273
plot_id,batch_id 0 21 miss% 0.027990033699564647
plot_id,batch_id 0 22 miss% 0.02504628107135177
plot_id,batch_id 0 23 miss% 0.01918233146250523
plot_id,batch_id 0 24 miss% 0.018158002330475376
plot_id,batch_id 0 25 miss% 0.02521739426192706
plot_id,batch_id 0 26 miss% 0.03560397653710789
plot_id,batch_id 0 27 miss% 0.026503717462381053
plot_id,batch_id 0 28 miss% 0.018768085648913524
plot_id,batch_id 0 29 miss% 0.028584539436373325
plot_id,batch_id 0 30 miss% 0.07193278658484058
plot_id,batch_id 0 31 miss% 0.018674030040421122
plot_id,batch_id 0 32 miss% 0.019038215031554315
plot_id,batch_id 0 33 miss% 0.017901247255903842
plot_id,batch_id 0 34 miss% 0.01756981404198665
plot_id,batch_id 0 35 miss% 0.03419829701405838
plot_id,batch_id 0 36 miss% 0.023364006985066455
plot_id,batch_id 0 37 miss% 0.02768541768579919
plot_id,batch_id 0 38 miss% 0.02194792354201153
plot_id,batch_id 0 39 miss% 0.026946432300876
plot_id,batch_id 0 40 miss% 0.08160394727699444
plot_id,batch_id 0 41 miss% 0.03356507400526962
plot_id,batch_id 0 42 miss% 0.020975357524113752
plot_id,batch_id 0 43 miss% 0.02797541236361585
plot_id,batch_id 0 44 miss% 0.028756483128618688
plot_id,batch_id 0 45 miss% 0.031559076069173866
plot_id,batch_id 0 46 miss% 0.035799855858044835
plot_id,batch_id 0 47 miss% 0.024218195117223888
plot_id,batch_id 0 48 miss% 0.023405995353997372
plot_id,batch_id 0 49 miss% 0.0270222434068196
plot_id,batch_id 0 50 miss% 0.035211803824409534
plot_id,batch_id 0 51 miss% 0.03128270124032086
plot_id,batch_id 0 52 miss% 0.02399855763785635
plot_id,batch_id 0 53 miss% 0.01577226654568026
plot_id,batch_id 0 54 miss% 0.03490126883235775
plot_id,batch_id 0 55 miss% 0.03786544174744516
plot_id,batch_id 0 56 miss% 0.024920961704040106
plot_id,batch_id 0 57 miss% 0.030004246099577872
plot_id,batch_id 0 58 miss% 0.03308345018229543
plot_id,batch_id 0 59 miss% 0.023856072368842737
plot_id,batch_id 0 60 miss% 0.04271726523679981
plot_id,batch_id 0 61 miss% 0.0367060898668299
plot_id,batch_id 0 62 miss% 0.02531504393176903
plot_id,batch_id 0 63 miss% 0.03011753556951777
plot_id,batch_id 0 64 miss% 0.034443295506092986
plot_id,batch_id 0 65 miss% 0.04572226169578285
plot_id,batch_id 0 66 miss% 0.02766366022193269
plot_id,batch_id 0 67 miss% 0.03265238371089733
plot_id,batch_id 0 68 miss% 0.03019145170020505
plot_id,batch_id 0 69 miss% 0.025890892960870635
plot_id,batch_id 0 70 miss% 0.024230949253786063
plot_id,batch_id 0 71 miss% 0.038021809044527434
plot_id,batch_id 0 72 miss% 0.020434746373087016
plot_id,batch_id 0 73 miss% 0.020122283688334637
plot_id,batch_id 0 74 miss% 0.03673257853488475
plot_id,batch_id 0 75 miss% 0.019877511650554208
plot_id,batch_id 0 76 miss% 0.032724041403174066
plot_id,batch_id 0 77 miss% 0.036625579831837236
plot_id,batch_id 0 78 miss% 0.028683399991678193
plot_id,batch_id 0 79 miss% 0.04390588228267626
plot_id,batch_id 0 80 miss% 0.041187044559936346
plot_id,batch_id 0 81 miss% 0.02756431315746944
plot_id,batch_id 0 82 miss% 0.01906120948362433
plot_id,batch_id 0 83 miss% 0.018526756000774507
plot_id,batch_id 0 84 miss% 0.024637689006827283
plot_id,batch_id 0 85 miss% 0.05506190124088145
plot_id,batch_id 0 86 miss% 0.028643231050251874
plot_id,batch_id 0 87 miss% 0.027054837754130954
plot_id,batch_id 0 88 miss% 0.02468610959200254
plot_id,batch_id 0 89 miss% 0.02578322252910967
plot_id,batch_id 0 90 miss% 0.03454167387029398
plot_id,batch_id 0 91 miss% 0.03956477386179387
plot_id,batch_id 0 92 miss% 0.029193654100561642
plot_id,batch_id 0 93 miss% 0.03706427661297492
plot_id,batch_id 0 94 miss% 0.04194514558612358
plot_id,batch_id 0 95 miss% 0.025069035339308963
plot_id,batch_id 0 96 miss% 0.041693232725489825
plot_id,batch_id 0 97 miss% 0.03319933366550244
plot_id,batch_id 0 98 miss% 0.03160425918449285
plot_id,batch_id 0 99 miss% 0.036823875804568196
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0444273  0.02762234 0.02126041 0.01850693 0.02362262 0.02896638
 0.021897   0.02720459 0.01777666 0.01761598 0.02482695 0.03287585
 0.02443627 0.02330554 0.02652332 0.03889663 0.03536491 0.04127768
 0.03150969 0.0351469  0.02242919 0.02799003 0.02504628 0.01918233
 0.018158   0.02521739 0.03560398 0.02650372 0.01876809 0.02858454
 0.07193279 0.01867403 0.01903822 0.01790125 0.01756981 0.0341983
 0.02336401 0.02768542 0.02194792 0.02694643 0.08160395 0.03356507
 0.02097536 0.02797541 0.02875648 0.03155908 0.03579986 0.0242182
 0.023406   0.02702224 0.0352118  0.0312827  0.02399856 0.01577227
 0.03490127 0.03786544 0.02492096 0.03000425 0.03308345 0.02385607
 0.04271727 0.03670609 0.02531504 0.03011754 0.0344433  0.04572226
 0.02766366 0.03265238 0.03019145 0.02589089 0.02423095 0.03802181
 0.02043475 0.02012228 0.03673258 0.01987751 0.03272404 0.03662558
 0.0286834  0.04390588 0.04118704 0.02756431 0.01906121 0.01852676
 0.02463769 0.0550619  0.02864323 0.02705484 0.02468611 0.02578322
 0.03454167 0.03956477 0.02919365 0.03706428 0.04194515 0.02506904
 0.04169323 0.03319933 0.03160426 0.03682388]
for model  195 the mean error 0.029912683135485753
all id 195 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 31 out win 3 err 0.029912683135485753
Launcher: Job 196 completed in 7135 seconds.
Launcher: Task 109 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  61841
Epoch:0, Train loss:0.572246, valid loss:0.569650
Epoch:1, Train loss:0.309897, valid loss:0.008151
Epoch:2, Train loss:0.011512, valid loss:0.003933
Epoch:3, Train loss:0.006467, valid loss:0.003281
Epoch:4, Train loss:0.005277, valid loss:0.002739
Epoch:5, Train loss:0.004603, valid loss:0.002715
Epoch:6, Train loss:0.004297, valid loss:0.002396
Epoch:7, Train loss:0.003833, valid loss:0.002075
Epoch:8, Train loss:0.003669, valid loss:0.002122
Epoch:9, Train loss:0.003501, valid loss:0.001727
Epoch:10, Train loss:0.003486, valid loss:0.001725
Epoch:11, Train loss:0.002425, valid loss:0.001576
Epoch:12, Train loss:0.002421, valid loss:0.001508
Epoch:13, Train loss:0.002348, valid loss:0.001284
Epoch:14, Train loss:0.002370, valid loss:0.001204
Epoch:15, Train loss:0.002283, valid loss:0.001462
Epoch:16, Train loss:0.002294, valid loss:0.001515
Epoch:17, Train loss:0.002168, valid loss:0.001223
Epoch:18, Train loss:0.002140, valid loss:0.001248
Epoch:19, Train loss:0.002135, valid loss:0.001270
Epoch:20, Train loss:0.002106, valid loss:0.001412
Epoch:21, Train loss:0.001586, valid loss:0.001084
Epoch:22, Train loss:0.001579, valid loss:0.001064
Epoch:23, Train loss:0.001602, valid loss:0.000933
Epoch:24, Train loss:0.001534, valid loss:0.000963
Epoch:25, Train loss:0.001598, valid loss:0.000966
Epoch:26, Train loss:0.001500, valid loss:0.000995
Epoch:27, Train loss:0.001560, valid loss:0.000951
Epoch:28, Train loss:0.001513, valid loss:0.000895
Epoch:29, Train loss:0.001490, valid loss:0.000954
Epoch:30, Train loss:0.001536, valid loss:0.001182
Epoch:31, Train loss:0.001234, valid loss:0.000855
Epoch:32, Train loss:0.001220, valid loss:0.000859
Epoch:33, Train loss:0.001226, valid loss:0.000864
Epoch:34, Train loss:0.001237, valid loss:0.000874
Epoch:35, Train loss:0.001220, valid loss:0.000825
Epoch:36, Train loss:0.001192, valid loss:0.000856
Epoch:37, Train loss:0.001222, valid loss:0.000852
Epoch:38, Train loss:0.001168, valid loss:0.000874
Epoch:39, Train loss:0.001201, valid loss:0.000822
Epoch:40, Train loss:0.001169, valid loss:0.000897
Epoch:41, Train loss:0.001056, valid loss:0.000773
Epoch:42, Train loss:0.001054, valid loss:0.000898
Epoch:43, Train loss:0.001060, valid loss:0.000791
Epoch:44, Train loss:0.001054, valid loss:0.000793
Epoch:45, Train loss:0.001055, valid loss:0.000876
Epoch:46, Train loss:0.001039, valid loss:0.000792
Epoch:47, Train loss:0.001039, valid loss:0.000783
Epoch:48, Train loss:0.001034, valid loss:0.000768
Epoch:49, Train loss:0.001033, valid loss:0.000788
Epoch:50, Train loss:0.001034, valid loss:0.000796
Epoch:51, Train loss:0.000977, valid loss:0.000759
Epoch:52, Train loss:0.000970, valid loss:0.000775
Epoch:53, Train loss:0.000971, valid loss:0.000743
Epoch:54, Train loss:0.000977, valid loss:0.000771
Epoch:55, Train loss:0.000963, valid loss:0.000757
Epoch:56, Train loss:0.000971, valid loss:0.000778
Epoch:57, Train loss:0.000962, valid loss:0.000749
Epoch:58, Train loss:0.000958, valid loss:0.000757
Epoch:59, Train loss:0.000972, valid loss:0.000771
Epoch:60, Train loss:0.000959, valid loss:0.000752
training time 6956.509035348892
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.0216532457832145
plot_id,batch_id 0 1 miss% 0.018190933668732093
plot_id,batch_id 0 2 miss% 0.027724994392759432
plot_id,batch_id 0 3 miss% 0.02872651290126651
plot_id,batch_id 0 4 miss% 0.033929821837211535
plot_id,batch_id 0 5 miss% 0.03215510120828964
plot_id,batch_id 0 6 miss% 0.030773782250735973
plot_id,batch_id 0 7 miss% 0.03108627137217914
plot_id,batch_id 0 8 miss% 0.025003460990329497
plot_id,batch_id 0 9 miss% 0.02607625072328538
plot_id,batch_id 0 10 miss% 0.037453879331224224
plot_id,batch_id 0 11 miss% 0.03912851041691243
plot_id,batch_id 0 12 miss% 0.02503514661308033
plot_id,batch_id 0 13 miss% 0.028230964861641058
plot_id,batch_id 0 14 miss% 0.03544782143320312
plot_id,batch_id 0 15 miss% 0.04612854991517434
plot_id,batch_id 0 16 miss% 0.03582798417490981
plot_id,batch_id 0 17 miss% 0.030396855026976533
plot_id,batch_id 0 18 miss% 0.037661993850805535
plot_id,batch_id 0 19 miss% 0.032034855143778934
plot_id,batch_id 0 20 miss% 0.03015966368679936
plot_id,batch_id 0 21 miss% 0.021450698204434872
plot_id,batch_id 0 22 miss% 0.034102137073794093
plot_id,batch_id 0 23 miss% 0.03188915939659511
plot_id,batch_id 0 24 miss% 0.034485817243511595
plot_id,batch_id 0 25 miss% 0.04246219330497302
plot_id,batch_id 0 26 miss% 0.032071563888539906
plot_id,batch_id 0 27 miss% 0.029679974767031832
plot_id,batch_id 0 28 miss% 0.030674371399513147
plot_id,batch_id 0 29 miss% 0.03271858156462585
plot_id,batch_id 0 30 miss% 0.0510923421441398
plot_id,batch_id 0 31 miss% 0.028245249935065476
plot_id,batch_id 0 32 miss% 0.03385127074235424
plot_id,batch_id 0 33 miss% 0.03628417305892689
plot_id,batch_id 0 34 miss% 0.03681689180706849
plot_id,batch_id 0 35 miss% 0.05906398310939192
plot_id,batch_id 0 36 miss% 0.037972443650876354
plot_id,batch_id 0 37 miss% 0.030937847215595737
plot_id,batch_id 0 38 miss% 0.031044173374150722
plot_id,batch_id 0 39 miss% 0.022443673637168537
plot_id,batch_id 0 40 miss% 0.05984355068444151
plot_id,batch_id 0 41 miss% 0.03303386313697658
plot_id,batch_id 0 42 miss% 0.021056319949271755
plot_id,batch_id 0 43 miss% 0.034146650404830156
plot_id,batch_id 0 44 miss% 0.026719454060944885
plot_id,batch_id 0 45 miss% 0.031169128225128424
plot_id,batch_id 0 46 miss% 0.024999349570094656
plot_id,batch_id 0 47 miss% 0.03508254172144253
plot_id,batch_id 0 48 miss% 0.03047252891663949
plot_id,batch_id 0 49 miss% 0.023220808352091345
plot_id,batch_id 0 50 miss% 0.04587109146535654
plot_id,batch_id 0 51 miss% 0.04043412677089292
plot_id,batch_id 0 52 miss% 0.03200359417119914
plot_id,batch_id 0 53 miss% 0.017305620030062842
plot_id,batch_id 0 54 miss% 0.030807839337256997
plot_id,batch_id 0 55 miss% 0.043311958620225445
plot_id,batch_id 0 56 miss% 0.03039373300155001
plot_id,batch_id 0 57 miss% 0.026842507103710066
plot_id,batch_id 0 58 miss% 0.027306091687833402
plot_id,batch_id 0 59 miss% 0.024501672217050754
plot_id,batch_id 0 60 miss% 0.029188163258118082
plot_id,batch_id 0 61 miss% 0.0228176424607343
plot_id,batch_id 0 62 miss% 0.026286615785751224
plot_id,batch_id 0 63 miss% 0.03213288713014846
plot_id,batch_id 0 64 miss% 0.023257681604064835
plot_id,batch_id 0 65 miss% 0.045964494781762635
plot_id,batch_id 0 66 miss% 0.04542882876753644
plot_id,batch_id 0 67 miss% 0.023206922781613844
plot_id,batch_id 0 68 miss% 0.03247735341959301
plot_id,batch_id 0 69 miss% 0.02239991609239037
plot_id,batch_id 0 70 miss% 0.04270607691675676
plot_id,batch_id 0 71 miss% 0.0478049130311916
plot_id,batch_id 0 72 miss% 0.03203728499212292
plot_id,batch_id 0 73 miss% 0.027840583696512385
plot_id,batch_id 0 74 miss% 0.03089060733329305
plot_id,batch_id 0 75 miss% 0.056782595450761274
plot_id,batch_id 0 76 miss% 0.03906624180398633
plot_id,batch_id 0 77 miss% 0.03637292108581015
plot_id,batch_id 0 78 miss% 0.042715295365631005
plot_id,batch_id 0 79 miss% 0.034428827300959065
plot_id,batch_id 0 80 miss% 0.044125010117720694
plot_id,batch_id 0 81 miss% 0.021844334685665108
plot_id,batch_id 0 82 miss% 0.029936066091724434
plot_id,batch_id 0 83 miss% 0.03308665806493163
plot_id,batch_id 0 84 miss% 0.033478928198829486
plot_id,batch_id 0 85 miss% 0.050985200034775933
plot_id,batch_id 0 86 miss% 0.02439332625186794
plot_id,batch_id 0 87 miss% 0.024828886095315528
plot_id,batch_id 0 88 miss% 0.046810340731865546
plot_id,batch_id 0 89 miss% 0.027200154870678197
plot_id,batch_id 0 90 miss% 0.0489504041727185
plot_id,batch_id 0 91 miss% 0.03855538780753007
plot_id,batch_id 0 92 miss% 0.02858223868660752
plot_id,batch_id 0 93 miss% 0.02046995316551927
plot_id,batch_id 0 94 miss% 0.043611259722874954
plot_id,batch_id 0 95 miss% 0.06396083937278131
plot_id,batch_id 0 96 miss% 0.035561429600267966
plot_id,batch_id 0 97 miss% 0.043043494859273446
plot_id,batch_id 0 98 miss% 0.04492898468961685
plot_id,batch_id 0 99 miss% 0.026631165581203542
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02165325 0.01819093 0.02772499 0.02872651 0.03392982 0.0321551
 0.03077378 0.03108627 0.02500346 0.02607625 0.03745388 0.03912851
 0.02503515 0.02823096 0.03544782 0.04612855 0.03582798 0.03039686
 0.03766199 0.03203486 0.03015966 0.0214507  0.03410214 0.03188916
 0.03448582 0.04246219 0.03207156 0.02967997 0.03067437 0.03271858
 0.05109234 0.02824525 0.03385127 0.03628417 0.03681689 0.05906398
 0.03797244 0.03093785 0.03104417 0.02244367 0.05984355 0.03303386
 0.02105632 0.03414665 0.02671945 0.03116913 0.02499935 0.03508254
 0.03047253 0.02322081 0.04587109 0.04043413 0.03200359 0.01730562
 0.03080784 0.04331196 0.03039373 0.02684251 0.02730609 0.02450167
 0.02918816 0.02281764 0.02628662 0.03213289 0.02325768 0.04596449
 0.04542883 0.02320692 0.03247735 0.02239992 0.04270608 0.04780491
 0.03203728 0.02784058 0.03089061 0.0567826  0.03906624 0.03637292
 0.0427153  0.03442883 0.04412501 0.02184433 0.02993607 0.03308666
 0.03347893 0.0509852  0.02439333 0.02482889 0.04681034 0.02720015
 0.0489504  0.03855539 0.02858224 0.02046995 0.04361126 0.06396084
 0.03556143 0.04304349 0.04492898 0.02663117]
for model  121 the mean error 0.03373425490387772
all id 121 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 25 out win 4 err 0.03373425490387772
Launcher: Job 122 completed in 7155 seconds.
Launcher: Task 28 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  79249
Epoch:0, Train loss:0.438591, valid loss:0.413955
Epoch:1, Train loss:0.029191, valid loss:0.005302
Epoch:2, Train loss:0.008290, valid loss:0.003442
Epoch:3, Train loss:0.006033, valid loss:0.002983
Epoch:4, Train loss:0.004968, valid loss:0.003664
Epoch:5, Train loss:0.004618, valid loss:0.002346
Epoch:6, Train loss:0.004046, valid loss:0.002112
Epoch:7, Train loss:0.003835, valid loss:0.002583
Epoch:8, Train loss:0.003739, valid loss:0.002334
Epoch:9, Train loss:0.003506, valid loss:0.002761
Epoch:10, Train loss:0.003391, valid loss:0.002092
Epoch:11, Train loss:0.002357, valid loss:0.001261
Epoch:12, Train loss:0.002359, valid loss:0.001407
Epoch:13, Train loss:0.002239, valid loss:0.001112
Epoch:14, Train loss:0.002267, valid loss:0.001310
Epoch:15, Train loss:0.002283, valid loss:0.001315
Epoch:16, Train loss:0.002193, valid loss:0.001433
Epoch:17, Train loss:0.002097, valid loss:0.001343
Epoch:18, Train loss:0.002056, valid loss:0.001202
Epoch:19, Train loss:0.002037, valid loss:0.001291
Epoch:20, Train loss:0.001970, valid loss:0.001212
Epoch:21, Train loss:0.001489, valid loss:0.001031
Epoch:22, Train loss:0.001472, valid loss:0.000943
Epoch:23, Train loss:0.001489, valid loss:0.001012
Epoch:24, Train loss:0.001445, valid loss:0.001213
Epoch:25, Train loss:0.001478, valid loss:0.000969
Epoch:26, Train loss:0.001434, valid loss:0.000970
Epoch:27, Train loss:0.001382, valid loss:0.001060
Epoch:28, Train loss:0.001387, valid loss:0.000961
Epoch:29, Train loss:0.001381, valid loss:0.001007
Epoch:30, Train loss:0.001398, valid loss:0.000967
Epoch:31, Train loss:0.001126, valid loss:0.000834
Epoch:32, Train loss:0.001108, valid loss:0.000886
Epoch:33, Train loss:0.001120, valid loss:0.000877
Epoch:34, Train loss:0.001094, valid loss:0.000846
Epoch:35, Train loss:0.001128, valid loss:0.000837
Epoch:36, Train loss:0.001086, valid loss:0.000819
Epoch:37, Train loss:0.001091, valid loss:0.000804
Epoch:38, Train loss:0.001071, valid loss:0.000936
Epoch:39, Train loss:0.001080, valid loss:0.000905
Epoch:40, Train loss:0.001065, valid loss:0.000811
Epoch:41, Train loss:0.000960, valid loss:0.000775
Epoch:42, Train loss:0.000939, valid loss:0.000765
Epoch:43, Train loss:0.000949, valid loss:0.000770
Epoch:44, Train loss:0.000946, valid loss:0.000814
Epoch:45, Train loss:0.000946, valid loss:0.000751
Epoch:46, Train loss:0.000929, valid loss:0.000755
Epoch:47, Train loss:0.000939, valid loss:0.000819
Epoch:48, Train loss:0.000932, valid loss:0.000766
Epoch:49, Train loss:0.000933, valid loss:0.000730
Epoch:50, Train loss:0.000920, valid loss:0.000722
Epoch:51, Train loss:0.000865, valid loss:0.000727
Epoch:52, Train loss:0.000863, valid loss:0.000719
Epoch:53, Train loss:0.000858, valid loss:0.000726
Epoch:54, Train loss:0.000859, valid loss:0.000740
Epoch:55, Train loss:0.000858, valid loss:0.000727
Epoch:56, Train loss:0.000856, valid loss:0.000719
Epoch:57, Train loss:0.000855, valid loss:0.000735
Epoch:58, Train loss:0.000849, valid loss:0.000722
Epoch:59, Train loss:0.000848, valid loss:0.000747
Epoch:60, Train loss:0.000849, valid loss:0.000699
training time 6993.2731239795685
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.027678812953108672
plot_id,batch_id 0 1 miss% 0.021992264822558425
plot_id,batch_id 0 2 miss% 0.02686777395931073
plot_id,batch_id 0 3 miss% 0.014184510198824753
plot_id,batch_id 0 4 miss% 0.017627627888736768
plot_id,batch_id 0 5 miss% 0.04778700134653565
plot_id,batch_id 0 6 miss% 0.03235207106721552
plot_id,batch_id 0 7 miss% 0.025709353672125883
plot_id,batch_id 0 8 miss% 0.02120679536690251
plot_id,batch_id 0 9 miss% 0.02122137975430161
plot_id,batch_id 0 10 miss% 0.044045650239098004
plot_id,batch_id 0 11 miss% 0.036100230667595656
plot_id,batch_id 0 12 miss% 0.02230841038601387
plot_id,batch_id 0 13 miss% 0.02250871035432033
plot_id,batch_id 0 14 miss% 0.025005995838535737
plot_id,batch_id 0 15 miss% 0.03801120161277556
plot_id,batch_id 0 16 miss% 0.03545390568382988
plot_id,batch_id 0 17 miss% 0.04296376493422395
plot_id,batch_id 0 18 miss% 0.025907270456670493
plot_id,batch_id 0 19 miss% 0.03313674264058808
plot_id,batch_id 0 20 miss% 0.04612459285887914
plot_id,batch_id 0 21 miss% 0.0176508534115828
plot_id,batch_id 0 22 miss% 0.02967727555836136
plot_id,batch_id 0 23 miss% 0.02212327253679756
plot_id,batch_id 0 24 miss% 0.020208855046953057
plot_id,batch_id 0 25 miss% 0.03626517018768538
plot_id,batch_id 0 26 miss% 0.024175135274725004
plot_id,batch_id 0 27 miss% 0.022808594457741936
plot_id,batch_id 0 28 miss% 0.026694614798268415
plot_id,batch_id 0 29 miss% 0.02376065115824167
plot_id,batch_id 0 30 miss% 0.035737154735348574
plot_id,batch_id 0 31 miss% 0.03669768816261606
plot_id,batch_id 0 32 miss% 0.024504784417425367
plot_id,batch_id 0 33 miss% 0.02162854462080647
plot_id,batch_id 0 34 miss% 0.025507922480415248
plot_id,batch_id 0 35 miss% 0.042319362562142325
plot_id,batch_id 0 36 miss% 0.0324149008273794
plot_id,batch_id 0 37 miss% 0.03548819395579814
plot_id,batch_id 0 38 miss% 0.026806295333334965
plot_id,batch_id 0 39 miss% 0.015005105163506617
plot_id,batch_id 0 40 miss% 0.05391441651248425
plot_id,batch_id 0 41 miss% 0.016737418507174397
plot_id,batch_id 0 42 miss% 0.011036069368617271
plot_id,batch_id 0 43 miss% 0.03369419364329236
plot_id,batch_id 0 44 miss% 0.02627529729814595
plot_id,batch_id 0 45 miss% 0.02583153776269929
plot_id,batch_id 0 46 miss% 0.02213727350827939
plot_id,batch_id 0 47 miss% 0.019122563371392644
plot_id,batch_id 0 48 miss% 0.030663656545238272
plot_id,batch_id 0 49 miss% 0.017944372105279282
plot_id,batch_id 0 50 miss% 0.02357233519971445
plot_id,batch_id 0 51 miss% 0.01967480345728615
plot_id,batch_id 0 52 miss% 0.026315159808363204
plot_id,batch_id 0 53 miss% 0.013168485272901024
plot_id,batch_id 0 54 miss% 0.03237131755415376
plot_id,batch_id 0 55 miss% 0.040138409673027876
plot_id,batch_id 0 56 miss% 0.01526438451697327
plot_id,batch_id 0 57 miss% 0.023492302697654923
plot_id,batch_id 0 58 miss% 0.021321426224731932
plot_id,batch_id 0 59 miss% 0.02110411214253945
plot_id,batch_id 0 60 miss% 0.047856369522051
plot_id,batch_id 0 61 miss% 0.01884187778331297
plot_id,batch_id 0 62 miss% 0.026857889600051425
plot_id,batch_id 0 63 miss% 0.029527994408867914
plot_id,batch_id 0 64 miss% 0.02541364392219924
plot_id,batch_id 0 65 miss% 0.038122084900256516
plot_id,batch_id 0 66 miss% 0.04950136934986692
plot_id,batch_id 0 67 miss% 0.03771172228268789
plot_id,batch_id 0 68 miss% 0.024187083479596142
plot_id,batch_id 0 69 the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  46193
Epoch:0, Train loss:0.507051, valid loss:0.475426
Epoch:1, Train loss:0.358835, valid loss:0.362518
Epoch:2, Train loss:0.347306, valid loss:0.360232
Epoch:3, Train loss:0.345416, valid loss:0.360126
Epoch:4, Train loss:0.344519, valid loss:0.359187
Epoch:5, Train loss:0.343984, valid loss:0.359188
Epoch:6, Train loss:0.343562, valid loss:0.359285
Epoch:7, Train loss:0.343144, valid loss:0.358870
Epoch:8, Train loss:0.343119, valid loss:0.358860
Epoch:9, Train loss:0.342811, valid loss:0.358573
Epoch:10, Train loss:0.342645, valid loss:0.358746
Epoch:11, Train loss:0.341922, valid loss:0.358280
Epoch:12, Train loss:0.341879, valid loss:0.358129
Epoch:13, Train loss:0.341835, valid loss:0.358418
Epoch:14, Train loss:0.341820, valid loss:0.358189
Epoch:15, Train loss:0.341729, valid loss:0.358345
Epoch:16, Train loss:0.341729, valid loss:0.358331
Epoch:17, Train loss:0.341659, valid loss:0.358117
Epoch:18, Train loss:0.341663, valid loss:0.358161
Epoch:19, Train loss:0.341615, valid loss:0.358062
Epoch:20, Train loss:0.341625, valid loss:0.358165
Epoch:21, Train loss:0.341215, valid loss:0.357840
Epoch:22, Train loss:0.341180, valid loss:0.357996
Epoch:23, Train loss:0.341189, valid loss:0.358004
Epoch:24, Train loss:0.341193, valid loss:0.357799
Epoch:25, Train loss:0.341188, valid loss:0.357967
Epoch:26, Train loss:0.341137, valid loss:0.357773
Epoch:27, Train loss:0.341138, valid loss:0.357784
Epoch:28, Train loss:0.341088, valid loss:0.357859
Epoch:29, Train loss:0.341142, valid loss:0.357908
Epoch:30, Train loss:0.341117, valid loss:0.357829
Epoch:31, Train loss:0.340920, valid loss:0.357810
Epoch:32, Train loss:0.340915, valid loss:0.357746
Epoch:33, Train loss:0.340906, valid loss:0.357767
Epoch:34, Train loss:0.340904, valid loss:0.357745
Epoch:35, Train loss:0.340906, valid loss:0.357713
Epoch:36, Train loss:0.340881, valid loss:0.357830
Epoch:37, Train loss:0.340885, valid loss:0.357766
Epoch:38, Train loss:0.340867, valid loss:0.357800
Epoch:39, Train loss:0.340868, valid loss:0.357751
Epoch:40, Train loss:0.340873, valid loss:0.357730
Epoch:41, Train loss:0.340774, valid loss:0.357720
Epoch:42, Train loss:0.340763, valid loss:0.357709
Epoch:43, Train loss:0.340773, valid loss:0.357697
Epoch:44, Train loss:0.340771, valid loss:0.357707
Epoch:45, Train loss:0.340764, valid loss:0.357708
Epoch:46, Train loss:0.340762, valid loss:0.357702
Epoch:47, Train loss:0.340747, valid loss:0.357719
Epoch:48, Train loss:0.340764, valid loss:0.357700
Epoch:49, Train loss:0.340746, valid loss:0.357697
Epoch:50, Train loss:0.340740, valid loss:0.357741
Epoch:51, Train loss:0.340706, valid loss:0.357684
Epoch:52, Train loss:0.340704, valid loss:0.357689
Epoch:53, Train loss:0.340697, valid loss:0.357678
Epoch:54, Train loss:0.340698, valid loss:0.357684
Epoch:55, Train loss:0.340695, valid loss:0.357686
Epoch:56, Train loss:0.340695, valid loss:0.357688
Epoch:57, Train loss:0.340693, valid loss:0.357684
Epoch:58, Train loss:0.340693, valid loss:0.357695
Epoch:59, Train loss:0.340692, valid loss:0.357680
Epoch:60, Train loss:0.340691, valid loss:0.357676
training time 7013.351707935333
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7796750760351013
plot_id,batch_id 0 1 miss% 0.829140544108412
plot_id,batch_id 0 2 miss% 0.8343082489625865
plot_id,batch_id 0 3 miss% 0.8398429934080146
plot_id,batch_id 0 4 miss% 0.8428906284450791
plot_id,batch_id 0 5 miss% 0.7751862442110954
plot_id,batch_id 0 6 miss% 0.8243105598636602
plot_id,batch_id 0 7 miss% 0.8359640497952668
plot_id,batch_id 0 8 miss% 0.8402619698626593
plot_id,batch_id 0 9 miss% 0.846991511586753
plot_id,batch_id 0 10 miss% 0.7645385097960837
plot_id,batch_id 0 11 miss% 0.824936130932089
plot_id,batch_id 0 12 miss% 0.8331119383766629
plot_id,batch_id 0 13 miss% 0.8379453178932381
plot_id,batch_id 0 14 miss% 0.8399516144333459
plot_id,batch_id 0 15 miss% 0.7675429140126523
plot_id,batch_id 0 16 miss% 0.8234694003117037
plot_id,batch_id 0 17 miss% 0.8338127647447029
plot_id,batch_id 0 18 miss% 0.839020275140772
plot_id,batch_id 0 19 miss% 0.842109382815684
plot_id,batch_id 0 20 miss% 0.8154761794118424
plot_id,batch_id 0 21 miss% 0.8356390414620196
plot_id,batch_id 0 22 miss% 0.8400909885110396
plot_id,batch_id 0 23 miss% 0.8437082825308179
plot_id,batch_id 0 24 miss% 0.8457382517712916
plot_id,batch_id 0 25 miss% 0.8042218729061077
plot_id,batch_id 0 26 miss% 0.8352245148731892
plot_id,batch_id 0 27 miss% 0.8391409656238284
plot_id,batch_id 0 28 miss% 0.8420359421637489
plot_id,batch_id 0 29 miss% 0.8442739132961841
plot_id,batch_id 0 30 miss% 0.7930706732189303
plot_id,batch_id 0 31 miss% 0.8304476769582209
plot_id,batch_id 0 32 miss% 0.8405050513153525
plot_id,batch_id 0 33 miss% 0.8421156267308613
plot_id,batch_id 0 34 miss% 0.8432571412470022
plot_id,batch_id 0 35 miss% 0.7912669906413258
plot_id,batch_id 0 36 miss% 0.83552228433816
plot_id,batch_id 0 37 miss% 0.8342053813546245
plot_id,batch_id 0 38 miss% 0.8443998092411586
plot_id,batch_id 0 39 miss% 0.844005125937795
plot_id,batch_id 0 40 miss% 0.8211893333141644
plot_id,batch_id 0 41 miss% 0.8403278031667638
plot_id,batch_id 0 42 miss% 0.8395287016813122
plot_id,batch_id 0 43 miss% 0.8454042391094847
plot_id,batch_id 0 44 miss% 0.8506126562574443
plot_id,batch_id 0 45 miss% 0.8196518867120247
plot_id,batch_id 0 46 miss% 0.8401690645640111
plot_id,batch_id 0 47 miss% 0.8404188659351589
plot_id,batch_id 0 48 miss% 0.8456337426569042
plot_id,batch_id 0 49 miss% 0.8504676020991145
plot_id,batch_id 0 50 miss% 0.8261574566315788
plot_id,batch_id 0 51 miss% 0.8384098685351152
plot_id,batch_id 0 52 miss% 0.8414357505640391
plot_id,batch_id 0 53 miss% 0.8459097542294272
plot_id,batch_id 0 54 miss% 0.8454881472753603
plot_id,batch_id 0 55 miss% 0.8182180456587456
plot_id,batch_id 0 56 miss% 0.8384442947543521
plot_id,batch_id 0 57 miss% 0.8417057960243811
plot_id,batch_id 0 58 miss% 0.8453458646350882
plot_id,batch_id 0 59 miss% 0.8446409611688298
plot_id,batch_id 0 60 miss% 0.7180405793109125
plot_id,batch_id 0 61 miss% 0.8133389458431174
plot_id,batch_id 0 62 miss% 0.8230928742999576
plot_id,batch_id 0 63 miss% 0.829800184427332
plot_id,batch_id 0 64 miss% 0.8381666166814304
plot_id,batch_id 0 65 miss% 0.7165572432990813
plot_id,batch_id 0 66 miss% 0.7961907523511564
plot_id,batch_id 0 67 miss% 0.8109480217886441
plot_id,batch_id 0 68 miss% 0.8295057267450282
plot_id,batch_id 0 69 miss% 0.8313875940448723
plot_id,batch_id 0 70 miss% 0.6765372849186104
plot_id,batch_id 0 71 miss% 0.8080603813517307
plot_id,batch_id 0 72 miss% 0.8074968538522401
plot_id,batch_id 0 73 miss% 0.8181953013774855
plot_id,batch_id 0 74 miss% 0.8299661896956684
plot_id,batch_id 0 75 miss% 0.6850450408979779
plot_id,batch_id 0 76 miss% 0.7924011374607847
plot_id,batch_id 0 77 miss% 0.7953737328972253
plot_id,batch_id 0 78 miss% 0.8177329216970161
plot_id,batch_id 0 79 miss% 0.8203815926799292
plot_id,batch_id 0 80 miss% 0.7468791322219974
plot_id,batch_id 0 81 miss% 0.8200203382779714
plot_id,batch_id 0 82 miss% 0.8306717978465809
plot_id,batch_id 0 83 miss% 0.8354625698354609
plot_id,batch_id 0 84 miss% 0.8396933922225914
plot_id,batch_id 0 85 miss% 0.7369636466364335
plot_id,batch_id 0 86 miss% 0.8105351358587449
plot_id,batch_id 0 87 miss% 0.8279177924009006
plot_id,batch_id 0 88 miss% 0.8356425121698033
plot_id,batch_id 0 89 miss% 0.8351866668031613
plot_id,batch_id 0 90 miss% 0.7019710694350971
plot_id,batch_id 0 91 miss% 0.8011772021470357
plot_id,batch_id 0 92 miss% 0.824433253873479
plot_id,batch_id 0 93 miss% 0.8251692632473541
plot_id,batch_id 0 94 miss% 0.8386874413445824
plot_id,batch_id 0 95 miss% 0.7267977983265049
plot_id,batch_id 0 96 miss% 0.7977032312004375
plot_id,batch_id 0 97 miss% 0.8155944583475949
plot_id,batch_id 0 98 miss% 0.8266861651291232
plot_id,batch_id 0 99 miss% 0.8298939570524064
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.77967508 0.82914054 0.83430825 0.83984299 0.84289063 0.77518624
 0.82431056 0.83596405 0.84026197 0.84699151 0.76453851 0.82493613
 0.83311194 0.83794532 0.83995161 0.76754291 0.8234694  0.83381276
 0.83902028 0.84210938 0.81547618 0.83563904 0.84009099 0.84370828
 0.84573825 0.80422187 0.83522451 0.83914097 0.84203594 0.84427391
 0.79307067 0.83044768 0.84050505 0.84211563 0.84325714 0.79126699
 0.83552228 0.83420538 0.84439981 0.84400513 0.82118933 0.8403278
 0.8395287  0.84540424 0.85061266 0.81965189 0.84016906 0.84041887
 0.84563374 0.8504676  0.82615746 0.83840987 0.84143575 0.84590975
 0.84548815 0.81821805 0.83844429 0.8417058  0.84534586 0.84464096
 0.71804058 0.81333895 0.82309287 0.82980018 0.83816662 0.71655724
 0.79619075 0.81094802 0.82950573 0.83138759 0.67653728 0.80806038
 0.80749685 0.8181953  0.82996619 0.68504504 0.79240114 0.79537373
 0.81773292 0.82038159 0.74687913 0.82002034 0.8306718  0.83546257
 0.83969339 0.73696365 0.81053514 0.82791779 0.83564251 0.83518667
 0.70197107 0.8011772  0.82443325 0.82516926 0.83868744 0.7267978
 0.79770323 0.81559446 0.82668617 0.82989396]
for model  86 the mean error 0.8184382142323982
all id 86 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 25 out win 5 err 0.8184382142323982
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  35921
Epoch:0, Train loss:0.543107, valid loss:0.533067
Epoch:1, Train loss:0.028410, valid loss:0.007031
Epoch:2, Train loss:0.007572, valid loss:0.003017
Epoch:3, Train loss:0.004527, valid loss:0.002535
Epoch:4, Train loss:0.003739, valid loss:0.002173
Epoch:5, Train loss:0.003068, valid loss:0.001964
Epoch:6, Train loss:0.002675, valid loss:0.001510
Epoch:7, Train loss:0.002448, valid loss:0.001255
Epoch:8, Train loss:0.002294, valid loss:0.001379
Epoch:9, Train loss:0.002166, valid loss:0.001922
Epoch:10, Train loss:0.002097, valid loss:0.001128
Epoch:11, Train loss:0.001655, valid loss:0.001129
Epoch:12, Train loss:0.001621, valid loss:0.001028
Epoch:13, Train loss:0.001596, valid loss:0.000976
Epoch:14, Train loss:0.001571, valid loss:0.000898
Epoch:15, Train loss:0.001514, valid loss:0.001026
Epoch:16, Train loss:0.001501, valid loss:0.001006
Epoch:17, Train loss:0.001457, valid loss:0.000823
Epoch:18, Train loss:0.001457, valid loss:0.000882
Epoch:19, Train loss:0.001434, valid loss:0.000878
Epoch:20, Train loss:0.001394, valid loss:0.000871
Epoch:21, Train loss:0.001169, valid loss:0.000749
Epoch:22, Train loss:0.001155, valid loss:0.000759
Epoch:23, Train loss:0.001155, valid loss:0.000707
Epoch:24, Train loss:0.001124, valid loss:0.000673
Epoch:25, Train loss:0.001121, valid loss:0.000767
Epoch:26, Train loss:0.001126, valid loss:0.000723
Epoch:27, Train loss:0.001109, valid loss:0.000688
Epoch:28, Train loss:0.001104, valid loss:0.000738
Epoch:29, Train loss:0.001091, valid loss:0.000659
Epoch:30, Train loss:0.001063, valid loss:0.000686
Epoch:31, Train loss:0.000956, valid loss:0.000630
Epoch:32, Train loss:0.000950, valid loss:0.000638
Epoch:33, Train loss:0.000956, valid loss:0.000609
Epoch:34, Train loss:0.000946, valid loss:0.000643
Epoch:35, Train loss:0.000942, valid loss:0.000645
Epoch:36, Train loss:0.000948, valid loss:0.000667
Epoch:37, Train loss:0.000931, valid loss:0.000625
Epoch:38, Train loss:0.000927, valid loss:0.000626
Epoch:39, Train loss:0.000923, valid loss:0.000665
Epoch:40, Train loss:0.000928, valid loss:0.000630
Epoch:41, Train loss:0.000859, valid loss:0.000593
Epoch:42, Train loss:0.000856, valid loss:0.000586
Epoch:43, Train loss:0.000855, valid loss:0.000591
Epoch:44, Train loss:0.000856, valid loss:0.000599
Epoch:45, Train loss:0.000855, valid loss:0.000591
Epoch:46, Train loss:0.000852, valid loss:0.000613
Epoch:47, Train loss:0.000845, valid loss:0.000595
Epoch:48, Train loss:0.000845, valid loss:0.000630
Epoch:49, Train loss:0.000842, valid loss:0.000609
Epoch:50, Train loss:0.000841, valid loss:0.000588
Epoch:51, Train loss:0.000813, valid loss:0.000598
Epoch:52, Train loss:0.000814, valid loss:0.000588
Epoch:53, Train loss:0.000811, valid loss:0.000586
Epoch:54, Train loss:0.000811, valid loss:0.000577
Epoch:55, Train loss:0.000809, valid loss:0.000579
Epoch:56, Train loss:0.000810, valid loss:0.000587
Epoch:57, Train loss:0.000807, valid loss:0.000578
Epoch:58, Train loss:0.000806, valid loss:0.000592
Epoch:59, Train loss:0.000803, valid loss:0.000574
Epoch:60, Train loss:0.000805, valid loss:0.000579
training time 7005.518435239792
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.04645375123606442
plot_id,batch_id 0 1 miss% 0.032816126341131555
plot_id,batch_id 0 2 miss% 0.028596027384991736
plot_id,batch_id 0 3 miss% 0.04071471308348492
plot_id,batch_id 0 4 miss% 0.019933426233261747
plot_id,batch_id 0 5 miss% 0.03760670256911879
plot_id,batch_id 0 6 miss% 0.046661142036239125
plot_id,batch_id 0 7 miss% 0.03598367051432972
plot_id,batch_id 0 8 miss% 0.01969650982528781
plot_id,batch_id 0 9 miss% 0.026637287862382775
plot_id,batch_id 0 10 miss% 0.061963536327906805
plot_id,batch_id 0 11 miss% 0.04147774866782171
plot_id,batch_id 0 12 miss% 0.04014158222480612
plot_id,batch_id 0 13 miss% 0.026425585632133687
plot_id,batch_id 0 14 miss% 0.031330141185779654
plot_id,batch_id 0 15 miss% 0.03898434170773508
plot_id,batch_id 0 16 miss% 0.04523550126624511
plot_id,batch_id 0 17 miss% 0.04953883227488482
plot_id,batch_id 0 18 miss% 0.040773156803041694
plot_id,batch_id 0 19 miss% 0.030372317927606477
plot_id,batch_id 0 20 miss% 0.04268614500823848
plot_id,batch_id 0 21 miss% 0.031979120140930024
plot_id,batch_id 0 22 miss% 0.03300980091238476
plot_id,batch_id 0 23 miss% 0.020578294841728708
plot_id,batch_id 0 24 miss% 0.021655892645802866
plot_id,batch_id 0 25 miss% 0.03502671359808669
plot_id,batch_id 0 26 miss% 0.03226175428228077
plot_id,batch_id 0 27 miss% 0.029043090198403766
plot_id,batch_id 0 28 miss% 0.021941510166836847
plot_id,batch_id 0 29 miss% 0.03150043113339703
plot_id,batch_id 0 30 miss% 0.04452376982921801
plot_id,batch_id 0 31 miss% 0.0493506640598345
plot_id,batch_id 0 32 miss% 0.021944560393425145
plot_id,batch_id 0 33 miss% 0.02857168479837644
plot_id,batch_id 0 34 miss% 0.01974816996705897
plot_id,batch_id 0 35 miss% 0.026548815025806152
plot_id,batch_id 0 36 miss% 0.03448996090084349
plot_id,batch_id 0 37 miss% 0.02523534639877998
plot_id,batch_id 0 38 miss% 0.022556942346455895
plot_id,batch_id 0 39 miss% 0.018437127494436767
plot_id,batch_id 0 40 miss% 0.0791121274442562
plot_id,batch_id 0 41 miss% 0.021201810389913717
plot_id,batch_id 0 42 miss% 0.014050878403998391
plot_id,batch_id 0 43 miss% 0.021173934209574242
plot_id,batch_id 0 44 miss% 0.03292055159495483
plot_id,batch_id 0 45 miss% 0.037558417685761916
plot_id,batch_id 0 46 miss% 0.038437515096994834
plot_id,batch_id 0 47 miss% 0.025456252588335254
plot_id,batch_id 0 48 miss% 0.026696848529908064
plot_id,batch_id 0 49 miss% 0.018030808711746213
plot_id,batch_id 0 50 miss% 0.0363757662830053
plot_id,batch_id 0 51 miss% 0.026190889812998587
plot_id,batch_id 0 52 miss% 0.020544089185030732
plot_id,batch_id 0 53 miss% 0.009262625990658066
plot_id,batch_id 0 54 miss% 0.028599742335245947
plot_id,batch_id 0 55 miss% 0.03749108280766144
plot_id,batch_id 0 56 miss% 0.027164981118347555
plot_id,batch_id 0 57 miss% 0.027167790757305365
plot_id,batch_id 0 58 miss% 0.019853764495088338
plot_id,batch_id 0 59 miss% 0.03201904625198528
plot_id,batch_id 0 60 miss% 0.05243253258925953
plot_id,batch_id 0 61 miss% 0.029512500012603592
plot_id,batch_id 0 62 miss% 0.027507330856949893
plot_id,batch_id 0 63 miss% 0.04130658829539844
plot_id,batch_id 0 64 miss% 0.03293842457859712
plot_id,batch_id 0 65 miss% 0.04430020674415668
plot_id,batch_id 0 66 miss% 0.026624031951545828
plot_id,batch_id 0 67 miss% 0.03742256009401101
Launcher: Job 87 completed in 7192 seconds.
Launcher: Task 131 done. Exiting.
miss% 0.02820771697531371
plot_id,batch_id 0 70 miss% 0.05927407813568089
plot_id,batch_id 0 71 miss% 0.07512813989311413
plot_id,batch_id 0 72 miss% 0.036415461261776395
plot_id,batch_id 0 73 miss% 0.029100983129685763
plot_id,batch_id 0 74 miss% 0.03311366671662455
plot_id,batch_id 0 75 miss% 0.042865793693613354
plot_id,batch_id 0 76 miss% 0.04390891041753954
plot_id,batch_id 0 77 miss% 0.044056998649862844
plot_id,batch_id 0 78 miss% 0.03120022069872281
plot_id,batch_id 0 79 miss% 0.03477496930646353
plot_id,batch_id 0 80 miss% 0.036780363268763945
plot_id,batch_id 0 81 miss% 0.026078380819783042
plot_id,batch_id 0 82 miss% 0.03569838212410665
plot_id,batch_id 0 83 miss% 0.02102765673750272
plot_id,batch_id 0 84 miss% 0.028406810278466216
plot_id,batch_id 0 85 miss% 0.03361300514554771
plot_id,batch_id 0 86 miss% 0.034667743233651654
plot_id,batch_id 0 87 miss% 0.030999752500674622
plot_id,batch_id 0 88 miss% 0.03768631962023417
plot_id,batch_id 0 89 miss% 0.01652893259747725
plot_id,batch_id 0 90 miss% 0.027378364551473692
plot_id,batch_id 0 91 miss% 0.0333031439569258
plot_id,batch_id 0 92 miss% 0.03220137131587162
plot_id,batch_id 0 93 miss% 0.025301382116284195
plot_id,batch_id 0 94 miss% 0.032215068377373986
plot_id,batch_id 0 95 miss% 0.059976098687525466
plot_id,batch_id 0 96 miss% 0.03637266356525174
plot_id,batch_id 0 97 miss% 0.04171349345142253
plot_id,batch_id 0 98 miss% 0.03561851802466882
plot_id,batch_id 0 99 miss% 0.02684609326401618
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02767881 0.02199226 0.02686777 0.01418451 0.01762763 0.047787
 0.03235207 0.02570935 0.0212068  0.02122138 0.04404565 0.03610023
 0.02230841 0.02250871 0.025006   0.0380112  0.03545391 0.04296376
 0.02590727 0.03313674 0.04612459 0.01765085 0.02967728 0.02212327
 0.02020886 0.03626517 0.02417514 0.02280859 0.02669461 0.02376065
 0.03573715 0.03669769 0.02450478 0.02162854 0.02550792 0.04231936
 0.0324149  0.03548819 0.0268063  0.01500511 0.05391442 0.01673742
 0.01103607 0.03369419 0.0262753  0.02583154 0.02213727 0.01912256
 0.03066366 0.01794437 0.02357234 0.0196748  0.02631516 0.01316849
 0.03237132 0.04013841 0.01526438 0.0234923  0.02132143 0.02110411
 0.04785637 0.01884188 0.02685789 0.02952799 0.02541364 0.03812208
 0.04950137 0.03771172 0.02418708 0.02820772 0.05927408 0.07512814
 0.03641546 0.02910098 0.03311367 0.04286579 0.04390891 0.044057
 0.03120022 0.03477497 0.03678036 0.02607838 0.03569838 0.02102766
 0.02840681 0.03361301 0.03466774 0.03099975 0.03768632 0.01652893
 0.02737836 0.03330314 0.03220137 0.02530138 0.03221507 0.0599761
 0.03637266 0.04171349 0.03561852 0.02684609]
for model  115 the mean error 0.0304992849432554
all id 115 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 25 out win 4 err 0.0304992849432554
Launcher: Job 116 completed in 7192 seconds.
Launcher: Task 19 done. Exiting.
plot_id,batch_id 0 68 miss% 0.02675157997961417
plot_id,batch_id 0 69 miss% 0.02092186070365256
plot_id,batch_id 0 70 miss% 0.03135518550817639
plot_id,batch_id 0 71 miss% 0.032383184024328625
plot_id,batch_id 0 72 miss% 0.03674603119180375
plot_id,batch_id 0 73 miss% 0.030024396726928786
plot_id,batch_id 0 74 miss% 0.05579357033812398
plot_id,batch_id 0 75 miss% 0.03913696114547941
plot_id,batch_id 0 76 miss% 0.028773005730229846
plot_id,batch_id 0 77 miss% 0.046979108721858794
plot_id,batch_id 0 78 miss% 0.043479964327405085
plot_id,batch_id 0 79 miss% 0.05280990429326654
plot_id,batch_id 0 80 miss% 0.05074510235967481
plot_id,batch_id 0 81 miss% 0.02235005954722298
plot_id,batch_id 0 82 miss% 0.0301346369399918
plot_id,batch_id 0 83 miss% 0.040268786081882134
plot_id,batch_id 0 84 miss% 0.032866430818973134
plot_id,batch_id 0 85 miss% 0.049052723974308675
plot_id,batch_id 0 86 miss% 0.039766361532317746
plot_id,batch_id 0 87 miss% 0.03299419830557991
plot_id,batch_id 0 88 miss% 0.029018580349871925
plot_id,batch_id 0 89 miss% 0.026692846958716567
plot_id,batch_id 0 90 miss% 0.044684482542878254
plot_id,batch_id 0 91 miss% 0.04093134363183191
plot_id,batch_id 0 92 miss% 0.029007948156606303
plot_id,batch_id 0 93 miss% 0.038156946488770084
plot_id,batch_id 0 94 miss% 0.038614903394477984
plot_id,batch_id 0 95 miss% 0.04092901348106492
plot_id,batch_id 0 96 miss% 0.044144660122923285
plot_id,batch_id 0 97 miss% 0.037709336317195426
plot_id,batch_id 0 98 miss% 0.03767690114197742
plot_id,batch_id 0 99 miss% 0.03620149521494853
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04645375 0.03281613 0.02859603 0.04071471 0.01993343 0.0376067
 0.04666114 0.03598367 0.01969651 0.02663729 0.06196354 0.04147775
 0.04014158 0.02642559 0.03133014 0.03898434 0.0452355  0.04953883
 0.04077316 0.03037232 0.04268615 0.03197912 0.0330098  0.02057829
 0.02165589 0.03502671 0.03226175 0.02904309 0.02194151 0.03150043
 0.04452377 0.04935066 0.02194456 0.02857168 0.01974817 0.02654882
 0.03448996 0.02523535 0.02255694 0.01843713 0.07911213 0.02120181
 0.01405088 0.02117393 0.03292055 0.03755842 0.03843752 0.02545625
 0.02669685 0.01803081 0.03637577 0.02619089 0.02054409 0.00926263
 0.02859974 0.03749108 0.02716498 0.02716779 0.01985376 0.03201905
 0.05243253 0.0295125  0.02750733 0.04130659 0.03293842 0.04430021
 0.02662403 0.03742256 0.02675158 0.02092186 0.03135519 0.03238318
 0.03674603 0.0300244  0.05579357 0.03913696 0.02877301 0.04697911
 0.04347996 0.0528099  0.0507451  0.02235006 0.03013464 0.04026879
 0.03286643 0.04905272 0.03976636 0.0329942  0.02901858 0.02669285
 0.04468448 0.04093134 0.02900795 0.03815695 0.0386149  0.04092901
 0.04414466 0.03770934 0.0376769  0.0362015 ]
for model  180 the mean error 0.03390886504113953
all id 180 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 31 out win 3 err 0.03390886504113953
Launcher: Job 181 completed in 7195 seconds.
Launcher: Task 169 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  21969
Epoch:0, Train loss:0.404525, valid loss:0.371485
Epoch:1, Train loss:0.228234, valid loss:0.230960
Epoch:2, Train loss:0.221569, valid loss:0.230921
Epoch:3, Train loss:0.220661, valid loss:0.230188
Epoch:4, Train loss:0.220267, valid loss:0.230214
Epoch:5, Train loss:0.220016, valid loss:0.229830
Epoch:6, Train loss:0.219780, valid loss:0.229815
Epoch:7, Train loss:0.219649, valid loss:0.229894
Epoch:8, Train loss:0.219515, valid loss:0.229944
Epoch:9, Train loss:0.219443, valid loss:0.229850
Epoch:10, Train loss:0.219358, valid loss:0.229426
Epoch:11, Train loss:0.218710, valid loss:0.229198
Epoch:12, Train loss:0.218696, valid loss:0.229460
Epoch:13, Train loss:0.218643, valid loss:0.229427
Epoch:14, Train loss:0.218588, valid loss:0.229327
Epoch:15, Train loss:0.218614, valid loss:0.229232
Epoch:16, Train loss:0.218558, valid loss:0.229316
Epoch:17, Train loss:0.218589, valid loss:0.229346
Epoch:18, Train loss:0.218525, valid loss:0.229258
Epoch:19, Train loss:0.218515, valid loss:0.229142
Epoch:20, Train loss:0.218470, valid loss:0.229301
Epoch:21, Train loss:0.218156, valid loss:0.229043
Epoch:22, Train loss:0.218145, valid loss:0.229129
Epoch:23, Train loss:0.218119, valid loss:0.228986
Epoch:24, Train loss:0.218126, valid loss:0.229068
Epoch:25, Train loss:0.218124, valid loss:0.229012
Epoch:26, Train loss:0.218109, valid loss:0.229104
Epoch:27, Train loss:0.218101, valid loss:0.229062
Epoch:28, Train loss:0.218079, valid loss:0.229199
Epoch:29, Train loss:0.218082, valid loss:0.229289
Epoch:30, Train loss:0.218081, valid loss:0.228981
Epoch:31, Train loss:0.217886, valid loss:0.228944
Epoch:32, Train loss:0.217888, valid loss:0.228967
Epoch:33, Train loss:0.217890, valid loss:0.228905
Epoch:34, Train loss:0.217880, valid loss:0.228979
Epoch:35, Train loss:0.217882, valid loss:0.228916
Epoch:36, Train loss:0.217880, valid loss:0.228970
Epoch:37, Train loss:0.217867, valid loss:0.228953
Epoch:38, Train loss:0.217863, valid loss:0.228945
Epoch:39, Train loss:0.217853, valid loss:0.228898
Epoch:40, Train loss:0.217866, valid loss:0.228878
Epoch:41, Train loss:0.217756, valid loss:0.228903
Epoch:42, Train loss:0.217760, valid loss:0.228883
Epoch:43, Train loss:0.217757, valid loss:0.228902
Epoch:44, Train loss:0.217764, valid loss:0.228902
Epoch:45, Train loss:0.217750, valid loss:0.228871
Epoch:46, Train loss:0.217743, valid loss:0.228851
Epoch:47, Train loss:0.217746, valid loss:0.228885
Epoch:48, Train loss:0.217740, valid loss:0.228928
Epoch:49, Train loss:0.217755, valid loss:0.228897
Epoch:50, Train loss:0.217741, valid loss:0.228880
Epoch:51, Train loss:0.217695, valid loss:0.228861
Epoch:52, Train loss:0.217688, valid loss:0.228872
Epoch:53, Train loss:0.217691, valid loss:0.228881
Epoch:54, Train loss:0.217683, valid loss:0.228858
Epoch:55, Train loss:0.217690, valid loss:0.228861
Epoch:56, Train loss:0.217686, valid loss:0.228853
Epoch:57, Train loss:0.217683, valid loss:0.228858
Epoch:58, Train loss:0.217680, valid loss:0.228884
Epoch:59, Train loss:0.217680, valid loss:0.228854
Epoch:60, Train loss:0.217680, valid loss:0.228856
training time 7039.341543197632
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.7310282805089471
plot_id,batch_id 0 1 miss% 0.7899016158805987
plot_id,batch_id 0 2 miss% 0.8019129614945262
plot_id,batch_id 0 3 miss% 0.8087641176881522
plot_id,batch_id 0 4 miss% 0.8067518459321326
plot_id,batch_id 0 5 miss% 0.7259026889337813
plot_id,batch_id 0 6 miss% 0.7827963271877014
plot_id,batch_id 0 7 miss% 0.7985341982274713
plot_id,batch_id 0 8 miss% 0.8043263773888394
plot_id,batch_id 0 9 miss% 0.8095050352947488
plot_id,batch_id 0 10 miss% 0.7009370870297866
plot_id,batch_id 0 11 miss% 0.783333153696409
plot_id,batch_id 0 12 miss% 0.7940749616315188
plot_id,batch_id 0 13 miss% 0.8036273071085341
plot_id,batch_id 0 14 miss% 0.80956755259787
plot_id,batch_id 0 15 miss% 0.718453814489178
plot_id,batch_id 0 16 miss% 0.7796711275963991
plot_id,batch_id 0 17 miss% 0.7968273497465683
plot_id,batch_id 0 18 miss% 0.8044817462395742
plot_id,batch_id 0 19 miss% 0.8087922183130593
plot_id,batch_id 0 20 miss% 0.7682259933384702
plot_id,batch_id 0 21 miss% 0.805181003323134
plot_id,batch_id 0 22 miss% 0.8100032873113245
plot_id,batch_id 0 23 miss% 0.8134471126213384
plot_id,batch_id 0 24 miss% 0.8151938572487624
plot_id,batch_id 0 25 miss% 0.7638042283965778
plot_id,batch_id 0 26 miss% 0.7990254366602332
plot_id,batch_id 0 27 miss% 0.8030878066233293
plot_id,batch_id 0 28 miss% 0.8081282410656372
plot_id,batch_id 0 29 miss% 0.8092764022238034
plot_id,batch_id 0 30 miss% 0.7543423391585565
plot_id,batch_id 0 31 miss% 0.7942689995078721
plot_id,batch_id 0 32 miss% 0.8032386193798312
plot_id,batch_id 0 33 miss% 0.8080325506644122
plot_id,batch_id 0 34 miss% 0.8083682683758717
plot_id,batch_id 0 35 miss% 0.7602593663271039
plot_id,batch_id 0 36 miss% 0.8015324175674605
plot_id,batch_id 0 37 miss% 0.8041541720612876
plot_id,batch_id 0 38 miss% 0.8082207584492651
plot_id,batch_id 0 39 miss% 0.8097323327840557
plot_id,batch_id 0 40 miss% 0.7819106400859199
plot_id,batch_id 0 41 miss% 0.805414739802588
plot_id,batch_id 0 42 miss% 0.8098787435278928
plot_id,batch_id 0 43 miss% 0.8122029840140864
plot_id,batch_id 0 44 miss% 0.8165492630227759
plot_id,batch_id 0 45 miss% 0.7763578101110326
plot_id,batch_id 0 46 miss% 0.8051990009558413
plot_id,batch_id 0 47 miss% 0.8126340082318512
plot_id,batch_id 0 48 miss% 0.8107786688119292
plot_id,batch_id 0 49 miss% 0.8154886766710554
plot_id,batch_id 0 50 miss% 0.7842704285725449
plot_id,batch_id 0 51 miss% 0.8033053457462619
plot_id,batch_id 0 52 miss% 0.807943202854806
plot_id,batch_id 0 53 miss% 0.809920748717849
plot_id,batch_id 0 54 miss% 0.8156496230280124
plot_id,batch_id 0 55 miss% 0.7602322601900491
plot_id,batch_id 0 56 miss% 0.8039020297033076
plot_id,batch_id 0 57 miss% 0.8067128515883886
plot_id,batch_id 0 58 miss% 0.8121181879647922
plot_id,batch_id 0 59 miss% 0.8151108420597816
plot_id,batch_id 0 60 miss% 0.6468548904735525
plot_id,batch_id 0 61 miss% 0.7488829423375931
plot_id,batch_id 0 62 miss% 0.787742480728282
plot_id,batch_id 0 63 miss% 0.7946417919937553
plot_id,batch_id 0 64 miss% 0.8031775704647466
plot_id,batch_id 0 65 miss% 0.6413367191646262
plot_id,batch_id 0 66 miss% 0.7511307419147324
plot_id,batch_id 0 67 miss% 0.77218913638949
plot_id,batch_id 0 68 miss% 0.7955054198152499
plot_id,batch_id 0 69 miss% 0.7982915360349179
plot_id,batch_id 0 70 miss% 0.6156801518734838
plot_id,batch_id 0 71 miss% 0.7635119747391986
plot_id,batch_id 0 72 miss% 0.7621008516657702
plot_id,batch_id 0 73 miss% 0.7797731571940011
plot_id,batch_id 0 74 miss% 0.7917620959040835
plot_id,batch_id 0 75 miss% 0.6133878410477809
plot_id,batch_id 0 76 miss% 0.7187753481558472
plot_id,batch_id 0 77 miss% 0.7502758545426242
plot_id,batch_id 0 78 miss% 0.7773551341957929
plot_id,batch_id 0 79 miss% 0.7869846090765588
plot_id,batch_id 0 80 miss% 0.6742079332037193
plot_id,batch_id 0 81 miss% 0.7769038027848427
plot_id,batch_id 0 82 miss% 0.7921986529677741
plot_id,batch_id 0 83 miss% 0.7998306875742399
plot_id,batch_id 0 84 miss% 0.8005445733922301
plot_id,batch_id 0 85 miss% 0.6727574699146054
plot_id,batch_id 0 86 miss% 0.7691058761796412
plot_id,batch_id 0 87 miss% 0.7902343191461935
plot_id,batch_id 0 88 miss% 0.797312805138521
plot_id,batch_id 0 89 miss% 0.7996287977879446
plot_id,batch_id 0 90 miss% 0.6418134791360176
plot_id,batch_id 0 91 miss% 0.7709961681388471
plot_id,batch_id 0 92 miss% 0.7778898952285126
plot_id,batch_id 0 93 miss% 0.7923639893548569
plot_id,batch_id 0 94 miss% 0.7994690838479673
plot_id,batch_id 0 95 miss% 0.6419507514440855
plot_id,batch_id 0 96 miss% 0.7480532699751458
plot_id,batch_id 0 97 miss% 0.7757708093995055
plot_id,batch_id 0 98 miss% 0.7865162774843077
plot_id,batch_id 0 99 miss% 0.7947938544537358
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73102828 0.78990162 0.80191296 0.80876412 0.80675185 0.72590269
 0.78279633 0.7985342  0.80432638 0.80950504 0.70093709 0.78333315
 0.79407496 0.80362731 0.80956755 0.71845381 0.77967113 0.79682735
 0.80448175 0.80879222 0.76822599 0.805181   0.81000329 0.81344711
 0.81519386 0.76380423 0.79902544 0.80308781 0.80812824 0.8092764
 0.75434234 0.794269   0.80323862 0.80803255 0.80836827 0.76025937
 0.80153242 0.80415417 0.80822076 0.80973233 0.78191064 0.80541474
 0.80987874 0.81220298 0.81654926 0.77635781 0.805199   0.81263401
 0.81077867 0.81548868 0.78427043 0.80330535 0.8079432  0.80992075
 0.81564962 0.76023226 0.80390203 0.80671285 0.81211819 0.81511084
 0.64685489 0.74888294 0.78774248 0.79464179 0.80317757 0.64133672
 0.75113074 0.77218914 0.79550542 0.79829154 0.61568015 0.76351197
 0.76210085 0.77977316 0.7917621  0.61338784 0.71877535 0.75027585
 0.77735513 0.78698461 0.67420793 0.7769038  0.79219865 0.79983069
 0.80054457 0.67275747 0.76910588 0.79023432 0.79731281 0.7996288
 0.64181348 0.77099617 0.7778899  0.79236399 0.79946908 0.64195075
 0.74805327 0.77577081 0.78651628 0.79479385]
for model  191 the mean error 0.7777399575999966
all id 191 hidden_dim 16 learning_rate 0.01 num_layers 3 frames 31 out win 5 err 0.7777399575999966
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  79249
Epoch:0, Train loss:0.438591, valid loss:0.413955
Epoch:1, Train loss:0.032303, valid loss:0.006516
Epoch:2, Train loss:0.008898, valid loss:0.004162
Epoch:3, Train loss:0.006869, valid loss:0.004234
Epoch:4, Train loss:0.005668, valid loss:0.002941
Epoch:5, Train loss:0.004968, valid loss:0.002829
Epoch:6, Train loss:0.004424, valid loss:0.002278
Epoch:7, Train loss:0.004072, valid loss:0.002217
Epoch:8, Train loss:0.003725, valid loss:0.002130
Epoch:9, Train loss:0.003599, valid loss:0.001928
Epoch:10, Train loss:0.003386, valid loss:0.001702
Epoch:11, Train loss:0.002484, valid loss:0.001369
Epoch:12, Train loss:0.002399, valid loss:0.001547
Epoch:13, Train loss:0.002406, valid loss:0.001194
Epoch:14, Train loss:0.002273, valid loss:0.001246
Epoch:15, Train loss:0.002291, valid loss:0.001260
Epoch:16, Train loss:0.002167, valid loss:0.001326
Epoch:17, Train loss:0.002174, valid loss:0.001363
Epoch:18, Train loss:0.002118, valid loss:0.001095
Epoch:19, Train loss:0.002042, valid loss:0.001120
Epoch:20, Train loss:0.002030, valid loss:0.001094
Epoch:21, Train loss:0.001614, valid loss:0.000946
Epoch:22, Train loss:0.001591, valid loss:0.001005
Epoch:23, Train loss:0.001599, valid loss:0.001004
Epoch:24, Train loss:0.001552, valid loss:0.000974
Epoch:25, Train loss:0.001517, valid loss:0.001016
Epoch:26, Train loss:0.001558, valid loss:0.001153
Epoch:27, Train loss:0.001492, valid loss:0.000912
Epoch:28, Train loss:0.001495, valid loss:0.000961
Epoch:29, Train loss:0.001458, valid loss:0.000882
Epoch:30, Train loss:0.001469, valid loss:0.000933
Epoch:31, Train loss:0.001260, valid loss:0.000836
Epoch:32, Train loss:0.001233, valid loss:0.000828
Epoch:33, Train loss:0.001261, valid loss:0.000927
Epoch:34, Train loss:0.001218, valid loss:0.000856
Epoch:35, Train loss:0.001235, valid loss:0.000805
Epoch:36, Train loss:0.001219, valid loss:0.000797
Epoch:37, Train loss:0.001206, valid loss:0.000894
Epoch:38, Train loss:0.001195, valid loss:0.000855
Epoch:39, Train loss:0.001191, valid loss:0.000840
Epoch:40, Train loss:0.001194, valid loss:0.000756
Epoch:41, Train loss:0.001089, valid loss:0.000787
Epoch:42, Train loss:0.001080, valid loss:0.000769
Epoch:43, Train loss:0.001088, valid loss:0.000796
Epoch:44, Train loss:0.001072, valid loss:0.000845
Epoch:45, Train loss:0.001077, valid loss:0.000745
Epoch:46, Train loss:0.001065, valid loss:0.000784
Epoch:47, Train loss:0.001065, valid loss:0.000817
Epoch:48, Train loss:0.001067, valid loss:0.000762
Epoch:49, Train loss:0.001059, valid loss:0.000742
Epoch:50, Train loss:0.001055, valid loss:0.000808
Epoch:51, Train loss:0.001008, valid loss:0.000740
Epoch:52, Train loss:0.001011, valid loss:0.000733
Epoch:53, Train loss:0.000999, valid loss:0.000722
Epoch:54, Train loss:0.000998, valid loss:0.000736
Epoch:55, Train loss:0.001000, valid loss:0.000755
Epoch:56, Train loss:0.000998, valid loss:0.000742
Epoch:57, Train loss:0.000993, valid loss:0.000737
Epoch:58, Train loss:0.000992, valid loss:0.000740
Epoch:59, Train loss:0.000990, valid loss:0.000762
Epoch:60, Train loss:0.000988, valid loss:0.000729
training time 7027.269499063492
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.032678089405225
plot_id,batch_id 0 1 miss% 0.021626527114368445
plot_id,batch_id 0 2 miss% 0.026909297686947723
plot_id,batch_id 0 3 miss% 0.026701879529351845
plot_id,batch_id 0 4 miss% 0.028006592509827722
plot_id,batch_id 0 5 miss% 0.03937268069028863
plot_id,batch_id 0 6 miss% 0.03806677699624708
plot_id,batch_id 0 7 miss% 0.030037734145694838
plot_id,batch_id 0 8 miss% 0.023882390513169913
plot_id,batch_id 0 9 miss% 0.029445507541583833
plot_id,batch_id 0 10 miss% 0.05755775647421938
plot_id,batch_id 0 11 miss% 0.05779569236658243
plot_id,batch_id 0 12 miss% 0.037849276040309
plot_id,batch_id 0 13 miss% 0.02945291574152519
plot_id,batch_id 0 14 miss% 0.030671252992472066
plot_id,batch_id 0 15 miss% 0.04088291525607477
plot_id,batch_id 0 16 miss% 0.03596185942653685
plot_id,batch_id 0 17 miss% 0.04988355948904361
plot_id,batch_id 0 18 miss% 0.03478776933733427
plot_id,batch_id 0 19 miss% 0.03247590160750606
plot_id,batch_id 0 20 miss% 0.03427075688337235
plot_id,batch_id 0 21 miss% 0.02712434827985386
plot_id,batch_id 0 22 miss% 0.028538875505517508
plot_id,batch_id 0 23 miss% 0.021148522719912802
plot_id,batch_id 0 24 miss% 0.021968143342326354
plot_id,batch_id 0 25 miss% 0.03046342846913107
plot_id,batch_id 0 26 miss% 0.024651725130667674
plot_id,batch_id 0 27 miss% 0.025067077625264715
plot_id,batch_id 0 28 miss% 0.03129313749442891
plot_id,batch_id 0 29 miss% 0.022000408650603903
plot_id,batch_id 0 30 miss% 0.039772308877832785
plot_id,batch_id 0 31 miss% 0.04487483022142116
plot_id,batch_id 0 32 miss% 0.03372475959525002
plot_id,batch_id 0 33 miss% 0.027278561297859355
plot_id,batch_id 0 34 miss% 0.020722682846173193
plot_id,batch_id 0 35 miss% 0.050235926182586356
plot_id,batch_id 0 36 miss% 0.043158293504827645
plot_id,batch_id 0 37 miss% 0.03942969348421774
plot_id,batch_id 0 38 miss% 0.03413007485781409
plot_id,batch_id 0 39 miss% 0.025289616046114814
plot_id,batch_id 0 40 miss% 0.0599636378776129
plot_id,batch_id 0 41 miss% 0.024192188783159232
plot_id,batch_id 0 42 miss% 0.0166049558824305
plot_id,batch_id 0 43 miss% 0.039431083298423784
plot_id,batch_id 0 44 miss% 0.026950045752789763
plot_id,batch_id 0 45 miss% 0.02841421074919587
plot_id,batch_id 0 46 miss% 0.031821189604948334
plot_id,batch_id 0 47 miss% 0.026162352492111105
plot_id,batch_id 0 48 miss% 0.027459273564587026
plot_id,batch_id 0 49 miss% 0.023858826208702288
plot_id,batch_id 0 50 miss% 0.04222442050963286
plot_id,batch_id 0 51 miss% 0.03192929501964233
plot_id,batch_id 0 52 miss% 0.030478092521040733
plot_id,batch_id 0 53 miss% 0.015073537962931677
plot_id,batch_id 0 54 miss% 0.03807620916184757
plot_id,batch_id 0 55 miss% 0.05651349970538208
plot_id,batch_id 0 56 miss% 0.02608522056865356
plot_id,batch_id 0 57 miss% 0.030733908455485966
plot_id,batch_id 0 58 miss% 0.02980583276434231
plot_id,batch_id 0 59 miss% 0.037421377459124265
plot_id,batch_id 0 60 miss% 0.033783864646310995
plot_id,batch_id 0 61 miss% 0.03634555070731465
plot_id,batch_id 0 62 miss% 0.03054935924062112
plot_id,batch_id 0 63 miss% 0.04600257136566339
plot_id,batch_id 0 64 miss% 0.02553747272190683
plot_id,batch_id 0 65 miss% 0.030001708109417685
plot_id,batch_id 0 66 miss% 0.03288125685078816
plot_id,batch_id 0 67 miss% 0.029288587323119262
plot_id,batch_id 0 68 miss% 0.025167030592081467
plot_id,batch_id 0 69 Launcher: Job 192 completed in 7217 seconds.
Launcher: Task 236 done. Exiting.
miss% 0.019825758630983556
plot_id,batch_id 0 70 miss% 0.035398453616593245
plot_id,batch_id 0 71 miss% 0.056095228861309625
plot_id,batch_id 0 72 miss% 0.038152406766261124
plot_id,batch_id 0 73 miss% 0.026026538730192075
plot_id,batch_id 0 74 miss% 0.04285544689098303
plot_id,batch_id 0 75 miss% 0.03853928984288589
plot_id,batch_id 0 76 miss% 0.03581245586305268
plot_id,batch_id 0 77 miss% 0.02906929549313062
plot_id,batch_id 0 78 miss% 0.03923914237039521
plot_id,batch_id 0 79 miss% 0.04040144100461905
plot_id,batch_id 0 80 miss% 0.036471715104287455
plot_id,batch_id 0 81 miss% 0.024149631896672425
plot_id,batch_id 0 82 miss% 0.025478067818482827
plot_id,batch_id 0 83 miss% 0.03936939334763895
plot_id,batch_id 0 84 miss% 0.025354754808230837
plot_id,batch_id 0 85 miss% 0.05097810359862678
plot_id,batch_id 0 86 miss% 0.02578683082180198
plot_id,batch_id 0 87 miss% 0.03519375581767379
plot_id,batch_id 0 88 miss% 0.05045673704062242
plot_id,batch_id 0 89 miss% 0.03683552294968444
plot_id,batch_id 0 90 miss% 0.03267207344720589
plot_id,batch_id 0 91 miss% 0.044797411035999275
plot_id,batch_id 0 92 miss% 0.02950562659076559
plot_id,batch_id 0 93 miss% 0.03296696590217267
plot_id,batch_id 0 94 miss% 0.04724660474737809
plot_id,batch_id 0 95 miss% 0.032916102478761256
plot_id,batch_id 0 96 miss% 0.030081937612837752
plot_id,batch_id 0 97 miss% 0.07067374726658415
plot_id,batch_id 0 98 miss% 0.046539101168627126
plot_id,batch_id 0 99 miss% 0.04340150708269956
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03267809 0.02162653 0.0269093  0.02670188 0.02800659 0.03937268
 0.03806678 0.03003773 0.02388239 0.02944551 0.05755776 0.05779569
 0.03784928 0.02945292 0.03067125 0.04088292 0.03596186 0.04988356
 0.03478777 0.0324759  0.03427076 0.02712435 0.02853888 0.02114852
 0.02196814 0.03046343 0.02465173 0.02506708 0.03129314 0.02200041
 0.03977231 0.04487483 0.03372476 0.02727856 0.02072268 0.05023593
 0.04315829 0.03942969 0.03413007 0.02528962 0.05996364 0.02419219
 0.01660496 0.03943108 0.02695005 0.02841421 0.03182119 0.02616235
 0.02745927 0.02385883 0.04222442 0.0319293  0.03047809 0.01507354
 0.03807621 0.0565135  0.02608522 0.03073391 0.02980583 0.03742138
 0.03378386 0.03634555 0.03054936 0.04600257 0.02553747 0.03000171
 0.03288126 0.02928859 0.02516703 0.01982576 0.03539845 0.05609523
 0.03815241 0.02602654 0.04285545 0.03853929 0.03581246 0.0290693
 0.03923914 0.04040144 0.03647172 0.02414963 0.02547807 0.03936939
 0.02535475 0.0509781  0.02578683 0.03519376 0.05045674 0.03683552
 0.03267207 0.04479741 0.02950563 0.03296697 0.0472466  0.0329161
 0.03008194 0.07067375 0.0465391  0.04340151]
for model  88 the mean error 0.034242371543859125
all id 88 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 25 out win 4 err 0.034242371543859125
Launcher: Job 89 completed in 7228 seconds.
Launcher: Task 15 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  21969
Epoch:0, Train loss:0.404525, valid loss:0.371485
Epoch:1, Train loss:0.229985, valid loss:0.231213
Epoch:2, Train loss:0.221968, valid loss:0.230898
Epoch:3, Train loss:0.220909, valid loss:0.230186
Epoch:4, Train loss:0.220268, valid loss:0.229975
Epoch:5, Train loss:0.219874, valid loss:0.229828
Epoch:6, Train loss:0.219625, valid loss:0.229945
Epoch:7, Train loss:0.219437, valid loss:0.229699
Epoch:8, Train loss:0.219274, valid loss:0.230225
Epoch:9, Train loss:0.219224, valid loss:0.229666
Epoch:10, Train loss:0.219140, valid loss:0.229493
Epoch:11, Train loss:0.218607, valid loss:0.229339
Epoch:12, Train loss:0.218635, valid loss:0.229280
Epoch:13, Train loss:0.218598, valid loss:0.229534
Epoch:14, Train loss:0.218540, valid loss:0.229268
Epoch:15, Train loss:0.218539, valid loss:0.229267
Epoch:16, Train loss:0.218488, valid loss:0.229413
Epoch:17, Train loss:0.218486, valid loss:0.229332
Epoch:18, Train loss:0.218481, valid loss:0.229218
Epoch:19, Train loss:0.218441, valid loss:0.229215
Epoch:20, Train loss:0.218408, valid loss:0.229212
Epoch:21, Train loss:0.218163, valid loss:0.229050
Epoch:22, Train loss:0.218154, valid loss:0.229049
Epoch:23, Train loss:0.218155, valid loss:0.229121
Epoch:24, Train loss:0.218145, valid loss:0.229031
Epoch:25, Train loss:0.218128, valid loss:0.229075
Epoch:26, Train loss:0.218104, valid loss:0.229115
Epoch:27, Train loss:0.218110, valid loss:0.229106
Epoch:28, Train loss:0.218104, valid loss:0.229068
Epoch:29, Train loss:0.218084, valid loss:0.229144
Epoch:30, Train loss:0.218079, valid loss:0.229080
Epoch:31, Train loss:0.217954, valid loss:0.228989
Epoch:32, Train loss:0.217939, valid loss:0.229022
Epoch:33, Train loss:0.217948, valid loss:0.229008
Epoch:34, Train loss:0.217934, valid loss:0.229002
Epoch:35, Train loss:0.217928, valid loss:0.229010
Epoch:36, Train loss:0.217928, valid loss:0.228994
Epoch:37, Train loss:0.217916, valid loss:0.229015
Epoch:38, Train loss:0.217915, valid loss:0.228999
Epoch:39, Train loss:0.217913, valid loss:0.228998
Epoch:40, Train loss:0.217915, valid loss:0.228977
Epoch:41, Train loss:0.217837, valid loss:0.228965
Epoch:42, Train loss:0.217837, valid loss:0.228955
Epoch:43, Train loss:0.217829, valid loss:0.229007
Epoch:44, Train loss:0.217830, valid loss:0.228965
Epoch:45, Train loss:0.217832, valid loss:0.229020
Epoch:46, Train loss:0.217826, valid loss:0.228965
Epoch:47, Train loss:0.217822, valid loss:0.228983
Epoch:48, Train loss:0.217822, valid loss:0.228951
Epoch:49, Train loss:0.217816, valid loss:0.228973
Epoch:50, Train loss:0.217816, valid loss:0.228952
Epoch:51, Train loss:0.217785, valid loss:0.228954
Epoch:52, Train loss:0.217779, valid loss:0.228936
Epoch:53, Train loss:0.217781, valid loss:0.228935
Epoch:54, Train loss:0.217779, valid loss:0.228943
Epoch:55, Train loss:0.217780, valid loss:0.228943
Epoch:56, Train loss:0.217776, valid loss:0.228929
Epoch:57, Train loss:0.217774, valid loss:0.228942
Epoch:58, Train loss:0.217773, valid loss:0.228934
Epoch:59, Train loss:0.217770, valid loss:0.228946
Epoch:60, Train loss:0.217771, valid loss:0.228937
training time 7065.8889417648315
total number of trained parameters for initialize model 21969
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.7295397576284323
plot_id,batch_id 0 1 miss% 0.7900041248739282
plot_id,batch_id 0 2 miss% 0.8006169296886672
plot_id,batch_id 0 3 miss% 0.8110045102501658
plot_id,batch_id 0 4 miss% 0.8069055615573062
plot_id,batch_id 0 5 miss% 0.7246998055126371
plot_id,batch_id 0 6 miss% 0.783412209946558
plot_id,batch_id 0 7 miss% 0.7982835989795446
plot_id,batch_id 0 8 miss% 0.8046923234420624
plot_id,batch_id 0 9 miss% 0.8111075204195592
plot_id,batch_id 0 10 miss% 0.7040895907196106
plot_id,batch_id 0 11 miss% 0.7830612094861126
plot_id,batch_id 0 12 miss% 0.795477186286188
plot_id,batch_id 0 13 miss% 0.8043637313074185
plot_id,batch_id 0 14 miss% 0.811042419820296
plot_id,batch_id 0 15 miss% 0.7309902907791642
plot_id,batch_id 0 16 miss% 0.779608222382215
plot_id,batch_id 0 17 miss% 0.7953787797233977
plot_id,batch_id 0 18 miss% 0.8065804361783612
plot_id,batch_id 0 19 miss% 0.8072565458205714
plot_id,batch_id 0 20 miss% 0.762887628640744
plot_id,batch_id 0 21 miss% 0.805613929788662
plot_id,batch_id 0 22 miss% 0.8088792746415752
plot_id,batch_id 0 23 miss% 0.8148382312810692
plot_id,batch_id 0 24 miss% 0.8171641424246903
plot_id,batch_id 0 25 miss% 0.7548145558817652
plot_id,batch_id 0 26 miss% 0.7983319557652203
plot_id,batch_id 0 27 miss% 0.8033416172322699
plot_id,batch_id 0 28 miss% 0.8079539832668046
plot_id,batch_id 0 29 miss% 0.8090964113051377
plot_id,batch_id 0 30 miss% 0.7560904895552264
plot_id,batch_id 0 31 miss% 0.793562421794089
plot_id,batch_id 0 32 miss% 0.8053890360744015
plot_id,batch_id 0 33 miss% 0.8078856451008596
plot_id,batch_id 0 34 miss% 0.8103664194655503
plot_id,batch_id 0 35 miss% 0.7500774769477105
plot_id,batch_id 0 36 miss% 0.8021552265614053
plot_id,batch_id 0 37 miss% 0.8037116938688017
plot_id,batch_id 0 38 miss% 0.8116342277672276
plot_id,batch_id 0 39 miss% 0.810660746948374
plot_id,batch_id 0 40 miss% 0.78224766098429
plot_id,batch_id 0 41 miss% 0.8067006398552391
plot_id,batch_id 0 42 miss% 0.8118995450840458
plot_id,batch_id 0 43 miss% 0.8150057172328485
plot_id,batch_id 0 44 miss% 0.8199080010999114
plot_id,batch_id 0 45 miss% 0.7795120316545934
plot_id,batch_id 0 46 miss% 0.8054678664689638
plot_id,batch_id 0 47 miss% 0.8139136000026311
plot_id,batch_id 0 48 miss% 0.8129187393370441
plot_id,batch_id 0 49 miss% 0.8184134619954837
plot_id,batch_id 0 50 miss% 0.7855623314589544
plot_id,batch_id 0 51 miss% 0.8041700008341645
plot_id,batch_id 0 52 miss% 0.8074204760953944
plot_id,batch_id 0 53 miss% 0.8137140617549238
plot_id,batch_id 0 54 miss% 0.8207448489213355
plot_id,batch_id 0 55 miss% 0.782944007439356
plot_id,batch_id 0 56 miss% 0.8051619570540921
plot_id,batch_id 0 57 miss% 0.8092191011682155
plot_id,batch_id 0 58 miss% 0.8144494377936022
plot_id,batch_id 0 59 miss% 0.8194204467259762
plot_id,batch_id 0 60 miss% 0.6511544377413928
plot_id,batch_id 0 61 miss% 0.7479997503144179
plot_id,batch_id 0 62 miss% 0.7836721889849513
plot_id,batch_id 0 63 miss% 0.7943138797095471
plot_id,batch_id 0 64 miss% 0.802335654656936
plot_id,batch_id 0 65 miss% 0.6379231409304236
plot_id,batch_id 0 66 miss% 0.7514088642430742
plot_id,batch_id 0 67 miss% 0.7687762051272223
plot_id,batch_id 0 68 miss% 0.7963117358573073
plot_id,batch_id 0 69 miss% 0.7973243552475227
plot_id,batch_id 0 70 miss% 0.6088098504177729
plot_id,batch_id 0 71 miss% 0.7634513973680022
plot_id,batch_id 0 72 miss% 0.7662252233556652
plot_id,batch_id 0 73 miss% 0.7808827285325401
plot_id,batch_id 0 74 miss% 0.7913254697608648
plot_id,batch_id 0 75 miss% 0.6070363926973328
plot_id,batch_id 0 76 miss% 0.7132282759946564
plot_id,batch_id 0 77 miss% 0.751929378615632
plot_id,batch_id 0 78 miss% 0.7756056240812644
plot_id,batch_id 0 79 miss% 0.7854835302024913
plot_id,batch_id 0 80 miss% 0.6734470672565792
plot_id,batch_id 0 81 miss% 0.7777571837916947
plot_id,batch_id 0 82 miss% 0.791347897501331
plot_id,batch_id 0 83 miss% 0.799669168305162
plot_id,batch_id 0 84 miss% 0.8016607418677049
plot_id,batch_id 0 85 miss% 0.6664153288900336
plot_id,batch_id 0 86 miss% 0.7707687013688069
plot_id,batch_id 0 87 miss% 0.78937945612258
plot_id,batch_id 0 88 miss% 0.7988861314669179
plot_id,batch_id 0 89 miss% 0.7994310078963298
plot_id,batch_id 0 90 miss% 0.6393163706023596
plot_id,batch_id 0 91 miss% 0.769720467495222
plot_id,batch_id 0 92 miss% 0.7822081718750737
plot_id,batch_id 0 93 miss% 0.7928334901741406
plot_id,batch_id 0 94 miss% 0.7992108368127383
plot_id,batch_id 0 95 miss% 0.6402590421210443
plot_id,batch_id 0 96 miss% 0.7542632862136257
plot_id,batch_id 0 97 miss% 0.7834506655888324
plot_id,batch_id 0 98 miss% 0.7853190386418137
plot_id,batch_id 0 99 miss% 0.7925518514657405
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.72953976 0.79000412 0.80061693 0.81100451 0.80690556 0.72469981
 0.78341221 0.7982836  0.80469232 0.81110752 0.70408959 0.78306121
 0.79547719 0.80436373 0.81104242 0.73099029 0.77960822 0.79537878
 0.80658044 0.80725655 0.76288763 0.80561393 0.80887927 0.81483823
 0.81716414 0.75481456 0.79833196 0.80334162 0.80795398 0.80909641
 0.75609049 0.79356242 0.80538904 0.80788565 0.81036642 0.75007748
 0.80215523 0.80371169 0.81163423 0.81066075 0.78224766 0.80670064
 0.81189955 0.81500572 0.819908   0.77951203 0.80546787 0.8139136
 0.81291874 0.81841346 0.78556233 0.80417    0.80742048 0.81371406
 0.82074485 0.78294401 0.80516196 0.8092191  0.81444944 0.81942045
 0.65115444 0.74799975 0.78367219 0.79431388 0.80233565 0.63792314
 0.75140886 0.76877621 0.79631174 0.79732436 0.60880985 0.7634514
 0.76622522 0.78088273 0.79132547 0.60703639 0.71322828 0.75192938
 0.77560562 0.78548353 0.67344707 0.77775718 0.7913479  0.79966917
 0.80166074 0.66641533 0.7707687  0.78937946 0.79888613 0.79943101
 0.63931637 0.76972047 0.78220817 0.79283349 0.79921084 0.64025904
 0.75426329 0.78345067 0.78531904 0.79255185]
for model  164 the mean error 0.778244577913716
all id 164 hidden_dim 16 learning_rate 0.005 num_layers 3 frames 31 out win 5 err 0.778244577913716
Launcher: Job 165 completed in 7244 seconds.
Launcher: Task 238 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  79249
Epoch:0, Train loss:0.317563, valid loss:0.289009
Epoch:1, Train loss:0.019522, valid loss:0.004299
Epoch:2, Train loss:0.005337, valid loss:0.002714
Epoch:3, Train loss:0.004020, valid loss:0.002249
Epoch:4, Train loss:0.003526, valid loss:0.001578
Epoch:5, Train loss:0.003068, valid loss:0.002076
Epoch:6, Train loss:0.002976, valid loss:0.001520
Epoch:7, Train loss:0.002765, valid loss:0.001388
Epoch:8, Train loss:0.002566, valid loss:0.002001
Epoch:9, Train loss:0.002521, valid loss:0.001455
Epoch:10, Train loss:0.002415, valid loss:0.001397
Epoch:11, Train loss:0.001611, valid loss:0.000851
Epoch:12, Train loss:0.001591, valid loss:0.001173
Epoch:13, Train loss:0.001603, valid loss:0.001062
Epoch:14, Train loss:0.001506, valid loss:0.001003
Epoch:15, Train loss:0.001525, valid loss:0.000832
Epoch:16, Train loss:0.001531, valid loss:0.001154
Epoch:17, Train loss:0.001530, valid loss:0.000803
Epoch:18, Train loss:0.001449, valid loss:0.000849
Epoch:19, Train loss:0.001407, valid loss:0.000799
Epoch:20, Train loss:0.001393, valid loss:0.000905
Epoch:21, Train loss:0.001018, valid loss:0.000607
Epoch:22, Train loss:0.001034, valid loss:0.000683
Epoch:23, Train loss:0.001020, valid loss:0.000647
Epoch:24, Train loss:0.001029, valid loss:0.000734
Epoch:25, Train loss:0.000992, valid loss:0.000734
Epoch:26, Train loss:0.000980, valid loss:0.000564
Epoch:27, Train loss:0.001013, valid loss:0.000622
Epoch:28, Train loss:0.000987, valid loss:0.000577
Epoch:29, Train loss:0.000969, valid loss:0.000590
Epoch:30, Train loss:0.000959, valid loss:0.000697
Epoch:31, Train loss:0.000774, valid loss:0.000508
Epoch:32, Train loss:0.000774, valid loss:0.000585
Epoch:33, Train loss:0.000774, valid loss:0.000538
Epoch:34, Train loss:0.000767, valid loss:0.000652
Epoch:35, Train loss:0.000762, valid loss:0.000559
Epoch:36, Train loss:0.000773, valid loss:0.000536
Epoch:37, Train loss:0.000760, valid loss:0.000507
Epoch:38, Train loss:0.000751, valid loss:0.000522
Epoch:39, Train loss:0.000739, valid loss:0.000550
Epoch:40, Train loss:0.000739, valid loss:0.000549
Epoch:41, Train loss:0.000649, valid loss:0.000498
Epoch:42, Train loss:0.000654, valid loss:0.000914
Epoch:43, Train loss:0.000653, valid loss:0.000483
Epoch:44, Train loss:0.000642, valid loss:0.000491
Epoch:45, Train loss:0.000647, valid loss:0.000479
Epoch:46, Train loss:0.000645, valid loss:0.000504
Epoch:47, Train loss:0.000643, valid loss:0.000479
Epoch:48, Train loss:0.000636, valid loss:0.000495
Epoch:49, Train loss:0.000645, valid loss:0.000508
Epoch:50, Train loss:0.000629, valid loss:0.000512
Epoch:51, Train loss:0.000591, valid loss:0.000460
Epoch:52, Train loss:0.000588, valid loss:0.000469
Epoch:53, Train loss:0.000591, valid loss:0.000469
Epoch:54, Train loss:0.000589, valid loss:0.000503
Epoch:55, Train loss:0.000590, valid loss:0.000483
Epoch:56, Train loss:0.000587, valid loss:0.000487
Epoch:57, Train loss:0.000585, valid loss:0.000475
Epoch:58, Train loss:0.000586, valid loss:0.000466
Epoch:59, Train loss:0.000583, valid loss:0.000577
Epoch:60, Train loss:0.000589, valid loss:0.000455
training time 7101.722224712372
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.024922470238897033
plot_id,batch_id 0 1 miss% 0.03238159257262864
plot_id,batch_id 0 2 miss% 0.025169844427721997
plot_id,batch_id 0 3 miss% 0.025738630696306178
plot_id,batch_id 0 4 miss% 0.017241713671345752
plot_id,batch_id 0 5 miss% 0.02743109659512287
plot_id,batch_id 0 6 miss% 0.018773353744440047
plot_id,batch_id 0 7 miss% 0.0343986559031976
plot_id,batch_id 0 8 miss% 0.017576702000562192
plot_id,batch_id 0 9 miss% 0.01631979729867707
plot_id,batch_id 0 10 miss% 0.04849322682528208
plot_id,batch_id 0 11 miss% 0.04174047587111839
plot_id,batch_id 0 12 miss% 0.027602584645746917
plot_id,batch_id 0 13 miss% 0.01775322110477066
plot_id,batch_id 0 14 miss% 0.02704442146415125
plot_id,batch_id 0 15 miss% 0.02361120236559662
plot_id,batch_id 0 16 miss% 0.025113663879780174
plot_id,batch_id 0 17 miss% 0.03861210498362473
plot_id,batch_id 0 18 miss% 0.030308597409980362
plot_id,batch_id 0 19 miss% 0.028182552765648794
plot_id,batch_id 0 20 miss% 0.044779937340922595
plot_id,batch_id 0 21 miss% 0.022054550279297827
plot_id,batch_id 0 22 miss% 0.03443387236928123
plot_id,batch_id 0 23 miss% 0.018297595288242718
plot_id,batch_id 0 24 miss% 0.02555237830430071
plot_id,batch_id 0 25 miss% 0.028907546248640777
plot_id,batch_id 0 26 miss% 0.0325173002745697
plot_id,batch_id 0 27 miss% 0.024586817311827897
plot_id,batch_id 0 28 miss% 0.017676352658152795
plot_id,batch_id 0 29 miss% 0.018286988088068907
plot_id,batch_id 0 30 miss% 0.04323218086483551
plot_id,batch_id 0 31 miss% 0.020490790460651758
plot_id,batch_id 0 32 miss% 0.022916368451150308
plot_id,batch_id 0 33 miss% 0.017832620966479678
plot_id,batch_id 0 34 miss% 0.02717207219733503
plot_id,batch_id 0 35 miss% 0.040189037997593734
plot_id,batch_id 0 36 miss% 0.028477177710192616
plot_id,batch_id 0 37 miss% 0.03927323193348724
plot_id,batch_id 0 38 miss% 0.02675190703397031
plot_id,batch_id 0 39 miss% 0.016968790289436896
plot_id,batch_id 0 40 miss% 0.09527992052518612
plot_id,batch_id 0 41 miss% 0.03751444201614262
plot_id,batch_id 0 42 miss% 0.023216476429833308
plot_id,batch_id 0 43 miss% 0.03367001310135399
plot_id,batch_id 0 44 miss% 0.028385976172387202
plot_id,batch_id 0 45 miss% 0.04377412079429409
plot_id,batch_id 0 46 miss% 0.018260610777166703
plot_id,batch_id 0 47 miss% 0.024899854061278204
plot_id,batch_id 0 48 miss% 0.024317100378724282
plot_id,batch_id 0 49 miss% 0.013527998692423313
plot_id,batch_id 0 50 miss% 0.04023806816307001
plot_id,batch_id 0 51 miss% 0.01813542341061249
plot_id,batch_id 0 52 miss% 0.023395795540633542
plot_id,batch_id 0 53 miss% 0.017277839770225133
plot_id,batch_id 0 54 miss% 0.022850645204610335
plot_id,batch_id 0 55 miss% 0.05107170410949251
plot_id,batch_id 0 56 miss% 0.03137025442602006
plot_id,batch_id 0 57 miss% 0.02190156624336958
plot_id,batch_id 0 58 miss% 0.016137976314591785
plot_id,batch_id 0 59 miss% 0.02470353841411174
plot_id,batch_id 0 60 miss% 0.04770142374262971
plot_id,batch_id 0 61 miss% 0.027782962631430274
plot_id,batch_id 0 62 miss% 0.02416539131527492
plot_id,batch_id 0 63 miss% 0.027146443778077727
plot_id,batch_id 0 64 miss% 0.029040652385805107
plot_id,batch_id 0 65 miss% 0.03570778266398127
plot_id,batch_id 0 66 miss% 0.03187583202256349
plot_id,batch_id 0 67 miss% 0.034249762215934246
plot_id,batch_id 0 68 miss% 0.03048843424697232
plot_id,batch_id 0 69 miss% 0.02752542120843026
plot_id,batch_id 0 70 miss% 0.02776639913594668
plot_id,batch_id 0 71 miss% 0.048247425265507536
plot_id,batch_id 0 72 miss% 0.022315898722296265
plot_id,batch_id 0 73 miss% 0.02040093130943855
plot_id,batch_id 0 74 miss% 0.042825801505250685
plot_id,batch_id 0 75 miss% 0.038646170949169036
plot_id,batch_id 0 76 miss% 0.02140484963298894
plot_id,batch_id 0 77 miss% 0.023635836483005317
plot_id,batch_id 0 78 miss% 0.03000923987955431
plot_id,batch_id 0 79 miss% 0.03278593094280164
plot_id,batch_id 0 80 miss% 0.05335201437136777
plot_id,batch_id 0 81 miss% 0.015062661957290058
plot_id,batch_id 0 82 miss% 0.026743151385554432
plot_id,batch_id 0 83 miss% 0.02787123111641313
plot_id,batch_id 0 84 miss% 0.022849904100220413
plot_id,batch_id 0 85 miss% 0.04020861163484343
plot_id,batch_id 0 86 miss% 0.028279865620911566
plot_id,batch_id 0 87 miss% 0.021165483328184242
plot_id,batch_id 0 88 miss% 0.02224394166368272
plot_id,batch_id 0 89 miss% 0.021251255851478242
plot_id,batch_id 0 90 miss% 0.03738437905726049
plot_id,batch_id 0 91 miss% 0.034882801488904346
plot_id,batch_id 0 92 miss% 0.02088435479239005
plot_id,batch_id 0 93 miss% 0.029994028746589223
plot_id,batch_id 0 94 miss% 0.028926964092238126
plot_id,batch_id 0 95 miss% 0.04538258072190072
plot_id,batch_id 0 96 miss% 0.03739084541635416
plot_id,batch_id 0 97 miss% 0.053221041378280896
plot_id,batch_id 0 98 miss% 0.021838843879143817
plot_id,batch_id 0 99 miss% 0.022920462445310874
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02492247 0.03238159 0.02516984 0.02573863 0.01724171 0.0274311
 0.01877335 0.03439866 0.0175767  0.0163198  0.04849323 0.04174048
 0.02760258 0.01775322 0.02704442 0.0236112  0.02511366 0.0386121
 0.0303086  0.02818255 0.04477994 0.02205455 0.03443387 0.0182976
 0.02555238 0.02890755 0.0325173  0.02458682 0.01767635 0.01828699
 0.04323218 0.02049079 0.02291637 0.01783262 0.02717207 0.04018904
 0.02847718 0.03927323 0.02675191 0.01696879 0.09527992 0.03751444
 0.02321648 0.03367001 0.02838598 0.04377412 0.01826061 0.02489985
 0.0243171  0.013528   0.04023807 0.01813542 0.0233958  0.01727784
 0.02285065 0.0510717  0.03137025 0.02190157 0.01613798 0.02470354
 0.04770142 0.02778296 0.02416539 0.02714644 0.02904065 0.03570778
 0.03187583 0.03424976 0.03048843 0.02752542 0.0277664  0.04824743
 0.0223159  0.02040093 0.0428258  0.03864617 0.02140485 0.02363584
 0.03000924 0.03278593 0.05335201 0.01506266 0.02674315 0.02787123
 0.0228499  0.04020861 0.02827987 0.02116548 0.02224394 0.02125126
 0.03738438 0.0348828  0.02088435 0.02999403 0.02892696 0.04538258
 0.03739085 0.05322104 0.02183884 0.02292046]
for model  222 the mean error 0.029443197621639418
all id 222 hidden_dim 32 learning_rate 0.02 num_layers 3 frames 31 out win 3 err 0.029443197621639418
Launcher: Job 223 completed in 7288 seconds.
Launcher: Task 79 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  107025
Epoch:0, Train loss:0.435344, valid loss:0.430397
Epoch:1, Train loss:0.029682, valid loss:0.004418
Epoch:2, Train loss:0.007142, valid loss:0.003402
Epoch:3, Train loss:0.005526, valid loss:0.003281
Epoch:4, Train loss:0.004400, valid loss:0.002621
Epoch:5, Train loss:0.003928, valid loss:0.001773
Epoch:6, Train loss:0.003298, valid loss:0.001839
Epoch:7, Train loss:0.003070, valid loss:0.001675
Epoch:8, Train loss:0.003024, valid loss:0.001681
Epoch:9, Train loss:0.002753, valid loss:0.001969
Epoch:10, Train loss:0.002803, valid loss:0.001861
Epoch:11, Train loss:0.001793, valid loss:0.000906
Epoch:12, Train loss:0.001795, valid loss:0.000975
Epoch:13, Train loss:0.001779, valid loss:0.000949
Epoch:14, Train loss:0.001754, valid loss:0.001017
Epoch:15, Train loss:0.001736, valid loss:0.001071
Epoch:16, Train loss:0.001630, valid loss:0.000993
Epoch:17, Train loss:0.001657, valid loss:0.000908
Epoch:18, Train loss:0.001610, valid loss:0.001513
Epoch:19, Train loss:0.001524, valid loss:0.001231
Epoch:20, Train loss:0.001539, valid loss:0.000913
Epoch:21, Train loss:0.001107, valid loss:0.000700
Epoch:22, Train loss:0.001087, valid loss:0.000708
Epoch:23, Train loss:0.001091, valid loss:0.000770
Epoch:24, Train loss:0.001095, valid loss:0.000693
Epoch:25, Train loss:0.001072, valid loss:0.000799
Epoch:26, Train loss:0.001059, valid loss:0.000778
Epoch:27, Train loss:0.001073, valid loss:0.000698
Epoch:28, Train loss:0.001019, valid loss:0.000669
Epoch:29, Train loss:0.001032, valid loss:0.000721
Epoch:30, Train loss:0.001010, valid loss:0.000810
Epoch:31, Train loss:0.000813, valid loss:0.000642
Epoch:32, Train loss:0.000811, valid loss:0.000615
Epoch:33, Train loss:0.000806, valid loss:0.000617
Epoch:34, Train loss:0.000810, valid loss:0.000619
Epoch:35, Train loss:0.000798, valid loss:0.000603
Epoch:36, Train loss:0.000787, valid loss:0.000585
Epoch:37, Train loss:0.000780, valid loss:0.000589
Epoch:38, Train loss:0.000788, valid loss:0.000607
Epoch:39, Train loss:0.000771, valid loss:0.000577
Epoch:40, Train loss:0.000776, valid loss:0.000642
Epoch:41, Train loss:0.000685, valid loss:0.000560
Epoch:42, Train loss:0.000683, valid loss:0.000553
Epoch:43, Train loss:0.000675, valid loss:0.000564
Epoch:44, Train loss:0.000678, valid loss:0.000596
Epoch:45, Train loss:0.000683, valid loss:0.000571
Epoch:46, Train loss:0.000671, valid loss:0.000571
Epoch:47, Train loss:0.000665, valid loss:0.000611
Epoch:48, Train loss:0.000668, valid loss:0.000579
Epoch:49, Train loss:0.000668, valid loss:0.000576
Epoch:50, Train loss:0.000657, valid loss:0.000554
Epoch:51, Train loss:0.000625, valid loss:0.000546
Epoch:52, Train loss:0.000620, valid loss:0.000555
Epoch:53, Train loss:0.000623, valid loss:0.000547
Epoch:54, Train loss:0.000622, valid loss:0.000541
Epoch:55, Train loss:0.000618, valid loss:0.000549
Epoch:56, Train loss:0.000615, valid loss:0.000554
Epoch:57, Train loss:0.000616, valid loss:0.000544
Epoch:58, Train loss:0.000613, valid loss:0.000548
Epoch:59, Train loss:0.000612, valid loss:0.000545
Epoch:60, Train loss:0.000612, valid loss:0.000548
training time 7140.472151756287
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.026738674609804267
plot_id,batch_id 0 1 miss% 0.02558314363405906
plot_id,batch_id 0 2 miss% 0.020433912865536412
plot_id,batch_id 0 3 miss% 0.03056117359269863
plot_id,batch_id 0 4 miss% 0.02641088936419115
plot_id,batch_id 0 5 miss% 0.03631117026989114
plot_id,batch_id 0 6 miss% 0.030254133077214102
plot_id,batch_id 0 7 miss% 0.026746539249370576
plot_id,batch_id 0 8 miss% 0.022109411545810056
plot_id,batch_id 0 9 miss% 0.01813537446505707
plot_id,batch_id 0 10 miss% 0.04612813013193793
plot_id,batch_id 0 11 miss% 0.03871637122378141
plot_id,batch_id 0 12 miss% 0.023934765650135137
plot_id,batch_id 0 13 miss% 0.027020630013462794
plot_id,batch_id 0 14 miss% 0.024709930687450945
plot_id,batch_id 0 15 miss% 0.03640565355656689
plot_id,batch_id 0 16 miss% 0.024563632050579782
plot_id,batch_id 0 17 miss% 0.03206556374939049
plot_id,batch_id 0 18 miss% 0.026729989169272717
plot_id,batch_id 0 19 miss% 0.02856047738587772
plot_id,batch_id 0 20 miss% 0.047270690478032604
plot_id,batch_id 0 21 miss% 0.014948185942852183
plot_id,batch_id 0 22 miss% 0.02431224594683019
plot_id,batch_id 0 23 miss% 0.020684213057068922
plot_id,batch_id 0 24 miss% 0.0184680847656708
plot_id,batch_id 0 25 miss% 0.02533532863268196
plot_id,batch_id 0 26 miss% 0.02911495314822126
plot_id,batch_id 0 27 miss% 0.013364749003421227
plot_id,batch_id 0 28 miss% 0.02590000148761256
plot_id,batch_id 0 29 miss% 0.025059753955454966
plot_id,batch_id 0 30 miss% 0.02913731304275294
plot_id,batch_id 0 31 miss% 0.017835711312907288
plot_id,batch_id 0 32 miss% 0.016881716990899572
plot_id,batch_id 0 33 miss% 0.027508203340643533
plot_id,batch_id 0 34 miss% 0.02549542892348556
plot_id,batch_id 0 35 miss% 0.04219656589671493
plot_id,batch_id 0 36 miss% 0.03021186652828987
plot_id,batch_id 0 37 miss% 0.021318877599140314
plot_id,batch_id 0 38 miss% 0.019853758579517163
plot_id,batch_id 0 39 miss% 0.01886907425128559
plot_id,batch_id 0 40 miss% 0.04769060179929883
plot_id,batch_id 0 41 miss% 0.027917956031928766
plot_id,batch_id 0 42 miss% 0.02544701655715136
plot_id,batch_id 0 43 miss% 0.022400899556851223
plot_id,batch_id 0 44 miss% 0.016469379506937342
plot_id,batch_id 0 45 miss% 0.0337660884503303
plot_id,batch_id 0 46 miss% 0.024131799355166374
plot_id,batch_id 0 47 miss% 0.02867653613030956
plot_id,batch_id 0 48 miss% 0.025420620564445325
plot_id,batch_id 0 49 miss% 0.016498761640302815
plot_id,batch_id 0 50 miss% 0.03698606963484291
plot_id,batch_id 0 51 miss% 0.028065984897569604
plot_id,batch_id 0 52 miss% 0.025096612314428397
plot_id,batch_id 0 53 miss% 0.012097578273460093
plot_id,batch_id 0 54 miss% 0.029638442812060798
plot_id,batch_id 0 55 miss% 0.027910976966642465
plot_id,batch_id 0 56 miss% 0.02429861444901986
plot_id,batch_id 0 57 miss% 0.022060565482073045
plot_id,batch_id 0 58 miss% 0.020860778162046963
plot_id,batch_id 0 59 miss% 0.025780648224865546
plot_id,batch_id 0 60 miss% 0.0475991089085256
plot_id,batch_id 0 61 miss% 0.03413271456171319
plot_id,batch_id 0 62 miss% 0.022050561615200127
plot_id,batch_id 0 63 miss% 0.022571024447848673
plot_id,batch_id 0 64 miss% 0.0284607029850024
plot_id,batch_id 0 65 miss% 0.02874627984905294
plot_id,batch_id 0 66 miss% 0.03628796252149887
plot_id,batch_id 0 67 miss% 0.01783693117540976
plot_id,batch_id 0 68 miss% 0.028760155336094332
plot_id,batch_id 0 69 miss% 0.023570253968781103
plot_id,batch_id 0 70 miss% 0.04922422070776147
plot_id,batch_id 0 71 miss% 0.05206255918093188
plot_id,batch_id 0 72 miss% 0.034197020414140265
plot_id,batch_id 0 73 miss% 0.02594113888184113
plot_id,batch_id 0 74 miss% 0.02992790414491603
plot_id,batch_id 0 75 miss% 0.039239131031160224
plot_id,batch_id 0 76 miss% 0.036062554844454545
plot_id,batch_id 0 77 miss% 0.032557004909454505
plot_id,batch_id 0 78 miss% 0.03489327505935663
plot_id,batch_id 0 79 miss% 0.028779516623149533
plot_id,batch_id 0 80 miss% 0.03470030836841392
plot_id,batch_id 0 81 miss% 0.023579376056490128
plot_id,batch_id 0 82 miss% 0.034345289557682596
plot_id,batch_id 0 83 miss% 0.022786831337196445
plot_id,batch_id 0 84 miss% 0.013732319541555689
plot_id,batch_id 0 85 miss% 0.050573284816100426
plot_id,batch_id 0 86 miss% 0.023970214264620565
plot_id,batch_id 0 87 miss% 0.021974608961068197
plot_id,batch_id 0 88 miss% 0.032471990223934176
plot_id,batch_id 0 89 miss% 0.026477009256894876
plot_id,batch_id 0 90 miss% 0.04261058589357094
plot_id,batch_id 0 91 miss% 0.03299710807886736
plot_id,batch_id 0 92 miss% 0.02771928178801992
plot_id,batch_id 0 93 miss% 0.03558127020683114
plot_id,batch_id 0 94 miss% 0.02231559517809632
plot_id,batch_id 0 95 miss% 0.02576889054959939
plot_id,batch_id 0 96 miss% 0.024089180877853257
plot_id,batch_id 0 97 miss% 0.03228866953754874
plot_id,batch_id 0 98 miss% 0.026395423086566092
plot_id,batch_id 0 99 miss% 0.03030140362647287
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02673867 0.02558314 0.02043391 0.03056117 0.02641089 0.03631117
 0.03025413 0.02674654 0.02210941 0.01813537 0.04612813 0.03871637
 0.02393477 0.02702063 0.02470993 0.03640565 0.02456363 0.03206556
 0.02672999 0.02856048 0.04727069 0.01494819 0.02431225 0.02068421
 0.01846808 0.02533533 0.02911495 0.01336475 0.0259     0.02505975
 0.02913731 0.01783571 0.01688172 0.0275082  0.02549543 0.04219657
 0.03021187 0.02131888 0.01985376 0.01886907 0.0476906  0.02791796
 0.02544702 0.0224009  0.01646938 0.03376609 0.0241318  0.02867654
 0.02542062 0.01649876 0.03698607 0.02806598 0.02509661 0.01209758
 0.02963844 0.02791098 0.02429861 0.02206057 0.02086078 0.02578065
 0.04759911 0.03413271 0.02205056 0.02257102 0.0284607  0.02874628
 0.03628796 0.01783693 0.02876016 0.02357025 0.04922422 0.05206256
 0.03419702 0.02594114 0.0299279  0.03923913 0.03606255 0.032557
 0.03489328 0.02877952 0.03470031 0.02357938 0.03434529 0.02278683
 0.01373232 0.05057328 0.02397021 0.02197461 0.03247199 0.02647701
 0.04261059 0.03299711 0.02771928 0.03558127 0.0223156  0.02576889
 0.02408918 0.03228867 0.02639542 0.0303014 ]
for model  123 the mean error 0.028246808823589795
all id 123 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 25 out win 3 err 0.028246808823589795
Launcher: Job 124 completed in 7334 seconds.
Launcher: Task 54 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  79249
Epoch:0, Train loss:0.317563, valid loss:0.289009
Epoch:1, Train loss:0.017894, valid loss:0.003206
Epoch:2, Train loss:0.004891, valid loss:0.002201
Epoch:3, Train loss:0.003630, valid loss:0.001752
Epoch:4, Train loss:0.002968, valid loss:0.001814
Epoch:5, Train loss:0.002661, valid loss:0.001554
Epoch:6, Train loss:0.002340, valid loss:0.001418
Epoch:7, Train loss:0.002186, valid loss:0.001242
Epoch:8, Train loss:0.002063, valid loss:0.001094
Epoch:9, Train loss:0.001907, valid loss:0.000946
Epoch:10, Train loss:0.001855, valid loss:0.001001
Epoch:11, Train loss:0.001376, valid loss:0.000794
Epoch:12, Train loss:0.001331, valid loss:0.000858
Epoch:13, Train loss:0.001318, valid loss:0.000830
Epoch:14, Train loss:0.001287, valid loss:0.000813
Epoch:15, Train loss:0.001251, valid loss:0.000768
Epoch:16, Train loss:0.001234, valid loss:0.000704
Epoch:17, Train loss:0.001254, valid loss:0.000706
Epoch:18, Train loss:0.001175, valid loss:0.000730
Epoch:19, Train loss:0.001166, valid loss:0.000807
Epoch:20, Train loss:0.001139, valid loss:0.000632
Epoch:21, Train loss:0.000930, valid loss:0.000657
Epoch:22, Train loss:0.000903, valid loss:0.000667
Epoch:23, Train loss:0.000901, valid loss:0.000561
Epoch:24, Train loss:0.000901, valid loss:0.000565
Epoch:25, Train loss:0.000878, valid loss:0.000680
Epoch:26, Train loss:0.000882, valid loss:0.000568
Epoch:27, Train loss:0.000876, valid loss:0.000575
Epoch:28, Train loss:0.000867, valid loss:0.000609
Epoch:29, Train loss:0.000853, valid loss:0.000521
Epoch:30, Train loss:0.000834, valid loss:0.000594
Epoch:31, Train loss:0.000741, valid loss:0.000538
Epoch:32, Train loss:0.000733, valid loss:0.000576
Epoch:33, Train loss:0.000725, valid loss:0.000500
Epoch:34, Train loss:0.000725, valid loss:0.000552
Epoch:35, Train loss:0.000725, valid loss:0.000525
Epoch:36, Train loss:0.000720, valid loss:0.000524
Epoch:37, Train loss:0.000715, valid loss:0.000498
Epoch:38, Train loss:0.000705, valid loss:0.000494
Epoch:39, Train loss:0.000712, valid loss:0.000487
Epoch:40, Train loss:0.000701, valid loss:0.000483
Epoch:41, Train loss:0.000655, valid loss:0.000481
Epoch:42, Train loss:0.000648, valid loss:0.000484
Epoch:43, Train loss:0.000647, valid loss:0.000476
Epoch:44, Train loss:0.000645, valid loss:0.000471
Epoch:45, Train loss:0.000649, valid loss:0.000472
Epoch:46, Train loss:0.000642, valid loss:0.000482
Epoch:47, Train loss:0.000641, valid loss:0.000479
Epoch:48, Train loss:0.000639, valid loss:0.000467
Epoch:49, Train loss:0.000635, valid loss:0.000477
Epoch:50, Train loss:0.000635, valid loss:0.000471
Epoch:51, Train loss:0.000612, valid loss:0.000462
Epoch:52, Train loss:0.000610, valid loss:0.000468
Epoch:53, Train loss:0.000610, valid loss:0.000463
Epoch:54, Train loss:0.000608, valid loss:0.000465
Epoch:55, Train loss:0.000608, valid loss:0.000469
Epoch:56, Train loss:0.000608, valid loss:0.000462
Epoch:57, Train loss:0.000607, valid loss:0.000449
Epoch:58, Train loss:0.000604, valid loss:0.000451
Epoch:59, Train loss:0.000604, valid loss:0.000462
Epoch:60, Train loss:0.000604, valid loss:0.000454
training time 7158.041286230087
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.014584116190161046
plot_id,batch_id 0 1 miss% 0.03270439670572233
plot_id,batch_id 0 2 miss% 0.02603733998306992
plot_id,batch_id 0 3 miss% 0.02587835620279013
plot_id,batch_id 0 4 miss% 0.025147214415513715
plot_id,batch_id 0 5 miss% 0.036947044420167705
plot_id,batch_id 0 6 miss% 0.02941510686466514
plot_id,batch_id 0 7 miss% 0.0330362506929439
plot_id,batch_id 0 8 miss% 0.02779798631681668
plot_id,batch_id 0 9 miss% 0.020971595875897142
plot_id,batch_id 0 10 miss% 0.04515494663061469
plot_id,batch_id 0 11 miss% 0.028936419465883673
plot_id,batch_id 0 12 miss% 0.03636914239621827
plot_id,batch_id 0 13 miss% 0.01910266504931087
plot_id,batch_id 0 14 miss% 0.02215472207205049
plot_id,batch_id 0 15 miss% 0.04306752192938635
plot_id,batch_id 0 16 miss% 0.025358317575396094
plot_id,batch_id 0 17 miss% 0.0370614843253345
plot_id,batch_id 0 18 miss% 0.03472992511020603
plot_id,batch_id 0 19 miss% 0.04335552543756215
plot_id,batch_id 0 20 miss% 0.03649244434476537
plot_id,batch_id 0 21 miss% 0.03314064159322153
plot_id,batch_id 0 22 miss% 0.021754469414337375
plot_id,batch_id 0 23 miss% 0.019963315150660313
plot_id,batch_id 0 24 miss% 0.021240324605163834
plot_id,batch_id 0 25 miss% 0.03662026436918916
plot_id,batch_id 0 26 miss% 0.03656594209715245
plot_id,batch_id 0 27 miss% 0.03420948868107364
plot_id,batch_id 0 28 miss% 0.01757592507479027
plot_id,batch_id 0 29 miss% 0.025160863885006747
plot_id,batch_id 0 30 miss% 0.0328806134469387
plot_id,batch_id 0 31 miss% 0.025965905369710957
plot_id,batch_id 0 32 miss% 0.025993025941265614
plot_id,batch_id 0 33 miss% 0.02773953535850136
plot_id,batch_id 0 34 miss% 0.025982842676841453
plot_id,batch_id 0 35 miss% 0.04405568736183384
plot_id,batch_id 0 36 miss% 0.027280370177342333
plot_id,batch_id 0 37 miss% 0.031087130276491685
plot_id,batch_id 0 38 miss% 0.018386863801813744
plot_id,batch_id 0 39 miss% 0.0181405708862499
plot_id,batch_id 0 40 miss% 0.06348523943301812
plot_id,batch_id 0 41 miss% 0.030311622300245814
plot_id,batch_id 0 42 miss% 0.021073257904904025
plot_id,batch_id 0 43 miss% 0.035024755402367796
plot_id,batch_id 0 44 miss% 0.021233355393414487
plot_id,batch_id 0 45 miss% 0.01705193471543965
plot_id,batch_id 0 46 miss% 0.01867091139571439
plot_id,batch_id 0 47 miss% 0.025666980385082788
plot_id,batch_id 0 48 miss% 0.018286374849939883
plot_id,batch_id 0 49 miss% 0.01582586855625516
plot_id,batch_id 0 50 miss% 0.024054847048752083
plot_id,batch_id 0 51 miss% 0.023593118579778062
plot_id,batch_id 0 52 miss% 0.02403752826835096
plot_id,batch_id 0 53 miss% 0.014285790863930909
plot_id,batch_id 0 54 miss% 0.023609980973040994
plot_id,batch_id 0 55 miss% 0.023349245183277658
plot_id,batch_id 0 56 miss% 0.02631081280923321
plot_id,batch_id 0 57 miss% 0.014741781469608358
plot_id,batch_id 0 58 miss% 0.026573118005905637
plot_id,batch_id 0 59 miss% 0.019004552627085215
plot_id,batch_id 0 60 miss% 0.03317452735734041
plot_id,batch_id 0 61 miss% 0.02947726276781371
plot_id,batch_id 0 62 miss% 0.025319125083879294
plot_id,batch_id 0 63 miss% 0.025137271458605077
plot_id,batch_id 0 64 miss% 0.04337673379787048
plot_id,batch_id 0 65 miss% 0.036567906498688525
plot_id,batch_id 0 66 miss% 0.04102458717743464
plot_id,batch_id 0 67 miss% 0.031232144396890326
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  35921
Epoch:0, Train loss:0.577978, valid loss:0.559762
Epoch:1, Train loss:0.040067, valid loss:0.006185
Epoch:2, Train loss:0.011138, valid loss:0.004030
Epoch:3, Train loss:0.008351, valid loss:0.004080
Epoch:4, Train loss:0.007153, valid loss:0.003472
Epoch:5, Train loss:0.006007, valid loss:0.003299
Epoch:6, Train loss:0.005369, valid loss:0.002815
Epoch:7, Train loss:0.005001, valid loss:0.002446
Epoch:8, Train loss:0.004950, valid loss:0.002451
Epoch:9, Train loss:0.004535, valid loss:0.002035
Epoch:10, Train loss:0.004317, valid loss:0.002536
Epoch:11, Train loss:0.003263, valid loss:0.001824
Epoch:12, Train loss:0.003244, valid loss:0.001643
Epoch:13, Train loss:0.003134, valid loss:0.001821
Epoch:14, Train loss:0.003101, valid loss:0.001773
Epoch:15, Train loss:0.002989, valid loss:0.001601
Epoch:16, Train loss:0.003014, valid loss:0.001496
Epoch:17, Train loss:0.002891, valid loss:0.001673
Epoch:18, Train loss:0.002859, valid loss:0.001883
Epoch:19, Train loss:0.002770, valid loss:0.001679
Epoch:20, Train loss:0.002742, valid loss:0.002337
Epoch:21, Train loss:0.002205, valid loss:0.001195
Epoch:22, Train loss:0.002196, valid loss:0.001298
Epoch:23, Train loss:0.002176, valid loss:0.001241
Epoch:24, Train loss:0.002194, valid loss:0.001473
Epoch:25, Train loss:0.002108, valid loss:0.001237
Epoch:26, Train loss:0.002093, valid loss:0.001225
Epoch:27, Train loss:0.002151, valid loss:0.001138
Epoch:28, Train loss:0.002056, valid loss:0.001217
Epoch:29, Train loss:0.002060, valid loss:0.001358
Epoch:30, Train loss:0.002082, valid loss:0.001222
Epoch:31, Train loss:0.001731, valid loss:0.001091
Epoch:32, Train loss:0.001728, valid loss:0.001082
Epoch:33, Train loss:0.001712, valid loss:0.001300
Epoch:34, Train loss:0.001711, valid loss:0.001050
Epoch:35, Train loss:0.001708, valid loss:0.001121
Epoch:36, Train loss:0.001748, valid loss:0.001071
Epoch:37, Train loss:0.001685, valid loss:0.001010
Epoch:38, Train loss:0.001721, valid loss:0.001165
Epoch:39, Train loss:0.001672, valid loss:0.001075
Epoch:40, Train loss:0.001700, valid loss:0.001131
Epoch:41, Train loss:0.001496, valid loss:0.001006
Epoch:42, Train loss:0.001484, valid loss:0.000943
Epoch:43, Train loss:0.001487, valid loss:0.001019
Epoch:44, Train loss:0.001487, valid loss:0.000963
Epoch:45, Train loss:0.001501, valid loss:0.000954
Epoch:46, Train loss:0.001472, valid loss:0.000952
Epoch:47, Train loss:0.001471, valid loss:0.000959
Epoch:48, Train loss:0.001459, valid loss:0.001043
Epoch:49, Train loss:0.001469, valid loss:0.000973
Epoch:50, Train loss:0.001456, valid loss:0.000936
Epoch:51, Train loss:0.001378, valid loss:0.000980
Epoch:52, Train loss:0.001375, valid loss:0.000919
Epoch:53, Train loss:0.001367, valid loss:0.000940
Epoch:54, Train loss:0.001369, valid loss:0.000924
Epoch:55, Train loss:0.001363, valid loss:0.000912
Epoch:56, Train loss:0.001357, valid loss:0.000927
Epoch:57, Train loss:0.001357, valid loss:0.000924
Epoch:58, Train loss:0.001356, valid loss:0.000923
Epoch:59, Train loss:0.001359, valid loss:0.000928
Epoch:60, Train loss:0.001352, valid loss:0.000920
training time 7146.6738448143005
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.030292091572791006
plot_id,batch_id 0 1 miss% 0.02512164026565835
plot_id,batch_id 0 2 miss% 0.02130400672143722
plot_id,batch_id 0 3 miss% 0.024746771646169365
plot_id,batch_id 0 4 miss% 0.02121903445650874
plot_id,batch_id 0 5 miss% 0.035792707610027084
plot_id,batch_id 0 6 miss% 0.02719099629148748
plot_id,batch_id 0 7 miss% 0.023914991563497547
plot_id,batch_id 0 8 miss% 0.030526902352378277
plot_id,batch_id 0 9 miss% 0.026653889283845187
plot_id,batch_id 0 10 miss% 0.03162500057687306
plot_id,batch_id 0 11 miss% 0.05430523214689064
plot_id,batch_id 0 12 miss% 0.027554506035859586
plot_id,batch_id 0 13 miss% 0.027499642664977952
plot_id,batch_id 0 14 miss% 0.02965005566095115
plot_id,batch_id 0 15 miss% 0.04142777622684339
plot_id,batch_id 0 16 miss% 0.02980498766174063
plot_id,batch_id 0 17 miss% 0.04413250122508816
plot_id,batch_id 0 18 miss% 0.030304817146636968
plot_id,batch_id 0 19 miss% 0.04622420485774007
plot_id,batch_id 0 20 miss% 0.057452015624308926
plot_id,batch_id 0 21 miss% 0.022339751175810982
plot_id,batch_id 0 22 miss% 0.030938315954663512
plot_id,batch_id 0 23 miss% 0.026808675713950108
plot_id,batch_id 0 24 miss% 0.02439691064765113
plot_id,batch_id 0 25 miss% 0.024960014861569162
plot_id,batch_id 0 26 miss% 0.02807572981106493
plot_id,batch_id 0 27 miss% 0.02852101596247804
plot_id,batch_id 0 28 miss% 0.026818412970129768
plot_id,batch_id 0 29 miss% 0.029333444357873103
plot_id,batch_id 0 30 miss% 0.030178865345503163
plot_id,batch_id 0 31 miss% 0.03079237743938333
plot_id,batch_id 0 32 miss% 0.04182655679460143
plot_id,batch_id 0 33 miss% 0.035108094814595044
plot_id,batch_id 0 34 miss% 0.03293325714311586
plot_id,batch_id 0 35 miss% 0.034470802845389475
plot_id,batch_id 0 36 miss% 0.03158696338050082
plot_id,batch_id 0 37 miss% 0.02565332432463048
plot_id,batch_id 0 38 miss% 0.03523114328162177
plot_id,batch_id 0 39 miss% 0.02718108117728027
plot_id,batch_id 0 40 miss% 0.06557979787334203
plot_id,batch_id 0 41 miss% 0.024873473133778534
plot_id,batch_id 0 42 miss% 0.022622378385449757
plot_id,batch_id 0 43 miss% 0.027612320920201876
plot_id,batch_id 0 44 miss% 0.01979874879013421
plot_id,batch_id 0 45 miss% 0.0315890345751602
plot_id,batch_id 0 46 miss% 0.013520049867203638
plot_id,batch_id 0 47 miss% 0.025233107224094633
plot_id,batch_id 0 48 miss% 0.019871002986671153
plot_id,batch_id 0 49 miss% 0.014576477890921993
plot_id,batch_id 0 50 miss% 0.025853301068887724
plot_id,batch_id 0 51 miss% 0.020273575174369025
plot_id,batch_id 0 52 miss% 0.01655055878320055
plot_id,batch_id 0 53 miss% 0.029700589575357747
plot_id,batch_id 0 54 miss% 0.026351833344170288
plot_id,batch_id 0 55 miss% 0.03161555710267985
plot_id,batch_id 0 56 miss% 0.01965896982160819
plot_id,batch_id 0 57 miss% 0.018155582563161302
plot_id,batch_id 0 58 miss% 0.02426801208822759
plot_id,batch_id 0 59 miss% 0.026978047517821024
plot_id,batch_id 0 60 miss% 0.0463714307343119
plot_id,batch_id 0 61 miss% 0.031182883956110997
plot_id,batch_id 0 62 miss% 0.025750445124236077
plot_id,batch_id 0 63 miss% 0.026554362035506696
plot_id,batch_id 0 64 miss% 0.0359857796467033
plot_id,batch_id 0 65 miss% 0.04876098359530205
plot_id,batch_id 0 66 miss% 0.044738397155900445
plot_id,batch_id 0 67 miss% 0.032196567686135914
plot_id,batch_id 0 68 miss% 0.02799907090729553
plot_id,batch_id 0 plot_id,batch_id 0 68 miss% 0.02350682872254463
plot_id,batch_id 0 69 miss% 0.030017246229524824
plot_id,batch_id 0 70 miss% 0.03555841842995988
plot_id,batch_id 0 71 miss% 0.04598980414682398
plot_id,batch_id 0 72 miss% 0.02231082638739481
plot_id,batch_id 0 73 miss% 0.021837896616429594
plot_id,batch_id 0 74 miss% 0.032341751973966035
plot_id,batch_id 0 75 miss% 0.04362362147286244
plot_id,batch_id 0 76 miss% 0.030738993977327565
plot_id,batch_id 0 77 miss% 0.02762984853962233
plot_id,batch_id 0 78 miss% 0.028359986279819453
plot_id,batch_id 0 79 miss% 0.051056625346760434
plot_id,batch_id 0 80 miss% 0.04361593593863773
plot_id,batch_id 0 81 miss% 0.025204964262878524
plot_id,batch_id 0 82 miss% 0.027441515811578186
plot_id,batch_id 0 83 miss% 0.023085012163485703
plot_id,batch_id 0 84 miss% 0.02318071135071476
plot_id,batch_id 0 85 miss% 0.04520092325204492
plot_id,batch_id 0 86 miss% 0.03022107822507176
plot_id,batch_id 0 87 miss% 0.02943838835047505
plot_id,batch_id 0 88 miss% 0.03726220365752234
plot_id,batch_id 0 89 miss% 0.02585250129049283
plot_id,batch_id 0 90 miss% 0.043664558289205166
plot_id,batch_id 0 91 miss% 0.04397578201824915
plot_id,batch_id 0 92 miss% 0.0359685513320709
plot_id,batch_id 0 93 miss% 0.02483136851643837
plot_id,batch_id 0 94 miss% 0.03774915334505822
plot_id,batch_id 0 95 miss% 0.040883280596536016
plot_id,batch_id 0 96 miss% 0.03872490178051921
plot_id,batch_id 0 97 miss% 0.043496842615348405
plot_id,batch_id 0 98 miss% 0.03356362802909637
plot_id,batch_id 0 99 miss% 0.02829802501986651
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01458412 0.0327044  0.02603734 0.02587836 0.02514721 0.03694704
 0.02941511 0.03303625 0.02779799 0.0209716  0.04515495 0.02893642
 0.03636914 0.01910267 0.02215472 0.04306752 0.02535832 0.03706148
 0.03472993 0.04335553 0.03649244 0.03314064 0.02175447 0.01996332
 0.02124032 0.03662026 0.03656594 0.03420949 0.01757593 0.02516086
 0.03288061 0.02596591 0.02599303 0.02773954 0.02598284 0.04405569
 0.02728037 0.03108713 0.01838686 0.01814057 0.06348524 0.03031162
 0.02107326 0.03502476 0.02123336 0.01705193 0.01867091 0.02566698
 0.01828637 0.01582587 0.02405485 0.02359312 0.02403753 0.01428579
 0.02360998 0.02334925 0.02631081 0.01474178 0.02657312 0.01900455
 0.03317453 0.02947726 0.02531913 0.02513727 0.04337673 0.03656791
 0.04102459 0.03123214 0.02350683 0.03001725 0.03555842 0.0459898
 0.02231083 0.0218379  0.03234175 0.04362362 0.03073899 0.02762985
 0.02835999 0.05105663 0.04361594 0.02520496 0.02744152 0.02308501
 0.02318071 0.04520092 0.03022108 0.02943839 0.0372622  0.0258525
 0.04366456 0.04397578 0.03596855 0.02483137 0.03774915 0.04088328
 0.0387249  0.04349684 0.03356363 0.02829803]
for model  168 the mean error 0.029941781108442547
all id 168 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 31 out win 3 err 0.029941781108442547
Launcher: Job 169 completed in 7346 seconds.
Launcher: Task 126 done. Exiting.
69 miss% 0.01883842671951533
plot_id,batch_id 0 70 miss% 0.05604254474894397
plot_id,batch_id 0 71 miss% 0.06887596873237228
plot_id,batch_id 0 72 miss% 0.03695764014093883
plot_id,batch_id 0 73 miss% 0.031153230889559184
plot_id,batch_id 0 74 miss% 0.030095045438018848
plot_id,batch_id 0 75 miss% 0.05162032491469882
plot_id,batch_id 0 76 miss% 0.04147282441403601
plot_id,batch_id 0 77 miss% 0.04095102782742337
plot_id,batch_id 0 78 miss% 0.05031888077288441
plot_id,batch_id 0 79 miss% 0.04369001173126114
plot_id,batch_id 0 80 miss% 0.03680286682310809
plot_id,batch_id 0 81 miss% 0.038721707896045086
plot_id,batch_id 0 82 miss% 0.04356450942014705
plot_id,batch_id 0 83 miss% 0.030094000670225877
plot_id,batch_id 0 84 miss% 0.03162558635005138
plot_id,batch_id 0 85 miss% 0.05821142458759913
plot_id,batch_id 0 86 miss% 0.03951148119314559
plot_id,batch_id 0 87 miss% 0.02683971356917706
plot_id,batch_id 0 88 miss% 0.02111846550796613
plot_id,batch_id 0 89 miss% 0.02773900659164127
plot_id,batch_id 0 90 miss% 0.04681362637319318
plot_id,batch_id 0 91 miss% 0.039433497111924454
plot_id,batch_id 0 92 miss% 0.033038609395871436
plot_id,batch_id 0 93 miss% 0.025265752925040907
plot_id,batch_id 0 94 miss% 0.044326335184838646
plot_id,batch_id 0 95 miss% 0.036518866881945306
plot_id,batch_id 0 96 miss% 0.03422663450124524
plot_id,batch_id 0 97 miss% 0.06068789753073101
plot_id,batch_id 0 98 miss% 0.033528917845608595
plot_id,batch_id 0 99 miss% 0.03542965442123312
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03029209 0.02512164 0.02130401 0.02474677 0.02121903 0.03579271
 0.027191   0.02391499 0.0305269  0.02665389 0.031625   0.05430523
 0.02755451 0.02749964 0.02965006 0.04142778 0.02980499 0.0441325
 0.03030482 0.0462242  0.05745202 0.02233975 0.03093832 0.02680868
 0.02439691 0.02496001 0.02807573 0.02852102 0.02681841 0.02933344
 0.03017887 0.03079238 0.04182656 0.03510809 0.03293326 0.0344708
 0.03158696 0.02565332 0.03523114 0.02718108 0.0655798  0.02487347
 0.02262238 0.02761232 0.01979875 0.03158903 0.01352005 0.02523311
 0.019871   0.01457648 0.0258533  0.02027358 0.01655056 0.02970059
 0.02635183 0.03161556 0.01965897 0.01815558 0.02426801 0.02697805
 0.04637143 0.03118288 0.02575045 0.02655436 0.03598578 0.04876098
 0.0447384  0.03219657 0.02799907 0.01883843 0.05604254 0.06887597
 0.03695764 0.03115323 0.03009505 0.05162032 0.04147282 0.04095103
 0.05031888 0.04369001 0.03680287 0.03872171 0.04356451 0.030094
 0.03162559 0.05821142 0.03951148 0.02683971 0.02111847 0.02773901
 0.04681363 0.0394335  0.03303861 0.02526575 0.04432634 0.03651887
 0.03422663 0.0606879  0.03352892 0.03542965]
for model  128 the mean error 0.032916353302318575
all id 128 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 25 out win 5 err 0.032916353302318575
Launcher: Job 129 completed in 7352 seconds.
Launcher: Task 63 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  107025
Epoch:0, Train loss:0.435344, valid loss:0.430397
Epoch:1, Train loss:0.038682, valid loss:0.005878
Epoch:2, Train loss:0.008519, valid loss:0.004072
Epoch:3, Train loss:0.006149, valid loss:0.003254
Epoch:4, Train loss:0.004983, valid loss:0.003505
Epoch:5, Train loss:0.004148, valid loss:0.002114
Epoch:6, Train loss:0.003859, valid loss:0.001989
Epoch:7, Train loss:0.003707, valid loss:0.002395
Epoch:8, Train loss:0.003511, valid loss:0.001873
Epoch:9, Train loss:0.003353, valid loss:0.001913
Epoch:10, Train loss:0.003453, valid loss:0.002567
Epoch:11, Train loss:0.002102, valid loss:0.001188
Epoch:12, Train loss:0.002059, valid loss:0.001388
Epoch:13, Train loss:0.002098, valid loss:0.001212
Epoch:14, Train loss:0.001999, valid loss:0.001202
Epoch:15, Train loss:0.001998, valid loss:0.001195
Epoch:16, Train loss:0.001917, valid loss:0.001282
Epoch:17, Train loss:0.001967, valid loss:0.001177
Epoch:18, Train loss:0.001926, valid loss:0.001857
Epoch:19, Train loss:0.001894, valid loss:0.001272
Epoch:20, Train loss:0.001859, valid loss:0.001231
Epoch:21, Train loss:0.001247, valid loss:0.000868
Epoch:22, Train loss:0.001267, valid loss:0.000787
Epoch:23, Train loss:0.001238, valid loss:0.000867
Epoch:24, Train loss:0.001226, valid loss:0.000820
Epoch:25, Train loss:0.001234, valid loss:0.000916
Epoch:26, Train loss:0.001229, valid loss:0.001011
Epoch:27, Train loss:0.001226, valid loss:0.000969
Epoch:28, Train loss:0.001182, valid loss:0.001040
Epoch:29, Train loss:0.001183, valid loss:0.000862
Epoch:30, Train loss:0.001163, valid loss:0.000830
Epoch:31, Train loss:0.000907, valid loss:0.000775
Epoch:32, Train loss:0.000883, valid loss:0.000670
Epoch:33, Train loss:0.000878, valid loss:0.000658
Epoch:34, Train loss:0.000890, valid loss:0.000751
Epoch:35, Train loss:0.000887, valid loss:0.000684
Epoch:36, Train loss:0.000880, valid loss:0.000687
Epoch:37, Train loss:0.000871, valid loss:0.000659
Epoch:38, Train loss:0.000879, valid loss:0.000685
Epoch:39, Train loss:0.000863, valid loss:0.000763
Epoch:40, Train loss:0.000860, valid loss:0.000686
Epoch:41, Train loss:0.000731, valid loss:0.000618
Epoch:42, Train loss:0.000710, valid loss:0.000598
Epoch:43, Train loss:0.000719, valid loss:0.000600
Epoch:44, Train loss:0.000713, valid loss:0.000659
Epoch:45, Train loss:0.000720, valid loss:0.000634
Epoch:46, Train loss:0.000705, valid loss:0.000617
Epoch:47, Train loss:0.000700, valid loss:0.000688
Epoch:48, Train loss:0.000710, valid loss:0.000746
Epoch:49, Train loss:0.000701, valid loss:0.000652
Epoch:50, Train loss:0.000711, valid loss:0.000654
Epoch:51, Train loss:0.000639, valid loss:0.000602
Epoch:52, Train loss:0.000636, valid loss:0.000618
Epoch:53, Train loss:0.000639, valid loss:0.000597
Epoch:54, Train loss:0.000634, valid loss:0.000599
Epoch:55, Train loss:0.000631, valid loss:0.000607
Epoch:56, Train loss:0.000632, valid loss:0.000587
Epoch:57, Train loss:0.000629, valid loss:0.000614
Epoch:58, Train loss:0.000625, valid loss:0.000586
Epoch:59, Train loss:0.000626, valid loss:0.000583
Epoch:60, Train loss:0.000625, valid loss:0.000585
training time 7179.258870124817
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.04027662217908481
plot_id,batch_id 0 1 miss% 0.019490132853992335
plot_id,batch_id 0 2 miss% 0.021364596956700732
plot_id,batch_id 0 3 miss% 0.023721183403966204
plot_id,batch_id 0 4 miss% 0.0357462225684882
plot_id,batch_id 0 5 miss% 0.028511570766841014
plot_id,batch_id 0 6 miss% 0.029859691632100234
plot_id,batch_id 0 7 miss% 0.028271520141058545
plot_id,batch_id 0 8 miss% 0.02630095501621486
plot_id,batch_id 0 9 miss% 0.022095294186744715
plot_id,batch_id 0 10 miss% 0.04248008265548133
plot_id,batch_id 0 11 miss% 0.051370102626814976
plot_id,batch_id 0 12 miss% 0.026955765112063042
plot_id,batch_id 0 13 miss% 0.018419911415967178
plot_id,batch_id 0 14 miss% 0.031924134573528745
plot_id,batch_id 0 15 miss% 0.054177649488075894
plot_id,batch_id 0 16 miss% 0.036782889415757865
plot_id,batch_id 0 17 miss% 0.04491375380582157
plot_id,batch_id 0 18 miss% 0.02589389617564098
plot_id,batch_id 0 19 miss% 0.03790222687017413
plot_id,batch_id 0 20 miss% 0.047804453969174446
plot_id,batch_id 0 21 miss% 0.028310038381860705
plot_id,batch_id 0 22 miss% 0.020098283030196954
plot_id,batch_id 0 23 miss% 0.024302568450583487
plot_id,batch_id 0 24 miss% 0.025971230153740477
plot_id,batch_id 0 25 miss% 0.025895949761752808
plot_id,batch_id 0 26 miss% 0.03239684373657107
plot_id,batch_id 0 27 miss% 0.02633144485875147
plot_id,batch_id 0 28 miss% 0.024476321944906284
plot_id,batch_id 0 29 miss% 0.028934540866168833
plot_id,batch_id 0 30 miss% 0.0416911430236085
plot_id,batch_id 0 31 miss% 0.024548385541894795
plot_id,batch_id 0 32 miss% 0.026498351222682986
plot_id,batch_id 0 33 miss% 0.028315721487603075
plot_id,batch_id 0 34 miss% 0.027617148974749255
plot_id,batch_id 0 35 miss% 0.04426544077649257
plot_id,batch_id 0 36 miss% 0.035902171127476165
plot_id,batch_id 0 37 miss% 0.029064920055042392
plot_id,batch_id 0 38 miss% 0.027652547208146565
plot_id,batch_id 0 39 miss% 0.024105255656095177
plot_id,batch_id 0 40 miss% 0.052511527570896464
plot_id,batch_id 0 41 miss% 0.02766436157664701
plot_id,batch_id 0 42 miss% 0.017947204896805694
plot_id,batch_id 0 43 miss% 0.025783207480738618
plot_id,batch_id 0 44 miss% 0.021894149924567265
plot_id,batch_id 0 45 miss% 0.04426457271998587
plot_id,batch_id 0 46 miss% 0.023417393368704745
plot_id,batch_id 0 47 miss% 0.02934786137939105
plot_id,batch_id 0 48 miss% 0.018478373940456868
plot_id,batch_id 0 49 miss% 0.020892822234317515
plot_id,batch_id 0 50 miss% 0.028950939994078266
plot_id,batch_id 0 51 miss% 0.024843965494691544
plot_id,batch_id 0 52 miss% 0.024517480958709332
plot_id,batch_id 0 53 miss% 0.016503163779350755
plot_id,batch_id 0 54 miss% 0.03697060255353649
plot_id,batch_id 0 55 miss% 0.03700946325998111
plot_id,batch_id 0 56 miss% 0.022238116938826297
plot_id,batch_id 0 57 miss% 0.021840038935200423
plot_id,batch_id 0 58 miss% 0.023428718997053587
plot_id,batch_id 0 59 miss% 0.023434384635889777
plot_id,batch_id 0 60 miss% 0.04416156239229941
plot_id,batch_id 0 61 miss% 0.04215976390972141
plot_id,batch_id 0 62 miss% 0.02390407534442195
plot_id,batch_id 0 63 miss% 0.036417470357848945
plot_id,batch_id 0 64 miss% 0.03211866972274803
plot_id,batch_id 0 65 miss% 0.02987090074351777
plot_id,batch_id 0 66 miss% 0.042261407201769116
plot_id,batch_id 0 67 miss% 0.032412461769921325
plot_id,batch_id 0 68 miss% 0.02816190930075416
plot_id,batch_id 0 69 miss% 0.015616281177008905
plot_id,batch_id 0 70 miss% 0.03908300714297398
plot_id,batch_id 0 71 miss% 0.06467261746260913
plot_id,batch_id 0 72 miss% 0.03588424399676393
plot_id,batch_id 0 73 miss% 0.028679099383656065
plot_id,batch_id 0 74 miss% 0.02844002546665772
plot_id,batch_id 0 75 miss% 0.03454297913263539
plot_id,batch_id 0 76 miss% 0.02434688015154113
plot_id,batch_id 0 77 miss% 0.01954879997145008
plot_id,batch_id 0 78 miss% 0.030786311629264207
plot_id,batch_id 0 79 miss% 0.02458345105827728
plot_id,batch_id 0 80 miss% 0.03781500622124135
plot_id,batch_id 0 81 miss% 0.016086879997812362
plot_id,batch_id 0 82 miss% 0.022220055412570685
plot_id,batch_id 0 83 miss% 0.036189054765392865
plot_id,batch_id 0 84 miss% 0.018483296640625455
plot_id,batch_id 0 85 miss% 0.035363577291868745
plot_id,batch_id 0 86 miss% 0.023512068622280287
plot_id,batch_id 0 87 miss% 0.027631672422369576
plot_id,batch_id 0 88 miss% 0.027378494430176405
plot_id,batch_id 0 89 miss% 0.02141212820475459
plot_id,batch_id 0 90 miss% 0.021771058903418683
plot_id,batch_id 0 91 miss% 0.02746656749069335
plot_id,batch_id 0 92 miss% 0.028963757575245715
plot_id,batch_id 0 93 miss% 0.03163785394048608
plot_id,batch_id 0 94 miss% 0.024222135093611773
plot_id,batch_id 0 95 miss% 0.03983295249756381
plot_id,batch_id 0 96 miss% 0.03138874570465185
plot_id,batch_id 0 97 miss% 0.03356025041196002
plot_id,batch_id 0 98 miss% 0.02260158825621863
plot_id,batch_id 0 99 miss% 0.023050736609568714
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04027662 0.01949013 0.0213646  0.02372118 0.03574622 0.02851157
 0.02985969 0.02827152 0.02630096 0.02209529 0.04248008 0.0513701
 0.02695577 0.01841991 0.03192413 0.05417765 0.03678289 0.04491375
 0.0258939  0.03790223 0.04780445 0.02831004 0.02009828 0.02430257
 0.02597123 0.02589595 0.03239684 0.02633144 0.02447632 0.02893454
 0.04169114 0.02454839 0.02649835 0.02831572 0.02761715 0.04426544
 0.03590217 0.02906492 0.02765255 0.02410526 0.05251153 0.02766436
 0.0179472  0.02578321 0.02189415 0.04426457 0.02341739 0.02934786
 0.01847837 0.02089282 0.02895094 0.02484397 0.02451748 0.01650316
 0.0369706  0.03700946 0.02223812 0.02184004 0.02342872 0.02343438
 0.04416156 0.04215976 0.02390408 0.03641747 0.03211867 0.0298709
 0.04226141 0.03241246 0.02816191 0.01561628 0.03908301 0.06467262
 0.03588424 0.0286791  0.02844003 0.03454298 0.02434688 0.0195488
 0.03078631 0.02458345 0.03781501 0.01608688 0.02222006 0.03618905
 0.0184833  0.03536358 0.02351207 0.02763167 0.02737849 0.02141213
 0.02177106 0.02746657 0.02896376 0.03163785 0.02422214 0.03983295
 0.03138875 0.03356025 0.02260159 0.02305074]
for model  150 the mean error 0.02990819110520196
all id 150 hidden_dim 32 learning_rate 0.02 num_layers 4 frames 25 out win 3 err 0.02990819110520196
Launcher: Job 151 completed in 7371 seconds.
Launcher: Task 29 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  46193
Epoch:0, Train loss:0.400516, valid loss:0.363185
Epoch:1, Train loss:0.227568, valid loss:0.230528
Epoch:2, Train loss:0.220197, valid loss:0.229831
Epoch:3, Train loss:0.219080, valid loss:0.229602
Epoch:4, Train loss:0.218515, valid loss:0.228988
Epoch:5, Train loss:0.218200, valid loss:0.229171
Epoch:6, Train loss:0.217965, valid loss:0.229195
Epoch:7, Train loss:0.217862, valid loss:0.228783
Epoch:8, Train loss:0.217713, valid loss:0.228987
Epoch:9, Train loss:0.217659, valid loss:0.228910
Epoch:10, Train loss:0.217599, valid loss:0.228768
Epoch:11, Train loss:0.217057, valid loss:0.228548
Epoch:12, Train loss:0.217052, valid loss:0.228542
Epoch:13, Train loss:0.217028, valid loss:0.228517
Epoch:14, Train loss:0.216988, valid loss:0.228654
Epoch:15, Train loss:0.216996, valid loss:0.228561
Epoch:16, Train loss:0.216979, valid loss:0.228574
Epoch:17, Train loss:0.216947, valid loss:0.228444
Epoch:18, Train loss:0.216942, valid loss:0.228399
Epoch:19, Train loss:0.216914, valid loss:0.228614
Epoch:20, Train loss:0.216908, valid loss:0.228370
Epoch:21, Train loss:0.216637, valid loss:0.228388
Epoch:22, Train loss:0.216629, valid loss:0.228352
Epoch:23, Train loss:0.216630, valid loss:0.228379
Epoch:24, Train loss:0.216641, valid loss:0.228455
Epoch:25, Train loss:0.216619, valid loss:0.228371
Epoch:26, Train loss:0.216608, valid loss:0.228358
Epoch:27, Train loss:0.216603, valid loss:0.228341
Epoch:28, Train loss:0.216587, valid loss:0.228275
Epoch:29, Train loss:0.216594, valid loss:0.228369
Epoch:30, Train loss:0.216574, valid loss:0.228307
Epoch:31, Train loss:0.216457, valid loss:0.228287
Epoch:32, Train loss:0.216443, valid loss:0.228296
Epoch:33, Train loss:0.216448, valid loss:0.228309
Epoch:34, Train loss:0.216451, valid loss:0.228247
Epoch:35, Train loss:0.216434, valid loss:0.228266
Epoch:36, Train loss:0.216451, valid loss:0.228291
Epoch:37, Train loss:0.216443, valid loss:0.228245
Epoch:38, Train loss:0.216428, valid loss:0.228285
Epoch:39, Train loss:0.216428, valid loss:0.228249
Epoch:40, Train loss:0.216417, valid loss:0.228285
Epoch:41, Train loss:0.216359, valid loss:0.228267
Epoch:42, Train loss:0.216363, valid loss:0.228244
Epoch:43, Train loss:0.216361, valid loss:0.228237
Epoch:44, Train loss:0.216354, valid loss:0.228273
Epoch:45, Train loss:0.216356, valid loss:0.228234
Epoch:46, Train loss:0.216352, valid loss:0.228245
Epoch:47, Train loss:0.216354, valid loss:0.228241
Epoch:48, Train loss:0.216347, valid loss:0.228253
Epoch:49, Train loss:0.216346, valid loss:0.228246
Epoch:50, Train loss:0.216342, valid loss:0.228235
Epoch:51, Train loss:0.216314, valid loss:0.228225
Epoch:52, Train loss:0.216315, valid loss:0.228230
Epoch:53, Train loss:0.216312, valid loss:0.228230
Epoch:54, Train loss:0.216314, valid loss:0.228227
Epoch:55, Train loss:0.216312, valid loss:0.228225
Epoch:56, Train loss:0.216310, valid loss:0.228237
Epoch:57, Train loss:0.216309, valid loss:0.228236
Epoch:58, Train loss:0.216306, valid loss:0.228230
Epoch:59, Train loss:0.216308, valid loss:0.228222
Epoch:60, Train loss:0.216305, valid loss:0.228232
training time 7198.565939664841
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.6547726924697036
plot_id,batch_id 0 1 miss% 0.7443342456591958
plot_id,batch_id 0 2 miss% 0.759813579882272
plot_id,batch_id 0 3 miss% 0.7716412096712018
plot_id,batch_id 0 4 miss% 0.7673791662613452
plot_id,batch_id 0 5 miss% 0.664703412262153
plot_id,batch_id 0 6 miss% 0.7421546578695115
plot_id,batch_id 0 7 miss% 0.7563703829550966
plot_id,batch_id 0 8 miss% 0.7646749521548023
plot_id,batch_id 0 9 miss% 0.7707991953372321
plot_id,batch_id 0 10 miss% 0.6345332437398918
plot_id,batch_id 0 11 miss% 0.7426734529172577
plot_id,batch_id 0 12 miss% 0.7540458046141906
plot_id,batch_id 0 13 miss% 0.7649424543211528
plot_id,batch_id 0 14 miss% 0.7697409305629243
plot_id,batch_id 0 15 miss% 0.6471412983263544
plot_id,batch_id 0 16 miss% 0.7368507321856578
plot_id,batch_id 0 17 miss% 0.7647182145769738
plot_id,batch_id 0 18 miss% 0.7637240433156931
plot_id,batch_id 0 19 miss% 0.7665736710372425
plot_id,batch_id 0 20 miss% 0.7126459789278629
plot_id,batch_id 0 21 miss% 0.763148040753785
plot_id,batch_id 0 22 miss% 0.7681274692458812
plot_id,batch_id 0 23 miss% 0.7753435229105771
plot_id,batch_id 0 24 miss% 0.7777646215550456
plot_id,batch_id 0 25 miss% 0.6957668147023349
plot_id,batch_id 0 26 miss% 0.7553499458992177
plot_id,batch_id 0 27 miss% 0.7695228608784097
plot_id,batch_id 0 28 miss% 0.7690698612415733
plot_id,batch_id 0 29 miss% 0.7766973174897631
plot_id,batch_id 0 30 miss% 0.7056403155113978
plot_id,batch_id 0 31 miss% 0.7531697346548669
plot_id,batch_id 0 32 miss% 0.7633298543035424
plot_id,batch_id 0 33 miss% 0.7691038350051635
plot_id,batch_id 0 34 miss% 0.7713944282111695
plot_id,batch_id 0 35 miss% 0.6980618877879509
plot_id,batch_id 0 36 miss% 0.757457359038282
plot_id,batch_id 0 37 miss% 0.7631865556232473
plot_id,batch_id 0 38 miss% 0.7706139196353947
plot_id,batch_id 0 39 miss% 0.77077218882892
plot_id,batch_id 0 40 miss% 0.735236979382611
plot_id,batch_id 0 41 miss% 0.7676923778591126
plot_id,batch_id 0 42 miss% 0.7688001645459757
plot_id,batch_id 0 43 miss% 0.7759143369785232
plot_id,batch_id 0 44 miss% 0.7836475091934857
plot_id,batch_id 0 45 miss% 0.7299106544610293
plot_id,batch_id 0 46 miss% 0.7666327734858908
plot_id,batch_id 0 47 miss% 0.7701715387648312
plot_id,batch_id 0 48 miss% 0.7764845634496984
plot_id,batch_id 0 49 miss% 0.7814414500018377
plot_id,batch_id 0 50 miss% 0.7397400605644542
plot_id,batch_id 0 51 miss% 0.7643585221562212
plot_id,batch_id 0 52 miss% 0.7705495846346492
plot_id,batch_id 0 53 miss% 0.7777363550191745
plot_id,batch_id 0 54 miss% 0.7845292755012552
plot_id,batch_id 0 55 miss% 0.7416139166508243
plot_id,batch_id 0 56 miss% 0.7669539984275805
plot_id,batch_id 0 57 miss% 0.7709605499857021
plot_id,batch_id 0 58 miss% 0.7787167165428988
plot_id,batch_id 0 59 miss% 0.7818574710759877
plot_id,batch_id 0 60 miss% 0.5618888463365097
plot_id,batch_id 0 61 miss% 0.7042408871451965
plot_id,batch_id 0 62 miss% 0.7309924252149771
plot_id,batch_id 0 63 miss% 0.7529440305324115
plot_id,batch_id 0 64 miss% 0.760154273974854
plot_id,batch_id 0 65 miss% 0.5518423130757497
plot_id,batch_id 0 66 miss% 0.697188208239789
plot_id,batch_id 0 67 miss% 0.7230117121459166
plot_id,batch_id 0 68 miss% 0.7488170128208509
plot_id,batch_id 0 69 miss% 0.7493136803155966
plot_id,batch_id 0 70 miss% 0.530792262616105
plot_id,batch_id 0 71 miss% 0.7101932118305199
plot_id,batch_id 0 72 miss% 0.7211483764481236
plot_id,batch_id 0 73 miss% 0.7380400076399638
plot_id,batch_id 0 74 miss% 0.7452604000869224
plot_id,batch_id 0 75 miss% 0.5179742024650434
plot_id,batch_id 0 76 miss% 0.6589224485234014
plot_id,batch_id 0 77 miss% 0.697528736988614
plot_id,batch_id 0 78 miss% 0.7335256264859621
plot_id,batch_id 0 79 miss% 0.7463374893989984
plot_id,batch_id 0 80 miss% 0.5931628352520564
plot_id,batch_id 0 81 miss% 0.7224736687281631
plot_id,batch_id 0 82 miss% 0.7463489593697641
plot_id,batch_id 0 83 miss% 0.755242618270393
plot_id,batch_id 0 84 miss% 0.7634650491590802
plot_id,batch_id 0 85 miss% 0.5870122991650759
plot_id,batch_id 0 86 miss% 0.7146742795901856
plot_id,batch_id 0 87 miss% 0.7420375802122154
plot_id,batch_id 0 88 miss% 0.7606981127783742
plot_id,batch_id 0 89 miss% 0.7659986143722908
plot_id,batch_id 0 90 miss% 0.5559048649488163
plot_id,batch_id 0 91 miss% 0.7145083828862177
plot_id,batch_id 0 92 miss% 0.7344998609889924
plot_id,batch_id 0 93 miss% 0.7438961988554114
plot_id,batch_id 0 94 miss% 0.7593111123806577
plot_id,batch_id 0 95 miss% 0.5477661102858568
plot_id,batch_id 0 96 miss% 0.7038535315248754
plot_id,batch_id 0 97 miss% 0.7259120335656124
plot_id,batch_id 0 98 miss% 0.7442612256833614
plot_id,batch_id 0 99 miss% 0.7518202400782844
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.65477269 0.74433425 0.75981358 0.77164121 0.76737917 0.66470341
 0.74215466 0.75637038 0.76467495 0.7707992  0.63453324 0.74267345
 0.7540458  0.76494245 0.76974093 0.6471413  0.73685073 0.76471821
 0.76372404 0.76657367 0.71264598 0.76314804 0.76812747 0.77534352
 0.77776462 0.69576681 0.75534995 0.76952286 0.76906986 0.77669732
 0.70564032 0.75316973 0.76332985 0.76910384 0.77139443 0.69806189
 0.75745736 0.76318656 0.77061392 0.77077219 0.73523698 0.76769238
 0.76880016 0.77591434 0.78364751 0.72991065 0.76663277 0.77017154
 0.77648456 0.78144145 0.73974006 0.76435852 0.77054958 0.77773636
 0.78452928 0.74161392 0.766954   0.77096055 0.77871672 0.78185747
 0.56188885 0.70424089 0.73099243 0.75294403 0.76015427 0.55184231
 0.69718821 0.72301171 0.74881701 0.74931368 0.53079226 0.71019321
 0.72114838 0.73804001 0.7452604  0.5179742  0.65892245 0.69752874
 0.73352563 0.74633749 0.59316284 0.72247367 0.74634896 0.75524262
 0.76346505 0.5870123  0.71467428 0.74203758 0.76069811 0.76599861
 0.55590486 0.71450838 0.73449986 0.7438962  0.75931111 0.54776611
 0.70385353 0.72591203 0.74426123 0.75182024]
for model  166 the mean error 0.7307366638138717
all id 166 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 31 out win 4 err 0.7307366638138717
Launcher: Job 167 completed in 7376 seconds.
Launcher: Task 207 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  35921
Epoch:0, Train loss:0.736710, valid loss:0.719174
Epoch:1, Train loss:0.061146, valid loss:0.023137
Epoch:2, Train loss:0.032328, valid loss:0.008270
Epoch:3, Train loss:0.014869, valid loss:0.007905
Epoch:4, Train loss:0.012234, valid loss:0.005363
Epoch:5, Train loss:0.010597, valid loss:0.004703
Epoch:6, Train loss:0.009606, valid loss:0.004005
Epoch:7, Train loss:0.008451, valid loss:0.004257
Epoch:8, Train loss:0.007487, valid loss:0.005825
Epoch:9, Train loss:0.007257, valid loss:0.004520
Epoch:10, Train loss:0.007029, valid loss:0.003596
Epoch:11, Train loss:0.005079, valid loss:0.002808
Epoch:12, Train loss:0.004784, valid loss:0.002666
Epoch:13, Train loss:0.004883, valid loss:0.002794
Epoch:14, Train loss:0.004567, valid loss:0.003556
Epoch:15, Train loss:0.004703, valid loss:0.002391
Epoch:16, Train loss:0.004488, valid loss:0.002135
Epoch:17, Train loss:0.004477, valid loss:0.002418
Epoch:18, Train loss:0.004226, valid loss:0.002528
Epoch:19, Train loss:0.004145, valid loss:0.002013
Epoch:20, Train loss:0.004092, valid loss:0.002115
Epoch:21, Train loss:0.003071, valid loss:0.001746
Epoch:22, Train loss:0.003179, valid loss:0.001833
Epoch:23, Train loss:0.003279, valid loss:0.003032
Epoch:24, Train loss:0.003035, valid loss:0.001954
Epoch:25, Train loss:0.003010, valid loss:0.001848
Epoch:26, Train loss:0.002991, valid loss:0.001668
Epoch:27, Train loss:0.003161, valid loss:0.001741
Epoch:28, Train loss:0.003041, valid loss:0.002429
Epoch:29, Train loss:0.002876, valid loss:0.001657
Epoch:30, Train loss:0.002866, valid loss:0.002313
Epoch:31, Train loss:0.002426, valid loss:0.001386
Epoch:32, Train loss:0.002376, valid loss:0.001464
Epoch:33, Train loss:0.002361, valid loss:0.001515
Epoch:34, Train loss:0.002354, valid loss:0.001437
Epoch:35, Train loss:0.002321, valid loss:0.001753
Epoch:36, Train loss:0.002353, valid loss:0.001548
Epoch:37, Train loss:0.002385, valid loss:0.001457
Epoch:38, Train loss:0.002341, valid loss:0.001684
Epoch:39, Train loss:0.002281, valid loss:0.001407
Epoch:40, Train loss:0.002314, valid loss:0.001567
Epoch:41, Train loss:0.002036, valid loss:0.001318
Epoch:42, Train loss:0.002022, valid loss:0.001295
Epoch:43, Train loss:0.002003, valid loss:0.001292
Epoch:44, Train loss:0.002016, valid loss:0.001336
Epoch:45, Train loss:0.002019, valid loss:0.001272
Epoch:46, Train loss:0.001967, valid loss:0.001294
Epoch:47, Train loss:0.001978, valid loss:0.001310
Epoch:48, Train loss:0.002003, valid loss:0.001276
Epoch:49, Train loss:0.001970, valid loss:0.001332
Epoch:50, Train loss:0.001950, valid loss:0.001234
Epoch:51, Train loss:0.001839, valid loss:0.001218
Epoch:52, Train loss:0.001826, valid loss:0.001173
Epoch:53, Train loss:0.001817, valid loss:0.001198
Epoch:54, Train loss:0.001820, valid loss:0.001202
Epoch:55, Train loss:0.001821, valid loss:0.001245
Epoch:56, Train loss:0.001812, valid loss:0.001214
Epoch:57, Train loss:0.001807, valid loss:0.001230
Epoch:58, Train loss:0.001801, valid loss:0.001219
Epoch:59, Train loss:0.001799, valid loss:0.001286
Epoch:60, Train loss:0.001810, valid loss:0.001243
training time 7212.232218742371
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.02044542709190694
plot_id,batch_id 0 1 miss% 0.02658775427633792
plot_id,batch_id 0 2 miss% 0.02624857491112615
plot_id,batch_id 0 3 miss% 0.023261995376121915
plot_id,batch_id 0 4 miss% 0.028685335243601034
plot_id,batch_id 0 5 miss% 0.03932033755503053
plot_id,batch_id 0 6 miss% 0.023320063516095075
plot_id,batch_id 0 7 miss% 0.02751398990337871
plot_id,batch_id 0 8 miss% 0.01996815932009401
plot_id,batch_id 0 9 miss% 0.02528484855376136
plot_id,batch_id 0 10 miss% 0.04391335715076133
plot_id,batch_id 0 11 miss% 0.037400750468409
plot_id,batch_id 0 12 miss% 0.03515698929003366
plot_id,batch_id 0 13 miss% 0.03196361460063482
plot_id,batch_id 0 14 miss% 0.0359675568909812
plot_id,batch_id 0 15 miss% 0.04484549063110884
plot_id,batch_id 0 16 miss% 0.033024486300287446
plot_id,batch_id 0 17 miss% 0.031980098823177386
plot_id,batch_id 0 18 miss% 0.02828005132294682
plot_id,batch_id 0 19 miss% 0.039601962071018446
plot_id,batch_id 0 20 miss% 0.05128172691240908
plot_id,batch_id 0 21 miss% 0.02257122392876885
plot_id,batch_id 0 22 miss% 0.03903727089619114
plot_id,batch_id 0 23 miss% 0.030939496278895938
plot_id,batch_id 0 24 miss% 0.03979762607824831
plot_id,batch_id 0 25 miss% 0.0351177323618492
plot_id,batch_id 0 26 miss% 0.03666968462458498
plot_id,batch_id 0 27 miss% 0.027737348625045895
plot_id,batch_id 0 28 miss% 0.02061191182164429
plot_id,batch_id 0 29 miss% 0.024794713159515567
plot_id,batch_id 0 30 miss% 0.039851510413328156
plot_id,batch_id 0 31 miss% 0.032301836892249025
plot_id,batch_id 0 32 miss% 0.04978252051735309
plot_id,batch_id 0 33 miss% 0.024382903093504004
plot_id,batch_id 0 34 miss% 0.024987399854478298
plot_id,batch_id 0 35 miss% 0.049102170142849295
plot_id,batch_id 0 36 miss% 0.05894241257720222
plot_id,batch_id 0 37 miss% 0.037273072803637536
plot_id,batch_id 0 38 miss% 0.03442691596433069
plot_id,batch_id 0 39 miss% 0.016901791864991123
plot_id,batch_id 0 40 miss% 0.042429951156650395
plot_id,batch_id 0 41 miss% 0.031827208101149064
plot_id,batch_id 0 42 miss% 0.023004291712464008
plot_id,batch_id 0 43 miss% 0.027622810967736576
plot_id,batch_id 0 44 miss% 0.021577431105173524
plot_id,batch_id 0 45 miss% 0.03504402788475723
plot_id,batch_id 0 46 miss% 0.02377239834414443
plot_id,batch_id 0 47 miss% 0.022427305609845984
plot_id,batch_id 0 48 miss% 0.02490053246727176
plot_id,batch_id 0 49 miss% 0.015799277621033073
plot_id,batch_id 0 50 miss% 0.04973838166608766
plot_id,batch_id 0 51 miss% 0.03095108441505338
plot_id,batch_id 0 52 miss% 0.02345005844719961
plot_id,batch_id 0 53 miss% 0.022287899952086146
plot_id,batch_id 0 54 miss% 0.019491321569027612
plot_id,batch_id 0 55 miss% 0.027674908880264835
plot_id,batch_id 0 56 miss% 0.04437113933456996
plot_id,batch_id 0 57 miss% 0.023915428830460987
plot_id,batch_id 0 58 miss% 0.02577376772902831
plot_id,batch_id 0 59 miss% 0.02692623828905848
plot_id,batch_id 0 60 miss% 0.04602972866878518
plot_id,batch_id 0 61 miss% 0.024256859445264966
plot_id,batch_id 0 62 miss% 0.03709995830342575
plot_id,batch_id 0 63 miss% 0.03780806562852085
plot_id,batch_id 0 64 miss% 0.0337858201365316
plot_id,batch_id 0 65 miss% 0.06680701884103207
plot_id,batch_id 0 66 miss% 0.07461748640836022
plot_id,batch_id 0 67 miss% 0.036326309727972884
plot_id,batch_id 0 68 miss% 0.028857710692265958
plot_id,batch_id 0 69 miss% 0.027746452921250615
plot_id,batch_id 0 70 miss% 0.054087352139216255
plot_id,batch_id 0 71 miss% 0.041412036956847975
plot_id,batch_id 0 72 miss% 0.032156838097791435
plot_id,batch_id 0 73 miss% 0.03195916629939545
plot_id,batch_id 0 74 miss% 0.05469491733059003
plot_id,batch_id 0 75 miss% 0.054788169545975214
plot_id,batch_id 0 76 miss% 0.057412136492168635
plot_id,batch_id 0 77 miss% 0.03882905856876445
plot_id,batch_id 0 78 miss% 0.041786709478139006
plot_id,batch_id 0 79 miss% 0.040888509144125604
plot_id,batch_id 0 80 miss% 0.04697957575473316
plot_id,batch_id 0 81 miss% 0.042826989714217764
plot_id,batch_id 0 82 miss% 0.03446784519429823
plot_id,batch_id 0 83 miss% 0.032674292494298984
plot_id,batch_id 0 84 miss% 0.02472835805276222
plot_id,batch_id 0 85 miss% 0.052812548210210135
plot_id,batch_id 0 86 miss% 0.03945792365946747
plot_id,batch_id 0 87 miss% 0.0339890916500453
plot_id,batch_id 0 88 miss% 0.0347284919273423
plot_id,batch_id 0 89 miss% 0.03540061226009498
plot_id,batch_id 0 90 miss% 0.02311277536161981
plot_id,batch_id 0 91 miss% 0.041261140475385365
plot_id,batch_id 0 92 miss% 0.034740705042235934
plot_id,batch_id 0 93 miss% 0.027990133396700347
plot_id,batch_id 0 94 miss% 0.047214277046621476
plot_id,batch_id 0 95 miss% 0.04429180078524679
plot_id,batch_id 0 96 miss% 0.047655941369387216
plot_id,batch_id 0 97 miss% 0.0571184450520129
plot_id,batch_id 0 98 miss% 0.03467139476175336
plot_id,batch_id 0 99 miss% 0.030454977816966437
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02044543 0.02658775 0.02624857 0.023262   0.02868534 0.03932034
 0.02332006 0.02751399 0.01996816 0.02528485 0.04391336 0.03740075
 0.03515699 0.03196361 0.03596756 0.04484549 0.03302449 0.0319801
 0.02828005 0.03960196 0.05128173 0.02257122 0.03903727 0.0309395
 0.03979763 0.03511773 0.03666968 0.02773735 0.02061191 0.02479471
 0.03985151 0.03230184 0.04978252 0.0243829  0.0249874  0.04910217
 0.05894241 0.03727307 0.03442692 0.01690179 0.04242995 0.03182721
 0.02300429 0.02762281 0.02157743 0.03504403 0.0237724  0.02242731
 0.02490053 0.01579928 0.04973838 0.03095108 0.02345006 0.0222879
 0.01949132 0.02767491 0.04437114 0.02391543 0.02577377 0.02692624
 0.04602973 0.02425686 0.03709996 0.03780807 0.03378582 0.06680702
 0.07461749 0.03632631 0.02885771 0.02774645 0.05408735 0.04141204
 0.03215684 0.03195917 0.05469492 0.05478817 0.05741214 0.03882906
 0.04178671 0.04088851 0.04697958 0.04282699 0.03446785 0.03267429
 0.02472836 0.05281255 0.03945792 0.03398909 0.03472849 0.03540061
 0.02311278 0.04126114 0.03474071 0.02799013 0.04721428 0.0442918
 0.04765594 0.05711845 0.03467139 0.03045498]
for model  74 the mean error 0.03510195201042777
all id 74 hidden_dim 16 learning_rate 0.02 num_layers 5 frames 21 out win 5 err 0.03510195201042777
Launcher: Job 75 completed in 7429 seconds.
Launcher: Task 165 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  107025
Epoch:0, Train loss:0.594995, valid loss:0.592053
Epoch:1, Train loss:0.041011, valid loss:0.006535
Epoch:2, Train loss:0.010004, valid loss:0.004581
Epoch:3, Train loss:0.006600, valid loss:0.003254
Epoch:4, Train loss:0.005602, valid loss:0.002906
Epoch:5, Train loss:0.005002, valid loss:0.002338
Epoch:6, Train loss:0.004416, valid loss:0.002057
Epoch:7, Train loss:0.004060, valid loss:0.002115
Epoch:8, Train loss:0.004150, valid loss:0.002213
Epoch:9, Train loss:0.003700, valid loss:0.001743
Epoch:10, Train loss:0.003474, valid loss:0.001780
Epoch:11, Train loss:0.002282, valid loss:0.001385
Epoch:12, Train loss:0.002282, valid loss:0.001510
Epoch:13, Train loss:0.002337, valid loss:0.001399
Epoch:14, Train loss:0.002321, valid loss:0.001972
Epoch:15, Train loss:0.002270, valid loss:0.001443
Epoch:16, Train loss:0.002193, valid loss:0.001315
Epoch:17, Train loss:0.002212, valid loss:0.001649
Epoch:18, Train loss:0.002072, valid loss:0.001239
Epoch:19, Train loss:0.002019, valid loss:0.001673
Epoch:20, Train loss:0.002068, valid loss:0.001295
Epoch:21, Train loss:0.001431, valid loss:0.000952
Epoch:22, Train loss:0.001410, valid loss:0.000919
Epoch:23, Train loss:0.001403, valid loss:0.000992
Epoch:24, Train loss:0.001410, valid loss:0.001031
Epoch:25, Train loss:0.001366, valid loss:0.001051
Epoch:26, Train loss:0.001403, valid loss:0.000904
Epoch:27, Train loss:0.001335, valid loss:0.001045
Epoch:28, Train loss:0.001359, valid loss:0.000990
Epoch:29, Train loss:0.001364, valid loss:0.000913
Epoch:30, Train loss:0.001313, valid loss:0.000879
Epoch:31, Train loss:0.001036, valid loss:0.000751
Epoch:32, Train loss:0.001038, valid loss:0.000759
Epoch:33, Train loss:0.001020, valid loss:0.000791
Epoch:34, Train loss:0.001034, valid loss:0.000752
Epoch:35, Train loss:0.001018, valid loss:0.000805
Epoch:36, Train loss:0.000997, valid loss:0.000821
Epoch:37, Train loss:0.001039, valid loss:0.000775
Epoch:38, Train loss:0.000982, valid loss:0.000757
Epoch:39, Train loss:0.001012, valid loss:0.000751
Epoch:40, Train loss:0.000969, valid loss:0.000745
Epoch:41, Train loss:0.000871, valid loss:0.000726
Epoch:42, Train loss:0.000858, valid loss:0.000715
Epoch:43, Train loss:0.000855, valid loss:0.000774
Epoch:44, Train loss:0.000865, valid loss:0.000754
Epoch:45, Train loss:0.000871, valid loss:0.000734
Epoch:46, Train loss:0.000858, valid loss:0.000738
Epoch:47, Train loss:0.000855, valid loss:0.000741
Epoch:48, Train loss:0.000846, valid loss:0.000753
Epoch:49, Train loss:0.000828, valid loss:0.000787
Epoch:50, Train loss:0.000851, valid loss:0.000697
Epoch:51, Train loss:0.000779, valid loss:0.000667
Epoch:52, Train loss:0.000775, valid loss:0.000698
Epoch:53, Train loss:0.000780, valid loss:0.000709
Epoch:54, Train loss:0.000772, valid loss:0.000692
Epoch:55, Train loss:0.000773, valid loss:0.000739
Epoch:56, Train loss:0.000773, valid loss:0.000684
Epoch:57, Train loss:0.000769, valid loss:0.000700
Epoch:58, Train loss:0.000768, valid loss:0.000694
Epoch:59, Train loss:0.000765, valid loss:0.000705
Epoch:60, Train loss:0.000764, valid loss:0.000705
training time 7244.233825445175
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.022138336701374942
plot_id,batch_id 0 1 miss% 0.02764910223000203
plot_id,batch_id 0 2 miss% 0.02237848113559768
plot_id,batch_id 0 3 miss% 0.027017308787909852
plot_id,batch_id 0 4 miss% 0.025464779683577764
plot_id,batch_id 0 5 miss% 0.033833461557014666
plot_id,batch_id 0 6 miss% 0.030826575059890407
plot_id,batch_id 0 7 miss% 0.019927059098661548
plot_id,batch_id 0 8 miss% 0.018169691076926127
plot_id,batch_id 0 9 miss% 0.02274599884436302
plot_id,batch_id 0 10 miss% 0.03336860554165949
plot_id,batch_id 0 11 miss% 0.032873908533588195
plot_id,batch_id 0 12 miss% 0.024630055666659434
plot_id,batch_id 0 13 miss% 0.023827673821071767
plot_id,batch_id 0 14 miss% 0.02612682914652463
plot_id,batch_id 0 15 miss% 0.028620609558051336
plot_id,batch_id 0 16 miss% 0.0251508299378072
plot_id,batch_id 0 17 miss% 0.03045947407265684
plot_id,batch_id 0 18 miss% 0.027309452289339373
plot_id,batch_id 0 19 miss% 0.02597018893288088
plot_id,batch_id 0 20 miss% 0.04509659753743813
plot_id,batch_id 0 21 miss% 0.016848328432095414
plot_id,batch_id 0 22 miss% 0.017968329172022363
plot_id,batch_id 0 23 miss% 0.019288244774132452
plot_id,batch_id 0 24 miss% 0.022177060860375786
plot_id,batch_id 0 25 miss% 0.028923217275830033
plot_id,batch_id 0 26 miss% 0.022498750881042216
plot_id,batch_id 0 27 miss% 0.01869668482940928
plot_id,batch_id 0 28 miss% 0.022559254309447295
plot_id,batch_id 0 29 miss% 0.02201761641565128
plot_id,batch_id 0 30 miss% 0.06347413988083463
plot_id,batch_id 0 31 miss% 0.021555027420171322
plot_id,batch_id 0 32 miss% 0.02207968608392545
plot_id,batch_id 0 33 miss% 0.018948301104853533
plot_id,batch_id 0 34 miss% 0.027026419937267794
plot_id,batch_id 0 35 miss% 0.03183038164939201
plot_id,batch_id 0 36 miss% 0.03273029103963278
plot_id,batch_id 0 37 miss% 0.028791699724167542
plot_id,batch_id 0 38 miss% 0.018015059794573105
plot_id,batch_id 0 39 miss% 0.01878628406131806
plot_id,batch_id 0 40 miss% 0.08731392176045275
plot_id,batch_id 0 41 miss% 0.020486250961165374
plot_id,batch_id 0 42 miss% 0.013723763897892205
plot_id,batch_id 0 43 miss% 0.026733648121953436
plot_id,batch_id 0 44 miss% 0.021460128733062602
plot_id,batch_id 0 45 miss% 0.02621234918396211
plot_id,batch_id 0 46 miss% 0.019877555384771657
plot_id,batch_id 0 47 miss% 0.022162302864581974
plot_id,batch_id 0 48 miss% 0.023338451021388137
plot_id,batch_id 0 49 miss% 0.015486466841774233
plot_id,batch_id 0 50 miss% 0.031035510856210497
plot_id,batch_id 0 51 miss% 0.018691888335884244
plot_id,batch_id 0 52 miss% 0.01685399277156117
plot_id,batch_id 0 53 miss% 0.010749580166886989
plot_id,batch_id 0 54 miss% 0.02547939377747519
plot_id,batch_id 0 55 miss% 0.027911131508425013
plot_id,batch_id 0 56 miss% 0.020688079982910872
plot_id,batch_id 0 57 miss% 0.023598218220216995
plot_id,batch_id 0 58 miss% 0.02802991883164276
plot_id,batch_id 0 59 miss% 0.027999097905111472
plot_id,batch_id 0 60 miss% 0.04362897808785066
plot_id,batch_id 0 61 miss% 0.022737713925220143
plot_id,batch_id 0 62 miss% 0.028127717067433985
plot_id,batch_id 0 63 miss% 0.023275658279012505
plot_id,batch_id 0 64 miss% 0.03136749680633234
plot_id,batch_id 0 65 miss% 0.05203997088795639
plot_id,batch_id 0 66 miss% 0.036750374589675804
plot_id,batch_id 0 67 miss% 0.029218119568083425
plot_id,batch_id 0 68 miss% 0.018127149318273885
plot_id,batch_id 0 69 miss% the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  28945
Epoch:0, Train loss:0.332371, valid loss:0.324912
Epoch:1, Train loss:0.027923, valid loss:0.006147
Epoch:2, Train loss:0.007970, valid loss:0.003744
Epoch:3, Train loss:0.005587, valid loss:0.002667
Epoch:4, Train loss:0.004754, valid loss:0.003068
Epoch:5, Train loss:0.004138, valid loss:0.002318
Epoch:6, Train loss:0.003702, valid loss:0.002080
Epoch:7, Train loss:0.003301, valid loss:0.002187
Epoch:8, Train loss:0.003075, valid loss:0.001583
Epoch:9, Train loss:0.002875, valid loss:0.001765
Epoch:10, Train loss:0.002847, valid loss:0.001462
Epoch:11, Train loss:0.002212, valid loss:0.001256
Epoch:12, Train loss:0.002169, valid loss:0.001464
Epoch:13, Train loss:0.002146, valid loss:0.001285
Epoch:14, Train loss:0.002111, valid loss:0.001335
Epoch:15, Train loss:0.002092, valid loss:0.001172
Epoch:16, Train loss:0.002033, valid loss:0.001095
Epoch:17, Train loss:0.002033, valid loss:0.001211
Epoch:18, Train loss:0.001952, valid loss:0.001155
Epoch:19, Train loss:0.001952, valid loss:0.001178
Epoch:20, Train loss:0.001913, valid loss:0.001128
Epoch:21, Train loss:0.001600, valid loss:0.000923
Epoch:22, Train loss:0.001607, valid loss:0.001025
Epoch:23, Train loss:0.001589, valid loss:0.000938
Epoch:24, Train loss:0.001591, valid loss:0.000913
Epoch:25, Train loss:0.001568, valid loss:0.000916
Epoch:26, Train loss:0.001535, valid loss:0.000840
Epoch:27, Train loss:0.001562, valid loss:0.000894
Epoch:28, Train loss:0.001508, valid loss:0.000860
Epoch:29, Train loss:0.001528, valid loss:0.000946
Epoch:30, Train loss:0.001553, valid loss:0.000964
Epoch:31, Train loss:0.001325, valid loss:0.000836
Epoch:32, Train loss:0.001335, valid loss:0.000832
Epoch:33, Train loss:0.001328, valid loss:0.000815
Epoch:34, Train loss:0.001319, valid loss:0.000855
Epoch:35, Train loss:0.001310, valid loss:0.000834
Epoch:36, Train loss:0.001300, valid loss:0.000791
Epoch:37, Train loss:0.001304, valid loss:0.000808
Epoch:38, Train loss:0.001298, valid loss:0.000754
Epoch:39, Train loss:0.001293, valid loss:0.000808
Epoch:40, Train loss:0.001302, valid loss:0.000792
Epoch:41, Train loss:0.001207, valid loss:0.000785
Epoch:42, Train loss:0.001191, valid loss:0.000757
Epoch:43, Train loss:0.001200, valid loss:0.000783
Epoch:44, Train loss:0.001187, valid loss:0.000748
Epoch:45, Train loss:0.001186, valid loss:0.000737
Epoch:46, Train loss:0.001188, valid loss:0.000730
Epoch:47, Train loss:0.001186, valid loss:0.000743
Epoch:48, Train loss:0.001181, valid loss:0.000747
Epoch:49, Train loss:0.001181, valid loss:0.000729
Epoch:50, Train loss:0.001177, valid loss:0.000736
Epoch:51, Train loss:0.001131, valid loss:0.000726
Epoch:52, Train loss:0.001126, valid loss:0.000739
Epoch:53, Train loss:0.001127, valid loss:0.000730
Epoch:54, Train loss:0.001124, valid loss:0.000736
Epoch:55, Train loss:0.001123, valid loss:0.000713
Epoch:56, Train loss:0.001123, valid loss:0.000713
Epoch:57, Train loss:0.001118, valid loss:0.000725
Epoch:58, Train loss:0.001120, valid loss:0.000734
Epoch:59, Train loss:0.001116, valid loss:0.000722
Epoch:60, Train loss:0.001116, valid loss:0.000722
training time 7244.117102146149
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.03167527528469836
plot_id,batch_id 0 1 miss% 0.024750375709811805
plot_id,batch_id 0 2 miss% 0.02288662440108052
plot_id,batch_id 0 3 miss% 0.028212347903118086
plot_id,batch_id 0 4 miss% 0.031078062560995982
plot_id,batch_id 0 5 miss% 0.047009647611139196
plot_id,batch_id 0 6 miss% 0.0340991530254903
plot_id,batch_id 0 7 miss% 0.0299829066469072
plot_id,batch_id 0 8 miss% 0.03774842531565197
plot_id,batch_id 0 9 miss% 0.016962018805050435
plot_id,batch_id 0 10 miss% 0.044434071852996845
plot_id,batch_id 0 11 miss% 0.041192575260008024
plot_id,batch_id 0 12 miss% 0.03355482109420672
plot_id,batch_id 0 13 miss% 0.029525532930760587
plot_id,batch_id 0 14 miss% 0.03332457019229347
plot_id,batch_id 0 15 miss% 0.04939350412070571
plot_id,batch_id 0 16 miss% 0.04487564840856888
plot_id,batch_id 0 17 miss% 0.03344320703772284
plot_id,batch_id 0 18 miss% 0.03465577204552694
plot_id,batch_id 0 19 miss% 0.04286835620470112
plot_id,batch_id 0 20 miss% 0.056726944970747276
plot_id,batch_id 0 21 miss% 0.02582810864901002
plot_id,batch_id 0 22 miss% 0.030005427459322612
plot_id,batch_id 0 23 miss% 0.03899564884284006
plot_id,batch_id 0 24 miss% 0.027343899497756622
plot_id,batch_id 0 25 miss% 0.04462803782455292
plot_id,batch_id 0 26 miss% 0.029184043190495116
plot_id,batch_id 0 27 miss% 0.030575254544857638
plot_id,batch_id 0 28 miss% 0.021039680889927356
plot_id,batch_id 0 29 miss% 0.02244711096444635
plot_id,batch_id 0 30 miss% 0.04274421513349535
plot_id,batch_id 0 31 miss% 0.03864627582730395
plot_id,batch_id 0 32 miss% 0.022621443376727644
plot_id,batch_id 0 33 miss% 0.019575806707247644
plot_id,batch_id 0 34 miss% 0.03703610344366994
plot_id,batch_id 0 35 miss% 0.04253861678119201
plot_id,batch_id 0 36 miss% 0.05394148260354582
plot_id,batch_id 0 37 miss% 0.03976161575346214
plot_id,batch_id 0 38 miss% 0.03179358975961176
plot_id,batch_id 0 39 miss% 0.018330732543984287
plot_id,batch_id 0 40 miss% 0.06320347667805884
plot_id,batch_id 0 41 miss% 0.03213519895638846
plot_id,batch_id 0 42 miss% 0.023128102932451895
plot_id,batch_id 0 43 miss% 0.029766515585610054
plot_id,batch_id 0 44 miss% 0.02331995695754884
plot_id,batch_id 0 45 miss% 0.03057178801621682
plot_id,batch_id 0 46 miss% 0.026824197343083243
plot_id,batch_id 0 47 miss% 0.02375500388402395
plot_id,batch_id 0 48 miss% 0.029420103333435636
plot_id,batch_id 0 49 miss% 0.021227300462293833
plot_id,batch_id 0 50 miss% 0.05066329114290461
plot_id,batch_id 0 51 miss% 0.03223070821994189
plot_id,batch_id 0 52 miss% 0.027706795522564664
plot_id,batch_id 0 53 miss% 0.019626545426365733
plot_id,batch_id 0 54 miss% 0.030390142890208385
plot_id,batch_id 0 55 miss% 0.04451464488716141
plot_id,batch_id 0 56 miss% 0.03194526614897979
plot_id,batch_id 0 57 miss% 0.03298650163313944
plot_id,batch_id 0 58 miss% 0.03380138204166306
plot_id,batch_id 0 59 miss% 0.02464166018725459
plot_id,batch_id 0 60 miss% 0.03063561937876019
plot_id,batch_id 0 61 miss% 0.027410135518934665
plot_id,batch_id 0 62 miss% 0.026184696293865273
plot_id,batch_id 0 63 miss% 0.03330293336511941
plot_id,batch_id 0 64 miss% 0.03138082958463631
plot_id,batch_id 0 65 miss% 0.05366542904433692
plot_id,batch_id 0 66 miss% 0.03878218433219914
plot_id,batch_id 0 67 miss% 0.04615189880584629
plot_id,batch_id 0the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  107025
Epoch:0, Train loss:0.594995, valid loss:0.592053
Epoch:1, Train loss:0.057084, valid loss:0.009343
Epoch:2, Train loss:0.013612, valid loss:0.006149
Epoch:3, Train loss:0.009358, valid loss:0.004615
Epoch:4, Train loss:0.007910, valid loss:0.004807
Epoch:5, Train loss:0.006728, valid loss:0.003316
Epoch:6, Train loss:0.005673, valid loss:0.003586
Epoch:7, Train loss:0.005081, valid loss:0.002984
Epoch:8, Train loss:0.004851, valid loss:0.002335
Epoch:9, Train loss:0.004679, valid loss:0.004155
Epoch:10, Train loss:0.004333, valid loss:0.002335
Epoch:11, Train loss:0.002808, valid loss:0.001779
Epoch:12, Train loss:0.002805, valid loss:0.001479
Epoch:13, Train loss:0.002749, valid loss:0.001732
Epoch:14, Train loss:0.002738, valid loss:0.002509
Epoch:15, Train loss:0.002658, valid loss:0.001759
Epoch:16, Train loss:0.002632, valid loss:0.001900
Epoch:17, Train loss:0.002539, valid loss:0.001937
Epoch:18, Train loss:0.002533, valid loss:0.001424
Epoch:19, Train loss:0.002472, valid loss:0.001802
Epoch:20, Train loss:0.002329, valid loss:0.001519
Epoch:21, Train loss:0.001624, valid loss:0.001073
Epoch:22, Train loss:0.001583, valid loss:0.001060
Epoch:23, Train loss:0.001630, valid loss:0.001064
Epoch:24, Train loss:0.001587, valid loss:0.001112
Epoch:25, Train loss:0.001524, valid loss:0.001159
Epoch:26, Train loss:0.001606, valid loss:0.001025
Epoch:27, Train loss:0.001549, valid loss:0.001090
Epoch:28, Train loss:0.001556, valid loss:0.001059
Epoch:29, Train loss:0.001456, valid loss:0.000889
Epoch:30, Train loss:0.001531, valid loss:0.001074
Epoch:31, Train loss:0.001126, valid loss:0.000781
Epoch:32, Train loss:0.001106, valid loss:0.000906
Epoch:33, Train loss:0.001118, valid loss:0.000793
Epoch:34, Train loss:0.001150, valid loss:0.000773
Epoch:35, Train loss:0.001111, valid loss:0.000952
Epoch:36, Train loss:0.001116, valid loss:0.000883
Epoch:37, Train loss:0.001087, valid loss:0.000851
Epoch:38, Train loss:0.001081, valid loss:0.000774
Epoch:39, Train loss:0.001100, valid loss:0.000803
Epoch:40, Train loss:0.001062, valid loss:0.000828
Epoch:41, Train loss:0.000938, valid loss:0.000788
Epoch:42, Train loss:0.000907, valid loss:0.000703
Epoch:43, Train loss:0.000892, valid loss:0.000707
Epoch:44, Train loss:0.000921, valid loss:0.000744
Epoch:45, Train loss:0.000883, valid loss:0.000748
Epoch:46, Train loss:0.000889, valid loss:0.000736
Epoch:47, Train loss:0.000903, valid loss:0.000747
Epoch:48, Train loss:0.000878, valid loss:0.000872
Epoch:49, Train loss:0.000882, valid loss:0.000698
Epoch:50, Train loss:0.000874, valid loss:0.000731
Epoch:51, Train loss:0.000803, valid loss:0.000666
Epoch:52, Train loss:0.000800, valid loss:0.000671
Epoch:53, Train loss:0.000801, valid loss:0.000665
Epoch:54, Train loss:0.000796, valid loss:0.000680
Epoch:55, Train loss:0.000799, valid loss:0.000706
Epoch:56, Train loss:0.000791, valid loss:0.000704
Epoch:57, Train loss:0.000791, valid loss:0.000661
Epoch:58, Train loss:0.000794, valid loss:0.000738
Epoch:59, Train loss:0.000786, valid loss:0.000686
Epoch:60, Train loss:0.000783, valid loss:0.000657
training time 7248.451376676559
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03980642062612257
plot_id,batch_id 0 1 miss% 0.028184215620646467
plot_id,batch_id 0 2 miss% 0.03241539800508904
plot_id,batch_id 0 3 miss% 0.02669936720174014
plot_id,batch_id 0 4 miss% 0.026409771685539526
plot_id,batch_id 0 5 miss% 0.03259693348258373
plot_id,batch_id 0 6 miss% 0.031845122298881715
plot_id,batch_id 0 7 miss% 0.017749046003972126
plot_id,batch_id 0 8 miss% 0.0158832141582313
plot_id,batch_id 0 9 miss% 0.01322425116475084
plot_id,batch_id 0 10 miss% 0.02876657452815373
plot_id,batch_id 0 11 miss% 0.04064986223519261
plot_id,batch_id 0 12 miss% 0.02461465337012203
plot_id,batch_id 0 13 miss% 0.018864997841970745
plot_id,batch_id 0 14 miss% 0.02632650723792656
plot_id,batch_id 0 15 miss% 0.04466649289008266
plot_id,batch_id 0 16 miss% 0.039799898449686434
plot_id,batch_id 0 17 miss% 0.028212447853754263
plot_id,batch_id 0 18 miss% 0.030760504171254248
plot_id,batch_id 0 19 miss% 0.031124723654167488
plot_id,batch_id 0 20 miss% 0.04462514133122861
plot_id,batch_id 0 21 miss% 0.021717896917524272
plot_id,batch_id 0 22 miss% 0.020107182330993734
plot_id,batch_id 0 23 miss% 0.022135250581047893
plot_id,batch_id 0 24 miss% 0.022600006544296426
plot_id,batch_id 0 25 miss% 0.033299980423587484
plot_id,batch_id 0 26 miss% 0.029699552365937815
plot_id,batch_id 0 27 miss% 0.02160516061388034
plot_id,batch_id 0 28 miss% 0.02261438217188293
plot_id,batch_id 0 29 miss% 0.017576262586728116
plot_id,batch_id 0 30 miss% 0.0308946316497356
plot_id,batch_id 0 31 miss% 0.023906953109387702
plot_id,batch_id 0 32 miss% 0.021784705643112034
plot_id,batch_id 0 33 miss% 0.0179356813672617
plot_id,batch_id 0 34 miss% 0.018760814552363312
plot_id,batch_id 0 35 miss% 0.051932501043515364
plot_id,batch_id 0 36 miss% 0.022146630240813515
plot_id,batch_id 0 37 miss% 0.024164257969278145
plot_id,batch_id 0 38 miss% 0.02999257334377212
plot_id,batch_id 0 39 miss% 0.023161754778920393
plot_id,batch_id 0 40 miss% 0.10960983998405402
plot_id,batch_id 0 41 miss% 0.023630908301645437
plot_id,batch_id 0 42 miss% 0.012415579508298243
plot_id,batch_id 0 43 miss% 0.019770092293320763
plot_id,batch_id 0 44 miss% 0.01991802819285012
plot_id,batch_id 0 45 miss% 0.03043803623522422
plot_id,batch_id 0 46 miss% 0.023727739654623714
plot_id,batch_id 0 47 miss% 0.017398037928798064
plot_id,batch_id 0 48 miss% 0.019028919926446772
plot_id,batch_id 0 49 miss% 0.013298535378265114
plot_id,batch_id 0 50 miss% 0.03888969219700469
plot_id,batch_id 0 51 miss% 0.022831673291031133
plot_id,batch_id 0 52 miss% 0.0171541667140432
plot_id,batch_id 0 53 miss% 0.01186732246645839
plot_id,batch_id 0 54 miss% 0.02858754258929563
plot_id,batch_id 0 55 miss% 0.043966102928582745
plot_id,batch_id 0 56 miss% 0.028185654068260516
plot_id,batch_id 0 57 miss% 0.017201665198942475
plot_id,batch_id 0 58 miss% 0.02508244353518575
plot_id,batch_id 0 59 miss% 0.017191395268644487
plot_id,batch_id 0 60 miss% 0.06095972388955781
plot_id,batch_id 0 61 miss% 0.035513317228782135
plot_id,batch_id 0 62 miss% 0.027039509098191705
plot_id,batch_id 0 63 miss% 0.035084732427825296
plot_id,batch_id 0 64 miss% 0.03239628946096552
plot_id,batch_id 0 65 miss% 0.04732179024751366
plot_id,batch_id 0 66 miss% 0.02942787157657717
plot_id,batch_id 0 67 miss% 0.033531838280180706
plot_id,batch_id 0 68 miss% 0.02655313719260811
plot_id,batch_id 0 69 miss% 0.020696474011645077
plot_id,batch_id 0 70 miss% 0.03085703517429949
plot_id,batch_id 0 71 miss% 0.03467171319800742
plot_id,batch_id 0 72 miss% 0.024398901355685006
plot_id,batch_id 0 73 miss% 0.032854441540062305
plot_id,batch_id 0 74 miss% 0.0341251015249707
plot_id,batch_id 0 75 miss% 0.03385607790382947
plot_id,batch_id 0 76 miss% 0.05236781824780455
plot_id,batch_id 0 77 miss% 0.026015692283762572
plot_id,batch_id 0 78 miss% 0.025178390033206658
plot_id,batch_id 0 79 miss% 0.04273290299088303
plot_id,batch_id 0 80 miss% 0.04655532085899254
plot_id,batch_id 0 81 miss% 0.02242228469881567
plot_id,batch_id 0 82 miss% 0.028346972572787652
plot_id,batch_id 0 83 miss% 0.027685615735174852
plot_id,batch_id 0 84 miss% 0.025184073599108032
plot_id,batch_id 0 85 miss% 0.044231523374725426
plot_id,batch_id 0 86 miss% 0.019905043153041185
plot_id,batch_id 0 87 miss% 0.026171045792265302
plot_id,batch_id 0 88 miss% 0.030508052111391345
plot_id,batch_id 0 89 miss% 0.02335263797904445
plot_id,batch_id 0 90 miss% 0.030559024566999708
plot_id,batch_id 0 91 miss% 0.03833067879825233
plot_id,batch_id 0 92 miss% 0.03157764532273932
plot_id,batch_id 0 93 miss% 0.026825747496698527
plot_id,batch_id 0 94 miss% 0.030206605696952216
plot_id,batch_id 0 95 miss% 0.03200660566785699
plot_id,batch_id 0 96 miss% 0.024998633867126852
plot_id,batch_id 0 97 miss% 0.04168088265048416
plot_id,batch_id 0 98 miss% 0.029109542364249917
plot_id,batch_id 0 99 miss% 0.030701245894321463
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02213834 0.0276491  0.02237848 0.02701731 0.02546478 0.03383346
 0.03082658 0.01992706 0.01816969 0.022746   0.03336861 0.03287391
 0.02463006 0.02382767 0.02612683 0.02862061 0.02515083 0.03045947
 0.02730945 0.02597019 0.0450966  0.01684833 0.01796833 0.01928824
 0.02217706 0.02892322 0.02249875 0.01869668 0.02255925 0.02201762
 0.06347414 0.02155503 0.02207969 0.0189483  0.02702642 0.03183038
 0.03273029 0.0287917  0.01801506 0.01878628 0.08731392 0.02048625
 0.01372376 0.02673365 0.02146013 0.02621235 0.01987756 0.0221623
 0.02333845 0.01548647 0.03103551 0.01869189 0.01685399 0.01074958
 0.02547939 0.02791113 0.02068808 0.02359822 0.02802992 0.0279991
 0.04362898 0.02273771 0.02812772 0.02327566 0.0313675  0.05203997
 0.03675037 0.02921812 0.01812715 0.02069647 0.03085704 0.03467171
 0.0243989  0.03285444 0.0341251  0.03385608 0.05236782 0.02601569
 0.02517839 0.0427329  0.04655532 0.02242228 0.02834697 0.02768562
 0.02518407 0.04423152 0.01990504 0.02617105 0.03050805 0.02335264
 0.03055902 0.03833068 0.03157765 0.02682575 0.03020661 0.03200661
 0.02499863 0.04168088 0.02910954 0.03070125]
for model  42 the mean error 0.028090183569814905
all id 42 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 21 out win 3 err 0.028090183569814905
Launcher: Job 43 completed in 7445 seconds.
Launcher: Task 168 done. Exiting.
 68 miss% 0.02914562963789018
plot_id,batch_id 0 69 miss% 0.02525909978480902
plot_id,batch_id 0 70 miss% 0.05127523564949591
plot_id,batch_id 0 71 miss% 0.04661858174251681
plot_id,batch_id 0 72 miss% 0.036756517640295784
plot_id,batch_id 0 73 miss% 0.04827106938137671
plot_id,batch_id 0 74 miss% 0.039047220262259937
plot_id,batch_id 0 75 miss% 0.045416618612967585
plot_id,batch_id 0 76 miss% 0.04565357218950937
plot_id,batch_id 0 77 miss% 0.041455326142529884
plot_id,batch_id 0 78 miss% 0.052779626021135354
plot_id,batch_id 0 79 miss% 0.05098616599033845
plot_id,batch_id 0 80 miss% 0.042253221012232764
plot_id,batch_id 0 81 miss% 0.03515406355323687
plot_id,batch_id 0 82 miss% 0.02859432395930559
plot_id,batch_id 0 83 miss% 0.045811884166085656
plot_id,batch_id 0 84 miss% 0.038994611860967275
plot_id,batch_id 0 85 miss% 0.049298387408131604
plot_id,batch_id 0 86 miss% 0.025907998003294568
plot_id,batch_id 0 87 miss% 0.0459687487597276
plot_id,batch_id 0 88 miss% 0.042039243754497344
plot_id,batch_id 0 89 miss% 0.037545853111710915
plot_id,batch_id 0 90 miss% 0.06846502578594396
plot_id,batch_id 0 91 miss% 0.03205112420431527
plot_id,batch_id 0 92 miss% 0.04137652827218389
plot_id,batch_id 0 93 miss% 0.04009011323681802
plot_id,batch_id 0 94 miss% 0.04110893179286328
plot_id,batch_id 0 95 miss% 0.07467949849188836
plot_id,batch_id 0 96 miss% 0.029162690378821715
plot_id,batch_id 0 97 miss% 0.058042200410127726
plot_id,batch_id 0 98 miss% 0.049479227960053516
plot_id,batch_id 0 99 miss% 0.05263510065727166
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03167528 0.02475038 0.02288662 0.02821235 0.03107806 0.04700965
 0.03409915 0.02998291 0.03774843 0.01696202 0.04443407 0.04119258
 0.03355482 0.02952553 0.03332457 0.0493935  0.04487565 0.03344321
 0.03465577 0.04286836 0.05672694 0.02582811 0.03000543 0.03899565
 0.0273439  0.04462804 0.02918404 0.03057525 0.02103968 0.02244711
 0.04274422 0.03864628 0.02262144 0.01957581 0.0370361  0.04253862
 0.05394148 0.03976162 0.03179359 0.01833073 0.06320348 0.0321352
 0.0231281  0.02976652 0.02331996 0.03057179 0.0268242  0.023755
 0.0294201  0.0212273  0.05066329 0.03223071 0.0277068  0.01962655
 0.03039014 0.04451464 0.03194527 0.0329865  0.03380138 0.02464166
 0.03063562 0.02741014 0.0261847  0.03330293 0.03138083 0.05366543
 0.03878218 0.0461519  0.02914563 0.0252591  0.05127524 0.04661858
 0.03675652 0.04827107 0.03904722 0.04541662 0.04565357 0.04145533
 0.05277963 0.05098617 0.04225322 0.03515406 0.02859432 0.04581188
 0.03899461 0.04929839 0.025908   0.04596875 0.04203924 0.03754585
 0.06846503 0.03205112 0.04137653 0.04009011 0.04110893 0.0746795
 0.02916269 0.0580422  0.04947923 0.0526351 ]
for model  173 the mean error 0.03676132683583227
all id 173 hidden_dim 16 learning_rate 0.005 num_layers 4 frames 31 out win 5 err 0.03676132683583227
Launcher: Job 174 completed in 7442 seconds.
Launcher: Task 25 done. Exiting.
0.025845480286805826
plot_id,batch_id 0 70 miss% 0.05908618107533558
plot_id,batch_id 0 71 miss% 0.0521249041549271
plot_id,batch_id 0 72 miss% 0.0400488692383226
plot_id,batch_id 0 73 miss% 0.026082067926707
plot_id,batch_id 0 74 miss% 0.029446016471394568
plot_id,batch_id 0 75 miss% 0.05732610241094512
plot_id,batch_id 0 76 miss% 0.03962619697610971
plot_id,batch_id 0 77 miss% 0.044011737724385165
plot_id,batch_id 0 78 miss% 0.03500306867020718
plot_id,batch_id 0 79 miss% 0.041615041676326056
plot_id,batch_id 0 80 miss% 0.03755826639524958
plot_id,batch_id 0 81 miss% 0.018030175901754018
plot_id,batch_id 0 82 miss% 0.021230138785356983
plot_id,batch_id 0 83 miss% 0.027384366015356532
plot_id,batch_id 0 84 miss% 0.0244373962701032
plot_id,batch_id 0 85 miss% 0.033984173957700294
plot_id,batch_id 0 86 miss% 0.02257516186254021
plot_id,batch_id 0 87 miss% 0.028152115256271978
plot_id,batch_id 0 88 miss% 0.032709080158700006
plot_id,batch_id 0 89 miss% 0.024916548028900823
plot_id,batch_id 0 90 miss% 0.04035010843370528
plot_id,batch_id 0 91 miss% 0.030522530808187415
plot_id,batch_id 0 92 miss% 0.020269279369299505
plot_id,batch_id 0 93 miss% 0.026096213212437432
plot_id,batch_id 0 94 miss% 0.033759822347064525
plot_id,batch_id 0 95 miss% 0.041082358628019865
plot_id,batch_id 0 96 miss% 0.03639129072683214
plot_id,batch_id 0 97 miss% 0.0455931326235823
plot_id,batch_id 0 98 miss% 0.029995406802494393
plot_id,batch_id 0 99 miss% 0.0259574939481084
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03980642 0.02818422 0.0324154  0.02669937 0.02640977 0.03259693
 0.03184512 0.01774905 0.01588321 0.01322425 0.02876657 0.04064986
 0.02461465 0.018865   0.02632651 0.04466649 0.0397999  0.02821245
 0.0307605  0.03112472 0.04462514 0.0217179  0.02010718 0.02213525
 0.02260001 0.03329998 0.02969955 0.02160516 0.02261438 0.01757626
 0.03089463 0.02390695 0.02178471 0.01793568 0.01876081 0.0519325
 0.02214663 0.02416426 0.02999257 0.02316175 0.10960984 0.02363091
 0.01241558 0.01977009 0.01991803 0.03043804 0.02372774 0.01739804
 0.01902892 0.01329854 0.03888969 0.02283167 0.01715417 0.01186732
 0.02858754 0.0439661  0.02818565 0.01720167 0.02508244 0.0171914
 0.06095972 0.03551332 0.02703951 0.03508473 0.03239629 0.04732179
 0.02942787 0.03353184 0.02655314 0.02584548 0.05908618 0.0521249
 0.04004887 0.02608207 0.02944602 0.0573261  0.0396262  0.04401174
 0.03500307 0.04161504 0.03755827 0.01803018 0.02123014 0.02738437
 0.0244374  0.03398417 0.02257516 0.02815212 0.03270908 0.02491655
 0.04035011 0.03052253 0.02026928 0.02609621 0.03375982 0.04108236
 0.03639129 0.04559313 0.02999541 0.02595749]
for model  69 the mean error 0.030184940332514433
all id 69 hidden_dim 32 learning_rate 0.02 num_layers 4 frames 21 out win 3 err 0.030184940332514433
Launcher: Job 70 completed in 7449 seconds.
Launcher: Task 179 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  46193
Epoch:0, Train loss:0.507051, valid loss:0.475426
Epoch:1, Train loss:0.359628, valid loss:0.362373
Epoch:2, Train loss:0.347791, valid loss:0.360072
Epoch:3, Train loss:0.346160, valid loss:0.359930
Epoch:4, Train loss:0.345378, valid loss:0.359788
Epoch:5, Train loss:0.344968, valid loss:0.359047
Epoch:6, Train loss:0.344270, valid loss:0.359626
Epoch:7, Train loss:0.344139, valid loss:0.359058
Epoch:8, Train loss:0.343982, valid loss:0.359130
Epoch:9, Train loss:0.343736, valid loss:0.359026
Epoch:10, Train loss:0.343711, valid loss:0.359366
Epoch:11, Train loss:0.342354, valid loss:0.358536
Epoch:12, Train loss:0.342289, valid loss:0.358657
Epoch:13, Train loss:0.342199, valid loss:0.358395
Epoch:14, Train loss:0.342137, valid loss:0.358303
Epoch:15, Train loss:0.342154, valid loss:0.358566
Epoch:16, Train loss:0.342157, valid loss:0.358462
Epoch:17, Train loss:0.342055, valid loss:0.358627
Epoch:18, Train loss:0.342091, valid loss:0.358348
Epoch:19, Train loss:0.342104, valid loss:0.358509
Epoch:20, Train loss:0.341958, valid loss:0.358244
Epoch:21, Train loss:0.341381, valid loss:0.358000
Epoch:22, Train loss:0.341386, valid loss:0.358017
Epoch:23, Train loss:0.341364, valid loss:0.357947
Epoch:24, Train loss:0.341347, valid loss:0.357982
Epoch:25, Train loss:0.341364, valid loss:0.357948
Epoch:26, Train loss:0.341329, valid loss:0.358050
Epoch:27, Train loss:0.341286, valid loss:0.358127
Epoch:28, Train loss:0.341304, valid loss:0.357925
Epoch:29, Train loss:0.341299, valid loss:0.358118
Epoch:30, Train loss:0.341289, valid loss:0.357965
Epoch:31, Train loss:0.340955, valid loss:0.358044
Epoch:32, Train loss:0.340942, valid loss:0.357902
Epoch:33, Train loss:0.340948, valid loss:0.357865
Epoch:34, Train loss:0.340947, valid loss:0.357914
Epoch:35, Train loss:0.340919, valid loss:0.357947
Epoch:36, Train loss:0.340946, valid loss:0.357879
Epoch:37, Train loss:0.340895, valid loss:0.357899
Epoch:38, Train loss:0.340893, valid loss:0.357909
Epoch:39, Train loss:0.340899, valid loss:0.357862
Epoch:40, Train loss:0.340902, valid loss:0.357866
Epoch:41, Train loss:0.340744, valid loss:0.357793
Epoch:42, Train loss:0.340719, valid loss:0.357778
Epoch:43, Train loss:0.340724, valid loss:0.357781
Epoch:44, Train loss:0.340732, valid loss:0.357848
Epoch:45, Train loss:0.340716, valid loss:0.357772
Epoch:46, Train loss:0.340738, valid loss:0.357776
Epoch:47, Train loss:0.340717, valid loss:0.357732
Epoch:48, Train loss:0.340706, valid loss:0.357776
Epoch:49, Train loss:0.340688, valid loss:0.357855
Epoch:50, Train loss:0.340712, valid loss:0.357802
Epoch:51, Train loss:0.340624, valid loss:0.357723
Epoch:52, Train loss:0.340613, valid loss:0.357720
Epoch:53, Train loss:0.340617, valid loss:0.357715
Epoch:54, Train loss:0.340618, valid loss:0.357754
Epoch:55, Train loss:0.340612, valid loss:0.357720
Epoch:56, Train loss:0.340611, valid loss:0.357711
Epoch:57, Train loss:0.340611, valid loss:0.357755
Epoch:58, Train loss:0.340610, valid loss:0.357807
Epoch:59, Train loss:0.340603, valid loss:0.357743
Epoch:60, Train loss:0.340602, valid loss:0.357767
training time 7363.911460161209
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7797626350850578
plot_id,batch_id 0 1 miss% 0.8304291030745216
plot_id,batch_id 0 2 miss% 0.833630173124541
plot_id,batch_id 0 3 miss% 0.840360146413329
plot_id,batch_id 0 4 miss% 0.8423100363281922
plot_id,batch_id 0 5 miss% 0.775969442365768
plot_id,batch_id 0 6 miss% 0.823553905320252
plot_id,batch_id 0 7 miss% 0.8357868320791106
plot_id,batch_id 0 8 miss% 0.8411065528876653
plot_id,batch_id 0 9 miss% 0.8433847681802238
plot_id,batch_id 0 10 miss% 0.7670229563249296
plot_id,batch_id 0 11 miss% 0.8232233659455236
plot_id,batch_id 0 12 miss% 0.8326007654222184
plot_id,batch_id 0 13 miss% 0.8398699655448029
plot_id,batch_id 0 14 miss% 0.8399271758295481
plot_id,batch_id 0 15 miss% 0.7706259646899803
plot_id,batch_id 0 16 miss% 0.8247247265622177
plot_id,batch_id 0 17 miss% 0.8338334342970994
plot_id,batch_id 0 18 miss% 0.8400185684501016
plot_id,batch_id 0 19 miss% 0.8428145305408852
plot_id,batch_id 0 20 miss% 0.8041385034305383
plot_id,batch_id 0 21 miss% 0.8355834083346039
plot_id,batch_id 0 22 miss% 0.8400237957566533
plot_id,batch_id 0 23 miss% 0.8433936121976785
plot_id,batch_id 0 24 miss% 0.844192411873376
plot_id,batch_id 0 25 miss% 0.8100444630725886
plot_id,batch_id 0 26 miss% 0.8359785161333875
plot_id,batch_id 0 27 miss% 0.8394185262901479
plot_id,batch_id 0 28 miss% 0.8423957399498638
plot_id,batch_id 0 29 miss% 0.843203326268693
plot_id,batch_id 0 30 miss% 0.7935835718788377
plot_id,batch_id 0 31 miss% 0.8306275855466708
plot_id,batch_id 0 32 miss% 0.8407791430645152
plot_id,batch_id 0 33 miss% 0.8413820795612682
plot_id,batch_id 0 34 miss% 0.8436844637615565
plot_id,batch_id 0 35 miss% 0.7931744999091792
plot_id,batch_id 0 36 miss% 0.837050850581574
plot_id,batch_id 0 37 miss% 0.8356195036447525
plot_id,batch_id 0 38 miss% 0.8432973128942273
plot_id,batch_id 0 39 miss% 0.8435142867423435
plot_id,batch_id 0 40 miss% 0.8209221074001148
plot_id,batch_id 0 41 miss% 0.8401517506461686
plot_id,batch_id 0 42 miss% 0.8403586109962531
plot_id,batch_id 0 43 miss% 0.8471019777942603
plot_id,batch_id 0 44 miss% 0.8504792306523818
plot_id,batch_id 0 45 miss% 0.8189317721864605
plot_id,batch_id 0 46 miss% 0.8402998276893064
plot_id,batch_id 0 47 miss% 0.8411721804940273
plot_id,batch_id 0 48 miss% 0.8464330125637244
plot_id,batch_id 0 49 miss% 0.8492804111507326
plot_id,batch_id 0 50 miss% 0.8307957388768344
plot_id,batch_id 0 51 miss% 0.8376351068852059
plot_id,batch_id 0 52 miss% 0.8421843140236388
plot_id,batch_id 0 53 miss% 0.844717119794388
plot_id,batch_id 0 54 miss% 0.8472651253142481
plot_id,batch_id 0 55 miss% 0.8222369568609771
plot_id,batch_id 0 56 miss% 0.8376869320559369
plot_id,batch_id 0 57 miss% 0.842647023873068
plot_id,batch_id 0 58 miss% 0.8452102633897277
plot_id,batch_id 0 59 miss% 0.8437274526102261
plot_id,batch_id 0 60 miss% 0.721042082338879
plot_id,batch_id 0 61 miss% 0.8103388543404896
plot_id,batch_id 0 62 miss% 0.8260380532017404
plot_id,batch_id 0 63 miss% 0.8292710606614754
plot_id,batch_id 0 64 miss% 0.8358391517603025
plot_id,batch_id 0 65 miss% 0.7206309723714887
plot_id,batch_id 0 66 miss% 0.7949580461011906
plot_id,batch_id 0 67 miss% 0.8109796190910187
plot_id,batch_id 0 68 miss% 0.8293905462392983
plot_id,batch_id 0 69 miss% 0.8310259450288785
plot_id,batch_id 0 70 miss% 0.6796000031690334
plot_id,batch_id 0 71 miss% 0.8063956033893429
plot_id,batch_id 0 72 miss% 0.8071288845189925
plot_id,batch_id 0 73 miss% 0.820659489894402
plot_id,batch_id 0 74 miss% 0.8273558060929788
plot_id,batch_id 0 75 miss% 0.6824550514501522
plot_id,batch_id 0 76 miss% 0.7968672313953784
plot_id,batch_id 0 77 miss% 0.7973804547103374
plot_id,batch_id 0 78 miss% 0.8185073619641058
plot_id,batch_id 0 79 miss% 0.8219753135836192
plot_id,batch_id 0 80 miss% 0.7430131247560188
plot_id,batch_id 0 81 miss% 0.8201500336688081
plot_id,batch_id 0 82 miss% 0.8291877891185855
plot_id,batch_id 0 83 miss% 0.8356324898219596
plot_id,batch_id 0 84 miss% 0.8385279096428181
plot_id,batch_id 0 85 miss% 0.7374508002160763
plot_id,batch_id 0 86 miss% 0.8116681381997282
plot_id,batch_id 0 87 miss% 0.8275226096582163
plot_id,batch_id 0 88 miss% 0.8352926875601339
plot_id,batch_id 0 89 miss% 0.8348384169379323
plot_id,batch_id 0 90 miss% 0.6981453005985542
plot_id,batch_id 0 91 miss% 0.801294853237918
plot_id,batch_id 0 92 miss% 0.8230679634391261
plot_id,batch_id 0 93 miss% 0.8233978725795195
plot_id,batch_id 0 94 miss% 0.8379223955910022
plot_id,batch_id 0 95 miss% 0.7264804123521673
plot_id,batch_id 0 96 miss% 0.7938616084231996
plot_id,batch_id 0 97 miss% 0.8180044110063543
plot_id,batch_id 0 98 miss% 0.8284310463496759
plot_id,batch_id 0 99 miss% 0.8287964222145132
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.77976264 0.8304291  0.83363017 0.84036015 0.84231004 0.77596944
 0.82355391 0.83578683 0.84110655 0.84338477 0.76702296 0.82322337
 0.83260077 0.83986997 0.83992718 0.77062596 0.82472473 0.83383343
 0.84001857 0.84281453 0.8041385  0.83558341 0.8400238  0.84339361
 0.84419241 0.81004446 0.83597852 0.83941853 0.84239574 0.84320333
 0.79358357 0.83062759 0.84077914 0.84138208 0.84368446 0.7931745
 0.83705085 0.8356195  0.84329731 0.84351429 0.82092211 0.84015175
 0.84035861 0.84710198 0.85047923 0.81893177 0.84029983 0.84117218
 0.84643301 0.84928041 0.83079574 0.83763511 0.84218431 0.84471712
 0.84726513 0.82223696 0.83768693 0.84264702 0.84521026 0.84372745
 0.72104208 0.81033885 0.82603805 0.82927106 0.83583915 0.72063097
 0.79495805 0.81097962 0.82939055 0.83102595 0.6796     0.8063956
 0.80712888 0.82065949 0.82735581 0.68245505 0.79686723 0.79738045
 0.81850736 0.82197531 0.74301312 0.82015003 0.82918779 0.83563249
 0.83852791 0.7374508  0.81166814 0.82752261 0.83529269 0.83483842
 0.6981453  0.80129485 0.82306796 0.82339787 0.8379224  0.72648041
 0.79386161 0.81800441 0.82843105 0.82879642]
for model  140 the mean error 0.8185180338129152
all id 140 hidden_dim 24 learning_rate 0.02 num_layers 3 frames 25 out win 5 err 0.8185180338129152
Launcher: Job 141 completed in 7542 seconds.
Launcher: Task 246 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  61841
Epoch:0, Train loss:0.710423, valid loss:0.705107
Epoch:1, Train loss:0.249487, valid loss:0.011161
Epoch:2, Train loss:0.017786, valid loss:0.007923
Epoch:3, Train loss:0.013528, valid loss:0.006275
Epoch:4, Train loss:0.011067, valid loss:0.005130
Epoch:5, Train loss:0.009286, valid loss:0.004831
Epoch:6, Train loss:0.008631, valid loss:0.005778
Epoch:7, Train loss:0.007349, valid loss:0.003582
Epoch:8, Train loss:0.006902, valid loss:0.003427
Epoch:9, Train loss:0.006208, valid loss:0.003658
Epoch:10, Train loss:0.006019, valid loss:0.003276
Epoch:11, Train loss:0.004321, valid loss:0.002353
Epoch:12, Train loss:0.004240, valid loss:0.002634
Epoch:13, Train loss:0.004114, valid loss:0.002379
Epoch:14, Train loss:0.003922, valid loss:0.002367
Epoch:15, Train loss:0.003702, valid loss:0.002584
Epoch:16, Train loss:0.003641, valid loss:0.002447
Epoch:17, Train loss:0.003618, valid loss:0.002196
Epoch:18, Train loss:0.003513, valid loss:0.002207
Epoch:19, Train loss:0.003438, valid loss:0.002002
Epoch:20, Train loss:0.003254, valid loss:0.001881
Epoch:21, Train loss:0.002641, valid loss:0.001898
Epoch:22, Train loss:0.002595, valid loss:0.001679
Epoch:23, Train loss:0.002602, valid loss:0.001694
Epoch:24, Train loss:0.002514, valid loss:0.001578
Epoch:25, Train loss:0.002486, valid loss:0.001670
Epoch:26, Train loss:0.002525, valid loss:0.001700
Epoch:27, Train loss:0.002487, valid loss:0.001501
Epoch:28, Train loss:0.002592, valid loss:0.001577
Epoch:29, Train loss:0.002340, valid loss:0.001744
Epoch:30, Train loss:0.002356, valid loss:0.001833
Epoch:31, Train loss:0.002034, valid loss:0.001442
Epoch:32, Train loss:0.001999, valid loss:0.001450
Epoch:33, Train loss:0.002009, valid loss:0.001361
Epoch:34, Train loss:0.002032, valid loss:0.001395
Epoch:35, Train loss:0.001975, valid loss:0.001356
Epoch:36, Train loss:0.001959, valid loss:0.001363
Epoch:37, Train loss:0.001958, valid loss:0.001398
Epoch:38, Train loss:0.001943, valid loss:0.001342
Epoch:39, Train loss:0.001931, valid loss:0.001243
Epoch:40, Train loss:0.001920, valid loss:0.001365
Epoch:41, Train loss:0.001742, valid loss:0.001232
Epoch:42, Train loss:0.001732, valid loss:0.001258
Epoch:43, Train loss:0.001742, valid loss:0.001245
Epoch:44, Train loss:0.001729, valid loss:0.001248
Epoch:45, Train loss:0.001734, valid loss:0.001251
Epoch:46, Train loss:0.001708, valid loss:0.001224
Epoch:47, Train loss:0.001702, valid loss:0.001201
Epoch:48, Train loss:0.001708, valid loss:0.001218
Epoch:49, Train loss:0.001694, valid loss:0.001217
Epoch:50, Train loss:0.001686, valid loss:0.001179
Epoch:51, Train loss:0.001607, valid loss:0.001207
Epoch:52, Train loss:0.001604, valid loss:0.001195
Epoch:53, Train loss:0.001593, valid loss:0.001241
Epoch:54, Train loss:0.001590, valid loss:0.001195
Epoch:55, Train loss:0.001600, valid loss:0.001178
Epoch:56, Train loss:0.001581, valid loss:0.001194
Epoch:57, Train loss:0.001587, valid loss:0.001152
Epoch:58, Train loss:0.001575, valid loss:0.001191
Epoch:59, Train loss:0.001572, valid loss:0.001178
Epoch:60, Train loss:0.001568, valid loss:0.001192
training time 7353.863426923752
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.045938379516980446
plot_id,batch_id 0 1 miss% 0.03309928404741688
plot_id,batch_id 0 2 miss% 0.031159821397211875
plot_id,batch_id 0 3 miss% 0.025891182020787218
plot_id,batch_id 0 4 miss% 0.027590055185575238
plot_id,batch_id 0 5 miss% 0.04443955402432261
plot_id,batch_id 0 6 miss% 0.03218731375598661
plot_id,batch_id 0 7 miss% 0.037044280792934765
plot_id,batch_id 0 8 miss% 0.03786966967322545
plot_id,batch_id 0 9 miss% 0.026983741446728663
plot_id,batch_id 0 10 miss% 0.03984252383662575
plot_id,batch_id 0 11 miss% 0.04544362932114532
plot_id,batch_id 0 12 miss% 0.034129430567112186
plot_id,batch_id 0 13 miss% 0.02634325239805632
plot_id,batch_id 0 14 miss% 0.042440069700684147
plot_id,batch_id 0 15 miss% 0.0454904200121687
plot_id,batch_id 0 16 miss% 0.033816458870907465
plot_id,batch_id 0 17 miss% 0.035853448074880494
plot_id,batch_id 0 18 miss% 0.04790416093935855
plot_id,batch_id 0 19 miss% 0.03815331193478632
plot_id,batch_id 0 20 miss% 0.04155746206107575
plot_id,batch_id 0 21 miss% 0.030897939021691127
plot_id,batch_id 0 22 miss% 0.027977868870508908
plot_id,batch_id 0 23 miss% 0.024957620546420926
plot_id,batch_id 0 24 miss% 0.0294149214438486
plot_id,batch_id 0 25 miss% 0.0407458874645886
plot_id,batch_id 0 26 miss% 0.030848178060443404
plot_id,batch_id 0 27 miss% 0.024359271276269686
plot_id,batch_id 0 28 miss% 0.01968469689702802
plot_id,batch_id 0 29 miss% 0.03091963389834077
plot_id,batch_id 0 30 miss% 0.03976401860660874
plot_id,batch_id 0 31 miss% 0.026742365605544074
plot_id,batch_id 0 32 miss% 0.053347278297725634
plot_id,batch_id 0 33 miss% 0.030959344695885273
plot_id,batch_id 0 34 miss% 0.03452060460807375
plot_id,batch_id 0 35 miss% 0.0450071912419323
plot_id,batch_id 0 36 miss% 0.05485141645188463
plot_id,batch_id 0 37 miss% 0.03887577624853176
plot_id,batch_id 0 38 miss% 0.032161535132913
plot_id,batch_id 0 39 miss% 0.029994697133220165
plot_id,batch_id 0 40 miss% 0.05653560251267073
plot_id,batch_id 0 41 miss% 0.024625299607108877
plot_id,batch_id 0 42 miss% 0.018838989551251833
plot_id,batch_id 0 43 miss% 0.0417796485644397
plot_id,batch_id 0 44 miss% 0.02242532264075456
plot_id,batch_id 0 45 miss% 0.04422456505221999
plot_id,batch_id 0 46 miss% 0.0314101617919348
plot_id,batch_id 0 47 miss% 0.0216867886939512
plot_id,batch_id 0 48 miss% 0.022885991359221373
plot_id,batch_id 0 49 miss% 0.028980954906924598
plot_id,batch_id 0 50 miss% 0.05599490706821166
plot_id,batch_id 0 51 miss% 0.029574480661459613
plot_id,batch_id 0 52 miss% 0.032997942333435636
plot_id,batch_id 0 53 miss% 0.009441212411789248
plot_id,batch_id 0 54 miss% 0.028839885264260037
plot_id,batch_id 0 55 miss% 0.036590102456168574
plot_id,batch_id 0 56 miss% 0.03636849665997307
plot_id,batch_id 0 57 miss% 0.030603564881804865
plot_id,batch_id 0 58 miss% 0.03202660071795809
plot_id,batch_id 0 59 miss% 0.0197678106073378
plot_id,batch_id 0 60 miss% 0.03671571135786267
plot_id,batch_id 0 61 miss% 0.027975622128334685
plot_id,batch_id 0 62 miss% 0.03344761916167059
plot_id,batch_id 0 63 miss% 0.028554539387963273
plot_id,batch_id 0 64 miss% 0.0459770620907121
plot_id,batch_id 0 65 miss% 0.03022080419357285
plot_id,batch_id 0 66 miss% 0.057311339847779606
plot_id,batch_id 0 67 miss% 0.03759987717983678
plot_id,batch_id 0 68 miss% 0.0332128902283408
plot_id,batch_id 0 69 miss% 0.03411328668564918
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  61841
Epoch:0, Train loss:0.548416, valid loss:0.542040
Epoch:1, Train loss:0.357869, valid loss:0.361149
Epoch:2, Train loss:0.347752, valid loss:0.360976
Epoch:3, Train loss:0.346173, valid loss:0.359898
Epoch:4, Train loss:0.345091, valid loss:0.360166
Epoch:5, Train loss:0.344895, valid loss:0.360090
Epoch:6, Train loss:0.344330, valid loss:0.359224
Epoch:7, Train loss:0.344327, valid loss:0.359075
Epoch:8, Train loss:0.343828, valid loss:0.358965
Epoch:9, Train loss:0.343930, valid loss:0.359078
Epoch:10, Train loss:0.343827, valid loss:0.359063
Epoch:11, Train loss:0.342230, valid loss:0.358717
Epoch:12, Train loss:0.342259, valid loss:0.358471
Epoch:13, Train loss:0.342308, valid loss:0.359499
Epoch:14, Train loss:0.342185, valid loss:0.359087
Epoch:15, Train loss:0.342219, valid loss:0.358357
Epoch:16, Train loss:0.342049, valid loss:0.358351
Epoch:17, Train loss:0.342082, valid loss:0.358662
Epoch:18, Train loss:0.342658, valid loss:0.358445
Epoch:19, Train loss:0.342026, valid loss:0.358316
Epoch:20, Train loss:0.342045, valid loss:0.358197
Epoch:21, Train loss:0.341378, valid loss:0.358106
Epoch:22, Train loss:0.341409, valid loss:0.358103
Epoch:23, Train loss:0.341377, valid loss:0.358085
Epoch:24, Train loss:0.341342, valid loss:0.357985
Epoch:25, Train loss:0.341307, valid loss:0.358026
Epoch:26, Train loss:0.341266, valid loss:0.357871
Epoch:27, Train loss:0.341260, valid loss:0.357938
Epoch:28, Train loss:0.341283, valid loss:0.357921
Epoch:29, Train loss:0.341289, valid loss:0.358067
Epoch:30, Train loss:0.341255, valid loss:0.358232
Epoch:31, Train loss:0.340964, valid loss:0.360350
Epoch:32, Train loss:0.340999, valid loss:0.357822
Epoch:33, Train loss:0.340920, valid loss:0.357773
Epoch:34, Train loss:0.340893, valid loss:0.357883
Epoch:35, Train loss:0.340880, valid loss:0.357784
Epoch:36, Train loss:0.340880, valid loss:0.357794
Epoch:37, Train loss:0.340864, valid loss:0.357949
Epoch:38, Train loss:0.340827, valid loss:0.357731
Epoch:39, Train loss:0.340881, valid loss:0.357774
Epoch:40, Train loss:0.340871, valid loss:0.357759
Epoch:41, Train loss:0.340655, valid loss:0.357664
Epoch:42, Train loss:0.340640, valid loss:0.357765
Epoch:43, Train loss:0.340662, valid loss:0.357685
Epoch:44, Train loss:0.340653, valid loss:0.357683
Epoch:45, Train loss:0.340629, valid loss:0.357659
Epoch:46, Train loss:0.340668, valid loss:0.357820
Epoch:47, Train loss:0.340651, valid loss:0.357763
Epoch:48, Train loss:0.340644, valid loss:0.357673
Epoch:49, Train loss:0.340626, valid loss:0.357686
Epoch:50, Train loss:0.340600, valid loss:0.357682
Epoch:51, Train loss:0.340544, valid loss:0.357615
Epoch:52, Train loss:0.340524, valid loss:0.357630
Epoch:53, Train loss:0.340534, valid loss:0.357629
Epoch:54, Train loss:0.340528, valid loss:0.357630
Epoch:55, Train loss:0.340526, valid loss:0.357660
Epoch:56, Train loss:0.340523, valid loss:0.357656
Epoch:57, Train loss:0.340522, valid loss:0.357642
Epoch:58, Train loss:0.340524, valid loss:0.357637
Epoch:59, Train loss:0.340512, valid loss:0.357658
Epoch:60, Train loss:0.340532, valid loss:0.357677
training time 7389.96524810791
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7846270582240311
plot_id,batch_id 0 1 miss% 0.8313739686952719
plot_id,batch_id 0 2 miss% 0.835951859621606
plot_id,batch_id 0 3 miss% 0.8413917755271281
plot_id,batch_id 0 4 miss% 0.8442075347447823
plot_id,batch_id 0 5 miss% 0.7810520267615245
plot_id,batch_id 0 6 miss% 0.8260167676790637
plot_id,batch_id 0 7 miss% 0.8363028768527039
plot_id,batch_id 0 8 miss% 0.8417478431408595
plot_id,batch_id 0 9 miss% 0.8475832737392567
plot_id,batch_id 0 10 miss% 0.7665722453334999
plot_id,batch_id 0 11 miss% 0.8246898795428298
plot_id,batch_id 0 12 miss% 0.8330250071896349
plot_id,batch_id 0 13 miss% 0.839118393207376
plot_id,batch_id 0 14 miss% 0.8402010111097465
plot_id,batch_id 0 15 miss% 0.7729086447572685
plot_id,batch_id 0 16 miss% 0.8229826769930907
plot_id,batch_id 0 17 miss% 0.8360167763156198
plot_id,batch_id 0 18 miss% 0.8435759054158024
plot_id,batch_id 0 19 miss% 0.8439538282759521
plot_id,batch_id 0 20 miss% 0.8072820255206303
plot_id,batch_id 0 21 miss% 0.8365870238348478
plot_id,batch_id 0 22 miss% 0.8418008021268548
plot_id,batch_id 0 23 miss% 0.8439842085307249
plot_id,batch_id 0 24 miss% 0.8449301963663479
plot_id,batch_id 0 25 miss% 0.8116612008438283
plot_id,batch_id 0 26 miss% 0.8362113610592274
plot_id,batch_id 0 27 miss% 0.8387210771853995
plot_id,batch_id 0 28 miss% 0.8432733316424601
plot_id,batch_id 0 29 miss% 0.8444475774907646
plot_id,batch_id 0 30 miss% 0.79560384142349
plot_id,batch_id 0 31 miss% 0.8342325776339011
plot_id,batch_id 0 32 miss% 0.8401101279746013
plot_id,batch_id 0 33 miss% 0.8427609371710477
plot_id,batch_id 0 34 miss% 0.8437395510580996
plot_id,batch_id 0 35 miss% 0.7898214236685994
plot_id,batch_id 0 36 miss% 0.8365641300784106
plot_id,batch_id 0 37 miss% 0.8375117821503952
plot_id,batch_id 0 38 miss% 0.8432581928786422
plot_id,batch_id 0 39 miss% 0.843565497449227
plot_id,batch_id 0 40 miss% 0.8231366274423495
plot_id,batch_id 0 41 miss% 0.8414090156550637
plot_id,batch_id 0 42 miss% 0.8422667654225283
plot_id,batch_id 0 43 miss% 0.8478900377989887
plot_id,batch_id 0 44 miss% 0.8492155065452932
plot_id,batch_id 0 45 miss% 0.8178567659844797
plot_id,batch_id 0 46 miss% 0.8412269048842786
plot_id,batch_id 0 47 miss% 0.8433981959378217
plot_id,batch_id 0 48 miss% 0.8462721095088632
plot_id,batch_id 0 49 miss% 0.8496376727781516
plot_id,batch_id 0 50 miss% 0.8258497415078744
plot_id,batch_id 0 51 miss% 0.8375012583236455
plot_id,batch_id 0 52 miss% 0.8420352248192683
plot_id,batch_id 0 53 miss% 0.8470208362125403
plot_id,batch_id 0 54 miss% 0.8475917401905398
plot_id,batch_id 0 55 miss% 0.8210766403378056
plot_id,batch_id 0 56 miss% 0.8416916804867298
plot_id,batch_id 0 57 miss% 0.8432583584643333
plot_id,batch_id 0 58 miss% 0.846568106848197
plot_id,batch_id 0 59 miss% 0.8452415017842263
plot_id,batch_id 0 60 miss% 0.7171666861108886
plot_id,batch_id 0 61 miss% 0.8126292233874672
plot_id,batch_id 0 62 miss% 0.8236920730299525
plot_id,batch_id 0 63 miss% 0.8313814391165635
plot_id,batch_id 0 64 miss% 0.8391166196198753
plot_id,batch_id 0 65 miss% 0.7194918217141033
plot_id,batch_id 0 66 miss% 0.8015699611718563
plot_id,batch_id 0 67 miss% 0.8102150330025776
plot_id,batch_id 0 68 miss% 0.8277983989160362
plot_id,batch_id 0 69 miss% 0.8326933278525687
plot_id,batch_id 0 70 miss% 0.6787552432717703
plot_id,batch_id 0 71 miss% 0.8084794143095655
plot_id,batch_id 0 72 miss% 0.8094060798718713
plot_id,batch_id 0 73 miss% 0.8189940975018645
plot_id,batch_id 0 74 miss% 0.8274379651228712
plot_id,batch_id 0 75 miss% 0.6825376293050085
plot_id,batch_id 0 76 miss% 0.802757576736787
plot_id,batch_id 0 77 miss% 0.7990836594168594
plot_id,batch_id 0 78 miss% 0.815387602225839
plot_id,batch_id 0 79 miss% 0.8202326345370288
plot_id,batch_id 0 80 miss% 0.7468979068715769
plot_id,batch_id 0 81 miss% 0.8223487522062437
plot_id,batch_id 0 82 miss% 0.8300372224237359
plot_id,batch_id 0 83 miss% 0.8354525735024362
plot_id,batch_id 0 84 miss% 0.8405463512534707
plot_id,batch_id 0 85 miss% 0.739022723695953
plot_id,batch_id 0 86 miss% 0.8130485957846737
plot_id,batch_id 0 87 miss% 0.8286125292822255
plot_id,batch_id 0 88 miss% 0.8355763566583645
plot_id,batch_id 0 89 miss% 0.8342039005073424
plot_id,batch_id 0 90 miss% 0.7016951017679388
plot_id,batch_id 0 91 miss% 0.8053192625785655
plot_id,batch_id 0 92 miss% 0.8222906132364137
plot_id,batch_id 0 93 miss% 0.8238127227272894
plot_id,batch_id 0 94 miss% 0.8393850044851785
plot_id,batch_id 0 95 miss% 0.7318422154551472
plot_id,batch_id 0 96 miss% 0.7953343161506967
plot_id,batch_id 0 97 miss% 0.8217886710331758
plot_id,batch_id 0 98 miss% 0.827047282104287
plot_id,batch_id 0 99 miss% 0.8308025660515619
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.78462706 0.83137397 0.83595186 0.84139178 0.84420753 0.78105203
 0.82601677 0.83630288 0.84174784 0.84758327 0.76657225 0.82468988
 0.83302501 0.83911839 0.84020101 0.77290864 0.82298268 0.83601678
 0.84357591 0.84395383 0.80728203 0.83658702 0.8418008  0.84398421
 0.8449302  0.8116612  0.83621136 0.83872108 0.84327333 0.84444758
 0.79560384 0.83423258 0.84011013 0.84276094 0.84373955 0.78982142
 0.83656413 0.83751178 0.84325819 0.8435655  0.82313663 0.84140902
 0.84226677 0.84789004 0.84921551 0.81785677 0.8412269  0.8433982
 0.84627211 0.84963767 0.82584974 0.83750126 0.84203522 0.84702084
 0.84759174 0.82107664 0.84169168 0.84325836 0.84656811 0.8452415
 0.71716669 0.81262922 0.82369207 0.83138144 0.83911662 0.71949182
 0.80156996 0.81021503 0.8277984  0.83269333 0.67875524 0.80847941
 0.80940608 0.8189941  0.82743797 0.68253763 0.80275758 0.79908366
 0.8153876  0.82023263 0.74689791 0.82234875 0.83003722 0.83545257
 0.84054635 0.73902272 0.8130486  0.82861253 0.83557636 0.8342039
 0.7016951  0.80531926 0.82229061 0.82381272 0.839385   0.73184222
 0.79533432 0.82178867 0.82704728 0.83080257]
for model  149 the mean error 0.8195340206414516
all id 149 hidden_dim 24 learning_rate 0.02 num_layers 4 frames 25 out win 5 err 0.8195340206414516
Launcher: Job 150 completed in 7565 seconds.
Launcher: Task 49 done. Exiting.
plot_id,batch_id 0 70 miss% 0.03232914311811865
plot_id,batch_id 0 71 miss% 0.037836378328395065
plot_id,batch_id 0 72 miss% 0.02744147919848096
plot_id,batch_id 0 73 miss% 0.05044417128278021
plot_id,batch_id 0 74 miss% 0.044199064692863596
plot_id,batch_id 0 75 miss% 0.05952722487064568
plot_id,batch_id 0 76 miss% 0.053583804291241585
plot_id,batch_id 0 77 miss% 0.03297405120370238
plot_id,batch_id 0 78 miss% 0.05674747328079589
plot_id,batch_id 0 79 miss% 0.046545561108365566
plot_id,batch_id 0 80 miss% 0.0566262901282309
plot_id,batch_id 0 81 miss% 0.021424386694139343
plot_id,batch_id 0 82 miss% 0.025338932293037172
plot_id,batch_id 0 83 miss% 0.032010837356740804
plot_id,batch_id 0 84 miss% 0.019471236057234383
plot_id,batch_id 0 85 miss% 0.05118295480453836
plot_id,batch_id 0 86 miss% 0.03628070125202699
plot_id,batch_id 0 87 miss% 0.0382031827165856
plot_id,batch_id 0 88 miss% 0.030770453153273543
plot_id,batch_id 0 89 miss% 0.050823338855356386
plot_id,batch_id 0 90 miss% 0.0507584384803683
plot_id,batch_id 0 91 miss% 0.03803367961499191
plot_id,batch_id 0 92 miss% 0.04820542408441772
plot_id,batch_id 0 93 miss% 0.039644080533872576
plot_id,batch_id 0 94 miss% 0.034501656382513006
plot_id,batch_id 0 95 miss% 0.05616087166517728
plot_id,batch_id 0 96 miss% 0.036990397617723235
plot_id,batch_id 0 97 miss% 0.06310123324675121
plot_id,batch_id 0 98 miss% 0.04640563093127393
plot_id,batch_id 0 99 miss% 0.05060844340909738
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04593838 0.03309928 0.03115982 0.02589118 0.02759006 0.04443955
 0.03218731 0.03704428 0.03786967 0.02698374 0.03984252 0.04544363
 0.03412943 0.02634325 0.04244007 0.04549042 0.03381646 0.03585345
 0.04790416 0.03815331 0.04155746 0.03089794 0.02797787 0.02495762
 0.02941492 0.04074589 0.03084818 0.02435927 0.0196847  0.03091963
 0.03976402 0.02674237 0.05334728 0.03095934 0.0345206  0.04500719
 0.05485142 0.03887578 0.03216154 0.0299947  0.0565356  0.0246253
 0.01883899 0.04177965 0.02242532 0.04422457 0.03141016 0.02168679
 0.02288599 0.02898095 0.05599491 0.02957448 0.03299794 0.00944121
 0.02883989 0.0365901  0.0363685  0.03060356 0.0320266  0.01976781
 0.03671571 0.02797562 0.03344762 0.02855454 0.04597706 0.0302208
 0.05731134 0.03759988 0.03321289 0.03411329 0.03232914 0.03783638
 0.02744148 0.05044417 0.04419906 0.05952722 0.0535838  0.03297405
 0.05674747 0.04654556 0.05662629 0.02142439 0.02533893 0.03201084
 0.01947124 0.05118295 0.0362807  0.03820318 0.03077045 0.05082334
 0.05075844 0.03803368 0.04820542 0.03964408 0.03450166 0.05616087
 0.0369904  0.06310123 0.04640563 0.05060844]
for model  14 the mean error 0.03678105295736769
all id 14 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 21 out win 5 err 0.03678105295736769
Launcher: Job 15 completed in 7572 seconds.
Launcher: Task 178 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  28945
Epoch:0, Train loss:0.332371, valid loss:0.324912
Epoch:1, Train loss:0.018307, valid loss:0.003858
Epoch:2, Train loss:0.005826, valid loss:0.003306
Epoch:3, Train loss:0.004735, valid loss:0.002975
Epoch:4, Train loss:0.004208, valid loss:0.002073
Epoch:5, Train loss:0.003957, valid loss:0.001713
Epoch:6, Train loss:0.003769, valid loss:0.002225
Epoch:7, Train loss:0.003637, valid loss:0.001924
Epoch:8, Train loss:0.003505, valid loss:0.001782
Epoch:9, Train loss:0.003418, valid loss:0.002373
Epoch:10, Train loss:0.003434, valid loss:0.001505
Epoch:11, Train loss:0.002393, valid loss:0.001327
Epoch:12, Train loss:0.002390, valid loss:0.001548
Epoch:13, Train loss:0.002351, valid loss:0.001496
Epoch:14, Train loss:0.002330, valid loss:0.001602
Epoch:15, Train loss:0.002348, valid loss:0.001443
Epoch:16, Train loss:0.002305, valid loss:0.001328
Epoch:17, Train loss:0.002250, valid loss:0.001251
Epoch:18, Train loss:0.002264, valid loss:0.001369
Epoch:19, Train loss:0.002259, valid loss:0.001319
Epoch:20, Train loss:0.002193, valid loss:0.001296
Epoch:21, Train loss:0.001723, valid loss:0.001123
Epoch:22, Train loss:0.001718, valid loss:0.001000
Epoch:23, Train loss:0.001688, valid loss:0.000999
Epoch:24, Train loss:0.001710, valid loss:0.001186
Epoch:25, Train loss:0.001690, valid loss:0.000934
Epoch:26, Train loss:0.001671, valid loss:0.000963
Epoch:27, Train loss:0.001666, valid loss:0.000984
Epoch:28, Train loss:0.001674, valid loss:0.001043
Epoch:29, Train loss:0.001670, valid loss:0.001014
Epoch:30, Train loss:0.001624, valid loss:0.001029
Epoch:31, Train loss:0.001376, valid loss:0.000899
Epoch:32, Train loss:0.001374, valid loss:0.000828
Epoch:33, Train loss:0.001363, valid loss:0.000854
Epoch:34, Train loss:0.001385, valid loss:0.000853
Epoch:35, Train loss:0.001384, valid loss:0.000832
Epoch:36, Train loss:0.001352, valid loss:0.000796
Epoch:37, Train loss:0.001347, valid loss:0.000782
Epoch:38, Train loss:0.001338, valid loss:0.000918
Epoch:39, Train loss:0.001341, valid loss:0.000815
Epoch:40, Train loss:0.001359, valid loss:0.000832
Epoch:41, Train loss:0.001192, valid loss:0.000776
Epoch:42, Train loss:0.001192, valid loss:0.000757
Epoch:43, Train loss:0.001202, valid loss:0.000776
Epoch:44, Train loss:0.001196, valid loss:0.000783
Epoch:45, Train loss:0.001190, valid loss:0.000761
Epoch:46, Train loss:0.001187, valid loss:0.000761
Epoch:47, Train loss:0.001185, valid loss:0.000808
Epoch:48, Train loss:0.001189, valid loss:0.000792
Epoch:49, Train loss:0.001177, valid loss:0.000739
Epoch:50, Train loss:0.001174, valid loss:0.000782
Epoch:51, Train loss:0.001099, valid loss:0.000760
Epoch:52, Train loss:0.001096, valid loss:0.000769
Epoch:53, Train loss:0.001096, valid loss:0.000720
Epoch:54, Train loss:0.001101, valid loss:0.000712
Epoch:55, Train loss:0.001086, valid loss:0.000738
Epoch:56, Train loss:0.001094, valid loss:0.000723
Epoch:57, Train loss:0.001090, valid loss:0.000728
Epoch:58, Train loss:0.001089, valid loss:0.000718
Epoch:59, Train loss:0.001084, valid loss:0.000745
Epoch:60, Train loss:0.001085, valid loss:0.000727
training time 7382.828750371933
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.04557331321367898
plot_id,batch_id 0 1 miss% 0.029685280691751848
plot_id,batch_id 0 2 miss% 0.025130848673056396
plot_id,batch_id 0 3 miss% 0.025030951920236515
plot_id,batch_id 0 4 miss% 0.03220771853369238
plot_id,batch_id 0 5 miss% 0.033403814696178445
plot_id,batch_id 0 6 miss% 0.03875510221527531
plot_id,batch_id 0 7 miss% 0.031017196584631045
plot_id,batch_id 0 8 miss% 0.034641254334793384
plot_id,batch_id 0 9 miss% 0.03158347385211558
plot_id,batch_id 0 10 miss% 0.0419622938621162
plot_id,batch_id 0 11 miss% 0.036637255053008876
plot_id,batch_id 0 12 miss% 0.02625865177013119
plot_id,batch_id 0 13 miss% 0.02617703686793178
plot_id,batch_id 0 14 miss% 0.03398148850939352
plot_id,batch_id 0 15 miss% 0.0591702842712411
plot_id,batch_id 0 16 miss% 0.035829720582429
plot_id,batch_id 0 17 miss% 0.037785279979099175
plot_id,batch_id 0 18 miss% 0.03780689897801567
plot_id,batch_id 0 19 miss% 0.034778236448568735
plot_id,batch_id 0 20 miss% 0.034576976100883255
plot_id,batch_id 0 21 miss% 0.019437944505879847
plot_id,batch_id 0 22 miss% 0.02493030086207917
plot_id,batch_id 0 23 miss% 0.026969858305367838
plot_id,batch_id 0 24 miss% 0.02477956084259339
plot_id,batch_id 0 25 miss% 0.04296217775673445
plot_id,batch_id 0 26 miss% 0.020817197609629946
plot_id,batch_id 0 27 miss% 0.02133026683408746
plot_id,batch_id 0 28 miss% 0.02789617689222844
plot_id,batch_id 0 29 miss% 0.03092449819601508
plot_id,batch_id 0 30 miss% 0.04310845352576157
plot_id,batch_id 0 31 miss% 0.03889176175595022
plot_id,batch_id 0 32 miss% 0.03317430006666072
plot_id,batch_id 0 33 miss% 0.0307949185425365
plot_id,batch_id 0 34 miss% 0.03722724993375833
plot_id,batch_id 0 35 miss% 0.06350016644133057
plot_id,batch_id 0 36 miss% 0.02676062977133821
plot_id,batch_id 0 37 miss% 0.03458456569100586
plot_id,batch_id 0 38 miss% 0.03419867843566336
plot_id,batch_id 0 39 miss% 0.02799331897637542
plot_id,batch_id 0 40 miss% 0.10143918668298517
plot_id,batch_id 0 41 miss% 0.03902793995439096
plot_id,batch_id 0 42 miss% 0.023816928711882762
plot_id,batch_id 0 43 miss% 0.02964455674840841
plot_id,batch_id 0 44 miss% 0.02329104577235583
plot_id,batch_id 0 45 miss% 0.028912147555973006
plot_id,batch_id 0 46 miss% 0.02899443850600354
plot_id,batch_id 0 47 miss% 0.024126104569446277
plot_id,batch_id 0 48 miss% 0.03473149608622622
plot_id,batch_id 0 49 miss% 0.022170988301810957
plot_id,batch_id 0 50 miss% 0.05575348252397233
plot_id,batch_id 0 51 miss% 0.023288531826378238
plot_id,batch_id 0 52 miss% 0.020040246166957834
plot_id,batch_id 0 53 miss% 0.022285653163012176
plot_id,batch_id 0 54 miss% 0.029286632492302823
plot_id,batch_id 0 55 miss% 0.04718176524215327
plot_id,batch_id 0 56 miss% 0.02551607855427532
plot_id,batch_id 0 57 miss% 0.025869884440390394
plot_id,batch_id 0 58 miss% 0.02910841919504485
plot_id,batch_id 0 59 miss% 0.02454392024344985
plot_id,batch_id 0 60 miss% 0.04360586749025449
plot_id,batch_id 0 61 miss% 0.03209538495632775
plot_id,batch_id 0 62 miss% 0.02968269058240125
plot_id,batch_id 0 63 miss% 0.03461204947708524
plot_id,batch_id 0 64 miss% 0.028247556314093137
plot_id,batch_id 0 65 miss% 0.04822984476696711
plot_id,batch_id 0 66 miss% 0.035233552232726256
plot_id,batch_id 0 67 miss% 0.040567701950118634
plot_id,batch_id 0 68 miss% 0.03133627281522025
plot_id,batch_id 0 69 miss% 0.031378391934266434
plot_id,batch_id 0 70 miss% 0.06015576150781286
plot_id,batch_id 0 71 miss% 0.04552032256739448
plot_id,batch_id 0 72 miss% 0.03229556848711565
plot_id,batch_id 0 73 miss% 0.03448402624146704
plot_id,batch_id 0 74 miss% 0.035866889701448953
plot_id,batch_id 0 75 miss% 0.06214173446456298
plot_id,batch_id 0 76 miss% 0.05780664927587403
plot_id,batch_id 0 77 miss% 0.057197935270466516
plot_id,batch_id 0 78 miss% 0.03886546301857493
plot_id,batch_id 0 79 miss% 0.051678363613935
plot_id,batch_id 0 80 miss% 0.053532361097214924
plot_id,batch_id 0 81 miss% 0.03421371939056872
plot_id,batch_id 0 82 miss% 0.03105110859655512
plot_id,batch_id 0 83 miss% 0.03751861834473123
plot_id,batch_id 0 84 miss% 0.029460368851282535
plot_id,batch_id 0 85 miss% 0.06665604494876641
plot_id,batch_id 0 86 miss% 0.032407544566359976
plot_id,batch_id 0 87 miss% 0.03412776337246116
plot_id,batch_id 0 88 miss% 0.04584835377635898
plot_id,batch_id 0 89 miss% 0.03870926151983473
plot_id,batch_id 0 90 miss% 0.04357851933172475
plot_id,batch_id 0 91 miss% 0.03516216632114623
plot_id,batch_id 0 92 miss% 0.046050992151384085
plot_id,batch_id 0 93 miss% 0.040447452242404064
plot_id,batch_id 0 94 miss% 0.034137356068170054
plot_id,batch_id 0 95 miss% 0.082917950822346
plot_id,batch_id 0 96 miss% 0.059016323086441
plot_id,batch_id 0 97 miss% 0.03765775277456825
plot_id,batch_id 0 98 miss% 0.04516251057868744
plot_id,batch_id 0 99 miss% 0.03811791661828832
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04557331 0.02968528 0.02513085 0.02503095 0.03220772 0.03340381
 0.0387551  0.0310172  0.03464125 0.03158347 0.04196229 0.03663726
 0.02625865 0.02617704 0.03398149 0.05917028 0.03582972 0.03778528
 0.0378069  0.03477824 0.03457698 0.01943794 0.0249303  0.02696986
 0.02477956 0.04296218 0.0208172  0.02133027 0.02789618 0.0309245
 0.04310845 0.03889176 0.0331743  0.03079492 0.03722725 0.06350017
 0.02676063 0.03458457 0.03419868 0.02799332 0.10143919 0.03902794
 0.02381693 0.02964456 0.02329105 0.02891215 0.02899444 0.0241261
 0.0347315  0.02217099 0.05575348 0.02328853 0.02004025 0.02228565
 0.02928663 0.04718177 0.02551608 0.02586988 0.02910842 0.02454392
 0.04360587 0.03209538 0.02968269 0.03461205 0.02824756 0.04822984
 0.03523355 0.0405677  0.03133627 0.03137839 0.06015576 0.04552032
 0.03229557 0.03448403 0.03586689 0.06214173 0.05780665 0.05719794
 0.03886546 0.05167836 0.05353236 0.03421372 0.03105111 0.03751862
 0.02946037 0.06665604 0.03240754 0.03412776 0.04584835 0.03870926
 0.04357852 0.03516217 0.04605099 0.04044745 0.03413736 0.08291795
 0.05901632 0.03765775 0.04516251 0.03811792]
for model  227 the mean error 0.03698080659948051
all id 227 hidden_dim 16 learning_rate 0.02 num_layers 4 frames 31 out win 5 err 0.03698080659948051
Launcher: Job 228 completed in 7578 seconds.
Launcher: Task 45 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  77489
Epoch:0, Train loss:0.507052, valid loss:0.511720
Epoch:1, Train loss:0.037972, valid loss:0.007749
Epoch:2, Train loss:0.010358, valid loss:0.004526
Epoch:3, Train loss:0.007033, valid loss:0.003808
Epoch:4, Train loss:0.005683, valid loss:0.003112
Epoch:5, Train loss:0.004799, valid loss:0.002291
Epoch:6, Train loss:0.004356, valid loss:0.002406
Epoch:7, Train loss:0.004096, valid loss:0.002814
Epoch:8, Train loss:0.003938, valid loss:0.001770
Epoch:9, Train loss:0.003566, valid loss:0.002036
Epoch:10, Train loss:0.003648, valid loss:0.001963
Epoch:11, Train loss:0.002337, valid loss:0.001692
Epoch:12, Train loss:0.002382, valid loss:0.001559
Epoch:13, Train loss:0.002293, valid loss:0.001251
Epoch:14, Train loss:0.002264, valid loss:0.001513
Epoch:15, Train loss:0.002243, valid loss:0.001435
Epoch:16, Train loss:0.002247, valid loss:0.001327
Epoch:17, Train loss:0.002080, valid loss:0.001385
Epoch:18, Train loss:0.002102, valid loss:0.001470
Epoch:19, Train loss:0.002145, valid loss:0.001186
Epoch:20, Train loss:0.001997, valid loss:0.001414
Epoch:21, Train loss:0.001464, valid loss:0.000998
Epoch:22, Train loss:0.001469, valid loss:0.001006
Epoch:23, Train loss:0.001451, valid loss:0.001161
Epoch:24, Train loss:0.001396, valid loss:0.000937
Epoch:25, Train loss:0.001459, valid loss:0.000951
Epoch:26, Train loss:0.001430, valid loss:0.000893
Epoch:27, Train loss:0.001338, valid loss:0.000920
Epoch:28, Train loss:0.001407, valid loss:0.000912
Epoch:29, Train loss:0.001349, valid loss:0.000865
Epoch:30, Train loss:0.001371, valid loss:0.000991
Epoch:31, Train loss:0.001064, valid loss:0.000817
Epoch:32, Train loss:0.001045, valid loss:0.000887
Epoch:33, Train loss:0.001043, valid loss:0.000801
Epoch:34, Train loss:0.001054, valid loss:0.000820
Epoch:35, Train loss:0.001037, valid loss:0.000842
Epoch:36, Train loss:0.001033, valid loss:0.000903
Epoch:37, Train loss:0.001005, valid loss:0.000857
Epoch:38, Train loss:0.001015, valid loss:0.000888
Epoch:39, Train loss:0.000998, valid loss:0.000762
Epoch:40, Train loss:0.001009, valid loss:0.000844
Epoch:41, Train loss:0.000869, valid loss:0.000796
Epoch:42, Train loss:0.000864, valid loss:0.000815
Epoch:43, Train loss:0.000855, valid loss:0.000823
Epoch:44, Train loss:0.000860, valid loss:0.000810
Epoch:45, Train loss:0.000858, valid loss:0.000782
Epoch:46, Train loss:0.000850, valid loss:0.000761
Epoch:47, Train loss:0.000861, valid loss:0.000797
Epoch:48, Train loss:0.000836, valid loss:0.000778
Epoch:49, Train loss:0.000841, valid loss:0.000752
Epoch:50, Train loss:0.000844, valid loss:0.000732
Epoch:51, Train loss:0.000771, valid loss:0.000739
Epoch:52, Train loss:0.000771, valid loss:0.000731
Epoch:53, Train loss:0.000768, valid loss:0.000727
Epoch:54, Train loss:0.000768, valid loss:0.000728
Epoch:55, Train loss:0.000767, valid loss:0.000774
Epoch:56, Train loss:0.000765, valid loss:0.000697
Epoch:57, Train loss:0.000760, valid loss:0.000718
Epoch:58, Train loss:0.000761, valid loss:0.000750
Epoch:59, Train loss:0.000757, valid loss:0.000703
Epoch:60, Train loss:0.000759, valid loss:0.000706
training time 7404.762645244598
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02808852627879414
plot_id,batch_id 0 1 miss% 0.021727399525570713
plot_id,batch_id 0 2 miss% 0.02696582256263497
plot_id,batch_id 0 3 miss% 0.019751326711475357
plot_id,batch_id 0 4 miss% 0.02483546419948652
plot_id,batch_id 0 5 miss% 0.025964561992696127
plot_id,batch_id 0 6 miss% 0.022868601480900027
plot_id,batch_id 0 7 miss% 0.02099406334543913
plot_id,batch_id 0 8 miss% 0.029306152614403606
plot_id,batch_id 0 9 miss% 0.019113345494726805
plot_id,batch_id 0 10 miss% 0.05488067920853449
plot_id,batch_id 0 11 miss% 0.03150371859983805
plot_id,batch_id 0 12 miss% 0.031602886889178984
plot_id,batch_id 0 13 miss% 0.017129391382839036
plot_id,batch_id 0 14 miss% 0.02845849092522136
plot_id,batch_id 0 15 miss% 0.04212693513846314
plot_id,batch_id 0 16 miss% 0.031344111512204065
plot_id,batch_id 0 17 miss% 0.038426680416854785
plot_id,batch_id 0 18 miss% 0.024678255174408362
plot_id,batch_id 0 19 miss% 0.024900600306376846
plot_id,batch_id 0 20 miss% 0.0352375052547784
plot_id,batch_id 0 21 miss% 0.024892464667695072
plot_id,batch_id 0 22 miss% 0.03516948281294801
plot_id,batch_id 0 23 miss% 0.024361774400410735
plot_id,batch_id 0 24 miss% 0.024902881643198486
plot_id,batch_id 0 25 miss% 0.030191061364878518
plot_id,batch_id 0 26 miss% 0.024480471323284805
plot_id,batch_id 0 27 miss% 0.031586497574904254
plot_id,batch_id 0 28 miss% 0.02727464111941148
plot_id,batch_id 0 29 miss% 0.016441405770727734
plot_id,batch_id 0 30 miss% 0.038804272328267095
plot_id,batch_id 0 31 miss% 0.03204593699483895
plot_id,batch_id 0 32 miss% 0.021582424789448475
plot_id,batch_id 0 33 miss% 0.029611650258806674
plot_id,batch_id 0 34 miss% 0.025693067709968008
plot_id,batch_id 0 35 miss% 0.03911355005368014
plot_id,batch_id 0 36 miss% 0.042998753837644184
plot_id,batch_id 0 37 miss% 0.020995070172765837
plot_id,batch_id 0 38 miss% 0.021597322835469803
plot_id,batch_id 0 39 miss% 0.02504817371413714
plot_id,batch_id 0 40 miss% 0.0876795744835855
plot_id,batch_id 0 41 miss% 0.037620339364103
plot_id,batch_id 0 42 miss% 0.012553324316398103
plot_id,batch_id 0 43 miss% 0.03199654314496711
plot_id,batch_id 0 44 miss% 0.03551157994593724
plot_id,batch_id 0 45 miss% 0.038354844572974646
plot_id,batch_id 0 46 miss% 0.033618890663328425
plot_id,batch_id 0 47 miss% 0.025034045213638566
plot_id,batch_id 0 48 miss% 0.011954306236284809
plot_id,batch_id 0 49 miss% 0.03341071029093581
plot_id,batch_id 0 50 miss% 0.018969977373944336
plot_id,batch_id 0 51 miss% 0.0361663708358643
plot_id,batch_id 0 52 miss% 0.01758713086440194
plot_id,batch_id 0 53 miss% 0.022333782757282573
plot_id,batch_id 0 54 miss% 0.023710506384890244
plot_id,batch_id 0 55 miss% 0.023183619184772325
plot_id,batch_id 0 56 miss% 0.022889730537819824
plot_id,batch_id 0 57 miss% 0.02019654177282555
plot_id,batch_id 0 58 miss% 0.023041312701802274
plot_id,batch_id 0 59 miss% 0.02844948670423039
plot_id,batch_id 0 60 miss% 0.0597326416126753
plot_id,batch_id 0 61 miss% 0.04056671610518071
plot_id,batch_id 0 62 miss% 0.017756894272874676
plot_id,batch_id 0 63 miss% 0.024282695900951105
plot_id,batch_id 0 64 miss% 0.031051515788237633
plot_id,batch_id 0 65 miss% 0.06298679811556997
plot_id,batch_id 0 66 miss% 0.046283147716790075
plot_id,batch_id 0 67 miss% 0.026746088704174636
plot_id,batch_id 0 68 miss% 0.021839564940097526
plot_id,batch_id 0the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  35921
Epoch:0, Train loss:0.543107, valid loss:0.533067
Epoch:1, Train loss:0.022221, valid loss:0.004600
Epoch:2, Train loss:0.005771, valid loss:0.002344
Epoch:3, Train loss:0.003995, valid loss:0.001932
Epoch:4, Train loss:0.003199, valid loss:0.002238
Epoch:5, Train loss:0.002815, valid loss:0.001505
Epoch:6, Train loss:0.002604, valid loss:0.001605
Epoch:7, Train loss:0.002548, valid loss:0.001397
Epoch:8, Train loss:0.002364, valid loss:0.001366
Epoch:9, Train loss:0.002304, valid loss:0.001343
Epoch:10, Train loss:0.002262, valid loss:0.001314
Epoch:11, Train loss:0.001637, valid loss:0.000940
Epoch:12, Train loss:0.001646, valid loss:0.000907
Epoch:13, Train loss:0.001622, valid loss:0.001004
Epoch:14, Train loss:0.001603, valid loss:0.000921
Epoch:15, Train loss:0.001569, valid loss:0.000842
Epoch:16, Train loss:0.001550, valid loss:0.000956
Epoch:17, Train loss:0.001521, valid loss:0.000888
Epoch:18, Train loss:0.001485, valid loss:0.001081
Epoch:19, Train loss:0.001514, valid loss:0.000931
Epoch:20, Train loss:0.001487, valid loss:0.000929
Epoch:21, Train loss:0.001190, valid loss:0.000833
Epoch:22, Train loss:0.001191, valid loss:0.000803
Epoch:23, Train loss:0.001160, valid loss:0.000756
Epoch:24, Train loss:0.001155, valid loss:0.000721
Epoch:25, Train loss:0.001141, valid loss:0.000714
Epoch:26, Train loss:0.001150, valid loss:0.000697
Epoch:27, Train loss:0.001132, valid loss:0.000739
Epoch:28, Train loss:0.001127, valid loss:0.000669
Epoch:29, Train loss:0.001128, valid loss:0.000662
Epoch:30, Train loss:0.001130, valid loss:0.000670
Epoch:31, Train loss:0.000964, valid loss:0.000575
Epoch:32, Train loss:0.000961, valid loss:0.000653
Epoch:33, Train loss:0.000959, valid loss:0.000575
Epoch:34, Train loss:0.000949, valid loss:0.000682
Epoch:35, Train loss:0.000941, valid loss:0.000574
Epoch:36, Train loss:0.000945, valid loss:0.000578
Epoch:37, Train loss:0.000943, valid loss:0.000617
Epoch:38, Train loss:0.000940, valid loss:0.000570
Epoch:39, Train loss:0.000928, valid loss:0.000590
Epoch:40, Train loss:0.000934, valid loss:0.000626
Epoch:41, Train loss:0.000846, valid loss:0.000546
Epoch:42, Train loss:0.000840, valid loss:0.000754
Epoch:43, Train loss:0.000850, valid loss:0.000539
Epoch:44, Train loss:0.000835, valid loss:0.000589
Epoch:45, Train loss:0.000835, valid loss:0.000568
Epoch:46, Train loss:0.000838, valid loss:0.000531
Epoch:47, Train loss:0.000833, valid loss:0.000539
Epoch:48, Train loss:0.000827, valid loss:0.000545
Epoch:49, Train loss:0.000825, valid loss:0.000643
Epoch:50, Train loss:0.000832, valid loss:0.000529
Epoch:51, Train loss:0.000787, valid loss:0.000517
Epoch:52, Train loss:0.000783, valid loss:0.000527
Epoch:53, Train loss:0.000781, valid loss:0.000516
Epoch:54, Train loss:0.000786, valid loss:0.000544
Epoch:55, Train loss:0.000784, valid loss:0.000532
Epoch:56, Train loss:0.000781, valid loss:0.000541
Epoch:57, Train loss:0.000782, valid loss:0.000542
Epoch:58, Train loss:0.000777, valid loss:0.000522
Epoch:59, Train loss:0.000777, valid loss:0.000545
Epoch:60, Train loss:0.000777, valid loss:0.000530
training time 7408.9987616539
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.023870850492508672
plot_id,batch_id 0 1 miss% 0.032128611713427235
plot_id,batch_id 0 2 miss% 0.0353843440130588
plot_id,batch_id 0 3 miss% 0.03581597881953472
plot_id,batch_id 0 4 miss% 0.0276619378297052
plot_id,batch_id 0 5 miss% 0.027582761919416558
plot_id,batch_id 0 6 miss% 0.0355204933563578
plot_id,batch_id 0 7 miss% 0.019937959018870244
plot_id,batch_id 0 8 miss% 0.02972461148184534
plot_id,batch_id 0 9 miss% 0.014751675606759489
plot_id,batch_id 0 10 miss% 0.06037589694268561
plot_id,batch_id 0 11 miss% 0.02935697657652586
plot_id,batch_id 0 12 miss% 0.024999369562261474
plot_id,batch_id 0 13 miss% 0.02383174693108959
plot_id,batch_id 0 14 miss% 0.03429158503353661
plot_id,batch_id 0 15 miss% 0.028152007612764905
plot_id,batch_id 0 16 miss% 0.03724123495834624
plot_id,batch_id 0 17 miss% 0.043082466349179324
plot_id,batch_id 0 18 miss% 0.027765061591014373
plot_id,batch_id 0 19 miss% 0.03448909205464119
plot_id,batch_id 0 20 miss% 0.03070644642592967
plot_id,batch_id 0 21 miss% 0.02323197554206951
plot_id,batch_id 0 22 miss% 0.026646410638919423
plot_id,batch_id 0 23 miss% 0.025562077595510286
plot_id,batch_id 0 24 miss% 0.0258265061670775
plot_id,batch_id 0 25 miss% 0.048231027590316194
plot_id,batch_id 0 26 miss% 0.021867865187888514
plot_id,batch_id 0 27 miss% 0.04147707243522448
plot_id,batch_id 0 28 miss% 0.0272798748568772
plot_id,batch_id 0 29 miss% 0.030011734868524844
plot_id,batch_id 0 30 miss% 0.0564391420778491
plot_id,batch_id 0 31 miss% 0.028729117137543277
plot_id,batch_id 0 32 miss% 0.027958890530294586
plot_id,batch_id 0 33 miss% 0.025918135106127213
plot_id,batch_id 0 34 miss% 0.03772302491677446
plot_id,batch_id 0 35 miss% 0.042037220102115816
plot_id,batch_id 0 36 miss% 0.04092589493004577
plot_id,batch_id 0 37 miss% 0.046110431493035675
plot_id,batch_id 0 38 miss% 0.027651223322027668
plot_id,batch_id 0 39 miss% 0.021342530993279124
plot_id,batch_id 0 40 miss% 0.08071935806978119
plot_id,batch_id 0 41 miss% 0.026462444642372004
plot_id,batch_id 0 42 miss% 0.024438780440659252
plot_id,batch_id 0 43 miss% 0.0265241366568443
plot_id,batch_id 0 44 miss% 0.024755867526180687
plot_id,batch_id 0 45 miss% 0.03352774783042539
plot_id,batch_id 0 46 miss% 0.030959529813438965
plot_id,batch_id 0 47 miss% 0.020070140205076536
plot_id,batch_id 0 48 miss% 0.02482301304004521
plot_id,batch_id 0 49 miss% 0.02178950752297164
plot_id,batch_id 0 50 miss% 0.03564146810004246
plot_id,batch_id 0 51 miss% 0.03165919807190771
plot_id,batch_id 0 52 miss% 0.03314254210606556
plot_id,batch_id 0 53 miss% 0.008697112291234292
plot_id,batch_id 0 54 miss% 0.03664851691008663
plot_id,batch_id 0 55 miss% 0.04520531273868924
plot_id,batch_id 0 56 miss% 0.024763264421049834
plot_id,batch_id 0 57 miss% 0.02679768081104631
plot_id,batch_id 0 58 miss% 0.023140987872215357
plot_id,batch_id 0 59 miss% 0.03130558838165965
plot_id,batch_id 0 60 miss% 0.04628775269204651
plot_id,batch_id 0 61 miss% 0.04560457033338244
plot_id,batch_id 0 62 miss% 0.0210097852653318
plot_id,batch_id 0 63 miss% 0.020176657159501274
plot_id,batch_id 0 64 miss% 0.033899688955293585
plot_id,batch_id 0 65 miss% 0.05780265130666963
plot_id,batch_id 0 66 miss% 0.028263234089484293
plot_id,batch_id 0 67 miss% 0.021483053058345768
plot_id,batch_id 0 68 miss% 0.0301864306331499
plot_id,batch_id 0 69 miss% 0.01955152439304481
plot_id,batch_id 0 70 miss% 0.03191899590474318
plot_id,batch_id 0 71 miss% 0.03653839486572372
plot_id,batch_id 0 72 miss% 0.03885114935273373
plot_id,batch_id 0 73 miss% 0.02596559854733856
plot_id,batch_id 0 74 miss% 0.05857835754844333
plot_id,batch_id 0 75 miss% 0.031541108103388234
plot_id,batch_id 0 76 miss% 0.037841336050732796
plot_id,batch_id 0 77 miss% 0.022150102150959677
plot_id,batch_id 0 78 miss% 0.02802064113343655
plot_id,batch_id 0 79 miss% 0.047110829739716815
plot_id,batch_id 0 80 miss% 0.04911941245722642
plot_id,batch_id 0 81 miss% 0.03204515358943966
plot_id,batch_id 0 82 miss% 0.026732505382737838
plot_id,batch_id 0 83 miss% 0.025904604240446945
plot_id,batch_id 0 84 miss% 0.0237867264366475
plot_id,batch_id 0 85 miss% 0.06090991573085159
plot_id,batch_id 0 86 miss% 0.028685791506357933
plot_id,batch_id 0 87 miss% 0.026129346740446917
plot_id,batch_id 0 88 miss% 0.03655832141313778
plot_id,batch_id 0 89 miss% 0.026985738119905664
plot_id,batch_id 0 90 miss% 0.038114147747808316
plot_id,batch_id 0 91 miss% 0.027552981851339694
plot_id,batch_id 0 92 miss% 0.024241249337567414
plot_id,batch_id 0 93 miss% 0.023938702546445232
plot_id,batch_id 0 94 miss% 0.024140505207270038
plot_id,batch_id 0 95 miss% 0.04920364117025064
plot_id,batch_id 0 96 miss% 0.024999260262016834
plot_id,batch_id 0 97 miss% 0.05121693355654049
plot_id,batch_id 0 98 miss% 0.03334098256507196
plot_id,batch_id 0 99 miss% 0.04624229413517955
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02387085 0.03212861 0.03538434 0.03581598 0.02766194 0.02758276
 0.03552049 0.01993796 0.02972461 0.01475168 0.0603759  0.02935698
 0.02499937 0.02383175 0.03429159 0.02815201 0.03724123 0.04308247
 0.02776506 0.03448909 0.03070645 0.02323198 0.02664641 0.02556208
 0.02582651 0.04823103 0.02186787 0.04147707 0.02727987 0.03001173
 0.05643914 0.02872912 0.02795889 0.02591814 0.03772302 0.04203722
 0.04092589 0.04611043 0.02765122 0.02134253 0.08071936 0.02646244
 0.02443878 0.02652414 0.02475587 0.03352775 0.03095953 0.02007014
 0.02482301 0.02178951 0.03564147 0.0316592  0.03314254 0.00869711
 0.03664852 0.04520531 0.02476326 0.02679768 0.02314099 0.03130559
 0.04628775 0.04560457 0.02100979 0.02017666 0.03389969 0.05780265
 0.02826323 0.02148305 0.03018643 0.01955152 0.031919   0.03653839
 0.03885115 0.0259656  0.05857836 0.03154111 0.03784134 0.0221501
 0.02802064 0.04711083 0.04911941 0.03204515 0.02673251 0.0259046
 0.02378673 0.06090992 0.02868579 0.02612935 0.03655832 0.02698574
 0.03811415 0.02755298 0.02424125 0.0239387  0.02414051 0.04920364
 0.02499926 0.05121693 0.03334098 0.04624229]
for model  207 the mean error 0.03255343536512907
all id 207 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 31 out win 3 err 0.03255343536512907
 69 miss% 0.02916710753833584
plot_id,batch_id 0 70 miss% 0.03833128427884639
plot_id,batch_id 0 71 miss% 0.07687027333814724
plot_id,batch_id 0 72 miss% 0.04457317046543521
plot_id,batch_id 0 73 miss% 0.024789333687871562
plot_id,batch_id 0 74 miss% 0.04525645034065192
plot_id,batch_id 0 75 miss% 0.04510727678517245
plot_id,batch_id 0 76 miss% 0.035903637240412886
plot_id,batch_id 0 77 miss% 0.026299444181866607
plot_id,batch_id 0 78 miss% 0.038906378140230284
plot_id,batch_id 0 79 miss% 0.04679864696847847
plot_id,batch_id 0 80 miss% 0.049878657715332404
plot_id,batch_id 0 81 miss% 0.025544392272838737
plot_id,batch_id 0 82 miss% 0.029003009038256474
plot_id,batch_id 0 83 miss% 0.02888563573472992
plot_id,batch_id 0 84 miss% 0.03177676048456811
plot_id,batch_id 0 85 miss% 0.047057384274523405
plot_id,batch_id 0 86 miss% 0.026798929244443496
plot_id,batch_id 0 87 miss% 0.02068522727902982
plot_id,batch_id 0 88 miss% 0.023364017884233524
plot_id,batch_id 0 89 miss% 0.02926397342287255
plot_id,batch_id 0 90 miss% 0.0480119008838147
plot_id,batch_id 0 91 miss% 0.03215699610340685
plot_id,batch_id 0 92 miss% 0.024493265810471444
plot_id,batch_id 0 93 miss% 0.037925976994097
plot_id,batch_id 0 94 miss% 0.025244336723680916
plot_id,batch_id 0 95 miss% 0.034868192370635646
plot_id,batch_id 0 96 miss% 0.02353429229819286
plot_id,batch_id 0 97 miss% 0.028424301459991565
plot_id,batch_id 0 98 miss% 0.03569865715223409
plot_id,batch_id 0 99 miss% 0.027651635262891112
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02808853 0.0217274  0.02696582 0.01975133 0.02483546 0.02596456
 0.0228686  0.02099406 0.02930615 0.01911335 0.05488068 0.03150372
 0.03160289 0.01712939 0.02845849 0.04212694 0.03134411 0.03842668
 0.02467826 0.0249006  0.03523751 0.02489246 0.03516948 0.02436177
 0.02490288 0.03019106 0.02448047 0.0315865  0.02727464 0.01644141
 0.03880427 0.03204594 0.02158242 0.02961165 0.02569307 0.03911355
 0.04299875 0.02099507 0.02159732 0.02504817 0.08767957 0.03762034
 0.01255332 0.03199654 0.03551158 0.03835484 0.03361889 0.02503405
 0.01195431 0.03341071 0.01896998 0.03616637 0.01758713 0.02233378
 0.02371051 0.02318362 0.02288973 0.02019654 0.02304131 0.02844949
 0.05973264 0.04056672 0.01775689 0.0242827  0.03105152 0.0629868
 0.04628315 0.02674609 0.02183956 0.02916711 0.03833128 0.07687027
 0.04457317 0.02478933 0.04525645 0.04510728 0.03590364 0.02629944
 0.03890638 0.04679865 0.04987866 0.02554439 0.02900301 0.02888564
 0.03177676 0.04705738 0.02679893 0.02068523 0.02336402 0.02926397
 0.0480119  0.032157   0.02449327 0.03792598 0.02524434 0.03486819
 0.02353429 0.0284243  0.03569866 0.02765164]
for model  156 the mean error 0.03134474648269542
all id 156 hidden_dim 24 learning_rate 0.02 num_layers 5 frames 25 out win 3 err 0.03134474648269542
Launcher: Job 208 completed in 7597 seconds.
Launcher: Task 184 done. Exiting.
Launcher: Job 157 completed in 7599 seconds.
Launcher: Task 174 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  35921
Epoch:0, Train loss:0.620031, valid loss:0.603796
Epoch:1, Train loss:0.048674, valid loss:0.018466
Epoch:2, Train loss:0.024188, valid loss:0.005779
Epoch:3, Train loss:0.010259, valid loss:0.004498
Epoch:4, Train loss:0.008310, valid loss:0.003837
Epoch:5, Train loss:0.007211, valid loss:0.003964
Epoch:6, Train loss:0.006331, valid loss:0.003000
Epoch:7, Train loss:0.005616, valid loss:0.002904
Epoch:8, Train loss:0.004951, valid loss:0.002908
Epoch:9, Train loss:0.004513, valid loss:0.002151
Epoch:10, Train loss:0.004099, valid loss:0.002416
Epoch:11, Train loss:0.003193, valid loss:0.001688
Epoch:12, Train loss:0.003142, valid loss:0.001684
Epoch:13, Train loss:0.002909, valid loss:0.001548
Epoch:14, Train loss:0.002894, valid loss:0.001521
Epoch:15, Train loss:0.002744, valid loss:0.001786
Epoch:16, Train loss:0.002709, valid loss:0.001408
Epoch:17, Train loss:0.002636, valid loss:0.001423
Epoch:18, Train loss:0.002547, valid loss:0.001461
Epoch:19, Train loss:0.002501, valid loss:0.001213
Epoch:20, Train loss:0.002445, valid loss:0.001225
Epoch:21, Train loss:0.002055, valid loss:0.001230
Epoch:22, Train loss:0.002026, valid loss:0.001122
Epoch:23, Train loss:0.001991, valid loss:0.001120
Epoch:24, Train loss:0.001979, valid loss:0.001057
Epoch:25, Train loss:0.001974, valid loss:0.001068
Epoch:26, Train loss:0.001938, valid loss:0.001173
Epoch:27, Train loss:0.001922, valid loss:0.001049
Epoch:28, Train loss:0.001878, valid loss:0.001083
Epoch:29, Train loss:0.001895, valid loss:0.001036
Epoch:30, Train loss:0.001867, valid loss:0.001097
Epoch:31, Train loss:0.001653, valid loss:0.001004
Epoch:32, Train loss:0.001645, valid loss:0.000996
Epoch:33, Train loss:0.001635, valid loss:0.000962
Epoch:34, Train loss:0.001640, valid loss:0.001000
Epoch:35, Train loss:0.001620, valid loss:0.000994
Epoch:36, Train loss:0.001604, valid loss:0.001011
Epoch:37, Train loss:0.001600, valid loss:0.001052
Epoch:38, Train loss:0.001597, valid loss:0.000943
Epoch:39, Train loss:0.001586, valid loss:0.000977
Epoch:40, Train loss:0.001572, valid loss:0.000975
Epoch:41, Train loss:0.001479, valid loss:0.000928
Epoch:42, Train loss:0.001474, valid loss:0.000913
Epoch:43, Train loss:0.001462, valid loss:0.000886
Epoch:44, Train loss:0.001464, valid loss:0.000908
Epoch:45, Train loss:0.001451, valid loss:0.000898
Epoch:46, Train loss:0.001455, valid loss:0.000877
Epoch:47, Train loss:0.001447, valid loss:0.000923
Epoch:48, Train loss:0.001445, valid loss:0.000899
Epoch:49, Train loss:0.001435, valid loss:0.000965
Epoch:50, Train loss:0.001436, valid loss:0.000910
Epoch:51, Train loss:0.001383, valid loss:0.000870
Epoch:52, Train loss:0.001381, valid loss:0.000877
Epoch:53, Train loss:0.001375, valid loss:0.000880
Epoch:54, Train loss:0.001379, valid loss:0.000872
Epoch:55, Train loss:0.001378, valid loss:0.000858
Epoch:56, Train loss:0.001367, valid loss:0.000873
Epoch:57, Train loss:0.001372, valid loss:0.000889
Epoch:58, Train loss:0.001368, valid loss:0.000864
Epoch:59, Train loss:0.001365, valid loss:0.000865
Epoch:60, Train loss:0.001366, valid loss:0.000861
training time 7421.454774856567
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.024756452700714762
plot_id,batch_id 0 1 miss% 0.02338700264633261
plot_id,batch_id 0 2 miss% 0.026890888221978694
plot_id,batch_id 0 3 miss% 0.026724177104266684
plot_id,batch_id 0 4 miss% 0.02575519382680808
plot_id,batch_id 0 5 miss% 0.05083547494093532
plot_id,batch_id 0 6 miss% 0.02201729702812577
plot_id,batch_id 0 7 miss% 0.029302753920833327
plot_id,batch_id 0 8 miss% 0.026279012397848175
plot_id,batch_id 0 9 miss% 0.018236730534060205
plot_id,batch_id 0 10 miss% 0.04433768799261381
plot_id,batch_id 0 11 miss% 0.05235082432115521
plot_id,batch_id 0 12 miss% 0.026096969348161494
plot_id,batch_id 0 13 miss% 0.03415322910524133
plot_id,batch_id 0 14 miss% 0.036107081359768915
plot_id,batch_id 0 15 miss% 0.036620216708911885
plot_id,batch_id 0 16 miss% 0.039398806622422004
plot_id,batch_id 0 17 miss% 0.052590774699878405
plot_id,batch_id 0 18 miss% 0.03624333490132659
plot_id,batch_id 0 19 miss% 0.03912895558377123
plot_id,batch_id 0 20 miss% 0.047203037222154895
plot_id,batch_id 0 21 miss% 0.01717221264837635
plot_id,batch_id 0 22 miss% 0.028404756693847035
plot_id,batch_id 0 23 miss% 0.0205870359205666
plot_id,batch_id 0 24 miss% 0.02421979012187991
plot_id,batch_id 0 25 miss% 0.023637736037337848
plot_id,batch_id 0 26 miss% 0.027905387888295958
plot_id,batch_id 0 27 miss% 0.02521809751880836
plot_id,batch_id 0 28 miss% 0.018886666296918576
plot_id,batch_id 0 29 miss% 0.022861828850996486
plot_id,batch_id 0 30 miss% 0.04110677917627823
plot_id,batch_id 0 31 miss% 0.04514260808963022
plot_id,batch_id 0 32 miss% 0.03198422418640175
plot_id,batch_id 0 33 miss% 0.025451190379254345
plot_id,batch_id 0 34 miss% 0.028096651528454766
plot_id,batch_id 0 35 miss% 0.04761926957271354
plot_id,batch_id 0 36 miss% 0.04087314765177098
plot_id,batch_id 0 37 miss% 0.025896640195214555
plot_id,batch_id 0 38 miss% 0.030271236254191813
plot_id,batch_id 0 39 miss% 0.01744984549751317
plot_id,batch_id 0 40 miss% 0.058578764070703775
plot_id,batch_id 0 41 miss% 0.02653191241041976
plot_id,batch_id 0 42 miss% 0.029110345365237576
plot_id,batch_id 0 43 miss% 0.03170655334024364
plot_id,batch_id 0 44 miss% 0.02357749035988922
plot_id,batch_id 0 45 miss% 0.029005800280387937
plot_id,batch_id 0 46 miss% 0.02146584969222978
plot_id,batch_id 0 47 miss% 0.024301870360561438
plot_id,batch_id 0 48 miss% 0.02732022918262071
plot_id,batch_id 0 49 miss% 0.02456539339566273
plot_id,batch_id 0 50 miss% 0.03352951445272133
plot_id,batch_id 0 51 miss% 0.029939899171571478
plot_id,batch_id 0 52 miss% 0.0271334869343791
plot_id,batch_id 0 53 miss% 0.021186239629321896
plot_id,batch_id 0 54 miss% 0.023928381037577877
plot_id,batch_id 0 55 miss% 0.044761231137292176
plot_id,batch_id 0 56 miss% 0.027398715893629497
plot_id,batch_id 0 57 miss% 0.02368783199415838
plot_id,batch_id 0 58 miss% 0.019420551809737337
plot_id,batch_id 0 59 miss% 0.021193961576028094
plot_id,batch_id 0 60 miss% 0.03162794376432298
plot_id,batch_id 0 61 miss% 0.0291741902989131
plot_id,batch_id 0 62 miss% 0.022967483340380332
plot_id,batch_id 0 63 miss% 0.030013466174835555
plot_id,batch_id 0 64 miss% 0.030803754874399503
plot_id,batch_id 0 65 miss% 0.054245849061817175
plot_id,batch_id 0 66 miss% 0.04832750953664364
plot_id,batch_id 0 67 miss% 0.02774896939426983
plot_id,batch_id 0 68 miss% 0.030725432630217705
plot_id,batch_id 0 69 miss% 0.017998565760520516
plot_id,batch_id 0 70 miss% 0.0437173847638101
plot_id,batch_id 0 71 miss% 0.04607469786757447
plot_id,batch_id 0 72 miss% 0.03876346198076853
plot_id,batch_id 0 73 miss% 0.04197794925401322
plot_id,batch_id 0 74 miss% 0.03990108571120514
plot_id,batch_id 0 75 miss% 0.04286034740476117
plot_id,batch_id 0 76 miss% 0.05366862093122377
plot_id,batch_id 0 77 miss% 0.040044987386669506
plot_id,batch_id 0 78 miss% 0.04952474624196343
plot_id,batch_id 0 79 miss% 0.03262636447529387
plot_id,batch_id 0 80 miss% 0.03263687795182939
plot_id,batch_id 0 81 miss% 0.02805976203871332
plot_id,batch_id 0 82 miss% 0.025420989269189436
plot_id,batch_id 0 83 miss% 0.03296324059893641
plot_id,batch_id 0 84 miss% 0.022875713467578256
plot_id,batch_id 0 85 miss% 0.034559321705909196
plot_id,batch_id 0 86 miss% 0.032472554562756176
plot_id,batch_id 0 87 miss% 0.027628629349478618
plot_id,batch_id 0 88 miss% 0.032476947740925244
plot_id,batch_id 0 89 miss% 0.029237722324711682
plot_id,batch_id 0 90 miss% 0.03425512120383267
plot_id,batch_id 0 91 miss% 0.03773192718324477
plot_id,batch_id 0 92 miss% 0.029627730895284764
plot_id,batch_id 0 93 miss% 0.03201916073849835
plot_id,batch_id 0 94 miss% 0.04541691708375697
plot_id,batch_id 0 95 miss% 0.04695088284771027
plot_id,batch_id 0 96 miss% 0.029768975683615796
plot_id,batch_id 0 97 miss% 0.05935759556480509
plot_id,batch_id 0 98 miss% 0.042851969901386525
plot_id,batch_id 0 99 miss% 0.037700141017345756
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02475645 0.023387   0.02689089 0.02672418 0.02575519 0.05083547
 0.0220173  0.02930275 0.02627901 0.01823673 0.04433769 0.05235082
 0.02609697 0.03415323 0.03610708 0.03662022 0.03939881 0.05259077
 0.03624333 0.03912896 0.04720304 0.01717221 0.02840476 0.02058704
 0.02421979 0.02363774 0.02790539 0.0252181  0.01888667 0.02286183
 0.04110678 0.04514261 0.03198422 0.02545119 0.02809665 0.04761927
 0.04087315 0.02589664 0.03027124 0.01744985 0.05857876 0.02653191
 0.02911035 0.03170655 0.02357749 0.0290058  0.02146585 0.02430187
 0.02732023 0.02456539 0.03352951 0.0299399  0.02713349 0.02118624
 0.02392838 0.04476123 0.02739872 0.02368783 0.01942055 0.02119396
 0.03162794 0.02917419 0.02296748 0.03001347 0.03080375 0.05424585
 0.04832751 0.02774897 0.03072543 0.01799857 0.04371738 0.0460747
 0.03876346 0.04197795 0.03990109 0.04286035 0.05366862 0.04004499
 0.04952475 0.03262636 0.03263688 0.02805976 0.02542099 0.03296324
 0.02287571 0.03455932 0.03247255 0.02762863 0.03247695 0.02923772
 0.03425512 0.03773193 0.02962773 0.03201916 0.04541692 0.04695088
 0.02976898 0.0593576  0.04285197 0.03770014]
for model  100 the mean error 0.03276350019773246
all id 100 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 25 out win 4 err 0.03276350019773246
Launcher: Job 101 completed in 7623 seconds.
Launcher: Task 173 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  61841
Epoch:0, Train loss:0.465810, valid loss:0.466722
Epoch:1, Train loss:0.104267, valid loss:0.002973
Epoch:2, Train loss:0.004600, valid loss:0.002023
Epoch:3, Train loss:0.003434, valid loss:0.001812
Epoch:4, Train loss:0.002854, valid loss:0.001582
Epoch:5, Train loss:0.002503, valid loss:0.001245
Epoch:6, Train loss:0.002303, valid loss:0.001284
Epoch:7, Train loss:0.002099, valid loss:0.001211
Epoch:8, Train loss:0.001995, valid loss:0.001350
Epoch:9, Train loss:0.001919, valid loss:0.001176
Epoch:10, Train loss:0.001826, valid loss:0.001132
Epoch:11, Train loss:0.001370, valid loss:0.000986
Epoch:12, Train loss:0.001380, valid loss:0.000983
Epoch:13, Train loss:0.001333, valid loss:0.000860
Epoch:14, Train loss:0.001310, valid loss:0.000824
Epoch:15, Train loss:0.001264, valid loss:0.000841
Epoch:16, Train loss:0.001254, valid loss:0.000719
Epoch:17, Train loss:0.001256, valid loss:0.000751
Epoch:18, Train loss:0.001200, valid loss:0.000711
Epoch:19, Train loss:0.001188, valid loss:0.000724
Epoch:20, Train loss:0.001178, valid loss:0.000727
Epoch:21, Train loss:0.000950, valid loss:0.000592
Epoch:22, Train loss:0.000925, valid loss:0.000605
Epoch:23, Train loss:0.000933, valid loss:0.000560
Epoch:24, Train loss:0.000918, valid loss:0.000636
Epoch:25, Train loss:0.000923, valid loss:0.000680
Epoch:26, Train loss:0.000928, valid loss:0.000566
Epoch:27, Train loss:0.000898, valid loss:0.000601
Epoch:28, Train loss:0.000890, valid loss:0.000703
Epoch:29, Train loss:0.000912, valid loss:0.000578
Epoch:30, Train loss:0.000903, valid loss:0.000670
Epoch:31, Train loss:0.000753, valid loss:0.000526
Epoch:32, Train loss:0.000747, valid loss:0.000509
Epoch:33, Train loss:0.000748, valid loss:0.000521
Epoch:34, Train loss:0.000762, valid loss:0.000530
Epoch:35, Train loss:0.000741, valid loss:0.000510
Epoch:36, Train loss:0.000734, valid loss:0.000560
Epoch:37, Train loss:0.000748, valid loss:0.000519
Epoch:38, Train loss:0.000739, valid loss:0.000491
Epoch:39, Train loss:0.000743, valid loss:0.000489
Epoch:40, Train loss:0.000719, valid loss:0.000492
Epoch:41, Train loss:0.000674, valid loss:0.000494
Epoch:42, Train loss:0.000666, valid loss:0.000495
Epoch:43, Train loss:0.000667, valid loss:0.000472
Epoch:44, Train loss:0.000666, valid loss:0.000489
Epoch:45, Train loss:0.000666, valid loss:0.000500
Epoch:46, Train loss:0.000667, valid loss:0.000464
Epoch:47, Train loss:0.000661, valid loss:0.000478
Epoch:48, Train loss:0.000659, valid loss:0.000503
Epoch:49, Train loss:0.000660, valid loss:0.000475
Epoch:50, Train loss:0.000655, valid loss:0.000475
Epoch:51, Train loss:0.000631, valid loss:0.000468
Epoch:52, Train loss:0.000630, valid loss:0.000465
Epoch:53, Train loss:0.000629, valid loss:0.000471
Epoch:54, Train loss:0.000627, valid loss:0.000461
Epoch:55, Train loss:0.000626, valid loss:0.000464
Epoch:56, Train loss:0.000628, valid loss:0.000456
Epoch:57, Train loss:0.000625, valid loss:0.000462
Epoch:58, Train loss:0.000624, valid loss:0.000471
Epoch:59, Train loss:0.000624, valid loss:0.000496
Epoch:60, Train loss:0.000622, valid loss:0.000465
training time 7450.236840724945
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.018054593748177666
plot_id,batch_id 0 1 miss% 0.027541519472596053
plot_id,batch_id 0 2 miss% 0.01724519343016515
plot_id,batch_id 0 3 miss% 0.01902091671299007
plot_id,batch_id 0 4 miss% 0.02038762777207774
plot_id,batch_id 0 5 miss% 0.028854466794399684
plot_id,batch_id 0 6 miss% 0.022674633950389007
plot_id,batch_id 0 7 miss% 0.02516948328605634
plot_id,batch_id 0 8 miss% 0.01875870536627538
plot_id,batch_id 0 9 miss% 0.023779762939687053
plot_id,batch_id 0 10 miss% 0.02926503243156913
plot_id,batch_id 0 11 miss% 0.027069516977277486
plot_id,batch_id 0 12 miss% 0.027612372636206126
plot_id,batch_id 0 13 miss% 0.03150837463638118
plot_id,batch_id 0 14 miss% 0.0188763588035641
plot_id,batch_id 0 15 miss% 0.03451295735923313
plot_id,batch_id 0 16 miss% 0.03213625537010997
plot_id,batch_id 0 17 miss% 0.030881559223252187
plot_id,batch_id 0 18 miss% 0.03339134730786955
plot_id,batch_id 0 19 miss% 0.02597505446761097
plot_id,batch_id 0 20 miss% 0.03332728619121023
plot_id,batch_id 0 21 miss% 0.02009481685174513
plot_id,batch_id 0 22 miss% 0.020783798421650367
plot_id,batch_id 0 23 miss% 0.017098080296591735
plot_id,batch_id 0 24 miss% 0.017351555212604362
plot_id,batch_id 0 25 miss% 0.03231676897797203
plot_id,batch_id 0 26 miss% 0.022928820672198823
plot_id,batch_id 0 27 miss% 0.020594664192389627
plot_id,batch_id 0 28 miss% 0.024541837985395422
plot_id,batch_id 0 29 miss% 0.018714523099254213
plot_id,batch_id 0 30 miss% 0.04247948612014795
plot_id,batch_id 0 31 miss% 0.024100985173736416
plot_id,batch_id 0 32 miss% 0.020436794242541377
plot_id,batch_id 0 33 miss% 0.018237846594341602
plot_id,batch_id 0 34 miss% 0.02762867025631846
plot_id,batch_id 0 35 miss% 0.045404456831609985
plot_id,batch_id 0 36 miss% 0.02544360038649011
plot_id,batch_id 0 37 miss% 0.02395920608817973
plot_id,batch_id 0 38 miss% 0.01985234001883556
plot_id,batch_id 0 39 miss% 0.015280680229487835
plot_id,batch_id 0 40 miss% 0.05288007940733295
plot_id,batch_id 0 41 miss% 0.016647357055072916
plot_id,batch_id 0 42 miss% 0.01570454580209455
plot_id,batch_id 0 43 miss% 0.02758659226820514
plot_id,batch_id 0 44 miss% 0.022141069714752364
plot_id,batch_id 0 45 miss% 0.025523582061546778
plot_id,batch_id 0 46 miss% 0.02627481463826152
plot_id,batch_id 0 47 miss% 0.01865778617138746
plot_id,batch_id 0 48 miss% 0.019611621462304323
plot_id,batch_id 0 49 miss% 0.0223534035356283
plot_id,batch_id 0 50 miss% 0.039452958965445414
plot_id,batch_id 0 51 miss% 0.027357114684912816
plot_id,batch_id 0 52 miss% 0.017651860189151377
plot_id,batch_id 0 53 miss% 0.009951993307434747
plot_id,batch_id 0 54 miss% 0.03118165104391887
plot_id,batch_id 0 55 miss% 0.04236008811256466
plot_id,batch_id 0 56 miss% 0.014796300800483655
plot_id,batch_id 0 57 miss% 0.025432431596813607
plot_id,batch_id 0 58 miss% 0.024751843458880634
plot_id,batch_id 0 59 miss% 0.017706136984205745
plot_id,batch_id 0 60 miss% 0.03166225565163098
plot_id,batch_id 0 61 miss% 0.025754145977816154
plot_id,batch_id 0 62 miss% 0.028472393937276613
plot_id,batch_id 0 63 miss% 0.029979719746936973
plot_id,batch_id 0 64 miss% 0.03343006842813993
plot_id,batch_id 0 65 miss% 0.03783074321947145
plot_id,batch_id 0 66 miss% 0.028023413279744808
plot_id,batch_id 0 67 miss% 0.03254905327915624
plot_id,batch_id 0 68 miss% 0.03327492807358895
plot_id,batch_id 0 69 miss% 0.03288332093301765
plot_id,batch_id 0 70 miss% 0.049330612838739886
plot_id,batch_id 0 71 miss% 0.03420046726467007
plot_id,batch_id 0 72 miss% 0.030872812177352545
plot_id,batch_id 0 73 miss% 0.019631513069971417
plot_id,batch_id 0 74 miss% 0.04363298687379785
plot_id,batch_id 0 75 miss% 0.026064013020074567
plot_id,batch_id 0 76 miss% 0.03234740398693912
plot_id,batch_id 0 77 miss% 0.02241474044924185
plot_id,batch_id 0 78 miss% 0.03162845892721466
plot_id,batch_id 0 79 miss% 0.031216573660419612
plot_id,batch_id 0 80 miss% 0.03795447681592934
plot_id,batch_id 0 81 miss% 0.0180296298880033
plot_id,batch_id 0 82 miss% 0.03294575070565361
plot_id,batch_id 0 83 miss% 0.028630684990421425
plot_id,batch_id 0 84 miss% 0.019622462540767962
plot_id,batch_id 0 85 miss% 0.06345310161221575
plot_id,batch_id 0 86 miss% 0.027783299286339335
plot_id,batch_id 0 87 miss% 0.02206913407652991
plot_id,batch_id 0 88 miss% 0.0337308091158201
plot_id,batch_id 0 89 miss% 0.0283697716058298
plot_id,batch_id 0 90 miss% 0.03995835929022584
plot_id,batch_id 0 91 miss% 0.038568618334588994
plot_id,batch_id 0 92 miss% 0.023273585555741137
plot_id,batch_id 0 93 miss% 0.03218880681628523
plot_id,batch_id 0 94 miss% 0.04345741439099981
plot_id,batch_id 0 95 miss% 0.039135591177119865
plot_id,batch_id 0 96 miss% 0.02433759842289558
plot_id,batch_id 0 97 miss% 0.060310509447912095
plot_id,batch_id 0 98 miss% 0.022972629888726875
plot_id,batch_id 0 99 miss% 0.03212985798153883
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01805459 0.02754152 0.01724519 0.01902092 0.02038763 0.02885447
 0.02267463 0.02516948 0.01875871 0.02377976 0.02926503 0.02706952
 0.02761237 0.03150837 0.01887636 0.03451296 0.03213626 0.03088156
 0.03339135 0.02597505 0.03332729 0.02009482 0.0207838  0.01709808
 0.01735156 0.03231677 0.02292882 0.02059466 0.02454184 0.01871452
 0.04247949 0.02410099 0.02043679 0.01823785 0.02762867 0.04540446
 0.0254436  0.02395921 0.01985234 0.01528068 0.05288008 0.01664736
 0.01570455 0.02758659 0.02214107 0.02552358 0.02627481 0.01865779
 0.01961162 0.0223534  0.03945296 0.02735711 0.01765186 0.00995199
 0.03118165 0.04236009 0.0147963  0.02543243 0.02475184 0.01770614
 0.03166226 0.02575415 0.02847239 0.02997972 0.03343007 0.03783074
 0.02802341 0.03254905 0.03327493 0.03288332 0.04933061 0.03420047
 0.03087281 0.01963151 0.04363299 0.02606401 0.0323474  0.02241474
 0.03162846 0.03121657 0.03795448 0.01802963 0.03294575 0.02863068
 0.01962246 0.0634531  0.0277833  0.02206913 0.03373081 0.02836977
 0.03995836 0.03856862 0.02327359 0.03218881 0.04345741 0.03913559
 0.0243376  0.06031051 0.02297263 0.03212986]
for model  174 the mean error 0.028054368985277328
all id 174 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 31 out win 3 err 0.028054368985277328
Launcher: Job 175 completed in 7638 seconds.
Launcher: Task 144 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  28945
Epoch:0, Train loss:0.332371, valid loss:0.324912
Epoch:1, Train loss:0.025640, valid loss:0.004316
Epoch:2, Train loss:0.007139, valid loss:0.003327
Epoch:3, Train loss:0.005117, valid loss:0.002403
Epoch:4, Train loss:0.004209, valid loss:0.002023
Epoch:5, Train loss:0.003715, valid loss:0.002404
Epoch:6, Train loss:0.003290, valid loss:0.001594
Epoch:7, Train loss:0.003252, valid loss:0.001811
Epoch:8, Train loss:0.003159, valid loss:0.001813
Epoch:9, Train loss:0.002968, valid loss:0.001618
Epoch:10, Train loss:0.002882, valid loss:0.001401
Epoch:11, Train loss:0.002136, valid loss:0.001219
Epoch:12, Train loss:0.002120, valid loss:0.001137
Epoch:13, Train loss:0.002115, valid loss:0.001558
Epoch:14, Train loss:0.002087, valid loss:0.001286
Epoch:15, Train loss:0.002025, valid loss:0.001015
Epoch:16, Train loss:0.002025, valid loss:0.001046
Epoch:17, Train loss:0.002032, valid loss:0.001237
Epoch:18, Train loss:0.001976, valid loss:0.001013
Epoch:19, Train loss:0.001897, valid loss:0.001051
Epoch:20, Train loss:0.001894, valid loss:0.001063
Epoch:21, Train loss:0.001522, valid loss:0.000998
Epoch:22, Train loss:0.001531, valid loss:0.000863
Epoch:23, Train loss:0.001523, valid loss:0.000898
Epoch:24, Train loss:0.001520, valid loss:0.000979
Epoch:25, Train loss:0.001504, valid loss:0.000896
Epoch:26, Train loss:0.001450, valid loss:0.000891
Epoch:27, Train loss:0.001467, valid loss:0.000853
Epoch:28, Train loss:0.001484, valid loss:0.000904
Epoch:29, Train loss:0.001478, valid loss:0.000806
Epoch:30, Train loss:0.001459, valid loss:0.000942
Epoch:31, Train loss:0.001247, valid loss:0.000795
Epoch:32, Train loss:0.001240, valid loss:0.000883
Epoch:33, Train loss:0.001224, valid loss:0.000706
Epoch:34, Train loss:0.001246, valid loss:0.000795
Epoch:35, Train loss:0.001230, valid loss:0.000719
Epoch:36, Train loss:0.001218, valid loss:0.000881
Epoch:37, Train loss:0.001212, valid loss:0.000738
Epoch:38, Train loss:0.001208, valid loss:0.000772
Epoch:39, Train loss:0.001216, valid loss:0.000671
Epoch:40, Train loss:0.001215, valid loss:0.000740
Epoch:41, Train loss:0.001089, valid loss:0.000700
Epoch:42, Train loss:0.001083, valid loss:0.000699
Epoch:43, Train loss:0.001088, valid loss:0.000739
Epoch:44, Train loss:0.001080, valid loss:0.000692
Epoch:45, Train loss:0.001083, valid loss:0.000684
Epoch:46, Train loss:0.001073, valid loss:0.000708
Epoch:47, Train loss:0.001073, valid loss:0.000717
Epoch:48, Train loss:0.001083, valid loss:0.000746
Epoch:49, Train loss:0.001067, valid loss:0.000677
Epoch:50, Train loss:0.001071, valid loss:0.000707
Epoch:51, Train loss:0.001013, valid loss:0.000641
Epoch:52, Train loss:0.001006, valid loss:0.000658
Epoch:53, Train loss:0.001009, valid loss:0.000669
Epoch:54, Train loss:0.001003, valid loss:0.000668
Epoch:55, Train loss:0.001004, valid loss:0.000657
Epoch:56, Train loss:0.001002, valid loss:0.000656
Epoch:57, Train loss:0.001001, valid loss:0.000668
Epoch:58, Train loss:0.001000, valid loss:0.000647
Epoch:59, Train loss:0.000998, valid loss:0.000685
Epoch:60, Train loss:0.000999, valid loss:0.000651
training time 7468.337826251984
total number of trained parameters for initialize model 28945
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.02999041977289067
plot_id,batch_id 0 1 miss% 0.021690651940430194
plot_id,batch_id 0 2 miss% 0.023528037262389678
plot_id,batch_id 0 3 miss% 0.027694480850465164
plot_id,batch_id 0 4 miss% 0.03084494038621122
plot_id,batch_id 0 5 miss% 0.04834198206950737
plot_id,batch_id 0 6 miss% 0.022938067624331777
plot_id,batch_id 0 7 miss% 0.019368130299072608
plot_id,batch_id 0 8 miss% 0.032337240613054055
plot_id,batch_id 0 9 miss% 0.025296871856075587
plot_id,batch_id 0 10 miss% 0.0519540289112079
plot_id,batch_id 0 11 miss% 0.033910179737117796
plot_id,batch_id 0 12 miss% 0.028488079429245736
plot_id,batch_id 0 13 miss% 0.028989846386074537
plot_id,batch_id 0 14 miss% 0.04168132748612653
plot_id,batch_id 0 15 miss% 0.04526523184593607
plot_id,batch_id 0 16 miss% 0.030651578934429772
plot_id,batch_id 0 17 miss% 0.03892376915029468
plot_id,batch_id 0 18 miss% 0.03155499875013981
plot_id,batch_id 0 19 miss% 0.04173297832979332
plot_id,batch_id 0 20 miss% 0.05928340127753529
plot_id,batch_id 0 21 miss% 0.02923544481856237
plot_id,batch_id 0 22 miss% 0.023589590950474568
plot_id,batch_id 0 23 miss% 0.027179186652974693
plot_id,batch_id 0 24 miss% 0.03451092113448831
plot_id,batch_id 0 25 miss% 0.032499625068105914
plot_id,batch_id 0 26 miss% 0.034815568659734605
plot_id,batch_id 0 27 miss% 0.022210446655839898
plot_id,batch_id 0 28 miss% 0.03137275812744047
plot_id,batch_id 0 29 miss% 0.030742998606062177
plot_id,batch_id 0 30 miss% 0.04538836063606358
plot_id,batch_id 0 31 miss% 0.02847181897698033
plot_id,batch_id 0 32 miss% 0.03332985484975361
plot_id,batch_id 0 33 miss% 0.02779124508149905
plot_id,batch_id 0 34 miss% 0.038891812878261864
plot_id,batch_id 0 35 miss% 0.03810356551838118
plot_id,batch_id 0 36 miss% 0.046362074009748525
plot_id,batch_id 0 37 miss% 0.03885837821867318
plot_id,batch_id 0 38 miss% 0.036511875785486404
plot_id,batch_id 0 39 miss% 0.02801349938624745
plot_id,batch_id 0 40 miss% 0.07797517578634595
plot_id,batch_id 0 41 miss% 0.029767183652151846
plot_id,batch_id 0 42 miss% 0.017422569061791193
plot_id,batch_id 0 43 miss% 0.03272694999422293
plot_id,batch_id 0 44 miss% 0.03205573878288845
plot_id,batch_id 0 45 miss% 0.031125254017467224
plot_id,batch_id 0 46 miss% 0.02139456368581486
plot_id,batch_id 0 47 miss% 0.01676181463937549
plot_id,batch_id 0 48 miss% 0.028381579350006462
plot_id,batch_id 0 49 miss% 0.03212226130936531
plot_id,batch_id 0 50 miss% 0.02851480776403964
plot_id,batch_id 0 51 miss% 0.018144660924149843
plot_id,batch_id 0 52 miss% 0.030287403167647126
plot_id,batch_id 0 53 miss% 0.029171385163377736
plot_id,batch_id 0 54 miss% 0.037530479676853266
plot_id,batch_id 0 55 miss% 0.03859907093543144
plot_id,batch_id 0 56 miss% 0.023775516772664845
plot_id,batch_id 0 57 miss% 0.03826493210011233
plot_id,batch_id 0 58 miss% 0.03597291776429281
plot_id,batch_id 0 59 miss% 0.02889485920882148
plot_id,batch_id 0 60 miss% 0.04132112079067483
plot_id,batch_id 0 61 miss% 0.030847552496681467
plot_id,batch_id 0 62 miss% 0.028230156472979405
plot_id,batch_id 0 63 miss% 0.023438293720185837
plot_id,batch_id 0 64 miss% 0.026536414668277684
plot_id,batch_id 0 65 miss% 0.040452231774272866
plot_id,batch_id 0 66 miss% 0.03049920013756037
plot_id,batch_id 0 67 miss% 0.026487553372014087
plot_id,batch_id 0 68 miss% 0.03906362134004496
plot_id,batch_id 0 69 miss% 0.02702341601998771
plot_id,batch_id 0 70 miss% 0.025054281455069567
plot_id,batch_id 0 71 miss% 0.03589646245179364
plot_id,batch_id 0 72 miss% 0.03813463059345531
plot_id,batch_id 0 73 miss% 0.030036230693913486
plot_id,batch_id 0 74 miss% 0.04373879001259224
plot_id,batch_id 0 75 miss% 0.06315212416055727
plot_id,batch_id 0 76 miss% 0.053444188384666136
plot_id,batch_id 0 77 miss% 0.049688631945526286
plot_id,batch_id 0 78 miss% 0.04430538147433112
plot_id,batch_id 0 79 miss% 0.04863295402480562
plot_id,batch_id 0 80 miss% 0.05092168113461914
plot_id,batch_id 0 81 miss% 0.025985235478333535
plot_id,batch_id 0 82 miss% 0.03285296731543312
plot_id,batch_id 0 83 miss% 0.038466796388870246
plot_id,batch_id 0 84 miss% 0.02793211872036327
plot_id,batch_id 0 85 miss% 0.07076610066122138
plot_id,batch_id 0 86 miss% 0.02189315672099962
plot_id,batch_id 0 87 miss% 0.02708172164436271
plot_id,batch_id 0 88 miss% 0.03544666971785072
plot_id,batch_id 0 89 miss% 0.03313459418513703
plot_id,batch_id 0 90 miss% 0.03571262544068824
plot_id,batch_id 0 91 miss% 0.029993143238211412
plot_id,batch_id 0 92 miss% 0.038463720708893
plot_id,batch_id 0 93 miss% 0.03358318188258131
plot_id,batch_id 0 94 miss% 0.037232309365251806
plot_id,batch_id 0 95 miss% 0.08707192780000396
plot_id,batch_id 0 96 miss% 0.05327487747752098
plot_id,batch_id 0 97 miss% 0.059048727148356024
plot_id,batch_id 0 98 miss% 0.03978777085081221
plot_id,batch_id 0 99 miss% 0.04560331741955932
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02999042 0.02169065 0.02352804 0.02769448 0.03084494 0.04834198
 0.02293807 0.01936813 0.03233724 0.02529687 0.05195403 0.03391018
 0.02848808 0.02898985 0.04168133 0.04526523 0.03065158 0.03892377
 0.031555   0.04173298 0.0592834  0.02923544 0.02358959 0.02717919
 0.03451092 0.03249963 0.03481557 0.02221045 0.03137276 0.030743
 0.04538836 0.02847182 0.03332985 0.02779125 0.03889181 0.03810357
 0.04636207 0.03885838 0.03651188 0.0280135  0.07797518 0.02976718
 0.01742257 0.03272695 0.03205574 0.03112525 0.02139456 0.01676181
 0.02838158 0.03212226 0.02851481 0.01814466 0.0302874  0.02917139
 0.03753048 0.03859907 0.02377552 0.03826493 0.03597292 0.02889486
 0.04132112 0.03084755 0.02823016 0.02343829 0.02653641 0.04045223
 0.0304992  0.02648755 0.03906362 0.02702342 0.02505428 0.03589646
 0.03813463 0.03003623 0.04373879 0.06315212 0.05344419 0.04968863
 0.04430538 0.04863295 0.05092168 0.02598524 0.03285297 0.0384668
 0.02793212 0.0707661  0.02189316 0.02708172 0.03544667 0.03313459
 0.03571263 0.02999314 0.03846372 0.03358318 0.03723231 0.08707193
 0.05327488 0.05904873 0.03978777 0.04560332]
for model  200 the mean error 0.03541470271972387
all id 200 hidden_dim 16 learning_rate 0.01 num_layers 4 frames 31 out win 5 err 0.03541470271972387
Launcher: Job 201 completed in 7664 seconds.
Launcher: Task 30 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  79249
Epoch:0, Train loss:0.430215, valid loss:0.408020
Epoch:1, Train loss:0.027605, valid loss:0.004566
Epoch:2, Train loss:0.008300, valid loss:0.005207
Epoch:3, Train loss:0.006417, valid loss:0.003220
Epoch:4, Train loss:0.005249, valid loss:0.002641
Epoch:5, Train loss:0.004858, valid loss:0.002877
Epoch:6, Train loss:0.004462, valid loss:0.002206
Epoch:7, Train loss:0.004068, valid loss:0.002293
Epoch:8, Train loss:0.004076, valid loss:0.002204
Epoch:9, Train loss:0.003753, valid loss:0.001921
Epoch:10, Train loss:0.003670, valid loss:0.002212
Epoch:11, Train loss:0.002547, valid loss:0.001580
Epoch:12, Train loss:0.002565, valid loss:0.001429
Epoch:13, Train loss:0.002569, valid loss:0.001364
Epoch:14, Train loss:0.002427, valid loss:0.001280
Epoch:15, Train loss:0.002356, valid loss:0.001553
Epoch:16, Train loss:0.002299, valid loss:0.001302
Epoch:17, Train loss:0.002379, valid loss:0.001278
Epoch:18, Train loss:0.002257, valid loss:0.001410
Epoch:19, Train loss:0.002296, valid loss:0.001355
Epoch:20, Train loss:0.002208, valid loss:0.001380
Epoch:21, Train loss:0.001691, valid loss:0.001144
Epoch:22, Train loss:0.001647, valid loss:0.001131
Epoch:23, Train loss:0.001648, valid loss:0.001064
Epoch:24, Train loss:0.001641, valid loss:0.001127
Epoch:25, Train loss:0.001642, valid loss:0.001167
Epoch:26, Train loss:0.001609, valid loss:0.001143
Epoch:27, Train loss:0.001598, valid loss:0.001193
Epoch:28, Train loss:0.001592, valid loss:0.001012
Epoch:29, Train loss:0.001542, valid loss:0.001028
Epoch:30, Train loss:0.001558, valid loss:0.001076
Epoch:31, Train loss:0.001271, valid loss:0.000878
Epoch:32, Train loss:0.001274, valid loss:0.000914
Epoch:33, Train loss:0.001257, valid loss:0.000911
Epoch:34, Train loss:0.001252, valid loss:0.000999
Epoch:35, Train loss:0.001249, valid loss:0.000947
Epoch:36, Train loss:0.001243, valid loss:0.000881
Epoch:37, Train loss:0.001259, valid loss:0.000946
Epoch:38, Train loss:0.001247, valid loss:0.000879
Epoch:39, Train loss:0.001222, valid loss:0.001009
Epoch:40, Train loss:0.001215, valid loss:0.000897
Epoch:41, Train loss:0.001083, valid loss:0.000853
Epoch:42, Train loss:0.001080, valid loss:0.000848
Epoch:43, Train loss:0.001072, valid loss:0.000853
Epoch:44, Train loss:0.001064, valid loss:0.000859
Epoch:45, Train loss:0.001075, valid loss:0.000864
Epoch:46, Train loss:0.001080, valid loss:0.000860
Epoch:47, Train loss:0.001065, valid loss:0.000823
Epoch:48, Train loss:0.001058, valid loss:0.000885
Epoch:49, Train loss:0.001051, valid loss:0.000861
Epoch:50, Train loss:0.001051, valid loss:0.000823
Epoch:51, Train loss:0.000982, valid loss:0.000812
Epoch:52, Train loss:0.000985, valid loss:0.000820
Epoch:53, Train loss:0.000974, valid loss:0.000785
Epoch:54, Train loss:0.000987, valid loss:0.000823
Epoch:55, Train loss:0.000977, valid loss:0.000793
Epoch:56, Train loss:0.000977, valid loss:0.000807
Epoch:57, Train loss:0.000978, valid loss:0.000833
Epoch:58, Train loss:0.000966, valid loss:0.000812
Epoch:59, Train loss:0.000963, valid loss:0.000798
Epoch:60, Train loss:0.000966, valid loss:0.000820
training time 7625.9761328697205
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02476304827552871
plot_id,batch_id 0 1 miss% 0.03225441865914809
plot_id,batch_id 0 2 miss% 0.02802298447049634
plot_id,batch_id 0 3 miss% 0.02258622689055317
plot_id,batch_id 0 4 miss% 0.01907884190411597
plot_id,batch_id 0 5 miss% 0.025929774240751335
plot_id,batch_id 0 6 miss% 0.029555372022298635
plot_id,batch_id 0 7 miss% 0.029114142784120375
plot_id,batch_id 0 8 miss% 0.02795009921078691
plot_id,batch_id 0 9 miss% 0.028626617299015496
plot_id,batch_id 0 10 miss% 0.05005143601304351
plot_id,batch_id 0 11 miss% 0.054383624693746614
plot_id,batch_id 0 12 miss% 0.023878009772736845
plot_id,batch_id 0 13 miss% 0.03631726727824455
plot_id,batch_id 0 14 miss% 0.044112031935093714
plot_id,batch_id 0 15 miss% 0.04523252652733131
plot_id,batch_id 0 16 miss% 0.03356442133816458
plot_id,batch_id 0 17 miss% 0.03729053214781816
plot_id,batch_id 0 18 miss% 0.03814946573796205
plot_id,batch_id 0 19 miss% 0.04084778070317815
plot_id,batch_id 0 20 miss% 0.06920599321419774
plot_id,batch_id 0 21 miss% 0.02118445419879437
plot_id,batch_id 0 22 miss% 0.01956438910925968
plot_id,batch_id 0 23 miss% 0.024018409806833438
plot_id,batch_id 0 24 miss% 0.016164616535615163
plot_id,batch_id 0 25 miss% 0.03802009593637422
plot_id,batch_id 0 26 miss% 0.019502361152543425
plot_id,batch_id 0 27 miss% 0.02160485517087777
plot_id,batch_id 0 28 miss% 0.019326750086380254
plot_id,batch_id 0 29 miss% 0.025888260007380176
plot_id,batch_id 0 30 miss% 0.04386310382395633
plot_id,batch_id 0 31 miss% 0.03114726290992846
plot_id,batch_id 0 32 miss% 0.027553248387668754
plot_id,batch_id 0 33 miss% 0.023522505277171982
plot_id,batch_id 0 34 miss% 0.02002366528614181
plot_id,batch_id 0 35 miss% 0.04557912002071407
plot_id,batch_id 0 36 miss% 0.04082993235959242
plot_id,batch_id 0 37 miss% 0.020974464438673353
plot_id,batch_id 0 38 miss% 0.03909006077144696
plot_id,batch_id 0 39 miss% 0.017709289565090717
plot_id,batch_id 0 40 miss% 0.08014203399124685
plot_id,batch_id 0 41 miss% 0.016667202001774763
plot_id,batch_id 0 42 miss% 0.021864820222384708
plot_id,batch_id 0 43 miss% 0.023882174169978496
plot_id,batch_id 0 44 miss% 0.01614126429202495
plot_id,batch_id 0 45 miss% 0.026332873653457968
plot_id,batch_id 0 46 miss% 0.011751393218412634
plot_id,batch_id 0 47 miss% 0.021168796297224447
plot_id,batch_id 0 48 miss% 0.025703635361842052
plot_id,batch_id 0 49 miss% 0.0243538373263663
plot_id,batch_id 0 50 miss% 0.04087537767793209
plot_id,batch_id 0 51 miss% 0.01913754379211024
plot_id,batch_id 0 52 miss% 0.016057437727089115
plot_id,batch_id 0 53 miss% 0.009617274370429514
plot_id,batch_id 0 54 miss% 0.032535954058853234
plot_id,batch_id 0 55 miss% 0.03338527257739313
plot_id,batch_id 0 56 miss% 0.02688509146731742
plot_id,batch_id 0 57 miss% 0.012001278330979463
plot_id,batch_id 0 58 miss% 0.009746457835246523
plot_id,batch_id 0 59 miss% 0.019263971835008793
plot_id,batch_id 0 60 miss% 0.03842738904142537
plot_id,batch_id 0 61 miss% 0.03185414162953995
plot_id,batch_id 0 62 miss% 0.0372550343481004
plot_id,batch_id 0 63 miss% 0.03732268071736153
plot_id,batch_id 0 64 miss% 0.026029429252167335
plot_id,batch_id 0 65 miss% 0.04551616287342885
plot_id,batch_id 0 66 miss% 0.03573270897484275
plot_id,batch_id 0 67 miss% 0.027031790543623112
plot_id,batch_id 0 68 miss% 0.03153195864624444
plot_id,batch_id 0 69 miss% 0.04389709079711904
plot_id,batch_id 0 70 miss% 0.05141165969973472
plot_id,batch_id 0 71 miss% 0.04954611918296423
plot_id,batch_id 0 72 miss% 0.040843420582542096
plot_id,batch_id 0 73 miss% 0.025534232935479427
plot_id,batch_id 0 74 miss% 0.025080774210090397
plot_id,batch_id 0 75 miss% 0.03385102439853681
plot_id,batch_id 0 76 miss% 0.026832155919610707
plot_id,batch_id 0 77 miss% 0.0287784506487113
plot_id,batch_id 0 78 miss% 0.0339008886621553
plot_id,batch_id 0 79 miss% 0.04288352463854808
plot_id,batch_id 0 80 miss% 0.029548722787370682
plot_id,batch_id 0 81 miss% 0.02298750518708425
plot_id,batch_id 0 82 miss% 0.02993664244860036
plot_id,batch_id 0 83 miss% 0.02842667721049649
plot_id,batch_id 0 84 miss% 0.038047214625294284
plot_id,batch_id 0 85 miss% 0.04906306463976469
plot_id,batch_id 0 86 miss% 0.03173981048268306
plot_id,batch_id 0 87 miss% 0.030615961725576492
plot_id,batch_id 0 88 miss% 0.0410906993440971
plot_id,batch_id 0 89 miss% 0.025887627994375738
plot_id,batch_id 0 90 miss% 0.03585271314434745
plot_id,batch_id 0 91 miss% 0.03555367011109138
plot_id,batch_id 0 92 miss% 0.03860432689084296
plot_id,batch_id 0 93 miss% 0.02888613162196873
plot_id,batch_id 0 94 miss% 0.038999068718955705
plot_id,batch_id 0 95 miss% 0.04261384998422026
plot_id,batch_id 0 96 miss% 0.026598978881680987
plot_id,batch_id 0 97 miss% 0.05493761411860497
plot_id,batch_id 0 98 miss% 0.027234792300194578
plot_id,batch_id 0 99 miss% 0.03856149619286608
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02476305 0.03225442 0.02802298 0.02258623 0.01907884 0.02592977
 0.02955537 0.02911414 0.0279501  0.02862662 0.05005144 0.05438362
 0.02387801 0.03631727 0.04411203 0.04523253 0.03356442 0.03729053
 0.03814947 0.04084778 0.06920599 0.02118445 0.01956439 0.02401841
 0.01616462 0.0380201  0.01950236 0.02160486 0.01932675 0.02588826
 0.0438631  0.03114726 0.02755325 0.02352251 0.02002367 0.04557912
 0.04082993 0.02097446 0.03909006 0.01770929 0.08014203 0.0166672
 0.02186482 0.02388217 0.01614126 0.02633287 0.01175139 0.0211688
 0.02570364 0.02435384 0.04087538 0.01913754 0.01605744 0.00961727
 0.03253595 0.03338527 0.02688509 0.01200128 0.00974646 0.01926397
 0.03842739 0.03185414 0.03725503 0.03732268 0.02602943 0.04551616
 0.03573271 0.02703179 0.03153196 0.04389709 0.05141166 0.04954612
 0.04084342 0.02553423 0.02508077 0.03385102 0.02683216 0.02877845
 0.03390089 0.04288352 0.02954872 0.02298751 0.02993664 0.02842668
 0.03804721 0.04906306 0.03173981 0.03061596 0.0410907  0.02588763
 0.03585271 0.03555367 0.03860433 0.02888613 0.03899907 0.04261385
 0.02659898 0.05493761 0.02723479 0.0385615 ]
for model  116 the mean error 0.0315244235628219
all id 116 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 25 out win 5 err 0.0315244235628219
Launcher: Job 117 completed in 7831 seconds.
Launcher: Task 8 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  77489
Epoch:0, Train loss:0.507052, valid loss:0.511720
Epoch:1, Train loss:0.043219, valid loss:0.012192
Epoch:2, Train loss:0.014670, valid loss:0.004248
Epoch:3, Train loss:0.006923, valid loss:0.003357
Epoch:4, Train loss:0.005241, valid loss:0.002475
Epoch:5, Train loss:0.004365, valid loss:0.002731
Epoch:6, Train loss:0.003755, valid loss:0.002547
Epoch:7, Train loss:0.003273, valid loss:0.002664
Epoch:8, Train loss:0.003078, valid loss:0.001860
Epoch:9, Train loss:0.002841, valid loss:0.001639
Epoch:10, Train loss:0.002813, valid loss:0.001903
Epoch:11, Train loss:0.001983, valid loss:0.001315
Epoch:12, Train loss:0.002003, valid loss:0.001297
Epoch:13, Train loss:0.001919, valid loss:0.001105
Epoch:14, Train loss:0.001860, valid loss:0.001060
Epoch:15, Train loss:0.001809, valid loss:0.001259
Epoch:16, Train loss:0.001809, valid loss:0.001105
Epoch:17, Train loss:0.001735, valid loss:0.001045
Epoch:18, Train loss:0.001696, valid loss:0.001258
Epoch:19, Train loss:0.001676, valid loss:0.001056
Epoch:20, Train loss:0.001614, valid loss:0.000941
Epoch:21, Train loss:0.001295, valid loss:0.000827
Epoch:22, Train loss:0.001291, valid loss:0.000854
Epoch:23, Train loss:0.001275, valid loss:0.000856
Epoch:24, Train loss:0.001239, valid loss:0.000874
Epoch:25, Train loss:0.001232, valid loss:0.000794
Epoch:26, Train loss:0.001239, valid loss:0.000891
Epoch:27, Train loss:0.001221, valid loss:0.000831
Epoch:28, Train loss:0.001240, valid loss:0.000825
Epoch:29, Train loss:0.001209, valid loss:0.000765
Epoch:30, Train loss:0.001161, valid loss:0.000889
Epoch:31, Train loss:0.001007, valid loss:0.000699
Epoch:32, Train loss:0.001010, valid loss:0.000790
Epoch:33, Train loss:0.000996, valid loss:0.000724
Epoch:34, Train loss:0.000994, valid loss:0.000745
Epoch:35, Train loss:0.000990, valid loss:0.000694
Epoch:36, Train loss:0.001001, valid loss:0.000735
Epoch:37, Train loss:0.000969, valid loss:0.000688
Epoch:38, Train loss:0.000979, valid loss:0.000725
Epoch:39, Train loss:0.000964, valid loss:0.000729
Epoch:40, Train loss:0.000961, valid loss:0.000746
Epoch:41, Train loss:0.000880, valid loss:0.000647
Epoch:42, Train loss:0.000877, valid loss:0.000687
Epoch:43, Train loss:0.000874, valid loss:0.000679
Epoch:44, Train loss:0.000871, valid loss:0.000670
Epoch:45, Train loss:0.000868, valid loss:0.000665
Epoch:46, Train loss:0.000864, valid loss:0.000661
Epoch:47, Train loss:0.000860, valid loss:0.000660
Epoch:48, Train loss:0.000856, valid loss:0.000633
Epoch:49, Train loss:0.000858, valid loss:0.000674
Epoch:50, Train loss:0.000854, valid loss:0.000669
Epoch:51, Train loss:0.000815, valid loss:0.000642
Epoch:52, Train loss:0.000814, valid loss:0.000676
Epoch:53, Train loss:0.000822, valid loss:0.000646
Epoch:54, Train loss:0.000812, valid loss:0.000656
Epoch:55, Train loss:0.000812, valid loss:0.000674
Epoch:56, Train loss:0.000807, valid loss:0.000635
Epoch:57, Train loss:0.000804, valid loss:0.000627
Epoch:58, Train loss:0.000805, valid loss:0.000653
Epoch:59, Train loss:0.000803, valid loss:0.000645
Epoch:60, Train loss:0.000802, valid loss:0.000633
training time 7662.7003264427185
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.03668555598012944
plot_id,batch_id 0 1 miss% 0.026219137306118752
plot_id,batch_id 0 2 miss% 0.01902751305802869
plot_id,batch_id 0 3 miss% 0.02225353994118343
plot_id,batch_id 0 4 miss% 0.03172393445187802
plot_id,batch_id 0 5 miss% 0.03131260456662694
plot_id,batch_id 0 6 miss% 0.032288909637272344
plot_id,batch_id 0 7 miss% 0.029811343973126504
plot_id,batch_id 0 8 miss% 0.024047581397989966
plot_id,batch_id 0 9 miss% 0.022161817074535677
plot_id,batch_id 0 10 miss% 0.03482791905715394
plot_id,batch_id 0 11 miss% 0.05885659713749148
plot_id,batch_id 0 12 miss% 0.027643218196080337
plot_id,batch_id 0 13 miss% 0.020006376433791172
plot_id,batch_id 0 14 miss% 0.028261804692393422
plot_id,batch_id 0 15 miss% 0.03850427348389657
plot_id,batch_id 0 16 miss% 0.03550158530779999
plot_id,batch_id 0 17 miss% 0.035935332962844915
plot_id,batch_id 0 18 miss% 0.01932392068872216
plot_id,batch_id 0 19 miss% 0.03412894135512005
plot_id,batch_id 0 20 miss% 0.036619548787245645
plot_id,batch_id 0 21 miss% 0.023337606988496448
plot_id,batch_id 0 22 miss% 0.022765288721607536
plot_id,batch_id 0 23 miss% 0.023745274149117646
plot_id,batch_id 0 24 miss% 0.025933299682947445
plot_id,batch_id 0 25 miss% 0.026580240556028874
plot_id,batch_id 0 26 miss% 0.026897597642820254
plot_id,batch_id 0 27 miss% 0.023896909074236278
plot_id,batch_id 0 28 miss% 0.022320905077462563
plot_id,batch_id 0 29 miss% 0.02004327206958518
plot_id,batch_id 0 30 miss% 0.045578946381145645
plot_id,batch_id 0 31 miss% 0.03348593213945309
plot_id,batch_id 0 32 miss% 0.01934734893260646
plot_id,batch_id 0 33 miss% 0.026384137260105345
plot_id,batch_id 0 34 miss% 0.017109388691235617
plot_id,batch_id 0 35 miss% 0.05315732354764696
plot_id,batch_id 0 36 miss% 0.025132296154754406
plot_id,batch_id 0 37 miss% 0.028987617453093903
plot_id,batch_id 0 38 miss% 0.032508209316037685
plot_id,batch_id 0 39 miss% 0.031212660374412417
plot_id,batch_id 0 40 miss% 0.06596747546295466
plot_id,batch_id 0 41 miss% 0.024818640237364162
plot_id,batch_id 0 42 miss% 0.012019091517935109
plot_id,batch_id 0 43 miss% 0.026863855804626798
plot_id,batch_id 0 44 miss% 0.020593135979272852
plot_id,batch_id 0 45 miss% 0.027744139335438383
plot_id,batch_id 0 46 miss% 0.029628054300188767
plot_id,batch_id 0 47 miss% 0.02677558134081083
plot_id,batch_id 0 48 miss% 0.01871525795090464
plot_id,batch_id 0 49 miss% 0.015887295614402563
plot_id,batch_id 0 50 miss% 0.027412695372019964
plot_id,batch_id 0 51 miss% 0.0257302047338305
plot_id,batch_id 0 52 miss% 0.023908739921794372
plot_id,batch_id 0 53 miss% 0.014373650009716016
plot_id,batch_id 0 54 miss% 0.04236807443058935
plot_id,batch_id 0 55 miss% 0.03839952123911078
plot_id,batch_id 0 56 miss% 0.020149424694763628
plot_id,batch_id 0 57 miss% 0.0251463976336512
plot_id,batch_id 0 58 miss% 0.024185889448597394
plot_id,batch_id 0 59 miss% 0.015549524443964086
plot_id,batch_id 0 60 miss% 0.044318795067696565
plot_id,batch_id 0 61 miss% 0.04488816142209112
plot_id,batch_id 0 62 miss% 0.03451478919649872
plot_id,batch_id 0 63 miss% 0.03706539361214532
plot_id,batch_id 0 64 miss% 0.030400937729854118
plot_id,batch_id 0 65 miss% 0.04805087543937501
plot_id,batch_id 0 66 miss% 0.036522412973286525
plot_id,batch_id 0 67 miss% 0.028680329399147438
plot_id,batch_id 0 68 miss% 0.027595907392773535
plot_id,batch_id 0 69 miss% 0.030542272193968992
plot_id,batch_id 0 70 miss% 0.049706918319359514
plot_id,batch_id 0 71 miss% 0.052775895457767896
plot_id,batch_id 0 72 miss% 0.03834783243109403
plot_id,batch_id 0 73 miss% 0.03130621074498021
plot_id,batch_id 0 74 miss% 0.029183327751551948
plot_id,batch_id 0 75 miss% 0.05029935875153054
plot_id,batch_id 0 76 miss% 0.03152230774268353
plot_id,batch_id 0 77 miss% 0.032953870438337696
plot_id,batch_id 0 78 miss% 0.0407284674833595
plot_id,batch_id 0 79 miss% 0.032646529818575486
plot_id,batch_id 0 80 miss% 0.03415140596584022
plot_id,batch_id 0 81 miss% 0.022618778755504408
plot_id,batch_id 0 82 miss% 0.02111197757287891
plot_id,batch_id 0 83 miss% 0.025350560673477743
plot_id,batch_id 0 84 miss% 0.021962424459042406
plot_id,batch_id 0 85 miss% 0.03489351877502537
plot_id,batch_id 0 86 miss% 0.03188397110451704
plot_id,batch_id 0 87 miss% 0.022159793174089025
plot_id,batch_id 0 88 miss% 0.03617327806343726
plot_id,batch_id 0 89 miss% 0.02252986636399804
plot_id,batch_id 0 90 miss% 0.03395093603830353
plot_id,batch_id 0 91 miss% 0.03944355658865468
plot_id,batch_id 0 92 miss% 0.0262786955537471
plot_id,batch_id 0 93 miss% 0.02737550311518547
plot_id,batch_id 0 94 miss% 0.047652612910560777
plot_id,batch_id 0 95 miss% 0.055916699545347434
plot_id,batch_id 0 96 miss% 0.023666399543618818
plot_id,batch_id 0 97 miss% 0.05155411345529371
plot_id,batch_id 0 98 miss% 0.03938917224667978
plot_id,batch_id 0 99 miss% 0.022369697549364496
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03668556 0.02621914 0.01902751 0.02225354 0.03172393 0.0313126
 0.03228891 0.02981134 0.02404758 0.02216182 0.03482792 0.0588566
 0.02764322 0.02000638 0.0282618  0.03850427 0.03550159 0.03593533
 0.01932392 0.03412894 0.03661955 0.02333761 0.02276529 0.02374527
 0.0259333  0.02658024 0.0268976  0.02389691 0.02232091 0.02004327
 0.04557895 0.03348593 0.01934735 0.02638414 0.01710939 0.05315732
 0.0251323  0.02898762 0.03250821 0.03121266 0.06596748 0.02481864
 0.01201909 0.02686386 0.02059314 0.02774414 0.02962805 0.02677558
 0.01871526 0.0158873  0.0274127  0.0257302  0.02390874 0.01437365
 0.04236807 0.03839952 0.02014942 0.0251464  0.02418589 0.01554952
 0.0443188  0.04488816 0.03451479 0.03706539 0.03040094 0.04805088
 0.03652241 0.02868033 0.02759591 0.03054227 0.04970692 0.0527759
 0.03834783 0.03130621 0.02918333 0.05029936 0.03152231 0.03295387
 0.04072847 0.03264653 0.03415141 0.02261878 0.02111198 0.02535056
 0.02196242 0.03489352 0.03188397 0.02215979 0.03617328 0.02252987
 0.03395094 0.03944356 0.0262787  0.0273755  0.04765261 0.0559167
 0.0236664  0.05155411 0.03938917 0.0223697 ]
for model  102 the mean error 0.03092285913994803
all id 102 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 25 out win 3 err 0.03092285913994803
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  79249
Epoch:0, Train loss:0.590469, valid loss:0.568793
Epoch:1, Train loss:0.049627, valid loss:0.010077
Epoch:2, Train loss:0.015688, valid loss:0.007015
Epoch:3, Train loss:0.012045, valid loss:0.006024
Epoch:4, Train loss:0.010300, valid loss:0.004956
Epoch:5, Train loss:0.008656, valid loss:0.004753
Epoch:6, Train loss:0.007747, valid loss:0.004064
Epoch:7, Train loss:0.006679, valid loss:0.003718
Epoch:8, Train loss:0.006362, valid loss:0.003953
Epoch:9, Train loss:0.005836, valid loss:0.003125
Epoch:10, Train loss:0.005652, valid loss:0.003020
Epoch:11, Train loss:0.004036, valid loss:0.002220
Epoch:12, Train loss:0.003943, valid loss:0.002445
Epoch:13, Train loss:0.003911, valid loss:0.002202
Epoch:14, Train loss:0.003790, valid loss:0.002120
Epoch:15, Train loss:0.003659, valid loss:0.002449
Epoch:16, Train loss:0.003595, valid loss:0.002224
Epoch:17, Train loss:0.003434, valid loss:0.002780
Epoch:18, Train loss:0.003471, valid loss:0.001996
Epoch:19, Train loss:0.003407, valid loss:0.002191
Epoch:20, Train loss:0.003190, valid loss:0.001945
Epoch:21, Train loss:0.002579, valid loss:0.001679
Epoch:22, Train loss:0.002541, valid loss:0.001909
Epoch:23, Train loss:0.002495, valid loss:0.001795
Epoch:24, Train loss:0.002439, valid loss:0.001789
Epoch:25, Train loss:0.002413, valid loss:0.001684
Epoch:26, Train loss:0.002461, valid loss:0.001593
Epoch:27, Train loss:0.002400, valid loss:0.001670
Epoch:28, Train loss:0.002444, valid loss:0.001601
Epoch:29, Train loss:0.002324, valid loss:0.001677
Epoch:30, Train loss:0.002335, valid loss:0.001642
Epoch:31, Train loss:0.001959, valid loss:0.001458
Epoch:32, Train loss:0.002008, valid loss:0.001370
Epoch:33, Train loss:0.001940, valid loss:0.001496
Epoch:34, Train loss:0.001931, valid loss:0.001428
Epoch:35, Train loss:0.001905, valid loss:0.001431
Epoch:36, Train loss:0.001927, valid loss:0.001468
Epoch:37, Train loss:0.001901, valid loss:0.001395
Epoch:38, Train loss:0.001926, valid loss:0.001513
Epoch:39, Train loss:0.001890, valid loss:0.001452
Epoch:40, Train loss:0.001834, valid loss:0.001402
Epoch:41, Train loss:0.001680, valid loss:0.001394
Epoch:42, Train loss:0.001667, valid loss:0.001311
Epoch:43, Train loss:0.001690, valid loss:0.001353
Epoch:44, Train loss:0.001671, valid loss:0.001357
Epoch:45, Train loss:0.001659, valid loss:0.001365
Epoch:46, Train loss:0.001650, valid loss:0.001332
Epoch:47, Train loss:0.001644, valid loss:0.001324
Epoch:48, Train loss:0.001641, valid loss:0.001353
Epoch:49, Train loss:0.001622, valid loss:0.001334
Epoch:50, Train loss:0.001615, valid loss:0.001318
Epoch:51, Train loss:0.001547, valid loss:0.001336
Epoch:52, Train loss:0.001538, valid loss:0.001330
Epoch:53, Train loss:0.001541, valid loss:0.001283
Epoch:54, Train loss:0.001534, valid loss:0.001329
Epoch:55, Train loss:0.001535, valid loss:0.001289
Epoch:56, Train loss:0.001522, valid loss:0.001301
Epoch:57, Train loss:0.001526, valid loss:0.001277
Epoch:58, Train loss:0.001514, valid loss:0.001277
Epoch:59, Train loss:0.001516, valid loss:0.001310
Epoch:60, Train loss:0.001503, valid loss:0.001293
training time 7654.31076669693
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.032532324093816865
plot_id,batch_id 0 1 miss% 0.01920058718967037
plot_id,batch_id 0 2 miss% 0.03388901297322567
plot_id,batch_id 0 3 miss% 0.031009853248667025
plot_id,batch_id 0 4 miss% 0.02680463788872948
plot_id,batch_id 0 5 miss% 0.028530893964261898
plot_id,batch_id 0 6 miss% 0.03244781131593144
plot_id,batch_id 0 7 miss% 0.05563701055714302
plot_id,batch_id 0 8 miss% 0.029683317351836984
plot_id,batch_id 0 9 miss% 0.027965521470197235
plot_id,batch_id 0 10 miss% 0.0379964014923376
plot_id,batch_id 0 11 miss% 0.04048984157503995
plot_id,batch_id 0 12 miss% 0.0462461843265818
plot_id,batch_id 0 13 miss% 0.030859616152591474
plot_id,batch_id 0 14 miss% 0.029901504844786923
plot_id,batch_id 0 15 miss% 0.05762338376045735
plot_id,batch_id 0 16 miss% 0.05137783196467044
plot_id,batch_id 0 17 miss% 0.04056108522259807
plot_id,batch_id 0 18 miss% 0.035782260242403455
plot_id,batch_id 0 19 miss% 0.04842248195452649
plot_id,batch_id 0 20 miss% 0.06407406921279442
plot_id,batch_id 0 21 miss% 0.014215530210198972
plot_id,batch_id 0 22 miss% 0.0335194405681905
plot_id,batch_id 0 23 miss% 0.024556273999633386
plot_id,batch_id 0 24 miss% 0.024630762052425214
plot_id,batch_id 0 25 miss% 0.032086824069976594
plot_id,batch_id 0 26 miss% 0.035483351669853704
plot_id,batch_id 0 27 miss% 0.0296072701277795
plot_id,batch_id 0 28 miss% 0.02136153396247235
plot_id,batch_id 0 29 miss% 0.017447583624796457
plot_id,batch_id 0 30 miss% 0.04932736073249226
plot_id,batch_id 0 31 miss% 0.045538142421935456
plot_id,batch_id 0 32 miss% 0.04730403957867361
plot_id,batch_id 0 33 miss% 0.035438837522237004
plot_id,batch_id 0 34 miss% 0.03352106184624244
plot_id,batch_id 0 35 miss% 0.030757276223411758
plot_id,batch_id 0 36 miss% 0.05203049624545088
plot_id,batch_id 0 37 miss% 0.03695976832897191
plot_id,batch_id 0 38 miss% 0.031486793883729707
plot_id,batch_id 0 39 miss% 0.02008835178029019
plot_id,batch_id 0 40 miss% 0.07505110818158409
plot_id,batch_id 0 41 miss% 0.03392107904049486
plot_id,batch_id 0 42 miss% 0.017092299916367
plot_id,batch_id 0 43 miss% 0.035983119650676465
plot_id,batch_id 0 44 miss% 0.018359553160469427
plot_id,batch_id 0 45 miss% 0.021476093987014607
plot_id,batch_id 0 46 miss% 0.026114854461081846
plot_id,batch_id 0 47 miss% 0.021016696565926037
plot_id,batch_id 0 48 miss% 0.023125615452314684
plot_id,batch_id 0 49 miss% 0.032433550407928585
plot_id,batch_id 0 50 miss% 0.04085294158959804
plot_id,batch_id 0 51 miss% 0.018094909444923098
plot_id,batch_id 0 52 miss% 0.02077027231973555
plot_id,batch_id 0 53 miss% 0.014812429103799122
plot_id,batch_id 0 54 miss% 0.027470258275211226
plot_id,batch_id 0 55 miss% 0.0311555919457784
plot_id,batch_id 0 56 miss% 0.027352001262815305
plot_id,batch_id 0 57 miss% 0.02420884653983953
plot_id,batch_id 0 58 miss% 0.02839087913144312
plot_id,batch_id 0 59 miss% 0.019395800374103263
plot_id,batch_id 0 60 miss% 0.036698096203469886
plot_id,batch_id 0 61 miss% 0.025166883428715814
plot_id,batch_id 0 62 miss% 0.032677217602683724
plot_id,batch_id 0 63 miss% 0.02769240584331566
plot_id,batch_id 0 64 miss% 0.03794572303668842
plot_id,batch_id 0 65 miss% 0.05300549873939491
plot_id,batch_id 0 66 miss% 0.026992567822821053
plot_id,batch_id 0 67 miss% 0.0331952741838119
plot_id,batch_id 0 68 miss% 0.033271693089325566
plot_id,batch_id 0 69 miss% 0.034964199208023475
Launcher: Job 103 completed in 7858 seconds.
Launcher: Task 198 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  77489
Epoch:0, Train loss:0.376916, valid loss:0.379508
Epoch:1, Train loss:0.021937, valid loss:0.003780
Epoch:2, Train loss:0.005205, valid loss:0.002323
Epoch:3, Train loss:0.003629, valid loss:0.002181
Epoch:4, Train loss:0.002949, valid loss:0.002107
Epoch:5, Train loss:0.002591, valid loss:0.001312
Epoch:6, Train loss:0.002420, valid loss:0.001231
Epoch:7, Train loss:0.002350, valid loss:0.001108
Epoch:8, Train loss:0.002225, valid loss:0.001037
Epoch:9, Train loss:0.002160, valid loss:0.001729
Epoch:10, Train loss:0.002007, valid loss:0.001090
Epoch:11, Train loss:0.001411, valid loss:0.000877
Epoch:12, Train loss:0.001364, valid loss:0.000763
Epoch:13, Train loss:0.001412, valid loss:0.000779
Epoch:14, Train loss:0.001337, valid loss:0.000856
Epoch:15, Train loss:0.001342, valid loss:0.000767
Epoch:16, Train loss:0.001263, valid loss:0.000781
Epoch:17, Train loss:0.001278, valid loss:0.000700
Epoch:18, Train loss:0.001243, valid loss:0.000877
Epoch:19, Train loss:0.001227, valid loss:0.000759
Epoch:20, Train loss:0.001206, valid loss:0.000728
Epoch:21, Train loss:0.000911, valid loss:0.000659
Epoch:22, Train loss:0.000899, valid loss:0.000636
Epoch:23, Train loss:0.000887, valid loss:0.000547
Epoch:24, Train loss:0.000902, valid loss:0.000536
Epoch:25, Train loss:0.000899, valid loss:0.000777
Epoch:26, Train loss:0.000852, valid loss:0.000739
Epoch:27, Train loss:0.000878, valid loss:0.000546
Epoch:28, Train loss:0.000848, valid loss:0.000549
Epoch:29, Train loss:0.000842, valid loss:0.000556
Epoch:30, Train loss:0.000839, valid loss:0.000561
Epoch:31, Train loss:0.000694, valid loss:0.000553
Epoch:32, Train loss:0.000688, valid loss:0.000517
Epoch:33, Train loss:0.000687, valid loss:0.000505
Epoch:34, Train loss:0.000683, valid loss:0.000499
Epoch:35, Train loss:0.000678, valid loss:0.000485
Epoch:36, Train loss:0.000692, valid loss:0.000479
Epoch:37, Train loss:0.000668, valid loss:0.000577
Epoch:38, Train loss:0.000659, valid loss:0.000474
Epoch:39, Train loss:0.000673, valid loss:0.000535
Epoch:40, Train loss:0.000668, valid loss:0.000495
Epoch:41, Train loss:0.000594, valid loss:0.000461
Epoch:42, Train loss:0.000586, valid loss:0.000459
Epoch:43, Train loss:0.000588, valid loss:0.000475
Epoch:44, Train loss:0.000589, valid loss:0.000468
Epoch:45, Train loss:0.000581, valid loss:0.000474
Epoch:46, Train loss:0.000585, valid loss:0.000462
Epoch:47, Train loss:0.000578, valid loss:0.000473
Epoch:48, Train loss:0.000585, valid loss:0.000487
Epoch:49, Train loss:0.000571, valid loss:0.000477
Epoch:50, Train loss:0.000575, valid loss:0.000463
Epoch:51, Train loss:0.000544, valid loss:0.000451
Epoch:52, Train loss:0.000541, valid loss:0.000460
Epoch:53, Train loss:0.000541, valid loss:0.000460
Epoch:54, Train loss:0.000538, valid loss:0.000447
Epoch:55, Train loss:0.000537, valid loss:0.000458
Epoch:56, Train loss:0.000538, valid loss:0.000468
Epoch:57, Train loss:0.000538, valid loss:0.000444
Epoch:58, Train loss:0.000533, valid loss:0.000447
Epoch:59, Train loss:0.000533, valid loss:0.000454
Epoch:60, Train loss:0.000532, valid loss:0.000454
training time 7681.813848257065
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.028064083666424273
plot_id,batch_id 0 1 miss% 0.026671847162015552
plot_id,batch_id 0 2 miss% 0.0229322830377266
plot_id,batch_id 0 3 miss% 0.02549339968356442
plot_id,batch_id 0 4 miss% 0.023637637254517094
plot_id,batch_id 0 5 miss% 0.02633838458444754
plot_id,batch_id 0 6 miss% 0.0302529840404784
plot_id,batch_id 0 7 miss% 0.027112062858734944
plot_id,batch_id 0 8 miss% 0.014177650045818344
plot_id,batch_id 0 9 miss% 0.01687880545595175
plot_id,batch_id 0 10 miss% 0.035098722358183905
plot_id,batch_id 0 11 miss% 0.0377143867395854
plot_id,batch_id 0 12 miss% 0.02123446030902968
plot_id,batch_id 0 13 miss% 0.01693204728305136
plot_id,batch_id 0 14 miss% 0.02160239151630924
plot_id,batch_id 0 15 miss% 0.037569678150334726
plot_id,batch_id 0 16 miss% 0.0343850870024886
plot_id,batch_id 0 17 miss% 0.033399342882133795
plot_id,batch_id 0 18 miss% 0.020298287960337575
plot_id,batch_id 0 19 miss% 0.02657347934937607
plot_id,batch_id 0 20 miss% 0.036980971423741044
plot_id,batch_id 0 21 miss% 0.02164436836725319
plot_id,batch_id 0 22 miss% 0.02047843112170085
plot_id,batch_id 0 23 miss% 0.022207758218356213
plot_id,batch_id 0 24 miss% 0.016979274660874025
plot_id,batch_id 0 25 miss% 0.020430353635588618
plot_id,batch_id 0 26 miss% 0.024646458371047755
plot_id,batch_id 0 27 miss% 0.03234610549811016
plot_id,batch_id 0 28 miss% 0.019571631000739036
plot_id,batch_id 0 29 miss% 0.017986291874859124
plot_id,batch_id 0 30 miss% 0.0852421273349106
plot_id,batch_id 0 31 miss% 0.026000115986553777
plot_id,batch_id 0 32 miss% 0.017399675395950623
plot_id,batch_id 0 33 miss% 0.016504082116075323
plot_id,batch_id 0 34 miss% 0.02080138800582306
plot_id,batch_id 0 35 miss% 0.02989917162977803
plot_id,batch_id 0 36 miss% 0.031132786240969445
plot_id,batch_id 0 37 miss% 0.024703521727749547
plot_id,batch_id 0 38 miss% 0.02495320053768626
plot_id,batch_id 0 39 miss% 0.01730282508578419
plot_id,batch_id 0 40 miss% 0.04985344228713236
plot_id,batch_id 0 41 miss% 0.029850903084774262
plot_id,batch_id 0 42 miss% 0.016709048021318673
plot_id,batch_id 0 43 miss% 0.023455557591524765
plot_id,batch_id 0 44 miss% 0.014321168184645752
plot_id,batch_id 0 45 miss% 0.020300828310137138
plot_id,batch_id 0 46 miss% 0.024621647405753774
plot_id,batch_id 0 47 miss% 0.019005230685410816
plot_id,batch_id 0 48 miss% 0.02501153571523043
plot_id,batch_id 0 49 miss% 0.013248931726073436
plot_id,batch_id 0 50 miss% 0.036034258608912385
plot_id,batch_id 0 51 miss% 0.016730356097094834
plot_id,batch_id 0 52 miss% 0.018374504573713674
plot_id,batch_id 0 53 miss% 0.00882009340448895
plot_id,batch_id 0 54 miss% 0.024328952900588577
plot_id,batch_id 0 55 miss% 0.029809189729959325
plot_id,batch_id 0 56 miss% 0.02669897823393353
plot_id,batch_id 0 57 miss% 0.02603885160717386
plot_id,batch_id 0 58 miss% 0.018303398124084307
plot_id,batch_id 0 59 miss% 0.018480229164388812
plot_id,batch_id 0 60 miss% 0.034062116181874366
plot_id,batch_id 0 61 miss% 0.02074680369718412
plot_id,batch_id 0 62 miss% 0.020963378155556465
plot_id,batch_id 0 63 miss% 0.02096868356964298
plot_id,batch_id 0 64 miss% 0.021412864903952267
plot_id,batch_id 0 65 miss% 0.03612136050468565
plot_id,batch_id 0 66 miss% 0.042445319806110174
plot_id,batch_id 0 67 miss% 0.029322611525053815
plot_id,batch_id 0 70 miss% 0.03498759412366718
plot_id,batch_id 0 71 miss% 0.040262673228301474
plot_id,batch_id 0 72 miss% 0.04329378277206405
plot_id,batch_id 0 73 miss% 0.047580500754467014
plot_id,batch_id 0 74 miss% 0.040715290590839226
plot_id,batch_id 0 75 miss% 0.04112678675421075
plot_id,batch_id 0 76 miss% 0.04020749380626494
plot_id,batch_id 0 77 miss% 0.04638616028512439
plot_id,batch_id 0 78 miss% 0.039300582083582426
plot_id,batch_id 0 79 miss% 0.05142964533243007
plot_id,batch_id 0 80 miss% 0.05161166653789866
plot_id,batch_id 0 81 miss% 0.03521322089886173
plot_id,batch_id 0 82 miss% 0.02273368103875829
plot_id,batch_id 0 83 miss% 0.04233263402447648
plot_id,batch_id 0 84 miss% 0.03136456263979665
plot_id,batch_id 0 85 miss% 0.052867884672683566
plot_id,batch_id 0 86 miss% 0.02431229291598227
plot_id,batch_id 0 87 miss% 0.042878502936607936
plot_id,batch_id 0 88 miss% 0.04074741118235193
plot_id,batch_id 0 89 miss% 0.035831821793315816
plot_id,batch_id 0 90 miss% 0.03723411573339057
plot_id,batch_id 0 91 miss% 0.029627536775929674
plot_id,batch_id 0 92 miss% 0.03807642400849441
plot_id,batch_id 0 93 miss% 0.04086306169076359
plot_id,batch_id 0 94 miss% 0.04334110045986012
plot_id,batch_id 0 95 miss% 0.04574506069699028
plot_id,batch_id 0 96 miss% 0.042182139443680995
plot_id,batch_id 0 97 miss% 0.05677458943345703
plot_id,batch_id 0 98 miss% 0.039193518890146224
plot_id,batch_id 0 99 miss% 0.03983358122165698
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03253232 0.01920059 0.03388901 0.03100985 0.02680464 0.02853089
 0.03244781 0.05563701 0.02968332 0.02796552 0.0379964  0.04048984
 0.04624618 0.03085962 0.0299015  0.05762338 0.05137783 0.04056109
 0.03578226 0.04842248 0.06407407 0.01421553 0.03351944 0.02455627
 0.02463076 0.03208682 0.03548335 0.02960727 0.02136153 0.01744758
 0.04932736 0.04553814 0.04730404 0.03543884 0.03352106 0.03075728
 0.0520305  0.03695977 0.03148679 0.02008835 0.07505111 0.03392108
 0.0170923  0.03598312 0.01835955 0.02147609 0.02611485 0.0210167
 0.02312562 0.03243355 0.04085294 0.01809491 0.02077027 0.01481243
 0.02747026 0.03115559 0.027352   0.02420885 0.02839088 0.0193958
 0.0366981  0.02516688 0.03267722 0.02769241 0.03794572 0.0530055
 0.02699257 0.03319527 0.03327169 0.0349642  0.03498759 0.04026267
 0.04329378 0.0475805  0.04071529 0.04112679 0.04020749 0.04638616
 0.03930058 0.05142965 0.05161167 0.03521322 0.02273368 0.04233263
 0.03136456 0.05286788 0.02431229 0.0428785  0.04074741 0.03583182
 0.03723412 0.02962754 0.03807642 0.04086306 0.0433411  0.04574506
 0.04218214 0.05677459 0.03919352 0.03983358]
for model  8 the mean error 0.03533139106346439
all id 8 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 21 out win 5 err 0.03533139106346439
Launcher: Job 9 completed in 7874 seconds.
Launcher: Task 152 done. Exiting.
plot_id,batch_id 0 68 miss% 0.026733616539606438
plot_id,batch_id 0 69 miss% 0.020267216069554096
plot_id,batch_id 0 70 miss% 0.032290564261628546
plot_id,batch_id 0 71 miss% 0.036857763538271505
plot_id,batch_id 0 72 miss% 0.02681128015001202
plot_id,batch_id 0 73 miss% 0.021056661891726867
plot_id,batch_id 0 74 miss% 0.04010249402762721
plot_id,batch_id 0 75 miss% 0.02111519907928662
plot_id,batch_id 0 76 miss% 0.031921810264139275
plot_id,batch_id 0 77 miss% 0.029150259379383273
plot_id,batch_id 0 78 miss% 0.024907338904037474
plot_id,batch_id 0 79 miss% 0.03805107913846991
plot_id,batch_id 0 80 miss% 0.04105942529286671
plot_id,batch_id 0 81 miss% 0.03500877545209586
plot_id,batch_id 0 82 miss% 0.01973709448572756
plot_id,batch_id 0 83 miss% 0.02825236427209656
plot_id,batch_id 0 84 miss% 0.023273348442212113
plot_id,batch_id 0 85 miss% 0.05822660400983303
plot_id,batch_id 0 86 miss% 0.031947706742989246
plot_id,batch_id 0 87 miss% 0.020570898664522275
plot_id,batch_id 0 88 miss% 0.025362194872373477
plot_id,batch_id 0 89 miss% 0.0181881770210683
plot_id,batch_id 0 90 miss% 0.03502908502243251
plot_id,batch_id 0 91 miss% 0.02602474599105216
plot_id,batch_id 0 92 miss% 0.03411505775159114
plot_id,batch_id 0 93 miss% 0.02285186003143679
plot_id,batch_id 0 94 miss% 0.028947636731268593
plot_id,batch_id 0 95 miss% 0.03412481752970687
plot_id,batch_id 0 96 miss% 0.02316126461291335
plot_id,batch_id 0 97 miss% 0.04726829668857148
plot_id,batch_id 0 98 miss% 0.0320402082906768
plot_id,batch_id 0 99 miss% 0.03658910485042971
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02806408 0.02667185 0.02293228 0.0254934  0.02363764 0.02633838
 0.03025298 0.02711206 0.01417765 0.01687881 0.03509872 0.03771439
 0.02123446 0.01693205 0.02160239 0.03756968 0.03438509 0.03339934
 0.02029829 0.02657348 0.03698097 0.02164437 0.02047843 0.02220776
 0.01697927 0.02043035 0.02464646 0.03234611 0.01957163 0.01798629
 0.08524213 0.02600012 0.01739968 0.01650408 0.02080139 0.02989917
 0.03113279 0.02470352 0.0249532  0.01730283 0.04985344 0.0298509
 0.01670905 0.02345556 0.01432117 0.02030083 0.02462165 0.01900523
 0.02501154 0.01324893 0.03603426 0.01673036 0.0183745  0.00882009
 0.02432895 0.02980919 0.02669898 0.02603885 0.0183034  0.01848023
 0.03406212 0.0207468  0.02096338 0.02096868 0.02141286 0.03612136
 0.04244532 0.02932261 0.02673362 0.02026722 0.03229056 0.03685776
 0.02681128 0.02105666 0.04010249 0.0211152  0.03192181 0.02915026
 0.02490734 0.03805108 0.04105943 0.03500878 0.01973709 0.02825236
 0.02327335 0.0582266  0.03194771 0.0205709  0.02536219 0.01818818
 0.03502909 0.02602475 0.03411506 0.02285186 0.02894764 0.03412482
 0.02316126 0.0472683  0.03204021 0.0365891 ]
for model  210 the mean error 0.027106617533740676
all id 210 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 31 out win 3 err 0.027106617533740676
Launcher: Job 211 completed in 7869 seconds.
Launcher: Task 124 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  61841
Epoch:0, Train loss:0.710423, valid loss:0.705107
Epoch:1, Train loss:0.520537, valid loss:0.521595
Epoch:2, Train loss:0.503296, valid loss:0.520196
Epoch:3, Train loss:0.500759, valid loss:0.519883
Epoch:4, Train loss:0.499327, valid loss:0.518354
Epoch:5, Train loss:0.498013, valid loss:0.518490
Epoch:6, Train loss:0.497696, valid loss:0.518224
Epoch:7, Train loss:0.496822, valid loss:0.517672
Epoch:8, Train loss:0.496805, valid loss:0.517861
Epoch:9, Train loss:0.496070, valid loss:0.516949
Epoch:10, Train loss:0.495803, valid loss:0.517759
Epoch:11, Train loss:0.494041, valid loss:0.516709
Epoch:12, Train loss:0.493969, valid loss:0.516272
Epoch:13, Train loss:0.493802, valid loss:0.516278
Epoch:14, Train loss:0.493877, valid loss:0.516903
Epoch:15, Train loss:0.493823, valid loss:0.516252
Epoch:16, Train loss:0.493601, valid loss:0.516158
Epoch:17, Train loss:0.493674, valid loss:0.516396
Epoch:18, Train loss:0.493937, valid loss:0.516559
Epoch:19, Train loss:0.493381, valid loss:0.516394
Epoch:20, Train loss:0.493303, valid loss:0.516396
Epoch:21, Train loss:0.492630, valid loss:0.516347
Epoch:22, Train loss:0.492463, valid loss:0.515693
Epoch:23, Train loss:0.492568, valid loss:0.515846
Epoch:24, Train loss:0.492467, valid loss:0.515592
Epoch:25, Train loss:0.492433, valid loss:0.515820
Epoch:26, Train loss:0.492402, valid loss:0.515909
Epoch:27, Train loss:0.492678, valid loss:0.515717
Epoch:28, Train loss:0.492373, valid loss:0.515863
Epoch:29, Train loss:0.492451, valid loss:0.516004
Epoch:30, Train loss:0.492362, valid loss:0.515941
Epoch:31, Train loss:0.491939, valid loss:0.515495
Epoch:32, Train loss:0.491926, valid loss:0.515566
Epoch:33, Train loss:0.491955, valid loss:0.515428
Epoch:34, Train loss:0.491893, valid loss:0.515537
Epoch:35, Train loss:0.491895, valid loss:0.515389
Epoch:36, Train loss:0.491850, valid loss:0.515552
Epoch:37, Train loss:0.491881, valid loss:0.515475
Epoch:38, Train loss:0.491895, valid loss:0.515365
Epoch:39, Train loss:0.491855, valid loss:0.515345
Epoch:40, Train loss:0.491818, valid loss:0.515380
Epoch:41, Train loss:0.491636, valid loss:0.515305
Epoch:42, Train loss:0.491595, valid loss:0.515333
Epoch:43, Train loss:0.491597, valid loss:0.515370
Epoch:44, Train loss:0.491612, valid loss:0.515344
Epoch:45, Train loss:0.491580, valid loss:0.515276
Epoch:46, Train loss:0.491630, valid loss:0.515483
Epoch:47, Train loss:0.491588, valid loss:0.515309
Epoch:48, Train loss:0.491597, valid loss:0.515389
Epoch:49, Train loss:0.491566, valid loss:0.515325
Epoch:50, Train loss:0.491573, valid loss:0.515345
Epoch:51, Train loss:0.491464, valid loss:0.515289
Epoch:52, Train loss:0.491447, valid loss:0.515295
Epoch:53, Train loss:0.491457, valid loss:0.515326
Epoch:54, Train loss:0.491465, valid loss:0.515279
Epoch:55, Train loss:0.491451, valid loss:0.515297
Epoch:56, Train loss:0.491427, valid loss:0.515261
Epoch:57, Train loss:0.491450, valid loss:0.515252
Epoch:58, Train loss:0.491425, valid loss:0.515266
Epoch:59, Train loss:0.491426, valid loss:0.515289
Epoch:60, Train loss:0.491442, valid loss:0.515270
training time 7701.444449424744
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.8193873232498485
plot_id,batch_id 0 1 miss% 0.8558157281769436
plot_id,batch_id 0 2 miss% 0.8603418506842234
plot_id,batch_id 0 3 miss% 0.8650550014204871
plot_id,batch_id 0 4 miss% 0.8687780271167387
plot_id,batch_id 0 5 miss% 0.8144752154385536
plot_id,batch_id 0 6 miss% 0.8530791835014877
plot_id,batch_id 0 7 miss% 0.8610915743130767
plot_id,batch_id 0 8 miss% 0.8654286163740862
plot_id,batch_id 0 9 miss% 0.8667916761971406
plot_id,batch_id 0 10 miss% 0.8111628072234724
plot_id,batch_id 0 11 miss% 0.8506884600523367
plot_id,batch_id 0 12 miss% 0.8604760946707488
plot_id,batch_id 0 13 miss% 0.8620893174003122
plot_id,batch_id 0 14 miss% 0.8663794359678735
plot_id,batch_id 0 15 miss% 0.8032163420218315
plot_id,batch_id 0 16 miss% 0.8507720315226588
plot_id,batch_id 0 17 miss% 0.8572588514832277
plot_id,batch_id 0 18 miss% 0.8621555151133845
plot_id,batch_id 0 19 miss% 0.8636068528706524
plot_id,batch_id 0 20 miss% 0.8442562181907779
plot_id,batch_id 0 21 miss% 0.8632772049466104
plot_id,batch_id 0 22 miss% 0.8643230690784013
plot_id,batch_id 0 23 miss% 0.8678873333055139
plot_id,batch_id 0 24 miss% 0.8686227217637247
plot_id,batch_id 0 25 miss% 0.8381590470243315
plot_id,batch_id 0 26 miss% 0.8582904160104858
plot_id,batch_id 0 27 miss% 0.8644529611403658
plot_id,batch_id 0 28 miss% 0.8670526577653715
plot_id,batch_id 0 29 miss% 0.8691046632359322
plot_id,batch_id 0 30 miss% 0.8308138532303235
plot_id,batch_id 0 31 miss% 0.8586322634398146
plot_id,batch_id 0 32 miss% 0.8625737073569151
plot_id,batch_id 0 33 miss% 0.8639940579168626
plot_id,batch_id 0 34 miss% 0.8650722881564935
plot_id,batch_id 0 35 miss% 0.8261349538365285
plot_id,batch_id 0 36 miss% 0.8599412232515488
plot_id,batch_id 0 37 miss% 0.8628377892334774
plot_id,batch_id 0 38 miss% 0.8670698925426457
plot_id,batch_id 0 39 miss% 0.8660328321013421
plot_id,batch_id 0 40 miss% 0.8493075217672689
plot_id,batch_id 0 41 miss% 0.8633277023536784
plot_id,batch_id 0 42 miss% 0.86408328493451
plot_id,batch_id 0 43 miss% 0.866298257030283
plot_id,batch_id 0 44 miss% 0.8681304696805061
plot_id,batch_id 0 45 miss% 0.8479277178241597
plot_id,batch_id 0 46 miss% 0.8607951108607926
plot_id,batch_id 0 47 miss% 0.8664720027506906
plot_id,batch_id 0 48 miss% 0.8701680091786917
plot_id,batch_id 0 49 miss% 0.8683716997632815
plot_id,batch_id 0 50 miss% 0.8496508738303502
plot_id,batch_id 0 51 miss% 0.8616642538008314
plot_id,batch_id 0 52 miss% 0.8658373528208732
plot_id,batch_id 0 53 miss% 0.869112803261659
plot_id,batch_id 0 54 miss% 0.869094148528099
plot_id,batch_id 0 55 miss% 0.849665742350832
plot_id,batch_id 0 56 miss% 0.8631673234640064
plot_id,batch_id 0 57 miss% 0.8667416579171472
plot_id,batch_id 0 58 miss% 0.8690721696131398
plot_id,batch_id 0 59 miss% 0.8684807955154159
plot_id,batch_id 0 60 miss% 0.772646894795771
plot_id,batch_id 0 61 miss% 0.8400955257979369
plot_id,batch_id 0 62 miss% 0.855473041895936
plot_id,batch_id 0 63 miss% 0.8576853251999372
plot_id,batch_id 0 64 miss% 0.861561909361109
plot_id,batch_id 0 65 miss% 0.7623215247256405
plot_id,batch_id 0 66 miss% 0.8361561273305144
plot_id,batch_id 0 67 miss% 0.8439969397472195
plot_id,batch_id 0 68 miss% 0.8567370620738565
plot_id,batch_id 0 69 miss% 0.8575635885106289
plot_id,batch_id 0 70 miss% 0.7232366997556963
plot_id,batch_id 0 71 miss% 0.836666307506671
plot_id,batch_id 0 72 miss% 0.8408184895300996
plot_id,batch_id 0 73 miss% 0.8500065203320225
plot_id,batch_id 0 74 miss% 0.8551593573503372
plot_id,batch_id 0 75 miss% 0.7560891810038844
plot_id,batch_id 0 76 miss% 0.82127237342912
plot_id,batch_id 0 77 miss% 0.8345988603077328
plot_id,batch_id 0 78 miss% 0.8421639630232881
plot_id,batch_id 0 79 miss% 0.8478010388577356
plot_id,batch_id 0 80 miss% 0.7870997448107355
plot_id,batch_id 0 81 miss% 0.851754911220678
plot_id,batch_id 0 82 miss% 0.8586057165540478
plot_id,batch_id 0 83 miss% 0.8611370741522358
plot_id,batch_id 0 84 miss% 0.8641300416649197
plot_id,batch_id 0 85 miss% 0.7800130889965016
plot_id,batch_id 0 86 miss% 0.8403020301946358
plot_id,batch_id 0 87 miss% 0.8536135694637329
plot_id,batch_id 0 88 miss% 0.8608056875453446
plot_id,batch_id 0 89 miss% 0.8615994352054235
plot_id,batch_id 0 90 miss% 0.7548250688235622
plot_id,batch_id 0 91 miss% 0.8379058442910692
plot_id,batch_id 0 92 miss% 0.8489618891946552
plot_id,batch_id 0 93 miss% 0.8521194456360361
plot_id,batch_id 0 94 miss% 0.8603272369004523
plot_id,batch_id 0 95 miss% 0.7627278692293612
plot_id,batch_id 0 96 miss% 0.8290251804623222
plot_id,batch_id 0 97 miss% 0.8478849864196322
plot_id,batch_id 0 98 miss% 0.852108925045106
plot_id,batch_id 0 99 miss% 0.8565598811912066
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.81938732 0.85581573 0.86034185 0.865055   0.86877803 0.81447522
 0.85307918 0.86109157 0.86542862 0.86679168 0.81116281 0.85068846
 0.86047609 0.86208932 0.86637944 0.80321634 0.85077203 0.85725885
 0.86215552 0.86360685 0.84425622 0.8632772  0.86432307 0.86788733
 0.86862272 0.83815905 0.85829042 0.86445296 0.86705266 0.86910466
 0.83081385 0.85863226 0.86257371 0.86399406 0.86507229 0.82613495
 0.85994122 0.86283779 0.86706989 0.86603283 0.84930752 0.8633277
 0.86408328 0.86629826 0.86813047 0.84792772 0.86079511 0.866472
 0.87016801 0.8683717  0.84965087 0.86166425 0.86583735 0.8691128
 0.86909415 0.84966574 0.86316732 0.86674166 0.86907217 0.8684808
 0.77264689 0.84009553 0.85547304 0.85768533 0.86156191 0.76232152
 0.83615613 0.84399694 0.85673706 0.85756359 0.7232367  0.83666631
 0.84081849 0.85000652 0.85515936 0.75608918 0.82127237 0.83459886
 0.84216396 0.84780104 0.78709974 0.85175491 0.85860572 0.86113707
 0.86413004 0.78001309 0.84030203 0.85361357 0.86080569 0.86159944
 0.75482507 0.83790584 0.84896189 0.85211945 0.86032724 0.76272787
 0.82902518 0.84788499 0.85210893 0.85655988]
for model  68 the mean error 0.8471750631310955
all id 68 hidden_dim 24 learning_rate 0.02 num_layers 4 frames 21 out win 5 err 0.8471750631310955
Launcher: Job 69 completed in 7881 seconds.
Launcher: Task 225 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  79249
Epoch:0, Train loss:0.430215, valid loss:0.408020
Epoch:1, Train loss:0.033170, valid loss:0.006259
Epoch:2, Train loss:0.010148, valid loss:0.005647
Epoch:3, Train loss:0.007608, valid loss:0.003562
Epoch:4, Train loss:0.006542, valid loss:0.003184
Epoch:5, Train loss:0.005794, valid loss:0.002702
Epoch:6, Train loss:0.005240, valid loss:0.003763
Epoch:7, Train loss:0.005039, valid loss:0.002328
Epoch:8, Train loss:0.004877, valid loss:0.002523
Epoch:9, Train loss:0.005015, valid loss:0.002010
Epoch:10, Train loss:0.004404, valid loss:0.002478
Epoch:11, Train loss:0.002974, valid loss:0.001690
Epoch:12, Train loss:0.002945, valid loss:0.001616
Epoch:13, Train loss:0.003130, valid loss:0.001449
Epoch:14, Train loss:0.002976, valid loss:0.001592
Epoch:15, Train loss:0.002713, valid loss:0.001459
Epoch:16, Train loss:0.002695, valid loss:0.002043
Epoch:17, Train loss:0.002763, valid loss:0.002220
Epoch:18, Train loss:0.002715, valid loss:0.001407
Epoch:19, Train loss:0.002794, valid loss:0.001683
Epoch:20, Train loss:0.002614, valid loss:0.001691
Epoch:21, Train loss:0.001872, valid loss:0.001142
Epoch:22, Train loss:0.001870, valid loss:0.001148
Epoch:23, Train loss:0.001850, valid loss:0.001202
Epoch:24, Train loss:0.001877, valid loss:0.001583
Epoch:25, Train loss:0.001782, valid loss:0.001191
Epoch:26, Train loss:0.001800, valid loss:0.001149
Epoch:27, Train loss:0.001774, valid loss:0.001195
Epoch:28, Train loss:0.001761, valid loss:0.001175
Epoch:29, Train loss:0.001726, valid loss:0.001178
Epoch:30, Train loss:0.001696, valid loss:0.001460
Epoch:31, Train loss:0.001326, valid loss:0.001085
Epoch:32, Train loss:0.001300, valid loss:0.000987
Epoch:33, Train loss:0.001336, valid loss:0.001003
Epoch:34, Train loss:0.001332, valid loss:0.001113
Epoch:35, Train loss:0.001322, valid loss:0.000950
Epoch:36, Train loss:0.001323, valid loss:0.001059
Epoch:37, Train loss:0.001266, valid loss:0.001037
Epoch:38, Train loss:0.001273, valid loss:0.001002
Epoch:39, Train loss:0.001291, valid loss:0.001013
Epoch:40, Train loss:0.001276, valid loss:0.001070
Epoch:41, Train loss:0.001075, valid loss:0.000938
Epoch:42, Train loss:0.001058, valid loss:0.000940
Epoch:43, Train loss:0.001056, valid loss:0.000912
Epoch:44, Train loss:0.001057, valid loss:0.001021
Epoch:45, Train loss:0.001068, valid loss:0.000952
Epoch:46, Train loss:0.001058, valid loss:0.000968
Epoch:47, Train loss:0.001039, valid loss:0.000978
Epoch:48, Train loss:0.001035, valid loss:0.000991
Epoch:49, Train loss:0.001031, valid loss:0.000969
Epoch:50, Train loss:0.001067, valid loss:0.000958
Epoch:51, Train loss:0.000948, valid loss:0.000892
Epoch:52, Train loss:0.000928, valid loss:0.000877
Epoch:53, Train loss:0.000931, valid loss:0.000835
Epoch:54, Train loss:0.000927, valid loss:0.000904
Epoch:55, Train loss:0.000926, valid loss:0.000887
Epoch:56, Train loss:0.000927, valid loss:0.000929
Epoch:57, Train loss:0.000922, valid loss:0.000887
Epoch:58, Train loss:0.000922, valid loss:0.000902
Epoch:59, Train loss:0.000916, valid loss:0.000845
Epoch:60, Train loss:0.000917, valid loss:0.000890
training time 7697.792241573334
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02693483497616764
plot_id,batch_id 0 1 miss% 0.01940255969101166
plot_id,batch_id 0 2 miss% 0.028724576620184853
plot_id,batch_id 0 3 miss% 0.034905149201373845
plot_id,batch_id 0 4 miss% 0.02503309723621858
plot_id,batch_id 0 5 miss% 0.027461871520070253
plot_id,batch_id 0 6 miss% 0.03072278861705336
plot_id,batch_id 0 7 miss% 0.033035881945495664
plot_id,batch_id 0 8 miss% 0.02837778988060028
plot_id,batch_id 0 9 miss% 0.026761978685862935
plot_id,batch_id 0 10 miss% 0.049754387802091495
plot_id,batch_id 0 11 miss% 0.04196659073255515
plot_id,batch_id 0 12 miss% 0.02132577747294774
plot_id,batch_id 0 13 miss% 0.02620313046821966
plot_id,batch_id 0 14 miss% 0.028065799149120687
plot_id,batch_id 0 15 miss% 0.04688339651743379
plot_id,batch_id 0 16 miss% 0.029451342358846874
plot_id,batch_id 0 17 miss% 0.039268151698131575
plot_id,batch_id 0 18 miss% 0.04032889380923895
plot_id,batch_id 0 19 miss% 0.03737143352111292
plot_id,batch_id 0 20 miss% 0.03555404051898972
plot_id,batch_id 0 21 miss% 0.015914813577695808
plot_id,batch_id 0 22 miss% 0.02515158042704168
plot_id,batch_id 0 23 miss% 0.027827881098404308
plot_id,batch_id 0 24 miss% 0.03056585546646279
plot_id,batch_id 0 25 miss% 0.03175510823278204
plot_id,batch_id 0 26 miss% 0.028091913165262637
plot_id,batch_id 0 27 miss% 0.030245096821200607
plot_id,batch_id 0 28 miss% 0.026314359207364694
plot_id,batch_id 0 29 miss% 0.022687962191174152
plot_id,batch_id 0 30 miss% 0.05227289777821991
plot_id,batch_id 0 31 miss% 0.028557164120355392
plot_id,batch_id 0 32 miss% 0.037101429114528406
plot_id,batch_id 0 33 miss% 0.030286789139019814
plot_id,batch_id 0 34 miss% 0.02546379425315174
plot_id,batch_id 0 35 miss% 0.03448832527703596
plot_id,batch_id 0 36 miss% 0.0320766313744283
plot_id,batch_id 0 37 miss% 0.03306906154746644
plot_id,batch_id 0 38 miss% 0.03209045825937139
plot_id,batch_id 0 39 miss% 0.023047258020131366
plot_id,batch_id 0 40 miss% 0.06055824624632908
plot_id,batch_id 0 41 miss% 0.03558931105292183
plot_id,batch_id 0 42 miss% 0.028031599654873018
plot_id,batch_id 0 43 miss% 0.030612154621512965
plot_id,batch_id 0 44 miss% 0.01996114568600192
plot_id,batch_id 0 45 miss% 0.027172484875574734
plot_id,batch_id 0 46 miss% 0.03577397737639362
plot_id,batch_id 0 47 miss% 0.030819054052258425
plot_id,batch_id 0 48 miss% 0.026017093062365156
plot_id,batch_id 0 49 miss% 0.014407988718264396
plot_id,batch_id 0 50 miss% 0.045902132973455516
plot_id,batch_id 0 51 miss% 0.03751562956401975
plot_id,batch_id 0 52 miss% 0.025462579161859365
plot_id,batch_id 0 53 miss% 0.011923378897941803
plot_id,batch_id 0 54 miss% 0.02436100820762645
plot_id,batch_id 0 55 miss% 0.041805815863263464
plot_id,batch_id 0 56 miss% 0.030286944371832943
plot_id,batch_id 0 57 miss% 0.030960888818185484
plot_id,batch_id 0 58 miss% 0.02488934248352466
plot_id,batch_id 0 59 miss% 0.022276593032670565
plot_id,batch_id 0 60 miss% 0.0519517885356699
plot_id,batch_id 0 61 miss% 0.027953182878771604
plot_id,batch_id 0 62 miss% 0.03462692591751354
plot_id,batch_id 0 63 miss% 0.030106687202729174
plot_id,batch_id 0 64 miss% 0.03828565644507166
plot_id,batch_id 0 65 miss% 0.049551953962668364
plot_id,batch_id 0 66 miss% 0.0645907251024023
plot_id,batch_id 0 67 miss% 0.03327878276564229
plot_id,batch_id 0 68 miss% 0.025918621140072394
plot_id,batch_id 0 69 miss% 0.026232775107341025
plot_id,batch_id 0 70 miss% 0.06649759399825396
plot_id,batch_id 0 71 miss% 0.06530441061218352
plot_id,batch_id 0 72 miss% 0.03537528454527296
plot_id,batch_id 0 73 miss% 0.023902028838890782
plot_id,batch_id 0 74 miss% 0.04036094430402139
plot_id,batch_id 0 75 miss% 0.04647255438944774
plot_id,batch_id 0 76 miss% 0.05196362809092762
plot_id,batch_id 0 77 miss% 0.03253175957900233
plot_id,batch_id 0 78 miss% 0.07126440775895514
plot_id,batch_id 0 79 miss% 0.037833077923109595
plot_id,batch_id 0 80 miss% 0.06833362225241175
plot_id,batch_id 0 81 miss% 0.03233927287032334
plot_id,batch_id 0 82 miss% 0.039279454085791664
plot_id,batch_id 0 83 miss% 0.04144800987750808
plot_id,batch_id 0 84 miss% 0.03026512001492764
plot_id,batch_id 0 85 miss% 0.04389390100368095
plot_id,batch_id 0 86 miss% 0.0370084397732481
plot_id,batch_id 0 87 miss% 0.02929715396767749
plot_id,batch_id 0 88 miss% 0.04112416857585312
plot_id,batch_id 0 89 miss% 0.03608699800127193
plot_id,batch_id 0 90 miss% 0.052574659986456274
plot_id,batch_id 0 91 miss% 0.030683346985327504
plot_id,batch_id 0 92 miss% 0.025719453030391602
plot_id,batch_id 0 93 miss% 0.022749587243742812
plot_id,batch_id 0 94 miss% 0.03105962990916221
plot_id,batch_id 0 95 miss% 0.04541198753548621
plot_id,batch_id 0 96 miss% 0.02870375653893438
plot_id,batch_id 0 97 miss% 0.05566618698905652
plot_id,batch_id 0 98 miss% 0.02995434244795102
plot_id,batch_id 0 99 miss% 0.03783245420072292
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02693483 0.01940256 0.02872458 0.03490515 0.0250331  0.02746187
 0.03072279 0.03303588 0.02837779 0.02676198 0.04975439 0.04196659
 0.02132578 0.02620313 0.0280658  0.0468834  0.02945134 0.03926815
 0.04032889 0.03737143 0.03555404 0.01591481 0.02515158 0.02782788
 0.03056586 0.03175511 0.02809191 0.0302451  0.02631436 0.02268796
 0.0522729  0.02855716 0.03710143 0.03028679 0.02546379 0.03448833
 0.03207663 0.03306906 0.03209046 0.02304726 0.06055825 0.03558931
 0.0280316  0.03061215 0.01996115 0.02717248 0.03577398 0.03081905
 0.02601709 0.01440799 0.04590213 0.03751563 0.02546258 0.01192338
 0.02436101 0.04180582 0.03028694 0.03096089 0.02488934 0.02227659
 0.05195179 0.02795318 0.03462693 0.03010669 0.03828566 0.04955195
 0.06459073 0.03327878 0.02591862 0.02623278 0.06649759 0.06530441
 0.03537528 0.02390203 0.04036094 0.04647255 0.05196363 0.03253176
 0.07126441 0.03783308 0.06833362 0.03233927 0.03927945 0.04144801
 0.03026512 0.0438939  0.03700844 0.02929715 0.04112417 0.036087
 0.05257466 0.03068335 0.02571945 0.02274959 0.03105963 0.04541199
 0.02870376 0.05566619 0.02995434 0.03783245]
for model  143 the mean error 0.03462303554602573
all id 143 hidden_dim 32 learning_rate 0.02 num_layers 3 frames 25 out win 5 err 0.03462303554602573
Launcher: Job 144 completed in 7906 seconds.
Launcher: Task 80 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  77489
Epoch:0, Train loss:0.633722, valid loss:0.631607
Epoch:1, Train loss:0.059687, valid loss:0.009713
Epoch:2, Train loss:0.016427, valid loss:0.007342
Epoch:3, Train loss:0.012261, valid loss:0.005445
Epoch:4, Train loss:0.010297, valid loss:0.005264
Epoch:5, Train loss:0.008223, valid loss:0.004041
Epoch:6, Train loss:0.007150, valid loss:0.003491
Epoch:7, Train loss:0.006694, valid loss:0.003119
Epoch:8, Train loss:0.006458, valid loss:0.003093
Epoch:9, Train loss:0.005874, valid loss:0.003704
Epoch:10, Train loss:0.005713, valid loss:0.004973
Epoch:11, Train loss:0.003976, valid loss:0.002053
Epoch:12, Train loss:0.003627, valid loss:0.002132
Epoch:13, Train loss:0.003694, valid loss:0.002296
Epoch:14, Train loss:0.003640, valid loss:0.001828
Epoch:15, Train loss:0.003435, valid loss:0.001856
Epoch:16, Train loss:0.003651, valid loss:0.002109
Epoch:17, Train loss:0.003412, valid loss:0.002873
Epoch:18, Train loss:0.003446, valid loss:0.002013
Epoch:19, Train loss:0.003282, valid loss:0.001866
Epoch:20, Train loss:0.003277, valid loss:0.001936
Epoch:21, Train loss:0.002419, valid loss:0.001627
Epoch:22, Train loss:0.002304, valid loss:0.001435
Epoch:23, Train loss:0.002329, valid loss:0.001598
Epoch:24, Train loss:0.002275, valid loss:0.001493
Epoch:25, Train loss:0.002314, valid loss:0.001508
Epoch:26, Train loss:0.002241, valid loss:0.001331
Epoch:27, Train loss:0.002366, valid loss:0.001442
Epoch:28, Train loss:0.002190, valid loss:0.001429
Epoch:29, Train loss:0.002227, valid loss:0.001465
Epoch:30, Train loss:0.002235, valid loss:0.001384
Epoch:31, Train loss:0.001758, valid loss:0.001205
Epoch:32, Train loss:0.001712, valid loss:0.001186
Epoch:33, Train loss:0.001712, valid loss:0.001442
Epoch:34, Train loss:0.001741, valid loss:0.001296
Epoch:35, Train loss:0.001731, valid loss:0.001327
Epoch:36, Train loss:0.001670, valid loss:0.001210
Epoch:37, Train loss:0.001765, valid loss:0.001177
Epoch:38, Train loss:0.001698, valid loss:0.001190
Epoch:39, Train loss:0.001641, valid loss:0.001304
Epoch:40, Train loss:0.001654, valid loss:0.001171
Epoch:41, Train loss:0.001505, valid loss:0.001123
Epoch:42, Train loss:0.001419, valid loss:0.001091
Epoch:43, Train loss:0.001422, valid loss:0.001086
Epoch:44, Train loss:0.001446, valid loss:0.001124
Epoch:45, Train loss:0.001410, valid loss:0.001082
Epoch:46, Train loss:0.001406, valid loss:0.001130
Epoch:47, Train loss:0.001404, valid loss:0.001073
Epoch:48, Train loss:0.001382, valid loss:0.001118
Epoch:49, Train loss:0.001385, valid loss:0.001098
Epoch:50, Train loss:0.001381, valid loss:0.001128
Epoch:51, Train loss:0.001296, valid loss:0.001068
Epoch:52, Train loss:0.001279, valid loss:0.001094
Epoch:53, Train loss:0.001273, valid loss:0.001049
Epoch:54, Train loss:0.001305, valid loss:0.001102
Epoch:55, Train loss:0.001275, valid loss:0.001042
Epoch:56, Train loss:0.001260, valid loss:0.001045
Epoch:57, Train loss:0.001254, valid loss:0.001080
Epoch:58, Train loss:0.001262, valid loss:0.001090
Epoch:59, Train loss:0.001255, valid loss:0.001052
Epoch:60, Train loss:0.001278, valid loss:0.001126
training time 7708.628987789154
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.04930633383484069
plot_id,batch_id 0 1 miss% 0.027676131371122695
plot_id,batch_id 0 2 miss% 0.03247852723357562
plot_id,batch_id 0 3 miss% 0.027001260836654856
plot_id,batch_id 0 4 miss% 0.026790978483055512
plot_id,batch_id 0 5 miss% 0.0460546086914333
plot_id,batch_id 0 6 miss% 0.03541386100904455
plot_id,batch_id 0 7 miss% 0.03865302740559296
plot_id,batch_id 0 8 miss% 0.023897973544020148
plot_id,batch_id 0 9 miss% 0.026074988286835726
plot_id,batch_id 0 10 miss% 0.0358183488151966
plot_id,batch_id 0 11 miss% 0.04817379512581065
plot_id,batch_id 0 12 miss% 0.030780083751465775
plot_id,batch_id 0 13 miss% 0.02559222875550809
plot_id,batch_id 0 14 miss% 0.03735021758898264
plot_id,batch_id 0 15 miss% 0.04336183531155072
plot_id,batch_id 0 16 miss% 0.026081689195898053
plot_id,batch_id 0 17 miss% 0.04647363104005892
plot_id,batch_id 0 18 miss% 0.05464970052865994
plot_id,batch_id 0 19 miss% 0.037402453713628506
plot_id,batch_id 0 20 miss% 0.055118039031869244
plot_id,batch_id 0 21 miss% 0.02995902010619904
plot_id,batch_id 0 22 miss% 0.030356252544583455
plot_id,batch_id 0 23 miss% 0.030103128181882766
plot_id,batch_id 0 24 miss% 0.026768281349087538
plot_id,batch_id 0 25 miss% 0.049689022934365466
plot_id,batch_id 0 26 miss% 0.04510131338943025
plot_id,batch_id 0 27 miss% 0.028358123890487096
plot_id,batch_id 0 28 miss% 0.024884330164680127
plot_id,batch_id 0 29 miss% 0.029272053148217635
plot_id,batch_id 0 30 miss% 0.05134468552221012
plot_id,batch_id 0 31 miss% 0.033701373695300026
plot_id,batch_id 0 32 miss% 0.03739542951551665
plot_id,batch_id 0 33 miss% 0.03408581455196815
plot_id,batch_id 0 34 miss% 0.027353465169864546
plot_id,batch_id 0 35 miss% 0.049089279894989284
plot_id,batch_id 0 36 miss% 0.052983922924238654
plot_id,batch_id 0 37 miss% 0.02786112723494452
plot_id,batch_id 0 38 miss% 0.027033310619529016
plot_id,batch_id 0 39 miss% 0.019475889921836576
plot_id,batch_id 0 40 miss% 0.0615781498176328
plot_id,batch_id 0 41 miss% 0.025453399624978872
plot_id,batch_id 0 42 miss% 0.013044787895609327
plot_id,batch_id 0 43 miss% 0.03659386545740438
plot_id,batch_id 0 44 miss% 0.01871307758538985
plot_id,batch_id 0 45 miss% 0.013037083776271977
plot_id,batch_id 0 46 miss% 0.028873013022030845
plot_id,batch_id 0 47 miss% 0.026530767361993158
plot_id,batch_id 0 48 miss% 0.026558954645771116
plot_id,batch_id 0 49 miss% 0.02834066743872609
plot_id,batch_id 0 50 miss% 0.032096841917071475
plot_id,batch_id 0 51 miss% 0.026658291206829585
plot_id,batch_id 0 52 miss% 0.030362152857773272
plot_id,batch_id 0 53 miss% 0.021086815455520755
plot_id,batch_id 0 54 miss% 0.03187584746620318
plot_id,batch_id 0 55 miss% 0.028310047386986562
plot_id,batch_id 0 56 miss% 0.02759835942401444
plot_id,batch_id 0 57 miss% 0.02513787431609746
plot_id,batch_id 0 58 miss% 0.024296805645967468
plot_id,batch_id 0 59 miss% 0.026369084248604388
plot_id,batch_id 0 60 miss% 0.032305811987706885
plot_id,batch_id 0 61 miss% 0.03377540255683582
plot_id,batch_id 0 62 miss% 0.03694195366744782
plot_id,batch_id 0 63 miss% 0.04556018425359428
plot_id,batch_id 0 64 miss% 0.04397682647815208
plot_id,batch_id 0 65 miss% 0.02941365242876447
plot_id,batch_id 0 66 miss% 0.0549165188848401
plot_id,batch_id 0 67 miss% 0.02669268753308919
plot_id,batch_id 0 68 miss% 0.02644096246668419
plot_id,batch_id 0 69 miss% 0.021292831978669272
plot_id,batch_id 0 70 miss% 0.03995019315634456
plot_id,batch_id 0 71 miss% 0.032252031428449277
plot_id,batch_id 0 72 miss% 0.042545996768441285
plot_id,batch_id 0 73 miss% 0.03566931638803654
plot_id,batch_id 0 74 miss% 0.044816978965671474
plot_id,batch_id 0 75 miss% 0.04276487750512294
plot_id,batch_id 0 76 miss% 0.0402906480637055
plot_id,batch_id 0 77 miss% 0.03826109574966361
plot_id,batch_id 0 78 miss% 0.034267753378455466
plot_id,batch_id 0 79 miss% 0.03649344548637715
plot_id,batch_id 0 80 miss% 0.061900237602106285
plot_id,batch_id 0 81 miss% 0.034454852099466735
plot_id,batch_id 0 82 miss% 0.0338927703510256
plot_id,batch_id 0 83 miss% 0.03372938722104457
plot_id,batch_id 0 84 miss% 0.028345120579887844
plot_id,batch_id 0 85 miss% 0.04419184168735681
plot_id,batch_id 0 86 miss% 0.048177742259361146
plot_id,batch_id 0 87 miss% 0.03719277440687487
plot_id,batch_id 0 88 miss% 0.03587203484302762
plot_id,batch_id 0 89 miss% 0.034288238714364414
plot_id,batch_id 0 90 miss% 0.03880798474301214
plot_id,batch_id 0 91 miss% 0.058787847629182134
plot_id,batch_id 0 92 miss% 0.031788060604260066
plot_id,batch_id 0 93 miss% 0.026631176685811407
plot_id,batch_id 0 94 miss% 0.05371377539566857
plot_id,batch_id 0 95 miss% 0.02920114669926885
plot_id,batch_id 0 96 miss% 0.03934009686571687
plot_id,batch_id 0 97 miss% 0.04212262800624251
plot_id,batch_id 0 98 miss% 0.03924886040381841
plot_id,batch_id 0 99 miss% 0.030360693249433045
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04930633 0.02767613 0.03247853 0.02700126 0.02679098 0.04605461
 0.03541386 0.03865303 0.02389797 0.02607499 0.03581835 0.0481738
 0.03078008 0.02559223 0.03735022 0.04336184 0.02608169 0.04647363
 0.0546497  0.03740245 0.05511804 0.02995902 0.03035625 0.03010313
 0.02676828 0.04968902 0.04510131 0.02835812 0.02488433 0.02927205
 0.05134469 0.03370137 0.03739543 0.03408581 0.02735347 0.04908928
 0.05298392 0.02786113 0.02703331 0.01947589 0.06157815 0.0254534
 0.01304479 0.03659387 0.01871308 0.01303708 0.02887301 0.02653077
 0.02655895 0.02834067 0.03209684 0.02665829 0.03036215 0.02108682
 0.03187585 0.02831005 0.02759836 0.02513787 0.02429681 0.02636908
 0.03230581 0.0337754  0.03694195 0.04556018 0.04397683 0.02941365
 0.05491652 0.02669269 0.02644096 0.02129283 0.03995019 0.03225203
 0.042546   0.03566932 0.04481698 0.04276488 0.04029065 0.0382611
 0.03426775 0.03649345 0.06190024 0.03445485 0.03389277 0.03372939
 0.02834512 0.04419184 0.04817774 0.03719277 0.03587203 0.03428824
 0.03880798 0.05878785 0.03178806 0.02663118 0.05371378 0.02920115
 0.0393401  0.04212263 0.03924886 0.03036069]
for model  50 the mean error 0.035021578640379955
all id 50 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 21 out win 5 err 0.035021578640379955
Launcher: Job 51 completed in 7924 seconds.
Launcher: Task 119 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  46193
Epoch:0, Train loss:0.374471, valid loss:0.341930
Epoch:1, Train loss:0.230674, valid loss:0.231527
Epoch:2, Train loss:0.222495, valid loss:0.230731
Epoch:3, Train loss:0.221199, valid loss:0.230417
Epoch:4, Train loss:0.220681, valid loss:0.230290
Epoch:5, Train loss:0.220316, valid loss:0.230820
Epoch:6, Train loss:0.220162, valid loss:0.229943
Epoch:7, Train loss:0.219980, valid loss:0.229757
Epoch:8, Train loss:0.219853, valid loss:0.229815
Epoch:9, Train loss:0.219676, valid loss:0.230147
Epoch:10, Train loss:0.219588, valid loss:0.229985
Epoch:11, Train loss:0.218679, valid loss:0.229499
Epoch:12, Train loss:0.218679, valid loss:0.229405
Epoch:13, Train loss:0.218645, valid loss:0.229225
Epoch:14, Train loss:0.218609, valid loss:0.229338
Epoch:15, Train loss:0.218593, valid loss:0.229519
Epoch:16, Train loss:0.218552, valid loss:0.229384
Epoch:17, Train loss:0.218489, valid loss:0.229549
Epoch:18, Train loss:0.218491, valid loss:0.229474
Epoch:19, Train loss:0.218480, valid loss:0.229477
Epoch:20, Train loss:0.218520, valid loss:0.229128
Epoch:21, Train loss:0.218025, valid loss:0.229131
Epoch:22, Train loss:0.218026, valid loss:0.229160
Epoch:23, Train loss:0.218019, valid loss:0.229020
Epoch:24, Train loss:0.217997, valid loss:0.229002
Epoch:25, Train loss:0.217993, valid loss:0.229081
Epoch:26, Train loss:0.218000, valid loss:0.228960
Epoch:27, Train loss:0.217968, valid loss:0.229177
Epoch:28, Train loss:0.217977, valid loss:0.228969
Epoch:29, Train loss:0.217947, valid loss:0.229064
Epoch:30, Train loss:0.217954, valid loss:0.228976
Epoch:31, Train loss:0.217719, valid loss:0.228974
Epoch:32, Train loss:0.217708, valid loss:0.228900
Epoch:33, Train loss:0.217723, valid loss:0.228930
Epoch:34, Train loss:0.217689, valid loss:0.228924
Epoch:35, Train loss:0.217701, valid loss:0.228878
Epoch:36, Train loss:0.217708, valid loss:0.228903
Epoch:37, Train loss:0.217684, valid loss:0.228953
Epoch:38, Train loss:0.217685, valid loss:0.229110
Epoch:39, Train loss:0.217670, valid loss:0.228877
Epoch:40, Train loss:0.217679, valid loss:0.228977
Epoch:41, Train loss:0.217544, valid loss:0.228828
Epoch:42, Train loss:0.217548, valid loss:0.228815
Epoch:43, Train loss:0.217546, valid loss:0.228884
Epoch:44, Train loss:0.217543, valid loss:0.228892
Epoch:45, Train loss:0.217531, valid loss:0.228882
Epoch:46, Train loss:0.217540, valid loss:0.228868
Epoch:47, Train loss:0.217529, valid loss:0.228895
Epoch:48, Train loss:0.217529, valid loss:0.228863
Epoch:49, Train loss:0.217538, valid loss:0.228879
Epoch:50, Train loss:0.217523, valid loss:0.228851
Epoch:51, Train loss:0.217454, valid loss:0.228834
Epoch:52, Train loss:0.217456, valid loss:0.228859
Epoch:53, Train loss:0.217452, valid loss:0.228814
Epoch:54, Train loss:0.217451, valid loss:0.228822
Epoch:55, Train loss:0.217451, valid loss:0.228836
Epoch:56, Train loss:0.217451, valid loss:0.228848
Epoch:57, Train loss:0.217443, valid loss:0.228817
Epoch:58, Train loss:0.217444, valid loss:0.228848
Epoch:59, Train loss:0.217445, valid loss:0.228856
Epoch:60, Train loss:0.217444, valid loss:0.228912
training time 7751.966219186783
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.7278281191498912
plot_id,batch_id 0 1 miss% 0.792145266304998
plot_id,batch_id 0 2 miss% 0.8017227972512354
plot_id,batch_id 0 3 miss% 0.8081011249468761
plot_id,batch_id 0 4 miss% 0.8072307091068562
plot_id,batch_id 0 5 miss% 0.7268297422437816
plot_id,batch_id 0 6 miss% 0.7869017160262294
plot_id,batch_id 0 7 miss% 0.7982581994933186
plot_id,batch_id 0 8 miss% 0.8051657686591529
plot_id,batch_id 0 9 miss% 0.8088333380779287
plot_id,batch_id 0 10 miss% 0.6999281231842728
plot_id,batch_id 0 11 miss% 0.7844648468383314
plot_id,batch_id 0 12 miss% 0.7946342457914604
plot_id,batch_id 0 13 miss% 0.8041887627225144
plot_id,batch_id 0 14 miss% 0.8091116348822581
plot_id,batch_id 0 15 miss% 0.7164599066376307
plot_id,batch_id 0 16 miss% 0.776017422374039
plot_id,batch_id 0 17 miss% 0.7966779554270649
plot_id,batch_id 0 18 miss% 0.8024502787646669
plot_id,batch_id 0 19 miss% 0.8054975699590102
plot_id,batch_id 0 20 miss% 0.7642894669161497
plot_id,batch_id 0 21 miss% 0.8030235453195854
plot_id,batch_id 0 22 miss% 0.809777971486204
plot_id,batch_id 0 23 miss% 0.8139007339765135
plot_id,batch_id 0 24 miss% 0.8150790045829835
plot_id,batch_id 0 25 miss% 0.7548545590959962
plot_id,batch_id 0 26 miss% 0.7966599207650593
plot_id,batch_id 0 27 miss% 0.8030464840912369
plot_id,batch_id 0 28 miss% 0.8074743624637237
plot_id,batch_id 0 29 miss% 0.8089642090356158
plot_id,batch_id 0 30 miss% 0.7543080236608521
plot_id,batch_id 0 31 miss% 0.7943423143532071
plot_id,batch_id 0 32 miss% 0.8054073960749214
plot_id,batch_id 0 33 miss% 0.8072798232268532
plot_id,batch_id 0 34 miss% 0.8076255719583935
plot_id,batch_id 0 35 miss% 0.7554947595947419
plot_id,batch_id 0 36 miss% 0.8018578104759855
plot_id,batch_id 0 37 miss% 0.8022409215593616
plot_id,batch_id 0 38 miss% 0.8098521059201883
plot_id,batch_id 0 39 miss% 0.8098354442402443
plot_id,batch_id 0 40 miss% 0.7828910162957456
plot_id,batch_id 0 41 miss% 0.8058491973513194
plot_id,batch_id 0 42 miss% 0.8125200022985603
plot_id,batch_id 0 43 miss% 0.8141847515368125
plot_id,batch_id 0 44 miss% 0.8201253078631334
plot_id,batch_id 0 45 miss% 0.7771472836159727
plot_id,batch_id 0 46 miss% 0.80571835920444
plot_id,batch_id 0 47 miss% 0.8133743564079564
plot_id,batch_id 0 48 miss% 0.8133559770039072
plot_id,batch_id 0 49 miss% 0.8180807466229724
plot_id,batch_id 0 50 miss% 0.7833608263653653
plot_id,batch_id 0 51 miss% 0.8020601574341834
plot_id,batch_id 0 52 miss% 0.8081993094994502
plot_id,batch_id 0 53 miss% 0.811683389374158
plot_id,batch_id 0 54 miss% 0.8185317752295117
plot_id,batch_id 0 55 miss% 0.7691232076814556
plot_id,batch_id 0 56 miss% 0.8023046637338008
plot_id,batch_id 0 57 miss% 0.8081229833656939
plot_id,batch_id 0 58 miss% 0.8138679584191916
plot_id,batch_id 0 59 miss% 0.8173303169197714
plot_id,batch_id 0 60 miss% 0.6496635418034687
plot_id,batch_id 0 61 miss% 0.7509492934964421
plot_id,batch_id 0 62 miss% 0.7843165966144326
plot_id,batch_id 0 63 miss% 0.7954260881278945
plot_id,batch_id 0 64 miss% 0.8022124718742543
plot_id,batch_id 0 65 miss% 0.6387652700475396
plot_id,batch_id 0 66 miss% 0.7510845890903945
plot_id,batch_id 0 67 miss% 0.7706421452349959
plot_id,batch_id 0 68 miss% 0.7935025637476887
plot_id,batch_id 0 69 miss% 0.7948320030753221
plot_id,batch_id 0 70 miss% 0.61082462325799
plot_id,batch_id 0 71 miss% 0.7676777739000185
plot_id,batch_id 0 72 miss% 0.7615269120709022
plot_id,batch_id 0 73 miss% 0.7770463041766139
plot_id,batch_id 0 74 miss% 0.7886321420350514
plot_id,batch_id 0 75 miss% 0.6146627216320388
plot_id,batch_id 0 76 miss% 0.7129783323650812
plot_id,batch_id 0 77 miss% 0.7509460220728312
plot_id,batch_id 0 78 miss% 0.7790443901836657
plot_id,batch_id 0 79 miss% 0.7889158458495823
plot_id,batch_id 0 80 miss% 0.6715313553821084
plot_id,batch_id 0 81 miss% 0.7760080862300295
plot_id,batch_id 0 82 miss% 0.7904758919072362
plot_id,batch_id 0 83 miss% 0.8021689280517781
plot_id,batch_id 0 84 miss% 0.8019431920386082
plot_id,batch_id 0 85 miss% 0.6669749145097918
plot_id,batch_id 0 86 miss% 0.7689691296994351
plot_id,batch_id 0 87 miss% 0.7905310158712925
plot_id,batch_id 0 88 miss% 0.796676055370644
plot_id,batch_id 0 89 miss% 0.7993310210684663
plot_id,batch_id 0 90 miss% 0.6374794244275622
plot_id,batch_id 0 91 miss% 0.7675251499070957
plot_id,batch_id 0 92 miss% 0.7768961760762711
plot_id,batch_id 0 93 miss% 0.7897188431186102
plot_id,batch_id 0 94 miss% 0.7996115861953084
plot_id,batch_id 0 95 miss% 0.6453617618457768
plot_id,batch_id 0 96 miss% 0.7524920403668398
plot_id,batch_id 0 97 miss% 0.776251748483598
plot_id,batch_id 0 98 miss% 0.7844863371113965
plot_id,batch_id 0 99 miss% 0.7940136854560627
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.72782812 0.79214527 0.8017228  0.80810112 0.80723071 0.72682974
 0.78690172 0.7982582  0.80516577 0.80883334 0.69992812 0.78446485
 0.79463425 0.80418876 0.80911163 0.71645991 0.77601742 0.79667796
 0.80245028 0.80549757 0.76428947 0.80302355 0.80977797 0.81390073
 0.815079   0.75485456 0.79665992 0.80304648 0.80747436 0.80896421
 0.75430802 0.79434231 0.8054074  0.80727982 0.80762557 0.75549476
 0.80185781 0.80224092 0.80985211 0.80983544 0.78289102 0.8058492
 0.81252    0.81418475 0.82012531 0.77714728 0.80571836 0.81337436
 0.81335598 0.81808075 0.78336083 0.80206016 0.80819931 0.81168339
 0.81853178 0.76912321 0.80230466 0.80812298 0.81386796 0.81733032
 0.64966354 0.75094929 0.7843166  0.79542609 0.80221247 0.63876527
 0.75108459 0.77064215 0.79350256 0.794832   0.61082462 0.76767777
 0.76152691 0.7770463  0.78863214 0.61466272 0.71297833 0.75094602
 0.77904439 0.78891585 0.67153136 0.77600809 0.79047589 0.80216893
 0.80194319 0.66697491 0.76896913 0.79053102 0.79667606 0.79933102
 0.63747942 0.76752515 0.77689618 0.78971884 0.79961159 0.64536176
 0.75249204 0.77625175 0.78448634 0.79401369]
for model  221 the mean error 0.7774768951663686
all id 221 hidden_dim 24 learning_rate 0.02 num_layers 3 frames 31 out win 5 err 0.7774768951663686
Launcher: Job 222 completed in 7926 seconds.
Launcher: Task 77 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  77489
Epoch:0, Train loss:0.648619, valid loss:0.649701
Epoch:1, Train loss:0.067164, valid loss:0.019405
Epoch:2, Train loss:0.027905, valid loss:0.007767
Epoch:3, Train loss:0.013181, valid loss:0.006117
Epoch:4, Train loss:0.010241, valid loss:0.004519
Epoch:5, Train loss:0.008309, valid loss:0.004192
Epoch:6, Train loss:0.007211, valid loss:0.003463
Epoch:7, Train loss:0.006096, valid loss:0.003140
Epoch:8, Train loss:0.005515, valid loss:0.003668
Epoch:9, Train loss:0.005232, valid loss:0.003072
Epoch:10, Train loss:0.004905, valid loss:0.002874
Epoch:11, Train loss:0.003459, valid loss:0.001854
Epoch:12, Train loss:0.003349, valid loss:0.001812
Epoch:13, Train loss:0.003357, valid loss:0.002091
Epoch:14, Train loss:0.003281, valid loss:0.001867
Epoch:15, Train loss:0.003204, valid loss:0.002528
Epoch:16, Train loss:0.003263, valid loss:0.001981
Epoch:17, Train loss:0.003005, valid loss:0.001759
Epoch:18, Train loss:0.002861, valid loss:0.001924
Epoch:19, Train loss:0.002950, valid loss:0.001570
Epoch:20, Train loss:0.002831, valid loss:0.001498
Epoch:21, Train loss:0.002183, valid loss:0.001443
Epoch:22, Train loss:0.002156, valid loss:0.001464
Epoch:23, Train loss:0.002195, valid loss:0.001344
Epoch:24, Train loss:0.002134, valid loss:0.001676
Epoch:25, Train loss:0.002117, valid loss:0.001359
Epoch:26, Train loss:0.002046, valid loss:0.001590
Epoch:27, Train loss:0.002008, valid loss:0.001362
Epoch:28, Train loss:0.002057, valid loss:0.001401
Epoch:29, Train loss:0.001977, valid loss:0.002232
Epoch:30, Train loss:0.001964, valid loss:0.001460
Epoch:31, Train loss:0.001630, valid loss:0.001209
Epoch:32, Train loss:0.001609, valid loss:0.001207
Epoch:33, Train loss:0.001623, valid loss:0.001244
Epoch:34, Train loss:0.001612, valid loss:0.001130
Epoch:35, Train loss:0.001583, valid loss:0.001131
Epoch:36, Train loss:0.001593, valid loss:0.001155
Epoch:37, Train loss:0.001618, valid loss:0.001138
Epoch:38, Train loss:0.001533, valid loss:0.001162
Epoch:39, Train loss:0.001497, valid loss:0.001072
Epoch:40, Train loss:0.001578, valid loss:0.001262
Epoch:41, Train loss:0.001389, valid loss:0.001070
Epoch:42, Train loss:0.001374, valid loss:0.001108
Epoch:43, Train loss:0.001365, valid loss:0.001111
Epoch:44, Train loss:0.001350, valid loss:0.001062
Epoch:45, Train loss:0.001354, valid loss:0.001079
Epoch:46, Train loss:0.001362, valid loss:0.001084
Epoch:47, Train loss:0.001345, valid loss:0.001106
Epoch:48, Train loss:0.001325, valid loss:0.001084
Epoch:49, Train loss:0.001334, valid loss:0.001061
Epoch:50, Train loss:0.001321, valid loss:0.001053
Epoch:51, Train loss:0.001260, valid loss:0.001029
Epoch:52, Train loss:0.001245, valid loss:0.001036
Epoch:53, Train loss:0.001245, valid loss:0.001008
Epoch:54, Train loss:0.001238, valid loss:0.001026
Epoch:55, Train loss:0.001241, valid loss:0.001056
Epoch:56, Train loss:0.001231, valid loss:0.001030
Epoch:57, Train loss:0.001233, valid loss:0.001055
Epoch:58, Train loss:0.001228, valid loss:0.001022
Epoch:59, Train loss:0.001221, valid loss:0.001024
Epoch:60, Train loss:0.001234, valid loss:0.001021
training time 7783.326704025269
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.025085400815709372
plot_id,batch_id 0 1 miss% 0.025468732029647837
plot_id,batch_id 0 2 miss% 0.019281650814262825
plot_id,batch_id 0 3 miss% 0.02511982432100243
plot_id,batch_id 0 4 miss% 0.02005408742843643
plot_id,batch_id 0 5 miss% 0.031070013151434955
plot_id,batch_id 0 6 miss% 0.022474221566016726
plot_id,batch_id 0 7 miss% 0.029186037317282587
plot_id,batch_id 0 8 miss% 0.02630617575566221
plot_id,batch_id 0 9 miss% 0.01768524375328159
plot_id,batch_id 0 10 miss% 0.03413349021780293
plot_id,batch_id 0 11 miss% 0.051553581424862584
plot_id,batch_id 0 12 miss% 0.02313994053057304
plot_id,batch_id 0 13 miss% 0.03791391746429623
plot_id,batch_id 0 14 miss% 0.03525506959752339
plot_id,batch_id 0 15 miss% 0.03765061616973391
plot_id,batch_id 0 16 miss% 0.03882162625154132
plot_id,batch_id 0 17 miss% 0.04943360802382683
plot_id,batch_id 0 18 miss% 0.0361390133515288
plot_id,batch_id 0 19 miss% 0.03246528299183889
plot_id,batch_id 0 20 miss% 0.08756655548148083
plot_id,batch_id 0 21 miss% 0.019972074903285682
plot_id,batch_id 0 22 miss% 0.034219412408010325
plot_id,batch_id 0 23 miss% 0.037858669906427146
plot_id,batch_id 0 24 miss% 0.02954562107089325
plot_id,batch_id 0 25 miss% 0.035542276541716794
plot_id,batch_id 0 26 miss% 0.025154784296337267
plot_id,batch_id 0 27 miss% 0.023743038710297187
plot_id,batch_id 0 28 miss% 0.03320373182431192
plot_id,batch_id 0 29 miss% 0.027966760036432798
plot_id,batch_id 0 30 miss% 0.03798088136200698
plot_id,batch_id 0 31 miss% 0.029042594935224878
plot_id,batch_id 0 32 miss% 0.027912709077333776
plot_id,batch_id 0 33 miss% 0.03351400245434475
plot_id,batch_id 0 34 miss% 0.029652594921442944
plot_id,batch_id 0 35 miss% 0.04250401114237663
plot_id,batch_id 0 36 miss% 0.0429638539069547
plot_id,batch_id 0 37 miss% 0.029929398578392007
plot_id,batch_id 0 38 miss% 0.031525237542645475
plot_id,batch_id 0 39 miss% 0.03233115102076453
plot_id,batch_id 0 40 miss% 0.06554587317878582
plot_id,batch_id 0 41 miss% 0.024811567591505688
plot_id,batch_id 0 42 miss% 0.022243269474810416
plot_id,batch_id 0 43 miss% 0.023666639945992454
plot_id,batch_id 0 44 miss% 0.025876356232993447
plot_id,batch_id 0 45 miss% 0.015908309266396504
plot_id,batch_id 0 46 miss% 0.029138221223561758
plot_id,batch_id 0 47 miss% 0.019517997630438256
plot_id,batch_id 0 48 miss% 0.02603426041440144
plot_id,batch_id 0 49 miss% 0.014537211008312269
plot_id,batch_id 0 50 miss% 0.031228504218502602
plot_id,batch_id 0 51 miss% 0.028960134389292867
plot_id,batch_id 0 52 miss% 0.031041825545349847
plot_id,batch_id 0 53 miss% 0.01218130123169993
plot_id,batch_id 0 54 miss% 0.02326339146170825
plot_id,batch_id 0 55 miss% 0.030420954438631798
plot_id,batch_id 0 56 miss% 0.041992078344181165
plot_id,batch_id 0 57 miss% 0.038662722171905556
plot_id,batch_id 0 58 miss% 0.02335809077244909
plot_id,batch_id 0 59 miss% 0.025701586844907344
plot_id,batch_id 0 60 miss% 0.062042794432091014
plot_id,batch_id 0 61 miss% 0.03313074813465673
plot_id,batch_id 0 62 miss% 0.026502681889659308
plot_id,batch_id 0 63 miss% 0.04263219605082974
plot_id,batch_id 0 64 miss% 0.03465041867507352
plot_id,batch_id 0 65 miss% 0.05428011218759049
plot_id,batch_id 0 66 miss% 0.05869256010940334
plot_id,batch_id 0 67 miss% 0.03163161104565331
plot_id,batch_id 0 68 miss% 0.023795350834754866
plot_id,batch_id 0 69 miss% the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  61841
Epoch:0, Train loss:0.548416, valid loss:0.542040
Epoch:1, Train loss:0.357904, valid loss:0.360886
Epoch:2, Train loss:0.317142, valid loss:0.007765
Epoch:3, Train loss:0.008264, valid loss:0.003857
Epoch:4, Train loss:0.006022, valid loss:0.002870
Epoch:5, Train loss:0.005549, valid loss:0.003966
Epoch:6, Train loss:0.004887, valid loss:0.002593
Epoch:7, Train loss:0.004594, valid loss:0.002153
Epoch:8, Train loss:0.004508, valid loss:0.002761
Epoch:9, Train loss:0.004303, valid loss:0.002544
Epoch:10, Train loss:0.003917, valid loss:0.001892
Epoch:11, Train loss:0.002916, valid loss:0.001815
Epoch:12, Train loss:0.002876, valid loss:0.001762
Epoch:13, Train loss:0.002714, valid loss:0.001675
Epoch:14, Train loss:0.002789, valid loss:0.001604
Epoch:15, Train loss:0.002671, valid loss:0.001598
Epoch:16, Train loss:0.002649, valid loss:0.001329
Epoch:17, Train loss:0.002599, valid loss:0.001343
Epoch:18, Train loss:0.002558, valid loss:0.001449
Epoch:19, Train loss:0.002492, valid loss:0.001249
Epoch:20, Train loss:0.002492, valid loss:0.001684
Epoch:21, Train loss:0.001883, valid loss:0.001141
Epoch:22, Train loss:0.001871, valid loss:0.001253
Epoch:23, Train loss:0.001831, valid loss:0.001220
Epoch:24, Train loss:0.001808, valid loss:0.001025
Epoch:25, Train loss:0.001822, valid loss:0.001083
Epoch:26, Train loss:0.001810, valid loss:0.001183
Epoch:27, Train loss:0.001762, valid loss:0.001054
Epoch:28, Train loss:0.001768, valid loss:0.001614
Epoch:29, Train loss:0.001786, valid loss:0.001121
Epoch:30, Train loss:0.001751, valid loss:0.001009
Epoch:31, Train loss:0.001426, valid loss:0.001044
Epoch:32, Train loss:0.001389, valid loss:0.000923
Epoch:33, Train loss:0.001379, valid loss:0.001013
Epoch:34, Train loss:0.001400, valid loss:0.001051
Epoch:35, Train loss:0.001406, valid loss:0.001011
Epoch:36, Train loss:0.001442, valid loss:0.001038
Epoch:37, Train loss:0.001330, valid loss:0.001020
Epoch:38, Train loss:0.001369, valid loss:0.001038
Epoch:39, Train loss:0.001367, valid loss:0.000997
Epoch:40, Train loss:0.001338, valid loss:0.001038
Epoch:41, Train loss:0.001196, valid loss:0.000918
Epoch:42, Train loss:0.001187, valid loss:0.000922
Epoch:43, Train loss:0.001182, valid loss:0.000957
Epoch:44, Train loss:0.001183, valid loss:0.000935
Epoch:45, Train loss:0.001156, valid loss:0.000887
Epoch:46, Train loss:0.001180, valid loss:0.001022
Epoch:47, Train loss:0.001158, valid loss:0.000943
Epoch:48, Train loss:0.001152, valid loss:0.000896
Epoch:49, Train loss:0.001159, valid loss:0.000889
Epoch:50, Train loss:0.001156, valid loss:0.000912
Epoch:51, Train loss:0.001070, valid loss:0.000889
Epoch:52, Train loss:0.001074, valid loss:0.000950
Epoch:53, Train loss:0.001065, valid loss:0.000900
Epoch:54, Train loss:0.001059, valid loss:0.000906
Epoch:55, Train loss:0.001064, valid loss:0.000907
Epoch:56, Train loss:0.001057, valid loss:0.000919
Epoch:57, Train loss:0.001055, valid loss:0.000884
Epoch:58, Train loss:0.001053, valid loss:0.000915
Epoch:59, Train loss:0.001058, valid loss:0.000887
Epoch:60, Train loss:0.001047, valid loss:0.000895
training time 7788.186707019806
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.03699832416443544
plot_id,batch_id 0 1 miss% 0.018177622988182002
plot_id,batch_id 0 2 miss% 0.022072182812158236
plot_id,batch_id 0 3 miss% 0.030440270131363432
plot_id,batch_id 0 4 miss% 0.0210714705997455
plot_id,batch_id 0 5 miss% 0.04225233239587776
plot_id,batch_id 0 6 miss% 0.021395947570523932
plot_id,batch_id 0 7 miss% 0.022265254366257405
plot_id,batch_id 0 8 miss% 0.02716321520410274
plot_id,batch_id 0 9 miss% 0.03640207597344985
plot_id,batch_id 0 10 miss% 0.0341694219487467
plot_id,batch_id 0 11 miss% 0.041716127330897204
plot_id,batch_id 0 12 miss% 0.021841565876646492
plot_id,batch_id 0 13 miss% 0.021814055411125823
plot_id,batch_id 0 14 miss% 0.03816258444421301
plot_id,batch_id 0 15 miss% 0.04204528972577325
plot_id,batch_id 0 16 miss% 0.03396877979851787
plot_id,batch_id 0 17 miss% 0.032698084516802756
plot_id,batch_id 0 18 miss% 0.03745701049744285
plot_id,batch_id 0 19 miss% 0.031174526308710575
plot_id,batch_id 0 20 miss% 0.047474090693672845
plot_id,batch_id 0 21 miss% 0.017591674041316868
plot_id,batch_id 0 22 miss% 0.028176760867944857
plot_id,batch_id 0 23 miss% 0.025722245349273358
plot_id,batch_id 0 24 miss% 0.01837526845992842
plot_id,batch_id 0 25 miss% 0.026049392985489146
plot_id,batch_id 0 26 miss% 0.02642291605344165
plot_id,batch_id 0 27 miss% 0.020368441726606533
plot_id,batch_id 0 28 miss% 0.030126172319648106
plot_id,batch_id 0 29 miss% 0.020355193712737283
plot_id,batch_id 0 30 miss% 0.03590960882544999
plot_id,batch_id 0 31 miss% 0.025953219466558358
plot_id,batch_id 0 32 miss% 0.026799250305516137
plot_id,batch_id 0 33 miss% 0.020870549637072904
plot_id,batch_id 0 34 miss% 0.01951735699940245
plot_id,batch_id 0 35 miss% 0.039453924375796576
plot_id,batch_id 0 36 miss% 0.03896086722400095
plot_id,batch_id 0 37 miss% 0.020699636911152362
plot_id,batch_id 0 38 miss% 0.014520320730149172
plot_id,batch_id 0 39 miss% 0.014758246273295259
plot_id,batch_id 0 40 miss% 0.06094198293452119
plot_id,batch_id 0 41 miss% 0.020423853511174452
plot_id,batch_id 0 42 miss% 0.0149910866250104
plot_id,batch_id 0 43 miss% 0.028026705535035445
plot_id,batch_id 0 44 miss% 0.017679340142405034
plot_id,batch_id 0 45 miss% 0.03696320496734342
plot_id,batch_id 0 46 miss% 0.02151777434871778
plot_id,batch_id 0 47 miss% 0.017482693532064365
plot_id,batch_id 0 48 miss% 0.019470962319535687
plot_id,batch_id 0 49 miss% 0.023580516323369247
plot_id,batch_id 0 50 miss% 0.04406898404525906
plot_id,batch_id 0 51 miss% 0.022153148488346475
plot_id,batch_id 0 52 miss% 0.018466210179375987
plot_id,batch_id 0 53 miss% 0.013900257374660703
plot_id,batch_id 0 54 miss% 0.026193431985200636
plot_id,batch_id 0 55 miss% 0.03820265021481174
plot_id,batch_id 0 56 miss% 0.025734676540634625
plot_id,batch_id 0 57 miss% 0.021100956036625162
plot_id,batch_id 0 58 miss% 0.01683572042243626
plot_id,batch_id 0 59 miss% 0.021575466474622593
plot_id,batch_id 0 60 miss% 0.025532903311725755
plot_id,batch_id 0 61 miss% 0.020409538464971386
plot_id,batch_id 0 62 miss% 0.024450546035017475
plot_id,batch_id 0 63 miss% 0.04662100475040074
plot_id,batch_id 0 64 miss% 0.024631193929829962
plot_id,batch_id 0 65 miss% 0.03582124098973277
plot_id,batch_id 0 66 miss% 0.0742933828790489
plot_id,batch_id 0 67 miss% 0.03262583160496929
plot_id,batch_id 0 68 miss% 0.03671017496291535
plot_id,batch_id 0.022684663786957805
plot_id,batch_id 0 70 miss% 0.04154736294945699
plot_id,batch_id 0 71 miss% 0.03172309777924961
plot_id,batch_id 0 72 miss% 0.04061653562680908
plot_id,batch_id 0 73 miss% 0.029403229300938784
plot_id,batch_id 0 74 miss% 0.031346149342872894
plot_id,batch_id 0 75 miss% 0.05567380178339872
plot_id,batch_id 0 76 miss% 0.060273638732686895
plot_id,batch_id 0 77 miss% 0.042092130945178254
plot_id,batch_id 0 78 miss% 0.03715677778005922
plot_id,batch_id 0 79 miss% 0.035277680158062576
plot_id,batch_id 0 80 miss% 0.03382040208689455
plot_id,batch_id 0 81 miss% 0.02962387501440854
plot_id,batch_id 0 82 miss% 0.029519641667479488
plot_id,batch_id 0 83 miss% 0.029971097003681613
plot_id,batch_id 0 84 miss% 0.02428504566451096
plot_id,batch_id 0 85 miss% 0.02864290481171752
plot_id,batch_id 0 86 miss% 0.0235914602668219
plot_id,batch_id 0 87 miss% 0.032174454332183106
plot_id,batch_id 0 88 miss% 0.036216097047526555
plot_id,batch_id 0 89 miss% 0.03281782184675886
plot_id,batch_id 0 90 miss% 0.03957802523912549
plot_id,batch_id 0 91 miss% 0.03255562223334935
plot_id,batch_id 0 92 miss% 0.03778967177476568
plot_id,batch_id 0 93 miss% 0.02908182015420816
plot_id,batch_id 0 94 miss% 0.042150525076608276
plot_id,batch_id 0 95 miss% 0.05930219704804397
plot_id,batch_id 0 96 miss% 0.028683756876406223
plot_id,batch_id 0 97 miss% 0.04873473033749692
plot_id,batch_id 0 98 miss% 0.033919640019155914
plot_id,batch_id 0 99 miss% 0.01750653850532098
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0250854  0.02546873 0.01928165 0.02511982 0.02005409 0.03107001
 0.02247422 0.02918604 0.02630618 0.01768524 0.03413349 0.05155358
 0.02313994 0.03791392 0.03525507 0.03765062 0.03882163 0.04943361
 0.03613901 0.03246528 0.08756656 0.01997207 0.03421941 0.03785867
 0.02954562 0.03554228 0.02515478 0.02374304 0.03320373 0.02796676
 0.03798088 0.02904259 0.02791271 0.033514   0.02965259 0.04250401
 0.04296385 0.0299294  0.03152524 0.03233115 0.06554587 0.02481157
 0.02224327 0.02366664 0.02587636 0.01590831 0.02913822 0.019518
 0.02603426 0.01453721 0.0312285  0.02896013 0.03104183 0.0121813
 0.02326339 0.03042095 0.04199208 0.03866272 0.02335809 0.02570159
 0.06204279 0.03313075 0.02650268 0.0426322  0.03465042 0.05428011
 0.05869256 0.03163161 0.02379535 0.02268466 0.04154736 0.0317231
 0.04061654 0.02940323 0.03134615 0.0556738  0.06027364 0.04209213
 0.03715678 0.03527768 0.0338204  0.02962388 0.02951964 0.0299711
 0.02428505 0.0286429  0.02359146 0.03217445 0.0362161  0.03281782
 0.03957803 0.03255562 0.03778967 0.02908182 0.04215053 0.0593022
 0.02868376 0.04873473 0.03391964 0.01750654]
for model  22 the mean error 0.03329576057034618
all id 22 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 21 out win 4 err 0.03329576057034618
Launcher: Job 23 completed in 7993 seconds.
Launcher: Task 224 done. Exiting.
0 69 miss% 0.037397651663891134
plot_id,batch_id 0 70 miss% 0.042391663570935094
plot_id,batch_id 0 71 miss% 0.04443949217372207
plot_id,batch_id 0 72 miss% 0.03613888476714304
plot_id,batch_id 0 73 miss% 0.0283635823794401
plot_id,batch_id 0 74 miss% 0.04521928638334016
plot_id,batch_id 0 75 miss% 0.03654615248485857
plot_id,batch_id 0 76 miss% 0.02799639598634791
plot_id,batch_id 0 77 miss% 0.028805651771866708
plot_id,batch_id 0 78 miss% 0.04098388179733013
plot_id,batch_id 0 79 miss% 0.03614118410268927
plot_id,batch_id 0 80 miss% 0.04820523699388695
plot_id,batch_id 0 81 miss% 0.03245055468148862
plot_id,batch_id 0 82 miss% 0.030631949460492327
plot_id,batch_id 0 83 miss% 0.04087338640292793
plot_id,batch_id 0 84 miss% 0.0313490055308982
plot_id,batch_id 0 85 miss% 0.03973280319311676
plot_id,batch_id 0 86 miss% 0.040656626628570856
plot_id,batch_id 0 87 miss% 0.03295701504008833
plot_id,batch_id 0 88 miss% 0.03890774902926049
plot_id,batch_id 0 89 miss% 0.03372272632620716
plot_id,batch_id 0 90 miss% 0.030178602774574164
plot_id,batch_id 0 91 miss% 0.027853207707759668
plot_id,batch_id 0 92 miss% 0.030766789917238135
plot_id,batch_id 0 93 miss% 0.041947814224592865
plot_id,batch_id 0 94 miss% 0.03979249923785691
plot_id,batch_id 0 95 miss% 0.04177816705800487
plot_id,batch_id 0 96 miss% 0.04323778862384257
plot_id,batch_id 0 97 miss% 0.041661173465382464
plot_id,batch_id 0 98 miss% 0.03264956694672196
plot_id,batch_id 0 99 miss% 0.02200010610193588
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03699832 0.01817762 0.02207218 0.03044027 0.02107147 0.04225233
 0.02139595 0.02226525 0.02716322 0.03640208 0.03416942 0.04171613
 0.02184157 0.02181406 0.03816258 0.04204529 0.03396878 0.03269808
 0.03745701 0.03117453 0.04747409 0.01759167 0.02817676 0.02572225
 0.01837527 0.02604939 0.02642292 0.02036844 0.03012617 0.02035519
 0.03590961 0.02595322 0.02679925 0.02087055 0.01951736 0.03945392
 0.03896087 0.02069964 0.01452032 0.01475825 0.06094198 0.02042385
 0.01499109 0.02802671 0.01767934 0.0369632  0.02151777 0.01748269
 0.01947096 0.02358052 0.04406898 0.02215315 0.01846621 0.01390026
 0.02619343 0.03820265 0.02573468 0.02110096 0.01683572 0.02157547
 0.0255329  0.02040954 0.02445055 0.046621   0.02463119 0.03582124
 0.07429338 0.03262583 0.03671017 0.03739765 0.04239166 0.04443949
 0.03613888 0.02836358 0.04521929 0.03654615 0.0279964  0.02880565
 0.04098388 0.03614118 0.04820524 0.03245055 0.03063195 0.04087339
 0.03134901 0.0397328  0.04065663 0.03295702 0.03890775 0.03372273
 0.0301786  0.02785321 0.03076679 0.04194781 0.0397925  0.04177817
 0.04323779 0.04166117 0.03264957 0.02200011]
for model  122 the mean error 0.030875733143795995
all id 122 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 25 out win 5 err 0.030875733143795995
Launcher: Job 123 completed in 7994 seconds.
Launcher: Task 31 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  46193
Epoch:0, Train loss:0.400516, valid loss:0.363185
Epoch:1, Train loss:0.226428, valid loss:0.230334
Epoch:2, Train loss:0.219986, valid loss:0.229553
Epoch:3, Train loss:0.218953, valid loss:0.229919
Epoch:4, Train loss:0.218467, valid loss:0.229253
Epoch:5, Train loss:0.218226, valid loss:0.229206
Epoch:6, Train loss:0.218015, valid loss:0.229037
Epoch:7, Train loss:0.217996, valid loss:0.228939
Epoch:8, Train loss:0.217805, valid loss:0.229465
Epoch:9, Train loss:0.217756, valid loss:0.228921
Epoch:10, Train loss:0.217610, valid loss:0.228701
Epoch:11, Train loss:0.217047, valid loss:0.228561
Epoch:12, Train loss:0.217071, valid loss:0.228736
Epoch:13, Train loss:0.217040, valid loss:0.228494
Epoch:14, Train loss:0.217004, valid loss:0.228500
Epoch:15, Train loss:0.216968, valid loss:0.228541
Epoch:16, Train loss:0.217017, valid loss:0.228695
Epoch:17, Train loss:0.216922, valid loss:0.228582
Epoch:18, Train loss:0.216925, valid loss:0.228562
Epoch:19, Train loss:0.216916, valid loss:0.228414
Epoch:20, Train loss:0.216914, valid loss:0.228424
Epoch:21, Train loss:0.216592, valid loss:0.228311
Epoch:22, Train loss:0.216616, valid loss:0.228348
Epoch:23, Train loss:0.216598, valid loss:0.228381
Epoch:24, Train loss:0.216610, valid loss:0.228285
Epoch:25, Train loss:0.216574, valid loss:0.228345
Epoch:26, Train loss:0.216590, valid loss:0.228476
Epoch:27, Train loss:0.216583, valid loss:0.228297
Epoch:28, Train loss:0.216578, valid loss:0.228487
Epoch:29, Train loss:0.216542, valid loss:0.228293
Epoch:30, Train loss:0.216566, valid loss:0.228275
Epoch:31, Train loss:0.216390, valid loss:0.228281
Epoch:32, Train loss:0.216398, valid loss:0.228247
Epoch:33, Train loss:0.216391, valid loss:0.228239
Epoch:34, Train loss:0.216389, valid loss:0.228305
Epoch:35, Train loss:0.216389, valid loss:0.228204
Epoch:36, Train loss:0.216380, valid loss:0.228261
Epoch:37, Train loss:0.216378, valid loss:0.228218
Epoch:38, Train loss:0.216379, valid loss:0.228241
Epoch:39, Train loss:0.216372, valid loss:0.228282
Epoch:40, Train loss:0.216372, valid loss:0.228226
Epoch:41, Train loss:0.216296, valid loss:0.228197
Epoch:42, Train loss:0.216292, valid loss:0.228203
Epoch:43, Train loss:0.216287, valid loss:0.228217
Epoch:44, Train loss:0.216294, valid loss:0.228193
Epoch:45, Train loss:0.216283, valid loss:0.228183
Epoch:46, Train loss:0.216288, valid loss:0.228198
Epoch:47, Train loss:0.216276, valid loss:0.228172
Epoch:48, Train loss:0.216290, valid loss:0.228191
Epoch:49, Train loss:0.216280, valid loss:0.228198
Epoch:50, Train loss:0.216278, valid loss:0.228189
Epoch:51, Train loss:0.216243, valid loss:0.228187
Epoch:52, Train loss:0.216239, valid loss:0.228164
Epoch:53, Train loss:0.216240, valid loss:0.228180
Epoch:54, Train loss:0.216241, valid loss:0.228186
Epoch:55, Train loss:0.216236, valid loss:0.228168
Epoch:56, Train loss:0.216238, valid loss:0.228189
Epoch:57, Train loss:0.216236, valid loss:0.228180
Epoch:58, Train loss:0.216234, valid loss:0.228188
Epoch:59, Train loss:0.216235, valid loss:0.228195
Epoch:60, Train loss:0.216233, valid loss:0.228171
training time 7834.555505037308
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.658006343970934
plot_id,batch_id 0 1 miss% 0.7449253008310084
plot_id,batch_id 0 2 miss% 0.7608484884245622
plot_id,batch_id 0 3 miss% 0.7718110306058454
plot_id,batch_id 0 4 miss% 0.769857520622087
plot_id,batch_id 0 5 miss% 0.6668396117728626
plot_id,batch_id 0 6 miss% 0.7426232347824676
plot_id,batch_id 0 7 miss% 0.7588646511338168
plot_id,batch_id 0 8 miss% 0.7656711997262183
plot_id,batch_id 0 9 miss% 0.7716714487794756
plot_id,batch_id 0 10 miss% 0.6309091574925906
plot_id,batch_id 0 11 miss% 0.7417302815869405
plot_id,batch_id 0 12 miss% 0.7531535583502729
plot_id,batch_id 0 13 miss% 0.7611774271655345
plot_id,batch_id 0 14 miss% 0.7703363954412201
plot_id,batch_id 0 15 miss% 0.6485179276807558
plot_id,batch_id 0 16 miss% 0.733112837067091
plot_id,batch_id 0 17 miss% 0.7602244087842737
plot_id,batch_id 0 18 miss% 0.7631097911148161
plot_id,batch_id 0 19 miss% 0.7657393682268052
plot_id,batch_id 0 20 miss% 0.7135787375814361
plot_id,batch_id 0 21 miss% 0.7635991578444883
plot_id,batch_id 0 22 miss% 0.7692675767479905
plot_id,batch_id 0 23 miss% 0.7760914463218218
plot_id,batch_id 0 24 miss% 0.7792743333115622
plot_id,batch_id 0 25 miss% 0.6980906105107797
plot_id,batch_id 0 26 miss% 0.7572034896376693
plot_id,batch_id 0 27 miss% 0.7698740173834541
plot_id,batch_id 0 28 miss% 0.7686814988131869
plot_id,batch_id 0 29 miss% 0.7779246214890648
plot_id,batch_id 0 30 miss% 0.7006278150679683
plot_id,batch_id 0 31 miss% 0.7538414720269105
plot_id,batch_id 0 32 miss% 0.7642928461569234
plot_id,batch_id 0 33 miss% 0.768486291010602
plot_id,batch_id 0 34 miss% 0.7704149656129734
plot_id,batch_id 0 35 miss% 0.695139055568486
plot_id,batch_id 0 36 miss% 0.7582039380173846
plot_id,batch_id 0 37 miss% 0.7628575805665687
plot_id,batch_id 0 38 miss% 0.7688328722724503
plot_id,batch_id 0 39 miss% 0.7714625052198161
plot_id,batch_id 0 40 miss% 0.7343657715305836
plot_id,batch_id 0 41 miss% 0.76674852791174
plot_id,batch_id 0 42 miss% 0.7683752189609713
plot_id,batch_id 0 43 miss% 0.7775949572908002
plot_id,batch_id 0 44 miss% 0.7834673059419469
plot_id,batch_id 0 45 miss% 0.7288455108852303
plot_id,batch_id 0 46 miss% 0.7668155472212442
plot_id,batch_id 0 47 miss% 0.7701517742814594
plot_id,batch_id 0 48 miss% 0.7760288768875973
plot_id,batch_id 0 49 miss% 0.782994320547472
plot_id,batch_id 0 50 miss% 0.7439719907324273
plot_id,batch_id 0 51 miss% 0.7638280740061103
plot_id,batch_id 0 52 miss% 0.7704383580004017
plot_id,batch_id 0 53 miss% 0.7772138664654936
plot_id,batch_id 0 54 miss% 0.7843276815042921
plot_id,batch_id 0 55 miss% 0.7367685656860616
plot_id,batch_id 0 56 miss% 0.7643171841171655
plot_id,batch_id 0 57 miss% 0.7720478537446455
plot_id,batch_id 0 58 miss% 0.7758284029923165
plot_id,batch_id 0 59 miss% 0.7806752963057495
plot_id,batch_id 0 60 miss% 0.565963015095243
plot_id,batch_id 0 61 miss% 0.7056931704730736
plot_id,batch_id 0 62 miss% 0.7339654022546759
plot_id,batch_id 0 63 miss% 0.755023997268883
plot_id,batch_id 0 64 miss% 0.7615181237464563
plot_id,batch_id 0 65 miss% 0.5578544185406942
plot_id,batch_id 0 66 miss% 0.6974651536460906
plot_id,batch_id 0 67 miss% 0.7240044936799571
plot_id,batch_id 0 68 miss% 0.7490395186227452
plot_id,batch_id 0 69 miss% 0.7506213491451389
plot_id,batch_id 0 70 miss% 0.5258128792804885
plot_id,batch_id 0 71 miss% 0.6985838626647947
plot_id,batch_id 0 72 miss% 0.7121883388282715
plot_id,batch_id 0 73 miss% 0.7363998581615145
plot_id,batch_id 0 74 miss% 0.7471322710049041
plot_id,batch_id 0 75 miss% 0.5151684740429636
plot_id,batch_id 0 76 miss% 0.6491698005707652
plot_id,batch_id 0 77 miss% 0.7002629263362277
plot_id,batch_id 0 78 miss% 0.7291292109244139
plot_id,batch_id 0 79 miss% 0.742354567020277
plot_id,batch_id 0 80 miss% 0.5910404153266249
plot_id,batch_id 0 81 miss% 0.722443358153146
plot_id,batch_id 0 82 miss% 0.7478277619512788
plot_id,batch_id 0 83 miss% 0.7560532496059968
plot_id,batch_id 0 84 miss% 0.7627556432640414
plot_id,batch_id 0 85 miss% 0.5917975517136794
plot_id,batch_id 0 86 miss% 0.717120501328211
plot_id,batch_id 0 87 miss% 0.7412935152737125
plot_id,batch_id 0 88 miss% 0.7581082000922033
plot_id,batch_id 0 89 miss% 0.7635581013200924
plot_id,batch_id 0 90 miss% 0.5479775441689824
plot_id,batch_id 0 91 miss% 0.7178895804222599
plot_id,batch_id 0 92 miss% 0.7360273410090602
plot_id,batch_id 0 93 miss% 0.745715694384736
plot_id,batch_id 0 94 miss% 0.7616845814062626
plot_id,batch_id 0 95 miss% 0.5506419109506228
plot_id,batch_id 0 96 miss% 0.7011083283029261
plot_id,batch_id 0 97 miss% 0.7291269502495753
plot_id,batch_id 0 98 miss% 0.7460410894932834
plot_id,batch_id 0 99 miss% 0.7551668888602447
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.65800634 0.7449253  0.76084849 0.77181103 0.76985752 0.66683961
 0.74262323 0.75886465 0.7656712  0.77167145 0.63090916 0.74173028
 0.75315356 0.76117743 0.7703364  0.64851793 0.73311284 0.76022441
 0.76310979 0.76573937 0.71357874 0.76359916 0.76926758 0.77609145
 0.77927433 0.69809061 0.75720349 0.76987402 0.7686815  0.77792462
 0.70062782 0.75384147 0.76429285 0.76848629 0.77041497 0.69513906
 0.75820394 0.76285758 0.76883287 0.77146251 0.73436577 0.76674853
 0.76837522 0.77759496 0.78346731 0.72884551 0.76681555 0.77015177
 0.77602888 0.78299432 0.74397199 0.76382807 0.77043836 0.77721387
 0.78432768 0.73676857 0.76431718 0.77204785 0.7758284  0.7806753
 0.56596302 0.70569317 0.7339654  0.755024   0.76151812 0.55785442
 0.69746515 0.72400449 0.74903952 0.75062135 0.52581288 0.69858386
 0.71218834 0.73639986 0.74713227 0.51516847 0.6491698  0.70026293
 0.72912921 0.74235457 0.59104042 0.72244336 0.74782776 0.75605325
 0.76275564 0.59179755 0.7171205  0.74129352 0.7581082  0.7635581
 0.54797754 0.71788958 0.73602734 0.74571569 0.76168458 0.55064191
 0.70110833 0.72912695 0.74604109 0.75516689]
for model  193 the mean error 0.7305240893831935
all id 193 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 31 out win 4 err 0.7305240893831935
Launcher: Job 194 completed in 8010 seconds.
Launcher: Task 219 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  77489
Epoch:0, Train loss:0.648619, valid loss:0.649701
Epoch:1, Train loss:0.058994, valid loss:0.017561
Epoch:2, Train loss:0.018658, valid loss:0.006092
Epoch:3, Train loss:0.012297, valid loss:0.005064
Epoch:4, Train loss:0.010449, valid loss:0.004372
Epoch:5, Train loss:0.009133, valid loss:0.004957
Epoch:6, Train loss:0.007707, valid loss:0.003895
Epoch:7, Train loss:0.007473, valid loss:0.003107
Epoch:8, Train loss:0.006634, valid loss:0.004929
Epoch:9, Train loss:0.006270, valid loss:0.003703
Epoch:10, Train loss:0.006279, valid loss:0.003739
Epoch:11, Train loss:0.004215, valid loss:0.002102
Epoch:12, Train loss:0.004134, valid loss:0.002178
Epoch:13, Train loss:0.004034, valid loss:0.002298
Epoch:14, Train loss:0.004021, valid loss:0.002382
Epoch:15, Train loss:0.003925, valid loss:0.002305
Epoch:16, Train loss:0.003970, valid loss:0.002946
Epoch:17, Train loss:0.003866, valid loss:0.002349
Epoch:18, Train loss:0.003611, valid loss:0.001948
Epoch:19, Train loss:0.003720, valid loss:0.001957
Epoch:20, Train loss:0.003452, valid loss:0.001827
Epoch:21, Train loss:0.002570, valid loss:0.001550
Epoch:22, Train loss:0.002582, valid loss:0.001893
Epoch:23, Train loss:0.002605, valid loss:0.001580
Epoch:24, Train loss:0.002508, valid loss:0.001656
Epoch:25, Train loss:0.002524, valid loss:0.001581
Epoch:26, Train loss:0.002482, valid loss:0.001647
Epoch:27, Train loss:0.002401, valid loss:0.001827
Epoch:28, Train loss:0.002463, valid loss:0.001596
Epoch:29, Train loss:0.002387, valid loss:0.001612
Epoch:30, Train loss:0.002391, valid loss:0.001504
Epoch:31, Train loss:0.001908, valid loss:0.001338
Epoch:32, Train loss:0.001898, valid loss:0.001379
Epoch:33, Train loss:0.001886, valid loss:0.001375
Epoch:34, Train loss:0.001885, valid loss:0.001312
Epoch:35, Train loss:0.001839, valid loss:0.001280
Epoch:36, Train loss:0.001864, valid loss:0.001279
Epoch:37, Train loss:0.001853, valid loss:0.001244
Epoch:38, Train loss:0.001790, valid loss:0.001338
Epoch:39, Train loss:0.001855, valid loss:0.001339
Epoch:40, Train loss:0.001780, valid loss:0.001348
Epoch:41, Train loss:0.001579, valid loss:0.001267
Epoch:42, Train loss:0.001564, valid loss:0.001208
Epoch:43, Train loss:0.001571, valid loss:0.001217
Epoch:44, Train loss:0.001545, valid loss:0.001216
Epoch:45, Train loss:0.001543, valid loss:0.001210
Epoch:46, Train loss:0.001523, valid loss:0.001160
Epoch:47, Train loss:0.001524, valid loss:0.001190
Epoch:48, Train loss:0.001522, valid loss:0.001197
Epoch:49, Train loss:0.001512, valid loss:0.001180
Epoch:50, Train loss:0.001507, valid loss:0.001173
Epoch:51, Train loss:0.001403, valid loss:0.001136
Epoch:52, Train loss:0.001386, valid loss:0.001175
Epoch:53, Train loss:0.001391, valid loss:0.001113
Epoch:54, Train loss:0.001386, valid loss:0.001096
Epoch:55, Train loss:0.001394, valid loss:0.001148
Epoch:56, Train loss:0.001378, valid loss:0.001109
Epoch:57, Train loss:0.001369, valid loss:0.001128
Epoch:58, Train loss:0.001385, valid loss:0.001154
Epoch:59, Train loss:0.001359, valid loss:0.001189
Epoch:60, Train loss:0.001371, valid loss:0.001113
training time 7822.770224332809
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.025314191978875553
plot_id,batch_id 0 1 miss% 0.022480625854016355
plot_id,batch_id 0 2 miss% 0.028520453689514978
plot_id,batch_id 0 3 miss% 0.032237351696096345
plot_id,batch_id 0 4 miss% 0.022444298991610614
plot_id,batch_id 0 5 miss% 0.039815166350399424
plot_id,batch_id 0 6 miss% 0.0353383258684987
plot_id,batch_id 0 7 miss% 0.03205385259976252
plot_id,batch_id 0 8 miss% 0.033434434261825643
plot_id,batch_id 0 9 miss% 0.04012887586340454
plot_id,batch_id 0 10 miss% 0.07083598952635538
plot_id,batch_id 0 11 miss% 0.05018458522268956
plot_id,batch_id 0 12 miss% 0.02198087848431237
plot_id,batch_id 0 13 miss% 0.02129487680888478
plot_id,batch_id 0 14 miss% 0.0328488996430455
plot_id,batch_id 0 15 miss% 0.05624402171237202
plot_id,batch_id 0 16 miss% 0.036379812180671725
plot_id,batch_id 0 17 miss% 0.029130771868406045
plot_id,batch_id 0 18 miss% 0.03953428406372718
plot_id,batch_id 0 19 miss% 0.03648472845007744
plot_id,batch_id 0 20 miss% 0.06301074271811137
plot_id,batch_id 0 21 miss% 0.021766512811877477
plot_id,batch_id 0 22 miss% 0.029512916669917795
plot_id,batch_id 0 23 miss% 0.029466922937222036
plot_id,batch_id 0 24 miss% 0.02468082682360259
plot_id,batch_id 0 25 miss% 0.02750040868654988
plot_id,batch_id 0 26 miss% 0.03146495113305234
plot_id,batch_id 0 27 miss% 0.018207312246686355
plot_id,batch_id 0 28 miss% 0.027114354221510355
plot_id,batch_id 0 29 miss% 0.029899481384508878
plot_id,batch_id 0 30 miss% 0.043950752490327934
plot_id,batch_id 0 31 miss% 0.035725142512255635
plot_id,batch_id 0 32 miss% 0.034668515920628896
plot_id,batch_id 0 33 miss% 0.02768513540649548
plot_id,batch_id 0 34 miss% 0.026995165494516308
plot_id,batch_id 0 35 miss% 0.05101695458023249
plot_id,batch_id 0 36 miss% 0.03869994500018341
plot_id,batch_id 0 37 miss% 0.025633597508680137
plot_id,batch_id 0 38 miss% 0.04027514137169504
plot_id,batch_id 0 39 miss% 0.02095544083156784
plot_id,batch_id 0 40 miss% 0.08060236066576995
plot_id,batch_id 0 41 miss% 0.023837002224670546
plot_id,batch_id 0 42 miss% 0.01957963625407471
plot_id,batch_id 0 43 miss% 0.030287058923759623
plot_id,batch_id 0 44 miss% 0.019475323265230476
plot_id,batch_id 0 45 miss% 0.025622626456529664
plot_id,batch_id 0 46 miss% 0.025698185796360344
plot_id,batch_id 0 47 miss% 0.022360740363474997
plot_id,batch_id 0 48 miss% 0.02907406037281255
plot_id,batch_id 0 49 miss% 0.028728206553315715
plot_id,batch_id 0 50 miss% 0.0278741132260792
plot_id,batch_id 0 51 miss% 0.023580214565044395
plot_id,batch_id 0 52 miss% 0.03140232836752401
plot_id,batch_id 0 53 miss% 0.012862387778929304
plot_id,batch_id 0 54 miss% 0.03322058764251246
plot_id,batch_id 0 55 miss% 0.03141616599496573
plot_id,batch_id 0 56 miss% 0.029293959198231162
plot_id,batch_id 0 57 miss% 0.03478902702907567
plot_id,batch_id 0 58 miss% 0.030539049551036448
plot_id,batch_id 0 59 miss% 0.02912264055279979
plot_id,batch_id 0 60 miss% 0.06798266304974622
plot_id,batch_id 0 61 miss% 0.02789993498741425
plot_id,batch_id 0 62 miss% 0.027047270918467696
plot_id,batch_id 0 63 miss% 0.028801606665362016
plot_id,batch_id 0 64 miss% 0.033094080750074285
plot_id,batch_id 0 65 miss% 0.06892878098890422
plot_id,batch_id 0 66 miss% 0.03289085240070384
plot_id,batch_id 0 67 miss% 0.03121257042071364
plot_id,batch_id 0 68 miss% 0.037398208416681955
plot_id,batch_id 0 69 miss% 0.03160826932778554
plot_id,batch_id 0 70 miss% 0.07366221631560477
plot_id,batch_id 0 71 miss% 0.05472716705938314
plot_id,batch_id 0 72 miss% 0.03277800276006764
plot_id,batch_id 0 73 miss% 0.04283110241183624
plot_id,batch_id 0 74 miss% 0.030004870759504285
plot_id,batch_id 0 75 miss% 0.04685782453394742
plot_id,batch_id 0 76 miss% 0.05526479018647977
plot_id,batch_id 0 77 miss% 0.047598356375793946
plot_id,batch_id 0 78 miss% 0.033972429871904776
plot_id,batch_id 0 79 miss% 0.05225118314371702
plot_id,batch_id 0 80 miss% 0.025945315397643595
plot_id,batch_id 0 81 miss% 0.03855256966818932
plot_id,batch_id 0 82 miss% 0.030408141591639605
plot_id,batch_id 0 83 miss% 0.03352887748169298
plot_id,batch_id 0 84 miss% 0.03483568547540656
plot_id,batch_id 0 85 miss% 0.04498752491252552
plot_id,batch_id 0 86 miss% 0.03040211284615697
plot_id,batch_id 0 87 miss% 0.034761908440650593
plot_id,batch_id 0 88 miss% 0.036428511101483006
plot_id,batch_id 0 89 miss% 0.028536444062120428
plot_id,batch_id 0 90 miss% 0.0483539546455028
plot_id,batch_id 0 91 miss% 0.030873463070163146
plot_id,batch_id 0 92 miss% 0.0331756584670691
plot_id,batch_id 0 93 miss% 0.03745589215329623
plot_id,batch_id 0 94 miss% 0.07169667671620877
plot_id,batch_id 0 95 miss% 0.05790810809971917
plot_id,batch_id 0 96 miss% 0.03699582126904481
plot_id,batch_id 0 97 miss% 0.03891290124387006
plot_id,batch_id 0 98 miss% 0.02492540396701714
plot_id,batch_id 0 99 miss% 0.055480212734867605
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02531419 0.02248063 0.02852045 0.03223735 0.0224443  0.03981517
 0.03533833 0.03205385 0.03343443 0.04012888 0.07083599 0.05018459
 0.02198088 0.02129488 0.0328489  0.05624402 0.03637981 0.02913077
 0.03953428 0.03648473 0.06301074 0.02176651 0.02951292 0.02946692
 0.02468083 0.02750041 0.03146495 0.01820731 0.02711435 0.02989948
 0.04395075 0.03572514 0.03466852 0.02768514 0.02699517 0.05101695
 0.03869995 0.0256336  0.04027514 0.02095544 0.08060236 0.023837
 0.01957964 0.03028706 0.01947532 0.02562263 0.02569819 0.02236074
 0.02907406 0.02872821 0.02787411 0.02358021 0.03140233 0.01286239
 0.03322059 0.03141617 0.02929396 0.03478903 0.03053905 0.02912264
 0.06798266 0.02789993 0.02704727 0.02880161 0.03309408 0.06892878
 0.03289085 0.03121257 0.03739821 0.03160827 0.07366222 0.05472717
 0.032778   0.0428311  0.03000487 0.04685782 0.05526479 0.04759836
 0.03397243 0.05225118 0.02594532 0.03855257 0.03040814 0.03352888
 0.03483569 0.04498752 0.03040211 0.03476191 0.03642851 0.02853644
 0.04835395 0.03087346 0.03317566 0.03745589 0.07169668 0.05790811
 0.03699582 0.0389129  0.0249254  0.05548021]
for model  76 the mean error 0.035752596853347265
all id 76 hidden_dim 24 learning_rate 0.02 num_layers 5 frames 21 out win 4 err 0.035752596853347265
Launcher: Job 77 completed in 8030 seconds.
Launcher: Task 227 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  35921
Epoch:0, Train loss:0.577978, valid loss:0.559762
Epoch:1, Train loss:0.049546, valid loss:0.016271
Epoch:2, Train loss:0.022049, valid loss:0.005459
Epoch:3, Train loss:0.011010, valid loss:0.004962
Epoch:4, Train loss:0.009105, valid loss:0.003871
Epoch:5, Train loss:0.007910, valid loss:0.003580
Epoch:6, Train loss:0.007031, valid loss:0.003802
Epoch:7, Train loss:0.006504, valid loss:0.003359
Epoch:8, Train loss:0.005906, valid loss:0.002848
Epoch:9, Train loss:0.005467, valid loss:0.002968
Epoch:10, Train loss:0.005135, valid loss:0.002994
Epoch:11, Train loss:0.004094, valid loss:0.002231
Epoch:12, Train loss:0.003967, valid loss:0.002167
Epoch:13, Train loss:0.003827, valid loss:0.002033
Epoch:14, Train loss:0.003729, valid loss:0.002011
Epoch:15, Train loss:0.003593, valid loss:0.002200
Epoch:16, Train loss:0.003490, valid loss:0.001899
Epoch:17, Train loss:0.003407, valid loss:0.002142
Epoch:18, Train loss:0.003353, valid loss:0.001750
Epoch:19, Train loss:0.003235, valid loss:0.001679
Epoch:20, Train loss:0.003117, valid loss:0.001827
Epoch:21, Train loss:0.002707, valid loss:0.001585
Epoch:22, Train loss:0.002662, valid loss:0.001633
Epoch:23, Train loss:0.002634, valid loss:0.001624
Epoch:24, Train loss:0.002588, valid loss:0.001663
Epoch:25, Train loss:0.002574, valid loss:0.001541
Epoch:26, Train loss:0.002514, valid loss:0.001472
Epoch:27, Train loss:0.002516, valid loss:0.001506
Epoch:28, Train loss:0.002482, valid loss:0.001460
Epoch:29, Train loss:0.002457, valid loss:0.001486
Epoch:30, Train loss:0.002457, valid loss:0.001448
Epoch:31, Train loss:0.002190, valid loss:0.001348
Epoch:32, Train loss:0.002166, valid loss:0.001350
Epoch:33, Train loss:0.002153, valid loss:0.001295
Epoch:34, Train loss:0.002151, valid loss:0.001330
Epoch:35, Train loss:0.002129, valid loss:0.001339
Epoch:36, Train loss:0.002109, valid loss:0.001370
Epoch:37, Train loss:0.002113, valid loss:0.001309
Epoch:38, Train loss:0.002097, valid loss:0.001320
Epoch:39, Train loss:0.002099, valid loss:0.001380
Epoch:40, Train loss:0.002068, valid loss:0.001351
Epoch:41, Train loss:0.001949, valid loss:0.001245
Epoch:42, Train loss:0.001942, valid loss:0.001243
Epoch:43, Train loss:0.001944, valid loss:0.001254
Epoch:44, Train loss:0.001935, valid loss:0.001259
Epoch:45, Train loss:0.001928, valid loss:0.001285
Epoch:46, Train loss:0.001922, valid loss:0.001256
Epoch:47, Train loss:0.001909, valid loss:0.001243
Epoch:48, Train loss:0.001898, valid loss:0.001223
Epoch:49, Train loss:0.001902, valid loss:0.001235
Epoch:50, Train loss:0.001899, valid loss:0.001199
Epoch:51, Train loss:0.001830, valid loss:0.001210
Epoch:52, Train loss:0.001828, valid loss:0.001183
Epoch:53, Train loss:0.001825, valid loss:0.001203
Epoch:54, Train loss:0.001825, valid loss:0.001192
Epoch:55, Train loss:0.001816, valid loss:0.001208
Epoch:56, Train loss:0.001819, valid loss:0.001192
Epoch:57, Train loss:0.001809, valid loss:0.001187
Epoch:58, Train loss:0.001808, valid loss:0.001204
Epoch:59, Train loss:0.001806, valid loss:0.001187
Epoch:60, Train loss:0.001802, valid loss:0.001165
training time 7834.720151901245
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.03756591774758263
plot_id,batch_id 0 1 miss% 0.016284991931602946
plot_id,batch_id 0 2 miss% 0.022531806870614943
plot_id,batch_id 0 3 miss% 0.026406828522206473
plot_id,batch_id 0 4 miss% 0.02189756737019684
plot_id,batch_id 0 5 miss% 0.04432782139067104
plot_id,batch_id 0 6 miss% 0.029069914452096054
plot_id,batch_id 0 7 miss% 0.03990477464381569
plot_id,batch_id 0 8 miss% 0.02756821492745546
plot_id,batch_id 0 9 miss% 0.03356317137957643
plot_id,batch_id 0 10 miss% 0.03643115136215214
plot_id,batch_id 0 11 miss% 0.038067780862181246
plot_id,batch_id 0 12 miss% 0.037218664823744585
plot_id,batch_id 0 13 miss% 0.03591953649692418
plot_id,batch_id 0 14 miss% 0.03707774680501397
plot_id,batch_id 0 15 miss% 0.02816565621160453
plot_id,batch_id 0 16 miss% 0.02508230277899292
plot_id,batch_id 0 17 miss% 0.0386759205748821
plot_id,batch_id 0 18 miss% 0.029758530409845902
plot_id,batch_id 0 19 miss% 0.05773870748301617
plot_id,batch_id 0 20 miss% 0.05455632545706561
plot_id,batch_id 0 21 miss% 0.021227295274218325
plot_id,batch_id 0 22 miss% 0.022181556293238116
plot_id,batch_id 0 23 miss% 0.019011257421756394
plot_id,batch_id 0 24 miss% 0.020345275053156243
plot_id,batch_id 0 25 miss% 0.04469535581239842
plot_id,batch_id 0 26 miss% 0.0195136421346986
plot_id,batch_id 0 27 miss% 0.02901418287623294
plot_id,batch_id 0 28 miss% 0.025643212312543063
plot_id,batch_id 0 29 miss% 0.028239738320010335
plot_id,batch_id 0 30 miss% 0.04207898629115065
plot_id,batch_id 0 31 miss% 0.03440348975512871
plot_id,batch_id 0 32 miss% 0.03199017139303318
plot_id,batch_id 0 33 miss% 0.02756925299349559
plot_id,batch_id 0 34 miss% 0.03600792203091433
plot_id,batch_id 0 35 miss% 0.03552613482892135
plot_id,batch_id 0 36 miss% 0.06485226124388116
plot_id,batch_id 0 37 miss% 0.03780173461013331
plot_id,batch_id 0 38 miss% 0.03377384712612044
plot_id,batch_id 0 39 miss% 0.028350165276789508
plot_id,batch_id 0 40 miss% 0.06671483507763269
plot_id,batch_id 0 41 miss% 0.02156354423261848
plot_id,batch_id 0 42 miss% 0.016048705687351783
plot_id,batch_id 0 43 miss% 0.01713494455177269
plot_id,batch_id 0 44 miss% 0.01878682392088212
plot_id,batch_id 0 45 miss% 0.03934916829094254
plot_id,batch_id 0 46 miss% 0.020820084783826406
plot_id,batch_id 0 47 miss% 0.020716252468427166
plot_id,batch_id 0 48 miss% 0.028221213388008853
plot_id,batch_id 0 49 miss% 0.02215955587574861
plot_id,batch_id 0 50 miss% 0.046705547569861526
plot_id,batch_id 0 51 miss% 0.019362131715303172
plot_id,batch_id 0 52 miss% 0.031149669197276162
plot_id,batch_id 0 53 miss% 0.02938170911378617
plot_id,batch_id 0 54 miss% 0.040996420183717415
plot_id,batch_id 0 55 miss% 0.037380633743319784
plot_id,batch_id 0 56 miss% 0.029368374269886754
plot_id,batch_id 0 57 miss% 0.03477604421412983
plot_id,batch_id 0 58 miss% 0.03079522839343998
plot_id,batch_id 0 59 miss% 0.0353620649795123
plot_id,batch_id 0 60 miss% 0.034308212931568015
plot_id,batch_id 0 61 miss% 0.04604563115935859
plot_id,batch_id 0 62 miss% 0.04468251967738737
plot_id,batch_id 0 63 miss% 0.030916393478303217
plot_id,batch_id 0 64 miss% 0.03772477242608931
plot_id,batch_id 0 65 miss% 0.03710838149948912
plot_id,batch_id 0 66 miss% 0.061360426147872425
plot_id,batch_id 0 67 miss% 0.04421335067681637
plot_id,batch_id 0 68 miss% 0.04257065112478768
plot_id,batch_id 0 69 miss% 0.031720518047379805
plot_id,batch_id 0 70 miss% 0.05091986026073446
plot_id,batch_id 0 71 miss% 0.031086419325056857
plot_id,batch_id 0 72 miss% 0.04518326449934398
plot_id,batch_id 0 73 miss% 0.04593600672929901
plot_id,batch_id 0 74 miss% 0.03649960886612407
plot_id,batch_id 0 75 miss% 0.05844872230928793
plot_id,batch_id 0 76 miss% 0.04715177829989765
plot_id,batch_id 0 77 miss% 0.0406921438982267
plot_id,batch_id 0 78 miss% 0.04010042833395762
plot_id,batch_id 0 79 miss% 0.027677978472111637
plot_id,batch_id 0 80 miss% 0.055687640962970436
plot_id,batch_id 0 81 miss% 0.03461310667229793
plot_id,batch_id 0 82 miss% 0.02672541540497904
plot_id,batch_id 0 83 miss% 0.044815697882397185
plot_id,batch_id 0 84 miss% 0.04172098814489018
plot_id,batch_id 0 85 miss% 0.041658718236348376
plot_id,batch_id 0 86 miss% 0.025529290454814297
plot_id,batch_id 0 87 miss% 0.041856910237502376
plot_id,batch_id 0 88 miss% 0.03401672165134171
plot_id,batch_id 0 89 miss% 0.040589196410118225
plot_id,batch_id 0 90 miss% 0.04091517525235821
plot_id,batch_id 0 91 miss% 0.04153707674833985
plot_id,batch_id 0 92 miss% 0.02337170602986727
plot_id,batch_id 0 93 miss% 0.034069567798050385
plot_id,batch_id 0 94 miss% 0.03640613179681711
plot_id,batch_id 0 95 miss% 0.04130831483544244
plot_id,batch_id 0 96 miss% 0.03972604375536549
plot_id,batch_id 0 97 miss% 0.0509014993306938
plot_id,batch_id 0 98 miss% 0.049965403124867205
plot_id,batch_id 0 99 miss% 0.02995237159574775
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03756592 0.01628499 0.02253181 0.02640683 0.02189757 0.04432782
 0.02906991 0.03990477 0.02756821 0.03356317 0.03643115 0.03806778
 0.03721866 0.03591954 0.03707775 0.02816566 0.0250823  0.03867592
 0.02975853 0.05773871 0.05455633 0.0212273  0.02218156 0.01901126
 0.02034528 0.04469536 0.01951364 0.02901418 0.02564321 0.02823974
 0.04207899 0.03440349 0.03199017 0.02756925 0.03600792 0.03552613
 0.06485226 0.03780173 0.03377385 0.02835017 0.06671484 0.02156354
 0.01604871 0.01713494 0.01878682 0.03934917 0.02082008 0.02071625
 0.02822121 0.02215956 0.04670555 0.01936213 0.03114967 0.02938171
 0.04099642 0.03738063 0.02936837 0.03477604 0.03079523 0.03536206
 0.03430821 0.04604563 0.04468252 0.03091639 0.03772477 0.03710838
 0.06136043 0.04421335 0.04257065 0.03172052 0.05091986 0.03108642
 0.04518326 0.04593601 0.03649961 0.05844872 0.04715178 0.04069214
 0.04010043 0.02767798 0.05568764 0.03461311 0.02672542 0.0448157
 0.04172099 0.04165872 0.02552929 0.04185691 0.03401672 0.0405892
 0.04091518 0.04153708 0.02337171 0.03406957 0.03640613 0.04130831
 0.03972604 0.0509015  0.0499654  0.02995237]
for model  101 the mean error 0.03536545809694808
all id 101 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 25 out win 5 err 0.03536545809694808
Launcher: Job 102 completed in 8043 seconds.
Launcher: Task 162 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  77489
Epoch:0, Train loss:0.489063, valid loss:0.488345
Epoch:1, Train loss:0.044756, valid loss:0.014902
Epoch:2, Train loss:0.014650, valid loss:0.005116
Epoch:3, Train loss:0.008060, valid loss:0.003589
Epoch:4, Train loss:0.006426, valid loss:0.003014
Epoch:5, Train loss:0.005338, valid loss:0.003246
Epoch:6, Train loss:0.004528, valid loss:0.002319
Epoch:7, Train loss:0.003985, valid loss:0.001842
Epoch:8, Train loss:0.003771, valid loss:0.002107
Epoch:9, Train loss:0.003407, valid loss:0.001878
Epoch:10, Train loss:0.003267, valid loss:0.001693
Epoch:11, Train loss:0.002361, valid loss:0.001180
Epoch:12, Train loss:0.002327, valid loss:0.001751
Epoch:13, Train loss:0.002254, valid loss:0.001246
Epoch:14, Train loss:0.002203, valid loss:0.001333
Epoch:15, Train loss:0.002132, valid loss:0.001255
Epoch:16, Train loss:0.002155, valid loss:0.001266
Epoch:17, Train loss:0.002034, valid loss:0.001019
Epoch:18, Train loss:0.002023, valid loss:0.000995
Epoch:19, Train loss:0.001920, valid loss:0.001963
Epoch:20, Train loss:0.001969, valid loss:0.001037
Epoch:21, Train loss:0.001507, valid loss:0.000839
Epoch:22, Train loss:0.001513, valid loss:0.000934
Epoch:23, Train loss:0.001479, valid loss:0.000823
Epoch:24, Train loss:0.001461, valid loss:0.001010
Epoch:25, Train loss:0.001473, valid loss:0.000944
Epoch:26, Train loss:0.001421, valid loss:0.000857
Epoch:27, Train loss:0.001457, valid loss:0.000902
Epoch:28, Train loss:0.001425, valid loss:0.000850
Epoch:29, Train loss:0.001400, valid loss:0.000954
Epoch:30, Train loss:0.001385, valid loss:0.000985
Epoch:31, Train loss:0.001194, valid loss:0.000830
Epoch:32, Train loss:0.001161, valid loss:0.000798
Epoch:33, Train loss:0.001164, valid loss:0.000760
Epoch:34, Train loss:0.001165, valid loss:0.000816
Epoch:35, Train loss:0.001152, valid loss:0.000795
Epoch:36, Train loss:0.001146, valid loss:0.000824
Epoch:37, Train loss:0.001116, valid loss:0.000825
Epoch:38, Train loss:0.001138, valid loss:0.000835
Epoch:39, Train loss:0.001123, valid loss:0.000773
Epoch:40, Train loss:0.001115, valid loss:0.000781
Epoch:41, Train loss:0.001014, valid loss:0.000750
Epoch:42, Train loss:0.001013, valid loss:0.000753
Epoch:43, Train loss:0.001002, valid loss:0.000755
Epoch:44, Train loss:0.001012, valid loss:0.000734
Epoch:45, Train loss:0.001004, valid loss:0.000760
Epoch:46, Train loss:0.001004, valid loss:0.000710
Epoch:47, Train loss:0.001000, valid loss:0.000778
Epoch:48, Train loss:0.000989, valid loss:0.000735
Epoch:49, Train loss:0.000987, valid loss:0.000771
Epoch:50, Train loss:0.000978, valid loss:0.000760
Epoch:51, Train loss:0.000934, valid loss:0.000735
Epoch:52, Train loss:0.000929, valid loss:0.000714
Epoch:53, Train loss:0.000926, valid loss:0.000709
Epoch:54, Train loss:0.000926, valid loss:0.000756
Epoch:55, Train loss:0.000929, valid loss:0.000710
Epoch:56, Train loss:0.000921, valid loss:0.000739
Epoch:57, Train loss:0.000929, valid loss:0.000711
Epoch:58, Train loss:0.000918, valid loss:0.000718
Epoch:59, Train loss:0.000914, valid loss:0.000731
Epoch:60, Train loss:0.000919, valid loss:0.000705
training time 7855.297528028488
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.0356677634314071
plot_id,batch_id 0 1 miss% 0.01563353956414809
plot_id,batch_id 0 2 miss% 0.022163574225204586
plot_id,batch_id 0 3 miss% 0.023774910536067066
plot_id,batch_id 0 4 miss% 0.023506638365136143
plot_id,batch_id 0 5 miss% 0.02627921998195572
plot_id,batch_id 0 6 miss% 0.025260947206627523
plot_id,batch_id 0 7 miss% 0.026261161041301384
plot_id,batch_id 0 8 miss% 0.029282105115851605
plot_id,batch_id 0 9 miss% 0.023112797466734485
plot_id,batch_id 0 10 miss% 0.028951882904719534
plot_id,batch_id 0 11 miss% 0.04163887841113294
plot_id,batch_id 0 12 miss% 0.030831087596788723
plot_id,batch_id 0 13 miss% 0.022336112662896684
plot_id,batch_id 0 14 miss% 0.0327568636589181
plot_id,batch_id 0 15 miss% 0.036711648127274
plot_id,batch_id 0 16 miss% 0.022261069644981084
plot_id,batch_id 0 17 miss% 0.04327908568330547
plot_id,batch_id 0 18 miss% 0.025304602204341595
plot_id,batch_id 0 19 miss% 0.04091766203999803
plot_id,batch_id 0 20 miss% 0.03622337777663798
plot_id,batch_id 0 21 miss% 0.026043980645249657
plot_id,batch_id 0 22 miss% 0.028009858279602305
plot_id,batch_id 0 23 miss% 0.019545514264092054
plot_id,batch_id 0 24 miss% 0.026614128530326586
plot_id,batch_id 0 25 miss% 0.033562959048374426
plot_id,batch_id 0 26 miss% 0.029583836980312642
plot_id,batch_id 0 27 miss% 0.021114326754792995
plot_id,batch_id 0 28 miss% 0.018727514961733353
plot_id,batch_id 0 29 miss% 0.02183104374396856
plot_id,batch_id 0 30 miss% 0.0455724482926249
plot_id,batch_id 0 31 miss% 0.02972752786681076
plot_id,batch_id 0 32 miss% 0.027698160606614487
plot_id,batch_id 0 33 miss% 0.03509191888744897
plot_id,batch_id 0 34 miss% 0.0253937353566131
plot_id,batch_id 0 35 miss% 0.041339189915959945
plot_id,batch_id 0 36 miss% 0.032703514946762595
plot_id,batch_id 0 37 miss% 0.020655814655837528
plot_id,batch_id 0 38 miss% 0.025864982999684587
plot_id,batch_id 0 39 miss% 0.021893064625712434
plot_id,batch_id 0 40 miss% 0.056875518329812656
plot_id,batch_id 0 41 miss% 0.021890804386269367
plot_id,batch_id 0 42 miss% 0.016601163611646468
plot_id,batch_id 0 43 miss% 0.024810985861393263
plot_id,batch_id 0 44 miss% 0.025165562126784995
plot_id,batch_id 0 45 miss% 0.023799016569288345
plot_id,batch_id 0 46 miss% 0.03481776532622558
plot_id,batch_id 0 47 miss% 0.01877756192659599
plot_id,batch_id 0 48 miss% 0.0236228543727318
plot_id,batch_id 0 49 miss% 0.02100231051095442
plot_id,batch_id 0 50 miss% 0.017609958398877856
plot_id,batch_id 0 51 miss% 0.025648514296182184
plot_id,batch_id 0 52 miss% 0.026087818599071836
plot_id,batch_id 0 53 miss% 0.008758424435080372
plot_id,batch_id 0 54 miss% 0.030999076078808703
plot_id,batch_id 0 55 miss% 0.03702949524597712
plot_id,batch_id 0 56 miss% 0.02594562288693974
plot_id,batch_id 0 57 miss% 0.01877763614455198
plot_id,batch_id 0 58 miss% 0.02825182078485976
plot_id,batch_id 0 59 miss% 0.02652112447095134
plot_id,batch_id 0 60 miss% 0.03402212317347727
plot_id,batch_id 0 61 miss% 0.027613429842879747
plot_id,batch_id 0 62 miss% 0.025921741686650814
plot_id,batch_id 0 63 miss% 0.025135150922527088
plot_id,batch_id 0 64 miss% 0.02455992819142071
plot_id,batch_id 0 65 miss% 0.05378176562034482
plot_id,batch_id 0 66 miss% 0.025607212197860805
plot_id,batch_id 0 67 miss% 0.024748298109790606
plot_id,batch_id 0 68 miss% 0.027922225271331293
plot_id,batch_id 0 69 miss% 0.01819276597918876
plot_id,batch_id 0 70 miss% 0.04030335581540718
plot_id,batch_id 0 71 miss% 0.061754374866162774
plot_id,batch_id 0 72 miss% 0.031995055539061595
plot_id,batch_id 0 73 miss% 0.023250858870781262
plot_id,batch_id 0 74 miss% 0.04262038957506432
plot_id,batch_id 0 75 miss% 0.029765893432983557
plot_id,batch_id 0 76 miss% 0.042771110905021004
plot_id,batch_id 0 77 miss% 0.030345619744049483
plot_id,batch_id 0 78 miss% 0.03260408195435059
plot_id,batch_id 0 79 miss% 0.04090184841699711
plot_id,batch_id 0 80 miss% 0.0400000270653523
plot_id,batch_id 0 81 miss% 0.019832500659591475
plot_id,batch_id 0 82 miss% 0.022797163431940826
plot_id,batch_id 0 83 miss% 0.031362057019579045
plot_id,batch_id 0 84 miss% 0.0204881517342935
plot_id,batch_id 0 85 miss% 0.04655652023180564
plot_id,batch_id 0 86 miss% 0.03364985833390379
plot_id,batch_id 0 87 miss% 0.03160619986627496
plot_id,batch_id 0 88 miss% 0.03590044736198922
plot_id,batch_id 0 89 miss% 0.035941376511166946
plot_id,batch_id 0 90 miss% 0.03722445060987869
plot_id,batch_id 0 91 miss% 0.02119322372374386
plot_id,batch_id 0 92 miss% 0.029035149543666774
plot_id,batch_id 0 93 miss% 0.03510989310354784
plot_id,batch_id 0 94 miss% 0.03176271392749464
plot_id,batch_id 0 95 miss% 0.046282235671985035
plot_id,batch_id 0 96 miss% 0.04685076052792767
plot_id,batch_id 0 97 miss% 0.04472988895700745
plot_id,batch_id 0 98 miss% 0.03056032791549014
plot_id,batch_id 0 99 miss% 0.03839058864493634
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03566776 0.01563354 0.02216357 0.02377491 0.02350664 0.02627922
 0.02526095 0.02626116 0.02928211 0.0231128  0.02895188 0.04163888
 0.03083109 0.02233611 0.03275686 0.03671165 0.02226107 0.04327909
 0.0253046  0.04091766 0.03622338 0.02604398 0.02800986 0.01954551
 0.02661413 0.03356296 0.02958384 0.02111433 0.01872751 0.02183104
 0.04557245 0.02972753 0.02769816 0.03509192 0.02539374 0.04133919
 0.03270351 0.02065581 0.02586498 0.02189306 0.05687552 0.0218908
 0.01660116 0.02481099 0.02516556 0.02379902 0.03481777 0.01877756
 0.02362285 0.02100231 0.01760996 0.02564851 0.02608782 0.00875842
 0.03099908 0.0370295  0.02594562 0.01877764 0.02825182 0.02652112
 0.03402212 0.02761343 0.02592174 0.02513515 0.02455993 0.05378177
 0.02560721 0.0247483  0.02792223 0.01819277 0.04030336 0.06175437
 0.03199506 0.02325086 0.04262039 0.02976589 0.04277111 0.03034562
 0.03260408 0.04090185 0.04000003 0.0198325  0.02279716 0.03136206
 0.02048815 0.04655652 0.03364986 0.0316062  0.03590045 0.03594138
 0.03722445 0.02119322 0.02903515 0.03510989 0.03176271 0.04628224
 0.04685076 0.04472989 0.03056033 0.03839059]
for model  103 the mean error 0.02999212248327881
all id 103 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 25 out win 4 err 0.02999212248327881
Launcher: Job 104 completed in 8055 seconds.
Launcher: Task 84 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  77489
Epoch:0, Train loss:0.648619, valid loss:0.649701
Epoch:1, Train loss:0.064064, valid loss:0.019540
Epoch:2, Train loss:0.017628, valid loss:0.006481
Epoch:3, Train loss:0.011281, valid loss:0.005399
Epoch:4, Train loss:0.008904, valid loss:0.004236
Epoch:5, Train loss:0.007109, valid loss:0.003566
Epoch:6, Train loss:0.006406, valid loss:0.002919
Epoch:7, Train loss:0.005637, valid loss:0.002501
Epoch:8, Train loss:0.005756, valid loss:0.002972
Epoch:9, Train loss:0.004998, valid loss:0.003152
Epoch:10, Train loss:0.004995, valid loss:0.003474
Epoch:11, Train loss:0.003415, valid loss:0.001705
Epoch:12, Train loss:0.003316, valid loss:0.001705
Epoch:13, Train loss:0.003278, valid loss:0.001803
Epoch:14, Train loss:0.003201, valid loss:0.001739
Epoch:15, Train loss:0.003070, valid loss:0.002102
Epoch:16, Train loss:0.003220, valid loss:0.001664
Epoch:17, Train loss:0.003099, valid loss:0.001848
Epoch:18, Train loss:0.003094, valid loss:0.001953
Epoch:19, Train loss:0.003007, valid loss:0.001602
Epoch:20, Train loss:0.002862, valid loss:0.001625
Epoch:21, Train loss:0.002106, valid loss:0.001460
Epoch:22, Train loss:0.002133, valid loss:0.001323
Epoch:23, Train loss:0.002096, valid loss:0.001299
Epoch:24, Train loss:0.002093, valid loss:0.001389
Epoch:25, Train loss:0.002074, valid loss:0.001512
Epoch:26, Train loss:0.002063, valid loss:0.001483
Epoch:27, Train loss:0.002048, valid loss:0.001372
Epoch:28, Train loss:0.001993, valid loss:0.001383
Epoch:29, Train loss:0.001907, valid loss:0.001453
Epoch:30, Train loss:0.001989, valid loss:0.001519
Epoch:31, Train loss:0.001592, valid loss:0.001131
Epoch:32, Train loss:0.001581, valid loss:0.001184
Epoch:33, Train loss:0.001556, valid loss:0.001250
Epoch:34, Train loss:0.001580, valid loss:0.001180
Epoch:35, Train loss:0.001517, valid loss:0.001194
Epoch:36, Train loss:0.001549, valid loss:0.001272
Epoch:37, Train loss:0.001502, valid loss:0.001219
Epoch:38, Train loss:0.001556, valid loss:0.001121
Epoch:39, Train loss:0.001518, valid loss:0.001127
Epoch:40, Train loss:0.001508, valid loss:0.001225
Epoch:41, Train loss:0.001319, valid loss:0.001114
Epoch:42, Train loss:0.001296, valid loss:0.001121
Epoch:43, Train loss:0.001293, valid loss:0.001079
Epoch:44, Train loss:0.001301, valid loss:0.001086
Epoch:45, Train loss:0.001294, valid loss:0.001059
Epoch:46, Train loss:0.001313, valid loss:0.001102
Epoch:47, Train loss:0.001280, valid loss:0.001159
Epoch:48, Train loss:0.001271, valid loss:0.001116
Epoch:49, Train loss:0.001284, valid loss:0.001067
Epoch:50, Train loss:0.001268, valid loss:0.001072
Epoch:51, Train loss:0.001182, valid loss:0.001063
Epoch:52, Train loss:0.001180, valid loss:0.001094
Epoch:53, Train loss:0.001171, valid loss:0.001031
Epoch:54, Train loss:0.001177, valid loss:0.001038
Epoch:55, Train loss:0.001169, valid loss:0.001085
Epoch:56, Train loss:0.001172, valid loss:0.001088
Epoch:57, Train loss:0.001170, valid loss:0.001039
Epoch:58, Train loss:0.001166, valid loss:0.001027
Epoch:59, Train loss:0.001165, valid loss:0.001116
Epoch:60, Train loss:0.001156, valid loss:0.001035
training time 7864.271919727325
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.030431011843138366
plot_id,batch_id 0 1 miss% 0.02320129588788536
plot_id,batch_id 0 2 miss% 0.024650863101910555
plot_id,batch_id 0 3 miss% 0.02748864191598778
plot_id,batch_id 0 4 miss% 0.021502242073897008
plot_id,batch_id 0 5 miss% 0.04525069050150819
plot_id,batch_id 0 6 miss% 0.0267659980181989
plot_id,batch_id 0 7 miss% 0.025101120810941988
plot_id,batch_id 0 8 miss% 0.019668767128970602
plot_id,batch_id 0 9 miss% 0.01589278704641354
plot_id,batch_id 0 10 miss% 0.046126444465550434
plot_id,batch_id 0 11 miss% 0.034684723243637174
plot_id,batch_id 0 12 miss% 0.023775575975433907
plot_id,batch_id 0 13 miss% 0.01607162162512204
plot_id,batch_id 0 14 miss% 0.029059395213662086
plot_id,batch_id 0 15 miss% 0.0405735088018967
plot_id,batch_id 0 16 miss% 0.02847933685493797
plot_id,batch_id 0 17 miss% 0.05072820944396286
plot_id,batch_id 0 18 miss% 0.03384053702364968
plot_id,batch_id 0 19 miss% 0.025131628587376994
plot_id,batch_id 0 20 miss% 0.05701864274564956
plot_id,batch_id 0 21 miss% 0.02932662244360383
plot_id,batch_id 0 22 miss% 0.03219340074723683
plot_id,batch_id 0 23 miss% 0.03637767244745882
plot_id,batch_id 0 24 miss% 0.03168093971443396
plot_id,batch_id 0 25 miss% 0.041791563900046604
plot_id,batch_id 0 26 miss% 0.0252513690061866
plot_id,batch_id 0 27 miss% 0.022455280511604465
plot_id,batch_id 0 28 miss% 0.032390251027934754
plot_id,batch_id 0 29 miss% 0.03272655576851351
plot_id,batch_id 0 30 miss% 0.042025630970483355
plot_id,batch_id 0 31 miss% 0.0373687196637907
plot_id,batch_id 0 32 miss% 0.033753812884053526
plot_id,batch_id 0 33 miss% 0.027170309590692953
plot_id,batch_id 0 34 miss% 0.0263894713399872
plot_id,batch_id 0 35 miss% 0.046288150095485155
plot_id,batch_id 0 36 miss% 0.043388480535024455
plot_id,batch_id 0 37 miss% 0.027598903858350413
plot_id,batch_id 0 38 miss% 0.020409797409803303
plot_id,batch_id 0 39 miss% 0.02644271039375012
plot_id,batch_id 0 40 miss% 0.05801253704874754
plot_id,batch_id 0 41 miss% 0.031023451793300138
plot_id,batch_id 0 42 miss% 0.02519755459883909
plot_id,batch_id 0 43 miss% 0.028843208821240595
plot_id,batch_id 0 44 miss% 0.025044918255919625
plot_id,batch_id 0 45 miss% 0.030252847792234746
plot_id,batch_id 0 46 miss% 0.02015933886934875
plot_id,batch_id 0 47 miss% 0.02808647730675644
plot_id,batch_id 0 48 miss% 0.02965321005653242
plot_id,batch_id 0 49 miss% 0.022602685075224463
plot_id,batch_id 0 50 miss% 0.03500916070867284
plot_id,batch_id 0 51 miss% 0.0278816893389917
plot_id,batch_id 0 52 miss% 0.02820752392353455
plot_id,batch_id 0 53 miss% 0.020865750923499883
plot_id,batch_id 0 54 miss% 0.03281660485959126
plot_id,batch_id 0 55 miss% 0.042794459486118416
plot_id,batch_id 0 56 miss% 0.030450566221934837
plot_id,batch_id 0 57 miss% 0.02370304872203981
plot_id,batch_id 0 58 miss% 0.02943218884591144
plot_id,batch_id 0 59 miss% 0.03972136346104689
plot_id,batch_id 0 60 miss% 0.05396523440746709
plot_id,batch_id 0 61 miss% 0.026172263955025508
plot_id,batch_id 0 62 miss% 0.03152775115173394
plot_id,batch_id 0 63 miss% 0.031701203737794746
plot_id,batch_id 0 64 miss% 0.03845614458166441
plot_id,batch_id 0 65 miss% 0.05892678242280928
plot_id,batch_id 0 66 miss% 0.04310394294474142
plot_id,batch_id 0 67 miss% 0.03429563173429238
plot_id,batch_id 0 68 miss% 0.031041123445781315
plot_id,batch_id 0 69 miss% 0.022292968292381112
plot_id,batch_id 0 70 miss% 0.06524598629231976
plot_id,batch_id 0 71 miss% 0.05392137356368771
plot_id,batch_id 0 72 miss% 0.02487121528602552
plot_id,batch_id 0 73 miss% 0.032433543668079726
plot_id,batch_id 0 74 miss% 0.024857240875773245
plot_id,batch_id 0 75 miss% 0.05297381186557864
plot_id,batch_id 0 76 miss% 0.048076395519051525
plot_id,batch_id 0 77 miss% 0.038547448721107845
plot_id,batch_id 0 78 miss% 0.03244472425078886
plot_id,batch_id 0 79 miss% 0.04163159928377119
plot_id,batch_id 0 80 miss% 0.04082812268994747
plot_id,batch_id 0 81 miss% 0.032543414141418855
plot_id,batch_id 0 82 miss% 0.028850015380572316
plot_id,batch_id 0 83 miss% 0.031956019454615084
plot_id,batch_id 0 84 miss% 0.019302175192005784
plot_id,batch_id 0 85 miss% 0.07641503512364485
plot_id,batch_id 0 86 miss% 0.024647239236086187
plot_id,batch_id 0 87 miss% 0.034290632091021234
plot_id,batch_id 0 88 miss% 0.038504956818801686
plot_id,batch_id 0 89 miss% 0.024849535732894036
plot_id,batch_id 0 90 miss% 0.04655779291236859
plot_id,batch_id 0 91 miss% 0.047827673213315776
plot_id,batch_id 0 92 miss% 0.024154245342199728
plot_id,batch_id 0 93 miss% 0.04005500140749838
plot_id,batch_id 0 94 miss% 0.02983225519630073
plot_id,batch_id 0 95 miss% 0.06032440573874386
plot_id,batch_id 0 96 miss% 0.034448992342377346
plot_id,batch_id 0 97 miss% 0.0442421975806915
plot_id,batch_id 0 98 miss% 0.024936003904306888
plot_id,batch_id 0 99 miss% 0.025366356064176906
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03043101 0.0232013  0.02465086 0.02748864 0.02150224 0.04525069
 0.026766   0.02510112 0.01966877 0.01589279 0.04612644 0.03468472
 0.02377558 0.01607162 0.0290594  0.04057351 0.02847934 0.05072821
 0.03384054 0.02513163 0.05701864 0.02932662 0.0321934  0.03637767
 0.03168094 0.04179156 0.02525137 0.02245528 0.03239025 0.03272656
 0.04202563 0.03736872 0.03375381 0.02717031 0.02638947 0.04628815
 0.04338848 0.0275989  0.0204098  0.02644271 0.05801254 0.03102345
 0.02519755 0.02884321 0.02504492 0.03025285 0.02015934 0.02808648
 0.02965321 0.02260269 0.03500916 0.02788169 0.02820752 0.02086575
 0.0328166  0.04279446 0.03045057 0.02370305 0.02943219 0.03972136
 0.05396523 0.02617226 0.03152775 0.0317012  0.03845614 0.05892678
 0.04310394 0.03429563 0.03104112 0.02229297 0.06524599 0.05392137
 0.02487122 0.03243354 0.02485724 0.05297381 0.0480764  0.03854745
 0.03244472 0.0416316  0.04082812 0.03254341 0.02885002 0.03195602
 0.01930218 0.07641504 0.02464724 0.03429063 0.03850496 0.02484954
 0.04655779 0.04782767 0.02415425 0.040055   0.02983226 0.06032441
 0.03444899 0.0442422  0.024936   0.02536636]
for model  49 the mean error 0.0338664972629052
all id 49 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 21 out win 4 err 0.0338664972629052
Launcher: Job 50 completed in 8072 seconds.
Launcher: Task 237 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  61841
Epoch:0, Train loss:0.439433, valid loss:0.435567
Epoch:1, Train loss:0.228682, valid loss:0.230567
Epoch:2, Train loss:0.220255, valid loss:0.230133
Epoch:3, Train loss:0.219341, valid loss:0.229363
Epoch:4, Train loss:0.219012, valid loss:0.229495
Epoch:5, Train loss:0.218605, valid loss:0.229145
Epoch:6, Train loss:0.218496, valid loss:0.229175
Epoch:7, Train loss:0.218299, valid loss:0.229737
Epoch:8, Train loss:0.218250, valid loss:0.229397
Epoch:9, Train loss:0.218116, valid loss:0.229203
Epoch:10, Train loss:0.218020, valid loss:0.229362
Epoch:11, Train loss:0.217222, valid loss:0.228730
Epoch:12, Train loss:0.217236, valid loss:0.228824
Epoch:13, Train loss:0.217194, valid loss:0.228557
Epoch:14, Train loss:0.217237, valid loss:0.228531
Epoch:15, Train loss:0.217115, valid loss:0.228786
Epoch:16, Train loss:0.217116, valid loss:0.228935
Epoch:17, Train loss:0.217130, valid loss:0.228655
Epoch:18, Train loss:0.217100, valid loss:0.228580
Epoch:19, Train loss:0.217026, valid loss:0.228652
Epoch:20, Train loss:0.217122, valid loss:0.228647
Epoch:21, Train loss:0.216652, valid loss:0.228408
Epoch:22, Train loss:0.216684, valid loss:0.228415
Epoch:23, Train loss:0.216660, valid loss:0.228479
Epoch:24, Train loss:0.216646, valid loss:0.228376
Epoch:25, Train loss:0.216660, valid loss:0.228367
Epoch:26, Train loss:0.216657, valid loss:0.228386
Epoch:27, Train loss:0.216628, valid loss:0.228309
Epoch:28, Train loss:0.216643, valid loss:0.228361
Epoch:29, Train loss:0.216603, valid loss:0.228364
Epoch:30, Train loss:0.216595, valid loss:0.228396
Epoch:31, Train loss:0.216399, valid loss:0.228271
Epoch:32, Train loss:0.216386, valid loss:0.228234
Epoch:33, Train loss:0.216381, valid loss:0.228313
Epoch:34, Train loss:0.216396, valid loss:0.228178
Epoch:35, Train loss:0.216371, valid loss:0.228269
Epoch:36, Train loss:0.216405, valid loss:0.228254
Epoch:37, Train loss:0.216377, valid loss:0.228207
Epoch:38, Train loss:0.216378, valid loss:0.228737
Epoch:39, Train loss:0.216382, valid loss:0.228257
Epoch:40, Train loss:0.216359, valid loss:0.228271
Epoch:41, Train loss:0.216271, valid loss:0.228176
Epoch:42, Train loss:0.216250, valid loss:0.228195
Epoch:43, Train loss:0.216254, valid loss:0.228228
Epoch:44, Train loss:0.216242, valid loss:0.228322
Epoch:45, Train loss:0.216253, valid loss:0.228180
Epoch:46, Train loss:0.216253, valid loss:0.228157
Epoch:47, Train loss:0.216247, valid loss:0.228210
Epoch:48, Train loss:0.216241, valid loss:0.228399
Epoch:49, Train loss:0.216243, valid loss:0.228183
Epoch:50, Train loss:0.216232, valid loss:0.228265
Epoch:51, Train loss:0.216177, valid loss:0.228159
Epoch:52, Train loss:0.216179, valid loss:0.228166
Epoch:53, Train loss:0.216177, valid loss:0.228180
Epoch:54, Train loss:0.216178, valid loss:0.228156
Epoch:55, Train loss:0.216181, valid loss:0.228152
Epoch:56, Train loss:0.216173, valid loss:0.228166
Epoch:57, Train loss:0.216176, valid loss:0.228169
Epoch:58, Train loss:0.216169, valid loss:0.228145
Epoch:59, Train loss:0.216168, valid loss:0.228157
Epoch:60, Train loss:0.216183, valid loss:0.228139
training time 7909.035856485367
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.6556108850758067
plot_id,batch_id 0 1 miss% 0.7438535510525107
plot_id,batch_id 0 2 miss% 0.760801278345917
plot_id,batch_id 0 3 miss% 0.7701571940705904
plot_id,batch_id 0 4 miss% 0.7681042744803651
plot_id,batch_id 0 5 miss% 0.6692996357337896
plot_id,batch_id 0 6 miss% 0.7418613582594888
plot_id,batch_id 0 7 miss% 0.7561724034470128
plot_id,batch_id 0 8 miss% 0.7656046549347624
plot_id,batch_id 0 9 miss% 0.7717578013054291
plot_id,batch_id 0 10 miss% 0.6297341831765695
plot_id,batch_id 0 11 miss% 0.7447234401645707
plot_id,batch_id 0 12 miss% 0.7536434068952443
plot_id,batch_id 0 13 miss% 0.7613963779510617
plot_id,batch_id 0 14 miss% 0.7706911504463256
plot_id,batch_id 0 15 miss% 0.648551852084151
plot_id,batch_id 0 16 miss% 0.7341506703435106
plot_id,batch_id 0 17 miss% 0.7598490128594184
plot_id,batch_id 0 18 miss% 0.7639416467653961
plot_id,batch_id 0 19 miss% 0.76780184898299
plot_id,batch_id 0 20 miss% 0.7088285223137131
plot_id,batch_id 0 21 miss% 0.761715962291854
plot_id,batch_id 0 22 miss% 0.7720820400628358
plot_id,batch_id 0 23 miss% 0.7748951042719633
plot_id,batch_id 0 24 miss% 0.7775653708516963
plot_id,batch_id 0 25 miss% 0.698611288217826
plot_id,batch_id 0 26 miss% 0.7566199548090655
plot_id,batch_id 0 27 miss% 0.7692806254976894
plot_id,batch_id 0 28 miss% 0.7677583804744147
plot_id,batch_id 0 29 miss% 0.7785047025848888
plot_id,batch_id 0 30 miss% 0.6974020009676354
plot_id,batch_id 0 31 miss% 0.7538336772913178
plot_id,batch_id 0 32 miss% 0.7634060116078837
plot_id,batch_id 0 33 miss% 0.768925368357069
plot_id,batch_id 0 34 miss% 0.7708421258400658
plot_id,batch_id 0 35 miss% 0.6894537236400888
plot_id,batch_id 0 36 miss% 0.7629980170894555
plot_id,batch_id 0 37 miss% 0.7638425736776132
plot_id,batch_id 0 38 miss% 0.7735328450959957
plot_id,batch_id 0 39 miss% 0.7725140884434756
plot_id,batch_id 0 40 miss% 0.7307159686304248
plot_id,batch_id 0 41 miss% 0.7684795050754614
plot_id,batch_id 0 42 miss% 0.7707599477805158
plot_id,batch_id 0 43 miss% 0.777316762657987
plot_id,batch_id 0 44 miss% 0.7797422397628685
plot_id,batch_id 0 45 miss% 0.7303508062506302
plot_id,batch_id 0 46 miss% 0.7667112382157032
plot_id,batch_id 0 47 miss% 0.7701849727856066
plot_id,batch_id 0 48 miss% 0.775726603265906
plot_id,batch_id 0 49 miss% 0.7803106052278523
plot_id,batch_id 0 50 miss% 0.737737821180519
plot_id,batch_id 0 51 miss% 0.7620244219016977
plot_id,batch_id 0 52 miss% 0.7720786863532195
plot_id,batch_id 0 53 miss% 0.776367810125781
plot_id,batch_id 0 54 miss% 0.7836754971279761
plot_id,batch_id 0 55 miss% 0.7358033639736534
plot_id,batch_id 0 56 miss% 0.7644648154281086
plot_id,batch_id 0 57 miss% 0.7710375188049149
plot_id,batch_id 0 58 miss% 0.7762034583529593
plot_id,batch_id 0 59 miss% 0.7822697071093717
plot_id,batch_id 0 60 miss% 0.5632711063644836
plot_id,batch_id 0 61 miss% 0.7054632562282628
plot_id,batch_id 0 62 miss% 0.732068007843482
plot_id,batch_id 0 63 miss% 0.7563804228079408
plot_id,batch_id 0 64 miss% 0.7597063001867218
plot_id,batch_id 0 65 miss% 0.5528646599196397
plot_id,batch_id 0 66 miss% 0.6992128335193954
plot_id,batch_id 0 67 miss% 0.7219990052953515
plot_id,batch_id 0 68 miss% 0.7498794792591308
plot_id,batch_id 0 69 miss% 0.7512933727414054
plot_id,batch_id 0 70 miss% 0.5334254831583488
plot_id,batch_id 0 71 miss% 0.6956243234047181
plot_id,batch_id 0 72 miss% 0.7158514916337304
plot_id,batch_id 0 73 miss% 0.7375102049709688
plot_id,batch_id 0 74 miss% 0.7470372293076231
plot_id,batch_id 0 75 miss% 0.5150308840122475
plot_id,batch_id 0 76 miss% 0.6526722742293193
plot_id,batch_id 0 77 miss% 0.6968463943713579
plot_id,batch_id 0 78 miss% 0.7345522824982951
plot_id,batch_id 0 79 miss% 0.7444911773750555
plot_id,batch_id 0 80 miss% 0.5942128817278705
plot_id,batch_id 0 81 miss% 0.7251054569636711
plot_id,batch_id 0 82 miss% 0.744228273838362
plot_id,batch_id 0 83 miss% 0.7560120805067791
plot_id,batch_id 0 84 miss% 0.7636692421340395
plot_id,batch_id 0 85 miss% 0.5948990653412574
plot_id,batch_id 0 86 miss% 0.7171265610720234
plot_id,batch_id 0 87 miss% 0.742554016997614
plot_id,batch_id 0 88 miss% 0.7584103704560893
plot_id,batch_id 0 89 miss% 0.7629011615203196
plot_id,batch_id 0 90 miss% 0.5597824694332616
plot_id,batch_id 0 91 miss% 0.7120933767422758
plot_id,batch_id 0 92 miss% 0.7344094533832103
plot_id,batch_id 0 93 miss% 0.746805205231444
plot_id,batch_id 0 94 miss% 0.7594895248559778
plot_id,batch_id 0 95 miss% 0.5512410840488002
plot_id,batch_id 0 96 miss% 0.6983307076093899
plot_id,batch_id 0 97 miss% 0.7331575074106942
plot_id,batch_id 0 98 miss% 0.7440340151179585
plot_id,batch_id 0 99 miss% 0.7511656611410875
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.65561089 0.74385355 0.76080128 0.77015719 0.76810427 0.66929964
 0.74186136 0.7561724  0.76560465 0.7717578  0.62973418 0.74472344
 0.75364341 0.76139638 0.77069115 0.64855185 0.73415067 0.75984901
 0.76394165 0.76780185 0.70882852 0.76171596 0.77208204 0.7748951
 0.77756537 0.69861129 0.75661995 0.76928063 0.76775838 0.7785047
 0.697402   0.75383368 0.76340601 0.76892537 0.77084213 0.68945372
 0.76299802 0.76384257 0.77353285 0.77251409 0.73071597 0.76847951
 0.77075995 0.77731676 0.77974224 0.73035081 0.76671124 0.77018497
 0.7757266  0.78031061 0.73773782 0.76202442 0.77207869 0.77636781
 0.7836755  0.73580336 0.76446482 0.77103752 0.77620346 0.78226971
 0.56327111 0.70546326 0.73206801 0.75638042 0.7597063  0.55286466
 0.69921283 0.72199901 0.74987948 0.75129337 0.53342548 0.69562432
 0.71585149 0.7375102  0.74703723 0.51503088 0.65267227 0.69684639
 0.73455228 0.74449118 0.59421288 0.72510546 0.74422827 0.75601208
 0.76366924 0.59489907 0.71712656 0.74255402 0.75841037 0.76290116
 0.55978247 0.71209338 0.73440945 0.74680521 0.75948952 0.55124108
 0.69833071 0.73315751 0.74403402 0.75116566]
for model  229 the mean error 0.7304708906443221
all id 229 hidden_dim 24 learning_rate 0.02 num_layers 4 frames 31 out win 4 err 0.7304708906443221
Launcher: Job 230 completed in 8082 seconds.
Launcher: Task 50 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  79249
Epoch:0, Train loss:0.430215, valid loss:0.408020
Epoch:1, Train loss:0.032477, valid loss:0.006883
Epoch:2, Train loss:0.010327, valid loss:0.004681
Epoch:3, Train loss:0.008029, valid loss:0.004956
Epoch:4, Train loss:0.006562, valid loss:0.003410
Epoch:5, Train loss:0.005489, valid loss:0.003186
Epoch:6, Train loss:0.005009, valid loss:0.002193
Epoch:7, Train loss:0.004558, valid loss:0.002348
Epoch:8, Train loss:0.004269, valid loss:0.002199
Epoch:9, Train loss:0.004088, valid loss:0.002401
Epoch:10, Train loss:0.003795, valid loss:0.002464
Epoch:11, Train loss:0.002804, valid loss:0.001669
Epoch:12, Train loss:0.002764, valid loss:0.001601
Epoch:13, Train loss:0.002672, valid loss:0.001525
Epoch:14, Train loss:0.002623, valid loss:0.001507
Epoch:15, Train loss:0.002551, valid loss:0.001470
Epoch:16, Train loss:0.002498, valid loss:0.001486
Epoch:17, Train loss:0.002424, valid loss:0.001407
Epoch:18, Train loss:0.002372, valid loss:0.001546
Epoch:19, Train loss:0.002359, valid loss:0.001295
Epoch:20, Train loss:0.002240, valid loss:0.001384
Epoch:21, Train loss:0.001782, valid loss:0.001089
Epoch:22, Train loss:0.001768, valid loss:0.001174
Epoch:23, Train loss:0.001758, valid loss:0.001072
Epoch:24, Train loss:0.001702, valid loss:0.001343
Epoch:25, Train loss:0.001740, valid loss:0.001224
Epoch:26, Train loss:0.001722, valid loss:0.001168
Epoch:27, Train loss:0.001700, valid loss:0.001191
Epoch:28, Train loss:0.001657, valid loss:0.001054
Epoch:29, Train loss:0.001660, valid loss:0.001272
Epoch:30, Train loss:0.001633, valid loss:0.001117
Epoch:31, Train loss:0.001399, valid loss:0.000980
Epoch:32, Train loss:0.001378, valid loss:0.001053
Epoch:33, Train loss:0.001383, valid loss:0.001045
Epoch:34, Train loss:0.001357, valid loss:0.001058
Epoch:35, Train loss:0.001352, valid loss:0.000949
Epoch:36, Train loss:0.001357, valid loss:0.001088
Epoch:37, Train loss:0.001337, valid loss:0.001041
Epoch:38, Train loss:0.001341, valid loss:0.000989
Epoch:39, Train loss:0.001342, valid loss:0.000956
Epoch:40, Train loss:0.001326, valid loss:0.001010
Epoch:41, Train loss:0.001191, valid loss:0.000970
Epoch:42, Train loss:0.001195, valid loss:0.000946
Epoch:43, Train loss:0.001185, valid loss:0.000931
Epoch:44, Train loss:0.001180, valid loss:0.000953
Epoch:45, Train loss:0.001177, valid loss:0.000968
Epoch:46, Train loss:0.001171, valid loss:0.000918
Epoch:47, Train loss:0.001169, valid loss:0.000939
Epoch:48, Train loss:0.001161, valid loss:0.000953
Epoch:49, Train loss:0.001161, valid loss:0.000993
Epoch:50, Train loss:0.001156, valid loss:0.000931
Epoch:51, Train loss:0.001106, valid loss:0.000911
Epoch:52, Train loss:0.001093, valid loss:0.000902
Epoch:53, Train loss:0.001088, valid loss:0.000926
Epoch:54, Train loss:0.001099, valid loss:0.000919
Epoch:55, Train loss:0.001087, valid loss:0.000919
Epoch:56, Train loss:0.001086, valid loss:0.000948
Epoch:57, Train loss:0.001086, valid loss:0.000910
Epoch:58, Train loss:0.001083, valid loss:0.000887
Epoch:59, Train loss:0.001073, valid loss:0.000929
Epoch:60, Train loss:0.001084, valid loss:0.000922
training time 7916.510545492172
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02711619919518831
plot_id,batch_id 0 1 miss% 0.02812872256311112
plot_id,batch_id 0 2 miss% 0.0308448561903004
plot_id,batch_id 0 3 miss% 0.019276010661790584
plot_id,batch_id 0 4 miss% 0.01752427849607937
plot_id,batch_id 0 5 miss% 0.029836951743493247
plot_id,batch_id 0 6 miss% 0.025104650952796476
plot_id,batch_id 0 7 miss% 0.02450046853299766
plot_id,batch_id 0 8 miss% 0.02496643544492651
plot_id,batch_id 0 9 miss% 0.017767853620935664
plot_id,batch_id 0 10 miss% 0.048061416421370046
plot_id,batch_id 0 11 miss% 0.04890096106328469
plot_id,batch_id 0 12 miss% 0.03225222260822822
plot_id,batch_id 0 13 miss% 0.043737730299455
plot_id,batch_id 0 14 miss% 0.02956570497002111
plot_id,batch_id 0 15 miss% 0.04646157762488939
plot_id,batch_id 0 16 miss% 0.026912067436759043
plot_id,batch_id 0 17 miss% 0.03301619458614544
plot_id,batch_id 0 18 miss% 0.033232913947736344
plot_id,batch_id 0 19 miss% 0.038834724298253846
plot_id,batch_id 0 20 miss% 0.053901411902428246
plot_id,batch_id 0 21 miss% 0.035054288347999424
plot_id,batch_id 0 22 miss% 0.033745740052294936
plot_id,batch_id 0 23 miss% 0.021208699514451514
plot_id,batch_id 0 24 miss% 0.024410710198532796
plot_id,batch_id 0 25 miss% 0.035151959456170105
plot_id,batch_id 0 26 miss% 0.020293392112159282
plot_id,batch_id 0 27 miss% 0.025278802082623855
plot_id,batch_id 0 28 miss% 0.016742211509793874
plot_id,batch_id 0 29 miss% 0.023169598821542896
plot_id,batch_id 0 30 miss% 0.033773153294650825
plot_id,batch_id 0 31 miss% 0.03236966338218101
plot_id,batch_id 0 32 miss% 0.040874526854913266
plot_id,batch_id 0 33 miss% 0.02662791986350036
plot_id,batch_id 0 34 miss% 0.02269842519790767
plot_id,batch_id 0 35 miss% 0.0326845861511682
plot_id,batch_id 0 36 miss% 0.0552285210834341
plot_id,batch_id 0 37 miss% 0.04365688321463982
plot_id,batch_id 0 38 miss% 0.0344827273086593
plot_id,batch_id 0 39 miss% 0.01338325956261341
plot_id,batch_id 0 40 miss% 0.08371316857897959
plot_id,batch_id 0 41 miss% 0.027858441322999857
plot_id,batch_id 0 42 miss% 0.018671922233129832
plot_id,batch_id 0 43 miss% 0.027335820441895056
plot_id,batch_id 0 44 miss% 0.016242849129143144
plot_id,batch_id 0 45 miss% 0.035636557451089214
plot_id,batch_id 0 46 miss% 0.024335197951243524
plot_id,batch_id 0 47 miss% 0.02221992743329445
plot_id,batch_id 0 48 miss% 0.01557214495635308
plot_id,batch_id 0 49 miss% 0.02345559344858137
plot_id,batch_id 0 50 miss% 0.027252804217412392
plot_id,batch_id 0 51 miss% 0.013521661768018568
plot_id,batch_id 0 52 miss% 0.02798215962032948
plot_id,batch_id 0 53 miss% 0.01589068929522497
plot_id,batch_id 0 54 miss% 0.02019826545013345
plot_id,batch_id 0 55 miss% 0.03809469000104505
plot_id,batch_id 0 56 miss% 0.025476082039242908
plot_id,batch_id 0 57 miss% 0.02042210541906244
plot_id,batch_id 0 58 miss% 0.02265302148581188
plot_id,batch_id 0 59 miss% 0.01788177989844523
plot_id,batch_id 0 60 miss% 0.04149064495056794
plot_id,batch_id 0 61 miss% 0.034827630449812665
plot_id,batch_id 0 62 miss% 0.02477849630509992
plot_id,batch_id 0 63 miss% 0.0369364270856554
plot_id,batch_id 0 64 miss% 0.028370744574055147
plot_id,batch_id 0 65 miss% 0.057603442934030975
plot_id,batch_id 0 66 miss% 0.04803815135567213
plot_id,batch_id 0 67 miss% 0.028804219909403333
plot_id,batch_id 0 68 miss% 0.03223246005248705
plot_id,batch_id 0 69 miss% 0.036973747530316604
plot_id,batch_id 0 70 miss% 0.04568351850037539
plot_id,batch_id 0 71 miss% 0.042196898772177104
plot_id,batch_id 0 72 miss% 0.029399326054375428
plot_id,batch_id 0 73 miss% 0.034755998089045184
plot_id,batch_id 0 74 miss% 0.03345436962835584
plot_id,batch_id 0 75 miss% 0.040236231842714944
plot_id,batch_id 0 76 miss% 0.04298557515694575
plot_id,batch_id 0 77 miss% 0.03017123596326192
plot_id,batch_id 0 78 miss% 0.04758762831070275
plot_id,batch_id 0 79 miss% 0.03971824981307989
plot_id,batch_id 0 80 miss% 0.03408729704350921
plot_id,batch_id 0 81 miss% 0.03654384108427202
plot_id,batch_id 0 82 miss% 0.023896691910234615
plot_id,batch_id 0 83 miss% 0.02128447892995024
plot_id,batch_id 0 84 miss% 0.02472179497390042
plot_id,batch_id 0 85 miss% 0.05307319650715179
plot_id,batch_id 0 86 miss% 0.026544008067794354
plot_id,batch_id 0 87 miss% 0.024931694888995864
plot_id,batch_id 0 88 miss% 0.034400519050640364
plot_id,batch_id 0 89 miss% 0.02633691857628955
plot_id,batch_id 0 90 miss% 0.035070364658266115
plot_id,batch_id 0 91 miss% 0.028428014891425244
plot_id,batch_id 0 92 miss% 0.03549998123840229
plot_id,batch_id 0 93 miss% 0.044739507666839616
plot_id,batch_id 0 94 miss% 0.025473538286819863
plot_id,batch_id 0 95 miss% 0.04574952140825106
plot_id,batch_id 0 96 miss% 0.03140082683586474
plot_id,batch_id 0 97 miss% 0.06595909773715128
plot_id,batch_id 0 98 miss% 0.02920384527754829
plot_id,batch_id 0 99 miss% 0.03349776311161445
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0271162  0.02812872 0.03084486 0.01927601 0.01752428 0.02983695
 0.02510465 0.02450047 0.02496644 0.01776785 0.04806142 0.04890096
 0.03225222 0.04373773 0.0295657  0.04646158 0.02691207 0.03301619
 0.03323291 0.03883472 0.05390141 0.03505429 0.03374574 0.0212087
 0.02441071 0.03515196 0.02029339 0.0252788  0.01674221 0.0231696
 0.03377315 0.03236966 0.04087453 0.02662792 0.02269843 0.03268459
 0.05522852 0.04365688 0.03448273 0.01338326 0.08371317 0.02785844
 0.01867192 0.02733582 0.01624285 0.03563656 0.0243352  0.02221993
 0.01557214 0.02345559 0.0272528  0.01352166 0.02798216 0.01589069
 0.02019827 0.03809469 0.02547608 0.02042211 0.02265302 0.01788178
 0.04149064 0.03482763 0.0247785  0.03693643 0.02837074 0.05760344
 0.04803815 0.02880422 0.03223246 0.03697375 0.04568352 0.0421969
 0.02939933 0.034756   0.03345437 0.04023623 0.04298558 0.03017124
 0.04758763 0.03971825 0.0340873  0.03654384 0.02389669 0.02128448
 0.02472179 0.0530732  0.02654401 0.02493169 0.03440052 0.02633692
 0.03507036 0.02842801 0.03549998 0.04473951 0.02547354 0.04574952
 0.03140083 0.0659591  0.02920385 0.03349776]
for model  89 the mean error 0.032122812021339195
all id 89 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 25 out win 5 err 0.032122812021339195
Launcher: Job 90 completed in 8120 seconds.
Launcher: Task 26 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  79249
Epoch:0, Train loss:0.307377, valid loss:0.282084
Epoch:1, Train loss:0.019042, valid loss:0.003303
Epoch:2, Train loss:0.005624, valid loss:0.002399
Epoch:3, Train loss:0.004210, valid loss:0.002348
Epoch:4, Train loss:0.003425, valid loss:0.001892
Epoch:5, Train loss:0.002924, valid loss:0.001866
Epoch:6, Train loss:0.002641, valid loss:0.001643
Epoch:7, Train loss:0.002411, valid loss:0.001266
Epoch:8, Train loss:0.002238, valid loss:0.001585
Epoch:9, Train loss:0.002122, valid loss:0.001194
Epoch:10, Train loss:0.002054, valid loss:0.001282
Epoch:11, Train loss:0.001528, valid loss:0.000900
Epoch:12, Train loss:0.001495, valid loss:0.000909
Epoch:13, Train loss:0.001472, valid loss:0.000864
Epoch:14, Train loss:0.001452, valid loss:0.000853
Epoch:15, Train loss:0.001422, valid loss:0.000977
Epoch:16, Train loss:0.001364, valid loss:0.000908
Epoch:17, Train loss:0.001361, valid loss:0.000828
Epoch:18, Train loss:0.001305, valid loss:0.000697
Epoch:19, Train loss:0.001294, valid loss:0.001146
Epoch:20, Train loss:0.001282, valid loss:0.000870
Epoch:21, Train loss:0.001017, valid loss:0.000747
Epoch:22, Train loss:0.001018, valid loss:0.000726
Epoch:23, Train loss:0.000971, valid loss:0.000677
Epoch:24, Train loss:0.000987, valid loss:0.000735
Epoch:25, Train loss:0.001011, valid loss:0.000651
Epoch:26, Train loss:0.000961, valid loss:0.000790
Epoch:27, Train loss:0.000959, valid loss:0.000692
Epoch:28, Train loss:0.000946, valid loss:0.000656
Epoch:29, Train loss:0.000945, valid loss:0.000635
Epoch:30, Train loss:0.000933, valid loss:0.000679
Epoch:31, Train loss:0.000814, valid loss:0.000646
Epoch:32, Train loss:0.000805, valid loss:0.000590
Epoch:33, Train loss:0.000803, valid loss:0.000601
Epoch:34, Train loss:0.000800, valid loss:0.000584
Epoch:35, Train loss:0.000797, valid loss:0.000586
Epoch:36, Train loss:0.000778, valid loss:0.000587
Epoch:37, Train loss:0.000782, valid loss:0.000586
Epoch:38, Train loss:0.000786, valid loss:0.000579
Epoch:39, Train loss:0.000778, valid loss:0.000720
Epoch:40, Train loss:0.000778, valid loss:0.000579
Epoch:41, Train loss:0.000706, valid loss:0.000565
Epoch:42, Train loss:0.000707, valid loss:0.000537
Epoch:43, Train loss:0.000705, valid loss:0.000539
Epoch:44, Train loss:0.000701, valid loss:0.000543
Epoch:45, Train loss:0.000698, valid loss:0.000555
Epoch:46, Train loss:0.000697, valid loss:0.000538
Epoch:47, Train loss:0.000704, valid loss:0.000534
Epoch:48, Train loss:0.000699, valid loss:0.000536
Epoch:49, Train loss:0.000689, valid loss:0.000552
Epoch:50, Train loss:0.000686, valid loss:0.000553
Epoch:51, Train loss:0.000664, valid loss:0.000534
Epoch:52, Train loss:0.000659, valid loss:0.000542
Epoch:53, Train loss:0.000657, valid loss:0.000526
Epoch:54, Train loss:0.000658, valid loss:0.000522
Epoch:55, Train loss:0.000654, valid loss:0.000525
Epoch:56, Train loss:0.000655, valid loss:0.000518
Epoch:57, Train loss:0.000654, valid loss:0.000529
Epoch:58, Train loss:0.000652, valid loss:0.000559
Epoch:59, Train loss:0.000656, valid loss:0.000531
Epoch:60, Train loss:0.000648, valid loss:0.000526
training time 8019.728028535843
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.03353774793319372
plot_id,batch_id 0 1 miss% 0.01824442475804421
plot_id,batch_id 0 2 miss% 0.029804074088778932
plot_id,batch_id 0 3 miss% 0.02752684178912976
plot_id,batch_id 0 4 miss% 0.0372573830634736
plot_id,batch_id 0 5 miss% 0.0560035933743809
plot_id,batch_id 0 6 miss% 0.02348795989575386
plot_id,batch_id 0 7 miss% 0.016156928247914815
plot_id,batch_id 0 8 miss% 0.03790957874843435
plot_id,batch_id 0 9 miss% 0.014596623074086358
plot_id,batch_id 0 10 miss% 0.026035830653798733
plot_id,batch_id 0 11 miss% 0.03451303331755108
plot_id,batch_id 0 12 miss% 0.027264717484525157
plot_id,batch_id 0 13 miss% 0.022757754371966876
plot_id,batch_id 0 14 miss% 0.023776276704872007
plot_id,batch_id 0 15 miss% 0.04015374500691076
plot_id,batch_id 0 16 miss% 0.023499229869676567
plot_id,batch_id 0 17 miss% 0.03184613645624391
plot_id,batch_id 0 18 miss% 0.024927483414212696
plot_id,batch_id 0 19 miss% 0.01936068995247655
plot_id,batch_id 0 20 miss% 0.033867126552148394
plot_id,batch_id 0 21 miss% 0.01988256102755024
plot_id,batch_id 0 22 miss% 0.030839651707910257
plot_id,batch_id 0 23 miss% 0.02316277256812844
plot_id,batch_id 0 24 miss% 0.03438438129690858
plot_id,batch_id 0 25 miss% 0.04589739684045898
plot_id,batch_id 0 26 miss% 0.02096221795733833
plot_id,batch_id 0 27 miss% 0.0232421829731472
plot_id,batch_id 0 28 miss% 0.020821106221058275
plot_id,batch_id 0 29 miss% 0.02763483808897012
plot_id,batch_id 0 30 miss% 0.05318801133623476
plot_id,batch_id 0 31 miss% 0.028846249972859735
plot_id,batch_id 0 32 miss% 0.02117716548494967
plot_id,batch_id 0 33 miss% 0.028499518750707057
plot_id,batch_id 0 34 miss% 0.028734607250991456
plot_id,batch_id 0 35 miss% 0.035537569216912236
plot_id,batch_id 0 36 miss% 0.029960441422552275
plot_id,batch_id 0 37 miss% 0.039929394091022336
plot_id,batch_id 0 38 miss% 0.03852240134832589
plot_id,batch_id 0 39 miss% 0.02519219426284177
plot_id,batch_id 0 40 miss% 0.06575748625104816
plot_id,batch_id 0 41 miss% 0.025027116515743696
plot_id,batch_id 0 42 miss% 0.027984169696999715
plot_id,batch_id 0 43 miss% 0.029676369397381535
plot_id,batch_id 0 44 miss% 0.016038113328905527
plot_id,batch_id 0 45 miss% 0.037808453777086685
plot_id,batch_id 0 46 miss% 0.039329214386311845
plot_id,batch_id 0 47 miss% 0.026041227092230126
plot_id,batch_id 0 48 miss% 0.017524021062469638
plot_id,batch_id 0 49 miss% 0.027377333158519337
plot_id,batch_id 0 50 miss% 0.03635181166864095
plot_id,batch_id 0 51 miss% 0.02631461177841121
plot_id,batch_id 0 52 miss% 0.021719288222398905
plot_id,batch_id 0 53 miss% 0.01980492346418249
plot_id,batch_id 0 54 miss% 0.031340382779442615
plot_id,batch_id 0 55 miss% 0.03988494758676959
plot_id,batch_id 0 56 miss% 0.017334073502623597
plot_id,batch_id 0 57 miss% 0.022393989021469957
plot_id,batch_id 0 58 miss% 0.02363737653755564
plot_id,batch_id 0 59 miss% 0.027257239072812692
plot_id,batch_id 0 60 miss% 0.03348387322146348
plot_id,batch_id 0 61 miss% 0.027822330685429723
plot_id,batch_id 0 62 miss% 0.02772208635123875
plot_id,batch_id 0 63 miss% 0.028780399723323635
plot_id,batch_id 0 64 miss% 0.029705976546727318
plot_id,batch_id 0 65 miss% 0.0405899222703126
plot_id,batch_id 0 66 miss% 0.03060442960655529
plot_id,batch_id 0 67 miss% 0.03631586619791503
plot_id,batch_id 0 68 miss% 0.0297325480725216
plot_id,batch_id 0 69 miss% 0.0305805440207735
plot_id,batch_id 0 70 miss% 0.03826575005804317
plot_id,batch_id 0 71 miss% 0.04206517186884457
plot_id,batch_id 0 72 miss% 0.030678933237430945
plot_id,batch_id 0 73 miss% 0.038102870120997584
plot_id,batch_id 0 74 miss% 0.0302312044385321
plot_id,batch_id 0 75 miss% 0.03511472702212017
plot_id,batch_id 0 76 miss% 0.05227407295358543
plot_id,batch_id 0 77 miss% 0.021699419380363193
plot_id,batch_id 0 78 miss% 0.03772641471735811
plot_id,batch_id 0 79 miss% 0.03222936150197011
plot_id,batch_id 0 80 miss% 0.05161178204672701
plot_id,batch_id 0 81 miss% 0.02376151571972771
plot_id,batch_id 0 82 miss% 0.026921513029803206
plot_id,batch_id 0 83 miss% 0.028234177853328015
plot_id,batch_id 0 84 miss% 0.022019910874041423
plot_id,batch_id 0 85 miss% 0.03852365376126204
plot_id,batch_id 0 86 miss% 0.033335295511450674
plot_id,batch_id 0 87 miss% 0.03344714939972054
plot_id,batch_id 0 88 miss% 0.03169611166787898
plot_id,batch_id 0 89 miss% 0.02386655218530642
plot_id,batch_id 0 90 miss% 0.03206169476982757
plot_id,batch_id 0 91 miss% 0.03157161863803606
plot_id,batch_id 0 92 miss% 0.033036343531418405
plot_id,batch_id 0 93 miss% 0.03312879311042223
plot_id,batch_id 0 94 miss% 0.03353238245386889
plot_id,batch_id 0 95 miss% 0.03523851645686821
plot_id,batch_id 0 96 miss% 0.045241514727987195
plot_id,batch_id 0 97 miss% 0.0472794414613847
plot_id,batch_id 0 98 miss% 0.02276472931311847
plot_id,batch_id 0 99 miss% 0.027292121919235312
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03353775 0.01824442 0.02980407 0.02752684 0.03725738 0.05600359
 0.02348796 0.01615693 0.03790958 0.01459662 0.02603583 0.03451303
 0.02726472 0.02275775 0.02377628 0.04015375 0.02349923 0.03184614
 0.02492748 0.01936069 0.03386713 0.01988256 0.03083965 0.02316277
 0.03438438 0.0458974  0.02096222 0.02324218 0.02082111 0.02763484
 0.05318801 0.02884625 0.02117717 0.02849952 0.02873461 0.03553757
 0.02996044 0.03992939 0.0385224  0.02519219 0.06575749 0.02502712
 0.02798417 0.02967637 0.01603811 0.03780845 0.03932921 0.02604123
 0.01752402 0.02737733 0.03635181 0.02631461 0.02171929 0.01980492
 0.03134038 0.03988495 0.01733407 0.02239399 0.02363738 0.02725724
 0.03348387 0.02782233 0.02772209 0.0287804  0.02970598 0.04058992
 0.03060443 0.03631587 0.02973255 0.03058054 0.03826575 0.04206517
 0.03067893 0.03810287 0.0302312  0.03511473 0.05227407 0.02169942
 0.03772641 0.03222936 0.05161178 0.02376152 0.02692151 0.02823418
 0.02201991 0.03852365 0.0333353  0.03344715 0.03169611 0.02386655
 0.03206169 0.03157162 0.03303634 0.03312879 0.03353238 0.03523852
 0.04524151 0.04727944 0.02276473 0.02729212]
for model  169 the mean error 0.030858347093043643
all id 169 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 31 out win 4 err 0.030858347093043643
Launcher: Job 170 completed in 8211 seconds.
Launcher: Task 117 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  79249
Epoch:0, Train loss:0.307377, valid loss:0.282084
Epoch:1, Train loss:0.026620, valid loss:0.004947
Epoch:2, Train loss:0.006555, valid loss:0.003222
Epoch:3, Train loss:0.004814, valid loss:0.003365
Epoch:4, Train loss:0.004059, valid loss:0.002370
Epoch:5, Train loss:0.003609, valid loss:0.001827
Epoch:6, Train loss:0.003323, valid loss:0.001488
Epoch:7, Train loss:0.003171, valid loss:0.001990
Epoch:8, Train loss:0.003015, valid loss:0.001660
Epoch:9, Train loss:0.002897, valid loss:0.001457
Epoch:10, Train loss:0.002768, valid loss:0.001818
Epoch:11, Train loss:0.001889, valid loss:0.001030
Epoch:12, Train loss:0.001824, valid loss:0.001326
Epoch:13, Train loss:0.001845, valid loss:0.001821
Epoch:14, Train loss:0.001819, valid loss:0.001300
Epoch:15, Train loss:0.001759, valid loss:0.001104
Epoch:16, Train loss:0.001764, valid loss:0.000897
Epoch:17, Train loss:0.001685, valid loss:0.001128
Epoch:18, Train loss:0.001691, valid loss:0.000926
Epoch:19, Train loss:0.001677, valid loss:0.001003
Epoch:20, Train loss:0.001663, valid loss:0.000860
Epoch:21, Train loss:0.001178, valid loss:0.000843
Epoch:22, Train loss:0.001187, valid loss:0.000770
Epoch:23, Train loss:0.001174, valid loss:0.000917
Epoch:24, Train loss:0.001161, valid loss:0.000791
Epoch:25, Train loss:0.001136, valid loss:0.000774
Epoch:26, Train loss:0.001328, valid loss:0.001976
Epoch:27, Train loss:0.001278, valid loss:0.000762
Epoch:28, Train loss:0.001107, valid loss:0.000803
Epoch:29, Train loss:0.001103, valid loss:0.000890
Epoch:30, Train loss:0.001105, valid loss:0.000958
Epoch:31, Train loss:0.000879, valid loss:0.000695
Epoch:32, Train loss:0.000891, valid loss:0.000649
Epoch:33, Train loss:0.000907, valid loss:0.000642
Epoch:34, Train loss:0.000885, valid loss:0.000628
Epoch:35, Train loss:0.000865, valid loss:0.000594
Epoch:36, Train loss:0.000882, valid loss:0.000706
Epoch:37, Train loss:0.000870, valid loss:0.000655
Epoch:38, Train loss:0.000878, valid loss:0.000580
Epoch:39, Train loss:0.000865, valid loss:0.000618
Epoch:40, Train loss:0.000852, valid loss:0.000644
Epoch:41, Train loss:0.000731, valid loss:0.000618
Epoch:42, Train loss:0.000732, valid loss:0.000558
Epoch:43, Train loss:0.000725, valid loss:0.000590
Epoch:44, Train loss:0.000739, valid loss:0.000590
Epoch:45, Train loss:0.000724, valid loss:0.000572
Epoch:46, Train loss:0.000722, valid loss:0.000551
Epoch:47, Train loss:0.000722, valid loss:0.000558
Epoch:48, Train loss:0.000716, valid loss:0.000591
Epoch:49, Train loss:0.000720, valid loss:0.000551
Epoch:50, Train loss:0.000714, valid loss:0.000603
Epoch:51, Train loss:0.000658, valid loss:0.000545
Epoch:52, Train loss:0.000657, valid loss:0.000545
Epoch:53, Train loss:0.000652, valid loss:0.000534
Epoch:54, Train loss:0.000659, valid loss:0.000541
Epoch:55, Train loss:0.000652, valid loss:0.000555
Epoch:56, Train loss:0.000653, valid loss:0.000527
Epoch:57, Train loss:0.000650, valid loss:0.000556
Epoch:58, Train loss:0.000646, valid loss:0.000599
Epoch:59, Train loss:0.000645, valid loss:0.000539
Epoch:60, Train loss:0.000649, valid loss:0.000554
training time 8040.956936120987
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.02929109369078854
plot_id,batch_id 0 1 miss% 0.021045426352646703
plot_id,batch_id 0 2 miss% 0.021495500926863458
plot_id,batch_id 0 3 miss% 0.027205309168804537
plot_id,batch_id 0 4 miss% 0.02354999398146496
plot_id,batch_id 0 5 miss% 0.03521785679985152
plot_id,batch_id 0 6 miss% 0.024235544732176116
plot_id,batch_id 0 7 miss% 0.028156796942415713
plot_id,batch_id 0 8 miss% 0.02946396773060301
plot_id,batch_id 0 9 miss% 0.018874159352296034
plot_id,batch_id 0 10 miss% 0.03572147454761728
plot_id,batch_id 0 11 miss% 0.03722376515965835
plot_id,batch_id 0 12 miss% 0.02479170716403308
plot_id,batch_id 0 13 miss% 0.024414164032395408
plot_id,batch_id 0 14 miss% 0.02518597173325052
plot_id,batch_id 0 15 miss% 0.0350074384820661
plot_id,batch_id 0 16 miss% 0.039247545329571525
plot_id,batch_id 0 17 miss% 0.030378326862166585
plot_id,batch_id 0 18 miss% 0.02863339405786119
plot_id,batch_id 0 19 miss% 0.023191447781304293
plot_id,batch_id 0 20 miss% 0.06201779494000514
plot_id,batch_id 0 21 miss% 0.02198576781172469
plot_id,batch_id 0 22 miss% 0.026419671928866875
plot_id,batch_id 0 23 miss% 0.026898719766302367
plot_id,batch_id 0 24 miss% 0.0251663613037651
plot_id,batch_id 0 25 miss% 0.026047700772299374
plot_id,batch_id 0 26 miss% 0.033979149945551915
plot_id,batch_id 0 27 miss% 0.023978395580444147
plot_id,batch_id 0 28 miss% 0.02057225491675776
plot_id,batch_id 0 29 miss% 0.027663448860766507
plot_id,batch_id 0 30 miss% 0.058964439560449874
plot_id,batch_id 0 31 miss% 0.027112860591210822
plot_id,batch_id 0 32 miss% 0.02878692493419562
plot_id,batch_id 0 33 miss% 0.022871676426345244
plot_id,batch_id 0 34 miss% 0.02343073611187685
plot_id,batch_id 0 35 miss% 0.04103607003774862
plot_id,batch_id 0 36 miss% 0.03539401997742292
plot_id,batch_id 0 37 miss% 0.03570773744069033
plot_id,batch_id 0 38 miss% 0.023697357758252314
plot_id,batch_id 0 39 miss% 0.01977238213287434
plot_id,batch_id 0 40 miss% 0.07235924844182318
plot_id,batch_id 0 41 miss% 0.02471280877954961
plot_id,batch_id 0 42 miss% 0.015747861855378957
plot_id,batch_id 0 43 miss% 0.02713740349705225
plot_id,batch_id 0 44 miss% 0.02631347085403282
plot_id,batch_id 0 45 miss% 0.025302803880153442
plot_id,batch_id 0 46 miss% 0.033812466937758634
plot_id,batch_id 0 47 miss% 0.022145854688121396
plot_id,batch_id 0 48 miss% 0.019834930202285536
plot_id,batch_id 0 49 miss% 0.02647124596019622
plot_id,batch_id 0 50 miss% 0.038540205849817606
plot_id,batch_id 0 51 miss% 0.03295705683270055
plot_id,batch_id 0 52 miss% 0.02282217265628234
plot_id,batch_id 0 53 miss% 0.012842166349640244
plot_id,batch_id 0 54 miss% 0.023228990195477
plot_id,batch_id 0 55 miss% 0.03305011601171292
plot_id,batch_id 0 56 miss% 0.02770811478579785
plot_id,batch_id 0 57 miss% 0.024062205568887815
plot_id,batch_id 0 58 miss% 0.016505357938300064
plot_id,batch_id 0 59 miss% 0.026131071568413086
plot_id,batch_id 0 60 miss% 0.03718127621175892
plot_id,batch_id 0 61 miss% 0.04176794355750077
plot_id,batch_id 0 62 miss% 0.024294561806459735
plot_id,batch_id 0 63 miss% 0.0349758346299031
plot_id,batch_id 0 64 miss% 0.03360912212290212
plot_id,batch_id 0 65 miss% 0.041034211493587124
plot_id,batch_id 0 66 miss% 0.027880856039355
plot_id,batch_id 0 67 miss% 0.0364051913771228
plot_id,batch_id 0 68 miss% 0.03027460898437098
plot_id,batch_id 0 69 miss% 0.023227738675522056
plot_id,batch_id 0 70 miss% 0.028937914793947812
plot_id,batch_id 0 71 miss% 0.06709927663553507
plot_id,batch_id 0 72 miss% 0.02771128789533877
plot_id,batch_id 0 73 miss% 0.03503597441703608
plot_id,batch_id 0 74 miss% 0.052898094421807346
plot_id,batch_id 0 75 miss% 0.04144999630881412
plot_id,batch_id 0 76 miss% 0.07008249852307062
plot_id,batch_id 0 77 miss% 0.0287279991618951
plot_id,batch_id 0 78 miss% 0.04618948223011833
plot_id,batch_id 0 79 miss% 0.038642756512565005
plot_id,batch_id 0 80 miss% 0.04910277843460387
plot_id,batch_id 0 81 miss% 0.02799787649021831
plot_id,batch_id 0 82 miss% 0.038637680131019585
plot_id,batch_id 0 83 miss% 0.032790010088855556
plot_id,batch_id 0 84 miss% 0.03267145359888164
plot_id,batch_id 0 85 miss% 0.034408633931915525
plot_id,batch_id 0 86 miss% 0.022416408285116082
plot_id,batch_id 0 87 miss% 0.023803804066654017
plot_id,batch_id 0 88 miss% 0.03126947064810845
plot_id,batch_id 0 89 miss% 0.026797516199256632
plot_id,batch_id 0 90 miss% 0.039980195198348674
plot_id,batch_id 0 91 miss% 0.04139774387104459
plot_id,batch_id 0 92 miss% 0.028730658541001666
plot_id,batch_id 0 93 miss% 0.032679941058286796
plot_id,batch_id 0 94 miss% 0.02887218769992559
plot_id,batch_id 0 95 miss% 0.03539402520627595
plot_id,batch_id 0 96 miss% 0.04509576649721042
plot_id,batch_id 0 97 miss% 0.0363648380319971
plot_id,batch_id 0 98 miss% 0.03882676858780675
plot_id,batch_id 0 99 miss% 0.030243943175406386
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02929109 0.02104543 0.0214955  0.02720531 0.02354999 0.03521786
 0.02423554 0.0281568  0.02946397 0.01887416 0.03572147 0.03722377
 0.02479171 0.02441416 0.02518597 0.03500744 0.03924755 0.03037833
 0.02863339 0.02319145 0.06201779 0.02198577 0.02641967 0.02689872
 0.02516636 0.0260477  0.03397915 0.0239784  0.02057225 0.02766345
 0.05896444 0.02711286 0.02878692 0.02287168 0.02343074 0.04103607
 0.03539402 0.03570774 0.02369736 0.01977238 0.07235925 0.02471281
 0.01574786 0.0271374  0.02631347 0.0253028  0.03381247 0.02214585
 0.01983493 0.02647125 0.03854021 0.03295706 0.02282217 0.01284217
 0.02322899 0.03305012 0.02770811 0.02406221 0.01650536 0.02613107
 0.03718128 0.04176794 0.02429456 0.03497583 0.03360912 0.04103421
 0.02788086 0.03640519 0.03027461 0.02322774 0.02893791 0.06709928
 0.02771129 0.03503597 0.05289809 0.04145    0.0700825  0.028728
 0.04618948 0.03864276 0.04910278 0.02799788 0.03863768 0.03279001
 0.03267145 0.03440863 0.02241641 0.0238038  0.03126947 0.02679752
 0.0399802  0.04139774 0.02873066 0.03267994 0.02887219 0.03539403
 0.04509577 0.03636484 0.03882677 0.03024394]
for model  223 the mean error 0.03170428234019311
all id 223 hidden_dim 32 learning_rate 0.02 num_layers 3 frames 31 out win 4 err 0.03170428234019311
Launcher: Job 224 completed in 8230 seconds.
Launcher: Task 106 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  79249
Epoch:0, Train loss:0.438591, valid loss:0.413955
Epoch:1, Train loss:0.033312, valid loss:0.005588
Epoch:2, Train loss:0.009939, valid loss:0.004003
Epoch:3, Train loss:0.007384, valid loss:0.003656
Epoch:4, Train loss:0.005814, valid loss:0.003194
Epoch:5, Train loss:0.005215, valid loss:0.002865
Epoch:6, Train loss:0.004723, valid loss:0.002418
Epoch:7, Train loss:0.004512, valid loss:0.002541
Epoch:8, Train loss:0.004349, valid loss:0.002846
Epoch:9, Train loss:0.004040, valid loss:0.003039
Epoch:10, Train loss:0.004003, valid loss:0.002649
Epoch:11, Train loss:0.002718, valid loss:0.001406
Epoch:12, Train loss:0.002653, valid loss:0.001462
Epoch:13, Train loss:0.002621, valid loss:0.001429
Epoch:14, Train loss:0.002517, valid loss:0.001992
Epoch:15, Train loss:0.002610, valid loss:0.001834
Epoch:16, Train loss:0.002414, valid loss:0.001613
Epoch:17, Train loss:0.002508, valid loss:0.001310
Epoch:18, Train loss:0.002395, valid loss:0.002155
Epoch:19, Train loss:0.002356, valid loss:0.001302
Epoch:20, Train loss:0.002321, valid loss:0.001328
Epoch:21, Train loss:0.001642, valid loss:0.001012
Epoch:22, Train loss:0.001660, valid loss:0.001182
Epoch:23, Train loss:0.001644, valid loss:0.000989
Epoch:24, Train loss:0.001620, valid loss:0.001093
Epoch:25, Train loss:0.001612, valid loss:0.001088
Epoch:26, Train loss:0.001600, valid loss:0.001133
Epoch:27, Train loss:0.001546, valid loss:0.000997
Epoch:28, Train loss:0.001585, valid loss:0.000998
Epoch:29, Train loss:0.001506, valid loss:0.001104
Epoch:30, Train loss:0.001527, valid loss:0.001092
Epoch:31, Train loss:0.001198, valid loss:0.000851
Epoch:32, Train loss:0.001195, valid loss:0.000950
Epoch:33, Train loss:0.001203, valid loss:0.000933
Epoch:34, Train loss:0.001192, valid loss:0.000978
Epoch:35, Train loss:0.001178, valid loss:0.001027
Epoch:36, Train loss:0.001170, valid loss:0.000868
Epoch:37, Train loss:0.001158, valid loss:0.000905
Epoch:38, Train loss:0.001167, valid loss:0.000907
Epoch:39, Train loss:0.001140, valid loss:0.001113
Epoch:40, Train loss:0.001144, valid loss:0.000900
Epoch:41, Train loss:0.000976, valid loss:0.000761
Epoch:42, Train loss:0.000965, valid loss:0.000771
Epoch:43, Train loss:0.000966, valid loss:0.000792
Epoch:44, Train loss:0.000975, valid loss:0.000800
Epoch:45, Train loss:0.000962, valid loss:0.000765
Epoch:46, Train loss:0.000986, valid loss:0.000999
Epoch:47, Train loss:0.000953, valid loss:0.000864
Epoch:48, Train loss:0.000951, valid loss:0.000781
Epoch:49, Train loss:0.000956, valid loss:0.000782
Epoch:50, Train loss:0.000956, valid loss:0.000790
Epoch:51, Train loss:0.000871, valid loss:0.000746
Epoch:52, Train loss:0.000864, valid loss:0.000760
Epoch:53, Train loss:0.000860, valid loss:0.000738
Epoch:54, Train loss:0.000859, valid loss:0.000778
Epoch:55, Train loss:0.000860, valid loss:0.000791
Epoch:56, Train loss:0.000866, valid loss:0.000756
Epoch:57, Train loss:0.000854, valid loss:0.000721
Epoch:58, Train loss:0.000855, valid loss:0.000748
Epoch:59, Train loss:0.000851, valid loss:0.000732
Epoch:60, Train loss:0.000856, valid loss:0.000753
training time 8054.981745481491
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02784340898922475
plot_id,batch_id 0 1 miss% 0.023651577851205408
plot_id,batch_id 0 2 miss% 0.021727716276941728
plot_id,batch_id 0 3 miss% 0.024476268225966293
plot_id,batch_id 0 4 miss% 0.02308783706970549
plot_id,batch_id 0 5 miss% 0.032671218063909546
plot_id,batch_id 0 6 miss% 0.028140435836970016
plot_id,batch_id 0 7 miss% 0.02272089024297483
plot_id,batch_id 0 8 miss% 0.024671122807361928
plot_id,batch_id 0 9 miss% 0.02550659960699831
plot_id,batch_id 0 10 miss% 0.02397542286852001
plot_id,batch_id 0 11 miss% 0.05361543800307676
plot_id,batch_id 0 12 miss% 0.026718631791180406
plot_id,batch_id 0 13 miss% 0.024281079145728468
plot_id,batch_id 0 14 miss% 0.029157083350532458
plot_id,batch_id 0 15 miss% 0.045209976658178413
plot_id,batch_id 0 16 miss% 0.03384675110246196
plot_id,batch_id 0 17 miss% 0.033712879502428135
plot_id,batch_id 0 18 miss% 0.022466132927326028
plot_id,batch_id 0 19 miss% 0.034079163710343925
plot_id,batch_id 0 20 miss% 0.05138086988192646
plot_id,batch_id 0 21 miss% 0.01459297377924141
plot_id,batch_id 0 22 miss% 0.01766643085388473
plot_id,batch_id 0 23 miss% 0.028124970618089686
plot_id,batch_id 0 24 miss% 0.01735727085723023
plot_id,batch_id 0 25 miss% 0.0447087601849974
plot_id,batch_id 0 26 miss% 0.02882145158223659
plot_id,batch_id 0 27 miss% 0.0253018457591328
plot_id,batch_id 0 28 miss% 0.027735962723188585
plot_id,batch_id 0 29 miss% 0.0209825353631567
plot_id,batch_id 0 30 miss% 0.036364695939794356
plot_id,batch_id 0 31 miss% 0.0327439738819492
plot_id,batch_id 0 32 miss% 0.023110640902087854
plot_id,batch_id 0 33 miss% 0.035008242302274666
plot_id,batch_id 0 34 miss% 0.024260634728798844
plot_id,batch_id 0 35 miss% 0.04784905451651789
plot_id,batch_id 0 36 miss% 0.0361035807774772
plot_id,batch_id 0 37 miss% 0.02608778863040773
plot_id,batch_id 0 38 miss% 0.03003206807059687
plot_id,batch_id 0 39 miss% 0.020471884728204458
plot_id,batch_id 0 40 miss% 0.08794011298882747
plot_id,batch_id 0 41 miss% 0.022915031494887193
plot_id,batch_id 0 42 miss% 0.015840291834697793
plot_id,batch_id 0 43 miss% 0.02409640453365143
plot_id,batch_id 0 44 miss% 0.023781538387905508
plot_id,batch_id 0 45 miss% 0.023320443877795475
plot_id,batch_id 0 46 miss% 0.023096543687578047
plot_id,batch_id 0 47 miss% 0.024681294290716742
plot_id,batch_id 0 48 miss% 0.023351060176185497
plot_id,batch_id 0 49 miss% 0.015123366819122215
plot_id,batch_id 0 50 miss% 0.03491769085302921
plot_id,batch_id 0 51 miss% 0.027654670416723863
plot_id,batch_id 0 52 miss% 0.027480552845684517
plot_id,batch_id 0 53 miss% 0.011251302209230096
plot_id,batch_id 0 54 miss% 0.03352550087675863
plot_id,batch_id 0 55 miss% 0.054184040968403485
plot_id,batch_id 0 56 miss% 0.025499394697559873
plot_id,batch_id 0 57 miss% 0.022013301513835428
plot_id,batch_id 0 58 miss% 0.02120875668876468
plot_id,batch_id 0 59 miss% 0.026387602475557003
plot_id,batch_id 0 60 miss% 0.030298397559261753
plot_id,batch_id 0 61 miss% 0.026583935884120308
plot_id,batch_id 0 62 miss% 0.022005147171507596
plot_id,batch_id 0 63 miss% 0.02298903847741423
plot_id,batch_id 0 64 miss% 0.028356631251473037
plot_id,batch_id 0 65 miss% 0.03849354126371781
plot_id,batch_id 0 66 miss% 0.055226970250948026
plot_id,batch_id 0 67 miss% 0.023983409616228548
plot_id,batch_id 0 68 miss% 0.022808028208405533
plot_id,batch_id 0 69 miss% 0.016888124329293323
plot_id,batch_id 0 70 miss% 0.04315161744610622
plot_id,batch_id 0 71 miss% 0.031771633315648896
plot_id,batch_id 0 72 miss% 0.03696393321158774
plot_id,batch_id 0 73 miss% 0.027545606141367376
plot_id,batch_id 0 74 miss% 0.03602217694569899
plot_id,batch_id 0 75 miss% 0.053017113246219986
plot_id,batch_id 0 76 miss% 0.039283811440950146
plot_id,batch_id 0 77 miss% 0.03227191762551099
plot_id,batch_id 0 78 miss% 0.04330852523371879
plot_id,batch_id 0 79 miss% 0.040599351848881006
plot_id,batch_id 0 80 miss% 0.043667336362261104
plot_id,batch_id 0 81 miss% 0.023755673736261796
plot_id,batch_id 0 82 miss% 0.02429620122194181
plot_id,batch_id 0 83 miss% 0.027909916182328524
plot_id,batch_id 0 84 miss% 0.020141002109553878
plot_id,batch_id 0 85 miss% 0.05001993756014861
plot_id,batch_id 0 86 miss% 0.039364870971073836
plot_id,batch_id 0 87 miss% 0.02559292787764726
plot_id,batch_id 0 88 miss% 0.027258711856864192
plot_id,batch_id 0 89 miss% 0.027519478644598607
plot_id,batch_id 0 90 miss% 0.02514833208184855
plot_id,batch_id 0 91 miss% 0.026427186053812363
plot_id,batch_id 0 92 miss% 0.03125053178590149
plot_id,batch_id 0 93 miss% 0.029008321218787683
plot_id,batch_id 0 94 miss% 0.03308844310822732
plot_id,batch_id 0 95 miss% 0.04683583929265491
plot_id,batch_id 0 96 miss% 0.041689098567976736
plot_id,batch_id 0 97 miss% 0.0480813971844619
plot_id,batch_id 0 98 miss% 0.03007185843999312
plot_id,batch_id 0 99 miss% 0.02568855437387307
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02784341 0.02365158 0.02172772 0.02447627 0.02308784 0.03267122
 0.02814044 0.02272089 0.02467112 0.0255066  0.02397542 0.05361544
 0.02671863 0.02428108 0.02915708 0.04520998 0.03384675 0.03371288
 0.02246613 0.03407916 0.05138087 0.01459297 0.01766643 0.02812497
 0.01735727 0.04470876 0.02882145 0.02530185 0.02773596 0.02098254
 0.0363647  0.03274397 0.02311064 0.03500824 0.02426063 0.04784905
 0.03610358 0.02608779 0.03003207 0.02047188 0.08794011 0.02291503
 0.01584029 0.0240964  0.02378154 0.02332044 0.02309654 0.02468129
 0.02335106 0.01512337 0.03491769 0.02765467 0.02748055 0.0112513
 0.0335255  0.05418404 0.02549939 0.0220133  0.02120876 0.0263876
 0.0302984  0.02658394 0.02200515 0.02298904 0.02835663 0.03849354
 0.05522697 0.02398341 0.02280803 0.01688812 0.04315162 0.03177163
 0.03696393 0.02754561 0.03602218 0.05301711 0.03928381 0.03227192
 0.04330853 0.04059935 0.04366734 0.02375567 0.0242962  0.02790992
 0.020141   0.05001994 0.03936487 0.02559293 0.02725871 0.02751948
 0.02514833 0.02642719 0.03125053 0.02900832 0.03308844 0.04683584
 0.0416891  0.0480814  0.03007186 0.02568855]
for model  142 the mean error 0.0305691870084742
all id 142 hidden_dim 32 learning_rate 0.02 num_layers 3 frames 25 out win 4 err 0.0305691870084742
Launcher: Job 143 completed in 8256 seconds.
Launcher: Task 202 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  134801
Epoch:0, Train loss:0.658736, valid loss:0.667970
Epoch:1, Train loss:0.515513, valid loss:0.518674
Epoch:2, Train loss:0.495315, valid loss:0.515702
Epoch:3, Train loss:0.492466, valid loss:0.515733
Epoch:4, Train loss:0.490398, valid loss:0.515668
Epoch:5, Train loss:0.489344, valid loss:0.513756
Epoch:6, Train loss:0.488950, valid loss:0.514025
Epoch:7, Train loss:0.488490, valid loss:0.515034
Epoch:8, Train loss:0.488328, valid loss:0.514607
Epoch:9, Train loss:0.488187, valid loss:0.514128
Epoch:10, Train loss:0.489724, valid loss:0.513745
Epoch:11, Train loss:0.486731, valid loss:0.512911
Epoch:12, Train loss:0.486582, valid loss:0.512749
Epoch:13, Train loss:0.486553, valid loss:0.512548
Epoch:14, Train loss:0.486571, valid loss:0.512681
Epoch:15, Train loss:0.486397, valid loss:0.513093
Epoch:16, Train loss:0.486454, valid loss:0.512534
Epoch:17, Train loss:0.486198, valid loss:0.512589
Epoch:18, Train loss:0.486237, valid loss:0.512317
Epoch:19, Train loss:0.486179, valid loss:0.512611
Epoch:20, Train loss:0.486142, valid loss:0.512528
Epoch:21, Train loss:0.485394, valid loss:0.512166
Epoch:22, Train loss:0.485394, valid loss:0.512072
Epoch:23, Train loss:0.485391, valid loss:0.512299
Epoch:24, Train loss:0.485461, valid loss:0.512187
Epoch:25, Train loss:0.485441, valid loss:0.512126
Epoch:26, Train loss:0.485348, valid loss:0.512353
Epoch:27, Train loss:0.485392, valid loss:0.512229
Epoch:28, Train loss:0.485285, valid loss:0.512405
Epoch:29, Train loss:0.485252, valid loss:0.512302
Epoch:30, Train loss:0.485322, valid loss:0.511966
Epoch:31, Train loss:0.484963, valid loss:0.511964
Epoch:32, Train loss:0.484973, valid loss:0.511933
Epoch:33, Train loss:0.484944, valid loss:0.511958
Epoch:34, Train loss:0.484970, valid loss:0.511895
Epoch:35, Train loss:0.484912, valid loss:0.511906
Epoch:36, Train loss:0.484936, valid loss:0.511947
Epoch:37, Train loss:0.484903, valid loss:0.512012
Epoch:38, Train loss:0.484917, valid loss:0.512010
Epoch:39, Train loss:0.484921, valid loss:0.511897
Epoch:40, Train loss:0.484933, valid loss:0.511948
Epoch:41, Train loss:0.484756, valid loss:0.511861
Epoch:42, Train loss:0.484754, valid loss:0.511895
Epoch:43, Train loss:0.484741, valid loss:0.511879
Epoch:44, Train loss:0.484731, valid loss:0.511875
Epoch:45, Train loss:0.484737, valid loss:0.511842
Epoch:46, Train loss:0.484722, valid loss:0.511897
Epoch:47, Train loss:0.484726, valid loss:0.511856
Epoch:48, Train loss:0.484717, valid loss:0.511880
Epoch:49, Train loss:0.484727, valid loss:0.511890
Epoch:50, Train loss:0.484710, valid loss:0.511866
Epoch:51, Train loss:0.484662, valid loss:0.511812
Epoch:52, Train loss:0.484651, valid loss:0.511839
Epoch:53, Train loss:0.484644, valid loss:0.511817
Epoch:54, Train loss:0.484642, valid loss:0.511847
Epoch:55, Train loss:0.484650, valid loss:0.511828
Epoch:56, Train loss:0.484641, valid loss:0.511843
Epoch:57, Train loss:0.484638, valid loss:0.511830
Epoch:58, Train loss:0.484641, valid loss:0.511820
Epoch:59, Train loss:0.484631, valid loss:0.511807
Epoch:60, Train loss:0.484625, valid loss:0.511834
training time 8134.107849359512
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.697326663776479
plot_id,batch_id 0 1 miss% 0.7737962169199687
plot_id,batch_id 0 2 miss% 0.7802350952815984
plot_id,batch_id 0 3 miss% 0.7886460341658282
plot_id,batch_id 0 4 miss% 0.7925927437400523
plot_id,batch_id 0 5 miss% 0.6912120582885394
plot_id,batch_id 0 6 miss% 0.7662875672968784
plot_id,batch_id 0 7 miss% 0.7802618699339295
plot_id,batch_id 0 8 miss% 0.7895563459247332
plot_id,batch_id 0 9 miss% 0.7961954283101985
plot_id,batch_id 0 10 miss% 0.6719413045482961
plot_id,batch_id 0 11 miss% 0.7660622190793068
plot_id,batch_id 0 12 miss% 0.7790675954102163
plot_id,batch_id 0 13 miss% 0.7895404849193575
plot_id,batch_id 0 14 miss% 0.7908332471296933
plot_id,batch_id 0 15 miss% 0.6808445499530318
plot_id,batch_id 0 16 miss% 0.7583332316745353
plot_id,batch_id 0 17 miss% 0.7799932259850992
plot_id,batch_id 0 18 miss% 0.7855923739559322
plot_id,batch_id 0 19 miss% 0.7905297304033053
plot_id,batch_id 0 20 miss% 0.7427876857481536
plot_id,batch_id 0 21 miss% 0.7812722095874984
plot_id,batch_id 0 22 miss% 0.7879769740374217
plot_id,batch_id 0 23 miss% 0.7927926851147326
plot_id,batch_id 0 24 miss% 0.7943996895967943
plot_id,batch_id 0 25 miss% 0.7286469064959498
plot_id,batch_id 0 26 miss% 0.7806228685659908
plot_id,batch_id 0 27 miss% 0.786716679671158
plot_id,batch_id 0 28 miss% 0.7918356337410886
plot_id,batch_id 0 29 miss% 0.793564951143738
plot_id,batch_id 0 30 miss% 0.7298671993594624
plot_id,batch_id 0 31 miss% 0.775640416462144
plot_id,batch_id 0 32 miss% 0.7888846197366087
plot_id,batch_id 0 33 miss% 0.7923748889345962
plot_id,batch_id 0 34 miss% 0.795118620294756
plot_id,batch_id 0 35 miss% 0.7203463583510573
plot_id,batch_id 0 36 miss% 0.786087382852988
plot_id,batch_id 0 37 miss% 0.7840741203894541
plot_id,batch_id 0 38 miss% 0.7949398846768649
plot_id,batch_id 0 39 miss% 0.7949112736650171
plot_id,batch_id 0 40 miss% 0.7715417610403174
plot_id,batch_id 0 41 miss% 0.786008763046765
plot_id,batch_id 0 42 miss% 0.7907709036417732
plot_id,batch_id 0 43 miss% 0.7943032805733923
plot_id,batch_id 0 44 miss% 0.7957951006821533
plot_id,batch_id 0 45 miss% 0.763501844545852
plot_id,batch_id 0 46 miss% 0.7858788585859563
plot_id,batch_id 0 47 miss% 0.7919386492784911
plot_id,batch_id 0 48 miss% 0.7984564903802358
plot_id,batch_id 0 49 miss% 0.7956726891131738
plot_id,batch_id 0 50 miss% 0.7672679755469013
plot_id,batch_id 0 51 miss% 0.7857152741883094
plot_id,batch_id 0 52 miss% 0.7918017570679885
plot_id,batch_id 0 53 miss% 0.795933394107854
plot_id,batch_id 0 54 miss% 0.7979972418428777
plot_id,batch_id 0 55 miss% 0.7617263084725104
plot_id,batch_id 0 56 miss% 0.7842267826225028
plot_id,batch_id 0 57 miss% 0.7940588067304009
plot_id,batch_id 0 58 miss% 0.7924524669746352
plot_id,batch_id 0 59 miss% 0.7967234236628401
plot_id,batch_id 0 60 miss% 0.6142714150523957
plot_id,batch_id 0 61 miss% 0.7358282657685062
plot_id,batch_id 0 62 miss% 0.7609508474437441
plot_id,batch_id 0 63 miss% 0.7811610362098863
plot_id,batch_id 0 64 miss% 0.7823490746668584
plot_id,batch_id 0 65 miss% 0.6053397476460542
plot_id,batch_id 0 66 miss% 0.7267207825361499
plot_id,batch_id 0 67 miss% 0.7488757101020527
plot_id,batch_id 0 68 miss% 0.7749777130076368
plot_id,batch_id 0 69 miss% 0.7776164165194964
plot_id,batch_id 0 70 miss% 0.5734526009594675
plot_id,batch_id 0 71 miss% 0.729151711545593
plot_id,batch_id 0 72 miss% 0.7394428498932706
plot_id,batch_id 0 73 miss% 0.761695221405219
plot_id,batch_id 0 74 miss% 0.7701919427503219
plot_id,batch_id 0 75 miss% 0.5669433386010981
plot_id,batch_id 0 76 miss% 0.6942332715222325
plot_id,batch_id 0 77 miss% 0.7306591634967836
plot_id,batch_id 0 78 miss% 0.7577027146115999
plot_id,batch_id 0 79 miss% 0.768758156815578
plot_id,batch_id 0 80 miss% 0.6359391793627751
plot_id,batch_id 0 81 miss% 0.7526939619510806
plot_id,batch_id 0 82 miss% 0.7750541759764931
plot_id,batch_id 0 83 miss% 0.7833210946743115
plot_id,batch_id 0 84 miss% 0.7833588382195033
plot_id,batch_id 0 85 miss% 0.6349937544161781
plot_id,batch_id 0 86 miss% 0.7464594976311318
plot_id,batch_id 0 87 miss% 0.7689693489417233
plot_id,batch_id 0 88 miss% 0.7831747236702088
plot_id,batch_id 0 89 miss% 0.7839721277270094
plot_id,batch_id 0 90 miss% 0.6035750888996361
plot_id,batch_id 0 91 miss% 0.7387233885339792
plot_id,batch_id 0 92 miss% 0.7603020512638636
plot_id,batch_id 0 93 miss% 0.777059403131805
plot_id,batch_id 0 94 miss% 0.7830881296164155
plot_id,batch_id 0 95 miss% 0.6068894094977777
plot_id,batch_id 0 96 miss% 0.7251675603870029
plot_id,batch_id 0 97 miss% 0.7503954734273645
plot_id,batch_id 0 98 miss% 0.7700576541845455
plot_id,batch_id 0 99 miss% 0.7740218050846901
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.69732666 0.77379622 0.7802351  0.78864603 0.79259274 0.69121206
 0.76628757 0.78026187 0.78955635 0.79619543 0.6719413  0.76606222
 0.7790676  0.78954048 0.79083325 0.68084455 0.75833323 0.77999323
 0.78559237 0.79052973 0.74278769 0.78127221 0.78797697 0.79279269
 0.79439969 0.72864691 0.78062287 0.78671668 0.79183563 0.79356495
 0.7298672  0.77564042 0.78888462 0.79237489 0.79511862 0.72034636
 0.78608738 0.78407412 0.79493988 0.79491127 0.77154176 0.78600876
 0.7907709  0.79430328 0.7957951  0.76350184 0.78587886 0.79193865
 0.79845649 0.79567269 0.76726798 0.78571527 0.79180176 0.79593339
 0.79799724 0.76172631 0.78422678 0.79405881 0.79245247 0.79672342
 0.61427142 0.73582827 0.76095085 0.78116104 0.78234907 0.60533975
 0.72672078 0.74887571 0.77497771 0.77761642 0.5734526  0.72915171
 0.73944285 0.76169522 0.77019194 0.56694334 0.69423327 0.73065916
 0.75770271 0.76875816 0.63593918 0.75269396 0.77505418 0.78332109
 0.78335884 0.63499375 0.7464595  0.76896935 0.78317472 0.78397213
 0.60357509 0.73872339 0.76030205 0.7770594  0.78308813 0.60688941
 0.72516756 0.75039547 0.77005765 0.77402181]
for model  78 the mean error 0.7567101945437882
all id 78 hidden_dim 32 learning_rate 0.02 num_layers 5 frames 21 out win 3 err 0.7567101945437882
Launcher: Job 79 completed in 8314 seconds.
Launcher: Task 220 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  107025
Epoch:0, Train loss:0.435344, valid loss:0.430397
Epoch:1, Train loss:0.033425, valid loss:0.005678
Epoch:2, Train loss:0.007970, valid loss:0.003815
Epoch:3, Train loss:0.005790, valid loss:0.002918
Epoch:4, Train loss:0.004583, valid loss:0.002136
Epoch:5, Train loss:0.003864, valid loss:0.002087
Epoch:6, Train loss:0.003406, valid loss:0.001917
Epoch:7, Train loss:0.003078, valid loss:0.001892
Epoch:8, Train loss:0.002964, valid loss:0.001553
Epoch:9, Train loss:0.002642, valid loss:0.001753
Epoch:10, Train loss:0.002566, valid loss:0.001714
Epoch:11, Train loss:0.001805, valid loss:0.001052
Epoch:12, Train loss:0.001802, valid loss:0.001013
Epoch:13, Train loss:0.001769, valid loss:0.001095
Epoch:14, Train loss:0.001679, valid loss:0.000930
Epoch:15, Train loss:0.001663, valid loss:0.001010
Epoch:16, Train loss:0.001618, valid loss:0.000954
Epoch:17, Train loss:0.001540, valid loss:0.000857
Epoch:18, Train loss:0.001509, valid loss:0.001019
Epoch:19, Train loss:0.001496, valid loss:0.000998
Epoch:20, Train loss:0.001478, valid loss:0.000899
Epoch:21, Train loss:0.001116, valid loss:0.000738
Epoch:22, Train loss:0.001076, valid loss:0.000745
Epoch:23, Train loss:0.001089, valid loss:0.000776
Epoch:24, Train loss:0.001089, valid loss:0.000878
Epoch:25, Train loss:0.001054, valid loss:0.000677
Epoch:26, Train loss:0.001044, valid loss:0.000660
Epoch:27, Train loss:0.001055, valid loss:0.000733
Epoch:28, Train loss:0.001012, valid loss:0.000714
Epoch:29, Train loss:0.001031, valid loss:0.000661
Epoch:30, Train loss:0.001021, valid loss:0.000758
Epoch:31, Train loss:0.000847, valid loss:0.000584
Epoch:32, Train loss:0.000836, valid loss:0.000719
Epoch:33, Train loss:0.000834, valid loss:0.000583
Epoch:34, Train loss:0.000838, valid loss:0.000627
Epoch:35, Train loss:0.000834, valid loss:0.000624
Epoch:36, Train loss:0.000821, valid loss:0.000629
Epoch:37, Train loss:0.000813, valid loss:0.000636
Epoch:38, Train loss:0.000811, valid loss:0.000569
Epoch:39, Train loss:0.000801, valid loss:0.000627
Epoch:40, Train loss:0.000798, valid loss:0.000631
Epoch:41, Train loss:0.000728, valid loss:0.000533
Epoch:42, Train loss:0.000721, valid loss:0.000537
Epoch:43, Train loss:0.000718, valid loss:0.000549
Epoch:44, Train loss:0.000717, valid loss:0.000532
Epoch:45, Train loss:0.000718, valid loss:0.000546
Epoch:46, Train loss:0.000715, valid loss:0.000526
Epoch:47, Train loss:0.000713, valid loss:0.000535
Epoch:48, Train loss:0.000707, valid loss:0.000564
Epoch:49, Train loss:0.000704, valid loss:0.000530
Epoch:50, Train loss:0.000701, valid loss:0.000562
Epoch:51, Train loss:0.000674, valid loss:0.000526
Epoch:52, Train loss:0.000670, valid loss:0.000530
Epoch:53, Train loss:0.000670, valid loss:0.000509
Epoch:54, Train loss:0.000665, valid loss:0.000518
Epoch:55, Train loss:0.000665, valid loss:0.000518
Epoch:56, Train loss:0.000663, valid loss:0.000529
Epoch:57, Train loss:0.000663, valid loss:0.000530
Epoch:58, Train loss:0.000661, valid loss:0.000518
Epoch:59, Train loss:0.000659, valid loss:0.000509
Epoch:60, Train loss:0.000658, valid loss:0.000526
training time 8152.078701734543
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02351603252787606
plot_id,batch_id 0 1 miss% 0.03290420119144784
plot_id,batch_id 0 2 miss% 0.03687094919678728
plot_id,batch_id 0 3 miss% 0.019616098343124187
plot_id,batch_id 0 4 miss% 0.028398335953727056
plot_id,batch_id 0 5 miss% 0.02580761760872452
plot_id,batch_id 0 6 miss% 0.03363474476443158
plot_id,batch_id 0 7 miss% 0.025618305307526436
plot_id,batch_id 0 8 miss% 0.023877914389837118
plot_id,batch_id 0 9 miss% 0.02024604782524193
plot_id,batch_id 0 10 miss% 0.03578571666751272
plot_id,batch_id 0 11 miss% 0.03874830490530556
plot_id,batch_id 0 12 miss% 0.02953466667715335
plot_id,batch_id 0 13 miss% 0.021495967585071305
plot_id,batch_id 0 14 miss% 0.030725373759797052
plot_id,batch_id 0 15 miss% 0.024597297661604657
plot_id,batch_id 0 16 miss% 0.02658821714429218
plot_id,batch_id 0 17 miss% 0.04196267776646354
plot_id,batch_id 0 18 miss% 0.027279519070237603
plot_id,batch_id 0 19 miss% 0.03026883139275043
plot_id,batch_id 0 20 miss% 0.03571232511209849
plot_id,batch_id 0 21 miss% 0.015196166275872416
plot_id,batch_id 0 22 miss% 0.0268190218702916
plot_id,batch_id 0 23 miss% 0.018564840263047798
plot_id,batch_id 0 24 miss% 0.02013406830203514
plot_id,batch_id 0 25 miss% 0.024100213587017093
plot_id,batch_id 0 26 miss% 0.03214594903174869
plot_id,batch_id 0 27 miss% 0.024921700479938753
plot_id,batch_id 0 28 miss% 0.01762550161829146
plot_id,batch_id 0 29 miss% 0.019898456647272852
plot_id,batch_id 0 30 miss% 0.044673532138414976
plot_id,batch_id 0 31 miss% 0.027477425487721252
plot_id,batch_id 0 32 miss% 0.027621658094217655
plot_id,batch_id 0 33 miss% 0.024758440272443904
plot_id,batch_id 0 34 miss% 0.0233488293393098
plot_id,batch_id 0 35 miss% 0.03664860776986225
plot_id,batch_id 0 36 miss% 0.03137814399480008
plot_id,batch_id 0 37 miss% 0.018900953977631592
plot_id,batch_id 0 38 miss% 0.02425759697830159
plot_id,batch_id 0 39 miss% 0.020518916582288278
plot_id,batch_id 0 40 miss% 0.06707032822098215
plot_id,batch_id 0 41 miss% 0.02762923535881519
plot_id,batch_id 0 42 miss% 0.024442365123002838
plot_id,batch_id 0 43 miss% 0.0258820074990892
plot_id,batch_id 0 44 miss% 0.026574550212852616
plot_id,batch_id 0 45 miss% 0.046572762741590105
plot_id,batch_id 0 46 miss% 0.02511328884379795
plot_id,batch_id 0 47 miss% 0.02867307183763708
plot_id,batch_id 0 48 miss% 0.024148289420564736
plot_id,batch_id 0 49 miss% 0.01954213821162721
plot_id,batch_id 0 50 miss% 0.0357808889572577
plot_id,batch_id 0 51 miss% 0.02730086977902532
plot_id,batch_id 0 52 miss% 0.027288638402545473
plot_id,batch_id 0 53 miss% 0.017608073598139387
plot_id,batch_id 0 54 miss% 0.029747660856953427
plot_id,batch_id 0 55 miss% 0.02987320764520378
plot_id,batch_id 0 56 miss% 0.01897699196080641
plot_id,batch_id 0 57 miss% 0.029425608494074593
plot_id,batch_id 0 58 miss% 0.023553513921664736
plot_id,batch_id 0 59 miss% 0.025228104829818992
plot_id,batch_id 0 60 miss% 0.03524942151910035
plot_id,batch_id 0 61 miss% 0.033413314217821725
plot_id,batch_id 0 62 miss% 0.023115501731970658
plot_id,batch_id 0 63 miss% 0.03237667463425929
plot_id,batch_id 0 64 miss% 0.03333666108642837
plot_id,batch_id 0 65 miss% 0.034044586027108054
plot_id,batch_id 0 66 miss% 0.03238033907198594
plot_id,batch_id 0 67 miss% 0.02861162513620793
plot_id,batch_id 0 68 miss% 0.030966926058836573
plot_id,batch_id 0 69 miss% 0.023500140344394474
plot_id,batch_id 0 70 miss% 0.0518265448007014
plot_id,batch_id 0 71 miss% 0.05019813218844122
plot_id,batch_id 0 72 miss% 0.032353965554952444
plot_id,batch_id 0 73 miss% 0.022114072418328676
plot_id,batch_id 0 74 miss% 0.030607210534516777
plot_id,batch_id 0 75 miss% 0.04169432698299654
plot_id,batch_id 0 76 miss% 0.03566685288682256
plot_id,batch_id 0 77 miss% 0.030801750163130057
plot_id,batch_id 0 78 miss% 0.025935441747713794
plot_id,batch_id 0 79 miss% 0.03750870881034425
plot_id,batch_id 0 80 miss% 0.03162528749929041
plot_id,batch_id 0 81 miss% 0.023266071499683686
plot_id,batch_id 0 82 miss% 0.029633454671628284
plot_id,batch_id 0 83 miss% 0.033888700704168405
plot_id,batch_id 0 84 miss% 0.02371688486165611
plot_id,batch_id 0 85 miss% 0.04090577881044506
plot_id,batch_id 0 86 miss% 0.02808130634177412
plot_id,batch_id 0 87 miss% 0.027661225811855254
plot_id,batch_id 0 88 miss% 0.02428747007659392
plot_id,batch_id 0 89 miss% 0.03086120001984798
plot_id,batch_id 0 90 miss% 0.0359626743291359
plot_id,batch_id 0 91 miss% 0.038477682840735765
plot_id,batch_id 0 92 miss% 0.04225956707002079
plot_id,batch_id 0 93 miss% 0.03098636831298154
plot_id,batch_id 0 94 miss% 0.020642807345600072
plot_id,batch_id 0 95 miss% 0.04288326703036332
plot_id,batch_id 0 96 miss% 0.03152840051117365
plot_id,batch_id 0 97 miss% 0.03842666000486747
plot_id,batch_id 0 98 miss% 0.02774147943788881
plot_id,batch_id 0 99 miss% 0.020574509981957603
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02351603 0.0329042  0.03687095 0.0196161  0.02839834 0.02580762
 0.03363474 0.02561831 0.02387791 0.02024605 0.03578572 0.0387483
 0.02953467 0.02149597 0.03072537 0.0245973  0.02658822 0.04196268
 0.02727952 0.03026883 0.03571233 0.01519617 0.02681902 0.01856484
 0.02013407 0.02410021 0.03214595 0.0249217  0.0176255  0.01989846
 0.04467353 0.02747743 0.02762166 0.02475844 0.02334883 0.03664861
 0.03137814 0.01890095 0.0242576  0.02051892 0.06707033 0.02762924
 0.02444237 0.02588201 0.02657455 0.04657276 0.02511329 0.02867307
 0.02414829 0.01954214 0.03578089 0.02730087 0.02728864 0.01760807
 0.02974766 0.02987321 0.01897699 0.02942561 0.02355351 0.0252281
 0.03524942 0.03341331 0.0231155  0.03237667 0.03333666 0.03404459
 0.03238034 0.02861163 0.03096693 0.02350014 0.05182654 0.05019813
 0.03235397 0.02211407 0.03060721 0.04169433 0.03566685 0.03080175
 0.02593544 0.03750871 0.03162529 0.02326607 0.02963345 0.0338887
 0.02371688 0.04090578 0.02808131 0.02766123 0.02428747 0.0308612
 0.03596267 0.03847768 0.04225957 0.03098637 0.02064281 0.04288327
 0.0315284  0.03842666 0.02774148 0.02057451]
for model  96 the mean error 0.029617237565546965
all id 96 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 25 out win 3 err 0.029617237565546965
Launcher: Job 97 completed in 8347 seconds.
Launcher: Task 140 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  35921
Epoch:0, Train loss:0.489925, valid loss:0.473190
Epoch:1, Train loss:0.029679, valid loss:0.007283
Epoch:2, Train loss:0.008587, valid loss:0.003532
Epoch:3, Train loss:0.005490, valid loss:0.002662
Epoch:4, Train loss:0.004606, valid loss:0.002548
Epoch:5, Train loss:0.004106, valid loss:0.002144
Epoch:6, Train loss:0.003680, valid loss:0.001681
Epoch:7, Train loss:0.003246, valid loss:0.001701
Epoch:8, Train loss:0.002948, valid loss:0.001368
Epoch:9, Train loss:0.002732, valid loss:0.001722
Epoch:10, Train loss:0.002579, valid loss:0.001314
Epoch:11, Train loss:0.002020, valid loss:0.001039
Epoch:12, Train loss:0.001972, valid loss:0.001107
Epoch:13, Train loss:0.001937, valid loss:0.001045
Epoch:14, Train loss:0.001900, valid loss:0.001101
Epoch:15, Train loss:0.001861, valid loss:0.000949
Epoch:16, Train loss:0.001825, valid loss:0.000979
Epoch:17, Train loss:0.001788, valid loss:0.000989
Epoch:18, Train loss:0.001750, valid loss:0.001182
Epoch:19, Train loss:0.001756, valid loss:0.001011
Epoch:20, Train loss:0.001712, valid loss:0.001013
Epoch:21, Train loss:0.001441, valid loss:0.000882
Epoch:22, Train loss:0.001423, valid loss:0.000893
Epoch:23, Train loss:0.001410, valid loss:0.000860
Epoch:24, Train loss:0.001398, valid loss:0.000833
Epoch:25, Train loss:0.001389, valid loss:0.001048
Epoch:26, Train loss:0.001380, valid loss:0.000846
Epoch:27, Train loss:0.001374, valid loss:0.000994
Epoch:28, Train loss:0.001363, valid loss:0.000881
Epoch:29, Train loss:0.001355, valid loss:0.000782
Epoch:30, Train loss:0.001347, valid loss:0.000896
Epoch:31, Train loss:0.001193, valid loss:0.000907
Epoch:32, Train loss:0.001203, valid loss:0.000755
Epoch:33, Train loss:0.001188, valid loss:0.000791
Epoch:34, Train loss:0.001179, valid loss:0.000774
Epoch:35, Train loss:0.001181, valid loss:0.000749
Epoch:36, Train loss:0.001166, valid loss:0.000757
Epoch:37, Train loss:0.001163, valid loss:0.000749
Epoch:38, Train loss:0.001167, valid loss:0.000870
Epoch:39, Train loss:0.001156, valid loss:0.000769
Epoch:40, Train loss:0.001171, valid loss:0.000751
Epoch:41, Train loss:0.001079, valid loss:0.000760
Epoch:42, Train loss:0.001078, valid loss:0.000784
Epoch:43, Train loss:0.001076, valid loss:0.000737
Epoch:44, Train loss:0.001072, valid loss:0.000717
Epoch:45, Train loss:0.001068, valid loss:0.000730
Epoch:46, Train loss:0.001065, valid loss:0.000713
Epoch:47, Train loss:0.001068, valid loss:0.000773
Epoch:48, Train loss:0.001062, valid loss:0.000700
Epoch:49, Train loss:0.001060, valid loss:0.000718
Epoch:50, Train loss:0.001059, valid loss:0.000738
Epoch:51, Train loss:0.001023, valid loss:0.000718
Epoch:52, Train loss:0.001018, valid loss:0.000690
Epoch:53, Train loss:0.001015, valid loss:0.000693
Epoch:54, Train loss:0.001012, valid loss:0.000702
Epoch:55, Train loss:0.001018, valid loss:0.000685
Epoch:56, Train loss:0.001009, valid loss:0.000702
Epoch:57, Train loss:0.001010, valid loss:0.000707
Epoch:58, Train loss:0.001005, valid loss:0.000694
Epoch:59, Train loss:0.001012, valid loss:0.000707
Epoch:60, Train loss:0.001005, valid loss:0.000686
training time 8186.069095849991
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.02705811574479732
plot_id,batch_id 0 1 miss% 0.017593319224916575
plot_id,batch_id 0 2 miss% 0.02745121639225312
plot_id,batch_id 0 3 miss% 0.02906103446984197
plot_id,batch_id 0 4 miss% 0.025528648167766906
plot_id,batch_id 0 5 miss% 0.03847212212964241
plot_id,batch_id 0 6 miss% 0.027608011149791112
plot_id,batch_id 0 7 miss% 0.021844707149501992
plot_id,batch_id 0 8 miss% 0.023618526648522053
plot_id,batch_id 0 9 miss% 0.022807279247327852
plot_id,batch_id 0 10 miss% 0.02652528564769693
plot_id,batch_id 0 11 miss% 0.025702207518098395
plot_id,batch_id 0 12 miss% 0.033293556006262374
plot_id,batch_id 0 13 miss% 0.03144117597684298
plot_id,batch_id 0 14 miss% 0.02657175555106464
plot_id,batch_id 0 15 miss% 0.0713254075070132
plot_id,batch_id 0 16 miss% 0.03967860768183962
plot_id,batch_id 0 17 miss% 0.027639302542558013
plot_id,batch_id 0 18 miss% 0.028094063394955363
plot_id,batch_id 0 19 miss% 0.04040184326015708
plot_id,batch_id 0 20 miss% 0.03474836696610606
plot_id,batch_id 0 21 miss% 0.028188381559042457
plot_id,batch_id 0 22 miss% 0.023697315099589795
plot_id,batch_id 0 23 miss% 0.02857899928854988
plot_id,batch_id 0 24 miss% 0.02909010348772149
plot_id,batch_id 0 25 miss% 0.03031147213367781
plot_id,batch_id 0 26 miss% 0.023110952158098327
plot_id,batch_id 0 27 miss% 0.02848154915689916
plot_id,batch_id 0 28 miss% 0.02020587803721337
plot_id,batch_id 0 29 miss% 0.02473649008886301
plot_id,batch_id 0 30 miss% 0.05821583783611645
plot_id,batch_id 0 31 miss% 0.0380767523486395
plot_id,batch_id 0 32 miss% 0.02952793639297102
plot_id,batch_id 0 33 miss% 0.03134875071845628
plot_id,batch_id 0 34 miss% 0.02118634303415532
plot_id,batch_id 0 35 miss% 0.05138029321890016
plot_id,batch_id 0 36 miss% 0.040498589135578814
plot_id,batch_id 0 37 miss% 0.027067767023183087
plot_id,batch_id 0 38 miss% 0.03443616430249165
plot_id,batch_id 0 39 miss% 0.022281722499751442
plot_id,batch_id 0 40 miss% 0.09483987880133997
plot_id,batch_id 0 41 miss% 0.040475535509686966
plot_id,batch_id 0 42 miss% 0.02123864525913519
plot_id,batch_id 0 43 miss% 0.0333854088982105
plot_id,batch_id 0 44 miss% 0.028819307997694214
plot_id,batch_id 0 45 miss% 0.03497628114094195
plot_id,batch_id 0 46 miss% 0.028588549295271312
plot_id,batch_id 0 47 miss% 0.028284590666566765
plot_id,batch_id 0 48 miss% 0.025027700131018845
plot_id,batch_id 0 49 miss% 0.02055181250277433
plot_id,batch_id 0 50 miss% 0.03778657782745484
plot_id,batch_id 0 51 miss% 0.019799338410624093
plot_id,batch_id 0 52 miss% 0.024175473290649816
plot_id,batch_id 0 53 miss% 0.017564455919830243
plot_id,batch_id 0 54 miss% 0.03365132506009451
plot_id,batch_id 0 55 miss% 0.03891898598427672
plot_id,batch_id 0 56 miss% 0.027281522140090386
plot_id,batch_id 0 57 miss% 0.018458933610044925
plot_id,batch_id 0 58 miss% 0.01908379828745806
plot_id,batch_id 0 59 miss% 0.03207265887633298
plot_id,batch_id 0 60 miss% 0.030786712353907186
plot_id,batch_id 0 61 miss% 0.022543602207509006
plot_id,batch_id 0 62 miss% 0.03545765455846685
plot_id,batch_id 0 63 miss% 0.02541184311114179
plot_id,batch_id 0 64 miss% 0.0232125170485519
plot_id,batch_id 0 65 miss% 0.0465869914177288
plot_id,batch_id 0 66 miss% 0.04018668087952229
plot_id,batch_id 0 67 miss% 0.029883850517388736
plot_id,batch_id 0 68 miss% 0.02373084006595743
plot_id,batch_id 0 69 miss% 0.021270806631403617
plot_id,batch_id 0 70 miss% 0.04360324939989566
plot_id,batch_id 0 71 miss% 0.041292681326064166
plot_id,batch_id 0 72 miss% 0.055640206169970965
plot_id,batch_id 0 73 miss% 0.03682445980925857
plot_id,batch_id 0 74 miss% 0.051896350818944365
plot_id,batch_id 0 75 miss% 0.03152827928228291
plot_id,batch_id 0 76 miss% 0.04922659040525372
plot_id,batch_id 0 77 miss% 0.021243781766119848
plot_id,batch_id 0 78 miss% 0.03661320423420629
plot_id,batch_id 0 79 miss% 0.034771498512976576
plot_id,batch_id 0 80 miss% 0.05377616492322745
plot_id,batch_id 0 81 miss% 0.016699951489803473
plot_id,batch_id 0 82 miss% 0.041466652427822846
plot_id,batch_id 0 83 miss% 0.03349845237630754
plot_id,batch_id 0 84 miss% 0.02346693445798977
plot_id,batch_id 0 85 miss% 0.04737079469106688
plot_id,batch_id 0 86 miss% 0.03136892759122285
plot_id,batch_id 0 87 miss% 0.031237823970831045
plot_id,batch_id 0 88 miss% 0.034576276535748386
plot_id,batch_id 0 89 miss% 0.03507614475961318
plot_id,batch_id 0 90 miss% 0.05766922591860557
plot_id,batch_id 0 91 miss% 0.037778107645478114
plot_id,batch_id 0 92 miss% 0.032714836610572104
plot_id,batch_id 0 93 miss% 0.031604100963085954
plot_id,batch_id 0 94 miss% 0.025769140728909543
plot_id,batch_id 0 95 miss% 0.045276444883151086
plot_id,batch_id 0 96 miss% 0.034037372978455056
plot_id,batch_id 0 97 miss% 0.046441283126723874
plot_id,batch_id 0 98 miss% 0.03711135941626237
plot_id,batch_id 0 99 miss% 0.026880280195000425
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02705812 0.01759332 0.02745122 0.02906103 0.02552865 0.03847212
 0.02760801 0.02184471 0.02361853 0.02280728 0.02652529 0.02570221
 0.03329356 0.03144118 0.02657176 0.07132541 0.03967861 0.0276393
 0.02809406 0.04040184 0.03474837 0.02818838 0.02369732 0.028579
 0.0290901  0.03031147 0.02311095 0.02848155 0.02020588 0.02473649
 0.05821584 0.03807675 0.02952794 0.03134875 0.02118634 0.05138029
 0.04049859 0.02706777 0.03443616 0.02228172 0.09483988 0.04047554
 0.02123865 0.03338541 0.02881931 0.03497628 0.02858855 0.02828459
 0.0250277  0.02055181 0.03778658 0.01979934 0.02417547 0.01756446
 0.03365133 0.03891899 0.02728152 0.01845893 0.0190838  0.03207266
 0.03078671 0.0225436  0.03545765 0.02541184 0.02321252 0.04658699
 0.04018668 0.02988385 0.02373084 0.02127081 0.04360325 0.04129268
 0.05564021 0.03682446 0.05189635 0.03152828 0.04922659 0.02124378
 0.0366132  0.0347715  0.05377616 0.01669995 0.04146665 0.03349845
 0.02346693 0.04737079 0.03136893 0.03123782 0.03457628 0.03507614
 0.05766923 0.03777811 0.03271484 0.0316041  0.02576914 0.04527644
 0.03403737 0.04644128 0.03711136 0.02688028]
for model  181 the mean error 0.032873987057127796
all id 181 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 31 out win 4 err 0.032873987057127796
Launcher: Job 182 completed in 8379 seconds.
Launcher: Task 176 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  35921
Epoch:0, Train loss:0.577978, valid loss:0.559762
Epoch:1, Train loss:0.036480, valid loss:0.007101
Epoch:2, Train loss:0.011202, valid loss:0.004878
Epoch:3, Train loss:0.009704, valid loss:0.004363
Epoch:4, Train loss:0.007394, valid loss:0.003226
Epoch:5, Train loss:0.006468, valid loss:0.003370
Epoch:6, Train loss:0.006105, valid loss:0.002957
Epoch:7, Train loss:0.006165, valid loss:0.003575
Epoch:8, Train loss:0.005512, valid loss:0.003401
Epoch:9, Train loss:0.005131, valid loss:0.002492
Epoch:10, Train loss:0.005149, valid loss:0.002255
Epoch:11, Train loss:0.003995, valid loss:0.002153
Epoch:12, Train loss:0.003612, valid loss:0.001796
Epoch:13, Train loss:0.003504, valid loss:0.002083
Epoch:14, Train loss:0.003511, valid loss:0.001801
Epoch:15, Train loss:0.003382, valid loss:0.001883
Epoch:16, Train loss:0.003358, valid loss:0.002161
Epoch:17, Train loss:0.003289, valid loss:0.002124
Epoch:18, Train loss:0.003333, valid loss:0.001740
Epoch:19, Train loss:0.003282, valid loss:0.002260
Epoch:20, Train loss:0.003271, valid loss:0.001823
Epoch:21, Train loss:0.002465, valid loss:0.001458
Epoch:22, Train loss:0.002395, valid loss:0.001378
Epoch:23, Train loss:0.002523, valid loss:0.001714
Epoch:24, Train loss:0.002475, valid loss:0.002088
Epoch:25, Train loss:0.002332, valid loss:0.001385
Epoch:26, Train loss:0.002448, valid loss:0.001623
Epoch:27, Train loss:0.002333, valid loss:0.001352
Epoch:28, Train loss:0.002354, valid loss:0.001382
Epoch:29, Train loss:0.002327, valid loss:0.001503
Epoch:30, Train loss:0.002291, valid loss:0.001343
Epoch:31, Train loss:0.001927, valid loss:0.001212
Epoch:32, Train loss:0.001931, valid loss:0.001232
Epoch:33, Train loss:0.001944, valid loss:0.001275
Epoch:34, Train loss:0.001908, valid loss:0.001404
Epoch:35, Train loss:0.001915, valid loss:0.001195
Epoch:36, Train loss:0.001908, valid loss:0.001150
Epoch:37, Train loss:0.001863, valid loss:0.001222
Epoch:38, Train loss:0.001883, valid loss:0.001243
Epoch:39, Train loss:0.001852, valid loss:0.001450
Epoch:40, Train loss:0.001878, valid loss:0.001235
Epoch:41, Train loss:0.001655, valid loss:0.001080
Epoch:42, Train loss:0.001653, valid loss:0.001099
Epoch:43, Train loss:0.001676, valid loss:0.001230
Epoch:44, Train loss:0.001623, valid loss:0.001128
Epoch:45, Train loss:0.001656, valid loss:0.001129
Epoch:46, Train loss:0.001625, valid loss:0.001111
Epoch:47, Train loss:0.001610, valid loss:0.001039
Epoch:48, Train loss:0.001609, valid loss:0.001173
Epoch:49, Train loss:0.001638, valid loss:0.001161
Epoch:50, Train loss:0.001605, valid loss:0.001074
Epoch:51, Train loss:0.001508, valid loss:0.001081
Epoch:52, Train loss:0.001509, valid loss:0.001014
Epoch:53, Train loss:0.001495, valid loss:0.001002
Epoch:54, Train loss:0.001496, valid loss:0.000992
Epoch:55, Train loss:0.001510, valid loss:0.001005
Epoch:56, Train loss:0.001496, valid loss:0.000999
Epoch:57, Train loss:0.001484, valid loss:0.000984
Epoch:58, Train loss:0.001482, valid loss:0.001004
Epoch:59, Train loss:0.001490, valid loss:0.000982
Epoch:60, Train loss:0.001487, valid loss:0.001093
training time 8200.776590824127
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.037288260913759266
plot_id,batch_id 0 1 miss% 0.03401232378911533
plot_id,batch_id 0 2 miss% 0.032858881117398986
plot_id,batch_id 0 3 miss% 0.031137644644709583
plot_id,batch_id 0 4 miss% 0.025509948291953417
plot_id,batch_id 0 5 miss% 0.03850209424985809
plot_id,batch_id 0 6 miss% 0.03403567133994897
plot_id,batch_id 0 7 miss% 0.03283979504300889
plot_id,batch_id 0 8 miss% 0.032426068518267154
plot_id,batch_id 0 9 miss% 0.01896063960468142
plot_id,batch_id 0 10 miss% 0.053977769870804264
plot_id,batch_id 0 11 miss% 0.04957449872344199
plot_id,batch_id 0 12 miss% 0.031513705990035785
plot_id,batch_id 0 13 miss% 0.03193404739986551
plot_id,batch_id 0 14 miss% 0.035563780864320854
plot_id,batch_id 0 15 miss% 0.05618653710580351
plot_id,batch_id 0 16 miss% 0.03861652019479436
plot_id,batch_id 0 17 miss% 0.03984144981615837
plot_id,batch_id 0 18 miss% 0.034855519719562474
plot_id,batch_id 0 19 miss% 0.042844118432597555
plot_id,batch_id 0 20 miss% 0.04372244536558578
plot_id,batch_id 0 21 miss% 0.032558995296008655
plot_id,batch_id 0 22 miss% 0.02752281951612728
plot_id,batch_id 0 23 miss% 0.0218483784241617
plot_id,batch_id 0 24 miss% 0.027863706026835917
plot_id,batch_id 0 25 miss% 0.02249286137619614
plot_id,batch_id 0 26 miss% 0.024387437055582852
plot_id,batch_id 0 27 miss% 0.03074081584622398
plot_id,batch_id 0 28 miss% 0.022134143648992757
plot_id,batch_id 0 29 miss% 0.0272584582839261
plot_id,batch_id 0 30 miss% 0.04305101916282538
plot_id,batch_id 0 31 miss% 0.03362842043884946
plot_id,batch_id 0 32 miss% 0.03398751425186882
plot_id,batch_id 0 33 miss% 0.021638564297257273
plot_id,batch_id 0 34 miss% 0.03069168294029807
plot_id,batch_id 0 35 miss% 0.03563156889281541
plot_id,batch_id 0 36 miss% 0.05007998061745081
plot_id,batch_id 0 37 miss% 0.023494461259543374
plot_id,batch_id 0 38 miss% 0.016092222647711794
plot_id,batch_id 0 39 miss% 0.022229989600133906
plot_id,batch_id 0 40 miss% 0.052368937310419174
plot_id,batch_id 0 41 miss% 0.028249415679151818
plot_id,batch_id 0 42 miss% 0.03837279466301862
plot_id,batch_id 0 43 miss% 0.02875022439723853
plot_id,batch_id 0 44 miss% 0.018298049263260594
plot_id,batch_id 0 45 miss% 0.03863727316817119
plot_id,batch_id 0 46 miss% 0.0299452273049339
plot_id,batch_id 0 47 miss% 0.024468550412843675
plot_id,batch_id 0 48 miss% 0.0194875201160509
plot_id,batch_id 0 49 miss% 0.021603979901295776
plot_id,batch_id 0 50 miss% 0.04776590566874196
plot_id,batch_id 0 51 miss% 0.023533970719845452
plot_id,batch_id 0 52 miss% 0.02522526790785269
plot_id,batch_id 0 53 miss% 0.021757773522742472
plot_id,batch_id 0 54 miss% 0.02342050986323776
plot_id,batch_id 0 55 miss% 0.046629132979624864
plot_id,batch_id 0 56 miss% 0.02511378734679156
plot_id,batch_id 0 57 miss% 0.01930957680752704
plot_id,batch_id 0 58 miss% 0.018248209347865727
plot_id,batch_id 0 59 miss% 0.02362352526038246
plot_id,batch_id 0 60 miss% 0.042970173624959716
plot_id,batch_id 0 61 miss% 0.04370331647260636
plot_id,batch_id 0 62 miss% 0.03128443076484643
plot_id,batch_id 0 63 miss% 0.03460021055735935
plot_id,batch_id 0 64 miss% 0.030672274561795547
plot_id,batch_id 0 65 miss% 0.052574880885783014
plot_id,batch_id 0 66 miss% 0.0747381325352174
plot_id,batch_id 0 67 miss% 0.029568751348142477
plot_id,batch_id 0 68 miss% 0.02939435962609216
plot_id,batch_id 0 69 miss% the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  35921
Epoch:0, Train loss:0.489925, valid loss:0.473190
Epoch:1, Train loss:0.022455, valid loss:0.003469
Epoch:2, Train loss:0.005945, valid loss:0.002859
Epoch:3, Train loss:0.004515, valid loss:0.002519
Epoch:4, Train loss:0.003744, valid loss:0.002786
Epoch:5, Train loss:0.003420, valid loss:0.001797
Epoch:6, Train loss:0.003062, valid loss:0.001327
Epoch:7, Train loss:0.002922, valid loss:0.001430
Epoch:8, Train loss:0.002826, valid loss:0.001843
Epoch:9, Train loss:0.002740, valid loss:0.001331
Epoch:10, Train loss:0.002609, valid loss:0.001426
Epoch:11, Train loss:0.001944, valid loss:0.001065
Epoch:12, Train loss:0.001918, valid loss:0.001063
Epoch:13, Train loss:0.001889, valid loss:0.000966
Epoch:14, Train loss:0.001884, valid loss:0.001078
Epoch:15, Train loss:0.001839, valid loss:0.001094
Epoch:16, Train loss:0.001794, valid loss:0.000932
Epoch:17, Train loss:0.001798, valid loss:0.000921
Epoch:18, Train loss:0.001760, valid loss:0.000924
Epoch:19, Train loss:0.001711, valid loss:0.000995
Epoch:20, Train loss:0.001722, valid loss:0.000987
Epoch:21, Train loss:0.001367, valid loss:0.001224
Epoch:22, Train loss:0.001371, valid loss:0.000851
Epoch:23, Train loss:0.001346, valid loss:0.000770
Epoch:24, Train loss:0.001352, valid loss:0.000870
Epoch:25, Train loss:0.001327, valid loss:0.000983
Epoch:26, Train loss:0.001341, valid loss:0.000772
Epoch:27, Train loss:0.001315, valid loss:0.000814
Epoch:28, Train loss:0.001301, valid loss:0.000808
Epoch:29, Train loss:0.001312, valid loss:0.000703
Epoch:30, Train loss:0.001302, valid loss:0.000778
Epoch:31, Train loss:0.001111, valid loss:0.000684
Epoch:32, Train loss:0.001114, valid loss:0.000718
Epoch:33, Train loss:0.001102, valid loss:0.000678
Epoch:34, Train loss:0.001102, valid loss:0.000701
Epoch:35, Train loss:0.001101, valid loss:0.000664
Epoch:36, Train loss:0.001084, valid loss:0.000679
Epoch:37, Train loss:0.001080, valid loss:0.000666
Epoch:38, Train loss:0.001076, valid loss:0.000630
Epoch:39, Train loss:0.001087, valid loss:0.000653
Epoch:40, Train loss:0.001059, valid loss:0.000663
Epoch:41, Train loss:0.000978, valid loss:0.000624
Epoch:42, Train loss:0.000972, valid loss:0.000641
Epoch:43, Train loss:0.000978, valid loss:0.000609
Epoch:44, Train loss:0.000971, valid loss:0.000630
Epoch:45, Train loss:0.000972, valid loss:0.000614
Epoch:46, Train loss:0.000959, valid loss:0.000593
Epoch:47, Train loss:0.000967, valid loss:0.000641
Epoch:48, Train loss:0.000957, valid loss:0.000607
Epoch:49, Train loss:0.000958, valid loss:0.000596
Epoch:50, Train loss:0.000957, valid loss:0.000623
Epoch:51, Train loss:0.000907, valid loss:0.000582
Epoch:52, Train loss:0.000909, valid loss:0.000587
Epoch:53, Train loss:0.000904, valid loss:0.000579
Epoch:54, Train loss:0.000902, valid loss:0.000578
Epoch:55, Train loss:0.000904, valid loss:0.000574
Epoch:56, Train loss:0.000901, valid loss:0.000589
Epoch:57, Train loss:0.000900, valid loss:0.000585
Epoch:58, Train loss:0.000897, valid loss:0.000584
Epoch:59, Train loss:0.000899, valid loss:0.000593
Epoch:60, Train loss:0.000894, valid loss:0.000590
training time 8211.216396570206
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.049283983183884744
plot_id,batch_id 0 1 miss% 0.028939835991373505
plot_id,batch_id 0 2 miss% 0.03031997787899465
plot_id,batch_id 0 3 miss% 0.03345241362503476
plot_id,batch_id 0 4 miss% 0.025325259927026483
plot_id,batch_id 0 5 miss% 0.04798308447660998
plot_id,batch_id 0 6 miss% 0.035521551969279176
plot_id,batch_id 0 7 miss% 0.025726879464848633
plot_id,batch_id 0 8 miss% 0.01676562619180058
plot_id,batch_id 0 9 miss% 0.029827745631030515
plot_id,batch_id 0 10 miss% 0.04434945177085457
plot_id,batch_id 0 11 miss% 0.047688002970634115
plot_id,batch_id 0 12 miss% 0.034158477486887634
plot_id,batch_id 0 13 miss% 0.021363865879717364
plot_id,batch_id 0 14 miss% 0.026448047796099723
plot_id,batch_id 0 15 miss% 0.041360471576690566
plot_id,batch_id 0 16 miss% 0.031400270890338144
plot_id,batch_id 0 17 miss% 0.04042152875304601
plot_id,batch_id 0 18 miss% 0.02638205087453293
plot_id,batch_id 0 19 miss% 0.02389115179379278
plot_id,batch_id 0 20 miss% 0.03807223957206349
plot_id,batch_id 0 21 miss% 0.014845898879570019
plot_id,batch_id 0 22 miss% 0.024475077231011062
plot_id,batch_id 0 23 miss% 0.025919310805058424
plot_id,batch_id 0 24 miss% 0.023168500640159167
plot_id,batch_id 0 25 miss% 0.02799942882692373
plot_id,batch_id 0 26 miss% 0.018031067404229825
plot_id,batch_id 0 27 miss% 0.025007510595565323
plot_id,batch_id 0 28 miss% 0.025257457562430956
plot_id,batch_id 0 29 miss% 0.026697924494604768
plot_id,batch_id 0 30 miss% 0.04556954842394573
plot_id,batch_id 0 31 miss% 0.025775295537602178
plot_id,batch_id 0 32 miss% 0.025941879489420304
plot_id,batch_id 0 33 miss% 0.025152618275718488
plot_id,batch_id 0 34 miss% 0.015954016620043723
plot_id,batch_id 0 35 miss% 0.04831215764862143
plot_id,batch_id 0 36 miss% 0.029182647231969183
plot_id,batch_id 0 37 miss% 0.01659552126021001
plot_id,batch_id 0 38 miss% 0.01936965916343947
plot_id,batch_id 0 39 miss% 0.01568032146495727
plot_id,batch_id 0 40 miss% 0.0741451364635374
plot_id,batch_id 0 41 miss% 0.027965058899649193
plot_id,batch_id 0 42 miss% 0.023526609225308776
plot_id,batch_id 0 43 miss% 0.022273054857595738
plot_id,batch_id 0 44 miss% 0.024299357884557164
plot_id,batch_id 0 45 miss% 0.03634242918275159
plot_id,batch_id 0 46 miss% 0.02933157702847725
plot_id,batch_id 0 47 miss% 0.021065675104010455
plot_id,batch_id 0 48 miss% 0.014972069090219163
plot_id,batch_id 0 49 miss% 0.018162773216521155
plot_id,batch_id 0 50 miss% 0.03499233468126646
plot_id,batch_id 0 51 miss% 0.020134917435541865
plot_id,batch_id 0 52 miss% 0.01847003156783478
plot_id,batch_id 0 53 miss% 0.011189808071462826
plot_id,batch_id 0 54 miss% 0.026123624405625392
plot_id,batch_id 0 55 miss% 0.050482668439940086
plot_id,batch_id 0 56 miss% 0.027606305495719914
plot_id,batch_id 0 57 miss% 0.01842700119149031
plot_id,batch_id 0 58 miss% 0.026249674738083668
plot_id,batch_id 0 59 miss% 0.020311983950570062
plot_id,batch_id 0 60 miss% 0.04058159795054982
plot_id,batch_id 0 61 miss% 0.03826991493261651
plot_id,batch_id 0 62 miss% 0.030857473915295242
plot_id,batch_id 0 63 miss% 0.029254407412966504
plot_id,batch_id 0 64 miss% 0.024373234276847584
plot_id,batch_id 0 65 miss% 0.045917169812710115
plot_id,batch_id 0 66 miss% 0.03631090284628537
plot_id,batch_id 0 67 miss% 0.025712128400127626
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  107025
Epoch:0, Train loss:0.591207, valid loss:0.587610
Epoch:1, Train loss:0.058626, valid loss:0.015970
Epoch:2, Train loss:0.016675, valid loss:0.005967
Epoch:3, Train loss:0.010822, valid loss:0.006721
Epoch:4, Train loss:0.008839, valid loss:0.004768
Epoch:5, Train loss:0.007677, valid loss:0.004422
Epoch:6, Train loss:0.006633, valid loss:0.003311
Epoch:7, Train loss:0.006098, valid loss:0.003087
Epoch:8, Train loss:0.005334, valid loss:0.002927
Epoch:9, Train loss:0.004895, valid loss:0.003178
Epoch:10, Train loss:0.004697, valid loss:0.002217
Epoch:11, Train loss:0.003248, valid loss:0.001805
Epoch:12, Train loss:0.003153, valid loss:0.002284
Epoch:13, Train loss:0.003304, valid loss:0.002416
Epoch:14, Train loss:0.002965, valid loss:0.001910
Epoch:15, Train loss:0.002879, valid loss:0.001925
Epoch:16, Train loss:0.002861, valid loss:0.001765
Epoch:17, Train loss:0.002715, valid loss:0.001523
Epoch:18, Train loss:0.002765, valid loss:0.001749
Epoch:19, Train loss:0.002751, valid loss:0.001670
Epoch:20, Train loss:0.002581, valid loss:0.001767
Epoch:21, Train loss:0.001983, valid loss:0.001332
Epoch:22, Train loss:0.001876, valid loss:0.001190
Epoch:23, Train loss:0.001897, valid loss:0.001425
Epoch:24, Train loss:0.001899, valid loss:0.001545
Epoch:25, Train loss:0.001893, valid loss:0.001344
Epoch:26, Train loss:0.001881, valid loss:0.001310
Epoch:27, Train loss:0.001822, valid loss:0.001239
Epoch:28, Train loss:0.001766, valid loss:0.001292
Epoch:29, Train loss:0.001769, valid loss:0.001236
Epoch:30, Train loss:0.001780, valid loss:0.001304
Epoch:31, Train loss:0.001435, valid loss:0.001113
Epoch:32, Train loss:0.001439, valid loss:0.001235
Epoch:33, Train loss:0.001415, valid loss:0.001118
Epoch:34, Train loss:0.001391, valid loss:0.001101
Epoch:35, Train loss:0.001413, valid loss:0.001119
Epoch:36, Train loss:0.001380, valid loss:0.001040
Epoch:37, Train loss:0.001350, valid loss:0.000978
Epoch:38, Train loss:0.001368, valid loss:0.001150
Epoch:39, Train loss:0.001362, valid loss:0.001120
Epoch:40, Train loss:0.001409, valid loss:0.001132
Epoch:41, Train loss:0.001214, valid loss:0.001016
Epoch:42, Train loss:0.001185, valid loss:0.001022
Epoch:43, Train loss:0.001198, valid loss:0.000993
Epoch:44, Train loss:0.001181, valid loss:0.001032
Epoch:45, Train loss:0.001175, valid loss:0.001027
Epoch:46, Train loss:0.001191, valid loss:0.001022
Epoch:47, Train loss:0.001159, valid loss:0.001002
Epoch:48, Train loss:0.001161, valid loss:0.001022
Epoch:49, Train loss:0.001162, valid loss:0.001006
Epoch:50, Train loss:0.001179, valid loss:0.001034
Epoch:51, Train loss:0.001093, valid loss:0.000973
Epoch:52, Train loss:0.001080, valid loss:0.001001
Epoch:53, Train loss:0.001076, valid loss:0.000977
Epoch:54, Train loss:0.001074, valid loss:0.000977
Epoch:55, Train loss:0.001075, valid loss:0.000969
Epoch:56, Train loss:0.001078, valid loss:0.000967
Epoch:57, Train loss:0.001067, valid loss:0.000970
Epoch:58, Train loss:0.001065, valid loss:0.000964
Epoch:59, Train loss:0.001070, valid loss:0.000983
Epoch:60, Train loss:0.001064, valid loss:0.000982
training time 8204.495976924896
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.037605054264815876
plot_id,batch_id 0 1 miss% 0.027446740402475456
plot_id,batch_id 0 2 miss% 0.021151944917125003
plot_id,batch_id 0 3 miss% 0.022465020761433564
plot_id,batch_id 0 4 miss% 0.02507472377520998
plot_id,batch_id 0 5 miss% 0.0318915273550517
plot_id,batch_id 0 6 miss% 0.033887707739387124
plot_id,batch_id 0 7 miss% 0.02453113713683383
plot_id,batch_id 0 8 miss% 0.0253289239985973
plot_id,batch_id 0 9 miss% 0.030862604893089576
plot_id,batch_id 0 10 miss% 0.036220035870261985
plot_id,batch_id 0 11 miss% 0.03267073824534541
plot_id,batch_id 0 12 miss% 0.027063973953673215
plot_id,batch_id 0 13 miss% 0.03673406979317357
plot_id,batch_id 0 14 miss% 0.034808841398305396
plot_id,batch_id 0 15 miss% 0.034457506189167805
plot_id,batch_id 0 16 miss% 0.024496424850419647
plot_id,batch_id 0 17 miss% 0.05369515664980326
plot_id,batch_id 0 18 miss% 0.03555047855654184
plot_id,batch_id 0 19 miss% 0.03016218961958384
plot_id,batch_id 0 20 miss% 0.06357604632318666
plot_id,batch_id 0 21 miss% 0.03536178299599399
plot_id,batch_id 0 22 miss% 0.020358415252871086
plot_id,batch_id 0 23 miss% 0.029378922940455483
plot_id,batch_id 0 24 miss% 0.027685539678047384
plot_id,batch_id 0 25 miss% 0.041255800907645565
plot_id,batch_id 0 26 miss% 0.027755613872321446
plot_id,batch_id 0 27 miss% 0.027320749107814655
plot_id,batch_id 0 28 miss% 0.018265630793445297
plot_id,batch_id 0 29 miss% 0.029231893664566178
plot_id,batch_id 0 30 miss% 0.04673169441255234
plot_id,batch_id 0 31 miss% 0.030089308317691705
plot_id,batch_id 0 32 miss% 0.02866621653471766
plot_id,batch_id 0 33 miss% 0.0263862348850609
plot_id,batch_id 0 34 miss% 0.02791236655294933
plot_id,batch_id 0 35 miss% 0.05982649166351488
plot_id,batch_id 0 36 miss% 0.039273363207936875
plot_id,batch_id 0 37 miss% 0.03643406644085933
plot_id,batch_id 0 38 miss% 0.03655650208748479
plot_id,batch_id 0 39 miss% 0.02596391931042681
plot_id,batch_id 0 40 miss% 0.06129439246145447
plot_id,batch_id 0 41 miss% 0.019466664276010025
plot_id,batch_id 0 42 miss% 0.01993344503719597
plot_id,batch_id 0 43 miss% 0.020875784529116218
plot_id,batch_id 0 44 miss% 0.021316554635328094
plot_id,batch_id 0 45 miss% 0.02578471167568718
plot_id,batch_id 0 46 miss% 0.021889948469684065
plot_id,batch_id 0 47 miss% 0.030838471819796306
plot_id,batch_id 0 48 miss% 0.022354272677691897
plot_id,batch_id 0 49 miss% 0.02724690162351979
plot_id,batch_id 0 50 miss% 0.0247423008216809
plot_id,batch_id 0 51 miss% 0.01786979230586633
plot_id,batch_id 0 52 miss% 0.028418674847135734
plot_id,batch_id 0 53 miss% 0.01662476219911382
plot_id,batch_id 0 54 miss% 0.024495073769398017
plot_id,batch_id 0 55 miss% 0.033346688583963
plot_id,batch_id 0 56 miss% 0.020517040459687025
plot_id,batch_id 0 57 miss% 0.028036984654407335
plot_id,batch_id 0 58 miss% 0.022933986677129616
plot_id,batch_id 0 59 miss% 0.02290907497083936
plot_id,batch_id 0 60 miss% 0.03143706520705713
plot_id,batch_id 0 61 miss% 0.031487069085426
plot_id,batch_id 0 62 miss% 0.023357544440018085
plot_id,batch_id 0 63 miss% 0.0405201712750705
plot_id,batch_id 0 64 miss% 0.027784092437702766
plot_id,batch_id 0 65 miss% 0.031226458582908217
plot_id,batch_id 0 66 miss% 0.04561082165794449
plot_id,batch_id 0 67 miss% 0.023457032547901564
plot_id,batch_id 0 68 miss% 0.031058932887141944
plot_id,batch_id 0 69 miss% 0.023713631086449166
plot_id,batch_id 0 68 miss% 0.026591035590095322
plot_id,batch_id 0 69 miss% 0.01777278112767862
plot_id,batch_id 0 70 miss% 0.03761072210214226
plot_id,batch_id 0 71 miss% 0.06508106763668513
plot_id,batch_id 0 72 miss% 0.03632932278437056
plot_id,batch_id 0 73 miss% 0.027409193747281693
plot_id,batch_id 0 74 miss% 0.033464362402214294
plot_id,batch_id 0 75 miss% 0.046582762510391534
plot_id,batch_id 0 76 miss% 0.05056543457393678
plot_id,batch_id 0 77 miss% 0.024815696857675044
plot_id,batch_id 0 78 miss% 0.04109903729514104
plot_id,batch_id 0 79 miss% 0.039165189922117635
plot_id,batch_id 0 80 miss% 0.04035359876799492
plot_id,batch_id 0 81 miss% 0.02213014560890949
plot_id,batch_id 0 82 miss% 0.026263314553425034
plot_id,batch_id 0 83 miss% 0.020329420687444325
plot_id,batch_id 0 84 miss% 0.027456269341526587
plot_id,batch_id 0 85 miss% 0.03814391174043377
plot_id,batch_id 0 86 miss% 0.019372674430033612
plot_id,batch_id 0 87 miss% 0.029828612187484473
plot_id,batch_id 0 88 miss% 0.02256864951720998
plot_id,batch_id 0 89 miss% 0.02992284699401553
plot_id,batch_id 0 90 miss% 0.04820733774447283
plot_id,batch_id 0 91 miss% 0.03584177797420231
plot_id,batch_id 0 92 miss% 0.0308169989275957
plot_id,batch_id 0 93 miss% 0.022464710728573
plot_id,batch_id 0 94 miss% 0.03323051441935439
plot_id,batch_id 0 95 miss% 0.05809091126705445
plot_id,batch_id 0 96 miss% 0.035980942960274695
plot_id,batch_id 0 97 miss% 0.04954413200387076
plot_id,batch_id 0 98 miss% 0.033210933340406285
plot_id,batch_id 0 99 miss% 0.03102086615157357
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04928398 0.02893984 0.03031998 0.03345241 0.02532526 0.04798308
 0.03552155 0.02572688 0.01676563 0.02982775 0.04434945 0.047688
 0.03415848 0.02136387 0.02644805 0.04136047 0.03140027 0.04042153
 0.02638205 0.02389115 0.03807224 0.0148459  0.02447508 0.02591931
 0.0231685  0.02799943 0.01803107 0.02500751 0.02525746 0.02669792
 0.04556955 0.0257753  0.02594188 0.02515262 0.01595402 0.04831216
 0.02918265 0.01659552 0.01936966 0.01568032 0.07414514 0.02796506
 0.02352661 0.02227305 0.02429936 0.03634243 0.02933158 0.02106568
 0.01497207 0.01816277 0.03499233 0.02013492 0.01847003 0.01118981
 0.02612362 0.05048267 0.02760631 0.018427   0.02624967 0.02031198
 0.0405816  0.03826991 0.03085747 0.02925441 0.02437323 0.04591717
 0.0363109  0.02571213 0.02659104 0.01777278 0.03761072 0.06508107
 0.03632932 0.02740919 0.03346436 0.04658276 0.05056543 0.0248157
 0.04109904 0.03916519 0.0403536  0.02213015 0.02626331 0.02032942
 0.02745627 0.03814391 0.01937267 0.02982861 0.02256865 0.02992285
 0.04820734 0.03584178 0.030817   0.02246471 0.03323051 0.05809091
 0.03598094 0.04954413 0.03321093 0.03102087]
for model  208 the mean error 0.03096231855633169
all id 208 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 31 out win 4 err 0.03096231855633169
Launcher: Job 209 completed in 8403 seconds.
Launcher: Task 250 done. Exiting.
0.030133765696319495
plot_id,batch_id 0 70 miss% 0.03638039557981232
plot_id,batch_id 0 71 miss% 0.05189468794708767
plot_id,batch_id 0 72 miss% 0.05034055701266896
plot_id,batch_id 0 73 miss% 0.03198602767807206
plot_id,batch_id 0 74 miss% 0.038788191173546364
plot_id,batch_id 0 75 miss% 0.06078851536688267
plot_id,batch_id 0 76 miss% 0.03695457172094114
plot_id,batch_id 0 77 miss% 0.06064649936042524
plot_id,batch_id 0 78 miss% 0.03463720639192956
plot_id,batch_id 0 79 miss% 0.03683491150676772
plot_id,batch_id 0 80 miss% 0.03794681749195677
plot_id,batch_id 0 81 miss% 0.040201368661225034
plot_id,batch_id 0 82 miss% 0.0392461985341555
plot_id,batch_id 0 83 miss% 0.051686883213499936
plot_id,batch_id 0 84 miss% 0.024386833290876365
plot_id,batch_id 0 85 miss% 0.06735345871860515
plot_id,batch_id 0 86 miss% 0.03792482741493199
plot_id,batch_id 0 87 miss% 0.036401444293809246
plot_id,batch_id 0 88 miss% 0.04303039538672006
plot_id,batch_id 0 89 miss% 0.03228472365644846
plot_id,batch_id 0 90 miss% 0.040450393622261535
plot_id,batch_id 0 91 miss% 0.0388206259775961
plot_id,batch_id 0 92 miss% 0.03392567162433518
plot_id,batch_id 0 93 miss% 0.032014731417747765
plot_id,batch_id 0 94 miss% 0.03703772643531512
plot_id,batch_id 0 95 miss% 0.04128490708714032
plot_id,batch_id 0 96 miss% 0.03839088861164609
plot_id,batch_id 0 97 miss% 0.05359534477546147
plot_id,batch_id 0 98 miss% 0.04256541747678989
plot_id,batch_id 0 99 miss% 0.044548384946241054
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03728826 0.03401232 0.03285888 0.03113764 0.02550995 0.03850209
 0.03403567 0.0328398  0.03242607 0.01896064 0.05397777 0.0495745
 0.03151371 0.03193405 0.03556378 0.05618654 0.03861652 0.03984145
 0.03485552 0.04284412 0.04372245 0.032559   0.02752282 0.02184838
 0.02786371 0.02249286 0.02438744 0.03074082 0.02213414 0.02725846
 0.04305102 0.03362842 0.03398751 0.02163856 0.03069168 0.03563157
 0.05007998 0.02349446 0.01609222 0.02222999 0.05236894 0.02824942
 0.03837279 0.02875022 0.01829805 0.03863727 0.02994523 0.02446855
 0.01948752 0.02160398 0.04776591 0.02353397 0.02522527 0.02175777
 0.02342051 0.04662913 0.02511379 0.01930958 0.01824821 0.02362353
 0.04297017 0.04370332 0.03128443 0.03460021 0.03067227 0.05257488
 0.07473813 0.02956875 0.02939436 0.03013377 0.0363804  0.05189469
 0.05034056 0.03198603 0.03878819 0.06078852 0.03695457 0.0606465
 0.03463721 0.03683491 0.03794682 0.04020137 0.0392462  0.05168688
 0.02438683 0.06735346 0.03792483 0.03640144 0.0430304  0.03228472
 0.04045039 0.03882063 0.03392567 0.03201473 0.03703773 0.04128491
 0.03839089 0.05359534 0.04256542 0.04454838]
for model  155 the mean error 0.035503332946674966
all id 155 hidden_dim 16 learning_rate 0.02 num_layers 5 frames 25 out win 5 err 0.035503332946674966
Launcher: Job 156 completed in 8406 seconds.
Launcher: Task 142 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  79249
Epoch:0, Train loss:0.307377, valid loss:0.282084
Epoch:1, Train loss:0.018037, valid loss:0.003129
Epoch:2, Train loss:0.005042, valid loss:0.002448
Epoch:3, Train loss:0.003674, valid loss:0.002372
Epoch:4, Train loss:0.003148, valid loss:0.001658
Epoch:5, Train loss:0.002912, valid loss:0.001565
Epoch:6, Train loss:0.002633, valid loss:0.001644
Epoch:7, Train loss:0.002470, valid loss:0.001654
Epoch:8, Train loss:0.002312, valid loss:0.001265
Epoch:9, Train loss:0.002263, valid loss:0.001162
Epoch:10, Train loss:0.002206, valid loss:0.001256
Epoch:11, Train loss:0.001534, valid loss:0.001023
Epoch:12, Train loss:0.001495, valid loss:0.000928
Epoch:13, Train loss:0.001507, valid loss:0.000873
Epoch:14, Train loss:0.001446, valid loss:0.000829
Epoch:15, Train loss:0.001414, valid loss:0.000965
Epoch:16, Train loss:0.001416, valid loss:0.000827
Epoch:17, Train loss:0.001422, valid loss:0.000766
Epoch:18, Train loss:0.001387, valid loss:0.000737
Epoch:19, Train loss:0.001331, valid loss:0.000822
Epoch:20, Train loss:0.001321, valid loss:0.000782
Epoch:21, Train loss:0.001025, valid loss:0.000774
Epoch:22, Train loss:0.001006, valid loss:0.000709
Epoch:23, Train loss:0.001001, valid loss:0.000679
Epoch:24, Train loss:0.000991, valid loss:0.000724
Epoch:25, Train loss:0.000998, valid loss:0.000615
Epoch:26, Train loss:0.000979, valid loss:0.000666
Epoch:27, Train loss:0.000952, valid loss:0.000672
Epoch:28, Train loss:0.000982, valid loss:0.000587
Epoch:29, Train loss:0.000960, valid loss:0.000658
Epoch:30, Train loss:0.000954, valid loss:0.000794
Epoch:31, Train loss:0.000779, valid loss:0.000619
Epoch:32, Train loss:0.000794, valid loss:0.000554
Epoch:33, Train loss:0.000791, valid loss:0.000570
Epoch:34, Train loss:0.000779, valid loss:0.000659
Epoch:35, Train loss:0.000772, valid loss:0.000561
Epoch:36, Train loss:0.000762, valid loss:0.000560
Epoch:37, Train loss:0.000757, valid loss:0.000578
Epoch:38, Train loss:0.000775, valid loss:0.000546
Epoch:39, Train loss:0.000764, valid loss:0.000567
Epoch:40, Train loss:0.000756, valid loss:0.000585
Epoch:41, Train loss:0.000670, valid loss:0.000516
Epoch:42, Train loss:0.000670, valid loss:0.000517
Epoch:43, Train loss:0.000670, valid loss:0.000519
Epoch:44, Train loss:0.000668, valid loss:0.000506
Epoch:45, Train loss:0.000664, valid loss:0.000519
Epoch:46, Train loss:0.000664, valid loss:0.000509
Epoch:47, Train loss:0.000661, valid loss:0.000507
Epoch:48, Train loss:0.000661, valid loss:0.000509
Epoch:49, Train loss:0.000658, valid loss:0.000534
Epoch:50, Train loss:0.000652, valid loss:0.000505
Epoch:51, Train loss:0.000619, valid loss:0.000516
Epoch:52, Train loss:0.000618, valid loss:0.000539
Epoch:53, Train loss:0.000617, valid loss:0.000509
Epoch:54, Train loss:0.000619, valid loss:0.000519
Epoch:55, Train loss:0.000612, valid loss:0.000523
Epoch:56, Train loss:0.000611, valid loss:0.000514
Epoch:57, Train loss:0.000610, valid loss:0.000530
Epoch:58, Train loss:0.000608, valid loss:0.000513
Epoch:59, Train loss:0.000611, valid loss:0.000532
Epoch:60, Train loss:0.000607, valid loss:0.000516
training time 8221.424384117126
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.042115739350981084
plot_id,batch_id 0 1 miss% 0.01793450358888982
plot_id,batch_id 0 2 miss% 0.025257946042627488
plot_id,batch_id 0 3 miss% 0.023668783135206803
plot_id,batch_id 0 4 miss% 0.022876574650886616
plot_id,batch_id 0 5 miss% 0.04123061944602684
plot_id,batch_id 0 6 miss% 0.02315134921974355
plot_id,batch_id 0 7 miss% 0.01836051151825277
plot_id,batch_id 0 8 miss% 0.017671802983974014
plot_id,batch_id 0 9 miss% 0.013036913389721448
plot_id,batch_id 0 10 miss% 0.046823090921115286
plot_id,batch_id 0 11 miss% 0.03966581329857514
plot_id,batch_id 0 12 miss% 0.017282034086751687
plot_id,batch_id 0 13 miss% 0.02659033873342009
plot_id,batch_id 0 14 miss% 0.026902625495719944
plot_id,batch_id 0 15 miss% 0.03537817033616504
plot_id,batch_id 0 16 miss% 0.023579828042647202
plot_id,batch_id 0 17 miss% 0.036553579701702976
plot_id,batch_id 0 18 miss% 0.020864463794297752
plot_id,batch_id 0 19 miss% 0.028915247779739764
plot_id,batch_id 0 20 miss% 0.038187549820348325
plot_id,batch_id 0 21 miss% 0.029529776229946327
plot_id,batch_id 0 22 miss% 0.030775848176900537
plot_id,batch_id 0 23 miss% 0.027178938104240435
plot_id,batch_id 0 24 miss% 0.025694811268739376
plot_id,batch_id 0 25 miss% 0.041700529183399004
plot_id,batch_id 0 26 miss% 0.019745646101596542
plot_id,batch_id 0 27 miss% 0.028646617640808354
plot_id,batch_id 0 28 miss% 0.022606056477574715
plot_id,batch_id 0 29 miss% 0.02041361014299388
plot_id,batch_id 0 30 miss% 0.060997344122805344
plot_id,batch_id 0 31 miss% 0.028188522166713914
plot_id,batch_id 0 32 miss% 0.018138620229207755
plot_id,batch_id 0 33 miss% 0.028368680435070716
plot_id,batch_id 0 34 miss% 0.027743887509084807
plot_id,batch_id 0 35 miss% 0.03824045245291694
plot_id,batch_id 0 36 miss% 0.033840902992844576
plot_id,batch_id 0 37 miss% 0.02663741673694034
plot_id,batch_id 0 38 miss% 0.02690627633127373
plot_id,batch_id 0 39 miss% 0.022102591367684145
plot_id,batch_id 0 40 miss% 0.08912991782400038
plot_id,batch_id 0 41 miss% 0.028094056622909896
plot_id,batch_id 0 42 miss% 0.012298767215676158
plot_id,batch_id 0 43 miss% 0.02870594322391255
plot_id,batch_id 0 44 miss% 0.026504716959413723
plot_id,batch_id 0 45 miss% 0.06645029409168969
plot_id,batch_id 0 46 miss% 0.028445564159580838
plot_id,batch_id 0 47 miss% 0.02147904456862552
plot_id,batch_id 0 48 miss% 0.01636742961060115
plot_id,batch_id 0 49 miss% 0.018809456954496675
plot_id,batch_id 0 50 miss% 0.02499400328640088
plot_id,batch_id 0 51 miss% 0.02459008676292069
plot_id,batch_id 0 52 miss% 0.013253820482085689
plot_id,batch_id 0 53 miss% 0.010809408486993882
plot_id,batch_id 0 54 miss% 0.0283101924971033
plot_id,batch_id 0 55 miss% 0.03401134128936727
plot_id,batch_id 0 56 miss% 0.02884750683213829
plot_id,batch_id 0 57 miss% 0.024546834943176585
plot_id,batch_id 0 58 miss% 0.02109481191466048
plot_id,batch_id 0 59 miss% 0.030318256330447064
plot_id,batch_id 0 60 miss% 0.03199980256822184
plot_id,batch_id 0 61 miss% 0.035900477787961606
plot_id,batch_id 0 62 miss% 0.04220441101717423
plot_id,batch_id 0 63 miss% 0.042939971944828965
plot_id,batch_id 0 64 miss% 0.026328476331515516
plot_id,batch_id 0 65 miss% 0.04629599583830205
plot_id,batch_id 0 66 miss% 0.021884406541208518
plot_id,batch_id 0 67 miss% 0.030835623991542122
plot_id,batch_id 0 70 miss% 0.041716552431575155
plot_id,batch_id 0 71 miss% 0.04580653443522684
plot_id,batch_id 0 72 miss% 0.03917796844500517
plot_id,batch_id 0 73 miss% 0.030473903806534062
plot_id,batch_id 0 74 miss% 0.029952206087424292
plot_id,batch_id 0 75 miss% 0.04323733469149731
plot_id,batch_id 0 76 miss% 0.06337634092849492
plot_id,batch_id 0 77 miss% 0.023737982164942682
plot_id,batch_id 0 78 miss% 0.023354100807110215
plot_id,batch_id 0 79 miss% 0.04369659536103781
plot_id,batch_id 0 80 miss% 0.04729300743274664
plot_id,batch_id 0 81 miss% 0.029269137221106592
plot_id,batch_id 0 82 miss% 0.02214113083088514
plot_id,batch_id 0 83 miss% 0.027004549825259943
plot_id,batch_id 0 84 miss% 0.04169696786486031
plot_id,batch_id 0 85 miss% 0.056232759813954376
plot_id,batch_id 0 86 miss% 0.02779659085330478
plot_id,batch_id 0 87 miss% 0.030460497211220693
plot_id,batch_id 0 88 miss% 0.03623211454675983
plot_id,batch_id 0 89 miss% 0.023480460972748928
plot_id,batch_id 0 90 miss% 0.034660223865409116
plot_id,batch_id 0 91 miss% 0.03112546626086982
plot_id,batch_id 0 92 miss% 0.03501902982964711
plot_id,batch_id 0 93 miss% 0.029190499972201547
plot_id,batch_id 0 94 miss% 0.03269186473220241
plot_id,batch_id 0 95 miss% 0.0456206336920389
plot_id,batch_id 0 96 miss% 0.038737725637463805
plot_id,batch_id 0 97 miss% 0.04940146654290012
plot_id,batch_id 0 98 miss% 0.02772045444634884
plot_id,batch_id 0 99 miss% 0.03221850575656797
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03760505 0.02744674 0.02115194 0.02246502 0.02507472 0.03189153
 0.03388771 0.02453114 0.02532892 0.0308626  0.03622004 0.03267074
 0.02706397 0.03673407 0.03480884 0.03445751 0.02449642 0.05369516
 0.03555048 0.03016219 0.06357605 0.03536178 0.02035842 0.02937892
 0.02768554 0.0412558  0.02775561 0.02732075 0.01826563 0.02923189
 0.04673169 0.03008931 0.02866622 0.02638623 0.02791237 0.05982649
 0.03927336 0.03643407 0.0365565  0.02596392 0.06129439 0.01946666
 0.01993345 0.02087578 0.02131655 0.02578471 0.02188995 0.03083847
 0.02235427 0.0272469  0.0247423  0.01786979 0.02841867 0.01662476
 0.02449507 0.03334669 0.02051704 0.02803698 0.02293399 0.02290907
 0.03143707 0.03148707 0.02335754 0.04052017 0.02778409 0.03122646
 0.04561082 0.02345703 0.03105893 0.02371363 0.04171655 0.04580653
 0.03917797 0.0304739  0.02995221 0.04323733 0.06337634 0.02373798
 0.0233541  0.0436966  0.04729301 0.02926914 0.02214113 0.02700455
 0.04169697 0.05623276 0.02779659 0.0304605  0.03623211 0.02348046
 0.03466022 0.03112547 0.03501903 0.0291905  0.03269186 0.04562063
 0.03873773 0.04940147 0.02772045 0.03221851]
for model  16 the mean error 0.032072063114895084
all id 16 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 21 out win 4 err 0.032072063114895084
Launcher: Job 17 completed in 8413 seconds.
Launcher: Task 194 done. Exiting.
plot_id,batch_id 0 68 miss% 0.024259098111432224
plot_id,batch_id 0 69 miss% 0.02871670111515104
plot_id,batch_id 0 70 miss% 0.03713794299648616
plot_id,batch_id 0 71 miss% 0.04536071798026579
plot_id,batch_id 0 72 miss% 0.028746812377453586
plot_id,batch_id 0 73 miss% 0.03143761794523961
plot_id,batch_id 0 74 miss% 0.033314730459910805
plot_id,batch_id 0 75 miss% 0.04608338536847078
plot_id,batch_id 0 76 miss% 0.0706912563188273
plot_id,batch_id 0 77 miss% 0.026879015938735696
plot_id,batch_id 0 78 miss% 0.034686445825503766
plot_id,batch_id 0 79 miss% 0.04144118329834837
plot_id,batch_id 0 80 miss% 0.02829288354882739
plot_id,batch_id 0 81 miss% 0.017549025272849055
plot_id,batch_id 0 82 miss% 0.03133801912854484
plot_id,batch_id 0 83 miss% 0.027019714658590693
plot_id,batch_id 0 84 miss% 0.01892537704265947
plot_id,batch_id 0 85 miss% 0.03943146479097937
plot_id,batch_id 0 86 miss% 0.03487604506131779
plot_id,batch_id 0 87 miss% 0.02763121543972776
plot_id,batch_id 0 88 miss% 0.0318358927839272
plot_id,batch_id 0 89 miss% 0.01991100587304777
plot_id,batch_id 0 90 miss% 0.04304160032198241
plot_id,batch_id 0 91 miss% 0.030995717529346205
plot_id,batch_id 0 92 miss% 0.02308401728069738
plot_id,batch_id 0 93 miss% 0.01830861693935955
plot_id,batch_id 0 94 miss% 0.04079589086140051
plot_id,batch_id 0 95 miss% 0.029178569237755032
plot_id,batch_id 0 96 miss% 0.034208246103247304
plot_id,batch_id 0 97 miss% 0.04786435003940511
plot_id,batch_id 0 98 miss% 0.03803538296086354
plot_id,batch_id 0 99 miss% 0.0408214841336389
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04211574 0.0179345  0.02525795 0.02366878 0.02287657 0.04123062
 0.02315135 0.01836051 0.0176718  0.01303691 0.04682309 0.03966581
 0.01728203 0.02659034 0.02690263 0.03537817 0.02357983 0.03655358
 0.02086446 0.02891525 0.03818755 0.02952978 0.03077585 0.02717894
 0.02569481 0.04170053 0.01974565 0.02864662 0.02260606 0.02041361
 0.06099734 0.02818852 0.01813862 0.02836868 0.02774389 0.03824045
 0.0338409  0.02663742 0.02690628 0.02210259 0.08912992 0.02809406
 0.01229877 0.02870594 0.02650472 0.06645029 0.02844556 0.02147904
 0.01636743 0.01880946 0.024994   0.02459009 0.01325382 0.01080941
 0.02831019 0.03401134 0.02884751 0.02454683 0.02109481 0.03031826
 0.0319998  0.03590048 0.04220441 0.04293997 0.02632848 0.046296
 0.02188441 0.03083562 0.0242591  0.0287167  0.03713794 0.04536072
 0.02874681 0.03143762 0.03331473 0.04608339 0.07069126 0.02687902
 0.03468645 0.04144118 0.02829288 0.01754903 0.03133802 0.02701971
 0.01892538 0.03943146 0.03487605 0.02763122 0.03183589 0.01991101
 0.0430416  0.03099572 0.02308402 0.01830862 0.04079589 0.02917857
 0.03420825 0.04786435 0.03803538 0.04082148]
for model  196 the mean error 0.03070854059828483
all id 196 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 31 out win 4 err 0.03070854059828483
Launcher: Job 197 completed in 8410 seconds.
Launcher: Task 27 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  134801
Epoch:0, Train loss:0.658736, valid loss:0.667970
Epoch:1, Train loss:0.508569, valid loss:0.516499
Epoch:2, Train loss:0.493053, valid loss:0.515318
Epoch:3, Train loss:0.490311, valid loss:0.513919
Epoch:4, Train loss:0.488827, valid loss:0.513339
Epoch:5, Train loss:0.487989, valid loss:0.513762
Epoch:6, Train loss:0.487721, valid loss:0.513151
Epoch:7, Train loss:0.487340, valid loss:0.513099
Epoch:8, Train loss:0.487235, valid loss:0.513048
Epoch:9, Train loss:0.486986, valid loss:0.512980
Epoch:10, Train loss:0.486864, valid loss:0.513662
Epoch:11, Train loss:0.485912, valid loss:0.512267
Epoch:12, Train loss:0.485821, valid loss:0.512378
Epoch:13, Train loss:0.485844, valid loss:0.512212
Epoch:14, Train loss:0.485885, valid loss:0.512458
Epoch:15, Train loss:0.485789, valid loss:0.512360
Epoch:16, Train loss:0.485703, valid loss:0.512143
Epoch:17, Train loss:0.485669, valid loss:0.512317
Epoch:18, Train loss:0.485654, valid loss:0.512278
Epoch:19, Train loss:0.485582, valid loss:0.512114
Epoch:20, Train loss:0.485522, valid loss:0.512264
Epoch:21, Train loss:0.485169, valid loss:0.511951
Epoch:22, Train loss:0.485136, valid loss:0.512235
Epoch:23, Train loss:0.485093, valid loss:0.512040
Epoch:24, Train loss:0.485126, valid loss:0.511992
Epoch:25, Train loss:0.485075, valid loss:0.511995
Epoch:26, Train loss:0.485082, valid loss:0.512141
Epoch:27, Train loss:0.485048, valid loss:0.512064
Epoch:28, Train loss:0.485022, valid loss:0.512084
Epoch:29, Train loss:0.485034, valid loss:0.511919
Epoch:30, Train loss:0.485012, valid loss:0.511928
Epoch:31, Train loss:0.484847, valid loss:0.511912
Epoch:32, Train loss:0.484836, valid loss:0.512052
Epoch:33, Train loss:0.484825, valid loss:0.511949
Epoch:34, Train loss:0.484795, valid loss:0.511919
Epoch:35, Train loss:0.484811, valid loss:0.511917
Epoch:36, Train loss:0.484797, valid loss:0.511932
Epoch:37, Train loss:0.484794, valid loss:0.511929
Epoch:38, Train loss:0.484810, valid loss:0.511856
Epoch:39, Train loss:0.484769, valid loss:0.511898
Epoch:40, Train loss:0.484768, valid loss:0.511926
Epoch:41, Train loss:0.484695, valid loss:0.511850
Epoch:42, Train loss:0.484687, valid loss:0.511865
Epoch:43, Train loss:0.484689, valid loss:0.511884
Epoch:44, Train loss:0.484684, valid loss:0.511867
Epoch:45, Train loss:0.484692, valid loss:0.511874
Epoch:46, Train loss:0.484687, valid loss:0.511847
Epoch:47, Train loss:0.484671, valid loss:0.511889
Epoch:48, Train loss:0.484670, valid loss:0.511841
Epoch:49, Train loss:0.484670, valid loss:0.511864
Epoch:50, Train loss:0.484659, valid loss:0.511882
Epoch:51, Train loss:0.484632, valid loss:0.511833
Epoch:52, Train loss:0.484630, valid loss:0.511858
Epoch:53, Train loss:0.484628, valid loss:0.511837
Epoch:54, Train loss:0.484625, valid loss:0.511833
Epoch:55, Train loss:0.484624, valid loss:0.511839
Epoch:56, Train loss:0.484624, valid loss:0.511835
Epoch:57, Train loss:0.484623, valid loss:0.511825
Epoch:58, Train loss:0.484624, valid loss:0.511832
Epoch:59, Train loss:0.484619, valid loss:0.511829
Epoch:60, Train loss:0.484613, valid loss:0.511841
training time 8262.519480228424
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.6983376694421127
plot_id,batch_id 0 1 miss% 0.7727551777495859
plot_id,batch_id 0 2 miss% 0.778474057051111
plot_id,batch_id 0 3 miss% 0.7891085776558466
plot_id,batch_id 0 4 miss% 0.7921103755243466
plot_id,batch_id 0 5 miss% 0.6895040394089593
plot_id,batch_id 0 6 miss% 0.7651718291421801
plot_id,batch_id 0 7 miss% 0.7804068592399883
plot_id,batch_id 0 8 miss% 0.7909804167200861
plot_id,batch_id 0 9 miss% 0.7975281260493542
plot_id,batch_id 0 10 miss% 0.6728348673894897
plot_id,batch_id 0 11 miss% 0.7651645470217086
plot_id,batch_id 0 12 miss% 0.777235169269034
plot_id,batch_id 0 13 miss% 0.7858656638972403
plot_id,batch_id 0 14 miss% 0.7882437766073652
plot_id,batch_id 0 15 miss% 0.6823233787217229
plot_id,batch_id 0 16 miss% 0.756952315247097
plot_id,batch_id 0 17 miss% 0.7774505167000693
plot_id,batch_id 0 18 miss% 0.7838650960568585
plot_id,batch_id 0 19 miss% 0.7916078691464612
plot_id,batch_id 0 20 miss% 0.7439046788869226
plot_id,batch_id 0 21 miss% 0.7822357600609781
plot_id,batch_id 0 22 miss% 0.7869536811111075
plot_id,batch_id 0 23 miss% 0.7936193153788406
plot_id,batch_id 0 24 miss% 0.7943151666745717
plot_id,batch_id 0 25 miss% 0.7285421588218042
plot_id,batch_id 0 26 miss% 0.7809866615450225
plot_id,batch_id 0 27 miss% 0.787888998836651
plot_id,batch_id 0 28 miss% 0.792179722484444
plot_id,batch_id 0 29 miss% 0.7932331952993567
plot_id,batch_id 0 30 miss% 0.726864829058575
plot_id,batch_id 0 31 miss% 0.7761071874267172
plot_id,batch_id 0 32 miss% 0.7903261880537994
plot_id,batch_id 0 33 miss% 0.7915782946289035
plot_id,batch_id 0 34 miss% 0.793856778441124
plot_id,batch_id 0 35 miss% 0.7267916401164702
plot_id,batch_id 0 36 miss% 0.7826270160100905
plot_id,batch_id 0 37 miss% 0.7802072746878723
plot_id,batch_id 0 38 miss% 0.7947123403712043
plot_id,batch_id 0 39 miss% 0.7956806102500025
plot_id,batch_id 0 40 miss% 0.7630021318964822
plot_id,batch_id 0 41 miss% 0.787661270705978
plot_id,batch_id 0 42 miss% 0.7898107481749029
plot_id,batch_id 0 43 miss% 0.7967997269407481
plot_id,batch_id 0 44 miss% 0.7958593618831882
plot_id,batch_id 0 45 miss% 0.757768775263112
plot_id,batch_id 0 46 miss% 0.7861406648485457
plot_id,batch_id 0 47 miss% 0.7919264500218026
plot_id,batch_id 0 48 miss% 0.7993971212378539
plot_id,batch_id 0 49 miss% 0.7974586519059693
plot_id,batch_id 0 50 miss% 0.7666541865177079
plot_id,batch_id 0 51 miss% 0.7884451312013803
plot_id,batch_id 0 52 miss% 0.7925338012582656
plot_id,batch_id 0 53 miss% 0.796792156784989
plot_id,batch_id 0 54 miss% 0.798724120812811
plot_id,batch_id 0 55 miss% 0.7550109652388503
plot_id,batch_id 0 56 miss% 0.7847455378473728
plot_id,batch_id 0 57 miss% 0.794537709614667
plot_id,batch_id 0 58 miss% 0.7946730093052079
plot_id,batch_id 0 59 miss% 0.7966578481047134
plot_id,batch_id 0 60 miss% 0.6108677991930931
plot_id,batch_id 0 61 miss% 0.7288607326455585
plot_id,batch_id 0 62 miss% 0.7602184937574731
plot_id,batch_id 0 63 miss% 0.7762569120566921
plot_id,batch_id 0 64 miss% 0.7807506507270163
plot_id,batch_id 0 65 miss% 0.6113362696793182
plot_id,batch_id 0 66 miss% 0.7296225485941321
plot_id,batch_id 0 67 miss% 0.746757511527429
plot_id,batch_id 0 68 miss% 0.7708964177802053
plot_id,batch_id 0 69 miss% 0.7760061115179512
plot_id,batch_id 0 70 miss% 0.5750347347224638
plot_id,batch_id 0 71 miss% 0.7371383656243572
plot_id,batch_id 0 72 miss% 0.7425037407506786
plot_id,batch_id 0 73 miss% 0.7603760267795593
plot_id,batch_id 0 74 miss% 0.772848366704684
plot_id,batch_id 0 75 miss% 0.5611112166178159
plot_id,batch_id 0 76 miss% 0.6864790071092601
plot_id,batch_id 0 77 miss% 0.7296562462535321
plot_id,batch_id 0 78 miss% 0.7559317258259219
plot_id,batch_id 0 79 miss% 0.768359130563751
plot_id,batch_id 0 80 miss% 0.6287890066320595
plot_id,batch_id 0 81 miss% 0.7519468050380798
plot_id,batch_id 0 82 miss% 0.7722381898877634
plot_id,batch_id 0 83 miss% 0.7826094140458308
plot_id,batch_id 0 84 miss% 0.7828947076667031
plot_id,batch_id 0 85 miss% 0.6357181567764644
plot_id,batch_id 0 86 miss% 0.7459199657152644
plot_id,batch_id 0 87 miss% 0.7651733975287085
plot_id,batch_id 0 88 miss% 0.7774121477411594
plot_id,batch_id 0 89 miss% 0.782473356884973
plot_id,batch_id 0 90 miss% 0.5992731390033043
plot_id,batch_id 0 91 miss% 0.7406192705656812
plot_id,batch_id 0 92 miss% 0.7575877259087034
plot_id,batch_id 0 93 miss% 0.7722081931237617
plot_id,batch_id 0 94 miss% 0.780478648762233
plot_id,batch_id 0 95 miss% 0.6033345749850264
plot_id,batch_id 0 96 miss% 0.720180943839305
plot_id,batch_id 0 97 miss% 0.7547794918122784
plot_id,batch_id 0 98 miss% 0.7666319315909451
plot_id,batch_id 0 99 miss% 0.773046969310568
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.69833767 0.77275518 0.77847406 0.78910858 0.79211038 0.68950404
 0.76517183 0.78040686 0.79098042 0.79752813 0.67283487 0.76516455
 0.77723517 0.78586566 0.78824378 0.68232338 0.75695232 0.77745052
 0.7838651  0.79160787 0.74390468 0.78223576 0.78695368 0.79361932
 0.79431517 0.72854216 0.78098666 0.787889   0.79217972 0.7932332
 0.72686483 0.77610719 0.79032619 0.79157829 0.79385678 0.72679164
 0.78262702 0.78020727 0.79471234 0.79568061 0.76300213 0.78766127
 0.78981075 0.79679973 0.79585936 0.75776878 0.78614066 0.79192645
 0.79939712 0.79745865 0.76665419 0.78844513 0.7925338  0.79679216
 0.79872412 0.75501097 0.78474554 0.79453771 0.79467301 0.79665785
 0.6108678  0.72886073 0.76021849 0.77625691 0.78075065 0.61133627
 0.72962255 0.74675751 0.77089642 0.77600611 0.57503473 0.73713837
 0.74250374 0.76037603 0.77284837 0.56111122 0.68647901 0.72965625
 0.75593173 0.76835913 0.62878901 0.75194681 0.77223819 0.78260941
 0.78289471 0.63571816 0.74591997 0.7651734  0.77741215 0.78247336
 0.59927314 0.74061927 0.75758773 0.77220819 0.78047865 0.60333457
 0.72018094 0.75477949 0.76663193 0.77304697]
for model  24 the mean error 0.7558745924099933
all id 24 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 21 out win 3 err 0.7558745924099933
Launcher: Job 25 completed in 8444 seconds.
Launcher: Task 200 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  107025
Epoch:0, Train loss:0.304599, valid loss:0.297865
Epoch:1, Train loss:0.020067, valid loss:0.002821
Epoch:2, Train loss:0.004765, valid loss:0.002478
Epoch:3, Train loss:0.003553, valid loss:0.001994
Epoch:4, Train loss:0.002781, valid loss:0.001536
Epoch:5, Train loss:0.002415, valid loss:0.001474
Epoch:6, Train loss:0.002126, valid loss:0.001532
Epoch:7, Train loss:0.002042, valid loss:0.001239
Epoch:8, Train loss:0.001885, valid loss:0.000911
Epoch:9, Train loss:0.001762, valid loss:0.000942
Epoch:10, Train loss:0.001683, valid loss:0.000901
Epoch:11, Train loss:0.001234, valid loss:0.001000
Epoch:12, Train loss:0.001225, valid loss:0.000733
Epoch:13, Train loss:0.001195, valid loss:0.000720
Epoch:14, Train loss:0.001161, valid loss:0.000727
Epoch:15, Train loss:0.001125, valid loss:0.000754
Epoch:16, Train loss:0.001114, valid loss:0.000722
Epoch:17, Train loss:0.001091, valid loss:0.000593
Epoch:18, Train loss:0.001062, valid loss:0.000850
Epoch:19, Train loss:0.001042, valid loss:0.000657
Epoch:20, Train loss:0.001043, valid loss:0.000686
Epoch:21, Train loss:0.000785, valid loss:0.000540
Epoch:22, Train loss:0.000786, valid loss:0.000583
Epoch:23, Train loss:0.000780, valid loss:0.000537
Epoch:24, Train loss:0.000785, valid loss:0.000529
Epoch:25, Train loss:0.000754, valid loss:0.000543
Epoch:26, Train loss:0.000773, valid loss:0.000548
Epoch:27, Train loss:0.000745, valid loss:0.000629
Epoch:28, Train loss:0.000736, valid loss:0.000516
Epoch:29, Train loss:0.000742, valid loss:0.000524
Epoch:30, Train loss:0.000723, valid loss:0.000549
Epoch:31, Train loss:0.000616, valid loss:0.000465
Epoch:32, Train loss:0.000614, valid loss:0.000470
Epoch:33, Train loss:0.000610, valid loss:0.000496
Epoch:34, Train loss:0.000615, valid loss:0.000457
Epoch:35, Train loss:0.000603, valid loss:0.000517
Epoch:36, Train loss:0.000610, valid loss:0.000483
Epoch:37, Train loss:0.000609, valid loss:0.000468
Epoch:38, Train loss:0.000594, valid loss:0.000459
Epoch:39, Train loss:0.000592, valid loss:0.000473
Epoch:40, Train loss:0.000592, valid loss:0.000457
Epoch:41, Train loss:0.000539, valid loss:0.000457
Epoch:42, Train loss:0.000534, valid loss:0.000435
Epoch:43, Train loss:0.000536, valid loss:0.000460
Epoch:44, Train loss:0.000532, valid loss:0.000440
Epoch:45, Train loss:0.000533, valid loss:0.000443
Epoch:46, Train loss:0.000528, valid loss:0.000444
Epoch:47, Train loss:0.000526, valid loss:0.000452
Epoch:48, Train loss:0.000524, valid loss:0.000438
Epoch:49, Train loss:0.000530, valid loss:0.000446
Epoch:50, Train loss:0.000519, valid loss:0.000436
Epoch:51, Train loss:0.000499, valid loss:0.000425
Epoch:52, Train loss:0.000498, valid loss:0.000425
Epoch:53, Train loss:0.000495, valid loss:0.000425
Epoch:54, Train loss:0.000496, valid loss:0.000437
Epoch:55, Train loss:0.000494, valid loss:0.000433
Epoch:56, Train loss:0.000494, valid loss:0.000426
Epoch:57, Train loss:0.000492, valid loss:0.000427
Epoch:58, Train loss:0.000491, valid loss:0.000430
Epoch:59, Train loss:0.000490, valid loss:0.000427
Epoch:60, Train loss:0.000490, valid loss:0.000422
training time 8305.234050273895
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.033184501983094104
plot_id,batch_id 0 1 miss% 0.03567239513874008
plot_id,batch_id 0 2 miss% 0.025754276370699096
plot_id,batch_id 0 3 miss% 0.030833605760868247
plot_id,batch_id 0 4 miss% 0.021632758840763287
plot_id,batch_id 0 5 miss% 0.04402220892188174
plot_id,batch_id 0 6 miss% 0.02894281312915589
plot_id,batch_id 0 7 miss% 0.02983839523249683
plot_id,batch_id 0 8 miss% 0.0222808266282499
plot_id,batch_id 0 9 miss% 0.018188879500490755
plot_id,batch_id 0 10 miss% 0.045676983959384944
plot_id,batch_id 0 11 miss% 0.027072056755351313
plot_id,batch_id 0 12 miss% 0.02114131255135333
plot_id,batch_id 0 13 miss% 0.024940561357878307
plot_id,batch_id 0 14 miss% 0.023482820793763053
plot_id,batch_id 0 15 miss% 0.02874889297319516
plot_id,batch_id 0 16 miss% 0.03447198137562439
plot_id,batch_id 0 17 miss% 0.02425616995586284
plot_id,batch_id 0 18 miss% 0.021588206347725826
plot_id,batch_id 0 19 miss% 0.028132946266742953
plot_id,batch_id 0 20 miss% 0.02046901618534684
plot_id,batch_id 0 21 miss% 0.02369980595321935
plot_id,batch_id 0 22 miss% 0.020581808852628076
plot_id,batch_id 0 23 miss% 0.024992544052283166
plot_id,batch_id 0 24 miss% 0.024916774673920696
plot_id,batch_id 0 25 miss% 0.03557381249564173
plot_id,batch_id 0 26 miss% 0.03586495801744055
plot_id,batch_id 0 27 miss% 0.02483445698114496
plot_id,batch_id 0 28 miss% 0.01782269619862866
plot_id,batch_id 0 29 miss% 0.025538682575796303
plot_id,batch_id 0 30 miss% 0.054554331408211205
plot_id,batch_id 0 31 miss% 0.027368875062304895
plot_id,batch_id 0 32 miss% 0.018352171542159817
plot_id,batch_id 0 33 miss% 0.02432536649847138
plot_id,batch_id 0 34 miss% 0.019472041368528394
plot_id,batch_id 0 35 miss% 0.03486972615484508
plot_id,batch_id 0 36 miss% 0.029727437051148994
plot_id,batch_id 0 37 miss% 0.01581408809128474
plot_id,batch_id 0 38 miss% 0.02131864240987772
plot_id,batch_id 0 39 miss% 0.015140052415127189
plot_id,batch_id 0 40 miss% 0.0443369316552643
plot_id,batch_id 0 41 miss% 0.03014846057486618
plot_id,batch_id 0 42 miss% 0.020122128937991642
plot_id,batch_id 0 43 miss% 0.0218057869993547
plot_id,batch_id 0 44 miss% 0.02215776572107781
plot_id,batch_id 0 45 miss% 0.04250956044320925
plot_id,batch_id 0 46 miss% 0.030290895657384884
plot_id,batch_id 0 47 miss% 0.02512998008685304
plot_id,batch_id 0 48 miss% 0.024624205361832484
plot_id,batch_id 0 49 miss% 0.027636338688577605
plot_id,batch_id 0 50 miss% 0.017241821003834704
plot_id,batch_id 0 51 miss% 0.02683780711505778
plot_id,batch_id 0 52 miss% 0.02421070551086459
plot_id,batch_id 0 53 miss% 0.015190488615323254
plot_id,batch_id 0 54 miss% 0.028433903912697272
plot_id,batch_id 0 55 miss% 0.03419209806835312
plot_id,batch_id 0 56 miss% 0.018961596534184878
plot_id,batch_id 0 57 miss% 0.020740807137945586
plot_id,batch_id 0 58 miss% 0.02546302191245701
plot_id,batch_id 0 59 miss% 0.025556020078205167
plot_id,batch_id 0 60 miss% 0.04776931457569107
plot_id,batch_id 0 61 miss% 0.020924777807815164
plot_id,batch_id 0 62 miss% 0.02937055537456179
plot_id,batch_id 0 63 miss% 0.027943742904343007
plot_id,batch_id 0 64 miss% 0.03025364889134433
plot_id,batch_id 0 65 miss% 0.0389784623536326
plot_id,batch_id 0 66 miss% 0.033532766925623445
plot_id,batch_id 0 67 miss% 0.02344481223505492
plot_id,batch_id 0 68 miss% 0.029254561393500866
plot_id,batch_id 0 69 miss% 0.024798774698709315
plot_id,batch_id 0 70 miss% 0.044712096043766685
plot_id,batch_id 0 71 miss% 0.037954181016193994
plot_id,batch_id 0 72 miss% 0.022782031765399075
plot_id,batch_id 0 73 miss% 0.025334877733894774
plot_id,batch_id 0 74 miss% 0.04156950535891699
plot_id,batch_id 0 75 miss% 0.027884575780062232
plot_id,batch_id 0 76 miss% 0.04215756672606982
plot_id,batch_id 0 77 miss% 0.032900568495556566
plot_id,batch_id 0 78 miss% 0.028567294283785433
plot_id,batch_id 0 79 miss% 0.039216548342152366
plot_id,batch_id 0 80 miss% 0.04156132698969139
plot_id,batch_id 0 81 miss% 0.025360206353621285
plot_id,batch_id 0 82 miss% 0.020158273920858238
plot_id,batch_id 0 83 miss% 0.027255434646974185
plot_id,batch_id 0 84 miss% 0.019575441240081533
plot_id,batch_id 0 85 miss% 0.05072460180941748
plot_id,batch_id 0 86 miss% 0.029377183612724518
plot_id,batch_id 0 87 miss% 0.02137457835388864
plot_id,batch_id 0 88 miss% 0.02624603625982399
plot_id,batch_id 0 89 miss% 0.024082706725994364
plot_id,batch_id 0 90 miss% 0.039228329225080696
plot_id,batch_id 0 91 miss% 0.03432227993177327
plot_id,batch_id 0 92 miss% 0.029218224525245612
plot_id,batch_id 0 93 miss% 0.028654924388354572
plot_id,batch_id 0 94 miss% 0.03654505395725352
plot_id,batch_id 0 95 miss% 0.0302995903539269
plot_id,batch_id 0 96 miss% 0.034141530026670405
plot_id,batch_id 0 97 miss% 0.039064540355014325
plot_id,batch_id 0 98 miss% 0.032627762310565654
plot_id,batch_id 0 99 miss% 0.02957228805674677
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0331845  0.0356724  0.02575428 0.03083361 0.02163276 0.04402221
 0.02894281 0.0298384  0.02228083 0.01818888 0.04567698 0.02707206
 0.02114131 0.02494056 0.02348282 0.02874889 0.03447198 0.02425617
 0.02158821 0.02813295 0.02046902 0.02369981 0.02058181 0.02499254
 0.02491677 0.03557381 0.03586496 0.02483446 0.0178227  0.02553868
 0.05455433 0.02736888 0.01835217 0.02432537 0.01947204 0.03486973
 0.02972744 0.01581409 0.02131864 0.01514005 0.04433693 0.03014846
 0.02012213 0.02180579 0.02215777 0.04250956 0.0302909  0.02512998
 0.02462421 0.02763634 0.01724182 0.02683781 0.02421071 0.01519049
 0.0284339  0.0341921  0.0189616  0.02074081 0.02546302 0.02555602
 0.04776931 0.02092478 0.02937056 0.02794374 0.03025365 0.03897846
 0.03353277 0.02344481 0.02925456 0.02479877 0.0447121  0.03795418
 0.02278203 0.02533488 0.04156951 0.02788458 0.04215757 0.03290057
 0.02856729 0.03921655 0.04156133 0.02536021 0.02015827 0.02725543
 0.01957544 0.0507246  0.02937718 0.02137458 0.02624604 0.02408271
 0.03922833 0.03432228 0.02921822 0.02865492 0.03654505 0.03029959
 0.03414153 0.03906454 0.03262776 0.02957229]
for model  177 the mean error 0.028794311815964493
all id 177 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 31 out win 3 err 0.028794311815964493
Launcher: Job 178 completed in 8491 seconds.
Launcher: Task 46 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  107025
Epoch:0, Train loss:0.591207, valid loss:0.587610
Epoch:1, Train loss:0.054868, valid loss:0.011054
Epoch:2, Train loss:0.014781, valid loss:0.006544
Epoch:3, Train loss:0.010351, valid loss:0.004858
Epoch:4, Train loss:0.008094, valid loss:0.003164
Epoch:5, Train loss:0.006688, valid loss:0.002943
Epoch:6, Train loss:0.005880, valid loss:0.002936
Epoch:7, Train loss:0.005565, valid loss:0.004274
Epoch:8, Train loss:0.005356, valid loss:0.002908
Epoch:9, Train loss:0.004705, valid loss:0.002603
Epoch:10, Train loss:0.004906, valid loss:0.002547
Epoch:11, Train loss:0.002991, valid loss:0.001615
Epoch:12, Train loss:0.002955, valid loss:0.001745
Epoch:13, Train loss:0.002982, valid loss:0.001739
Epoch:14, Train loss:0.002799, valid loss:0.001888
Epoch:15, Train loss:0.002935, valid loss:0.001580
Epoch:16, Train loss:0.002810, valid loss:0.001787
Epoch:17, Train loss:0.002676, valid loss:0.001623
Epoch:18, Train loss:0.002753, valid loss:0.001573
Epoch:19, Train loss:0.002664, valid loss:0.002061
Epoch:20, Train loss:0.002544, valid loss:0.001982
Epoch:21, Train loss:0.001827, valid loss:0.001183
Epoch:22, Train loss:0.001792, valid loss:0.001057
Epoch:23, Train loss:0.001823, valid loss:0.001071
Epoch:24, Train loss:0.001773, valid loss:0.001291
Epoch:25, Train loss:0.001794, valid loss:0.001252
Epoch:26, Train loss:0.001701, valid loss:0.001092
Epoch:27, Train loss:0.001698, valid loss:0.001145
Epoch:28, Train loss:0.001716, valid loss:0.001175
Epoch:29, Train loss:0.001663, valid loss:0.001081
Epoch:30, Train loss:0.001699, valid loss:0.001107
Epoch:31, Train loss:0.001305, valid loss:0.000969
Epoch:32, Train loss:0.001293, valid loss:0.000987
Epoch:33, Train loss:0.001272, valid loss:0.000952
Epoch:34, Train loss:0.001261, valid loss:0.000909
Epoch:35, Train loss:0.001290, valid loss:0.000976
Epoch:36, Train loss:0.001272, valid loss:0.000994
Epoch:37, Train loss:0.001241, valid loss:0.000915
Epoch:38, Train loss:0.001227, valid loss:0.000916
Epoch:39, Train loss:0.001224, valid loss:0.000971
Epoch:40, Train loss:0.001266, valid loss:0.001003
Epoch:41, Train loss:0.001060, valid loss:0.000921
Epoch:42, Train loss:0.001046, valid loss:0.000871
Epoch:43, Train loss:0.001038, valid loss:0.000851
Epoch:44, Train loss:0.001047, valid loss:0.000843
Epoch:45, Train loss:0.001039, valid loss:0.000885
Epoch:46, Train loss:0.001062, valid loss:0.000935
Epoch:47, Train loss:0.001021, valid loss:0.000872
Epoch:48, Train loss:0.001021, valid loss:0.000915
Epoch:49, Train loss:0.001011, valid loss:0.000839
Epoch:50, Train loss:0.001040, valid loss:0.000839
Epoch:51, Train loss:0.000939, valid loss:0.000840
Epoch:52, Train loss:0.000930, valid loss:0.000911
Epoch:53, Train loss:0.000929, valid loss:0.000817
Epoch:54, Train loss:0.000933, valid loss:0.000837
Epoch:55, Train loss:0.000921, valid loss:0.000800
Epoch:56, Train loss:0.000920, valid loss:0.000872
Epoch:57, Train loss:0.000927, valid loss:0.000818
Epoch:58, Train loss:0.000921, valid loss:0.000801
Epoch:59, Train loss:0.000915, valid loss:0.000805
Epoch:60, Train loss:0.000914, valid loss:0.000806
training time 8321.608100652695
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.027658123508007136
plot_id,batch_id 0 1 miss% 0.026358503781939217
plot_id,batch_id 0 2 miss% 0.018127899149569464
plot_id,batch_id 0 3 miss% 0.026547191127606316
plot_id,batch_id 0 4 miss% 0.021280418185799285
plot_id,batch_id 0 5 miss% 0.05056667191894032
plot_id,batch_id 0 6 miss% 0.02906297759361615
plot_id,batch_id 0 7 miss% 0.024508722932524613
plot_id,batch_id 0 8 miss% 0.030371832381128398
plot_id,batch_id 0 9 miss% 0.012620351521519893
plot_id,batch_id 0 10 miss% 0.031905748573760453
plot_id,batch_id 0 11 miss% 0.04718797094892786
plot_id,batch_id 0 12 miss% 0.021722996809533667
plot_id,batch_id 0 13 miss% 0.020562523958007596
plot_id,batch_id 0 14 miss% 0.024338173127520225
plot_id,batch_id 0 15 miss% 0.033578651962940645
plot_id,batch_id 0 16 miss% 0.026108561900481133
plot_id,batch_id 0 17 miss% 0.03331903652824827
plot_id,batch_id 0 18 miss% 0.02716117411292175
plot_id,batch_id 0 19 miss% 0.021821955248645088
plot_id,batch_id 0 20 miss% 0.08583298124919335
plot_id,batch_id 0 21 miss% 0.016372049943771037
plot_id,batch_id 0 22 miss% 0.03661363737428791
plot_id,batch_id 0 23 miss% 0.02815188464561668
plot_id,batch_id 0 24 miss% 0.02328521766081378
plot_id,batch_id 0 25 miss% 0.03663643828343645
plot_id,batch_id 0 26 miss% 0.03284377821842473
plot_id,batch_id 0 27 miss% 0.027564358767946762
plot_id,batch_id 0 28 miss% 0.027652064980602038
plot_id,batch_id 0 29 miss% 0.02001586717949073
plot_id,batch_id 0 30 miss% 0.045141610234845815
plot_id,batch_id 0 31 miss% 0.03655165826696466
plot_id,batch_id 0 32 miss% 0.02540658173403615
plot_id,batch_id 0 33 miss% 0.023481059521880523
plot_id,batch_id 0 34 miss% 0.02105688568818223
plot_id,batch_id 0 35 miss% 0.03871622500062017
plot_id,batch_id 0 36 miss% 0.03310513015947764
plot_id,batch_id 0 37 miss% 0.024479106841807364
plot_id,batch_id 0 38 miss% 0.022379835661495876
plot_id,batch_id 0 39 miss% 0.02221366785113114
plot_id,batch_id 0 40 miss% 0.0767914547261794
plot_id,batch_id 0 41 miss% 0.025683386377932417
plot_id,batch_id 0 42 miss% 0.016828211965345303
plot_id,batch_id 0 43 miss% 0.02417172498839151
plot_id,batch_id 0 44 miss% 0.023463834041927506
plot_id,batch_id 0 45 miss% 0.01609079556709233
plot_id,batch_id 0 46 miss% 0.022727182083332773
plot_id,batch_id 0 47 miss% 0.021836739773886758
plot_id,batch_id 0 48 miss% 0.02194791868898675
plot_id,batch_id 0 49 miss% 0.020177450376858162
plot_id,batch_id 0 50 miss% 0.025722247331649555
plot_id,batch_id 0 51 miss% 0.029321278762832647
plot_id,batch_id 0 52 miss% 0.018406952050966104
plot_id,batch_id 0 53 miss% 0.011830174639117215
plot_id,batch_id 0 54 miss% 0.020354592555549233
plot_id,batch_id 0 55 miss% 0.022292935695912112
plot_id,batch_id 0 56 miss% 0.02221615767502945
plot_id,batch_id 0 57 miss% 0.02073886734097902
plot_id,batch_id 0 58 miss% 0.01840871383058994
plot_id,batch_id 0 59 miss% 0.021128567007168433
plot_id,batch_id 0 60 miss% 0.03377297994480799
plot_id,batch_id 0 61 miss% 0.031834299954182896
plot_id,batch_id 0 62 miss% 0.0373147836315024
plot_id,batch_id 0 63 miss% 0.02431518212428904
plot_id,batch_id 0 64 miss% 0.033557602813750045
plot_id,batch_id 0 65 miss% 0.03131933723248528
plot_id,batch_id 0 66 miss% 0.03203116462454796
plot_id,batch_id 0 67 miss% 0.03209265672720213
plot_id,batch_id 0 68 miss% 0.022250536029089718
plot_id,batch_id 0 69 miss% the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 43200
total number of trained parameters  134801
Epoch:0, Train loss:0.658736, valid loss:0.667970
Epoch:1, Train loss:0.507865, valid loss:0.517352
Epoch:2, Train loss:0.492812, valid loss:0.515422
Epoch:3, Train loss:0.490078, valid loss:0.514403
Epoch:4, Train loss:0.488938, valid loss:0.513933
Epoch:5, Train loss:0.488372, valid loss:0.513796
Epoch:6, Train loss:0.487959, valid loss:0.513119
Epoch:7, Train loss:0.487734, valid loss:0.514729
Epoch:8, Train loss:0.487533, valid loss:0.513258
Epoch:9, Train loss:0.487314, valid loss:0.513028
Epoch:10, Train loss:0.487161, valid loss:0.512926
Epoch:11, Train loss:0.486067, valid loss:0.512812
Epoch:12, Train loss:0.485998, valid loss:0.512399
Epoch:13, Train loss:0.485951, valid loss:0.512796
Epoch:14, Train loss:0.485912, valid loss:0.512421
Epoch:15, Train loss:0.485930, valid loss:0.512500
Epoch:16, Train loss:0.485859, valid loss:0.512554
Epoch:17, Train loss:0.485823, valid loss:0.512243
Epoch:18, Train loss:0.485720, valid loss:0.512345
Epoch:19, Train loss:0.485838, valid loss:0.512574
Epoch:20, Train loss:0.485679, valid loss:0.512613
Epoch:21, Train loss:0.485209, valid loss:0.512059
Epoch:22, Train loss:0.485103, valid loss:0.512060
Epoch:23, Train loss:0.485161, valid loss:0.512039
Epoch:24, Train loss:0.485129, valid loss:0.512131
Epoch:25, Train loss:0.485130, valid loss:0.512322
Epoch:26, Train loss:0.485109, valid loss:0.512743
Epoch:27, Train loss:0.485110, valid loss:0.512511
Epoch:28, Train loss:0.485101, valid loss:0.512067
Epoch:29, Train loss:0.485070, valid loss:0.512249
Epoch:30, Train loss:0.485086, valid loss:0.511970
Epoch:31, Train loss:0.484834, valid loss:0.511984
Epoch:32, Train loss:0.484804, valid loss:0.511910
Epoch:33, Train loss:0.484800, valid loss:0.511948
Epoch:34, Train loss:0.484822, valid loss:0.511895
Epoch:35, Train loss:0.484807, valid loss:0.511901
Epoch:36, Train loss:0.484807, valid loss:0.512021
Epoch:37, Train loss:0.484788, valid loss:0.511916
Epoch:38, Train loss:0.484764, valid loss:0.511911
Epoch:39, Train loss:0.484776, valid loss:0.511889
Epoch:40, Train loss:0.484786, valid loss:0.511919
Epoch:41, Train loss:0.484670, valid loss:0.511859
Epoch:42, Train loss:0.484655, valid loss:0.511874
Epoch:43, Train loss:0.484672, valid loss:0.511878
Epoch:44, Train loss:0.484647, valid loss:0.511890
Epoch:45, Train loss:0.484659, valid loss:0.511869
Epoch:46, Train loss:0.484656, valid loss:0.511904
Epoch:47, Train loss:0.484655, valid loss:0.511904
Epoch:48, Train loss:0.484645, valid loss:0.511886
Epoch:49, Train loss:0.484635, valid loss:0.511857
Epoch:50, Train loss:0.484631, valid loss:0.511881
Epoch:51, Train loss:0.484593, valid loss:0.511846
Epoch:52, Train loss:0.484590, valid loss:0.511834
Epoch:53, Train loss:0.484590, valid loss:0.511860
Epoch:54, Train loss:0.484591, valid loss:0.511848
Epoch:55, Train loss:0.484585, valid loss:0.511860
Epoch:56, Train loss:0.484584, valid loss:0.511846
Epoch:57, Train loss:0.484582, valid loss:0.511856
Epoch:58, Train loss:0.484587, valid loss:0.511858
Epoch:59, Train loss:0.484580, valid loss:0.511867
Epoch:60, Train loss:0.484575, valid loss:0.511862
training time 8345.686485528946
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.7018010146553233
plot_id,batch_id 0 1 miss% 0.7725957283156482
plot_id,batch_id 0 2 miss% 0.7790433958655582
plot_id,batch_id 0 3 miss% 0.787694001104116
plot_id,batch_id 0 4 miss% 0.7948402196860266
plot_id,batch_id 0 5 miss% 0.6938823013229735
plot_id,batch_id 0 6 miss% 0.7662282716598642
plot_id,batch_id 0 7 miss% 0.7815327722501804
plot_id,batch_id 0 8 miss% 0.7898359790390309
plot_id,batch_id 0 9 miss% 0.7978007038974729
plot_id,batch_id 0 10 miss% 0.6683202324911939
plot_id,batch_id 0 11 miss% 0.7673030306369245
plot_id,batch_id 0 12 miss% 0.7769854891869938
plot_id,batch_id 0 13 miss% 0.7892293923751855
plot_id,batch_id 0 14 miss% 0.7897743155371009
plot_id,batch_id 0 15 miss% 0.6794866226022598
plot_id,batch_id 0 16 miss% 0.7571426229457711
plot_id,batch_id 0 17 miss% 0.7784802818872397
plot_id,batch_id 0 18 miss% 0.7843410223044154
plot_id,batch_id 0 19 miss% 0.7896518741341491
plot_id,batch_id 0 20 miss% 0.7404620560017222
plot_id,batch_id 0 21 miss% 0.7805502003175279
plot_id,batch_id 0 22 miss% 0.78977093480331
plot_id,batch_id 0 23 miss% 0.7938280836504742
plot_id,batch_id 0 24 miss% 0.7951137718001505
plot_id,batch_id 0 25 miss% 0.728641776635944
plot_id,batch_id 0 26 miss% 0.7805544292893513
plot_id,batch_id 0 27 miss% 0.7857128439891745
plot_id,batch_id 0 28 miss% 0.7920281237242225
plot_id,batch_id 0 29 miss% 0.7947618021627562
plot_id,batch_id 0 30 miss% 0.729566367686449
plot_id,batch_id 0 31 miss% 0.7758101979213458
plot_id,batch_id 0 32 miss% 0.7879752364344373
plot_id,batch_id 0 33 miss% 0.7925198516389858
plot_id,batch_id 0 34 miss% 0.793733585413912
plot_id,batch_id 0 35 miss% 0.7208219135179674
plot_id,batch_id 0 36 miss% 0.7839009488804459
plot_id,batch_id 0 37 miss% 0.781882589450669
plot_id,batch_id 0 38 miss% 0.7944786746929905
plot_id,batch_id 0 39 miss% 0.7952728735966647
plot_id,batch_id 0 40 miss% 0.770285620567364
plot_id,batch_id 0 41 miss% 0.7878185876491116
plot_id,batch_id 0 42 miss% 0.7896905636634333
plot_id,batch_id 0 43 miss% 0.7966277867006276
plot_id,batch_id 0 44 miss% 0.7959190147364456
plot_id,batch_id 0 45 miss% 0.7583052166884405
plot_id,batch_id 0 46 miss% 0.7856757652839906
plot_id,batch_id 0 47 miss% 0.7919600852889401
plot_id,batch_id 0 48 miss% 0.7997525909939416
plot_id,batch_id 0 49 miss% 0.796564455454068
plot_id,batch_id 0 50 miss% 0.7629035250346107
plot_id,batch_id 0 51 miss% 0.7864514055738618
plot_id,batch_id 0 52 miss% 0.7923282614644356
plot_id,batch_id 0 53 miss% 0.7968015877870303
plot_id,batch_id 0 54 miss% 0.7982370370280836
plot_id,batch_id 0 55 miss% 0.7550259146571596
plot_id,batch_id 0 56 miss% 0.7863406850803806
plot_id,batch_id 0 57 miss% 0.792807540617133
plot_id,batch_id 0 58 miss% 0.7922071929009773
plot_id,batch_id 0 59 miss% 0.7971246678669064
plot_id,batch_id 0 60 miss% 0.6112183474806188
plot_id,batch_id 0 61 miss% 0.7303479464183755
plot_id,batch_id 0 62 miss% 0.757979784736898
plot_id,batch_id 0 63 miss% 0.7787624860429789
plot_id,batch_id 0 64 miss% 0.7807536575369711
plot_id,batch_id 0 65 miss% 0.6068140458134277
plot_id,batch_id 0 66 miss% 0.7325930553546741
plot_id,batch_id 0 67 miss% 0.749181176728868
plot_id,batch_id 0 68 miss% 0.7751538752450439
plot_id,batch_id 0 69 miss% 0.7775786599967834
plot_id,batch_id 0 70 miss% 0.5738443479455461
plot_id,batch_id 0 71 miss% 0.7390551948386391
plot_id,batch_id 0 72 miss% 0.7459648791031948
plot_id,batch_id 0 73 miss% 0.7658008751120736
plot_id,batch_id 0 74 miss% 0.7722476652949009
plot_id,batch_id 0 75 miss% 0.5609454324201116
plot_id,batch_id 0 76 miss% 0.6836462301606745
plot_id,batch_id 0 77 miss% 0.7335545524840221
plot_id,batch_id 0 78 miss% 0.7543697628542568
plot_id,batch_id 0 79 miss% 0.7655400125761272
plot_id,batch_id 0 80 miss% 0.635288050230249
plot_id,batch_id 0 81 miss% 0.7548975600304401
plot_id,batch_id 0 82 miss% 0.775380123058149
plot_id,batch_id 0 83 miss% 0.7837800816863869
plot_id,batch_id 0 84 miss% 0.7817035541432883
plot_id,batch_id 0 85 miss% 0.6356753174699845
plot_id,batch_id 0 86 miss% 0.7443083885565897
plot_id,batch_id 0 87 miss% 0.7653552637116565
plot_id,batch_id 0 88 miss% 0.7810449104046928
plot_id,batch_id 0 89 miss% 0.7828996443518732
plot_id,batch_id 0 90 miss% 0.6055797987374756
plot_id,batch_id 0 91 miss% 0.741835423945856
plot_id,batch_id 0 92 miss% 0.7567875997925745
plot_id,batch_id 0 93 miss% 0.7703408693478417
plot_id,batch_id 0 94 miss% 0.7808161332337982
plot_id,batch_id 0 95 miss% 0.6040094176600992
plot_id,batch_id 0 96 miss% 0.7235453268608908
plot_id,batch_id 0 97 miss% 0.7494831674208966
plot_id,batch_id 0 98 miss% 0.7666582004798024
plot_id,batch_id 0 99 miss% 0.7740060283640179
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.70180101 0.77259573 0.7790434  0.787694   0.79484022 0.6938823
 0.76622827 0.78153277 0.78983598 0.7978007  0.66832023 0.76730303
 0.77698549 0.78922939 0.78977432 0.67948662 0.75714262 0.77848028
 0.78434102 0.78965187 0.74046206 0.7805502  0.78977093 0.79382808
 0.79511377 0.72864178 0.78055443 0.78571284 0.79202812 0.7947618
 0.72956637 0.7758102  0.78797524 0.79251985 0.79373359 0.72082191
 0.78390095 0.78188259 0.79447867 0.79527287 0.77028562 0.78781859
 0.78969056 0.79662779 0.79591901 0.75830522 0.78567577 0.79196009
 0.79975259 0.79656446 0.76290353 0.78645141 0.79232826 0.79680159
 0.79823704 0.75502591 0.78634069 0.79280754 0.79220719 0.79712467
 0.61121835 0.73034795 0.75797978 0.77876249 0.78075366 0.60681405
 0.73259306 0.74918118 0.77515388 0.77757866 0.57384435 0.73905519
 0.74596488 0.76580088 0.77224767 0.56094543 0.68364623 0.73355455
 0.75436976 0.76554001 0.63528805 0.75489756 0.77538012 0.78378008
 0.78170355 0.63567532 0.74430839 0.76535526 0.78104491 0.78289964
 0.6055798  0.74183542 0.7567876  0.77034087 0.78081613 0.60400942
 0.72354533 0.74948317 0.7666582  0.77400603]
for model  51 the mean error 0.7562492988617359
all id 51 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 21 out win 3 err 0.7562492988617359
Launcher: Job 52 completed in 8525 seconds.
Launcher: Task 191 done. Exiting.
0.02423763372421794
plot_id,batch_id 0 70 miss% 0.055712128512918006
plot_id,batch_id 0 71 miss% 0.054754819735933914
plot_id,batch_id 0 72 miss% 0.03288272945431578
plot_id,batch_id 0 73 miss% 0.0207302895744048
plot_id,batch_id 0 74 miss% 0.025549042258712313
plot_id,batch_id 0 75 miss% 0.03929960868173492
plot_id,batch_id 0 76 miss% 0.03796414186527822
plot_id,batch_id 0 77 miss% 0.035473610368865205
plot_id,batch_id 0 78 miss% 0.03776043217830752
plot_id,batch_id 0 79 miss% 0.03982211834025805
plot_id,batch_id 0 80 miss% 0.048698842035062584
plot_id,batch_id 0 81 miss% 0.03133921782313452
plot_id,batch_id 0 82 miss% 0.02381848784059505
plot_id,batch_id 0 83 miss% 0.022048672252044
plot_id,batch_id 0 84 miss% 0.025608556525788848
plot_id,batch_id 0 85 miss% 0.04373422488365609
plot_id,batch_id 0 86 miss% 0.03271103945198722
plot_id,batch_id 0 87 miss% 0.03324679625898361
plot_id,batch_id 0 88 miss% 0.02690420699124008
plot_id,batch_id 0 89 miss% 0.031438760539415134
plot_id,batch_id 0 90 miss% 0.031034512677316342
plot_id,batch_id 0 91 miss% 0.04026703311776824
plot_id,batch_id 0 92 miss% 0.038641002285512566
plot_id,batch_id 0 93 miss% 0.02784607689679379
plot_id,batch_id 0 94 miss% 0.044344092149766645
plot_id,batch_id 0 95 miss% 0.041069401424700454
plot_id,batch_id 0 96 miss% 0.03940430343390545
plot_id,batch_id 0 97 miss% 0.06147441943034813
plot_id,batch_id 0 98 miss% 0.04030808680046669
plot_id,batch_id 0 99 miss% 0.029789044746161758
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02765812 0.0263585  0.0181279  0.02654719 0.02128042 0.05056667
 0.02906298 0.02450872 0.03037183 0.01262035 0.03190575 0.04718797
 0.021723   0.02056252 0.02433817 0.03357865 0.02610856 0.03331904
 0.02716117 0.02182196 0.08583298 0.01637205 0.03661364 0.02815188
 0.02328522 0.03663644 0.03284378 0.02756436 0.02765206 0.02001587
 0.04514161 0.03655166 0.02540658 0.02348106 0.02105689 0.03871623
 0.03310513 0.02447911 0.02237984 0.02221367 0.07679145 0.02568339
 0.01682821 0.02417172 0.02346383 0.0160908  0.02272718 0.02183674
 0.02194792 0.02017745 0.02572225 0.02932128 0.01840695 0.01183017
 0.02035459 0.02229294 0.02221616 0.02073887 0.01840871 0.02112857
 0.03377298 0.0318343  0.03731478 0.02431518 0.0335576  0.03131934
 0.03203116 0.03209266 0.02225054 0.02423763 0.05571213 0.05475482
 0.03288273 0.02073029 0.02554904 0.03929961 0.03796414 0.03547361
 0.03776043 0.03982212 0.04869884 0.03133922 0.02381849 0.02204867
 0.02560856 0.04373422 0.03271104 0.0332468  0.02690421 0.03143876
 0.03103451 0.04026703 0.038641   0.02784608 0.04434409 0.0410694
 0.0394043  0.06147442 0.04030809 0.02978904]
for model  43 the mean error 0.03058850585356841
all id 43 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 21 out win 4 err 0.03058850585356841
Launcher: Job 44 completed in 8530 seconds.
Launcher: Task 147 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  107025
Epoch:0, Train loss:0.591207, valid loss:0.587610
Epoch:1, Train loss:0.060892, valid loss:0.014915
Epoch:2, Train loss:0.017101, valid loss:0.006465
Epoch:3, Train loss:0.012043, valid loss:0.005010
Epoch:4, Train loss:0.009843, valid loss:0.005451
Epoch:5, Train loss:0.007982, valid loss:0.003135
Epoch:6, Train loss:0.007265, valid loss:0.006110
Epoch:7, Train loss:0.006606, valid loss:0.004191
Epoch:8, Train loss:0.006670, valid loss:0.002790
Epoch:9, Train loss:0.005785, valid loss:0.003233
Epoch:10, Train loss:0.005662, valid loss:0.003406
Epoch:11, Train loss:0.003578, valid loss:0.002079
Epoch:12, Train loss:0.003486, valid loss:0.002189
Epoch:13, Train loss:0.003787, valid loss:0.001807
Epoch:14, Train loss:0.003458, valid loss:0.001773
Epoch:15, Train loss:0.003530, valid loss:0.001962
Epoch:16, Train loss:0.003374, valid loss:0.002227
Epoch:17, Train loss:0.003307, valid loss:0.002509
Epoch:18, Train loss:0.003387, valid loss:0.001718
Epoch:19, Train loss:0.003170, valid loss:0.001824
Epoch:20, Train loss:0.003184, valid loss:0.002156
Epoch:21, Train loss:0.002122, valid loss:0.001720
Epoch:22, Train loss:0.002439, valid loss:0.001712
Epoch:23, Train loss:0.002209, valid loss:0.001775
Epoch:24, Train loss:0.002128, valid loss:0.001415
Epoch:25, Train loss:0.002082, valid loss:0.001365
Epoch:26, Train loss:0.002082, valid loss:0.001261
Epoch:27, Train loss:0.001983, valid loss:0.001902
Epoch:28, Train loss:0.002102, valid loss:0.001330
Epoch:29, Train loss:0.002003, valid loss:0.001240
Epoch:30, Train loss:0.002005, valid loss:0.001369
Epoch:31, Train loss:0.001497, valid loss:0.001240
Epoch:32, Train loss:0.001471, valid loss:0.001187
Epoch:33, Train loss:0.001486, valid loss:0.001041
Epoch:34, Train loss:0.001443, valid loss:0.001223
Epoch:35, Train loss:0.001484, valid loss:0.001114
Epoch:36, Train loss:0.001495, valid loss:0.001185
Epoch:37, Train loss:0.001418, valid loss:0.001047
Epoch:38, Train loss:0.001463, valid loss:0.001049
Epoch:39, Train loss:0.001465, valid loss:0.001099
Epoch:40, Train loss:0.001440, valid loss:0.001052
Epoch:41, Train loss:0.001170, valid loss:0.000960
Epoch:42, Train loss:0.001174, valid loss:0.000946
Epoch:43, Train loss:0.001172, valid loss:0.000978
Epoch:44, Train loss:0.001183, valid loss:0.000961
Epoch:45, Train loss:0.001155, valid loss:0.000967
Epoch:46, Train loss:0.001166, valid loss:0.001106
Epoch:47, Train loss:0.001162, valid loss:0.000931
Epoch:48, Train loss:0.001129, valid loss:0.000947
Epoch:49, Train loss:0.001133, valid loss:0.000967
Epoch:50, Train loss:0.001172, valid loss:0.000934
Epoch:51, Train loss:0.001029, valid loss:0.000922
Epoch:52, Train loss:0.001022, valid loss:0.000960
Epoch:53, Train loss:0.001023, valid loss:0.000908
Epoch:54, Train loss:0.001010, valid loss:0.000903
Epoch:55, Train loss:0.001022, valid loss:0.000906
Epoch:56, Train loss:0.001016, valid loss:0.000909
Epoch:57, Train loss:0.001013, valid loss:0.000938
Epoch:58, Train loss:0.001008, valid loss:0.000911
Epoch:59, Train loss:0.001018, valid loss:0.000881
Epoch:60, Train loss:0.000995, valid loss:0.000915
training time 8333.943231344223
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.030498669355308695
plot_id,batch_id 0 1 miss% 0.021585554811003486
plot_id,batch_id 0 2 miss% 0.022763330747512377
plot_id,batch_id 0 3 miss% 0.030835431750253967
plot_id,batch_id 0 4 miss% 0.02327102042289757
plot_id,batch_id 0 5 miss% 0.04761184643216137
plot_id,batch_id 0 6 miss% 0.02907805175028573
plot_id,batch_id 0 7 miss% 0.02841334990525658
plot_id,batch_id 0 8 miss% 0.03132055598963573
plot_id,batch_id 0 9 miss% 0.023906890156820904
plot_id,batch_id 0 10 miss% 0.039301346844202466
plot_id,batch_id 0 11 miss% 0.03138492192471206
plot_id,batch_id 0 12 miss% 0.022427061248995437
plot_id,batch_id 0 13 miss% 0.02661142307097848
plot_id,batch_id 0 14 miss% 0.031285269027896676
plot_id,batch_id 0 15 miss% 0.043081394289875896
plot_id,batch_id 0 16 miss% 0.03411990050438237
plot_id,batch_id 0 17 miss% 0.03323839965762954
plot_id,batch_id 0 18 miss% 0.034310606533877935
plot_id,batch_id 0 19 miss% 0.04711972175895664
plot_id,batch_id 0 20 miss% 0.06214886251956075
plot_id,batch_id 0 21 miss% 0.02894664099153296
plot_id,batch_id 0 22 miss% 0.024492802010160058
plot_id,batch_id 0 23 miss% 0.02859769892438455
plot_id,batch_id 0 24 miss% 0.031697441750636775
plot_id,batch_id 0 25 miss% 0.02934635872558366
plot_id,batch_id 0 26 miss% 0.03318305874698704
plot_id,batch_id 0 27 miss% 0.028298028579519935
plot_id,batch_id 0 28 miss% 0.026069359889536518
plot_id,batch_id 0 29 miss% 0.03043305281314918
plot_id,batch_id 0 30 miss% 0.04542437106516944
plot_id,batch_id 0 31 miss% 0.030976251788249944
plot_id,batch_id 0 32 miss% 0.03198068098967345
plot_id,batch_id 0 33 miss% 0.022805279486691206
plot_id,batch_id 0 34 miss% 0.034776731203368355
plot_id,batch_id 0 35 miss% 0.0459957331340333
plot_id,batch_id 0 36 miss% 0.03576631101697187
plot_id,batch_id 0 37 miss% 0.019895427007109823
plot_id,batch_id 0 38 miss% 0.023385304587751423
plot_id,batch_id 0 39 miss% 0.027986881879330742
plot_id,batch_id 0 40 miss% 0.05238620005198463
plot_id,batch_id 0 41 miss% 0.017310942929182283
plot_id,batch_id 0 42 miss% 0.02210070363644186
plot_id,batch_id 0 43 miss% 0.036207930835048606
plot_id,batch_id 0 44 miss% 0.01715523731553341
plot_id,batch_id 0 45 miss% 0.0290908277755937
plot_id,batch_id 0 46 miss% 0.017134257975695472
plot_id,batch_id 0 47 miss% 0.028131004991634234
plot_id,batch_id 0 48 miss% 0.028629661893852798
plot_id,batch_id 0 49 miss% 0.028031557200507297
plot_id,batch_id 0 50 miss% 0.022539833239831563
plot_id,batch_id 0 51 miss% 0.027806737041208913
plot_id,batch_id 0 52 miss% 0.027182041910252693
plot_id,batch_id 0 53 miss% 0.019160781060022763
plot_id,batch_id 0 54 miss% 0.02738533043816926
plot_id,batch_id 0 55 miss% 0.026055814163277062
plot_id,batch_id 0 56 miss% 0.024950429986395656
plot_id,batch_id 0 57 miss% 0.01909537312322431
plot_id,batch_id 0 58 miss% 0.01703333338486738
plot_id,batch_id 0 59 miss% 0.026931362772178793
plot_id,batch_id 0 60 miss% 0.05763810564782495
plot_id,batch_id 0 61 miss% 0.025023769703266495
plot_id,batch_id 0 62 miss% 0.028758981846140435
plot_id,batch_id 0 63 miss% 0.03266038676387606
plot_id,batch_id 0 64 miss% 0.03138458392801796
plot_id,batch_id 0 65 miss% 0.04375593012400068
plot_id,batch_id 0 66 miss% 0.041434817853702524
plot_id,batch_id 0 67 miss% 0.025551914056839497
plot_id,batch_id 0 68 miss% 0.025470778072605667
plot_id,batch_id 0 69 miss% the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  46193
Epoch:0, Train loss:0.374471, valid loss:0.341930
Epoch:1, Train loss:0.227914, valid loss:0.231075
Epoch:2, Train loss:0.221663, valid loss:0.230704
Epoch:3, Train loss:0.220644, valid loss:0.230256
Epoch:4, Train loss:0.220106, valid loss:0.230127
Epoch:5, Train loss:0.219799, valid loss:0.229745
Epoch:6, Train loss:0.219594, valid loss:0.229746
Epoch:7, Train loss:0.219432, valid loss:0.229693
Epoch:8, Train loss:0.219305, valid loss:0.229595
Epoch:9, Train loss:0.219235, valid loss:0.229503
Epoch:10, Train loss:0.219102, valid loss:0.229582
Epoch:11, Train loss:0.218435, valid loss:0.229206
Epoch:12, Train loss:0.218403, valid loss:0.229242
Epoch:13, Train loss:0.218381, valid loss:0.229085
Epoch:14, Train loss:0.218376, valid loss:0.229049
Epoch:15, Train loss:0.218310, valid loss:0.229233
Epoch:16, Train loss:0.218296, valid loss:0.229060
Epoch:17, Train loss:0.218242, valid loss:0.229335
Epoch:18, Train loss:0.218258, valid loss:0.229187
Epoch:19, Train loss:0.218247, valid loss:0.229271
Epoch:20, Train loss:0.218193, valid loss:0.229320
Epoch:21, Train loss:0.217875, valid loss:0.229003
Epoch:22, Train loss:0.217884, valid loss:0.229031
Epoch:23, Train loss:0.217897, valid loss:0.228907
Epoch:24, Train loss:0.217853, valid loss:0.228924
Epoch:25, Train loss:0.217848, valid loss:0.228930
Epoch:26, Train loss:0.217828, valid loss:0.228879
Epoch:27, Train loss:0.217822, valid loss:0.228915
Epoch:28, Train loss:0.217829, valid loss:0.228952
Epoch:29, Train loss:0.217808, valid loss:0.228921
Epoch:30, Train loss:0.217806, valid loss:0.228982
Epoch:31, Train loss:0.217631, valid loss:0.228864
Epoch:32, Train loss:0.217617, valid loss:0.228887
Epoch:33, Train loss:0.217626, valid loss:0.228895
Epoch:34, Train loss:0.217624, valid loss:0.228860
Epoch:35, Train loss:0.217608, valid loss:0.228836
Epoch:36, Train loss:0.217616, valid loss:0.228879
Epoch:37, Train loss:0.217599, valid loss:0.228893
Epoch:38, Train loss:0.217610, valid loss:0.228913
Epoch:39, Train loss:0.217596, valid loss:0.228822
Epoch:40, Train loss:0.217590, valid loss:0.228875
Epoch:41, Train loss:0.217501, valid loss:0.228788
Epoch:42, Train loss:0.217496, valid loss:0.228791
Epoch:43, Train loss:0.217502, valid loss:0.228822
Epoch:44, Train loss:0.217503, valid loss:0.228839
Epoch:45, Train loss:0.217494, valid loss:0.228846
Epoch:46, Train loss:0.217485, valid loss:0.228816
Epoch:47, Train loss:0.217500, valid loss:0.228809
Epoch:48, Train loss:0.217483, valid loss:0.228807
Epoch:49, Train loss:0.217484, valid loss:0.228795
Epoch:50, Train loss:0.217491, valid loss:0.228817
Epoch:51, Train loss:0.217437, valid loss:0.228793
Epoch:52, Train loss:0.217438, valid loss:0.228781
Epoch:53, Train loss:0.217438, valid loss:0.228777
Epoch:54, Train loss:0.217434, valid loss:0.228777
Epoch:55, Train loss:0.217432, valid loss:0.228782
Epoch:56, Train loss:0.217435, valid loss:0.228793
Epoch:57, Train loss:0.217432, valid loss:0.228804
Epoch:58, Train loss:0.217427, valid loss:0.228778
Epoch:59, Train loss:0.217429, valid loss:0.228783
Epoch:60, Train loss:0.217425, valid loss:0.228789
training time 8363.389516830444
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.7315238654034387
plot_id,batch_id 0 1 miss% 0.789833228658767
plot_id,batch_id 0 2 miss% 0.8010042599640412
plot_id,batch_id 0 3 miss% 0.8084027180813267
plot_id,batch_id 0 4 miss% 0.8077865327531346
plot_id,batch_id 0 5 miss% 0.7286796789017529
plot_id,batch_id 0 6 miss% 0.7832734899167993
plot_id,batch_id 0 7 miss% 0.7977548931777741
plot_id,batch_id 0 8 miss% 0.8042701714968518
plot_id,batch_id 0 9 miss% 0.8090467661830645
plot_id,batch_id 0 10 miss% 0.7010945226302213
plot_id,batch_id 0 11 miss% 0.7856593103002979
plot_id,batch_id 0 12 miss% 0.7938111051937331
plot_id,batch_id 0 13 miss% 0.8026724105696952
plot_id,batch_id 0 14 miss% 0.8091822179379351
plot_id,batch_id 0 15 miss% 0.7148561078525624
plot_id,batch_id 0 16 miss% 0.779364952889592
plot_id,batch_id 0 17 miss% 0.7962464953663222
plot_id,batch_id 0 18 miss% 0.8056290829015739
plot_id,batch_id 0 19 miss% 0.8051286678115406
plot_id,batch_id 0 20 miss% 0.7726461097804923
plot_id,batch_id 0 21 miss% 0.8055439595335403
plot_id,batch_id 0 22 miss% 0.8092530699512607
plot_id,batch_id 0 23 miss% 0.8146852713394472
plot_id,batch_id 0 24 miss% 0.8158627016302051
plot_id,batch_id 0 25 miss% 0.7551852200108372
plot_id,batch_id 0 26 miss% 0.7985758553068426
plot_id,batch_id 0 27 miss% 0.8032216404629418
plot_id,batch_id 0 28 miss% 0.8074283779897052
plot_id,batch_id 0 29 miss% 0.8081419282132967
plot_id,batch_id 0 30 miss% 0.7548538454093592
plot_id,batch_id 0 31 miss% 0.7943398292787929
plot_id,batch_id 0 32 miss% 0.8037549572009972
plot_id,batch_id 0 33 miss% 0.8068552276189704
plot_id,batch_id 0 34 miss% 0.8080616912298875
plot_id,batch_id 0 35 miss% 0.7461767365805528
plot_id,batch_id 0 36 miss% 0.7998351459358605
plot_id,batch_id 0 37 miss% 0.8032261286833259
plot_id,batch_id 0 38 miss% 0.8118040340441235
plot_id,batch_id 0 39 miss% 0.8088904259879843
plot_id,batch_id 0 40 miss% 0.7842797057609603
plot_id,batch_id 0 41 miss% 0.8050758163080809
plot_id,batch_id 0 42 miss% 0.8102932990266333
plot_id,batch_id 0 43 miss% 0.8138932090956039
plot_id,batch_id 0 44 miss% 0.8167519358166371
plot_id,batch_id 0 45 miss% 0.7733711729675748
plot_id,batch_id 0 46 miss% 0.8065453538436361
plot_id,batch_id 0 47 miss% 0.8110225535338356
plot_id,batch_id 0 48 miss% 0.8121608811920457
plot_id,batch_id 0 49 miss% 0.8172861758570279
plot_id,batch_id 0 50 miss% 0.7830671152727435
plot_id,batch_id 0 51 miss% 0.8044070549303993
plot_id,batch_id 0 52 miss% 0.8083918180426273
plot_id,batch_id 0 53 miss% 0.8108989588121136
plot_id,batch_id 0 54 miss% 0.8177376621590873
plot_id,batch_id 0 55 miss% 0.7705792447542763
plot_id,batch_id 0 56 miss% 0.8033427470123432
plot_id,batch_id 0 57 miss% 0.8083232833961792
plot_id,batch_id 0 58 miss% 0.8125202702742917
plot_id,batch_id 0 59 miss% 0.8165375331525672
plot_id,batch_id 0 60 miss% 0.6479759782721293
plot_id,batch_id 0 61 miss% 0.7486226097382794
plot_id,batch_id 0 62 miss% 0.7839236122243279
plot_id,batch_id 0 63 miss% 0.7962257990786008
plot_id,batch_id 0 64 miss% 0.8046559524660644
plot_id,batch_id 0 65 miss% 0.6425862154884329
plot_id,batch_id 0 66 miss% 0.7492237811189635
plot_id,batch_id 0 67 miss% 0.772312778508339
plot_id,batch_id 0 68 miss% 0.7961034833881686
plot_id,batch_id 0 69 miss% 0.7959586168201982
plot_id,batch_id 0 700.03612755554051731
plot_id,batch_id 0 70 miss% 0.05038652658225274
plot_id,batch_id 0 71 miss% 0.040789537740568
plot_id,batch_id 0 72 miss% 0.028677072184466385
plot_id,batch_id 0 73 miss% 0.018507893257500314
plot_id,batch_id 0 74 miss% 0.038620973560227645
plot_id,batch_id 0 75 miss% 0.04331184122295798
plot_id,batch_id 0 76 miss% 0.07626107980527087
plot_id,batch_id 0 77 miss% 0.03974498530971539
plot_id,batch_id 0 78 miss% 0.02906357262538051
plot_id,batch_id 0 79 miss% 0.037861696392742304
plot_id,batch_id 0 80 miss% 0.03536964254113131
plot_id,batch_id 0 81 miss% 0.030952741688717724
plot_id,batch_id 0 82 miss% 0.025998762359485977
plot_id,batch_id 0 83 miss% 0.03276656981625099
plot_id,batch_id 0 84 miss% 0.038241734214905454
plot_id,batch_id 0 85 miss% 0.04305486708621848
plot_id,batch_id 0 86 miss% 0.0355519809071779
plot_id,batch_id 0 87 miss% 0.024194721632175616
plot_id,batch_id 0 88 miss% 0.025254450962697295
plot_id,batch_id 0 89 miss% 0.024000347872507058
plot_id,batch_id 0 90 miss% 0.03713315277892981
plot_id,batch_id 0 91 miss% 0.026200326572538377
plot_id,batch_id 0 92 miss% 0.029122728438275555
plot_id,batch_id 0 93 miss% 0.026565896504873846
plot_id,batch_id 0 94 miss% 0.035845739520385465
plot_id,batch_id 0 95 miss% 0.04029371874576791
plot_id,batch_id 0 96 miss% 0.030386897259676623
plot_id,batch_id 0 97 miss% 0.050932519492201414
plot_id,batch_id 0 98 miss% 0.026619384130761835
plot_id,batch_id 0 99 miss% 0.02522411159079097
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03049867 0.02158555 0.02276333 0.03083543 0.02327102 0.04761185
 0.02907805 0.02841335 0.03132056 0.02390689 0.03930135 0.03138492
 0.02242706 0.02661142 0.03128527 0.04308139 0.0341199  0.0332384
 0.03431061 0.04711972 0.06214886 0.02894664 0.0244928  0.0285977
 0.03169744 0.02934636 0.03318306 0.02829803 0.02606936 0.03043305
 0.04542437 0.03097625 0.03198068 0.02280528 0.03477673 0.04599573
 0.03576631 0.01989543 0.0233853  0.02798688 0.0523862  0.01731094
 0.0221007  0.03620793 0.01715524 0.02909083 0.01713426 0.028131
 0.02862966 0.02803156 0.02253983 0.02780674 0.02718204 0.01916078
 0.02738533 0.02605581 0.02495043 0.01909537 0.01703333 0.02693136
 0.05763811 0.02502377 0.02875898 0.03266039 0.03138458 0.04375593
 0.04143482 0.02555191 0.02547078 0.03612756 0.05038653 0.04078954
 0.02867707 0.01850789 0.03862097 0.04331184 0.07626108 0.03974499
 0.02906357 0.0378617  0.03536964 0.03095274 0.02599876 0.03276657
 0.03824173 0.04305487 0.03555198 0.02419472 0.02525445 0.02400035
 0.03713315 0.02620033 0.02912273 0.0265659  0.03584574 0.04029372
 0.0303869  0.05093252 0.02661938 0.02522411]
for model  70 the mean error 0.031834326813502914
all id 70 hidden_dim 32 learning_rate 0.02 num_layers 4 frames 21 out win 4 err 0.031834326813502914
Launcher: Job 71 completed in 8543 seconds.
Launcher: Task 186 done. Exiting.
 miss% 0.6121525622330736
plot_id,batch_id 0 71 miss% 0.7603503090026611
plot_id,batch_id 0 72 miss% 0.7645130933489991
plot_id,batch_id 0 73 miss% 0.7788759778933175
plot_id,batch_id 0 74 miss% 0.793723294948853
plot_id,batch_id 0 75 miss% 0.6102553593810232
plot_id,batch_id 0 76 miss% 0.713569860655114
plot_id,batch_id 0 77 miss% 0.7512558156430018
plot_id,batch_id 0 78 miss% 0.7802781997351368
plot_id,batch_id 0 79 miss% 0.7853357951377649
plot_id,batch_id 0 80 miss% 0.6733738941447702
plot_id,batch_id 0 81 miss% 0.7766446602292535
plot_id,batch_id 0 82 miss% 0.7941062291693044
plot_id,batch_id 0 83 miss% 0.8006062248228348
plot_id,batch_id 0 84 miss% 0.8009382802249535
plot_id,batch_id 0 85 miss% 0.6697471946896028
plot_id,batch_id 0 86 miss% 0.7699610973201186
plot_id,batch_id 0 87 miss% 0.7918033584811976
plot_id,batch_id 0 88 miss% 0.7971107595098408
plot_id,batch_id 0 89 miss% 0.7995916919606534
plot_id,batch_id 0 90 miss% 0.635724515176067
plot_id,batch_id 0 91 miss% 0.7701739480607246
plot_id,batch_id 0 92 miss% 0.7818824967801493
plot_id,batch_id 0 93 miss% 0.7936461797760953
plot_id,batch_id 0 94 miss% 0.8003722620515733
plot_id,batch_id 0 95 miss% 0.6373807543665445
plot_id,batch_id 0 96 miss% 0.7513083859977875
plot_id,batch_id 0 97 miss% 0.7756413836320977
plot_id,batch_id 0 98 miss% 0.7860725825642924
plot_id,batch_id 0 99 miss% 0.7962187984998935
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73152387 0.78983323 0.80100426 0.80840272 0.80778653 0.72867968
 0.78327349 0.79775489 0.80427017 0.80904677 0.70109452 0.78565931
 0.79381111 0.80267241 0.80918222 0.71485611 0.77936495 0.7962465
 0.80562908 0.80512867 0.77264611 0.80554396 0.80925307 0.81468527
 0.8158627  0.75518522 0.79857586 0.80322164 0.80742838 0.80814193
 0.75485385 0.79433983 0.80375496 0.80685523 0.80806169 0.74617674
 0.79983515 0.80322613 0.81180403 0.80889043 0.78427971 0.80507582
 0.8102933  0.81389321 0.81675194 0.77337117 0.80654535 0.81102255
 0.81216088 0.81728618 0.78306712 0.80440705 0.80839182 0.81089896
 0.81773766 0.77057924 0.80334275 0.80832328 0.81252027 0.81653753
 0.64797598 0.74862261 0.78392361 0.7962258  0.80465595 0.64258622
 0.74922378 0.77231278 0.79610348 0.79595862 0.61215256 0.76035031
 0.76451309 0.77887598 0.79372329 0.61025536 0.71356986 0.75125582
 0.7802782  0.7853358  0.67337389 0.77664466 0.79410623 0.80060622
 0.80093828 0.66974719 0.7699611  0.79180336 0.79711076 0.79959169
 0.63572452 0.77017395 0.7818825  0.79364618 0.80037226 0.63738075
 0.75130839 0.77564138 0.78607258 0.7962188 ]
for model  194 the mean error 0.7776025221792771
all id 194 hidden_dim 24 learning_rate 0.01 num_layers 3 frames 31 out win 5 err 0.7776025221792771
Launcher: Job 195 completed in 8539 seconds.
Launcher: Task 244 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  35921
Epoch:0, Train loss:0.489925, valid loss:0.473190
Epoch:1, Train loss:0.023217, valid loss:0.004948
Epoch:2, Train loss:0.006818, valid loss:0.003764
Epoch:3, Train loss:0.004886, valid loss:0.002636
Epoch:4, Train loss:0.004134, valid loss:0.002237
Epoch:5, Train loss:0.003635, valid loss:0.002048
Epoch:6, Train loss:0.003436, valid loss:0.002049
Epoch:7, Train loss:0.003309, valid loss:0.002753
Epoch:8, Train loss:0.003175, valid loss:0.001410
Epoch:9, Train loss:0.002978, valid loss:0.001819
Epoch:10, Train loss:0.002985, valid loss:0.001978
Epoch:11, Train loss:0.002023, valid loss:0.001614
Epoch:12, Train loss:0.002027, valid loss:0.001186
Epoch:13, Train loss:0.001949, valid loss:0.000921
Epoch:14, Train loss:0.001925, valid loss:0.001220
Epoch:15, Train loss:0.001915, valid loss:0.001126
Epoch:16, Train loss:0.001918, valid loss:0.001331
Epoch:17, Train loss:0.001865, valid loss:0.001074
Epoch:18, Train loss:0.001842, valid loss:0.000904
Epoch:19, Train loss:0.001786, valid loss:0.001261
Epoch:20, Train loss:0.001804, valid loss:0.001397
Epoch:21, Train loss:0.001360, valid loss:0.000879
Epoch:22, Train loss:0.001344, valid loss:0.000831
Epoch:23, Train loss:0.001327, valid loss:0.000767
Epoch:24, Train loss:0.001305, valid loss:0.001060
Epoch:25, Train loss:0.001321, valid loss:0.000871
Epoch:26, Train loss:0.001319, valid loss:0.001132
Epoch:27, Train loss:0.001354, valid loss:0.000842
Epoch:28, Train loss:0.001274, valid loss:0.000848
Epoch:29, Train loss:0.001274, valid loss:0.000654
Epoch:30, Train loss:0.001276, valid loss:0.000894
Epoch:31, Train loss:0.001042, valid loss:0.000804
Epoch:32, Train loss:0.001058, valid loss:0.000682
Epoch:33, Train loss:0.001048, valid loss:0.000838
Epoch:34, Train loss:0.001029, valid loss:0.000661
Epoch:35, Train loss:0.001023, valid loss:0.000634
Epoch:36, Train loss:0.001021, valid loss:0.000689
Epoch:37, Train loss:0.001016, valid loss:0.000645
Epoch:38, Train loss:0.001033, valid loss:0.000845
Epoch:39, Train loss:0.001021, valid loss:0.000616
Epoch:40, Train loss:0.001029, valid loss:0.000700
Epoch:41, Train loss:0.000880, valid loss:0.000567
Epoch:42, Train loss:0.000878, valid loss:0.000593
Epoch:43, Train loss:0.000884, valid loss:0.000553
Epoch:44, Train loss:0.000889, valid loss:0.000573
Epoch:45, Train loss:0.000886, valid loss:0.000607
Epoch:46, Train loss:0.000873, valid loss:0.000561
Epoch:47, Train loss:0.000879, valid loss:0.000562
Epoch:48, Train loss:0.000878, valid loss:0.000590
Epoch:49, Train loss:0.000868, valid loss:0.000584
Epoch:50, Train loss:0.000875, valid loss:0.000577
Epoch:51, Train loss:0.000800, valid loss:0.000569
Epoch:52, Train loss:0.000797, valid loss:0.000557
Epoch:53, Train loss:0.000802, valid loss:0.000568
Epoch:54, Train loss:0.000803, valid loss:0.000558
Epoch:55, Train loss:0.000806, valid loss:0.000579
Epoch:56, Train loss:0.000798, valid loss:0.000583
Epoch:57, Train loss:0.000802, valid loss:0.000575
Epoch:58, Train loss:0.000800, valid loss:0.000558
Epoch:59, Train loss:0.000797, valid loss:0.000567
Epoch:60, Train loss:0.000793, valid loss:0.000618
training time 8359.912845373154
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.033062964261444755
plot_id,batch_id 0 1 miss% 0.019184492213935797
plot_id,batch_id 0 2 miss% 0.024706516986215944
plot_id,batch_id 0 3 miss% 0.03244217413000472
plot_id,batch_id 0 4 miss% 0.028953868120452926
plot_id,batch_id 0 5 miss% 0.0633729889000103
plot_id,batch_id 0 6 miss% 0.03540917940792473
plot_id,batch_id 0 7 miss% 0.024642368444563086
plot_id,batch_id 0 8 miss% 0.031084590266915874
plot_id,batch_id 0 9 miss% 0.028290193583217688
plot_id,batch_id 0 10 miss% 0.029068281169896546
plot_id,batch_id 0 11 miss% 0.04379619440585209
plot_id,batch_id 0 12 miss% 0.03552964438506255
plot_id,batch_id 0 13 miss% 0.028760560821089638
plot_id,batch_id 0 14 miss% 0.03866653543447325
plot_id,batch_id 0 15 miss% 0.06582147544700505
plot_id,batch_id 0 16 miss% 0.03321548820854353
plot_id,batch_id 0 17 miss% 0.046882461260327385
plot_id,batch_id 0 18 miss% 0.02519825873966881
plot_id,batch_id 0 19 miss% 0.04146984938939412
plot_id,batch_id 0 20 miss% 0.05385873006704602
plot_id,batch_id 0 21 miss% 0.029657570716638455
plot_id,batch_id 0 22 miss% 0.026787057220717447
plot_id,batch_id 0 23 miss% 0.021579960148142004
plot_id,batch_id 0 24 miss% 0.022022871170240953
plot_id,batch_id 0 25 miss% 0.027755682589431747
plot_id,batch_id 0 26 miss% 0.023541850637841512
plot_id,batch_id 0 27 miss% 0.030918027531901784
plot_id,batch_id 0 28 miss% 0.02850830690468689
plot_id,batch_id 0 29 miss% 0.03461953274867292
plot_id,batch_id 0 30 miss% 0.044943636402117654
plot_id,batch_id 0 31 miss% 0.0282137045647477
plot_id,batch_id 0 32 miss% 0.029879347997584942
plot_id,batch_id 0 33 miss% 0.030673185252040387
plot_id,batch_id 0 34 miss% 0.02782298258160613
plot_id,batch_id 0 35 miss% 0.05692603815665407
plot_id,batch_id 0 36 miss% 0.042524577436671866
plot_id,batch_id 0 37 miss% 0.04682322464669523
plot_id,batch_id 0 38 miss% 0.0384283523019604
plot_id,batch_id 0 39 miss% 0.017005757544276096
plot_id,batch_id 0 40 miss% 0.057360796122532434
plot_id,batch_id 0 41 miss% 0.022169399079666068
plot_id,batch_id 0 42 miss% 0.01661982688394265
plot_id,batch_id 0 43 miss% 0.13983531302187505
plot_id,batch_id 0 44 miss% 0.029533945887635676
plot_id,batch_id 0 45 miss% 0.03696853179611605
plot_id,batch_id 0 46 miss% 0.03410118815236997
plot_id,batch_id 0 47 miss% 0.02541757379291344
plot_id,batch_id 0 48 miss% 0.025515677276149187
plot_id,batch_id 0 49 miss% 0.022779385119695676
plot_id,batch_id 0 50 miss% 0.027889493024074497
plot_id,batch_id 0 51 miss% 0.03046727500489643
plot_id,batch_id 0 52 miss% 0.020429037787210284
plot_id,batch_id 0 53 miss% 0.01243576571052598
plot_id,batch_id 0 54 miss% 0.023788599919965132
plot_id,batch_id 0 55 miss% 0.04490526837187052
plot_id,batch_id 0 56 miss% 0.02406543928686123
plot_id,batch_id 0 57 miss% 0.01935275456464041
plot_id,batch_id 0 58 miss% 0.017736504571265986
plot_id,batch_id 0 59 miss% 0.024048818873625224
plot_id,batch_id 0 60 miss% 0.06316449171055384
plot_id,batch_id 0 61 miss% 0.050305776655165085
plot_id,batch_id 0 62 miss% 0.034846664144591824
plot_id,batch_id 0 63 miss% 0.028026835175094533
plot_id,batch_id 0 64 miss% 0.033078981077231756
plot_id,batch_id 0 65 miss% 0.059810842460669436
plot_id,batch_id 0 66 miss% 0.04541755599151505
plot_id,batch_id 0 67 miss% 0.03266746153600202
plot_id,batch_id 0 68 miss% 0.036859034051064825
plot_id,batch_id 0 69 miss% 0.021307093057218555
plot_id,batch_id 0 70 miss% 0.043307276914944445
plot_id,batch_id 0 71 miss% 0.06477786475191506
plot_id,batch_id 0 72 miss% 0.02776116468914635
plot_id,batch_id 0 73 miss% 0.042738581282699306
plot_id,batch_id 0 74 miss% 0.04135874481539198
plot_id,batch_id 0 75 miss% 0.04796095438379619
plot_id,batch_id 0 76 miss% 0.034780930439240666
plot_id,batch_id 0 77 miss% 0.0317202876023337
plot_id,batch_id 0 78 miss% 0.035773322459739604
plot_id,batch_id 0 79 miss% 0.03892996460528617
plot_id,batch_id 0 80 miss% 0.030695302500092052
plot_id,batch_id 0 81 miss% 0.0253104472061756
plot_id,batch_id 0 82 miss% 0.03047895271323253
plot_id,batch_id 0 83 miss% 0.03362972679118445
plot_id,batch_id 0 84 miss% 0.019432845141437192
plot_id,batch_id 0 85 miss% 0.043721339833718786
plot_id,batch_id 0 86 miss% 0.031155173988465044
plot_id,batch_id 0 87 miss% 0.03133811759688593
plot_id,batch_id 0 88 miss% 0.032909743180653074
plot_id,batch_id 0 89 miss% 0.025722330063447433
plot_id,batch_id 0 90 miss% 0.04403066458595029
plot_id,batch_id 0 91 miss% 0.03784741160452111
plot_id,batch_id 0 92 miss% 0.0335213744305825
plot_id,batch_id 0 93 miss% 0.031758702205794354
plot_id,batch_id 0 94 miss% 0.028544325082885895
plot_id,batch_id 0 95 miss% 0.039794763856809474
plot_id,batch_id 0 96 miss% 0.045401698502498755
plot_id,batch_id 0 97 miss% 0.046038624699118
plot_id,batch_id 0 98 miss% 0.026677579485179945
plot_id,batch_id 0 99 miss% 0.03180998484630513
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03306296 0.01918449 0.02470652 0.03244217 0.02895387 0.06337299
 0.03540918 0.02464237 0.03108459 0.02829019 0.02906828 0.04379619
 0.03552964 0.02876056 0.03866654 0.06582148 0.03321549 0.04688246
 0.02519826 0.04146985 0.05385873 0.02965757 0.02678706 0.02157996
 0.02202287 0.02775568 0.02354185 0.03091803 0.02850831 0.03461953
 0.04494364 0.0282137  0.02987935 0.03067319 0.02782298 0.05692604
 0.04252458 0.04682322 0.03842835 0.01700576 0.0573608  0.0221694
 0.01661983 0.13983531 0.02953395 0.03696853 0.03410119 0.02541757
 0.02551568 0.02277939 0.02788949 0.03046728 0.02042904 0.01243577
 0.0237886  0.04490527 0.02406544 0.01935275 0.0177365  0.02404882
 0.06316449 0.05030578 0.03484666 0.02802684 0.03307898 0.05981084
 0.04541756 0.03266746 0.03685903 0.02130709 0.04330728 0.06477786
 0.02776116 0.04273858 0.04135874 0.04796095 0.03478093 0.03172029
 0.03577332 0.03892996 0.0306953  0.02531045 0.03047895 0.03362973
 0.01943285 0.04372134 0.03115517 0.03133812 0.03290974 0.02572233
 0.04403066 0.03784741 0.03352137 0.0317587  0.02854433 0.03979476
 0.0454017  0.04603862 0.02667758 0.03180998]
for model  235 the mean error 0.035118820125620426
all id 235 hidden_dim 16 learning_rate 0.02 num_layers 5 frames 31 out win 4 err 0.035118820125620426
Launcher: Job 236 completed in 8552 seconds.
Launcher: Task 143 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  77489
Epoch:0, Train loss:0.376916, valid loss:0.379508
Epoch:1, Train loss:0.021297, valid loss:0.005325
Epoch:2, Train loss:0.007112, valid loss:0.002715
Epoch:3, Train loss:0.004450, valid loss:0.002428
Epoch:4, Train loss:0.003739, valid loss:0.001857
Epoch:5, Train loss:0.003311, valid loss:0.001778
Epoch:6, Train loss:0.002994, valid loss:0.001640
Epoch:7, Train loss:0.002768, valid loss:0.001496
Epoch:8, Train loss:0.002807, valid loss:0.001404
Epoch:9, Train loss:0.002580, valid loss:0.001918
Epoch:10, Train loss:0.002485, valid loss:0.001466
Epoch:11, Train loss:0.001674, valid loss:0.001236
Epoch:12, Train loss:0.001701, valid loss:0.000996
Epoch:13, Train loss:0.001721, valid loss:0.001117
Epoch:14, Train loss:0.001635, valid loss:0.000898
Epoch:15, Train loss:0.001585, valid loss:0.001102
Epoch:16, Train loss:0.001604, valid loss:0.000875
Epoch:17, Train loss:0.001569, valid loss:0.001134
Epoch:18, Train loss:0.001509, valid loss:0.000983
Epoch:19, Train loss:0.001485, valid loss:0.001179
Epoch:20, Train loss:0.001484, valid loss:0.000918
Epoch:21, Train loss:0.001105, valid loss:0.000706
Epoch:22, Train loss:0.001120, valid loss:0.000732
Epoch:23, Train loss:0.001150, valid loss:0.000755
Epoch:24, Train loss:0.001099, valid loss:0.000799
Epoch:25, Train loss:0.001050, valid loss:0.000703
Epoch:26, Train loss:0.001077, valid loss:0.000806
Epoch:27, Train loss:0.001057, valid loss:0.000803
Epoch:28, Train loss:0.001077, valid loss:0.000749
Epoch:29, Train loss:0.001033, valid loss:0.000653
Epoch:30, Train loss:0.001039, valid loss:0.000786
Epoch:31, Train loss:0.000842, valid loss:0.000562
Epoch:32, Train loss:0.000849, valid loss:0.000578
Epoch:33, Train loss:0.000821, valid loss:0.000615
Epoch:34, Train loss:0.000850, valid loss:0.000584
Epoch:35, Train loss:0.000825, valid loss:0.000617
Epoch:36, Train loss:0.000827, valid loss:0.000562
Epoch:37, Train loss:0.000831, valid loss:0.000609
Epoch:38, Train loss:0.000823, valid loss:0.000613
Epoch:39, Train loss:0.000810, valid loss:0.000702
Epoch:40, Train loss:0.000797, valid loss:0.000549
Epoch:41, Train loss:0.000708, valid loss:0.000526
Epoch:42, Train loss:0.000731, valid loss:0.000548
Epoch:43, Train loss:0.000698, valid loss:0.000555
Epoch:44, Train loss:0.000710, valid loss:0.000584
Epoch:45, Train loss:0.000706, valid loss:0.000565
Epoch:46, Train loss:0.000693, valid loss:0.000518
Epoch:47, Train loss:0.000685, valid loss:0.000564
Epoch:48, Train loss:0.000684, valid loss:0.000530
Epoch:49, Train loss:0.000681, valid loss:0.000516
Epoch:50, Train loss:0.000681, valid loss:0.000518
Epoch:51, Train loss:0.000636, valid loss:0.000505
Epoch:52, Train loss:0.000637, valid loss:0.000490
Epoch:53, Train loss:0.000629, valid loss:0.000494
Epoch:54, Train loss:0.000629, valid loss:0.000515
Epoch:55, Train loss:0.000631, valid loss:0.000476
Epoch:56, Train loss:0.000626, valid loss:0.000525
Epoch:57, Train loss:0.000628, valid loss:0.000487
Epoch:58, Train loss:0.000623, valid loss:0.000489
Epoch:59, Train loss:0.000624, valid loss:0.000490
Epoch:60, Train loss:0.000625, valid loss:0.000482
training time 8459.718605041504
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.028605430954385575
plot_id,batch_id 0 1 miss% 0.028341263774012748
plot_id,batch_id 0 2 miss% 0.022810303640811103
plot_id,batch_id 0 3 miss% 0.02236149248379814
plot_id,batch_id 0 4 miss% 0.028575248728822635
plot_id,batch_id 0 5 miss% 0.026842899486454217
plot_id,batch_id 0 6 miss% 0.02391111430643152
plot_id,batch_id 0 7 miss% 0.029376581019344927
plot_id,batch_id 0 8 miss% 0.019185458335571635
plot_id,batch_id 0 9 miss% 0.027061646488622176
plot_id,batch_id 0 10 miss% 0.03016310744882042
plot_id,batch_id 0 11 miss% 0.04196690629881426
plot_id,batch_id 0 12 miss% 0.029738399233638375
plot_id,batch_id 0 13 miss% 0.020485597440868426
plot_id,batch_id 0 14 miss% 0.02227755739982136
plot_id,batch_id 0 15 miss% 0.035859151859888215
plot_id,batch_id 0 16 miss% 0.03354900205205502
plot_id,batch_id 0 17 miss% 0.034164238287890274
plot_id,batch_id 0 18 miss% 0.03172660243102892
plot_id,batch_id 0 19 miss% 0.027633995203124654
plot_id,batch_id 0 20 miss% 0.030394500129848775
plot_id,batch_id 0 21 miss% 0.028724224724147943
plot_id,batch_id 0 22 miss% 0.026413466776936866
plot_id,batch_id 0 23 miss% 0.01872789229919095
plot_id,batch_id 0 24 miss% 0.02743659766507293
plot_id,batch_id 0 25 miss% 0.025557129736528385
plot_id,batch_id 0 26 miss% 0.03685655601013786
plot_id,batch_id 0 27 miss% 0.021073709205677728
plot_id,batch_id 0 28 miss% 0.02293959836504058
plot_id,batch_id 0 29 miss% 0.026585460483238007
plot_id,batch_id 0 30 miss% 0.05362659046279829
plot_id,batch_id 0 31 miss% 0.022248451313249023
plot_id,batch_id 0 32 miss% 0.030221573258355113
plot_id,batch_id 0 33 miss% 0.022408373717170492
plot_id,batch_id 0 34 miss% 0.026605164680640406
plot_id,batch_id 0 35 miss% 0.03957338534989849
plot_id,batch_id 0 36 miss% 0.04201272693602508
plot_id,batch_id 0 37 miss% 0.020631598009638347
plot_id,batch_id 0 38 miss% 0.02871189303958145
plot_id,batch_id 0 39 miss% 0.019476430691180144
plot_id,batch_id 0 40 miss% 0.06598447742436155
plot_id,batch_id 0 41 miss% 0.027974572995702815
plot_id,batch_id 0 42 miss% 0.019317801909993864
plot_id,batch_id 0 43 miss% 0.030277826555441867
plot_id,batch_id 0 44 miss% 0.02846387711901767
plot_id,batch_id 0 45 miss% 0.03773505565536498
plot_id,batch_id 0 46 miss% 0.03688830784229071
plot_id,batch_id 0 47 miss% 0.027155460974379163
plot_id,batch_id 0 48 miss% 0.017296703137785724
plot_id,batch_id 0 49 miss% 0.02401884770674671
plot_id,batch_id 0 50 miss% 0.03724111842734992
plot_id,batch_id 0 51 miss% 0.027083497813966836
plot_id,batch_id 0 52 miss% 0.025611747087591473
plot_id,batch_id 0 53 miss% 0.017355728135749704
plot_id,batch_id 0 54 miss% 0.03291069945498649
plot_id,batch_id 0 55 miss% 0.04834935308843089
plot_id,batch_id 0 56 miss% 0.023045191266570685
plot_id,batch_id 0 57 miss% 0.021996821665506382
plot_id,batch_id 0 58 miss% 0.02113350655903101
plot_id,batch_id 0 59 miss% 0.020953365978162813
plot_id,batch_id 0 60 miss% 0.0387632548626671
plot_id,batch_id 0 61 miss% 0.022741667358501978
plot_id,batch_id 0 62 miss% 0.027706799948326963
plot_id,batch_id 0 63 miss% 0.01973287228554635
plot_id,batch_id 0 64 miss% 0.03483233396794348
plot_id,batch_id 0 65 miss% 0.040711771403386436
plot_id,batch_id 0 66 miss% 0.03788288955165487
plot_id,batch_id 0 67 miss% 0.02503130275151929
plot_id,batch_id 0 68 miss% 0.02448980742674297
plot_id,batch_id 0 69 miss% 0.023481959812203947
plot_id,batch_id 0 70 miss% 0.028899646754080414
plot_id,batch_id 0 71 miss% 0.026123863072355247
plot_id,batch_id 0 72 miss% 0.022346722188364334
plot_id,batch_id 0 73 miss% 0.03519979412535289
plot_id,batch_id 0 74 miss% 0.04157024377769547
plot_id,batch_id 0 75 miss% 0.03436884237898847
plot_id,batch_id 0 76 miss% 0.03826719803789286
plot_id,batch_id 0 77 miss% 0.028784151989393766
plot_id,batch_id 0 78 miss% 0.02836538275399971
plot_id,batch_id 0 79 miss% 0.031776966120280914
plot_id,batch_id 0 80 miss% 0.040861194722231896
plot_id,batch_id 0 81 miss% 0.01812318384119325
plot_id,batch_id 0 82 miss% 0.020701159119632932
plot_id,batch_id 0 83 miss% 0.02394212201699535
plot_id,batch_id 0 84 miss% 0.024462235921303
plot_id,batch_id 0 85 miss% 0.05769324839794342
plot_id,batch_id 0 86 miss% 0.02718039994110281
plot_id,batch_id 0 87 miss% 0.03298739445000116
plot_id,batch_id 0 88 miss% 0.02388060660182289
plot_id,batch_id 0 89 miss% 0.018551061297827767
plot_id,batch_id 0 90 miss% 0.02743278834470547
plot_id,batch_id 0 91 miss% 0.0348386664815944
plot_id,batch_id 0 92 miss% 0.032155379829248584
plot_id,batch_id 0 93 miss% 0.031688803654522
plot_id,batch_id 0 94 miss% 0.03816928688995246
plot_id,batch_id 0 95 miss% 0.036970723196214035
plot_id,batch_id 0 96 miss% 0.02278676712215874
plot_id,batch_id 0 97 miss% 0.04167882752692313
plot_id,batch_id 0 98 miss% 0.03202800128473655
plot_id,batch_id 0 99 miss% 0.02736796057751476
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02860543 0.02834126 0.0228103  0.02236149 0.02857525 0.0268429
 0.02391111 0.02937658 0.01918546 0.02706165 0.03016311 0.04196691
 0.0297384  0.0204856  0.02227756 0.03585915 0.033549   0.03416424
 0.0317266  0.027634   0.0303945  0.02872422 0.02641347 0.01872789
 0.0274366  0.02555713 0.03685656 0.02107371 0.0229396  0.02658546
 0.05362659 0.02224845 0.03022157 0.02240837 0.02660516 0.03957339
 0.04201273 0.0206316  0.02871189 0.01947643 0.06598448 0.02797457
 0.0193178  0.03027783 0.02846388 0.03773506 0.03688831 0.02715546
 0.0172967  0.02401885 0.03724112 0.0270835  0.02561175 0.01735573
 0.0329107  0.04834935 0.02304519 0.02199682 0.02113351 0.02095337
 0.03876325 0.02274167 0.0277068  0.01973287 0.03483233 0.04071177
 0.03788289 0.0250313  0.02448981 0.02348196 0.02889965 0.02612386
 0.02234672 0.03519979 0.04157024 0.03436884 0.0382672  0.02878415
 0.02836538 0.03177697 0.04086119 0.01812318 0.02070116 0.02394212
 0.02446224 0.05769325 0.0271804  0.03298739 0.02388061 0.01855106
 0.02743279 0.03483867 0.03215538 0.0316888  0.03816929 0.03697072
 0.02278677 0.04167883 0.032028   0.02736796]
for model  237 the mean error 0.02950232562311515
all id 237 hidden_dim 24 learning_rate 0.02 num_layers 5 frames 31 out win 3 err 0.02950232562311515
Launcher: Job 238 completed in 8645 seconds.
Launcher: Task 138 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  77489
Epoch:0, Train loss:0.376916, valid loss:0.379508
Epoch:1, Train loss:0.028418, valid loss:0.008142
Epoch:2, Train loss:0.007104, valid loss:0.002557
Epoch:3, Train loss:0.004224, valid loss:0.002314
Epoch:4, Train loss:0.003367, valid loss:0.001875
Epoch:5, Train loss:0.002896, valid loss:0.001738
Epoch:6, Train loss:0.002528, valid loss:0.001519
Epoch:7, Train loss:0.002310, valid loss:0.001199
Epoch:8, Train loss:0.002128, valid loss:0.001082
Epoch:9, Train loss:0.002049, valid loss:0.001617
Epoch:10, Train loss:0.001923, valid loss:0.001008
Epoch:11, Train loss:0.001433, valid loss:0.000883
Epoch:12, Train loss:0.001441, valid loss:0.000837
Epoch:13, Train loss:0.001379, valid loss:0.000787
Epoch:14, Train loss:0.001365, valid loss:0.000741
Epoch:15, Train loss:0.001345, valid loss:0.000813
Epoch:16, Train loss:0.001300, valid loss:0.000746
Epoch:17, Train loss:0.001286, valid loss:0.000786
Epoch:18, Train loss:0.001247, valid loss:0.000762
Epoch:19, Train loss:0.001221, valid loss:0.000912
Epoch:20, Train loss:0.001211, valid loss:0.000734
Epoch:21, Train loss:0.000955, valid loss:0.000594
Epoch:22, Train loss:0.000959, valid loss:0.000563
Epoch:23, Train loss:0.000959, valid loss:0.000583
Epoch:24, Train loss:0.000930, valid loss:0.000590
Epoch:25, Train loss:0.000916, valid loss:0.000557
Epoch:26, Train loss:0.000926, valid loss:0.000570
Epoch:27, Train loss:0.000906, valid loss:0.000590
Epoch:28, Train loss:0.000885, valid loss:0.000578
Epoch:29, Train loss:0.000888, valid loss:0.000580
Epoch:30, Train loss:0.000903, valid loss:0.000634
Epoch:31, Train loss:0.000762, valid loss:0.000540
Epoch:32, Train loss:0.000755, valid loss:0.000532
Epoch:33, Train loss:0.000752, valid loss:0.000504
Epoch:34, Train loss:0.000760, valid loss:0.000495
Epoch:35, Train loss:0.000742, valid loss:0.000505
Epoch:36, Train loss:0.000740, valid loss:0.000496
Epoch:37, Train loss:0.000733, valid loss:0.000597
Epoch:38, Train loss:0.000726, valid loss:0.000507
Epoch:39, Train loss:0.000721, valid loss:0.000522
Epoch:40, Train loss:0.000724, valid loss:0.000490
Epoch:41, Train loss:0.000663, valid loss:0.000491
Epoch:42, Train loss:0.000660, valid loss:0.000497
Epoch:43, Train loss:0.000658, valid loss:0.000491
Epoch:44, Train loss:0.000657, valid loss:0.000497
Epoch:45, Train loss:0.000660, valid loss:0.000479
Epoch:46, Train loss:0.000656, valid loss:0.000480
Epoch:47, Train loss:0.000645, valid loss:0.000490
Epoch:48, Train loss:0.000652, valid loss:0.000477
Epoch:49, Train loss:0.000650, valid loss:0.000498
Epoch:50, Train loss:0.000643, valid loss:0.000486
Epoch:51, Train loss:0.000618, valid loss:0.000483
Epoch:52, Train loss:0.000616, valid loss:0.000474
Epoch:53, Train loss:0.000615, valid loss:0.000473
Epoch:54, Train loss:0.000612, valid loss:0.000484
Epoch:55, Train loss:0.000611, valid loss:0.000470
Epoch:56, Train loss:0.000611, valid loss:0.000474
Epoch:57, Train loss:0.000611, valid loss:0.000469
Epoch:58, Train loss:0.000607, valid loss:0.000477
Epoch:59, Train loss:0.000608, valid loss:0.000472
Epoch:60, Train loss:0.000607, valid loss:0.000477
training time 8475.763482570648
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.04530026896523408
plot_id,batch_id 0 1 miss% 0.03433382837237432
plot_id,batch_id 0 2 miss% 0.026120988711097436
plot_id,batch_id 0 3 miss% 0.024664780656649243
plot_id,batch_id 0 4 miss% 0.01797731598391107
plot_id,batch_id 0 5 miss% 0.02910793978239099
plot_id,batch_id 0 6 miss% 0.0342286192171442
plot_id,batch_id 0 7 miss% 0.03768397896041388
plot_id,batch_id 0 8 miss% 0.031430090035459275
plot_id,batch_id 0 9 miss% 0.024479244273574025
plot_id,batch_id 0 10 miss% 0.059709377956388275
plot_id,batch_id 0 11 miss% 0.046279327754029057
plot_id,batch_id 0 12 miss% 0.030060225496946688
plot_id,batch_id 0 13 miss% 0.026515101526581195
plot_id,batch_id 0 14 miss% 0.02715863978307981
plot_id,batch_id 0 15 miss% 0.042058140201023886
plot_id,batch_id 0 16 miss% 0.03905706120335802
plot_id,batch_id 0 17 miss% 0.039527061422597605
plot_id,batch_id 0 18 miss% 0.035527226877497466
plot_id,batch_id 0 19 miss% 0.028509634426285728
plot_id,batch_id 0 20 miss% 0.05367097441442619
plot_id,batch_id 0 21 miss% 0.02100577290773057
plot_id,batch_id 0 22 miss% 0.018997367452088722
plot_id,batch_id 0 23 miss% 0.024473383393606555
plot_id,batch_id 0 24 miss% 0.020930293141036633
plot_id,batch_id 0 25 miss% 0.02891031964746241
plot_id,batch_id 0 26 miss% 0.030846962428541392
plot_id,batch_id 0 27 miss% 0.024333213089554863
plot_id,batch_id 0 28 miss% 0.023063809161655326
plot_id,batch_id 0 29 miss% 0.025347280950951925
plot_id,batch_id 0 30 miss% 0.06310104577055367
plot_id,batch_id 0 31 miss% 0.03981854126675817
plot_id,batch_id 0 32 miss% 0.03298119208226372
plot_id,batch_id 0 33 miss% 0.02511478407937418
plot_id,batch_id 0 34 miss% 0.0250201988876563
plot_id,batch_id 0 35 miss% 0.030830550065086046
plot_id,batch_id 0 36 miss% 0.03087969992279518
plot_id,batch_id 0 37 miss% 0.02808694573630638
plot_id,batch_id 0 38 miss% 0.02476071157615008
plot_id,batch_id 0 39 miss% 0.027746452118941875
plot_id,batch_id 0 40 miss% 0.06131561905262308
plot_id,batch_id 0 41 miss% 0.02728508713390593
plot_id,batch_id 0 42 miss% 0.029655182097864687
plot_id,batch_id 0 43 miss% 0.03348455790253566
plot_id,batch_id 0 44 miss% 0.016730630412916468
plot_id,batch_id 0 45 miss% 0.027440057618344407
plot_id,batch_id 0 46 miss% 0.030858740823431756
plot_id,batch_id 0 47 miss% 0.01806274156200415
plot_id,batch_id 0 48 miss% 0.024310470061634795
plot_id,batch_id 0 49 miss% 0.022218266089057005
plot_id,batch_id 0 50 miss% 0.03631016463768415
plot_id,batch_id 0 51 miss% 0.02886161710736874
plot_id,batch_id 0 52 miss% 0.017308307195351195
plot_id,batch_id 0 53 miss% 0.020015367556378443
plot_id,batch_id 0 54 miss% 0.03709394532943824
plot_id,batch_id 0 55 miss% 0.027804408993887506
plot_id,batch_id 0 56 miss% 0.021087914484537746
plot_id,batch_id 0 57 miss% 0.025015096344336782
plot_id,batch_id 0 58 miss% 0.017604064542276705
plot_id,batch_id 0 59 miss% 0.029330929138585714
plot_id,batch_id 0 60 miss% 0.05798512915344959
plot_id,batch_id 0 61 miss% 0.0305451912435623
plot_id,batch_id 0 62 miss% 0.02586770577986232
plot_id,batch_id 0 63 miss% 0.035945460446888206
plot_id,batch_id 0 64 miss% 0.03625694905930209
plot_id,batch_id 0 65 miss% 0.04217033961644124
plot_id,batch_id 0 66 miss% 0.03690093111484191
plot_id,batch_id 0 67 miss% 0.026029048349575118
plot_id,batch_id 0 68 miss% 0.033208809556219826
plot_id,batch_id 0 69 miss% 0.01885814133175358
plot_id,batch_id 0 70 miss% 0.045860827420836164
plot_id,batch_id 0 71 miss% 0.03693362908445813
plot_id,batch_id 0 72 miss% 0.023787074829753226
plot_id,batch_id 0 73 miss% 0.02477066484858485
plot_id,batch_id 0 74 miss% 0.05393689464537865
plot_id,batch_id 0 75 miss% 0.04219685495658024
plot_id,batch_id 0 76 miss% 0.03953420864164692
plot_id,batch_id 0 77 miss% 0.02891448824040372
plot_id,batch_id 0 78 miss% 0.03892098059647262
plot_id,batch_id 0 79 miss% 0.0478162566568028
plot_id,batch_id 0 80 miss% 0.056522018276201694
plot_id,batch_id 0 81 miss% 0.022066039041667096
plot_id,batch_id 0 82 miss% 0.03849317621336846
plot_id,batch_id 0 83 miss% 0.03194388759622006
plot_id,batch_id 0 84 miss% 0.030791874028033198
plot_id,batch_id 0 85 miss% 0.05113006814145385
plot_id,batch_id 0 86 miss% 0.029275015998742174
plot_id,batch_id 0 87 miss% 0.03709469978346132
plot_id,batch_id 0 88 miss% 0.027216223635720127
plot_id,batch_id 0 89 miss% 0.02961507174893852
plot_id,batch_id 0 90 miss% 0.03083707091015926
plot_id,batch_id 0 91 miss% 0.043158192892526294
plot_id,batch_id 0 92 miss% 0.030108906295493024
plot_id,batch_id 0 93 miss% 0.03938389181450145
plot_id,batch_id 0 94 miss% 0.0286267161201854
plot_id,batch_id 0 95 miss% 0.043937292269117
plot_id,batch_id 0 96 miss% 0.03226961981440557
plot_id,batch_id 0 97 miss% 0.03914486066440696
plot_id,batch_id 0 98 miss% 0.029981536457839755
plot_id,batch_id 0 99 miss% 0.03164232378426549
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04530027 0.03433383 0.02612099 0.02466478 0.01797732 0.02910794
 0.03422862 0.03768398 0.03143009 0.02447924 0.05970938 0.04627933
 0.03006023 0.0265151  0.02715864 0.04205814 0.03905706 0.03952706
 0.03552723 0.02850963 0.05367097 0.02100577 0.01899737 0.02447338
 0.02093029 0.02891032 0.03084696 0.02433321 0.02306381 0.02534728
 0.06310105 0.03981854 0.03298119 0.02511478 0.0250202  0.03083055
 0.0308797  0.02808695 0.02476071 0.02774645 0.06131562 0.02728509
 0.02965518 0.03348456 0.01673063 0.02744006 0.03085874 0.01806274
 0.02431047 0.02221827 0.03631016 0.02886162 0.01730831 0.02001537
 0.03709395 0.02780441 0.02108791 0.0250151  0.01760406 0.02933093
 0.05798513 0.03054519 0.02586771 0.03594546 0.03625695 0.04217034
 0.03690093 0.02602905 0.03320881 0.01885814 0.04586083 0.03693363
 0.02378707 0.02477066 0.05393689 0.04219685 0.03953421 0.02891449
 0.03892098 0.04781626 0.05652202 0.02206604 0.03849318 0.03194389
 0.03079187 0.05113007 0.02927502 0.0370947  0.02721622 0.02961507
 0.03083707 0.04315819 0.03010891 0.03938389 0.02862672 0.04393729
 0.03226962 0.03914486 0.02998154 0.03164232]
for model  183 the mean error 0.03261119588842659
all id 183 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 31 out win 3 err 0.03261119588842659
Launcher: Job 184 completed in 8664 seconds.
Launcher: Task 197 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  61841
Epoch:0, Train loss:0.548416, valid loss:0.542040
Epoch:1, Train loss:0.199934, valid loss:0.005914
Epoch:2, Train loss:0.010287, valid loss:0.005145
Epoch:3, Train loss:0.007527, valid loss:0.003677
Epoch:4, Train loss:0.006297, valid loss:0.003988
Epoch:5, Train loss:0.005405, valid loss:0.003615
Epoch:6, Train loss:0.004826, valid loss:0.003234
Epoch:7, Train loss:0.004405, valid loss:0.002159
Epoch:8, Train loss:0.004091, valid loss:0.002069
Epoch:9, Train loss:0.003869, valid loss:0.001832
Epoch:10, Train loss:0.003618, valid loss:0.001958
Epoch:11, Train loss:0.002685, valid loss:0.001597
Epoch:12, Train loss:0.002694, valid loss:0.001661
Epoch:13, Train loss:0.002672, valid loss:0.001616
Epoch:14, Train loss:0.002508, valid loss:0.001767
Epoch:15, Train loss:0.002472, valid loss:0.001595
Epoch:16, Train loss:0.002438, valid loss:0.001293
Epoch:17, Train loss:0.002348, valid loss:0.001605
Epoch:18, Train loss:0.002319, valid loss:0.001330
Epoch:19, Train loss:0.002266, valid loss:0.001789
Epoch:20, Train loss:0.002285, valid loss:0.001201
Epoch:21, Train loss:0.001797, valid loss:0.001054
Epoch:22, Train loss:0.001766, valid loss:0.001178
Epoch:23, Train loss:0.001775, valid loss:0.001102
Epoch:24, Train loss:0.001793, valid loss:0.001046
Epoch:25, Train loss:0.001710, valid loss:0.001058
Epoch:26, Train loss:0.001724, valid loss:0.001221
Epoch:27, Train loss:0.001684, valid loss:0.001098
Epoch:28, Train loss:0.001668, valid loss:0.001103
Epoch:29, Train loss:0.001685, valid loss:0.001401
Epoch:30, Train loss:0.001648, valid loss:0.001163
Epoch:31, Train loss:0.001437, valid loss:0.000965
Epoch:32, Train loss:0.001403, valid loss:0.000961
Epoch:33, Train loss:0.001408, valid loss:0.000886
Epoch:34, Train loss:0.001404, valid loss:0.000901
Epoch:35, Train loss:0.001375, valid loss:0.000930
Epoch:36, Train loss:0.001389, valid loss:0.000924
Epoch:37, Train loss:0.001368, valid loss:0.001146
Epoch:38, Train loss:0.001365, valid loss:0.000926
Epoch:39, Train loss:0.001364, valid loss:0.000931
Epoch:40, Train loss:0.001341, valid loss:0.000942
Epoch:41, Train loss:0.001235, valid loss:0.000867
Epoch:42, Train loss:0.001234, valid loss:0.000853
Epoch:43, Train loss:0.001221, valid loss:0.000911
Epoch:44, Train loss:0.001220, valid loss:0.000883
Epoch:45, Train loss:0.001220, valid loss:0.000844
Epoch:46, Train loss:0.001208, valid loss:0.000848
Epoch:47, Train loss:0.001207, valid loss:0.000927
Epoch:48, Train loss:0.001202, valid loss:0.000842
Epoch:49, Train loss:0.001191, valid loss:0.000843
Epoch:50, Train loss:0.001183, valid loss:0.000895
Epoch:51, Train loss:0.001134, valid loss:0.000809
Epoch:52, Train loss:0.001131, valid loss:0.000814
Epoch:53, Train loss:0.001127, valid loss:0.000837
Epoch:54, Train loss:0.001127, valid loss:0.000810
Epoch:55, Train loss:0.001126, valid loss:0.000819
Epoch:56, Train loss:0.001119, valid loss:0.000865
Epoch:57, Train loss:0.001117, valid loss:0.000822
Epoch:58, Train loss:0.001114, valid loss:0.000821
Epoch:59, Train loss:0.001117, valid loss:0.000802
Epoch:60, Train loss:0.001117, valid loss:0.000806
training time 8535.809472322464
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.026198679837000444
plot_id,batch_id 0 1 miss% 0.01539033840010114
plot_id,batch_id 0 2 miss% 0.02461622130439604
plot_id,batch_id 0 3 miss% 0.027181731961838755
plot_id,batch_id 0 4 miss% 0.021838861115572192
plot_id,batch_id 0 5 miss% 0.040201927423868515
plot_id,batch_id 0 6 miss% 0.02825638789986801
plot_id,batch_id 0 7 miss% 0.027578543172697957
plot_id,batch_id 0 8 miss% 0.030822849147095734
plot_id,batch_id 0 9 miss% 0.024698299364296964
plot_id,batch_id 0 10 miss% 0.05303860812584274
plot_id,batch_id 0 11 miss% 0.04639099207649845
plot_id,batch_id 0 12 miss% 0.019817986117078083
plot_id,batch_id 0 13 miss% 0.026705804755788258
plot_id,batch_id 0 14 miss% 0.03393350407109138
plot_id,batch_id 0 15 miss% 0.04509510013872605
plot_id,batch_id 0 16 miss% 0.03491115345227343
plot_id,batch_id 0 17 miss% 0.06281667964511668
plot_id,batch_id 0 18 miss% 0.028371215874334655
plot_id,batch_id 0 19 miss% 0.04152148403486751
plot_id,batch_id 0 20 miss% 0.02872143404843655
plot_id,batch_id 0 21 miss% 0.018074585503987137
plot_id,batch_id 0 22 miss% 0.028148986785978592
plot_id,batch_id 0 23 miss% 0.0280213735846627
plot_id,batch_id 0 24 miss% 0.031104419899541005
plot_id,batch_id 0 25 miss% 0.04151796266395373
plot_id,batch_id 0 26 miss% 0.018246700816598585
plot_id,batch_id 0 27 miss% 0.026420046766480017
plot_id,batch_id 0 28 miss% 0.02803456181818352
plot_id,batch_id 0 29 miss% 0.025709882843267515
plot_id,batch_id 0 30 miss% 0.03315696425335565
plot_id,batch_id 0 31 miss% 0.029278679390771393
plot_id,batch_id 0 32 miss% 0.04041647201630284
plot_id,batch_id 0 33 miss% 0.03033789621680444
plot_id,batch_id 0 34 miss% 0.02909068369018675
plot_id,batch_id 0 35 miss% 0.03151524621208131
plot_id,batch_id 0 36 miss% 0.03332645559707099
plot_id,batch_id 0 37 miss% 0.04498961369711952
plot_id,batch_id 0 38 miss% 0.028829240153195518
plot_id,batch_id 0 39 miss% 0.03043428784896643
plot_id,batch_id 0 40 miss% 0.05452842211024287
plot_id,batch_id 0 41 miss% 0.022751151283130884
plot_id,batch_id 0 42 miss% 0.020676800345127762
plot_id,batch_id 0 43 miss% 0.0246903815805494
plot_id,batch_id 0 44 miss% 0.014897292061074319
plot_id,batch_id 0 45 miss% 0.037382253528334776
plot_id,batch_id 0 46 miss% 0.016869392999371038
plot_id,batch_id 0 47 miss% 0.022495833786918173
plot_id,batch_id 0 48 miss% 0.031352389157677164
plot_id,batch_id 0 49 miss% 0.0222529606821729
plot_id,batch_id 0 50 miss% 0.03529084521039069
plot_id,batch_id 0 51 miss% 0.02651867462764793
plot_id,batch_id 0 52 miss% 0.024251243494330058
plot_id,batch_id 0 53 miss% 0.019187039105190366
plot_id,batch_id 0 54 miss% 0.035940970276020195
plot_id,batch_id 0 55 miss% 0.04688694012718451
plot_id,batch_id 0 56 miss% 0.022022504345581142
plot_id,batch_id 0 57 miss% 0.031646127425008136
plot_id,batch_id 0 58 miss% 0.024223919658606513
plot_id,batch_id 0 59 miss% 0.029852314126403476
plot_id,batch_id 0 60 miss% 0.04118355209888929
plot_id,batch_id 0 61 miss% 0.04131735803017033
plot_id,batch_id 0 62 miss% 0.024017787808385913
plot_id,batch_id 0 63 miss% 0.02552164040956896
plot_id,batch_id 0 64 miss% 0.03588127118174532
plot_id,batch_id 0 65 miss% 0.06334456529962257
plot_id,batch_id 0 66 miss% 0.05322877966925786
plot_id,batch_id 0 67 miss% 0.03359771129105171
plot_id,batch_id 0 68 miss% 0.031262700688514666
plot_id,batch_id 0 69 the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  46193
Epoch:0, Train loss:0.374471, valid loss:0.341930
Epoch:1, Train loss:0.096294, valid loss:0.004885
Epoch:2, Train loss:0.006885, valid loss:0.003152
Epoch:3, Train loss:0.005297, valid loss:0.002736
Epoch:4, Train loss:0.004512, valid loss:0.002301
Epoch:5, Train loss:0.004034, valid loss:0.002074
Epoch:6, Train loss:0.003626, valid loss:0.001992
Epoch:7, Train loss:0.003304, valid loss:0.001482
Epoch:8, Train loss:0.003045, valid loss:0.001534
Epoch:9, Train loss:0.002849, valid loss:0.001417
Epoch:10, Train loss:0.002654, valid loss:0.001524
Epoch:11, Train loss:0.002081, valid loss:0.001267
Epoch:12, Train loss:0.001985, valid loss:0.001332
Epoch:13, Train loss:0.001999, valid loss:0.001007
Epoch:14, Train loss:0.001950, valid loss:0.001106
Epoch:15, Train loss:0.001877, valid loss:0.001009
Epoch:16, Train loss:0.001849, valid loss:0.001193
Epoch:17, Train loss:0.001818, valid loss:0.001054
Epoch:18, Train loss:0.001792, valid loss:0.001013
Epoch:19, Train loss:0.001772, valid loss:0.000942
Epoch:20, Train loss:0.001743, valid loss:0.001059
Epoch:21, Train loss:0.001421, valid loss:0.000941
Epoch:22, Train loss:0.001422, valid loss:0.000806
Epoch:23, Train loss:0.001393, valid loss:0.000874
Epoch:24, Train loss:0.001379, valid loss:0.000844
Epoch:25, Train loss:0.001370, valid loss:0.000831
Epoch:26, Train loss:0.001354, valid loss:0.000812
Epoch:27, Train loss:0.001363, valid loss:0.001036
Epoch:28, Train loss:0.001360, valid loss:0.000864
Epoch:29, Train loss:0.001325, valid loss:0.000859
Epoch:30, Train loss:0.001338, valid loss:0.000800
Epoch:31, Train loss:0.001159, valid loss:0.000767
Epoch:32, Train loss:0.001141, valid loss:0.000730
Epoch:33, Train loss:0.001152, valid loss:0.000727
Epoch:34, Train loss:0.001143, valid loss:0.000751
Epoch:35, Train loss:0.001127, valid loss:0.000775
Epoch:36, Train loss:0.001138, valid loss:0.000742
Epoch:37, Train loss:0.001115, valid loss:0.000685
Epoch:38, Train loss:0.001122, valid loss:0.000754
Epoch:39, Train loss:0.001111, valid loss:0.000705
Epoch:40, Train loss:0.001120, valid loss:0.000717
Epoch:41, Train loss:0.001026, valid loss:0.000708
Epoch:42, Train loss:0.001014, valid loss:0.000675
Epoch:43, Train loss:0.001011, valid loss:0.000691
Epoch:44, Train loss:0.001010, valid loss:0.000685
Epoch:45, Train loss:0.001013, valid loss:0.000684
Epoch:46, Train loss:0.001004, valid loss:0.000660
Epoch:47, Train loss:0.001010, valid loss:0.000677
Epoch:48, Train loss:0.001001, valid loss:0.000676
Epoch:49, Train loss:0.000993, valid loss:0.000736
Epoch:50, Train loss:0.000994, valid loss:0.000678
Epoch:51, Train loss:0.000954, valid loss:0.000675
Epoch:52, Train loss:0.000948, valid loss:0.000668
Epoch:53, Train loss:0.000947, valid loss:0.000663
Epoch:54, Train loss:0.000942, valid loss:0.000652
Epoch:55, Train loss:0.000948, valid loss:0.000653
Epoch:56, Train loss:0.000944, valid loss:0.000662
Epoch:57, Train loss:0.000940, valid loss:0.000674
Epoch:58, Train loss:0.000938, valid loss:0.000654
Epoch:59, Train loss:0.000939, valid loss:0.000651
Epoch:60, Train loss:0.000937, valid loss:0.000651
training time 8547.663746833801
total number of trained parameters for initialize model 46193
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.040301620412750136
plot_id,batch_id 0 1 miss% 0.029889693617618982
plot_id,batch_id 0 2 miss% 0.026628824762985188
plot_id,batch_id 0 3 miss% 0.03576984961108987
plot_id,batch_id 0 4 miss% 0.0339756272684775
plot_id,batch_id 0 5 miss% 0.036131666442981686
plot_id,batch_id 0 6 miss% 0.03489368464755261
plot_id,batch_id 0 7 miss% 0.026795079830266418
plot_id,batch_id 0 8 miss% 0.02479570439416893
plot_id,batch_id 0 9 miss% 0.023009637636409376
plot_id,batch_id 0 10 miss% 0.06041182330977678
plot_id,batch_id 0 11 miss% 0.052343193896152135
plot_id,batch_id 0 12 miss% 0.03539412896335305
plot_id,batch_id 0 13 miss% 0.026994467046390636
plot_id,batch_id 0 14 miss% 0.030416020577781675
plot_id,batch_id 0 15 miss% 0.043269973023658134
plot_id,batch_id 0 16 miss% 0.032145818486857915
plot_id,batch_id 0 17 miss% 0.03599036698213852
plot_id,batch_id 0 18 miss% 0.03148636849517757
plot_id,batch_id 0 19 miss% 0.034162284868958816
plot_id,batch_id 0 20 miss% 0.03977636486606483
plot_id,batch_id 0 21 miss% 0.029035815490178807
plot_id,batch_id 0 22 miss% 0.0255580941923548
plot_id,batch_id 0 23 miss% 0.02504627437537989
plot_id,batch_id 0 24 miss% 0.023829996298591127
plot_id,batch_id 0 25 miss% 0.039071423481257944
plot_id,batch_id 0 26 miss% 0.025597067915252788
plot_id,batch_id 0 27 miss% 0.024527088855845848
plot_id,batch_id 0 28 miss% 0.026994056351067264
plot_id,batch_id 0 29 miss% 0.027779206143909944
plot_id,batch_id 0 30 miss% 0.023433388565222285
plot_id,batch_id 0 31 miss% 0.04109138701627206
plot_id,batch_id 0 32 miss% 0.037821286412129305
plot_id,batch_id 0 33 miss% 0.02395650105863057
plot_id,batch_id 0 34 miss% 0.04568024715675916
plot_id,batch_id 0 35 miss% 0.050437301550060126
plot_id,batch_id 0 36 miss% 0.039948044143209654
plot_id,batch_id 0 37 miss% 0.025815300794243688
plot_id,batch_id 0 38 miss% 0.02240850880659316
plot_id,batch_id 0 39 miss% 0.028837404668508714
plot_id,batch_id 0 40 miss% 0.08567379874833325
plot_id,batch_id 0 41 miss% 0.0183017159127127
plot_id,batch_id 0 42 miss% 0.024150609062073877
plot_id,batch_id 0 43 miss% 0.029259063328269663
plot_id,batch_id 0 44 miss% 0.019886513542377023
plot_id,batch_id 0 45 miss% 0.02709303952394731
plot_id,batch_id 0 46 miss% 0.030789629872209866
plot_id,batch_id 0 47 miss% 0.025039209573707855
plot_id,batch_id 0 48 miss% 0.029103088524068964
plot_id,batch_id 0 49 miss% 0.0236448969702078
plot_id,batch_id 0 50 miss% 0.0383248427898339
plot_id,batch_id 0 51 miss% 0.030861713428064493
plot_id,batch_id 0 52 miss% 0.031836398750939666
plot_id,batch_id 0 53 miss% 0.026079459518843422
plot_id,batch_id 0 54 miss% 0.03584265937646749
plot_id,batch_id 0 55 miss% 0.03748797233202901
plot_id,batch_id 0 56 miss% 0.02566683432139901
plot_id,batch_id 0 57 miss% 0.022939243301901658
plot_id,batch_id 0 58 miss% 0.024716427285892614
plot_id,batch_id 0 59 miss% 0.024683265880550444
plot_id,batch_id 0 60 miss% 0.03252456587998593
plot_id,batch_id 0 61 miss% 0.030807753392182648
plot_id,batch_id 0 62 miss% 0.025685979561942855
plot_id,batch_id 0 63 miss% 0.05023984370089943
plot_id,batch_id 0 64 miss% 0.02227528876281294
plot_id,batch_id 0 65 miss% 0.027679791298704032
plot_id,batch_id 0 66 miss% 0.04762619255305222
plot_id,batch_id 0 67 miss% 0.03416336697332035
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  61841
Epoch:0, Train loss:0.415387, valid loss:0.408024
Epoch:1, Train loss:0.110781, valid loss:0.004882
Epoch:2, Train loss:0.006361, valid loss:0.003123
Epoch:3, Train loss:0.004873, valid loss:0.002464
Epoch:4, Train loss:0.004057, valid loss:0.002005
Epoch:5, Train loss:0.003595, valid loss:0.001716
Epoch:6, Train loss:0.003326, valid loss:0.001683
Epoch:7, Train loss:0.003101, valid loss:0.001612
Epoch:8, Train loss:0.002887, valid loss:0.001874
Epoch:9, Train loss:0.002810, valid loss:0.001526
Epoch:10, Train loss:0.002669, valid loss:0.001494
Epoch:11, Train loss:0.001925, valid loss:0.001051
Epoch:12, Train loss:0.001960, valid loss:0.001204
Epoch:13, Train loss:0.001902, valid loss:0.000983
Epoch:14, Train loss:0.001841, valid loss:0.001323
Epoch:15, Train loss:0.001835, valid loss:0.001162
Epoch:16, Train loss:0.001770, valid loss:0.001015
Epoch:17, Train loss:0.001778, valid loss:0.000884
Epoch:18, Train loss:0.001725, valid loss:0.000926
Epoch:19, Train loss:0.001765, valid loss:0.001008
Epoch:20, Train loss:0.001711, valid loss:0.001107
Epoch:21, Train loss:0.001325, valid loss:0.000906
Epoch:22, Train loss:0.001314, valid loss:0.000870
Epoch:23, Train loss:0.001312, valid loss:0.000864
Epoch:24, Train loss:0.001290, valid loss:0.000902
Epoch:25, Train loss:0.001260, valid loss:0.000802
Epoch:26, Train loss:0.001264, valid loss:0.000797
Epoch:27, Train loss:0.001244, valid loss:0.000768
Epoch:28, Train loss:0.001223, valid loss:0.000839
Epoch:29, Train loss:0.001245, valid loss:0.000830
Epoch:30, Train loss:0.001226, valid loss:0.000739
Epoch:31, Train loss:0.001024, valid loss:0.000751
Epoch:32, Train loss:0.001015, valid loss:0.000690
Epoch:33, Train loss:0.001001, valid loss:0.000803
Epoch:34, Train loss:0.001014, valid loss:0.000686
Epoch:35, Train loss:0.001004, valid loss:0.000811
Epoch:36, Train loss:0.001012, valid loss:0.000674
Epoch:37, Train loss:0.000989, valid loss:0.000965
Epoch:38, Train loss:0.001002, valid loss:0.000664
Epoch:39, Train loss:0.000986, valid loss:0.000867
Epoch:40, Train loss:0.000968, valid loss:0.000651
Epoch:41, Train loss:0.000874, valid loss:0.000637
Epoch:42, Train loss:0.000876, valid loss:0.000645
Epoch:43, Train loss:0.000872, valid loss:0.000612
Epoch:44, Train loss:0.000869, valid loss:0.000599
Epoch:45, Train loss:0.000863, valid loss:0.000638
Epoch:46, Train loss:0.000856, valid loss:0.000611
Epoch:47, Train loss:0.000862, valid loss:0.000692
Epoch:48, Train loss:0.000865, valid loss:0.000627
Epoch:49, Train loss:0.000848, valid loss:0.000611
Epoch:50, Train loss:0.000847, valid loss:0.000752
Epoch:51, Train loss:0.000800, valid loss:0.000612
Epoch:52, Train loss:0.000790, valid loss:0.000611
Epoch:53, Train loss:0.000787, valid loss:0.000591
Epoch:54, Train loss:0.000789, valid loss:0.000598
Epoch:55, Train loss:0.000785, valid loss:0.000582
Epoch:56, Train loss:0.000794, valid loss:0.000607
Epoch:57, Train loss:0.000782, valid loss:0.000596
Epoch:58, Train loss:0.000782, valid loss:0.000594
Epoch:59, Train loss:0.000783, valid loss:0.000586
Epoch:60, Train loss:0.000779, valid loss:0.000582
training time 8551.222617864609
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.016135918323976525
plot_id,batch_id 0 1 miss% 0.026217849603646936
plot_id,batch_id 0 2 miss% 0.030625020731870872
plot_id,batch_id 0 3 miss% 0.02685424898724381
plot_id,batch_id 0 4 miss% 0.03447943384518089
plot_id,batch_id 0 5 miss% 0.03591161601233796
plot_id,batch_id 0 6 miss% 0.025544497340600526
plot_id,batch_id 0 7 miss% 0.02199703683329779
plot_id,batch_id 0 8 miss% 0.025005757406514124
plot_id,batch_id 0 9 miss% 0.023120779696757254
plot_id,batch_id 0 10 miss% 0.028663595330541407
plot_id,batch_id 0 11 miss% 0.03665829425348855
plot_id,batch_id 0 12 miss% 0.02503167674269462
plot_id,batch_id 0 13 miss% 0.024503418080923055
plot_id,batch_id 0 14 miss% 0.035140512470442926
plot_id,batch_id 0 15 miss% 0.04486239576493898
plot_id,batch_id 0 16 miss% 0.029867581298326933
plot_id,batch_id 0 17 miss% 0.02875606404286249
plot_id,batch_id 0 18 miss% 0.03669165990644884
plot_id,batch_id 0 19 miss% 0.03857792632801427
plot_id,batch_id 0 20 miss% 0.02584043031985874
plot_id,batch_id 0 21 miss% 0.02759791388850921
plot_id,batch_id 0 22 miss% 0.02775908876320072
plot_id,batch_id 0 23 miss% 0.03231287882931087
plot_id,batch_id 0 24 miss% 0.03706563618292938
plot_id,batch_id 0 25 miss% 0.03244491933927938
plot_id,batch_id 0 26 miss% 0.023445391025044164
plot_id,batch_id 0 27 miss% 0.031906321247629765
plot_id,batch_id 0 28 miss% 0.03578250290531394
plot_id,batch_id 0 29 miss% 0.038298280830468316
plot_id,batch_id 0 30 miss% 0.055998586726653005
plot_id,batch_id 0 31 miss% 0.03845596500636186
plot_id,batch_id 0 32 miss% 0.03419046268250437
plot_id,batch_id 0 33 miss% 0.03153971420188768
plot_id,batch_id 0 34 miss% 0.02868450217837343
plot_id,batch_id 0 35 miss% 0.03448705957492928
plot_id,batch_id 0 36 miss% 0.03593208283401611
plot_id,batch_id 0 37 miss% 0.02903159243773453
plot_id,batch_id 0 38 miss% 0.025770261135776358
plot_id,batch_id 0 39 miss% 0.02659575907066823
plot_id,batch_id 0 40 miss% 0.053116986990243684
plot_id,batch_id 0 41 miss% 0.020100555049946806
plot_id,batch_id 0 42 miss% 0.026517852215618812
plot_id,batch_id 0 43 miss% 0.03182157980965772
plot_id,batch_id 0 44 miss% 0.03433041158687326
plot_id,batch_id 0 45 miss% 0.024462076377032453
plot_id,batch_id 0 46 miss% 0.026927911968793735
plot_id,batch_id 0 47 miss% 0.02401999847943999
plot_id,batch_id 0 48 miss% 0.033230705067586644
plot_id,batch_id 0 49 miss% 0.027380756966428225
plot_id,batch_id 0 50 miss% 0.03193738428868429
plot_id,batch_id 0 51 miss% 0.032445517151180686
plot_id,batch_id 0 52 miss% 0.036324224360223555
plot_id,batch_id 0 53 miss% 0.01620933264852931
plot_id,batch_id 0 54 miss% 0.037224075494198504
plot_id,batch_id 0 55 miss% 0.041177067279561656
plot_id,batch_id 0 56 miss% 0.032271389241002695
plot_id,batch_id 0 57 miss% 0.026013024677677132
plot_id,batch_id 0 58 miss% 0.03690855204278334
plot_id,batch_id 0 59 miss% 0.04238492724197507
plot_id,batch_id 0 60 miss% 0.027900953038223947
plot_id,batch_id 0 61 miss% 0.027562177410809262
plot_id,batch_id 0 62 miss% 0.020745183666288462
plot_id,batch_id 0 63 miss% 0.024964695803183466
plot_id,batch_id 0 64 miss% 0.03271806440199772
plot_id,batch_id 0 65 miss% 0.03540828134642011
plot_id,batch_id 0 66 miss% 0.033853410436248524
plot_id,batch_id 0 67 miss% 0.034383988613361505
miss% 0.026073101643239723
plot_id,batch_id 0 70 miss% 0.062274790272412874
plot_id,batch_id 0 71 miss% 0.03250526898989834
plot_id,batch_id 0 72 miss% 0.041116998523446115
plot_id,batch_id 0 73 miss% 0.028512782068608734
plot_id,batch_id 0 74 miss% 0.05229415908446085
plot_id,batch_id 0 75 miss% 0.04002624488285956
plot_id,batch_id 0 76 miss% 0.026911318163726793
plot_id,batch_id 0 77 miss% 0.030282162946099835
plot_id,batch_id 0 78 miss% 0.03303521997996662
plot_id,batch_id 0 79 miss% 0.03998915650939748
plot_id,batch_id 0 80 miss% 0.05592847518008037
plot_id,batch_id 0 81 miss% 0.034735739630721026
plot_id,batch_id 0 82 miss% 0.03120902203569731
plot_id,batch_id 0 83 miss% 0.03906456343481953
plot_id,batch_id 0 84 miss% 0.028893951544765293
plot_id,batch_id 0 85 miss% 0.04349471671178795
plot_id,batch_id 0 86 miss% 0.03645108686318924
plot_id,batch_id 0 87 miss% 0.03285365034590421
plot_id,batch_id 0 88 miss% 0.033890191311606886
plot_id,batch_id 0 89 miss% 0.027291714566233364
plot_id,batch_id 0 90 miss% 0.04312554716313916
plot_id,batch_id 0 91 miss% 0.03329735219810423
plot_id,batch_id 0 92 miss% 0.03391115681343838
plot_id,batch_id 0 93 miss% 0.027533177574190052
plot_id,batch_id 0 94 miss% 0.030588742325841047
plot_id,batch_id 0 95 miss% 0.034775101472833414
plot_id,batch_id 0 96 miss% 0.03415347023641524
plot_id,batch_id 0 97 miss% 0.06177838866644521
plot_id,batch_id 0 98 miss% 0.031987554162484666
plot_id,batch_id 0 99 miss% 0.04130565570354728
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02619868 0.01539034 0.02461622 0.02718173 0.02183886 0.04020193
 0.02825639 0.02757854 0.03082285 0.0246983  0.05303861 0.04639099
 0.01981799 0.0267058  0.0339335  0.0450951  0.03491115 0.06281668
 0.02837122 0.04152148 0.02872143 0.01807459 0.02814899 0.02802137
 0.03110442 0.04151796 0.0182467  0.02642005 0.02803456 0.02570988
 0.03315696 0.02927868 0.04041647 0.0303379  0.02909068 0.03151525
 0.03332646 0.04498961 0.02882924 0.03043429 0.05452842 0.02275115
 0.0206768  0.02469038 0.01489729 0.03738225 0.01686939 0.02249583
 0.03135239 0.02225296 0.03529085 0.02651867 0.02425124 0.01918704
 0.03594097 0.04688694 0.0220225  0.03164613 0.02422392 0.02985231
 0.04118355 0.04131736 0.02401779 0.02552164 0.03588127 0.06334457
 0.05322878 0.03359771 0.0312627  0.0260731  0.06227479 0.03250527
 0.041117   0.02851278 0.05229416 0.04002624 0.02691132 0.03028216
 0.03303522 0.03998916 0.05592848 0.03473574 0.03120902 0.03906456
 0.02889395 0.04349472 0.03645109 0.03285365 0.03389019 0.02729171
 0.04312555 0.03329735 0.03391116 0.02753318 0.03058874 0.0347751
 0.03415347 0.06177839 0.03198755 0.04130566]
for model  95 the mean error 0.03327125145138826
all id 95 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 25 out win 5 err 0.03327125145138826
Launcher: Job 96 completed in 8744 seconds.
Launcher: Task 164 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  77489
Epoch:0, Train loss:0.489063, valid loss:0.488345
Epoch:1, Train loss:0.038461, valid loss:0.009566
Epoch:2, Train loss:0.012616, valid loss:0.004660
Epoch:3, Train loss:0.008036, valid loss:0.004191
Epoch:4, Train loss:0.006235, valid loss:0.004098
Epoch:5, Train loss:0.005274, valid loss:0.003049
Epoch:6, Train loss:0.004793, valid loss:0.002975
Epoch:7, Train loss:0.004417, valid loss:0.002945
Epoch:8, Train loss:0.004290, valid loss:0.002457
Epoch:9, Train loss:0.004035, valid loss:0.002169
Epoch:10, Train loss:0.004101, valid loss:0.002256
Epoch:11, Train loss:0.002640, valid loss:0.001629
Epoch:12, Train loss:0.002648, valid loss:0.001361
Epoch:13, Train loss:0.002569, valid loss:0.001532
Epoch:14, Train loss:0.002652, valid loss:0.001437
Epoch:15, Train loss:0.002575, valid loss:0.001351
Epoch:16, Train loss:0.002435, valid loss:0.001346
Epoch:17, Train loss:0.002403, valid loss:0.001291
Epoch:18, Train loss:0.002467, valid loss:0.001748
Epoch:19, Train loss:0.002359, valid loss:0.001747
Epoch:20, Train loss:0.002415, valid loss:0.001413
Epoch:21, Train loss:0.001656, valid loss:0.000994
Epoch:22, Train loss:0.001710, valid loss:0.001040
Epoch:23, Train loss:0.001670, valid loss:0.001055
Epoch:24, Train loss:0.001652, valid loss:0.001142
Epoch:25, Train loss:0.001669, valid loss:0.000984
Epoch:26, Train loss:0.001624, valid loss:0.000876
Epoch:27, Train loss:0.001586, valid loss:0.001012
Epoch:28, Train loss:0.001598, valid loss:0.001026
Epoch:29, Train loss:0.001619, valid loss:0.000959
Epoch:30, Train loss:0.001585, valid loss:0.001023
Epoch:31, Train loss:0.001221, valid loss:0.000913
Epoch:32, Train loss:0.001205, valid loss:0.000832
Epoch:33, Train loss:0.001235, valid loss:0.000861
Epoch:34, Train loss:0.001227, valid loss:0.000941
Epoch:35, Train loss:0.001202, valid loss:0.000856
Epoch:36, Train loss:0.001189, valid loss:0.000905
Epoch:37, Train loss:0.001176, valid loss:0.000914
Epoch:38, Train loss:0.001198, valid loss:0.000958
Epoch:39, Train loss:0.001205, valid loss:0.000946
Epoch:40, Train loss:0.001174, valid loss:0.000883
Epoch:41, Train loss:0.001008, valid loss:0.000795
Epoch:42, Train loss:0.000990, valid loss:0.000797
Epoch:43, Train loss:0.000992, valid loss:0.000750
Epoch:44, Train loss:0.000974, valid loss:0.000765
Epoch:45, Train loss:0.000998, valid loss:0.000800
Epoch:46, Train loss:0.000988, valid loss:0.000741
Epoch:47, Train loss:0.000983, valid loss:0.000803
Epoch:48, Train loss:0.000971, valid loss:0.000779
Epoch:49, Train loss:0.000965, valid loss:0.000761
Epoch:50, Train loss:0.000968, valid loss:0.000781
Epoch:51, Train loss:0.000884, valid loss:0.000775
Epoch:52, Train loss:0.000873, valid loss:0.000738
Epoch:53, Train loss:0.000884, valid loss:0.000734
Epoch:54, Train loss:0.000877, valid loss:0.000759
Epoch:55, Train loss:0.000874, valid loss:0.000780
Epoch:56, Train loss:0.000878, valid loss:0.000731
Epoch:57, Train loss:0.000875, valid loss:0.000743
Epoch:58, Train loss:0.000872, valid loss:0.000757
Epoch:59, Train loss:0.000861, valid loss:0.000763
Epoch:60, Train loss:0.000876, valid loss:0.000729
training time 8552.842576026917
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02075676994231466
plot_id,batch_id 0 1 miss% 0.02515410610902613
plot_id,batch_id 0 2 miss% 0.02508645914765315
plot_id,batch_id 0 3 miss% 0.02793927032522569
plot_id,batch_id 0 4 miss% 0.027450257273121847
plot_id,batch_id 0 5 miss% 0.030877150706780623
plot_id,batch_id 0 6 miss% 0.028729017324340998
plot_id,batch_id 0 7 miss% 0.02479474136683749
plot_id,batch_id 0 8 miss% 0.02683221613138945
plot_id,batch_id 0 9 miss% 0.024482218860687455
plot_id,batch_id 0 10 miss% 0.055943811013621246
plot_id,batch_id 0 11 miss% 0.042755179351423436
plot_id,batch_id 0 12 miss% 0.024297234172971655
plot_id,batch_id 0 13 miss% 0.023347629121226697
plot_id,batch_id 0 14 miss% 0.026873906012679905
plot_id,batch_id 0 15 miss% 0.038972835840170444
plot_id,batch_id 0 16 miss% 0.03843286427204886
plot_id,batch_id 0 17 miss% 0.03173559495967123
plot_id,batch_id 0 18 miss% 0.033944682277325534
plot_id,batch_id 0 19 miss% 0.03692368608617346
plot_id,batch_id 0 20 miss% 0.029369521444548928
plot_id,batch_id 0 21 miss% 0.03292826548387854
plot_id,batch_id 0 22 miss% 0.02209976636436044
plot_id,batch_id 0 23 miss% 0.02051197618949667
plot_id,batch_id 0 24 miss% 0.02035320024356757
plot_id,batch_id 0 25 miss% 0.02941946230796989
plot_id,batch_id 0 26 miss% 0.0218863122824403
plot_id,batch_id 0 27 miss% 0.026153180475323
plot_id,batch_id 0 28 miss% 0.02751426051790614
plot_id,batch_id 0 29 miss% 0.025360460291622942
plot_id,batch_id 0 30 miss% 0.04378833191584058
plot_id,batch_id 0 31 miss% 0.02961711840603616
plot_id,batch_id 0 32 miss% 0.03199559850987545
plot_id,batch_id 0 33 miss% 0.023644270194269294
plot_id,batch_id 0 34 miss% 0.024311181648349882
plot_id,batch_id 0 35 miss% 0.03994416957822702
plot_id,batch_id 0 36 miss% 0.03402529737234983
plot_id,batch_id 0 37 miss% 0.013768031466718119
plot_id,batch_id 0 38 miss% 0.030194249488120377
plot_id,batch_id 0 39 miss% 0.020835899388665702
plot_id,batch_id 0 40 miss% 0.04614628895012641
plot_id,batch_id 0 41 miss% 0.015591763390398205
plot_id,batch_id 0 42 miss% 0.012742861362062733
plot_id,batch_id 0 43 miss% 0.01653750072887583
plot_id,batch_id 0 44 miss% 0.0261219821000593
plot_id,batch_id 0 45 miss% 0.030343856390528702
plot_id,batch_id 0 46 miss% 0.02209531966237504
plot_id,batch_id 0 47 miss% 0.0220138865515486
plot_id,batch_id 0 48 miss% 0.018103492200390803
plot_id,batch_id 0 49 miss% 0.0214906682333858
plot_id,batch_id 0 50 miss% 0.033092328970879416
plot_id,batch_id 0 51 miss% 0.02552611320937161
plot_id,batch_id 0 52 miss% 0.02127024404320361
plot_id,batch_id 0 53 miss% 0.016908269430033544
plot_id,batch_id 0 54 miss% 0.02165898315771667
plot_id,batch_id 0 55 miss% 0.029547708634159443
plot_id,batch_id 0 56 miss% 0.02206509069592073
plot_id,batch_id 0 57 miss% 0.021543908331145598
plot_id,batch_id 0 58 miss% 0.026154903204497235
plot_id,batch_id 0 59 miss% 0.03130405810525081
plot_id,batch_id 0 60 miss% 0.03236233354457391
plot_id,batch_id 0 61 miss% 0.021553855265554
plot_id,batch_id 0 62 miss% 0.02037850377490955
plot_id,batch_id 0 63 miss% 0.028323644642730253
plot_id,batch_id 0 64 miss% 0.030438889464600414
plot_id,batch_id 0 65 miss% 0.03600961766162503
plot_id,batch_id 0 66 miss% 0.04656313557267957
plot_id,batch_id 0 67 miss% 0.03009348077252248
plot_id,batch_id 0 68 miss% 0.03041966514777796
plot_id,batch_id 0 69 miss% plot_id,batch_id 0 68 miss% 0.018082478861764645
plot_id,batch_id 0 69 miss% 0.022707504394370536
plot_id,batch_id 0 70 miss% 0.050835183357136875
plot_id,batch_id 0 71 miss% 0.04800000423191404
plot_id,batch_id 0 72 miss% 0.0458496222801349
plot_id,batch_id 0 73 miss% 0.030393276777642874
plot_id,batch_id 0 74 miss% 0.03155366204485269
plot_id,batch_id 0 75 miss% 0.046554844392161586
plot_id,batch_id 0 76 miss% 0.04394006371353986
plot_id,batch_id 0 77 miss% 0.02221125448087324
plot_id,batch_id 0 78 miss% 0.04040634454478141
plot_id,batch_id 0 79 miss% 0.03336809489585721
plot_id,batch_id 0 80 miss% 0.04964962941327532
plot_id,batch_id 0 81 miss% 0.03762036203042664
plot_id,batch_id 0 82 miss% 0.04860906112551427
plot_id,batch_id 0 83 miss% 0.032248212843719246
plot_id,batch_id 0 84 miss% 0.021866016304014253
plot_id,batch_id 0 85 miss% 0.048362927677142384
plot_id,batch_id 0 86 miss% 0.035476214306669474
plot_id,batch_id 0 87 miss% 0.029728386214793002
plot_id,batch_id 0 88 miss% 0.03182536826892664
plot_id,batch_id 0 89 miss% 0.02676102100318149
plot_id,batch_id 0 90 miss% 0.039950076149821476
plot_id,batch_id 0 91 miss% 0.042847530072644004
plot_id,batch_id 0 92 miss% 0.04072510273985071
plot_id,batch_id 0 93 miss% 0.03734749135074048
plot_id,batch_id 0 94 miss% 0.04697449546480076
plot_id,batch_id 0 95 miss% 0.03729753592050111
plot_id,batch_id 0 96 miss% 0.03359066175790209
plot_id,batch_id 0 97 miss% 0.07438556850827825
plot_id,batch_id 0 98 miss% 0.03847483521681538
plot_id,batch_id 0 99 miss% 0.0484837241299847
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04030162 0.02988969 0.02662882 0.03576985 0.03397563 0.03613167
 0.03489368 0.02679508 0.0247957  0.02300964 0.06041182 0.05234319
 0.03539413 0.02699447 0.03041602 0.04326997 0.03214582 0.03599037
 0.03148637 0.03416228 0.03977636 0.02903582 0.02555809 0.02504627
 0.02383    0.03907142 0.02559707 0.02452709 0.02699406 0.02777921
 0.02343339 0.04109139 0.03782129 0.0239565  0.04568025 0.0504373
 0.03994804 0.0258153  0.02240851 0.0288374  0.0856738  0.01830172
 0.02415061 0.02925906 0.01988651 0.02709304 0.03078963 0.02503921
 0.02910309 0.0236449  0.03832484 0.03086171 0.0318364  0.02607946
 0.03584266 0.03748797 0.02566683 0.02293924 0.02471643 0.02468327
 0.03252457 0.03080775 0.02568598 0.05023984 0.02227529 0.02767979
 0.04762619 0.03416337 0.01808248 0.0227075  0.05083518 0.048
 0.04584962 0.03039328 0.03155366 0.04655484 0.04394006 0.02221125
 0.04040634 0.03336809 0.04964963 0.03762036 0.04860906 0.03224821
 0.02186602 0.04836293 0.03547621 0.02972839 0.03182537 0.02676102
 0.03995008 0.04284753 0.0407251  0.03734749 0.0469745  0.03729754
 0.03359066 0.07438557 0.03847484 0.04848372]
for model  167 the mean error 0.034399603110568396
all id 167 hidden_dim 24 learning_rate 0.005 num_layers 3 frames 31 out win 5 err 0.034399603110568396
plot_id,batch_id 0 68 miss% 0.034045482796835363
plot_id,batch_id 0 69 miss% 0.019930786899221908
plot_id,batch_id 0 70 miss% 0.05197992972993268
plot_id,batch_id 0 71 miss% 0.04860825633428023
plot_id,batch_id 0 72 miss% 0.04594146934733495
plot_id,batch_id 0 73 miss% 0.03444710766651215
plot_id,batch_id 0 74 miss% 0.02586738121450773
plot_id,batch_id 0 75 miss% 0.06572694627611401
plot_id,batch_id 0 76 miss% 0.048715727886290555
plot_id,batch_id 0 77 miss% 0.030753915515888387
plot_id,batch_id 0 78 miss% 0.032535492830683695
plot_id,batch_id 0 79 miss% 0.037495370235851706
plot_id,batch_id 0 80 miss% 0.0421310640784058
plot_id,batch_id 0 81 miss% 0.024116787753923223
plot_id,batch_id 0 82 miss% 0.026644634039070858
plot_id,batch_id 0 83 miss% 0.037066148250899926
plot_id,batch_id 0 84 miss% 0.021821433350109217
plot_id,batch_id 0 85 miss% 0.06161224891292719
plot_id,batch_id 0 86 miss% 0.030361931738083384
plot_id,batch_id 0 87 miss% 0.03441284556969695
plot_id,batch_id 0 88 miss% 0.039017143005661864
plot_id,batch_id 0 89 miss% 0.026123007028177884
plot_id,batch_id 0 90 miss% 0.04077519866847765
plot_id,batch_id 0 91 miss% 0.0398781013487579
plot_id,batch_id 0 92 miss% 0.027583430435355348
plot_id,batch_id 0 93 miss% 0.023086864572186686
plot_id,batch_id 0 94 miss% 0.033778556895714476
plot_id,batch_id 0 95 miss% 0.037831554645957326
plot_id,batch_id 0 96 miss% 0.03869236397270719
plot_id,batch_id 0 97 miss% 0.038292376426262605
plot_id,batch_id 0 98 miss% 0.0361141451226534
plot_id,batch_id 0 99 miss% 0.028628297861498675
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01613592 0.02621785 0.03062502 0.02685425 0.03447943 0.03591162
 0.0255445  0.02199704 0.02500576 0.02312078 0.0286636  0.03665829
 0.02503168 0.02450342 0.03514051 0.0448624  0.02986758 0.02875606
 0.03669166 0.03857793 0.02584043 0.02759791 0.02775909 0.03231288
 0.03706564 0.03244492 0.02344539 0.03190632 0.0357825  0.03829828
 0.05599859 0.03845597 0.03419046 0.03153971 0.0286845  0.03448706
 0.03593208 0.02903159 0.02577026 0.02659576 0.05311699 0.02010056
 0.02651785 0.03182158 0.03433041 0.02446208 0.02692791 0.02402
 0.03323071 0.02738076 0.03193738 0.03244552 0.03632422 0.01620933
 0.03722408 0.04117707 0.03227139 0.02601302 0.03690855 0.04238493
 0.02790095 0.02756218 0.02074518 0.0249647  0.03271806 0.03540828
 0.03385341 0.03438399 0.03404548 0.01993079 0.05197993 0.04860826
 0.04594147 0.03444711 0.02586738 0.06572695 0.04871573 0.03075392
 0.03253549 0.03749537 0.04213106 0.02411679 0.02664463 0.03706615
 0.02182143 0.06161225 0.03036193 0.03441285 0.03901714 0.02612301
 0.0407752  0.0398781  0.02758343 0.02308686 0.03377856 0.03783155
 0.03869236 0.03829238 0.03611415 0.0286283 ]
for model  203 the mean error 0.0328414171624451
all id 203 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 31 out win 5 err 0.0328414171624451
Launcher: Job 168 completed in 8747 seconds.
Launcher: Task 223 done. Exiting.
Launcher: Job 204 completed in 8745 seconds.
Launcher: Task 69 done. Exiting.
0.030986319313346364
plot_id,batch_id 0 70 miss% 0.048711825844552914
plot_id,batch_id 0 71 miss% 0.05762683000953502
plot_id,batch_id 0 72 miss% 0.023894160905650226
plot_id,batch_id 0 73 miss% 0.032028356119794474
plot_id,batch_id 0 74 miss% 0.03901207280487292
plot_id,batch_id 0 75 miss% 0.03646958840980472
plot_id,batch_id 0 76 miss% 0.026078297791155468
plot_id,batch_id 0 77 miss% 0.042619488713129934
plot_id,batch_id 0 78 miss% 0.03426085356881338
plot_id,batch_id 0 79 miss% 0.031346313398935795
plot_id,batch_id 0 80 miss% 0.05870436071743261
plot_id,batch_id 0 81 miss% 0.03037247596938392
plot_id,batch_id 0 82 miss% 0.028929326707243068
plot_id,batch_id 0 83 miss% 0.031061624234173732
plot_id,batch_id 0 84 miss% 0.04014788190806036
plot_id,batch_id 0 85 miss% 0.05616204996020528
plot_id,batch_id 0 86 miss% 0.03039165390370634
plot_id,batch_id 0 87 miss% 0.030176258721355514
plot_id,batch_id 0 88 miss% 0.03475884545566566
plot_id,batch_id 0 89 miss% 0.03061171023567652
plot_id,batch_id 0 90 miss% 0.037382914554190995
plot_id,batch_id 0 91 miss% 0.032970988923923056
plot_id,batch_id 0 92 miss% 0.03577848070213143
plot_id,batch_id 0 93 miss% 0.037209057495248186
plot_id,batch_id 0 94 miss% 0.03823874818502106
plot_id,batch_id 0 95 miss% 0.05046991344900791
plot_id,batch_id 0 96 miss% 0.039619692209798274
plot_id,batch_id 0 97 miss% 0.049765003160783854
plot_id,batch_id 0 98 miss% 0.036682022586416
plot_id,batch_id 0 99 miss% 0.03307337834742676
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02075677 0.02515411 0.02508646 0.02793927 0.02745026 0.03087715
 0.02872902 0.02479474 0.02683222 0.02448222 0.05594381 0.04275518
 0.02429723 0.02334763 0.02687391 0.03897284 0.03843286 0.03173559
 0.03394468 0.03692369 0.02936952 0.03292827 0.02209977 0.02051198
 0.0203532  0.02941946 0.02188631 0.02615318 0.02751426 0.02536046
 0.04378833 0.02961712 0.0319956  0.02364427 0.02431118 0.03994417
 0.0340253  0.01376803 0.03019425 0.0208359  0.04614629 0.01559176
 0.01274286 0.0165375  0.02612198 0.03034386 0.02209532 0.02201389
 0.01810349 0.02149067 0.03309233 0.02552611 0.02127024 0.01690827
 0.02165898 0.02954771 0.02206509 0.02154391 0.0261549  0.03130406
 0.03236233 0.02155386 0.0203785  0.02832364 0.03043889 0.03600962
 0.04656314 0.03009348 0.03041967 0.03098632 0.04871183 0.05762683
 0.02389416 0.03202836 0.03901207 0.03646959 0.0260783  0.04261949
 0.03426085 0.03134631 0.05870436 0.03037248 0.02892933 0.03106162
 0.04014788 0.05616205 0.03039165 0.03017626 0.03475885 0.03061171
 0.03738291 0.03297099 0.03577848 0.03720906 0.03823875 0.05046991
 0.03961969 0.049765   0.03668202 0.03307338]
for model  157 the mean error 0.030849930313696018
all id 157 hidden_dim 24 learning_rate 0.02 num_layers 5 frames 25 out win 4 err 0.030849930313696018
Launcher: Job 158 completed in 8752 seconds.
Launcher: Task 133 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  77489
Epoch:0, Train loss:0.633722, valid loss:0.631607
Epoch:1, Train loss:0.069831, valid loss:0.023577
Epoch:2, Train loss:0.036159, valid loss:0.009263
Epoch:3, Train loss:0.015492, valid loss:0.007369
Epoch:4, Train loss:0.011648, valid loss:0.005512
Epoch:5, Train loss:0.009642, valid loss:0.004587
Epoch:6, Train loss:0.008323, valid loss:0.004697
Epoch:7, Train loss:0.007211, valid loss:0.003682
Epoch:8, Train loss:0.006575, valid loss:0.003873
Epoch:9, Train loss:0.006140, valid loss:0.004699
Epoch:10, Train loss:0.005772, valid loss:0.002772
Epoch:11, Train loss:0.004221, valid loss:0.002346
Epoch:12, Train loss:0.004158, valid loss:0.002412
Epoch:13, Train loss:0.003958, valid loss:0.002114
Epoch:14, Train loss:0.003906, valid loss:0.001999
Epoch:15, Train loss:0.003744, valid loss:0.002764
Epoch:16, Train loss:0.003643, valid loss:0.002229
Epoch:17, Train loss:0.003443, valid loss:0.002110
Epoch:18, Train loss:0.003458, valid loss:0.002030
Epoch:19, Train loss:0.003277, valid loss:0.002139
Epoch:20, Train loss:0.003273, valid loss:0.002221
Epoch:21, Train loss:0.002561, valid loss:0.001612
Epoch:22, Train loss:0.002488, valid loss:0.001460
Epoch:23, Train loss:0.002523, valid loss:0.001473
Epoch:24, Train loss:0.002447, valid loss:0.001656
Epoch:25, Train loss:0.002394, valid loss:0.001575
Epoch:26, Train loss:0.002431, valid loss:0.001504
Epoch:27, Train loss:0.002384, valid loss:0.001642
Epoch:28, Train loss:0.002337, valid loss:0.001639
Epoch:29, Train loss:0.002287, valid loss:0.001481
Epoch:30, Train loss:0.002286, valid loss:0.001486
Epoch:31, Train loss:0.001902, valid loss:0.001309
Epoch:32, Train loss:0.001942, valid loss:0.001383
Epoch:33, Train loss:0.001918, valid loss:0.001396
Epoch:34, Train loss:0.001881, valid loss:0.001342
Epoch:35, Train loss:0.001860, valid loss:0.001303
Epoch:36, Train loss:0.001862, valid loss:0.001388
Epoch:37, Train loss:0.001907, valid loss:0.001280
Epoch:38, Train loss:0.001801, valid loss:0.001311
Epoch:39, Train loss:0.001819, valid loss:0.001365
Epoch:40, Train loss:0.001808, valid loss:0.001373
Epoch:41, Train loss:0.001631, valid loss:0.001179
Epoch:42, Train loss:0.001613, valid loss:0.001251
Epoch:43, Train loss:0.001626, valid loss:0.001187
Epoch:44, Train loss:0.001613, valid loss:0.001192
Epoch:45, Train loss:0.001597, valid loss:0.001220
Epoch:46, Train loss:0.001583, valid loss:0.001205
Epoch:47, Train loss:0.001575, valid loss:0.001208
Epoch:48, Train loss:0.001581, valid loss:0.001253
Epoch:49, Train loss:0.001567, valid loss:0.001222
Epoch:50, Train loss:0.001552, valid loss:0.001198
Epoch:51, Train loss:0.001476, valid loss:0.001182
Epoch:52, Train loss:0.001470, valid loss:0.001190
Epoch:53, Train loss:0.001473, valid loss:0.001155
Epoch:54, Train loss:0.001466, valid loss:0.001153
Epoch:55, Train loss:0.001458, valid loss:0.001183
Epoch:56, Train loss:0.001448, valid loss:0.001162
Epoch:57, Train loss:0.001459, valid loss:0.001182
Epoch:58, Train loss:0.001442, valid loss:0.001156
Epoch:59, Train loss:0.001451, valid loss:0.001155
Epoch:60, Train loss:0.001444, valid loss:0.001178
training time 8622.209948778152
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.0331121515638525
plot_id,batch_id 0 1 miss% 0.023692687804070862
plot_id,batch_id 0 2 miss% 0.02252231005656974
plot_id,batch_id 0 3 miss% 0.03006306867154419
plot_id,batch_id 0 4 miss% 0.027479150905186264
plot_id,batch_id 0 5 miss% 0.031123782442120256
plot_id,batch_id 0 6 miss% 0.027812639234771112
plot_id,batch_id 0 7 miss% 0.029816345325173233
plot_id,batch_id 0 8 miss% 0.033618656853357604
plot_id,batch_id 0 9 miss% 0.023563301349853145
plot_id,batch_id 0 10 miss% 0.03223419480101479
plot_id,batch_id 0 11 miss% 0.03961017839053798
plot_id,batch_id 0 12 miss% 0.029748000389600685
plot_id,batch_id 0 13 miss% 0.03022108814351172
plot_id,batch_id 0 14 miss% 0.03393282214199077
plot_id,batch_id 0 15 miss% 0.043691782699313876
plot_id,batch_id 0 16 miss% 0.03251643705934877
plot_id,batch_id 0 17 miss% 0.03479710028398103
plot_id,batch_id 0 18 miss% 0.03466185174646135
plot_id,batch_id 0 19 miss% 0.04638898046284049
plot_id,batch_id 0 20 miss% 0.03418060524031933
plot_id,batch_id 0 21 miss% 0.01741599441140106
plot_id,batch_id 0 22 miss% 0.03466634054269619
plot_id,batch_id 0 23 miss% 0.030887506696402837
plot_id,batch_id 0 24 miss% 0.03687802276438304
plot_id,batch_id 0 25 miss% 0.04712568491332353
plot_id,batch_id 0 26 miss% 0.03910629716828103
plot_id,batch_id 0 27 miss% 0.03176596734739821
plot_id,batch_id 0 28 miss% 0.02160419555978159
plot_id,batch_id 0 29 miss% 0.028336937669160733
plot_id,batch_id 0 30 miss% 0.03625004487361025
plot_id,batch_id 0 31 miss% 0.027249937405002592
plot_id,batch_id 0 32 miss% 0.03764432836190875
plot_id,batch_id 0 33 miss% 0.035423852751443184
plot_id,batch_id 0 34 miss% 0.022637224193679903
plot_id,batch_id 0 35 miss% 0.04456740895699321
plot_id,batch_id 0 36 miss% 0.04707843751549196
plot_id,batch_id 0 37 miss% 0.026144683783705925
plot_id,batch_id 0 38 miss% 0.02506830448027101
plot_id,batch_id 0 39 miss% 0.029312606410117424
plot_id,batch_id 0 40 miss% 0.03241410046882956
plot_id,batch_id 0 41 miss% 0.0243655379970282
plot_id,batch_id 0 42 miss% 0.018362832835846155
plot_id,batch_id 0 43 miss% 0.03649084938140715
plot_id,batch_id 0 44 miss% 0.01677172522208634
plot_id,batch_id 0 45 miss% 0.026731445553824895
plot_id,batch_id 0 46 miss% 0.024606823700553983
plot_id,batch_id 0 47 miss% 0.026432881168345575
plot_id,batch_id 0 48 miss% 0.0255026533979883
plot_id,batch_id 0 49 miss% 0.023183827151718565
plot_id,batch_id 0 50 miss% 0.03276931231423598
plot_id,batch_id 0 51 miss% 0.02151385034752823
plot_id,batch_id 0 52 miss% 0.02051136199112271
plot_id,batch_id 0 53 miss% 0.019690491264046334
plot_id,batch_id 0 54 miss% 0.018806047259567235
plot_id,batch_id 0 55 miss% 0.028252939496001675
plot_id,batch_id 0 56 miss% 0.027621457480381487
plot_id,batch_id 0 57 miss% 0.031439088932546606
plot_id,batch_id 0 58 miss% 0.020385546095113016
plot_id,batch_id 0 59 miss% 0.022256878406443945
plot_id,batch_id 0 60 miss% 0.0426478856658905
plot_id,batch_id 0 61 miss% 0.03926207770508656
plot_id,batch_id 0 62 miss% 0.035425323180828715
plot_id,batch_id 0 63 miss% 0.04169707043306712
plot_id,batch_id 0 64 miss% 0.03267788803937894
plot_id,batch_id 0 65 miss% 0.037205957138923704
plot_id,batch_id 0 66 miss% 0.08239548050741304
plot_id,batch_id 0 67 miss% 0.023840691430902967
plot_id,batch_id 0 68 miss% 0.02337883642222241
plot_id,batch_id 0 69 miss% 0.04105537890052197
plot_id,batch_id 0 70 miss% 0.041698421268798073
plot_id,batch_id 0 71 miss% 0.03436477285014298
plot_id,batch_id 0 72 miss% 0.03755474084636001
plot_id,batch_id 0 73 miss% 0.023202087636463146
plot_id,batch_id 0 74 miss% 0.04842509528236583
plot_id,batch_id 0 75 miss% 0.043363174327972265
plot_id,batch_id 0 76 miss% 0.04450728979811365
plot_id,batch_id 0 77 miss% 0.039205797731589664
plot_id,batch_id 0 78 miss% 0.028283991185758688
plot_id,batch_id 0 79 miss% 0.05432381290907241
plot_id,batch_id 0 80 miss% 0.04790241965495094
plot_id,batch_id 0 81 miss% 0.03230162507464991
plot_id,batch_id 0 82 miss% 0.029800765071972067
plot_id,batch_id 0 83 miss% 0.03322994246297627
plot_id,batch_id 0 84 miss% 0.03428216783062478
plot_id,batch_id 0 85 miss% 0.0456257697612842
plot_id,batch_id 0 86 miss% 0.04690661656487306
plot_id,batch_id 0 87 miss% 0.04337925932383631
plot_id,batch_id 0 88 miss% 0.04796858484942228
plot_id,batch_id 0 89 miss% 0.03680623356905396
plot_id,batch_id 0 90 miss% 0.06395432633190165
plot_id,batch_id 0 91 miss% 0.03986426513773768
plot_id,batch_id 0 92 miss% 0.03426253320041003
plot_id,batch_id 0 93 miss% 0.031020258422220153
plot_id,batch_id 0 94 miss% 0.03154522855635128
plot_id,batch_id 0 95 miss% 0.04594101108480441
plot_id,batch_id 0 96 miss% 0.03387093591490607
plot_id,batch_id 0 97 miss% 0.049513881734688785
plot_id,batch_id 0 98 miss% 0.03186173242622988
plot_id,batch_id 0 99 miss% 0.02943343983736478
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03311215 0.02369269 0.02252231 0.03006307 0.02747915 0.03112378
 0.02781264 0.02981635 0.03361866 0.0235633  0.03223419 0.03961018
 0.029748   0.03022109 0.03393282 0.04369178 0.03251644 0.0347971
 0.03466185 0.04638898 0.03418061 0.01741599 0.03466634 0.03088751
 0.03687802 0.04712568 0.0391063  0.03176597 0.0216042  0.02833694
 0.03625004 0.02724994 0.03764433 0.03542385 0.02263722 0.04456741
 0.04707844 0.02614468 0.0250683  0.02931261 0.0324141  0.02436554
 0.01836283 0.03649085 0.01677173 0.02673145 0.02460682 0.02643288
 0.02550265 0.02318383 0.03276931 0.02151385 0.02051136 0.01969049
 0.01880605 0.02825294 0.02762146 0.03143909 0.02038555 0.02225688
 0.04264789 0.03926208 0.03542532 0.04169707 0.03267789 0.03720596
 0.08239548 0.02384069 0.02337884 0.04105538 0.04169842 0.03436477
 0.03755474 0.02320209 0.0484251  0.04336317 0.04450729 0.0392058
 0.02828399 0.05432381 0.04790242 0.03230163 0.02980077 0.03322994
 0.03428217 0.04562577 0.04690662 0.04337926 0.04796858 0.03680623
 0.06395433 0.03986427 0.03426253 0.03102026 0.03154523 0.04594101
 0.03387094 0.04951388 0.03186173 0.02943344]
for model  23 the mean error 0.03376047331906219
all id 23 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 21 out win 5 err 0.03376047331906219
Launcher: Job 24 completed in 8838 seconds.
Launcher: Task 212 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  77489
Epoch:0, Train loss:0.633722, valid loss:0.631607
Epoch:1, Train loss:0.063990, valid loss:0.018473
Epoch:2, Train loss:0.025423, valid loss:0.010742
Epoch:3, Train loss:0.014059, valid loss:0.007233
Epoch:4, Train loss:0.011680, valid loss:0.005539
Epoch:5, Train loss:0.009798, valid loss:0.004116
Epoch:6, Train loss:0.013215, valid loss:0.004968
Epoch:7, Train loss:0.008764, valid loss:0.004405
Epoch:8, Train loss:0.008156, valid loss:0.004364
Epoch:9, Train loss:0.007179, valid loss:0.003793
Epoch:10, Train loss:0.007099, valid loss:0.003545
Epoch:11, Train loss:0.004524, valid loss:0.002462
Epoch:12, Train loss:0.004350, valid loss:0.002479
Epoch:13, Train loss:0.004411, valid loss:0.003039
Epoch:14, Train loss:0.004259, valid loss:0.002433
Epoch:15, Train loss:0.004416, valid loss:0.002458
Epoch:16, Train loss:0.004151, valid loss:0.002456
Epoch:17, Train loss:0.003965, valid loss:0.002264
Epoch:18, Train loss:0.003947, valid loss:0.001859
Epoch:19, Train loss:0.003799, valid loss:0.002573
Epoch:20, Train loss:0.003708, valid loss:0.002516
Epoch:21, Train loss:0.002676, valid loss:0.001569
Epoch:22, Train loss:0.002758, valid loss:0.001459
Epoch:23, Train loss:0.002597, valid loss:0.001608
Epoch:24, Train loss:0.002666, valid loss:0.001854
Epoch:25, Train loss:0.002700, valid loss:0.001844
Epoch:26, Train loss:0.002628, valid loss:0.001983
Epoch:27, Train loss:0.002599, valid loss:0.001742
Epoch:28, Train loss:0.002526, valid loss:0.001494
Epoch:29, Train loss:0.002526, valid loss:0.002108
Epoch:30, Train loss:0.002498, valid loss:0.001526
Epoch:31, Train loss:0.001990, valid loss:0.001316
Epoch:32, Train loss:0.001894, valid loss:0.001304
Epoch:33, Train loss:0.001943, valid loss:0.001623
Epoch:34, Train loss:0.001928, valid loss:0.001222
Epoch:35, Train loss:0.001952, valid loss:0.001539
Epoch:36, Train loss:0.001887, valid loss:0.001294
Epoch:37, Train loss:0.001896, valid loss:0.001310
Epoch:38, Train loss:0.001957, valid loss:0.001395
Epoch:39, Train loss:0.001856, valid loss:0.001380
Epoch:40, Train loss:0.001834, valid loss:0.001330
Epoch:41, Train loss:0.001566, valid loss:0.001168
Epoch:42, Train loss:0.001559, valid loss:0.001168
Epoch:43, Train loss:0.001562, valid loss:0.001142
Epoch:44, Train loss:0.001564, valid loss:0.001227
Epoch:45, Train loss:0.001544, valid loss:0.001217
Epoch:46, Train loss:0.001543, valid loss:0.001141
Epoch:47, Train loss:0.001514, valid loss:0.001158
Epoch:48, Train loss:0.001538, valid loss:0.001120
Epoch:49, Train loss:0.001499, valid loss:0.001233
Epoch:50, Train loss:0.001513, valid loss:0.001219
Epoch:51, Train loss:0.001371, valid loss:0.001100
Epoch:52, Train loss:0.001387, valid loss:0.001103
Epoch:53, Train loss:0.001372, valid loss:0.001083
Epoch:54, Train loss:0.001367, valid loss:0.001093
Epoch:55, Train loss:0.001363, valid loss:0.001115
Epoch:56, Train loss:0.001365, valid loss:0.001154
Epoch:57, Train loss:0.001359, valid loss:0.001076
Epoch:58, Train loss:0.001369, valid loss:0.001111
Epoch:59, Train loss:0.001337, valid loss:0.001096
Epoch:60, Train loss:0.001338, valid loss:0.001091
training time 8658.746204853058
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.036875026499670796
plot_id,batch_id 0 1 miss% 0.026717750777226157
plot_id,batch_id 0 2 miss% 0.027745463665844586
plot_id,batch_id 0 3 miss% 0.02781231840997528
plot_id,batch_id 0 4 miss% 0.033614181835078596
plot_id,batch_id 0 5 miss% 0.042145320751045635
plot_id,batch_id 0 6 miss% 0.028805079605491248
plot_id,batch_id 0 7 miss% 0.03433779125350042
plot_id,batch_id 0 8 miss% 0.04330654681411179
plot_id,batch_id 0 9 miss% 0.023829442735419537
plot_id,batch_id 0 10 miss% 0.03041538163093691
plot_id,batch_id 0 11 miss% 0.0359068970394645
plot_id,batch_id 0 12 miss% 0.03903870308063575
plot_id,batch_id 0 13 miss% 0.02716896068278653
plot_id,batch_id 0 14 miss% 0.04092816662401469
plot_id,batch_id 0 15 miss% 0.04603355036020621
plot_id,batch_id 0 16 miss% 0.025600011251418597
plot_id,batch_id 0 17 miss% 0.03113075003643059
plot_id,batch_id 0 18 miss% 0.03591169993919461
plot_id,batch_id 0 19 miss% 0.041761855608276624
plot_id,batch_id 0 20 miss% 0.04172154082253658
plot_id,batch_id 0 21 miss% 0.026130534054960873
plot_id,batch_id 0 22 miss% 0.051604864927983746
plot_id,batch_id 0 23 miss% 0.028022824658049294
plot_id,batch_id 0 24 miss% 0.03497152160441786
plot_id,batch_id 0 25 miss% 0.03353530206269923
plot_id,batch_id 0 26 miss% 0.028429302010464307
plot_id,batch_id 0 27 miss% 0.02659226098765113
plot_id,batch_id 0 28 miss% 0.027837683276843203
plot_id,batch_id 0 29 miss% 0.03050262627928571
plot_id,batch_id 0 30 miss% 0.05799534079777037
plot_id,batch_id 0 31 miss% 0.04013522088348591
plot_id,batch_id 0 32 miss% 0.03480282569878285
plot_id,batch_id 0 33 miss% 0.03074202949472905
plot_id,batch_id 0 34 miss% 0.02762417638182587
plot_id,batch_id 0 35 miss% 0.058472099778685865
plot_id,batch_id 0 36 miss% 0.04932095992618359
plot_id,batch_id 0 37 miss% 0.039834678306060606
plot_id,batch_id 0 38 miss% 0.029067570271442425
plot_id,batch_id 0 39 miss% 0.021760162075673876
plot_id,batch_id 0 40 miss% 0.04978468211822709
plot_id,batch_id 0 41 miss% 0.031602876405138765
plot_id,batch_id 0 42 miss% 0.025810338187497697
plot_id,batch_id 0 43 miss% 0.03584740130368588
plot_id,batch_id 0 44 miss% 0.034295128831957414
plot_id,batch_id 0 45 miss% 0.032215572232848086
plot_id,batch_id 0 46 miss% 0.03451811496149669
plot_id,batch_id 0 47 miss% 0.033645679636159914
plot_id,batch_id 0 48 miss% 0.029729257679117053
plot_id,batch_id 0 49 miss% 0.023578328878320846
plot_id,batch_id 0 50 miss% 0.05145509826508991
plot_id,batch_id 0 51 miss% 0.0205483912801217
plot_id,batch_id 0 52 miss% 0.027720336691426373
plot_id,batch_id 0 53 miss% 0.018484384280352595
plot_id,batch_id 0 54 miss% 0.02549763515609092
plot_id,batch_id 0 55 miss% 0.04028361456767368
plot_id,batch_id 0 56 miss% 0.026061728380142694
plot_id,batch_id 0 57 miss% 0.023189552820328082
plot_id,batch_id 0 58 miss% 0.023833871529028074
plot_id,batch_id 0 59 miss% 0.029496486931589752
plot_id,batch_id 0 60 miss% 0.04534276082670196
plot_id,batch_id 0 61 miss% 0.038049391135948525
plot_id,batch_id 0 62 miss% 0.028442219294919725
plot_id,batch_id 0 63 miss% 0.03917433591115835
plot_id,batch_id 0 64 miss% 0.055188199420817506
plot_id,batch_id 0 65 miss% 0.04355039912075384
plot_id,batch_id 0 66 miss% 0.10029203279012895
plot_id,batch_id 0 67 miss% 0.03234482808223713
plot_id,batch_id 0 68 miss% 0.034264911275524924
plot_id,batch_id 0 69 miss% 0.02304758144683694
plot_id,batch_id 0 70 miss% 0.06366504199905634
plot_id,batch_id 0 71 miss% 0.034500610694518485
plot_id,batch_id 0 72 miss% 0.03146088749902267
plot_id,batch_id 0 73 miss% 0.020047775939371647
plot_id,batch_id 0 74 miss% 0.02865619048705381
plot_id,batch_id 0 75 miss% 0.0598996976158223
plot_id,batch_id 0 76 miss% 0.04118741145437451
plot_id,batch_id 0 77 miss% 0.0407665447132112
plot_id,batch_id 0 78 miss% 0.026306894352315518
plot_id,batch_id 0 79 miss% 0.04505062883036678
plot_id,batch_id 0 80 miss% 0.07019008743237168
plot_id,batch_id 0 81 miss% 0.04575830319234443
plot_id,batch_id 0 82 miss% 0.043849746474589765
plot_id,batch_id 0 83 miss% 0.02823350199625992
plot_id,batch_id 0 84 miss% 0.03614884894998984
plot_id,batch_id 0 85 miss% 0.05532402299394849
plot_id,batch_id 0 86 miss% 0.04375553057693651
plot_id,batch_id 0 87 miss% 0.034237247767841726
plot_id,batch_id 0 88 miss% 0.04667337873635019
plot_id,batch_id 0 89 miss% 0.04264075498944634
plot_id,batch_id 0 90 miss% 0.06974252115080737
plot_id,batch_id 0 91 miss% 0.038376111770177954
plot_id,batch_id 0 92 miss% 0.04092002119558382
plot_id,batch_id 0 93 miss% 0.03726961798698967
plot_id,batch_id 0 94 miss% 0.057893006273055256
plot_id,batch_id 0 95 miss% 0.07011176974356043
plot_id,batch_id 0 96 miss% 0.04037916297763083
plot_id,batch_id 0 97 miss% 0.04760661998488201
plot_id,batch_id 0 98 miss% 0.034853625439174606
plot_id,batch_id 0 99 miss% 0.03564158096619145
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03687503 0.02671775 0.02774546 0.02781232 0.03361418 0.04214532
 0.02880508 0.03433779 0.04330655 0.02382944 0.03041538 0.0359069
 0.0390387  0.02716896 0.04092817 0.04603355 0.02560001 0.03113075
 0.0359117  0.04176186 0.04172154 0.02613053 0.05160486 0.02802282
 0.03497152 0.0335353  0.0284293  0.02659226 0.02783768 0.03050263
 0.05799534 0.04013522 0.03480283 0.03074203 0.02762418 0.0584721
 0.04932096 0.03983468 0.02906757 0.02176016 0.04978468 0.03160288
 0.02581034 0.0358474  0.03429513 0.03221557 0.03451811 0.03364568
 0.02972926 0.02357833 0.0514551  0.02054839 0.02772034 0.01848438
 0.02549764 0.04028361 0.02606173 0.02318955 0.02383387 0.02949649
 0.04534276 0.03804939 0.02844222 0.03917434 0.0551882  0.0435504
 0.10029203 0.03234483 0.03426491 0.02304758 0.06366504 0.03450061
 0.03146089 0.02004778 0.02865619 0.0598997  0.04118741 0.04076654
 0.02630689 0.04505063 0.07019009 0.0457583  0.04384975 0.0282335
 0.03614885 0.05532402 0.04375553 0.03423725 0.04667338 0.04264075
 0.06974252 0.03837611 0.04092002 0.03726962 0.05789301 0.07011177
 0.04037916 0.04760662 0.03485363 0.03564158]
for model  77 the mean error 0.03766630706524829
all id 77 hidden_dim 24 learning_rate 0.02 num_layers 5 frames 21 out win 5 err 0.03766630706524829
Launcher: Job 78 completed in 8873 seconds.
Launcher: Task 233 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  61841
Epoch:0, Train loss:0.415387, valid loss:0.408024
Epoch:1, Train loss:0.071167, valid loss:0.004568
Epoch:2, Train loss:0.006213, valid loss:0.002635
Epoch:3, Train loss:0.004665, valid loss:0.002699
Epoch:4, Train loss:0.003953, valid loss:0.002601
Epoch:5, Train loss:0.003540, valid loss:0.002285
Epoch:6, Train loss:0.003206, valid loss:0.001808
Epoch:7, Train loss:0.002936, valid loss:0.001447
Epoch:8, Train loss:0.002789, valid loss:0.001331
Epoch:9, Train loss:0.002586, valid loss:0.001702
Epoch:10, Train loss:0.002538, valid loss:0.001308
Epoch:11, Train loss:0.001872, valid loss:0.001163
Epoch:12, Train loss:0.001839, valid loss:0.001107
Epoch:13, Train loss:0.001812, valid loss:0.000995
Epoch:14, Train loss:0.001778, valid loss:0.001009
Epoch:15, Train loss:0.001753, valid loss:0.000979
Epoch:16, Train loss:0.001693, valid loss:0.001152
Epoch:17, Train loss:0.001678, valid loss:0.000850
Epoch:18, Train loss:0.001641, valid loss:0.000934
Epoch:19, Train loss:0.001629, valid loss:0.001166
Epoch:20, Train loss:0.001605, valid loss:0.001099
Epoch:21, Train loss:0.001295, valid loss:0.000828
Epoch:22, Train loss:0.001269, valid loss:0.000868
Epoch:23, Train loss:0.001242, valid loss:0.000844
Epoch:24, Train loss:0.001275, valid loss:0.000935
Epoch:25, Train loss:0.001226, valid loss:0.000848
Epoch:26, Train loss:0.001219, valid loss:0.000826
Epoch:27, Train loss:0.001223, valid loss:0.000876
Epoch:28, Train loss:0.001200, valid loss:0.000747
Epoch:29, Train loss:0.001180, valid loss:0.000870
Epoch:30, Train loss:0.001201, valid loss:0.000795
Epoch:31, Train loss:0.001012, valid loss:0.000738
Epoch:32, Train loss:0.001001, valid loss:0.000676
Epoch:33, Train loss:0.001022, valid loss:0.000732
Epoch:34, Train loss:0.000987, valid loss:0.000674
Epoch:35, Train loss:0.000996, valid loss:0.000858
Epoch:36, Train loss:0.000990, valid loss:0.000662
Epoch:37, Train loss:0.000977, valid loss:0.000763
Epoch:38, Train loss:0.000974, valid loss:0.000672
Epoch:39, Train loss:0.000969, valid loss:0.000662
Epoch:40, Train loss:0.000965, valid loss:0.000808
Epoch:41, Train loss:0.000878, valid loss:0.000679
Epoch:42, Train loss:0.000878, valid loss:0.000667
Epoch:43, Train loss:0.000876, valid loss:0.000650
Epoch:44, Train loss:0.000867, valid loss:0.000634
Epoch:45, Train loss:0.000861, valid loss:0.000663
Epoch:46, Train loss:0.000866, valid loss:0.000662
Epoch:47, Train loss:0.000865, valid loss:0.000652
Epoch:48, Train loss:0.000854, valid loss:0.000630
Epoch:49, Train loss:0.000862, valid loss:0.000616
Epoch:50, Train loss:0.000849, valid loss:0.000673
Epoch:51, Train loss:0.000816, valid loss:0.000625
Epoch:52, Train loss:0.000814, valid loss:0.000638
Epoch:53, Train loss:0.000805, valid loss:0.000629
Epoch:54, Train loss:0.000809, valid loss:0.000634
Epoch:55, Train loss:0.000812, valid loss:0.000621
Epoch:56, Train loss:0.000804, valid loss:0.000662
Epoch:57, Train loss:0.000805, valid loss:0.000627
Epoch:58, Train loss:0.000801, valid loss:0.000643
Epoch:59, Train loss:0.000802, valid loss:0.000639
Epoch:60, Train loss:0.000803, valid loss:0.000627
training time 8812.341044902802
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.02694241400797547
plot_id,batch_id 0 1 miss% 0.023628477080677696
plot_id,batch_id 0 2 miss% 0.022451559893555934
plot_id,batch_id 0 3 miss% 0.01335156027350688
plot_id,batch_id 0 4 miss% 0.02118277784357221
plot_id,batch_id 0 5 miss% 0.03469335637326178
plot_id,batch_id 0 6 miss% 0.02781881227659246
plot_id,batch_id 0 7 miss% 0.025572526892707118
plot_id,batch_id 0 8 miss% 0.02167273582395257
plot_id,batch_id 0 9 miss% 0.02074587334033624
plot_id,batch_id 0 10 miss% 0.03486582449573365
plot_id,batch_id 0 11 miss% 0.04069857808955892
plot_id,batch_id 0 12 miss% 0.030058389992242678
plot_id,batch_id 0 13 miss% 0.030408948034954652
plot_id,batch_id 0 14 miss% 0.03369751180245837
plot_id,batch_id 0 15 miss% 0.0393853169492128
plot_id,batch_id 0 16 miss% 0.031769191402835725
plot_id,batch_id 0 17 miss% 0.058694850044981886
plot_id,batch_id 0 18 miss% 0.031059344332571008
plot_id,batch_id 0 19 miss% 0.032368684114334065
plot_id,batch_id 0 20 miss% 0.03604380647400139
plot_id,batch_id 0 21 miss% 0.020203232401124965
plot_id,batch_id 0 22 miss% 0.03253006311692779
plot_id,batch_id 0 23 miss% 0.018347090817645458
plot_id,batch_id 0 24 miss% 0.02942244237852571
plot_id,batch_id 0 25 miss% 0.03317434656368693
plot_id,batch_id 0 26 miss% 0.02115621188321499
plot_id,batch_id 0 27 miss% 0.02331326557688666
plot_id,batch_id 0 28 miss% 0.01771598787503735
plot_id,batch_id 0 29 miss% 0.031188740863837085
plot_id,batch_id 0 30 miss% 0.025789276185928647
plot_id,batch_id 0 31 miss% 0.03323839503394753
plot_id,batch_id 0 32 miss% 0.03428670085381163
plot_id,batch_id 0 33 miss% 0.027903909018195564
plot_id,batch_id 0 34 miss% 0.025070126369072854
plot_id,batch_id 0 35 miss% 0.03675289518903678
plot_id,batch_id 0 36 miss% 0.04102753007390184
plot_id,batch_id 0 37 miss% 0.03660763540382068
plot_id,batch_id 0 38 miss% 0.0221996842539577
plot_id,batch_id 0 39 miss% 0.017493932678371606
plot_id,batch_id 0 40 miss% 0.057295533100240666
plot_id,batch_id 0 41 miss% 0.03105412148534757
plot_id,batch_id 0 42 miss% 0.024674831406324337
plot_id,batch_id 0 43 miss% 0.025940432064618723
plot_id,batch_id 0 44 miss% 0.01650451258337565
plot_id,batch_id 0 45 miss% 0.020029715427312262
plot_id,batch_id 0 46 miss% 0.02570584577537965
plot_id,batch_id 0 47 miss% 0.01800124023479159
plot_id,batch_id 0 48 miss% 0.01822024329034587
plot_id,batch_id 0 49 miss% 0.021399889469424425
plot_id,batch_id 0 50 miss% 0.03570737842109921
plot_id,batch_id 0 51 miss% 0.02219692545501871
plot_id,batch_id 0 52 miss% 0.023680028908273646
plot_id,batch_id 0 53 miss% 0.01424096419900801
plot_id,batch_id 0 54 miss% 0.03548632295712464
plot_id,batch_id 0 55 miss% 0.05610776082715923
plot_id,batch_id 0 56 miss% 0.0289616896765127
plot_id,batch_id 0 57 miss% 0.01979335420282892
plot_id,batch_id 0 58 miss% 0.029960855111669345
plot_id,batch_id 0 59 miss% 0.027088073929126072
plot_id,batch_id 0 60 miss% 0.034102509618953905
plot_id,batch_id 0 61 miss% 0.03367831786216707
plot_id,batch_id 0 62 miss% 0.02625366280953879
plot_id,batch_id 0 63 miss% 0.039860600293229194
plot_id,batch_id 0 64 miss% 0.03804832619580208
plot_id,batch_id 0 65 miss% 0.02673604478975251
plot_id,batch_id 0 66 miss% 0.04159474741280151
plot_id,batch_id 0 67 miss% 0.026160785268104628
plot_id,batch_id 0 68 miss% 0.015685822126881896
plot_id,batch_id 0 69 miss% 0.02287095022476743
plot_id,batch_id 0 70 miss% 0.060681677733889
plot_id,batch_id 0 71 miss% 0.051358342813822865
plot_id,batch_id 0 72 miss% 0.03339811052602952
plot_id,batch_id 0 73 miss% 0.0309313503345568
plot_id,batch_id 0 74 miss% 0.03002336102265369
plot_id,batch_id 0 75 miss% 0.030707101585475155
plot_id,batch_id 0 76 miss% 0.05808109843761944
plot_id,batch_id 0 77 miss% 0.0309514898807043
plot_id,batch_id 0 78 miss% 0.037267704586576814
plot_id,batch_id 0 79 miss% 0.05359419532162243
plot_id,batch_id 0 80 miss% 0.04298088903593068
plot_id,batch_id 0 81 miss% 0.027588193947473066
plot_id,batch_id 0 82 miss% 0.04018077054171715
plot_id,batch_id 0 83 miss% 0.02166202210690023
plot_id,batch_id 0 84 miss% 0.014537405646785071
plot_id,batch_id 0 85 miss% 0.042858795336619314
plot_id,batch_id 0 86 miss% 0.030691484573542576
plot_id,batch_id 0 87 miss% 0.03088881065985881
plot_id,batch_id 0 88 miss% 0.039096805231070626
plot_id,batch_id 0 89 miss% 0.029798667563233638
plot_id,batch_id 0 90 miss% 0.03621817645613064
plot_id,batch_id 0 91 miss% 0.04645667744677444
plot_id,batch_id 0 92 miss% 0.02767687155491647
plot_id,batch_id 0 93 miss% 0.03186824905736926
plot_id,batch_id 0 94 miss% 0.03366273851976357
plot_id,batch_id 0 95 miss% 0.05998614519303574
plot_id,batch_id 0 96 miss% 0.04113034067981216
plot_id,batch_id 0 97 miss% 0.04402658187771282
plot_id,batch_id 0 98 miss% 0.04279227859656183
plot_id,batch_id 0 99 miss% 0.02114408475008831
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02694241 0.02362848 0.02245156 0.01335156 0.02118278 0.03469336
 0.02781881 0.02557253 0.02167274 0.02074587 0.03486582 0.04069858
 0.03005839 0.03040895 0.03369751 0.03938532 0.03176919 0.05869485
 0.03105934 0.03236868 0.03604381 0.02020323 0.03253006 0.01834709
 0.02942244 0.03317435 0.02115621 0.02331327 0.01771599 0.03118874
 0.02578928 0.0332384  0.0342867  0.02790391 0.02507013 0.0367529
 0.04102753 0.03660764 0.02219968 0.01749393 0.05729553 0.03105412
 0.02467483 0.02594043 0.01650451 0.02002972 0.02570585 0.01800124
 0.01822024 0.02139989 0.03570738 0.02219693 0.02368003 0.01424096
 0.03548632 0.05610776 0.02896169 0.01979335 0.02996086 0.02708807
 0.03410251 0.03367832 0.02625366 0.0398606  0.03804833 0.02673604
 0.04159475 0.02616079 0.01568582 0.02287095 0.06068168 0.05135834
 0.03339811 0.03093135 0.03002336 0.0307071  0.0580811  0.03095149
 0.0372677  0.0535942  0.04298089 0.02758819 0.04018077 0.02166202
 0.01453741 0.0428588  0.03069148 0.03088881 0.03909681 0.02979867
 0.03621818 0.04645668 0.02767687 0.03186825 0.03366274 0.05998615
 0.04113034 0.04402658 0.04279228 0.02114408]
for model  176 the mean error 0.03143813912221182
all id 176 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 31 out win 5 err 0.03143813912221182
Launcher: Job 177 completed in 9007 seconds.
Launcher: Task 67 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  134801
Epoch:0, Train loss:0.637387, valid loss:0.638600
Epoch:1, Train loss:0.518454, valid loss:0.521676
Epoch:2, Train loss:0.501777, valid loss:0.519085
Epoch:3, Train loss:0.498602, valid loss:0.517883
Epoch:4, Train loss:0.497011, valid loss:0.517419
Epoch:5, Train loss:0.495951, valid loss:0.517463
Epoch:6, Train loss:0.495291, valid loss:0.516832
Epoch:7, Train loss:0.495135, valid loss:0.516864
Epoch:8, Train loss:0.494679, valid loss:0.516793
Epoch:9, Train loss:0.494557, valid loss:0.516539
Epoch:10, Train loss:0.494236, valid loss:0.516446
Epoch:11, Train loss:0.492948, valid loss:0.515859
Epoch:12, Train loss:0.492908, valid loss:0.515813
Epoch:13, Train loss:0.492917, valid loss:0.515770
Epoch:14, Train loss:0.492786, valid loss:0.516223
Epoch:15, Train loss:0.492699, valid loss:0.515820
Epoch:16, Train loss:0.492739, valid loss:0.515792
Epoch:17, Train loss:0.492616, valid loss:0.515825
Epoch:18, Train loss:0.492597, valid loss:0.515576
Epoch:19, Train loss:0.492564, valid loss:0.516196
Epoch:20, Train loss:0.492455, valid loss:0.515755
Epoch:21, Train loss:0.491983, valid loss:0.515675
Epoch:22, Train loss:0.491915, valid loss:0.515444
Epoch:23, Train loss:0.491853, valid loss:0.515399
Epoch:24, Train loss:0.491844, valid loss:0.515567
Epoch:25, Train loss:0.491833, valid loss:0.515442
Epoch:26, Train loss:0.491831, valid loss:0.515409
Epoch:27, Train loss:0.491803, valid loss:0.515621
Epoch:28, Train loss:0.491778, valid loss:0.515360
Epoch:29, Train loss:0.491734, valid loss:0.515352
Epoch:30, Train loss:0.491793, valid loss:0.515396
Epoch:31, Train loss:0.491449, valid loss:0.515239
Epoch:32, Train loss:0.491462, valid loss:0.515247
Epoch:33, Train loss:0.491420, valid loss:0.515227
Epoch:34, Train loss:0.491432, valid loss:0.515231
Epoch:35, Train loss:0.491420, valid loss:0.515297
Epoch:36, Train loss:0.491404, valid loss:0.515222
Epoch:37, Train loss:0.491409, valid loss:0.515298
Epoch:38, Train loss:0.491394, valid loss:0.515152
Epoch:39, Train loss:0.491401, valid loss:0.515218
Epoch:40, Train loss:0.491368, valid loss:0.515182
Epoch:41, Train loss:0.491232, valid loss:0.515150
Epoch:42, Train loss:0.491233, valid loss:0.515213
Epoch:43, Train loss:0.491238, valid loss:0.515210
Epoch:44, Train loss:0.491226, valid loss:0.515168
Epoch:45, Train loss:0.491233, valid loss:0.515172
Epoch:46, Train loss:0.491216, valid loss:0.515174
Epoch:47, Train loss:0.491222, valid loss:0.515125
Epoch:48, Train loss:0.491207, valid loss:0.515105
Epoch:49, Train loss:0.491199, valid loss:0.515148
Epoch:50, Train loss:0.491199, valid loss:0.515190
Epoch:51, Train loss:0.491132, valid loss:0.515128
Epoch:52, Train loss:0.491127, valid loss:0.515128
Epoch:53, Train loss:0.491127, valid loss:0.515130
Epoch:54, Train loss:0.491129, valid loss:0.515127
Epoch:55, Train loss:0.491128, valid loss:0.515125
Epoch:56, Train loss:0.491121, valid loss:0.515115
Epoch:57, Train loss:0.491119, valid loss:0.515100
Epoch:58, Train loss:0.491115, valid loss:0.515124
Epoch:59, Train loss:0.491115, valid loss:0.515111
Epoch:60, Train loss:0.491110, valid loss:0.515094
training time 8845.770616054535
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.8160844864877569
plot_id,batch_id 0 1 miss% 0.8547406257227156
plot_id,batch_id 0 2 miss% 0.8607498577027237
plot_id,batch_id 0 3 miss% 0.8657336717902554
plot_id,batch_id 0 4 miss% 0.8686597135901405
plot_id,batch_id 0 5 miss% 0.8131937876131794
plot_id,batch_id 0 6 miss% 0.8512488145720362
plot_id,batch_id 0 7 miss% 0.8614495792713761
plot_id,batch_id 0 8 miss% 0.8624829147329225
plot_id,batch_id 0 9 miss% 0.8646483388871081
plot_id,batch_id 0 10 miss% 0.8037848798755464
plot_id,batch_id 0 11 miss% 0.850676249291008
plot_id,batch_id 0 12 miss% 0.8609349271368502
plot_id,batch_id 0 13 miss% 0.8596596417987091
plot_id,batch_id 0 14 miss% 0.8671476306548865
plot_id,batch_id 0 15 miss% 0.8033554689692401
plot_id,batch_id 0 16 miss% 0.8466826637088622
plot_id,batch_id 0 17 miss% 0.8568103213972045
plot_id,batch_id 0 18 miss% 0.8613484326288218
plot_id,batch_id 0 19 miss% 0.8618779536456125
plot_id,batch_id 0 20 miss% 0.8414006453757495
plot_id,batch_id 0 21 miss% 0.8618228067808199
plot_id,batch_id 0 22 miss% 0.8649479800574609
plot_id,batch_id 0 23 miss% 0.8677903735027307
plot_id,batch_id 0 24 miss% 0.8696756869290514
plot_id,batch_id 0 25 miss% 0.8346288620763872
plot_id,batch_id 0 26 miss% 0.8591918184881695
plot_id,batch_id 0 27 miss% 0.8640407970112387
plot_id,batch_id 0 28 miss% 0.8658215637396843
plot_id,batch_id 0 29 miss% 0.8678778100962716
plot_id,batch_id 0 30 miss% 0.8275103812188693
plot_id,batch_id 0 31 miss% 0.8577959956260323
plot_id,batch_id 0 32 miss% 0.8613019146380595
plot_id,batch_id 0 33 miss% 0.8637437583224561
plot_id,batch_id 0 34 miss% 0.865145125718679
plot_id,batch_id 0 35 miss% 0.8214775091985907
plot_id,batch_id 0 36 miss% 0.8588814057624631
plot_id,batch_id 0 37 miss% 0.8616074341158587
plot_id,batch_id 0 38 miss% 0.8663374493834433
plot_id,batch_id 0 39 miss% 0.8657003969699794
plot_id,batch_id 0 40 miss% 0.8506032775461673
plot_id,batch_id 0 41 miss% 0.8625340856582812
plot_id,batch_id 0 42 miss% 0.8642116156159657
plot_id,batch_id 0 43 miss% 0.8663453581403541
plot_id,batch_id 0 44 miss% 0.8684244236040187
plot_id,batch_id 0 45 miss% 0.8472206908319454
plot_id,batch_id 0 46 miss% 0.8615571957882433
plot_id,batch_id 0 47 miss% 0.8653122303039892
plot_id,batch_id 0 48 miss% 0.8699877752009717
plot_id,batch_id 0 49 miss% 0.8681690774279786
plot_id,batch_id 0 50 miss% 0.8488441584669185
plot_id,batch_id 0 51 miss% 0.8620689758896473
plot_id,batch_id 0 52 miss% 0.865159804396909
plot_id,batch_id 0 53 miss% 0.867861814880755
plot_id,batch_id 0 54 miss% 0.8696581248558612
plot_id,batch_id 0 55 miss% 0.8532425074360812
plot_id,batch_id 0 56 miss% 0.8641854497575988
plot_id,batch_id 0 57 miss% 0.8667990820399344
plot_id,batch_id 0 58 miss% 0.8691963716826245
plot_id,batch_id 0 59 miss% 0.8680205125647948
plot_id,batch_id 0 60 miss% 0.7693309252645899
plot_id,batch_id 0 61 miss% 0.8404630095472642
plot_id,batch_id 0 62 miss% 0.8525443026299233
plot_id,batch_id 0 63 miss% 0.8581736217034726
plot_id,batch_id 0 64 miss% 0.8606623552798194
plot_id,batch_id 0 65 miss% 0.7564734687113561
plot_id,batch_id 0 66 miss% 0.8291896465718329
plot_id,batch_id 0 67 miss% 0.8422345450281287
plot_id,batch_id 0 68 miss% 0.8573956823112874
plot_id,batch_id 0 69 miss% 0.8574312669799695
plot_id,batch_id 0 70 miss% 0.7219701277946625
plot_id,batch_id 0 71 miss% 0.8357476382341605
plot_id,batch_id 0 72 miss% 0.8388320138730561
plot_id,batch_id 0 73 miss% 0.8481888540677098
plot_id,batch_id 0 74 miss% 0.8536633789061987
plot_id,batch_id 0 75 miss% 0.7578247384953471
plot_id,batch_id 0 76 miss% 0.8171314754247968
plot_id,batch_id 0 77 miss% 0.8288993236622173
plot_id,batch_id 0 78 miss% 0.8415979321681273
plot_id,batch_id 0 79 miss% 0.8518806722421367
plot_id,batch_id 0 80 miss% 0.787550531158315
plot_id,batch_id 0 81 miss% 0.8491401716021122
plot_id,batch_id 0 82 miss% 0.8568935728944236
plot_id,batch_id 0 83 miss% 0.8586875251632194
plot_id,batch_id 0 84 miss% 0.8628635866014485
plot_id,batch_id 0 85 miss% 0.7771996454808069
plot_id,batch_id 0 86 miss% 0.8393008594613967
plot_id,batch_id 0 87 miss% 0.8499073050245848
plot_id,batch_id 0 88 miss% 0.8606167804061639
plot_id,batch_id 0 89 miss% 0.8611725723361664
plot_id,batch_id 0 90 miss% 0.7506277365310243
plot_id,batch_id 0 91 miss% 0.8382732095806608
plot_id,batch_id 0 92 miss% 0.8456479474362617
plot_id,batch_id 0 93 miss% 0.8494637677062151
plot_id,batch_id 0 94 miss% 0.8610315410699011
plot_id,batch_id 0 95 miss% 0.7641277901705237
plot_id,batch_id 0 96 miss% 0.8241786347298
plot_id,batch_id 0 97 miss% 0.8470778761515045
plot_id,batch_id 0 98 miss% 0.8503680214665257
plot_id,batch_id 0 99 miss% 0.8552985197837256
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.81608449 0.85474063 0.86074986 0.86573367 0.86865971 0.81319379
 0.85124881 0.86144958 0.86248291 0.86464834 0.80378488 0.85067625
 0.86093493 0.85965964 0.86714763 0.80335547 0.84668266 0.85681032
 0.86134843 0.86187795 0.84140065 0.86182281 0.86494798 0.86779037
 0.86967569 0.83462886 0.85919182 0.8640408  0.86582156 0.86787781
 0.82751038 0.857796   0.86130191 0.86374376 0.86514513 0.82147751
 0.85888141 0.86160743 0.86633745 0.8657004  0.85060328 0.86253409
 0.86421162 0.86634536 0.86842442 0.84722069 0.8615572  0.86531223
 0.86998778 0.86816908 0.84884416 0.86206898 0.8651598  0.86786181
 0.86965812 0.85324251 0.86418545 0.86679908 0.86919637 0.86802051
 0.76933093 0.84046301 0.8525443  0.85817362 0.86066236 0.75647347
 0.82918965 0.84223455 0.85739568 0.85743127 0.72197013 0.83574764
 0.83883201 0.84818885 0.85366338 0.75782474 0.81713148 0.82889932
 0.84159793 0.85188067 0.78755053 0.84914017 0.85689357 0.85868753
 0.86286359 0.77719965 0.83930086 0.84990731 0.86061678 0.86117257
 0.75062774 0.83827321 0.84564795 0.84946377 0.86103154 0.76412779
 0.82417863 0.84707788 0.85036802 0.85529852]
for model  26 the mean error 0.8460643478022882
all id 26 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 21 out win 5 err 0.8460643478022882
Launcher: Job 27 completed in 9026 seconds.
Launcher: Task 68 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  61841
Epoch:0, Train loss:0.415387, valid loss:0.408024
Epoch:1, Train loss:0.230115, valid loss:0.232252
Epoch:2, Train loss:0.222289, valid loss:0.230110
Epoch:3, Train loss:0.220979, valid loss:0.230745
Epoch:4, Train loss:0.220481, valid loss:0.230190
Epoch:5, Train loss:0.220315, valid loss:0.230127
Epoch:6, Train loss:0.219961, valid loss:0.229869
Epoch:7, Train loss:0.219857, valid loss:0.229902
Epoch:8, Train loss:0.219603, valid loss:0.229570
Epoch:9, Train loss:0.219559, valid loss:0.229653
Epoch:10, Train loss:0.219748, valid loss:0.229719
Epoch:11, Train loss:0.218549, valid loss:0.229472
Epoch:12, Train loss:0.218562, valid loss:0.229372
Epoch:13, Train loss:0.218513, valid loss:0.229297
Epoch:14, Train loss:0.218505, valid loss:0.229244
Epoch:15, Train loss:0.218504, valid loss:0.229296
Epoch:16, Train loss:0.218409, valid loss:0.229281
Epoch:17, Train loss:0.218399, valid loss:0.229187
Epoch:18, Train loss:0.218389, valid loss:0.229376
Epoch:19, Train loss:0.218391, valid loss:0.229236
Epoch:20, Train loss:0.218360, valid loss:0.229143
Epoch:21, Train loss:0.217909, valid loss:0.229023
Epoch:22, Train loss:0.217892, valid loss:0.229176
Epoch:23, Train loss:0.217902, valid loss:0.229045
Epoch:24, Train loss:0.217915, valid loss:0.229097
Epoch:25, Train loss:0.217898, valid loss:0.229097
Epoch:26, Train loss:0.217892, valid loss:0.228974
Epoch:27, Train loss:0.217871, valid loss:0.228898
Epoch:28, Train loss:0.217860, valid loss:0.228964
Epoch:29, Train loss:0.217827, valid loss:0.228979
Epoch:30, Train loss:0.217830, valid loss:0.229002
Epoch:31, Train loss:0.217608, valid loss:0.228874
Epoch:32, Train loss:0.217614, valid loss:0.229178
Epoch:33, Train loss:0.217603, valid loss:0.228873
Epoch:34, Train loss:0.217585, valid loss:0.228896
Epoch:35, Train loss:0.217587, valid loss:0.228870
Epoch:36, Train loss:0.217643, valid loss:0.228846
Epoch:37, Train loss:0.217582, valid loss:0.228834
Epoch:38, Train loss:0.217572, valid loss:0.228884
Epoch:39, Train loss:0.217571, valid loss:0.228898
Epoch:40, Train loss:0.217574, valid loss:0.229004
Epoch:41, Train loss:0.217446, valid loss:0.228834
Epoch:42, Train loss:0.217447, valid loss:0.228838
Epoch:43, Train loss:0.217429, valid loss:0.228816
Epoch:44, Train loss:0.217445, valid loss:0.228817
Epoch:45, Train loss:0.217433, valid loss:0.228797
Epoch:46, Train loss:0.217426, valid loss:0.228805
Epoch:47, Train loss:0.217432, valid loss:0.228882
Epoch:48, Train loss:0.217433, valid loss:0.228818
Epoch:49, Train loss:0.217431, valid loss:0.228810
Epoch:50, Train loss:0.217418, valid loss:0.228863
Epoch:51, Train loss:0.217368, valid loss:0.228800
Epoch:52, Train loss:0.217358, valid loss:0.228818
Epoch:53, Train loss:0.217358, valid loss:0.228823
Epoch:54, Train loss:0.217357, valid loss:0.228784
Epoch:55, Train loss:0.217353, valid loss:0.228800
Epoch:56, Train loss:0.217355, valid loss:0.228902
Epoch:57, Train loss:0.217355, valid loss:0.228805
Epoch:58, Train loss:0.217352, valid loss:0.228801
Epoch:59, Train loss:0.217350, valid loss:0.228808
Epoch:60, Train loss:0.217348, valid loss:0.228823
training time 8850.352708101273
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.7276947431398324
plot_id,batch_id 0 1 miss% 0.7907208481798902
plot_id,batch_id 0 2 miss% 0.8011757333676205
plot_id,batch_id 0 3 miss% 0.8085308863258441
plot_id,batch_id 0 4 miss% 0.806377635898702
plot_id,batch_id 0 5 miss% 0.7286148443152688
plot_id,batch_id 0 6 miss% 0.784386352679619
plot_id,batch_id 0 7 miss% 0.7995514689561813
plot_id,batch_id 0 8 miss% 0.8051339710010531
plot_id,batch_id 0 9 miss% 0.8100579118584724
plot_id,batch_id 0 10 miss% 0.7019462171689488
plot_id,batch_id 0 11 miss% 0.7843630344110003
plot_id,batch_id 0 12 miss% 0.7946048040827951
plot_id,batch_id 0 13 miss% 0.8031302927409362
plot_id,batch_id 0 14 miss% 0.8078086214455893
plot_id,batch_id 0 15 miss% 0.7113545400864005
plot_id,batch_id 0 16 miss% 0.7757895683198615
plot_id,batch_id 0 17 miss% 0.7963466862247021
plot_id,batch_id 0 18 miss% 0.804020505624596
plot_id,batch_id 0 19 miss% 0.8058138082625947
plot_id,batch_id 0 20 miss% 0.7720882325692818
plot_id,batch_id 0 21 miss% 0.8039278661730584
plot_id,batch_id 0 22 miss% 0.8087829760442853
plot_id,batch_id 0 23 miss% 0.812088370364205
plot_id,batch_id 0 24 miss% 0.8132853731604825
plot_id,batch_id 0 25 miss% 0.7500278081204376
plot_id,batch_id 0 26 miss% 0.7978774548573476
plot_id,batch_id 0 27 miss% 0.802941173526413
plot_id,batch_id 0 28 miss% 0.8070834884112524
plot_id,batch_id 0 29 miss% 0.8089409329618837
plot_id,batch_id 0 30 miss% 0.7557013491847859
plot_id,batch_id 0 31 miss% 0.7943979101553941
plot_id,batch_id 0 32 miss% 0.8031374769647571
plot_id,batch_id 0 33 miss% 0.8072194765862964
plot_id,batch_id 0 34 miss% 0.8087283737957582
plot_id,batch_id 0 35 miss% 0.7546796464517372
plot_id,batch_id 0 36 miss% 0.7989310838981567
plot_id,batch_id 0 37 miss% 0.8039293907246138
plot_id,batch_id 0 38 miss% 0.8075138946877078
plot_id,batch_id 0 39 miss% 0.8091563492298542
plot_id,batch_id 0 40 miss% 0.7821905332131169
plot_id,batch_id 0 41 miss% 0.8059196884791376
plot_id,batch_id 0 42 miss% 0.8092574472247837
plot_id,batch_id 0 43 miss% 0.8124355095706741
plot_id,batch_id 0 44 miss% 0.8156280220836111
plot_id,batch_id 0 45 miss% 0.7767728097038483
plot_id,batch_id 0 46 miss% 0.8057141015165931
plot_id,batch_id 0 47 miss% 0.810976817582534
plot_id,batch_id 0 48 miss% 0.8118547629885489
plot_id,batch_id 0 49 miss% 0.8146836526880357
plot_id,batch_id 0 50 miss% 0.7889138870599609
plot_id,batch_id 0 51 miss% 0.8026439638019611
plot_id,batch_id 0 52 miss% 0.8089597884350985
plot_id,batch_id 0 53 miss% 0.8099608012232458
plot_id,batch_id 0 54 miss% 0.8155772768273755
plot_id,batch_id 0 55 miss% 0.7641628217760306
plot_id,batch_id 0 56 miss% 0.8025941613540297
plot_id,batch_id 0 57 miss% 0.8074472801824252
plot_id,batch_id 0 58 miss% 0.8118602418337363
plot_id,batch_id 0 59 miss% 0.8153618609759923
plot_id,batch_id 0 60 miss% 0.6488719908055689
plot_id,batch_id 0 61 miss% 0.7496896782892996
plot_id,batch_id 0 62 miss% 0.78558714317163
plot_id,batch_id 0 63 miss% 0.7943504612990264
plot_id,batch_id 0 64 miss% 0.80327557333915
plot_id,batch_id 0 65 miss% 0.6428372622545343
plot_id,batch_id 0 66 miss% 0.7495097221874865
plot_id,batch_id 0 67 miss% 0.7723893360008468
plot_id,batch_id 0 68 miss% 0.7938828172962669
plot_id,batch_id 0 69 miss% 0.7964332693040758
plot_id,batch_id 0 70 miss% 0.6106859701581447
plot_id,batch_id 0 71 miss% 0.7601826417823638
plot_id,batch_id 0 72 miss% 0.7620632697086965
plot_id,batch_id 0 73 miss% 0.7800033386735951
plot_id,batch_id 0 74 miss% 0.7902714164166144
plot_id,batch_id 0 75 miss% 0.604183400637102
plot_id,batch_id 0 76 miss% 0.7117527778763078
plot_id,batch_id 0 77 miss% 0.753319940842288
plot_id,batch_id 0 78 miss% 0.7753320168467978
plot_id,batch_id 0 79 miss% 0.7855670062781807
plot_id,batch_id 0 80 miss% 0.6718881510700433
plot_id,batch_id 0 81 miss% 0.778659789604613
plot_id,batch_id 0 82 miss% 0.7914228352093756
plot_id,batch_id 0 83 miss% 0.7983350096509558
plot_id,batch_id 0 84 miss% 0.8004925749295724
plot_id,batch_id 0 85 miss% 0.6687373773259669
plot_id,batch_id 0 86 miss% 0.7691097871479434
plot_id,batch_id 0 87 miss% 0.792827168639155
plot_id,batch_id 0 88 miss% 0.7992113637113332
plot_id,batch_id 0 89 miss% 0.7995663586063739
plot_id,batch_id 0 90 miss% 0.6359550231577517
plot_id,batch_id 0 91 miss% 0.7662656085339238
plot_id,batch_id 0 92 miss% 0.7794375401776469
plot_id,batch_id 0 93 miss% 0.7886728872083396
plot_id,batch_id 0 94 miss% 0.79860768234847
plot_id,batch_id 0 95 miss% 0.6306954191918623
plot_id,batch_id 0 96 miss% 0.7461796605311191
plot_id,batch_id 0 97 miss% 0.7748661061598386
plot_id,batch_id 0 98 miss% 0.7865477451828456
plot_id,batch_id 0 99 miss% 0.7954293311254378
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.72769474 0.79072085 0.80117573 0.80853089 0.80637764 0.72861484
 0.78438635 0.79955147 0.80513397 0.81005791 0.70194622 0.78436303
 0.7946048  0.80313029 0.80780862 0.71135454 0.77578957 0.79634669
 0.80402051 0.80581381 0.77208823 0.80392787 0.80878298 0.81208837
 0.81328537 0.75002781 0.79787745 0.80294117 0.80708349 0.80894093
 0.75570135 0.79439791 0.80313748 0.80721948 0.80872837 0.75467965
 0.79893108 0.80392939 0.80751389 0.80915635 0.78219053 0.80591969
 0.80925745 0.81243551 0.81562802 0.77677281 0.8057141  0.81097682
 0.81185476 0.81468365 0.78891389 0.80264396 0.80895979 0.8099608
 0.81557728 0.76416282 0.80259416 0.80744728 0.81186024 0.81536186
 0.64887199 0.74968968 0.78558714 0.79435046 0.80327557 0.64283726
 0.74950972 0.77238934 0.79388282 0.79643327 0.61068597 0.76018264
 0.76206327 0.78000334 0.79027142 0.6041834  0.71175278 0.75331994
 0.77533202 0.78556701 0.67188815 0.77865979 0.79142284 0.79833501
 0.80049257 0.66873738 0.76910979 0.79282717 0.79921136 0.79956636
 0.63595502 0.76626561 0.77943754 0.78867289 0.79860768 0.63069542
 0.74617966 0.77486611 0.78654775 0.79542933]
for model  230 the mean error 0.7768387298315887
all id 230 hidden_dim 24 learning_rate 0.02 num_layers 4 frames 31 out win 5 err 0.7768387298315887
Launcher: Job 231 completed in 9025 seconds.
Launcher: Task 61 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  107025
Epoch:0, Train loss:0.586984, valid loss:0.582692
Epoch:1, Train loss:0.063089, valid loss:0.014216
Epoch:2, Train loss:0.018342, valid loss:0.008225
Epoch:3, Train loss:0.012949, valid loss:0.005697
Epoch:4, Train loss:0.010816, valid loss:0.005200
Epoch:5, Train loss:0.009196, valid loss:0.004781
Epoch:6, Train loss:0.008067, valid loss:0.004334
Epoch:7, Train loss:0.006965, valid loss:0.003913
Epoch:8, Train loss:0.006459, valid loss:0.003465
Epoch:9, Train loss:0.005824, valid loss:0.002907
Epoch:10, Train loss:0.005440, valid loss:0.002986
Epoch:11, Train loss:0.003969, valid loss:0.002460
Epoch:12, Train loss:0.003781, valid loss:0.002145
Epoch:13, Train loss:0.003567, valid loss:0.002230
Epoch:14, Train loss:0.003543, valid loss:0.002092
Epoch:15, Train loss:0.003536, valid loss:0.002069
Epoch:16, Train loss:0.003246, valid loss:0.001997
Epoch:17, Train loss:0.003292, valid loss:0.001834
Epoch:18, Train loss:0.003207, valid loss:0.001929
Epoch:19, Train loss:0.003017, valid loss:0.001664
Epoch:20, Train loss:0.003035, valid loss:0.001919
Epoch:21, Train loss:0.002311, valid loss:0.001529
Epoch:22, Train loss:0.002297, valid loss:0.001567
Epoch:23, Train loss:0.002236, valid loss:0.001540
Epoch:24, Train loss:0.002180, valid loss:0.001466
Epoch:25, Train loss:0.002247, valid loss:0.001496
Epoch:26, Train loss:0.002193, valid loss:0.001576
Epoch:27, Train loss:0.002073, valid loss:0.001435
Epoch:28, Train loss:0.002136, valid loss:0.001505
Epoch:29, Train loss:0.002043, valid loss:0.001441
Epoch:30, Train loss:0.002056, valid loss:0.001561
Epoch:31, Train loss:0.001718, valid loss:0.001271
Epoch:32, Train loss:0.001675, valid loss:0.001388
Epoch:33, Train loss:0.001676, valid loss:0.001246
Epoch:34, Train loss:0.001675, valid loss:0.001231
Epoch:35, Train loss:0.001670, valid loss:0.001200
Epoch:36, Train loss:0.001645, valid loss:0.001297
Epoch:37, Train loss:0.001602, valid loss:0.001425
Epoch:38, Train loss:0.001649, valid loss:0.001189
Epoch:39, Train loss:0.001630, valid loss:0.001414
Epoch:40, Train loss:0.001572, valid loss:0.001439
Epoch:41, Train loss:0.001424, valid loss:0.001155
Epoch:42, Train loss:0.001393, valid loss:0.001149
Epoch:43, Train loss:0.001393, valid loss:0.001218
Epoch:44, Train loss:0.001386, valid loss:0.001223
Epoch:45, Train loss:0.001382, valid loss:0.001175
Epoch:46, Train loss:0.001399, valid loss:0.001166
Epoch:47, Train loss:0.001373, valid loss:0.001184
Epoch:48, Train loss:0.001361, valid loss:0.001163
Epoch:49, Train loss:0.001391, valid loss:0.001138
Epoch:50, Train loss:0.001357, valid loss:0.001165
Epoch:51, Train loss:0.001279, valid loss:0.001110
Epoch:52, Train loss:0.001258, valid loss:0.001100
Epoch:53, Train loss:0.001266, valid loss:0.001100
Epoch:54, Train loss:0.001262, valid loss:0.001127
Epoch:55, Train loss:0.001252, valid loss:0.001145
Epoch:56, Train loss:0.001260, valid loss:0.001135
Epoch:57, Train loss:0.001247, valid loss:0.001156
Epoch:58, Train loss:0.001245, valid loss:0.001135
Epoch:59, Train loss:0.001244, valid loss:0.001118
Epoch:60, Train loss:0.001242, valid loss:0.001081
training time 8828.501428842545
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.04762917851515227
plot_id,batch_id 0 1 miss% 0.027073769229174715
plot_id,batch_id 0 2 miss% 0.02086365218817619
plot_id,batch_id 0 3 miss% 0.02892876675545904
plot_id,batch_id 0 4 miss% 0.02742437002293827
plot_id,batch_id 0 5 miss% 0.038924649706366325
plot_id,batch_id 0 6 miss% 0.032582023649800305
plot_id,batch_id 0 7 miss% 0.03069359290323364
plot_id,batch_id 0 8 miss% 0.03253378965564391
plot_id,batch_id 0 9 miss% 0.016939421565532506
plot_id,batch_id 0 10 miss% 0.037550438097843766
plot_id,batch_id 0 11 miss% 0.0435122588935241
plot_id,batch_id 0 12 miss% 0.03038812455210443
plot_id,batch_id 0 13 miss% 0.026274744143623966
plot_id,batch_id 0 14 miss% 0.033774826201217176
plot_id,batch_id 0 15 miss% 0.045347955007578114
plot_id,batch_id 0 16 miss% 0.028329882626024712
plot_id,batch_id 0 17 miss% 0.041785636927063136
plot_id,batch_id 0 18 miss% 0.0371452828582093
plot_id,batch_id 0 19 miss% 0.038230737112657696
plot_id,batch_id 0 20 miss% 0.03158048327048234
plot_id,batch_id 0 21 miss% 0.018498955399908437
plot_id,batch_id 0 22 miss% 0.026912360239902025
plot_id,batch_id 0 23 miss% 0.030940727882693336
plot_id,batch_id 0 24 miss% 0.03728911863654335
plot_id,batch_id 0 25 miss% 0.04548814391372768
plot_id,batch_id 0 26 miss% 0.016781002415894555
plot_id,batch_id 0 27 miss% 0.02236130364695124
plot_id,batch_id 0 28 miss% 0.021815248361309377
plot_id,batch_id 0 29 miss% 0.037358177697452415
plot_id,batch_id 0 30 miss% 0.03988336521294884
plot_id,batch_id 0 31 miss% 0.03532860883387024
plot_id,batch_id 0 32 miss% 0.03966888946480309
plot_id,batch_id 0 33 miss% 0.030051771565142923
plot_id,batch_id 0 34 miss% 0.026647859377099008
plot_id,batch_id 0 35 miss% 0.04353716356667046
plot_id,batch_id 0 36 miss% 0.055120072983439725
plot_id,batch_id 0 37 miss% 0.021273479680248005
plot_id,batch_id 0 38 miss% 0.036205335755020156
plot_id,batch_id 0 39 miss% 0.03256071697617904
plot_id,batch_id 0 40 miss% 0.047259768686501225
plot_id,batch_id 0 41 miss% 0.01696546380026278
plot_id,batch_id 0 42 miss% 0.018066717657183467
plot_id,batch_id 0 43 miss% 0.02916402322161792
plot_id,batch_id 0 44 miss% 0.026962676301864042
plot_id,batch_id 0 45 miss% 0.029868382384517847
plot_id,batch_id 0 46 miss% 0.016441101801809892
plot_id,batch_id 0 47 miss% 0.023533516757250746
plot_id,batch_id 0 48 miss% 0.017899450769178756
plot_id,batch_id 0 49 miss% 0.02337328443981242
plot_id,batch_id 0 50 miss% 0.045715112352455234
plot_id,batch_id 0 51 miss% 0.027314103305497408
plot_id,batch_id 0 52 miss% 0.018644429318772623
plot_id,batch_id 0 53 miss% 0.016917100119979687
plot_id,batch_id 0 54 miss% 0.02323594725309613
plot_id,batch_id 0 55 miss% 0.041761901600309355
plot_id,batch_id 0 56 miss% 0.029996180118799332
plot_id,batch_id 0 57 miss% 0.02274687594891569
plot_id,batch_id 0 58 miss% 0.02918528086832827
plot_id,batch_id 0 59 miss% 0.025390663820110033
plot_id,batch_id 0 60 miss% 0.057946501977817
plot_id,batch_id 0 61 miss% 0.023439192034537457
plot_id,batch_id 0 62 miss% 0.04623278845513906
plot_id,batch_id 0 63 miss% 0.034600442893004665
plot_id,batch_id 0 64 miss% 0.03002519590985081
plot_id,batch_id 0 65 miss% 0.05216290723367796
plot_id,batch_id 0 66 miss% 0.040368103831190175
plot_id,batch_id 0 67 miss% 0.02329916212590719
plot_id,batch_id 0 68 miss% 0.03298976585628773
plot_id,batch_id 0 69 miss% the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  107025
Epoch:0, Train loss:0.586984, valid loss:0.582692
Epoch:1, Train loss:0.050807, valid loss:0.009108
Epoch:2, Train loss:0.015095, valid loss:0.006254
Epoch:3, Train loss:0.011505, valid loss:0.005320
Epoch:4, Train loss:0.009739, valid loss:0.005442
Epoch:5, Train loss:0.008781, valid loss:0.004528
Epoch:6, Train loss:0.007535, valid loss:0.003552
Epoch:7, Train loss:0.006726, valid loss:0.003511
Epoch:8, Train loss:0.006858, valid loss:0.003663
Epoch:9, Train loss:0.005779, valid loss:0.003306
Epoch:10, Train loss:0.005878, valid loss:0.002801
Epoch:11, Train loss:0.003896, valid loss:0.002281
Epoch:12, Train loss:0.003834, valid loss:0.001916
Epoch:13, Train loss:0.003489, valid loss:0.002175
Epoch:14, Train loss:0.003757, valid loss:0.002551
Epoch:15, Train loss:0.003615, valid loss:0.001998
Epoch:16, Train loss:0.003486, valid loss:0.002154
Epoch:17, Train loss:0.003450, valid loss:0.001809
Epoch:18, Train loss:0.003222, valid loss:0.001693
Epoch:19, Train loss:0.003264, valid loss:0.001709
Epoch:20, Train loss:0.003114, valid loss:0.001747
Epoch:21, Train loss:0.002259, valid loss:0.001460
Epoch:22, Train loss:0.002244, valid loss:0.001500
Epoch:23, Train loss:0.002191, valid loss:0.001496
Epoch:24, Train loss:0.002159, valid loss:0.001620
Epoch:25, Train loss:0.002123, valid loss:0.001492
Epoch:26, Train loss:0.002225, valid loss:0.001589
Epoch:27, Train loss:0.002048, valid loss:0.001453
Epoch:28, Train loss:0.002073, valid loss:0.001721
Epoch:29, Train loss:0.002094, valid loss:0.001579
Epoch:30, Train loss:0.002060, valid loss:0.001504
Epoch:31, Train loss:0.001576, valid loss:0.001299
Epoch:32, Train loss:0.001550, valid loss:0.001345
Epoch:33, Train loss:0.001582, valid loss:0.001339
Epoch:34, Train loss:0.001568, valid loss:0.001218
Epoch:35, Train loss:0.001607, valid loss:0.001218
Epoch:36, Train loss:0.001517, valid loss:0.001275
Epoch:37, Train loss:0.001513, valid loss:0.001223
Epoch:38, Train loss:0.001534, valid loss:0.001241
Epoch:39, Train loss:0.001538, valid loss:0.001406
Epoch:40, Train loss:0.001504, valid loss:0.001477
Epoch:41, Train loss:0.001284, valid loss:0.001150
Epoch:42, Train loss:0.001265, valid loss:0.001196
Epoch:43, Train loss:0.001274, valid loss:0.001206
Epoch:44, Train loss:0.001251, valid loss:0.001134
Epoch:45, Train loss:0.001266, valid loss:0.001176
Epoch:46, Train loss:0.001270, valid loss:0.001177
Epoch:47, Train loss:0.001244, valid loss:0.001164
Epoch:48, Train loss:0.001236, valid loss:0.001109
Epoch:49, Train loss:0.001218, valid loss:0.001131
Epoch:50, Train loss:0.001231, valid loss:0.001203
Epoch:51, Train loss:0.001134, valid loss:0.001119
Epoch:52, Train loss:0.001126, valid loss:0.001098
Epoch:53, Train loss:0.001131, valid loss:0.001119
Epoch:54, Train loss:0.001142, valid loss:0.001084
Epoch:55, Train loss:0.001132, valid loss:0.001132
Epoch:56, Train loss:0.001115, valid loss:0.001112
Epoch:57, Train loss:0.001110, valid loss:0.001121
Epoch:58, Train loss:0.001109, valid loss:0.001102
Epoch:59, Train loss:0.001112, valid loss:0.001090
Epoch:60, Train loss:0.001109, valid loss:0.001110
training time 8836.395760297775
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03369248491623551
plot_id,batch_id 0 1 miss% 0.030469451150288495
plot_id,batch_id 0 2 miss% 0.022232343494842703
plot_id,batch_id 0 3 miss% 0.020282461605888008
plot_id,batch_id 0 4 miss% 0.022743379945140144
plot_id,batch_id 0 5 miss% 0.02820412202005531
plot_id,batch_id 0 6 miss% 0.03703225224565522
plot_id,batch_id 0 7 miss% 0.027184060838536736
plot_id,batch_id 0 8 miss% 0.02405602877416844
plot_id,batch_id 0 9 miss% 0.02829048339351592
plot_id,batch_id 0 10 miss% 0.039035736864003814
plot_id,batch_id 0 11 miss% 0.034368937927557595
plot_id,batch_id 0 12 miss% 0.023941424186063452
plot_id,batch_id 0 13 miss% 0.017327293264000353
plot_id,batch_id 0 14 miss% 0.02984527135568447
plot_id,batch_id 0 15 miss% 0.05577959893306528
plot_id,batch_id 0 16 miss% 0.027918605401842267
plot_id,batch_id 0 17 miss% 0.039466115144283734
plot_id,batch_id 0 18 miss% 0.03352621055378778
plot_id,batch_id 0 19 miss% 0.027213554811596777
plot_id,batch_id 0 20 miss% 0.03946049996706955
plot_id,batch_id 0 21 miss% 0.019817811405692384
plot_id,batch_id 0 22 miss% 0.033814095311062504
plot_id,batch_id 0 23 miss% 0.029361290633873438
plot_id,batch_id 0 24 miss% 0.025524672058187783
plot_id,batch_id 0 25 miss% 0.03963376649770294
plot_id,batch_id 0 26 miss% 0.01924414199967506
plot_id,batch_id 0 27 miss% 0.022669789332013968
plot_id,batch_id 0 28 miss% 0.02734735008109939
plot_id,batch_id 0 29 miss% 0.017398609420913605
plot_id,batch_id 0 30 miss% 0.03470718618842788
plot_id,batch_id 0 31 miss% 0.030395445637141357
plot_id,batch_id 0 32 miss% 0.026153889716852237
plot_id,batch_id 0 33 miss% 0.02985989111786908
plot_id,batch_id 0 34 miss% 0.03224428574065962
plot_id,batch_id 0 35 miss% 0.02853964257174954
plot_id,batch_id 0 36 miss% 0.03568649733059589
plot_id,batch_id 0 37 miss% 0.025441379565516932
plot_id,batch_id 0 38 miss% 0.030037379100011155
plot_id,batch_id 0 39 miss% 0.027903618354067922
plot_id,batch_id 0 40 miss% 0.04749769787475982
plot_id,batch_id 0 41 miss% 0.02241970802581579
plot_id,batch_id 0 42 miss% 0.009263762585452485
plot_id,batch_id 0 43 miss% 0.03626609562883298
plot_id,batch_id 0 44 miss% 0.02914086371489942
plot_id,batch_id 0 45 miss% 0.03926238156287725
plot_id,batch_id 0 46 miss% 0.027406047399914837
plot_id,batch_id 0 47 miss% 0.020941475714463875
plot_id,batch_id 0 48 miss% 0.026470115602794976
plot_id,batch_id 0 49 miss% 0.01983123645985913
plot_id,batch_id 0 50 miss% 0.025289360721719837
plot_id,batch_id 0 51 miss% 0.02815322599780652
plot_id,batch_id 0 52 miss% 0.0194949033491355
plot_id,batch_id 0 53 miss% 0.011364794213921973
plot_id,batch_id 0 54 miss% 0.027230831337343806
plot_id,batch_id 0 55 miss% 0.02261697691681347
plot_id,batch_id 0 56 miss% 0.02702668743475144
plot_id,batch_id 0 57 miss% 0.02385522570218416
plot_id,batch_id 0 58 miss% 0.020668716775494044
plot_id,batch_id 0 59 miss% 0.02764445506019447
plot_id,batch_id 0 60 miss% 0.03330814973728283
plot_id,batch_id 0 61 miss% 0.029331783590574072
plot_id,batch_id 0 62 miss% 0.031352659755123906
plot_id,batch_id 0 63 miss% 0.03095295057486556
plot_id,batch_id 0 64 miss% 0.03619143953460317
plot_id,batch_id 0 65 miss% 0.02276996177727701
plot_id,batch_id 0 66 miss% 0.057514424787321314
plot_id,batch_id 0 67 miss% 0.03226147673602723
plot_id,batch_id 0 68 miss% 0.03003520247646656
plot_id,batch_id 0 69 miss% 0.0279740181274778130.030218268692615482
plot_id,batch_id 0 70 miss% 0.024650943244012527
plot_id,batch_id 0 71 miss% 0.04763570962245578
plot_id,batch_id 0 72 miss% 0.027870314643614762
plot_id,batch_id 0 73 miss% 0.05585663886533339
plot_id,batch_id 0 74 miss% 0.036482225376680194
plot_id,batch_id 0 75 miss% 0.04130046786720592
plot_id,batch_id 0 76 miss% 0.04401704100095624
plot_id,batch_id 0 77 miss% 0.037969912054058974
plot_id,batch_id 0 78 miss% 0.03542932374187048
plot_id,batch_id 0 79 miss% 0.0488451362195336
plot_id,batch_id 0 80 miss% 0.03607982824834769
plot_id,batch_id 0 81 miss% 0.033108584087099546
plot_id,batch_id 0 82 miss% 0.02441952369800986
plot_id,batch_id 0 83 miss% 0.035239457545175996
plot_id,batch_id 0 84 miss% 0.029022139720037445
plot_id,batch_id 0 85 miss% 0.042243735588262124
plot_id,batch_id 0 86 miss% 0.024070855152996364
plot_id,batch_id 0 87 miss% 0.040689764570764564
plot_id,batch_id 0 88 miss% 0.03980672772686092
plot_id,batch_id 0 89 miss% 0.026589186819478014
plot_id,batch_id 0 90 miss% 0.04901280496043478
plot_id,batch_id 0 91 miss% 0.027585352478863182
plot_id,batch_id 0 92 miss% 0.03745053061136927
plot_id,batch_id 0 93 miss% 0.03148982212534498
plot_id,batch_id 0 94 miss% 0.04253523061981811
plot_id,batch_id 0 95 miss% 0.047907132873847624
plot_id,batch_id 0 96 miss% 0.03520309431012386
plot_id,batch_id 0 97 miss% 0.07068671043704314
plot_id,batch_id 0 98 miss% 0.04418410023267032
plot_id,batch_id 0 99 miss% 0.020154237041356493
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04762918 0.02707377 0.02086365 0.02892877 0.02742437 0.03892465
 0.03258202 0.03069359 0.03253379 0.01693942 0.03755044 0.04351226
 0.03038812 0.02627474 0.03377483 0.04534796 0.02832988 0.04178564
 0.03714528 0.03823074 0.03158048 0.01849896 0.02691236 0.03094073
 0.03728912 0.04548814 0.016781   0.0223613  0.02181525 0.03735818
 0.03988337 0.03532861 0.03966889 0.03005177 0.02664786 0.04353716
 0.05512007 0.02127348 0.03620534 0.03256072 0.04725977 0.01696546
 0.01806672 0.02916402 0.02696268 0.02986838 0.0164411  0.02353352
 0.01789945 0.02337328 0.04571511 0.0273141  0.01864443 0.0169171
 0.02323595 0.0417619  0.02999618 0.02274688 0.02918528 0.02539066
 0.0579465  0.02343919 0.04623279 0.03460044 0.0300252  0.05216291
 0.0403681  0.02329916 0.03298977 0.03021827 0.02465094 0.04763571
 0.02787031 0.05585664 0.03648223 0.04130047 0.04401704 0.03796991
 0.03542932 0.04884514 0.03607983 0.03310858 0.02441952 0.03523946
 0.02902214 0.04224374 0.02407086 0.04068976 0.03980673 0.02658919
 0.0490128  0.02758535 0.03745053 0.03148982 0.04253523 0.04790713
 0.03520309 0.07068671 0.0441841  0.02015424]
for model  17 the mean error 0.03354496726511526
all id 17 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 21 out win 5 err 0.03354496726511526
Launcher: Job 18 completed in 9046 seconds.
Launcher: Task 181 done. Exiting.

plot_id,batch_id 0 70 miss% 0.04557322736118463
plot_id,batch_id 0 71 miss% 0.04100066396859958
plot_id,batch_id 0 72 miss% 0.03278520359808129
plot_id,batch_id 0 73 miss% 0.023926030814687724
plot_id,batch_id 0 74 miss% 0.037445215148747914
plot_id,batch_id 0 75 miss% 0.05546669195327837
plot_id,batch_id 0 76 miss% 0.0278687834329663
plot_id,batch_id 0 77 miss% 0.020699123290361478
plot_id,batch_id 0 78 miss% 0.03368125565248297
plot_id,batch_id 0 79 miss% 0.04197076088443282
plot_id,batch_id 0 80 miss% 0.044957246217106214
plot_id,batch_id 0 81 miss% 0.023110186884860617
plot_id,batch_id 0 82 miss% 0.027351920197998026
plot_id,batch_id 0 83 miss% 0.028026912655223196
plot_id,batch_id 0 84 miss% 0.02374147325550919
plot_id,batch_id 0 85 miss% 0.043841470488782425
plot_id,batch_id 0 86 miss% 0.03183389867750179
plot_id,batch_id 0 87 miss% 0.029380331948501606
plot_id,batch_id 0 88 miss% 0.030164214868002175
plot_id,batch_id 0 89 miss% 0.032831637844643526
plot_id,batch_id 0 90 miss% 0.048717331038553455
plot_id,batch_id 0 91 miss% 0.03499066787570873
plot_id,batch_id 0 92 miss% 0.030503386198938897
plot_id,batch_id 0 93 miss% 0.03027258136671326
plot_id,batch_id 0 94 miss% 0.04606872819197381
plot_id,batch_id 0 95 miss% 0.03953114975580464
plot_id,batch_id 0 96 miss% 0.03664590016035093
plot_id,batch_id 0 97 miss% 0.04007410404791077
plot_id,batch_id 0 98 miss% 0.041439460197920816
plot_id,batch_id 0 99 miss% 0.03977697873727522
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03369248 0.03046945 0.02223234 0.02028246 0.02274338 0.02820412
 0.03703225 0.02718406 0.02405603 0.02829048 0.03903574 0.03436894
 0.02394142 0.01732729 0.02984527 0.0557796  0.02791861 0.03946612
 0.03352621 0.02721355 0.0394605  0.01981781 0.0338141  0.02936129
 0.02552467 0.03963377 0.01924414 0.02266979 0.02734735 0.01739861
 0.03470719 0.03039545 0.02615389 0.02985989 0.03224429 0.02853964
 0.0356865  0.02544138 0.03003738 0.02790362 0.0474977  0.02241971
 0.00926376 0.0362661  0.02914086 0.03926238 0.02740605 0.02094148
 0.02647012 0.01983124 0.02528936 0.02815323 0.0194949  0.01136479
 0.02723083 0.02261698 0.02702669 0.02385523 0.02066872 0.02764446
 0.03330815 0.02933178 0.03135266 0.03095295 0.03619144 0.02276996
 0.05751442 0.03226148 0.0300352  0.02797402 0.04557323 0.04100066
 0.0327852  0.02392603 0.03744522 0.05546669 0.02786878 0.02069912
 0.03368126 0.04197076 0.04495725 0.02311019 0.02735192 0.02802691
 0.02374147 0.04384147 0.0318339  0.02938033 0.03016421 0.03283164
 0.04871733 0.03499067 0.03050339 0.03027258 0.04606873 0.03953115
 0.0366459  0.0400741  0.04143946 0.03977698]
for model  44 the mean error 0.030850642247465518
all id 44 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 21 out win 5 err 0.030850642247465518
Launcher: Job 45 completed in 9053 seconds.
Launcher: Task 145 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  79249
Epoch:0, Train loss:0.298934, valid loss:0.276258
Epoch:1, Train loss:0.019635, valid loss:0.004418
Epoch:2, Train loss:0.006120, valid loss:0.003371
Epoch:3, Train loss:0.004662, valid loss:0.002286
Epoch:4, Train loss:0.003968, valid loss:0.002241
Epoch:5, Train loss:0.003510, valid loss:0.001629
Epoch:6, Train loss:0.003165, valid loss:0.001514
Epoch:7, Train loss:0.002893, valid loss:0.001375
Epoch:8, Train loss:0.002714, valid loss:0.001341
Epoch:9, Train loss:0.002545, valid loss:0.001239
Epoch:10, Train loss:0.002404, valid loss:0.001281
Epoch:11, Train loss:0.001802, valid loss:0.000971
Epoch:12, Train loss:0.001764, valid loss:0.001040
Epoch:13, Train loss:0.001733, valid loss:0.001000
Epoch:14, Train loss:0.001686, valid loss:0.001109
Epoch:15, Train loss:0.001658, valid loss:0.001100
Epoch:16, Train loss:0.001577, valid loss:0.001137
Epoch:17, Train loss:0.001632, valid loss:0.001001
Epoch:18, Train loss:0.001517, valid loss:0.000862
Epoch:19, Train loss:0.001535, valid loss:0.000892
Epoch:20, Train loss:0.001500, valid loss:0.000925
Epoch:21, Train loss:0.001185, valid loss:0.000922
Epoch:22, Train loss:0.001195, valid loss:0.000780
Epoch:23, Train loss:0.001176, valid loss:0.000857
Epoch:24, Train loss:0.001154, valid loss:0.000747
Epoch:25, Train loss:0.001156, valid loss:0.000795
Epoch:26, Train loss:0.001132, valid loss:0.000756
Epoch:27, Train loss:0.001162, valid loss:0.000758
Epoch:28, Train loss:0.001111, valid loss:0.000756
Epoch:29, Train loss:0.001103, valid loss:0.000744
Epoch:30, Train loss:0.001102, valid loss:0.000820
Epoch:31, Train loss:0.000934, valid loss:0.000680
Epoch:32, Train loss:0.000927, valid loss:0.000707
Epoch:33, Train loss:0.000933, valid loss:0.000675
Epoch:34, Train loss:0.000919, valid loss:0.000745
Epoch:35, Train loss:0.000941, valid loss:0.000700
Epoch:36, Train loss:0.000917, valid loss:0.000715
Epoch:37, Train loss:0.000924, valid loss:0.000730
Epoch:38, Train loss:0.000905, valid loss:0.000694
Epoch:39, Train loss:0.000908, valid loss:0.000667
Epoch:40, Train loss:0.000903, valid loss:0.000691
Epoch:41, Train loss:0.000813, valid loss:0.000636
Epoch:42, Train loss:0.000810, valid loss:0.000639
Epoch:43, Train loss:0.000813, valid loss:0.000647
Epoch:44, Train loss:0.000806, valid loss:0.000655
Epoch:45, Train loss:0.000807, valid loss:0.000652
Epoch:46, Train loss:0.000803, valid loss:0.000677
Epoch:47, Train loss:0.000808, valid loss:0.000650
Epoch:48, Train loss:0.000798, valid loss:0.000670
Epoch:49, Train loss:0.000795, valid loss:0.000657
Epoch:50, Train loss:0.000797, valid loss:0.000644
Epoch:51, Train loss:0.000760, valid loss:0.000648
Epoch:52, Train loss:0.000753, valid loss:0.000637
Epoch:53, Train loss:0.000752, valid loss:0.000625
Epoch:54, Train loss:0.000754, valid loss:0.000625
Epoch:55, Train loss:0.000748, valid loss:0.000638
Epoch:56, Train loss:0.000748, valid loss:0.000642
Epoch:57, Train loss:0.000749, valid loss:0.000633
Epoch:58, Train loss:0.000745, valid loss:0.000619
Epoch:59, Train loss:0.000741, valid loss:0.000641
Epoch:60, Train loss:0.000743, valid loss:0.000627
training time 8937.671095371246
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.031738515692263554
plot_id,batch_id 0 1 miss% 0.01524544392459113
plot_id,batch_id 0 2 miss% 0.02054705676499574
plot_id,batch_id 0 3 miss% 0.025021884557436074
plot_id,batch_id 0 4 miss% 0.02971648306484135
plot_id,batch_id 0 5 miss% 0.052836030109854014
plot_id,batch_id 0 6 miss% 0.02375143506482379
plot_id,batch_id 0 7 miss% 0.02563496464043373
plot_id,batch_id 0 8 miss% 0.025896933045677
plot_id,batch_id 0 9 miss% 0.030308314120540466
plot_id,batch_id 0 10 miss% 0.03670679670849163
plot_id,batch_id 0 11 miss% 0.034676307613935385
plot_id,batch_id 0 12 miss% 0.02462278242401133
plot_id,batch_id 0 13 miss% 0.02325970244784217
plot_id,batch_id 0 14 miss% 0.034732686607744576
plot_id,batch_id 0 15 miss% 0.043979744236300765
plot_id,batch_id 0 16 miss% 0.029526008805436037
plot_id,batch_id 0 17 miss% 0.038182014771732664
plot_id,batch_id 0 18 miss% 0.035373438013782094
plot_id,batch_id 0 19 miss% 0.043193334886202764
plot_id,batch_id 0 20 miss% 0.03185604264546347
plot_id,batch_id 0 21 miss% 0.01989640845930762
plot_id,batch_id 0 22 miss% 0.0242280618295945
plot_id,batch_id 0 23 miss% 0.02543273454687416
plot_id,batch_id 0 24 miss% 0.023217513611661227
plot_id,batch_id 0 25 miss% 0.027343997095682963
plot_id,batch_id 0 26 miss% 0.023917524796490123
plot_id,batch_id 0 27 miss% 0.029498695516291962
plot_id,batch_id 0 28 miss% 0.017773067344910413
plot_id,batch_id 0 29 miss% 0.03140024271721706
plot_id,batch_id 0 30 miss% 0.05594552459169467
plot_id,batch_id 0 31 miss% 0.03714943352673454
plot_id,batch_id 0 32 miss% 0.02710777503874189
plot_id,batch_id 0 33 miss% 0.026863534321971076
plot_id,batch_id 0 34 miss% 0.031140495910815087
plot_id,batch_id 0 35 miss% 0.04618730458923134
plot_id,batch_id 0 36 miss% 0.03389290487289092
plot_id,batch_id 0 37 miss% 0.031907452619791984
plot_id,batch_id 0 38 miss% 0.021264985289575503
plot_id,batch_id 0 39 miss% 0.022056567440604215
plot_id,batch_id 0 40 miss% 0.050457756736820236
plot_id,batch_id 0 41 miss% 0.02718772270066035
plot_id,batch_id 0 42 miss% 0.015622807928282207
plot_id,batch_id 0 43 miss% 0.030101599986053815
plot_id,batch_id 0 44 miss% 0.025839112423498872
plot_id,batch_id 0 45 miss% 0.023405172321112715
plot_id,batch_id 0 46 miss% 0.024697254260854302
plot_id,batch_id 0 47 miss% 0.02502879312397667
plot_id,batch_id 0 48 miss% 0.01832070138581261
plot_id,batch_id 0 49 miss% 0.012095342068588923
plot_id,batch_id 0 50 miss% 0.02952417768129937
plot_id,batch_id 0 51 miss% 0.018144695314548567
plot_id,batch_id 0 52 miss% 0.022977142829658192
plot_id,batch_id 0 53 miss% 0.015016226395150108
plot_id,batch_id 0 54 miss% 0.018898567438748914
plot_id,batch_id 0 55 miss% 0.032099143994128554
plot_id,batch_id 0 56 miss% 0.025639636636848005
plot_id,batch_id 0 57 miss% 0.026736528873741044
plot_id,batch_id 0 58 miss% 0.03090847715754728
plot_id,batch_id 0 59 miss% 0.024418994917501154
plot_id,batch_id 0 60 miss% 0.04036153240827684
plot_id,batch_id 0 61 miss% 0.02295318347231681
plot_id,batch_id 0 62 miss% 0.024544984410717993
plot_id,batch_id 0 63 miss% 0.03341675807429819
plot_id,batch_id 0 64 miss% 0.037397785424958065
plot_id,batch_id 0 65 miss% 0.0336772961619499
plot_id,batch_id 0 66 miss% 0.05247811717432665
plot_id,batch_id 0 67 miss% 0.030673431309552704
plot_id,batch_id 0 68 miss% 0.028043257933432892
plot_id,batch_id 0 69 miss% 0.025613478757555187
plot_id,batch_id 0 70 miss% 0.03910430905943647
plot_id,batch_id 0 71 miss% 0.05519890161603027
plot_id,batch_id 0 72 miss% 0.03805736287620323
plot_id,batch_id 0 73 miss% 0.021850277732116653
plot_id,batch_id 0 74 miss% 0.03419586380259099
plot_id,batch_id 0 75 miss% 0.03739686511644477
plot_id,batch_id 0 76 miss% 0.04820288624881154
plot_id,batch_id 0 77 miss% 0.03324984282583116
plot_id,batch_id 0 78 miss% 0.04139937851378914
plot_id,batch_id 0 79 miss% 0.04154858112956405
plot_id,batch_id 0 80 miss% 0.030628072774311545
plot_id,batch_id 0 81 miss% 0.0238634562948557
plot_id,batch_id 0 82 miss% 0.028863251554253826
plot_id,batch_id 0 83 miss% 0.028353758243875715
plot_id,batch_id 0 84 miss% 0.03092206153884839
plot_id,batch_id 0 85 miss% 0.04172032678982659
plot_id,batch_id 0 86 miss% 0.0314505527580721
plot_id,batch_id 0 87 miss% 0.026577288872600698
plot_id,batch_id 0 88 miss% 0.029380623404584482
plot_id,batch_id 0 89 miss% 0.0266320306390846
plot_id,batch_id 0 90 miss% 0.034639445829093236
plot_id,batch_id 0 91 miss% 0.027021697937640015
plot_id,batch_id 0 92 miss% 0.028424270571230266
plot_id,batch_id 0 93 miss% 0.030173167699030598
plot_id,batch_id 0 94 miss% 0.03676515161465302
plot_id,batch_id 0 95 miss% 0.04737803715395076
plot_id,batch_id 0 96 miss% 0.04173234416337585
plot_id,batch_id 0 97 miss% 0.047226194651478835
plot_id,batch_id 0 98 miss% 0.03645894244216731
plot_id,batch_id 0 99 miss% 0.03366107157330769
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03173852 0.01524544 0.02054706 0.02502188 0.02971648 0.05283603
 0.02375144 0.02563496 0.02589693 0.03030831 0.0367068  0.03467631
 0.02462278 0.0232597  0.03473269 0.04397974 0.02952601 0.03818201
 0.03537344 0.04319333 0.03185604 0.01989641 0.02422806 0.02543273
 0.02321751 0.027344   0.02391752 0.0294987  0.01777307 0.03140024
 0.05594552 0.03714943 0.02710778 0.02686353 0.0311405  0.0461873
 0.0338929  0.03190745 0.02126499 0.02205657 0.05045776 0.02718772
 0.01562281 0.0301016  0.02583911 0.02340517 0.02469725 0.02502879
 0.0183207  0.01209534 0.02952418 0.0181447  0.02297714 0.01501623
 0.01889857 0.03209914 0.02563964 0.02673653 0.03090848 0.02441899
 0.04036153 0.02295318 0.02454498 0.03341676 0.03739779 0.0336773
 0.05247812 0.03067343 0.02804326 0.02561348 0.03910431 0.0551989
 0.03805736 0.02185028 0.03419586 0.03739687 0.04820289 0.03324984
 0.04139938 0.04154858 0.03062807 0.02386346 0.02886325 0.02835376
 0.03092206 0.04172033 0.03145055 0.02657729 0.02938062 0.02663203
 0.03463945 0.0270217  0.02842427 0.03017317 0.03676515 0.04737804
 0.04173234 0.04722619 0.03645894 0.03366107]
for model  170 the mean error 0.030913858389957576
all id 170 hidden_dim 32 learning_rate 0.005 num_layers 3 frames 31 out win 5 err 0.030913858389957576
Launcher: Job 171 completed in 9133 seconds.
Launcher: Task 102 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  61841
Epoch:0, Train loss:0.439433, valid loss:0.435567
Epoch:1, Train loss:0.226345, valid loss:0.230351
Epoch:2, Train loss:0.219784, valid loss:0.229338
Epoch:3, Train loss:0.218898, valid loss:0.229096
Epoch:4, Train loss:0.218481, valid loss:0.229206
Epoch:5, Train loss:0.218207, valid loss:0.229105
Epoch:6, Train loss:0.218074, valid loss:0.229451
Epoch:7, Train loss:0.217956, valid loss:0.228931
Epoch:8, Train loss:0.217799, valid loss:0.228663
Epoch:9, Train loss:0.217750, valid loss:0.228829
Epoch:10, Train loss:0.217711, valid loss:0.229002
Epoch:11, Train loss:0.217050, valid loss:0.228535
Epoch:12, Train loss:0.217050, valid loss:0.228418
Epoch:13, Train loss:0.217035, valid loss:0.228499
Epoch:14, Train loss:0.217044, valid loss:0.228581
Epoch:15, Train loss:0.216964, valid loss:0.228400
Epoch:16, Train loss:0.216948, valid loss:0.228673
Epoch:17, Train loss:0.216928, valid loss:0.228542
Epoch:18, Train loss:0.216916, valid loss:0.228404
Epoch:19, Train loss:0.216914, valid loss:0.228542
Epoch:20, Train loss:0.216876, valid loss:0.228491
Epoch:21, Train loss:0.216575, valid loss:0.228329
Epoch:22, Train loss:0.216587, valid loss:0.228431
Epoch:23, Train loss:0.216567, valid loss:0.228338
Epoch:24, Train loss:0.216573, valid loss:0.228316
Epoch:25, Train loss:0.216562, valid loss:0.228490
Epoch:26, Train loss:0.216546, valid loss:0.228283
Epoch:27, Train loss:0.216561, valid loss:0.228245
Epoch:28, Train loss:0.216544, valid loss:0.228391
Epoch:29, Train loss:0.216504, valid loss:0.228284
Epoch:30, Train loss:0.216507, valid loss:0.228363
Epoch:31, Train loss:0.216381, valid loss:0.228228
Epoch:32, Train loss:0.216362, valid loss:0.228314
Epoch:33, Train loss:0.216357, valid loss:0.228210
Epoch:34, Train loss:0.216361, valid loss:0.228284
Epoch:35, Train loss:0.216355, valid loss:0.228198
Epoch:36, Train loss:0.216351, valid loss:0.228280
Epoch:37, Train loss:0.216345, valid loss:0.228251
Epoch:38, Train loss:0.216338, valid loss:0.228209
Epoch:39, Train loss:0.216349, valid loss:0.228219
Epoch:40, Train loss:0.216324, valid loss:0.228259
Epoch:41, Train loss:0.216248, valid loss:0.228161
Epoch:42, Train loss:0.216245, valid loss:0.228166
Epoch:43, Train loss:0.216259, valid loss:0.228169
Epoch:44, Train loss:0.216242, valid loss:0.228228
Epoch:45, Train loss:0.216251, valid loss:0.228171
Epoch:46, Train loss:0.216242, valid loss:0.228179
Epoch:47, Train loss:0.216245, valid loss:0.228194
Epoch:48, Train loss:0.216234, valid loss:0.228159
Epoch:49, Train loss:0.216238, valid loss:0.228193
Epoch:50, Train loss:0.216237, valid loss:0.228216
Epoch:51, Train loss:0.216191, valid loss:0.228168
Epoch:52, Train loss:0.216193, valid loss:0.228146
Epoch:53, Train loss:0.216193, valid loss:0.228164
Epoch:54, Train loss:0.216187, valid loss:0.228153
Epoch:55, Train loss:0.216191, valid loss:0.228157
Epoch:56, Train loss:0.216185, valid loss:0.228159
Epoch:57, Train loss:0.216186, valid loss:0.228147
Epoch:58, Train loss:0.216186, valid loss:0.228158
Epoch:59, Train loss:0.216180, valid loss:0.228171
Epoch:60, Train loss:0.216184, valid loss:0.228156
training time 8958.639474868774
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.6557531814043996
plot_id,batch_id 0 1 miss% 0.7458258555002405
plot_id,batch_id 0 2 miss% 0.7592505006732245
plot_id,batch_id 0 3 miss% 0.7683333940980627
plot_id,batch_id 0 4 miss% 0.7677368018882709
plot_id,batch_id 0 5 miss% 0.6641306488099359
plot_id,batch_id 0 6 miss% 0.7465831704020699
plot_id,batch_id 0 7 miss% 0.7566198709686338
plot_id,batch_id 0 8 miss% 0.767285663702869
plot_id,batch_id 0 9 miss% 0.7702042877684286
plot_id,batch_id 0 10 miss% 0.6271836642570482
plot_id,batch_id 0 11 miss% 0.7454476002637423
plot_id,batch_id 0 12 miss% 0.7556108936178677
plot_id,batch_id 0 13 miss% 0.7621040400679636
plot_id,batch_id 0 14 miss% 0.7691653029324298
plot_id,batch_id 0 15 miss% 0.6505212426828233
plot_id,batch_id 0 16 miss% 0.7366093793233808
plot_id,batch_id 0 17 miss% 0.765470565390788
plot_id,batch_id 0 18 miss% 0.7635318959301168
plot_id,batch_id 0 19 miss% 0.7667542076883409
plot_id,batch_id 0 20 miss% 0.7093089186685396
plot_id,batch_id 0 21 miss% 0.7618978055646445
plot_id,batch_id 0 22 miss% 0.7687952386773514
plot_id,batch_id 0 23 miss% 0.7765792022864538
plot_id,batch_id 0 24 miss% 0.7782867209375669
plot_id,batch_id 0 25 miss% 0.6948792804602071
plot_id,batch_id 0 26 miss% 0.7566156722220343
plot_id,batch_id 0 27 miss% 0.7692805746225392
plot_id,batch_id 0 28 miss% 0.767383688192717
plot_id,batch_id 0 29 miss% 0.7776799469812371
plot_id,batch_id 0 30 miss% 0.7081044594824785
plot_id,batch_id 0 31 miss% 0.757962015006969
plot_id,batch_id 0 32 miss% 0.7649460084028624
plot_id,batch_id 0 33 miss% 0.7687510020537323
plot_id,batch_id 0 34 miss% 0.7693455024593744
plot_id,batch_id 0 35 miss% 0.6918778467716055
plot_id,batch_id 0 36 miss% 0.7589854258370622
plot_id,batch_id 0 37 miss% 0.7628929912616462
plot_id,batch_id 0 38 miss% 0.7703834357231216
plot_id,batch_id 0 39 miss% 0.7722792312223905
plot_id,batch_id 0 40 miss% 0.7395072652841742
plot_id,batch_id 0 41 miss% 0.766862273974931
plot_id,batch_id 0 42 miss% 0.7691444237052666
plot_id,batch_id 0 43 miss% 0.7760429622079126
plot_id,batch_id 0 44 miss% 0.7818916654189447
plot_id,batch_id 0 45 miss% 0.734224560685587
plot_id,batch_id 0 46 miss% 0.7669067739043217
plot_id,batch_id 0 47 miss% 0.7704811682560743
plot_id,batch_id 0 48 miss% 0.776124842844356
plot_id,batch_id 0 49 miss% 0.7809625634114151
plot_id,batch_id 0 50 miss% 0.7404509515038896
plot_id,batch_id 0 51 miss% 0.7636531485860009
plot_id,batch_id 0 52 miss% 0.7704687320471941
plot_id,batch_id 0 53 miss% 0.7749653454199432
plot_id,batch_id 0 54 miss% 0.7839066316012868
plot_id,batch_id 0 55 miss% 0.7442281477076366
plot_id,batch_id 0 56 miss% 0.7638730718712571
plot_id,batch_id 0 57 miss% 0.7701195109079585
plot_id,batch_id 0 58 miss% 0.7762789741004211
plot_id,batch_id 0 59 miss% 0.7791447562948024
plot_id,batch_id 0 60 miss% 0.568525673197777
plot_id,batch_id 0 61 miss% 0.7078381634219434
plot_id,batch_id 0 62 miss% 0.7340910581149386
plot_id,batch_id 0 63 miss% 0.7530392380893052
plot_id,batch_id 0 64 miss% 0.7580709272048373
plot_id,batch_id 0 65 miss% 0.5621137541293555
plot_id,batch_id 0 66 miss% 0.6999802083523532
plot_id,batch_id 0 67 miss% 0.721879046655854
plot_id,batch_id 0 68 miss% 0.7486341397740317
plot_id,batch_id 0 69 miss% 0.7504320859834643
plot_id,batch_id 0 70 miss% 0.52587249557373
plot_id,batch_id 0 71 miss% 0.6983707534695696
plot_id,batch_id 0 72 miss% 0.712750414818015
plot_id,batch_id 0 73 miss% 0.7395420487794558
plot_id,batch_id 0 74 miss% 0.747289354359142
plot_id,batch_id 0 75 miss% 0.5220652883339514
plot_id,batch_id 0 76 miss% 0.6481811651958026
plot_id,batch_id 0 77 miss% 0.6995876187098424
plot_id,batch_id 0 78 miss% 0.7309118068241908
plot_id,batch_id 0 79 miss% 0.7434806934340322
plot_id,batch_id 0 80 miss% 0.5924210846810862
plot_id,batch_id 0 81 miss% 0.7234334543888571
plot_id,batch_id 0 82 miss% 0.7443538467684253
plot_id,batch_id 0 83 miss% 0.7570122296089997
plot_id,batch_id 0 84 miss% 0.7620373799655489
plot_id,batch_id 0 85 miss% 0.5912893739432848
plot_id,batch_id 0 86 miss% 0.7169726921211844
plot_id,batch_id 0 87 miss% 0.7404631899507331
plot_id,batch_id 0 88 miss% 0.7595435753133799
plot_id,batch_id 0 89 miss% 0.76321990411979
plot_id,batch_id 0 90 miss% 0.551733179741111
plot_id,batch_id 0 91 miss% 0.7154565432795212
plot_id,batch_id 0 92 miss% 0.7330125471000543
plot_id,batch_id 0 93 miss% 0.7448185966564074
plot_id,batch_id 0 94 miss% 0.7628159499768474
plot_id,batch_id 0 95 miss% 0.5574882249691362
plot_id,batch_id 0 96 miss% 0.702728979835844
plot_id,batch_id 0 97 miss% 0.7276200048376044
plot_id,batch_id 0 98 miss% 0.7421755502526421
plot_id,batch_id 0 99 miss% 0.7531305995125448
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.65575318 0.74582586 0.7592505  0.76833339 0.7677368  0.66413065
 0.74658317 0.75661987 0.76728566 0.77020429 0.62718366 0.7454476
 0.75561089 0.76210404 0.7691653  0.65052124 0.73660938 0.76547057
 0.7635319  0.76675421 0.70930892 0.76189781 0.76879524 0.7765792
 0.77828672 0.69487928 0.75661567 0.76928057 0.76738369 0.77767995
 0.70810446 0.75796202 0.76494601 0.768751   0.7693455  0.69187785
 0.75898543 0.76289299 0.77038344 0.77227923 0.73950727 0.76686227
 0.76914442 0.77604296 0.78189167 0.73422456 0.76690677 0.77048117
 0.77612484 0.78096256 0.74045095 0.76365315 0.77046873 0.77496535
 0.78390663 0.74422815 0.76387307 0.77011951 0.77627897 0.77914476
 0.56852567 0.70783816 0.73409106 0.75303924 0.75807093 0.56211375
 0.69998021 0.72187905 0.74863414 0.75043209 0.5258725  0.69837075
 0.71275041 0.73954205 0.74728935 0.52206529 0.64818117 0.69958762
 0.73091181 0.74348069 0.59242108 0.72343345 0.74435385 0.75701223
 0.76203738 0.59128937 0.71697269 0.74046319 0.75954358 0.7632199
 0.55173318 0.71545654 0.73301255 0.7448186  0.76281595 0.55748822
 0.70272898 0.72762    0.74217555 0.7531306 ]
for model  202 the mean error 0.7307397774538313
all id 202 hidden_dim 24 learning_rate 0.01 num_layers 4 frames 31 out win 4 err 0.7307397774538313
Launcher: Job 203 completed in 9134 seconds.
Launcher: Task 136 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  61841
Epoch:0, Train loss:0.439433, valid loss:0.435567
Epoch:1, Train loss:0.086318, valid loss:0.005792
Epoch:2, Train loss:0.013061, valid loss:0.005507
Epoch:3, Train loss:0.009676, valid loss:0.002427
Epoch:4, Train loss:0.004108, valid loss:0.001743
Epoch:5, Train loss:0.003500, valid loss:0.001647
Epoch:6, Train loss:0.003048, valid loss:0.001692
Epoch:7, Train loss:0.002796, valid loss:0.001484
Epoch:8, Train loss:0.002621, valid loss:0.001305
Epoch:9, Train loss:0.002449, valid loss:0.001226
Epoch:10, Train loss:0.002286, valid loss:0.001526
Epoch:11, Train loss:0.001733, valid loss:0.000976
Epoch:12, Train loss:0.001679, valid loss:0.001187
Epoch:13, Train loss:0.001656, valid loss:0.001020
Epoch:14, Train loss:0.001620, valid loss:0.001082
Epoch:15, Train loss:0.001555, valid loss:0.000947
Epoch:16, Train loss:0.001529, valid loss:0.000952
Epoch:17, Train loss:0.001522, valid loss:0.000948
Epoch:18, Train loss:0.001499, valid loss:0.000993
Epoch:19, Train loss:0.001469, valid loss:0.000874
Epoch:20, Train loss:0.001428, valid loss:0.000817
Epoch:21, Train loss:0.001143, valid loss:0.000951
Epoch:22, Train loss:0.001135, valid loss:0.000777
Epoch:23, Train loss:0.001118, valid loss:0.000732
Epoch:24, Train loss:0.001122, valid loss:0.000943
Epoch:25, Train loss:0.001111, valid loss:0.000849
Epoch:26, Train loss:0.001105, valid loss:0.000725
Epoch:27, Train loss:0.001084, valid loss:0.000767
Epoch:28, Train loss:0.001078, valid loss:0.000730
Epoch:29, Train loss:0.001061, valid loss:0.000752
Epoch:30, Train loss:0.001047, valid loss:0.000720
Epoch:31, Train loss:0.000916, valid loss:0.000669
Epoch:32, Train loss:0.000892, valid loss:0.000681
Epoch:33, Train loss:0.000892, valid loss:0.000659
Epoch:34, Train loss:0.000899, valid loss:0.000731
Epoch:35, Train loss:0.000889, valid loss:0.000632
Epoch:36, Train loss:0.000892, valid loss:0.000657
Epoch:37, Train loss:0.000875, valid loss:0.000673
Epoch:38, Train loss:0.000864, valid loss:0.000649
Epoch:39, Train loss:0.000875, valid loss:0.000661
Epoch:40, Train loss:0.000864, valid loss:0.000652
Epoch:41, Train loss:0.000790, valid loss:0.000629
Epoch:42, Train loss:0.000785, valid loss:0.000617
Epoch:43, Train loss:0.000782, valid loss:0.000625
Epoch:44, Train loss:0.000780, valid loss:0.000618
Epoch:45, Train loss:0.000784, valid loss:0.000620
Epoch:46, Train loss:0.000775, valid loss:0.000602
Epoch:47, Train loss:0.000774, valid loss:0.000604
Epoch:48, Train loss:0.000772, valid loss:0.000601
Epoch:49, Train loss:0.000768, valid loss:0.000632
Epoch:50, Train loss:0.000768, valid loss:0.000620
Epoch:51, Train loss:0.000734, valid loss:0.000602
Epoch:52, Train loss:0.000729, valid loss:0.000594
Epoch:53, Train loss:0.000728, valid loss:0.000598
Epoch:54, Train loss:0.000727, valid loss:0.000612
Epoch:55, Train loss:0.000724, valid loss:0.000607
Epoch:56, Train loss:0.000720, valid loss:0.000606
Epoch:57, Train loss:0.000725, valid loss:0.000625
Epoch:58, Train loss:0.000723, valid loss:0.000581
Epoch:59, Train loss:0.000721, valid loss:0.000602
Epoch:60, Train loss:0.000719, valid loss:0.000591
training time 8962.957636833191
total number of trained parameters for initialize model 61841
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.044990684211389756
plot_id,batch_id 0 1 miss% 0.039984515745118875
plot_id,batch_id 0 2 miss% 0.03236385188053336
plot_id,batch_id 0 3 miss% 0.02520245679863386
plot_id,batch_id 0 4 miss% 0.03964100628405799
plot_id,batch_id 0 5 miss% 0.026170961275098565
plot_id,batch_id 0 6 miss% 0.0355825184662337
plot_id,batch_id 0 7 miss% 0.03065175102937915
plot_id,batch_id 0 8 miss% 0.034140147185820036
plot_id,batch_id 0 9 miss% 0.028163188733775362
plot_id,batch_id 0 10 miss% 0.02769297900964123
plot_id,batch_id 0 11 miss% 0.04018680814741492
plot_id,batch_id 0 12 miss% 0.029542651434206695
plot_id,batch_id 0 13 miss% 0.023380062171262934
plot_id,batch_id 0 14 miss% 0.028780649179308795
plot_id,batch_id 0 15 miss% 0.0345456695646219
plot_id,batch_id 0 16 miss% 0.035814586297649297
plot_id,batch_id 0 17 miss% 0.03412122089529495
plot_id,batch_id 0 18 miss% 0.030337947829201177
plot_id,batch_id 0 19 miss% 0.03236614523849171
plot_id,batch_id 0 20 miss% 0.034496763655305566
plot_id,batch_id 0 21 miss% 0.02645330807412246
plot_id,batch_id 0 22 miss% 0.03957117895007211
plot_id,batch_id 0 23 miss% 0.027789067162575273
plot_id,batch_id 0 24 miss% 0.025335694312437432
plot_id,batch_id 0 25 miss% 0.03861477727652363
plot_id,batch_id 0 26 miss% 0.03298334120595257
plot_id,batch_id 0 27 miss% 0.03677839210431604
plot_id,batch_id 0 28 miss% 0.029762799185130754
plot_id,batch_id 0 29 miss% 0.024882310737970648
plot_id,batch_id 0 30 miss% 0.04313200781415875
plot_id,batch_id 0 31 miss% 0.03385294203778623
plot_id,batch_id 0 32 miss% 0.03078894913394058
plot_id,batch_id 0 33 miss% 0.026050014440104446
plot_id,batch_id 0 34 miss% 0.020441405573629574
plot_id,batch_id 0 35 miss% 0.028673373190812954
plot_id,batch_id 0 36 miss% 0.03587263555949022
plot_id,batch_id 0 37 miss% 0.028377911828738486
plot_id,batch_id 0 38 miss% 0.026059130543022398
plot_id,batch_id 0 39 miss% 0.027629027456033
plot_id,batch_id 0 40 miss% 0.0720335601395433
plot_id,batch_id 0 41 miss% 0.021102804444051163
plot_id,batch_id 0 42 miss% 0.0207691464296047
plot_id,batch_id 0 43 miss% 0.0302406576957588
plot_id,batch_id 0 44 miss% 0.019099075519578956
plot_id,batch_id 0 45 miss% 0.042795816203426804
plot_id,batch_id 0 46 miss% 0.029059279203504273
plot_id,batch_id 0 47 miss% 0.02282948642464054
plot_id,batch_id 0 48 miss% 0.019037898641195726
plot_id,batch_id 0 49 miss% 0.01845485055463967
plot_id,batch_id 0 50 miss% 0.029011074755977247
plot_id,batch_id 0 51 miss% 0.03206732813445947
plot_id,batch_id 0 52 miss% 0.02901890246821327
plot_id,batch_id 0 53 miss% 0.011790421796575723
plot_id,batch_id 0 54 miss% 0.017392605905451754
plot_id,batch_id 0 55 miss% 0.024301517917787945
plot_id,batch_id 0 56 miss% 0.02619004003255073
plot_id,batch_id 0 57 miss% 0.018928581345530184
plot_id,batch_id 0 58 miss% 0.026111590666767496
plot_id,batch_id 0 59 miss% 0.017368877634947974
plot_id,batch_id 0 60 miss% 0.04120488816950628
plot_id,batch_id 0 61 miss% 0.03054180643963383
plot_id,batch_id 0 62 miss% 0.018441371035155918
plot_id,batch_id 0 63 miss% 0.030479270104962482
plot_id,batch_id 0 64 miss% 0.03083731071751904
plot_id,batch_id 0 65 miss% 0.028587907217025966
plot_id,batch_id 0 66 miss% 0.027607318019406017
plot_id,batch_id 0 67 miss% 0.03622820259415475
plot_id,batch_id 0 68 miss% 0.02823323803729404
plot_id,batch_id 0 69 miss% 0.027437114108658376
plot_id,batch_id 0 70 miss% 0.02922953262795758
plot_id,batch_id 0 71 miss% 0.040216222964834875
plot_id,batch_id 0 72 miss% 0.04363789993697045
plot_id,batch_id 0 73 miss% 0.032711868475194604
plot_id,batch_id 0 74 miss% 0.03449481327551055
plot_id,batch_id 0 75 miss% 0.035568071151638486
plot_id,batch_id 0 76 miss% 0.06356353110933655
plot_id,batch_id 0 77 miss% 0.03098506309804926
plot_id,batch_id 0 78 miss% 0.041822609550584706
plot_id,batch_id 0 79 miss% 0.04566384307392384
plot_id,batch_id 0 80 miss% 0.023460956624695526
plot_id,batch_id 0 81 miss% 0.023288683874892715
plot_id,batch_id 0 82 miss% 0.03122681806159007
plot_id,batch_id 0 83 miss% 0.023945765962452504
plot_id,batch_id 0 84 miss% 0.023435639204179872
plot_id,batch_id 0 85 miss% 0.05620976296939519
plot_id,batch_id 0 86 miss% 0.023357711173907683
plot_id,batch_id 0 87 miss% 0.033433461703047146
plot_id,batch_id 0 88 miss% 0.021458263832362868
plot_id,batch_id 0 89 miss% 0.026816296099782717
plot_id,batch_id 0 90 miss% 0.03150444399590504
plot_id,batch_id 0 91 miss% 0.04059074612547298
plot_id,batch_id 0 92 miss% 0.027324522967813274
plot_id,batch_id 0 93 miss% 0.02156506555097464
plot_id,batch_id 0 94 miss% 0.04507372166666007
plot_id,batch_id 0 95 miss% 0.053241371355461455
plot_id,batch_id 0 96 miss% 0.04200141453865108
plot_id,batch_id 0 97 miss% 0.057752861939888954
plot_id,batch_id 0 98 miss% 0.026392536476625098
plot_id,batch_id 0 99 miss% 0.031057827842274847
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04499068 0.03998452 0.03236385 0.02520246 0.03964101 0.02617096
 0.03558252 0.03065175 0.03414015 0.02816319 0.02769298 0.04018681
 0.02954265 0.02338006 0.02878065 0.03454567 0.03581459 0.03412122
 0.03033795 0.03236615 0.03449676 0.02645331 0.03957118 0.02778907
 0.02533569 0.03861478 0.03298334 0.03677839 0.0297628  0.02488231
 0.04313201 0.03385294 0.03078895 0.02605001 0.02044141 0.02867337
 0.03587264 0.02837791 0.02605913 0.02762903 0.07203356 0.0211028
 0.02076915 0.03024066 0.01909908 0.04279582 0.02905928 0.02282949
 0.0190379  0.01845485 0.02901107 0.03206733 0.0290189  0.01179042
 0.01739261 0.02430152 0.02619004 0.01892858 0.02611159 0.01736888
 0.04120489 0.03054181 0.01844137 0.03047927 0.03083731 0.02858791
 0.02760732 0.0362282  0.02823324 0.02743711 0.02922953 0.04021622
 0.0436379  0.03271187 0.03449481 0.03556807 0.06356353 0.03098506
 0.04182261 0.04566384 0.02346096 0.02328868 0.03122682 0.02394577
 0.02343564 0.05620976 0.02335771 0.03343346 0.02145826 0.0268163
 0.03150444 0.04059075 0.02732452 0.02156507 0.04507372 0.05324137
 0.04200141 0.05775286 0.02639254 0.03105783]
for model  175 the mean error 0.03159438101206814
all id 175 hidden_dim 24 learning_rate 0.005 num_layers 4 frames 31 out win 4 err 0.03159438101206814
Launcher: Job 176 completed in 9156 seconds.
Launcher: Task 134 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  107025
Epoch:0, Train loss:0.586984, valid loss:0.582692
Epoch:1, Train loss:0.057726, valid loss:0.010474
Epoch:2, Train loss:0.017849, valid loss:0.006787
Epoch:3, Train loss:0.012988, valid loss:0.007562
Epoch:4, Train loss:0.010476, valid loss:0.004493
Epoch:5, Train loss:0.008886, valid loss:0.004315
Epoch:6, Train loss:0.008052, valid loss:0.005281
Epoch:7, Train loss:0.007104, valid loss:0.004382
Epoch:8, Train loss:0.007223, valid loss:0.003263
Epoch:9, Train loss:0.006495, valid loss:0.004212
Epoch:10, Train loss:0.006021, valid loss:0.003311
Epoch:11, Train loss:0.004097, valid loss:0.002349
Epoch:12, Train loss:0.004028, valid loss:0.002417
Epoch:13, Train loss:0.004070, valid loss:0.003052
Epoch:14, Train loss:0.003912, valid loss:0.002499
Epoch:15, Train loss:0.004012, valid loss:0.002709
Epoch:16, Train loss:0.003830, valid loss:0.002298
Epoch:17, Train loss:0.003572, valid loss:0.001988
Epoch:18, Train loss:0.003671, valid loss:0.001878
Epoch:19, Train loss:0.003380, valid loss:0.002207
Epoch:20, Train loss:0.003698, valid loss:0.001735
Epoch:21, Train loss:0.002390, valid loss:0.001477
Epoch:22, Train loss:0.002394, valid loss:0.001573
Epoch:23, Train loss:0.002359, valid loss:0.001352
Epoch:24, Train loss:0.002293, valid loss:0.001938
Epoch:25, Train loss:0.002232, valid loss:0.001690
Epoch:26, Train loss:0.002368, valid loss:0.001631
Epoch:27, Train loss:0.002222, valid loss:0.001649
Epoch:28, Train loss:0.002264, valid loss:0.001558
Epoch:29, Train loss:0.002264, valid loss:0.002145
Epoch:30, Train loss:0.002186, valid loss:0.002052
Epoch:31, Train loss:0.001678, valid loss:0.001207
Epoch:32, Train loss:0.001580, valid loss:0.001244
Epoch:33, Train loss:0.001642, valid loss:0.001134
Epoch:34, Train loss:0.001574, valid loss:0.001188
Epoch:35, Train loss:0.001666, valid loss:0.001200
Epoch:36, Train loss:0.001605, valid loss:0.001258
Epoch:37, Train loss:0.001576, valid loss:0.001245
Epoch:38, Train loss:0.001535, valid loss:0.001083
Epoch:39, Train loss:0.001637, valid loss:0.001285
Epoch:40, Train loss:0.001528, valid loss:0.001242
Epoch:41, Train loss:0.001273, valid loss:0.001067
Epoch:42, Train loss:0.001263, valid loss:0.001029
Epoch:43, Train loss:0.001270, valid loss:0.001065
Epoch:44, Train loss:0.001268, valid loss:0.001109
Epoch:45, Train loss:0.001239, valid loss:0.001055
Epoch:46, Train loss:0.001241, valid loss:0.001099
Epoch:47, Train loss:0.001264, valid loss:0.001083
Epoch:48, Train loss:0.001232, valid loss:0.000996
Epoch:49, Train loss:0.001214, valid loss:0.001078
Epoch:50, Train loss:0.001224, valid loss:0.001054
Epoch:51, Train loss:0.001095, valid loss:0.001000
Epoch:52, Train loss:0.001094, valid loss:0.000991
Epoch:53, Train loss:0.001085, valid loss:0.001022
Epoch:54, Train loss:0.001086, valid loss:0.001003
Epoch:55, Train loss:0.001091, valid loss:0.001009
Epoch:56, Train loss:0.001075, valid loss:0.000961
Epoch:57, Train loss:0.001074, valid loss:0.001023
Epoch:58, Train loss:0.001066, valid loss:0.000975
Epoch:59, Train loss:0.001066, valid loss:0.001085
Epoch:60, Train loss:0.001062, valid loss:0.000992
training time 8959.80346441269
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.03541005234442713
plot_id,batch_id 0 1 miss% 0.03985355458090526
plot_id,batch_id 0 2 miss% 0.03574893255399994
plot_id,batch_id 0 3 miss% 0.03355796539430506
plot_id,batch_id 0 4 miss% 0.028830314955104238
plot_id,batch_id 0 5 miss% 0.037430682525088
plot_id,batch_id 0 6 miss% 0.034617781605725706
plot_id,batch_id 0 7 miss% 0.03678468557983423
plot_id,batch_id 0 8 miss% 0.036874835020863236
plot_id,batch_id 0 9 miss% 0.03276484933022367
plot_id,batch_id 0 10 miss% 0.03398339021258571
plot_id,batch_id 0 11 miss% 0.047078926800129456
plot_id,batch_id 0 12 miss% 0.046688291209841824
plot_id,batch_id 0 13 miss% 0.05006919123640232
plot_id,batch_id 0 14 miss% 0.029576440040835893
plot_id,batch_id 0 15 miss% 0.05095128135025113
plot_id,batch_id 0 16 miss% 0.027999033207970352
plot_id,batch_id 0 17 miss% 0.03702459507705796
plot_id,batch_id 0 18 miss% 0.04910116335538141
plot_id,batch_id 0 19 miss% 0.02522111961107499
plot_id,batch_id 0 20 miss% 0.0362801577342739
plot_id,batch_id 0 21 miss% 0.03314993380583163
plot_id,batch_id 0 22 miss% 0.027667327093210143
plot_id,batch_id 0 23 miss% 0.027905142178350058
plot_id,batch_id 0 24 miss% 0.024666206614478953
plot_id,batch_id 0 25 miss% 0.03314817209881916
plot_id,batch_id 0 26 miss% 0.0209398023355446
plot_id,batch_id 0 27 miss% 0.02458611322404726
plot_id,batch_id 0 28 miss% 0.020627190678939117
plot_id,batch_id 0 29 miss% 0.02687873178117514
plot_id,batch_id 0 30 miss% 0.042046462384214736
plot_id,batch_id 0 31 miss% 0.03670378237630286
plot_id,batch_id 0 32 miss% 0.038961884870495025
plot_id,batch_id 0 33 miss% 0.03514837919321967
plot_id,batch_id 0 34 miss% 0.024112761187787355
plot_id,batch_id 0 35 miss% 0.04382840316996096
plot_id,batch_id 0 36 miss% 0.043851453225654716
plot_id,batch_id 0 37 miss% 0.026190785401705167
plot_id,batch_id 0 38 miss% 0.029849194171462635
plot_id,batch_id 0 39 miss% 0.02179213917692459
plot_id,batch_id 0 40 miss% 0.054119245410998704
plot_id,batch_id 0 41 miss% 0.032491586254784044
plot_id,batch_id 0 42 miss% 0.025487845046602092
plot_id,batch_id 0 43 miss% 0.024312222626369073
plot_id,batch_id 0 44 miss% 0.02538155720967995
plot_id,batch_id 0 45 miss% 0.02060939537688436
plot_id,batch_id 0 46 miss% 0.02750984023083485
plot_id,batch_id 0 47 miss% 0.027920414049066773
plot_id,batch_id 0 48 miss% 0.021560558454154343
plot_id,batch_id 0 49 miss% 0.026961164077845424
plot_id,batch_id 0 50 miss% 0.024653452305427634
plot_id,batch_id 0 51 miss% 0.025051181950648215
plot_id,batch_id 0 52 miss% 0.023404976198654552
plot_id,batch_id 0 53 miss% 0.021294113372293033
plot_id,batch_id 0 54 miss% 0.030472684620036886
plot_id,batch_id 0 55 miss% 0.03322532630462896
plot_id,batch_id 0 56 miss% 0.022493901291146527
plot_id,batch_id 0 57 miss% 0.025483486508757702
plot_id,batch_id 0 58 miss% 0.0248573610457437
plot_id,batch_id 0 59 miss% 0.02410496507865973
plot_id,batch_id 0 60 miss% 0.03614012474467255
plot_id,batch_id 0 61 miss% 0.025496173624865774
plot_id,batch_id 0 62 miss% 0.03522620517545009
plot_id,batch_id 0 63 miss% 0.030438810757322722
plot_id,batch_id 0 64 miss% 0.0351618236240986
plot_id,batch_id 0 65 miss% 0.048940221427187916
plot_id,batch_id 0 66 miss% 0.026001145417720806
plot_id,batch_id 0 67 miss% 0.024360336852163007
plot_id,batch_id 0 68 miss% 0.03724985773122676
plot_id,batch_id 0 69 miss% 0.03427268869627801
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  79249
Epoch:0, Train loss:0.298934, valid loss:0.276258
Epoch:1, Train loss:0.021744, valid loss:0.005037
Epoch:2, Train loss:0.006380, valid loss:0.002675
Epoch:3, Train loss:0.004809, valid loss:0.002465
Epoch:4, Train loss:0.004241, valid loss:0.002243
Epoch:5, Train loss:0.004043, valid loss:0.002247
Epoch:6, Train loss:0.003696, valid loss:0.001796
Epoch:7, Train loss:0.003529, valid loss:0.001686
Epoch:8, Train loss:0.003435, valid loss:0.001635
Epoch:9, Train loss:0.003269, valid loss:0.001383
Epoch:10, Train loss:0.003140, valid loss:0.001822
Epoch:11, Train loss:0.002149, valid loss:0.001254
Epoch:12, Train loss:0.002137, valid loss:0.001624
Epoch:13, Train loss:0.002091, valid loss:0.002562
Epoch:14, Train loss:0.002065, valid loss:0.001260
Epoch:15, Train loss:0.001987, valid loss:0.001237
Epoch:16, Train loss:0.002007, valid loss:0.001533
Epoch:17, Train loss:0.001964, valid loss:0.001104
Epoch:18, Train loss:0.001942, valid loss:0.001343
Epoch:19, Train loss:0.001884, valid loss:0.001108
Epoch:20, Train loss:0.001894, valid loss:0.001064
Epoch:21, Train loss:0.001357, valid loss:0.000848
Epoch:22, Train loss:0.001338, valid loss:0.000936
Epoch:23, Train loss:0.001359, valid loss:0.000828
Epoch:24, Train loss:0.001381, valid loss:0.000838
Epoch:25, Train loss:0.001314, valid loss:0.000844
Epoch:26, Train loss:0.001304, valid loss:0.000817
Epoch:27, Train loss:0.001305, valid loss:0.000825
Epoch:28, Train loss:0.001342, valid loss:0.000851
Epoch:29, Train loss:0.001267, valid loss:0.000799
Epoch:30, Train loss:0.001296, valid loss:0.000935
Epoch:31, Train loss:0.001004, valid loss:0.000774
Epoch:32, Train loss:0.001021, valid loss:0.000963
Epoch:33, Train loss:0.001014, valid loss:0.000708
Epoch:34, Train loss:0.001012, valid loss:0.000763
Epoch:35, Train loss:0.000986, valid loss:0.000762
Epoch:36, Train loss:0.001001, valid loss:0.000683
Epoch:37, Train loss:0.000998, valid loss:0.000727
Epoch:38, Train loss:0.000982, valid loss:0.000672
Epoch:39, Train loss:0.000955, valid loss:0.000727
Epoch:40, Train loss:0.001001, valid loss:0.000775
Epoch:41, Train loss:0.000830, valid loss:0.000634
Epoch:42, Train loss:0.000825, valid loss:0.000683
Epoch:43, Train loss:0.000830, valid loss:0.000687
Epoch:44, Train loss:0.000825, valid loss:0.000645
Epoch:45, Train loss:0.000833, valid loss:0.000614
Epoch:46, Train loss:0.000812, valid loss:0.000639
Epoch:47, Train loss:0.000811, valid loss:0.000612
Epoch:48, Train loss:0.000805, valid loss:0.000628
Epoch:49, Train loss:0.000827, valid loss:0.000624
Epoch:50, Train loss:0.000803, valid loss:0.000612
Epoch:51, Train loss:0.000735, valid loss:0.000588
Epoch:52, Train loss:0.000736, valid loss:0.000603
Epoch:53, Train loss:0.000729, valid loss:0.000597
Epoch:54, Train loss:0.000726, valid loss:0.000583
Epoch:55, Train loss:0.000728, valid loss:0.000598
Epoch:56, Train loss:0.000729, valid loss:0.000574
Epoch:57, Train loss:0.000734, valid loss:0.000617
Epoch:58, Train loss:0.000724, valid loss:0.000592
Epoch:59, Train loss:0.000731, valid loss:0.000601
Epoch:60, Train loss:0.000716, valid loss:0.000614
training time 8984.196621418
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.038656806768440535
plot_id,batch_id 0 1 miss% 0.023716527102603732
plot_id,batch_id 0 2 miss% 0.024148162347380677
plot_id,batch_id 0 3 miss% 0.026806499580583187
plot_id,batch_id 0 4 miss% 0.023533508468869247
plot_id,batch_id 0 5 miss% 0.030831518719209694
plot_id,batch_id 0 6 miss% 0.028152300879304343
plot_id,batch_id 0 7 miss% 0.026897964030399405
plot_id,batch_id 0 8 miss% 0.03053625850341381
plot_id,batch_id 0 9 miss% 0.028042845602425034
plot_id,batch_id 0 10 miss% 0.0322800274605465
plot_id,batch_id 0 11 miss% 0.04142613981247939
plot_id,batch_id 0 12 miss% 0.02705841249011239
plot_id,batch_id 0 13 miss% 0.02363727704535402
plot_id,batch_id 0 14 miss% 0.02458703859806
plot_id,batch_id 0 15 miss% 0.040995389929743435
plot_id,batch_id 0 16 miss% 0.030477996564579623
plot_id,batch_id 0 17 miss% 0.0364562131731962
plot_id,batch_id 0 18 miss% 0.03210239626505681
plot_id,batch_id 0 19 miss% 0.03015181685400121
plot_id,batch_id 0 20 miss% 0.0471242963985435
plot_id,batch_id 0 21 miss% 0.020781404328140046
plot_id,batch_id 0 22 miss% 0.0225957675556435
plot_id,batch_id 0 23 miss% 0.026266781419905315
plot_id,batch_id 0 24 miss% 0.028228306103166147
plot_id,batch_id 0 25 miss% 0.03688292873327491
plot_id,batch_id 0 26 miss% 0.030068630500301546
plot_id,batch_id 0 27 miss% 0.02490868874955885
plot_id,batch_id 0 28 miss% 0.026711875885476098
plot_id,batch_id 0 29 miss% 0.029906223312280277
plot_id,batch_id 0 30 miss% 0.04223952535447862
plot_id,batch_id 0 31 miss% 0.0349732815484253
plot_id,batch_id 0 32 miss% 0.03532479451946811
plot_id,batch_id 0 33 miss% 0.01939284537692913
plot_id,batch_id 0 34 miss% 0.023598305347887222
plot_id,batch_id 0 35 miss% 0.05958855071236549
plot_id,batch_id 0 36 miss% 0.042094629065527696
plot_id,batch_id 0 37 miss% 0.03280504298532665
plot_id,batch_id 0 38 miss% 0.022460687483773025
plot_id,batch_id 0 39 miss% 0.020177309135967412
plot_id,batch_id 0 40 miss% 0.07084241281186486
plot_id,batch_id 0 41 miss% 0.020294471766982295
plot_id,batch_id 0 42 miss% 0.01694862986262789
plot_id,batch_id 0 43 miss% 0.03049946581724122
plot_id,batch_id 0 44 miss% 0.014712733994312515
plot_id,batch_id 0 45 miss% 0.05250267007741477
plot_id,batch_id 0 46 miss% 0.030297949574628853
plot_id,batch_id 0 47 miss% 0.02713541063679378
plot_id,batch_id 0 48 miss% 0.02743444951148961
plot_id,batch_id 0 49 miss% 0.03306406187798923
plot_id,batch_id 0 50 miss% 0.041245438069004825
plot_id,batch_id 0 51 miss% 0.02725990296436333
plot_id,batch_id 0 52 miss% 0.030481000192391073
plot_id,batch_id 0 53 miss% 0.016516593973408905
plot_id,batch_id 0 54 miss% 0.024560134044481756
plot_id,batch_id 0 55 miss% 0.032718539086820375
plot_id,batch_id 0 56 miss% 0.02030415349462513
plot_id,batch_id 0 57 miss% 0.022781157940991358
plot_id,batch_id 0 58 miss% 0.022587068024797038
plot_id,batch_id 0 59 miss% 0.02442758308792059
plot_id,batch_id 0 60 miss% 0.0382874130527008
plot_id,batch_id 0 61 miss% 0.0229441534673939
plot_id,batch_id 0 62 miss% 0.02456515405563951
plot_id,batch_id 0 63 miss% 0.03227861028464763
plot_id,batch_id 0 64 miss% 0.021638818848599677
plot_id,batch_id 0 65 miss% 0.03218356723074166
plot_id,batch_id 0 66 miss% 0.04060033148477067
plot_id,batch_id 0 67 miss% 0.020463105204559795
plot_id,batch_id 0 68 plot_id,batch_id 0 70 miss% 0.04266509769191306
plot_id,batch_id 0 71 miss% 0.03817332090905811
plot_id,batch_id 0 72 miss% 0.03529014033340574
plot_id,batch_id 0 73 miss% 0.04121891135713975
plot_id,batch_id 0 74 miss% 0.03067064680431151
plot_id,batch_id 0 75 miss% 0.061542145643725565
plot_id,batch_id 0 76 miss% 0.026816383543883958
plot_id,batch_id 0 77 miss% 0.03970120062219125
plot_id,batch_id 0 78 miss% 0.0316758752355109
plot_id,batch_id 0 79 miss% 0.047031928082379396
plot_id,batch_id 0 80 miss% 0.0543491744674532
plot_id,batch_id 0 81 miss% 0.0326522103548833
plot_id,batch_id 0 82 miss% 0.028659279563001905
plot_id,batch_id 0 83 miss% 0.05256926246869667
plot_id,batch_id 0 84 miss% 0.04518701971351715
plot_id,batch_id 0 85 miss% 0.06143238473036686
plot_id,batch_id 0 86 miss% 0.034966768276799086
plot_id,batch_id 0 87 miss% 0.03747972630526052
plot_id,batch_id 0 88 miss% 0.03491512651329379
plot_id,batch_id 0 89 miss% 0.03980265116246525
plot_id,batch_id 0 90 miss% 0.04565944618340683
plot_id,batch_id 0 91 miss% 0.03246491390751598
plot_id,batch_id 0 92 miss% 0.023699546910935367
plot_id,batch_id 0 93 miss% 0.031826164196650415
plot_id,batch_id 0 94 miss% 0.046032148172668444
plot_id,batch_id 0 95 miss% 0.039478328077853564
plot_id,batch_id 0 96 miss% 0.03735776137256233
plot_id,batch_id 0 97 miss% 0.036121824961643956
plot_id,batch_id 0 98 miss% 0.03411380879831155
plot_id,batch_id 0 99 miss% 0.036058211583473124
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03541005 0.03985355 0.03574893 0.03355797 0.02883031 0.03743068
 0.03461778 0.03678469 0.03687484 0.03276485 0.03398339 0.04707893
 0.04668829 0.05006919 0.02957644 0.05095128 0.02799903 0.0370246
 0.04910116 0.02522112 0.03628016 0.03314993 0.02766733 0.02790514
 0.02466621 0.03314817 0.0209398  0.02458611 0.02062719 0.02687873
 0.04204646 0.03670378 0.03896188 0.03514838 0.02411276 0.0438284
 0.04385145 0.02619079 0.02984919 0.02179214 0.05411925 0.03249159
 0.02548785 0.02431222 0.02538156 0.0206094  0.02750984 0.02792041
 0.02156056 0.02696116 0.02465345 0.02505118 0.02340498 0.02129411
 0.03047268 0.03322533 0.0224939  0.02548349 0.02485736 0.02410497
 0.03614012 0.02549617 0.03522621 0.03043881 0.03516182 0.04894022
 0.02600115 0.02436034 0.03724986 0.03427269 0.0426651  0.03817332
 0.03529014 0.04121891 0.03067065 0.06154215 0.02681638 0.0397012
 0.03167588 0.04703193 0.05434917 0.03265221 0.02865928 0.05256926
 0.04518702 0.06143238 0.03496677 0.03747973 0.03491513 0.03980265
 0.04565945 0.03246491 0.02369955 0.03182616 0.04603215 0.03947833
 0.03735776 0.03612182 0.03411381 0.03605821]
for model  71 the mean error 0.03422195182102886
all id 71 hidden_dim 32 learning_rate 0.02 num_layers 4 frames 21 out win 5 err 0.03422195182102886
Launcher: Job 72 completed in 9176 seconds.
Launcher: Task 172 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  107025
Epoch:0, Train loss:0.426322, valid loss:0.420939
Epoch:1, Train loss:0.030567, valid loss:0.006446
Epoch:2, Train loss:0.008809, valid loss:0.004322
Epoch:3, Train loss:0.006813, valid loss:0.004057
Epoch:4, Train loss:0.005828, valid loss:0.002498
Epoch:5, Train loss:0.004822, valid loss:0.002813
Epoch:6, Train loss:0.004501, valid loss:0.003122
Epoch:7, Train loss:0.004371, valid loss:0.002329
Epoch:8, Train loss:0.003793, valid loss:0.002507
Epoch:9, Train loss:0.003858, valid loss:0.001925
Epoch:10, Train loss:0.003599, valid loss:0.002178
Epoch:11, Train loss:0.002435, valid loss:0.001370
Epoch:12, Train loss:0.002447, valid loss:0.001665
Epoch:13, Train loss:0.002367, valid loss:0.001611
Epoch:14, Train loss:0.002399, valid loss:0.001786
Epoch:15, Train loss:0.002316, valid loss:0.001223
Epoch:16, Train loss:0.002308, valid loss:0.001122
Epoch:17, Train loss:0.002126, valid loss:0.001203
Epoch:18, Train loss:0.002166, valid loss:0.001392
Epoch:19, Train loss:0.002063, valid loss:0.001138
Epoch:20, Train loss:0.002115, valid loss:0.001164
Epoch:21, Train loss:0.001500, valid loss:0.000956
Epoch:22, Train loss:0.001535, valid loss:0.001082
Epoch:23, Train loss:0.001515, valid loss:0.000960
Epoch:24, Train loss:0.001442, valid loss:0.001291
Epoch:25, Train loss:0.001482, valid loss:0.001044
Epoch:26, Train loss:0.001443, valid loss:0.001027
Epoch:27, Train loss:0.001416, valid loss:0.001035
Epoch:28, Train loss:0.001452, valid loss:0.001390
Epoch:29, Train loss:0.001423, valid loss:0.000950
Epoch:30, Train loss:0.001377, valid loss:0.000932
Epoch:31, Train loss:0.001117, valid loss:0.000890
Epoch:32, Train loss:0.001096, valid loss:0.000846
Epoch:33, Train loss:0.001111, valid loss:0.000901
Epoch:34, Train loss:0.001097, valid loss:0.000887
Epoch:35, Train loss:0.001107, valid loss:0.000860
Epoch:36, Train loss:0.001078, valid loss:0.000867
Epoch:37, Train loss:0.001056, valid loss:0.000824
Epoch:38, Train loss:0.001070, valid loss:0.000961
Epoch:39, Train loss:0.001058, valid loss:0.000859
Epoch:40, Train loss:0.001045, valid loss:0.000800
Epoch:41, Train loss:0.000915, valid loss:0.000793
Epoch:42, Train loss:0.000908, valid loss:0.000832
Epoch:43, Train loss:0.000901, valid loss:0.000785
Epoch:44, Train loss:0.000908, valid loss:0.000829
Epoch:45, Train loss:0.000894, valid loss:0.000861
Epoch:46, Train loss:0.000908, valid loss:0.000814
Epoch:47, Train loss:0.000901, valid loss:0.000859
Epoch:48, Train loss:0.000882, valid loss:0.000783
Epoch:49, Train loss:0.000886, valid loss:0.000841
Epoch:50, Train loss:0.000898, valid loss:0.000779
Epoch:51, Train loss:0.000816, valid loss:0.000759
Epoch:52, Train loss:0.000811, valid loss:0.000754
Epoch:53, Train loss:0.000808, valid loss:0.000785
Epoch:54, Train loss:0.000812, valid loss:0.000787
Epoch:55, Train loss:0.000814, valid loss:0.000801
Epoch:56, Train loss:0.000801, valid loss:0.000777
Epoch:57, Train loss:0.000805, valid loss:0.000776
Epoch:58, Train loss:0.000799, valid loss:0.000791
Epoch:59, Train loss:0.000808, valid loss:0.000798
Epoch:60, Train loss:0.000795, valid loss:0.000765
training time 8983.630376577377
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.027630356321985745
plot_id,batch_id 0 1 miss% 0.017642031373452896
plot_id,batch_id 0 2 miss% 0.021510476419338915
plot_id,batch_id 0 3 miss% 0.026241346845225506
plot_id,batch_id 0 4 miss% 0.017153573776648147
plot_id,batch_id 0 5 miss% 0.04192020208442642
plot_id,batch_id 0 6 miss% 0.03705349779925939
plot_id,batch_id 0 7 miss% 0.023996848436651064
plot_id,batch_id 0 8 miss% 0.020984882767085107
plot_id,batch_id 0 9 miss% 0.021931849307517286
plot_id,batch_id 0 10 miss% 0.052902538255212024
plot_id,batch_id 0 11 miss% 0.04248544788973091
plot_id,batch_id 0 12 miss% 0.029293049520177333
plot_id,batch_id 0 13 miss% 0.027469236408890263
plot_id,batch_id 0 14 miss% 0.02198954382463438
plot_id,batch_id 0 15 miss% 0.03643188128009064
plot_id,batch_id 0 16 miss% 0.01981266311787297
plot_id,batch_id 0 17 miss% 0.03294689371322723
plot_id,batch_id 0 18 miss% 0.021426252233434806
plot_id,batch_id 0 19 miss% 0.03848059018301429
plot_id,batch_id 0 20 miss% 0.03609682942783811
plot_id,batch_id 0 21 miss% 0.018621590796920065
plot_id,batch_id 0 22 miss% 0.02269896857131292
plot_id,batch_id 0 23 miss% 0.022220889705039836
plot_id,batch_id 0 24 miss% 0.02751384782744033
plot_id,batch_id 0 25 miss% 0.032994772076049804
plot_id,batch_id 0 26 miss% 0.03411026147220679
plot_id,batch_id 0 27 miss% 0.024286051725931363
plot_id,batch_id 0 28 miss% 0.02268550825661235
plot_id,batch_id 0 29 miss% 0.022546887136559425
plot_id,batch_id 0 30 miss% 0.037077743717648694
plot_id,batch_id 0 31 miss% 0.03331807567767155
plot_id,batch_id 0 32 miss% 0.0288391877394048
plot_id,batch_id 0 33 miss% 0.02387610103312236
plot_id,batch_id 0 34 miss% 0.02507980100717176
plot_id,batch_id 0 35 miss% 0.030313699549600573
plot_id,batch_id 0 36 miss% 0.04034828558942258
plot_id,batch_id 0 37 miss% 0.01894011922433194
plot_id,batch_id 0 38 miss% 0.026377370131759538
plot_id,batch_id 0 39 miss% 0.025829706104424296
plot_id,batch_id 0 40 miss% 0.05949723963220329
plot_id,batch_id 0 41 miss% 0.024568744730325574
plot_id,batch_id 0 42 miss% 0.02426110102194965
plot_id,batch_id 0 43 miss% 0.028804374821613046
plot_id,batch_id 0 44 miss% 0.021369430437517063
plot_id,batch_id 0 45 miss% 0.035305451225588556
plot_id,batch_id 0 46 miss% 0.021049221388074805
plot_id,batch_id 0 47 miss% 0.021326061628384057
plot_id,batch_id 0 48 miss% 0.03053970838604326
plot_id,batch_id 0 49 miss% 0.027973137376371627
plot_id,batch_id 0 50 miss% 0.039462362150130124
plot_id,batch_id 0 51 miss% 0.017717042190377178
plot_id,batch_id 0 52 miss% 0.0241689831205382
plot_id,batch_id 0 53 miss% 0.0226677930835579
plot_id,batch_id 0 54 miss% 0.030830266393177257
plot_id,batch_id 0 55 miss% 0.024206879844700224
plot_id,batch_id 0 56 miss% 0.028829454787341346
plot_id,batch_id 0 57 miss% 0.025727996936291485
plot_id,batch_id 0 58 miss% 0.029653888267651952
plot_id,batch_id 0 59 miss% 0.028500817566432454
plot_id,batch_id 0 60 miss% 0.026941070206896352
plot_id,batch_id 0 61 miss% 0.02955495301248938
plot_id,batch_id 0 62 miss% 0.02819452516575423
plot_id,batch_id 0 63 miss% 0.03239733019720133
plot_id,batch_id 0 64 miss% 0.028897113276189067
plot_id,batch_id 0 65 miss% 0.04163207791502979
plot_id,batch_id 0 66 miss% 0.025220828063076253
plot_id,batch_id 0 67 miss% 0.022755152051882405
plot_id,batch_id 0 68 miss% 0.028491482783534104
miss% 0.024057039087327144
plot_id,batch_id 0 69 miss% 0.034398134454057205
plot_id,batch_id 0 70 miss% 0.03091919730018835
plot_id,batch_id 0 71 miss% 0.047243323948578705
plot_id,batch_id 0 72 miss% 0.027352151499819493
plot_id,batch_id 0 73 miss% 0.02746512822809372
plot_id,batch_id 0 74 miss% 0.022096882329857757
plot_id,batch_id 0 75 miss% 0.05128601286942449
plot_id,batch_id 0 76 miss% 0.025525912641849818
plot_id,batch_id 0 77 miss% 0.040464849498376414
plot_id,batch_id 0 78 miss% 0.030296695145040686
plot_id,batch_id 0 79 miss% 0.03778466415718914
plot_id,batch_id 0 80 miss% 0.04656530534325143
plot_id,batch_id 0 81 miss% 0.02016229198171262
plot_id,batch_id 0 82 miss% 0.021953466169037518
plot_id,batch_id 0 83 miss% 0.03640247746598256
plot_id,batch_id 0 84 miss% 0.03654700118479386
plot_id,batch_id 0 85 miss% 0.03790426113893184
plot_id,batch_id 0 86 miss% 0.033498819188842004
plot_id,batch_id 0 87 miss% 0.024421058008744303
plot_id,batch_id 0 88 miss% 0.040638052890198964
plot_id,batch_id 0 89 miss% 0.02421738625674307
plot_id,batch_id 0 90 miss% 0.029519195157441173
plot_id,batch_id 0 91 miss% 0.0282607414000168
plot_id,batch_id 0 92 miss% 0.03385191980783323
plot_id,batch_id 0 93 miss% 0.022207436353300074
plot_id,batch_id 0 94 miss% 0.03715182321426978
plot_id,batch_id 0 95 miss% 0.05322716169055063
plot_id,batch_id 0 96 miss% 0.03110455658603617
plot_id,batch_id 0 97 miss% 0.04019057972960047
plot_id,batch_id 0 98 miss% 0.03460887265323958
plot_id,batch_id 0 99 miss% 0.03015802426906416
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03865681 0.02371653 0.02414816 0.0268065  0.02353351 0.03083152
 0.0281523  0.02689796 0.03053626 0.02804285 0.03228003 0.04142614
 0.02705841 0.02363728 0.02458704 0.04099539 0.030478   0.03645621
 0.0321024  0.03015182 0.0471243  0.0207814  0.02259577 0.02626678
 0.02822831 0.03688293 0.03006863 0.02490869 0.02671188 0.02990622
 0.04223953 0.03497328 0.03532479 0.01939285 0.02359831 0.05958855
 0.04209463 0.03280504 0.02246069 0.02017731 0.07084241 0.02029447
 0.01694863 0.03049947 0.01471273 0.05250267 0.03029795 0.02713541
 0.02743445 0.03306406 0.04124544 0.0272599  0.030481   0.01651659
 0.02456013 0.03271854 0.02030415 0.02278116 0.02258707 0.02442758
 0.03828741 0.02294415 0.02456515 0.03227861 0.02163882 0.03218357
 0.04060033 0.02046311 0.02405704 0.03439813 0.0309192  0.04724332
 0.02735215 0.02746513 0.02209688 0.05128601 0.02552591 0.04046485
 0.0302967  0.03778466 0.04656531 0.02016229 0.02195347 0.03640248
 0.036547   0.03790426 0.03349882 0.02442106 0.04063805 0.02421739
 0.0295192  0.02826074 0.03385192 0.02220744 0.03715182 0.05322716
 0.03110456 0.04019058 0.03460887 0.03015802]
for model  224 the mean error 0.03107680376800794
all id 224 hidden_dim 32 learning_rate 0.02 num_layers 3 frames 31 out win 5 err 0.03107680376800794
Launcher: Job 225 completed in 9177 seconds.
Launcher: Task 110 done. Exiting.
plot_id,batch_id 0 69 miss% 0.03943482276616182
plot_id,batch_id 0 70 miss% 0.03683634039736473
plot_id,batch_id 0 71 miss% 0.03814370574244425
plot_id,batch_id 0 72 miss% 0.035922087325091584
plot_id,batch_id 0 73 miss% 0.026936590944513634
plot_id,batch_id 0 74 miss% 0.037210325838234685
plot_id,batch_id 0 75 miss% 0.043071124583005424
plot_id,batch_id 0 76 miss% 0.04391589321909094
plot_id,batch_id 0 77 miss% 0.020254700874175333
plot_id,batch_id 0 78 miss% 0.031449081237241425
plot_id,batch_id 0 79 miss% 0.05119605678215708
plot_id,batch_id 0 80 miss% 0.04330851981145511
plot_id,batch_id 0 81 miss% 0.026861539314178024
plot_id,batch_id 0 82 miss% 0.028879333553931304
plot_id,batch_id 0 83 miss% 0.027092013320478003
plot_id,batch_id 0 84 miss% 0.04135326819332935
plot_id,batch_id 0 85 miss% 0.05091190076880008
plot_id,batch_id 0 86 miss% 0.04058481461459447
plot_id,batch_id 0 87 miss% 0.0373237183990569
plot_id,batch_id 0 88 miss% 0.03155534775609391
plot_id,batch_id 0 89 miss% 0.028950144372108192
plot_id,batch_id 0 90 miss% 0.03730282230210736
plot_id,batch_id 0 91 miss% 0.034686970894628265
plot_id,batch_id 0 92 miss% 0.025806003178776647
plot_id,batch_id 0 93 miss% 0.02805371021053084
plot_id,batch_id 0 94 miss% 0.04177858245507278
plot_id,batch_id 0 95 miss% 0.030762083049947833
plot_id,batch_id 0 96 miss% 0.03396365954377152
plot_id,batch_id 0 97 miss% 0.027070886137802626
plot_id,batch_id 0 98 miss% 0.03517380689883668
plot_id,batch_id 0 99 miss% 0.03482280898970682
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02763036 0.01764203 0.02151048 0.02624135 0.01715357 0.0419202
 0.0370535  0.02399685 0.02098488 0.02193185 0.05290254 0.04248545
 0.02929305 0.02746924 0.02198954 0.03643188 0.01981266 0.03294689
 0.02142625 0.03848059 0.03609683 0.01862159 0.02269897 0.02222089
 0.02751385 0.03299477 0.03411026 0.02428605 0.02268551 0.02254689
 0.03707774 0.03331808 0.02883919 0.0238761  0.0250798  0.0303137
 0.04034829 0.01894012 0.02637737 0.02582971 0.05949724 0.02456874
 0.0242611  0.02880437 0.02136943 0.03530545 0.02104922 0.02132606
 0.03053971 0.02797314 0.03946236 0.01771704 0.02416898 0.02266779
 0.03083027 0.02420688 0.02882945 0.025728   0.02965389 0.02850082
 0.02694107 0.02955495 0.02819453 0.03239733 0.02889711 0.04163208
 0.02522083 0.02275515 0.02849148 0.03943482 0.03683634 0.03814371
 0.03592209 0.02693659 0.03721033 0.04307112 0.04391589 0.0202547
 0.03144908 0.05119606 0.04330852 0.02686154 0.02887933 0.02709201
 0.04135327 0.0509119  0.04058481 0.03732372 0.03155535 0.02895014
 0.03730282 0.03468697 0.025806   0.02805371 0.04177858 0.03076208
 0.03396366 0.02707089 0.03517381 0.03482281]
for model  125 the mean error 0.030542360094633545
all id 125 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 25 out win 5 err 0.030542360094633545
Launcher: Job 126 completed in 9187 seconds.
Launcher: Task 57 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  107025
Epoch:0, Train loss:0.304599, valid loss:0.297865
Epoch:1, Train loss:0.018169, valid loss:0.002942
Epoch:2, Train loss:0.004387, valid loss:0.002231
Epoch:3, Train loss:0.003244, valid loss:0.001555
Epoch:4, Train loss:0.002749, valid loss:0.001798
Epoch:5, Train loss:0.002464, valid loss:0.001484
Epoch:6, Train loss:0.002258, valid loss:0.001768
Epoch:7, Train loss:0.002136, valid loss:0.001301
Epoch:8, Train loss:0.002070, valid loss:0.001072
Epoch:9, Train loss:0.001950, valid loss:0.001057
Epoch:10, Train loss:0.001864, valid loss:0.000931
Epoch:11, Train loss:0.001304, valid loss:0.001068
Epoch:12, Train loss:0.001271, valid loss:0.000712
Epoch:13, Train loss:0.001245, valid loss:0.000907
Epoch:14, Train loss:0.001196, valid loss:0.000822
Epoch:15, Train loss:0.001225, valid loss:0.000711
Epoch:16, Train loss:0.001159, valid loss:0.000766
Epoch:17, Train loss:0.001122, valid loss:0.000675
Epoch:18, Train loss:0.001130, valid loss:0.000628
Epoch:19, Train loss:0.001129, valid loss:0.000723
Epoch:20, Train loss:0.001098, valid loss:0.000742
Epoch:21, Train loss:0.000804, valid loss:0.000570
Epoch:22, Train loss:0.000801, valid loss:0.000573
Epoch:23, Train loss:0.000806, valid loss:0.000672
Epoch:24, Train loss:0.000793, valid loss:0.000544
Epoch:25, Train loss:0.000779, valid loss:0.000518
Epoch:26, Train loss:0.000784, valid loss:0.000608
Epoch:27, Train loss:0.000778, valid loss:0.000544
Epoch:28, Train loss:0.000743, valid loss:0.000543
Epoch:29, Train loss:0.000772, valid loss:0.000569
Epoch:30, Train loss:0.000739, valid loss:0.000652
Epoch:31, Train loss:0.000616, valid loss:0.000486
Epoch:32, Train loss:0.000607, valid loss:0.000458
Epoch:33, Train loss:0.000606, valid loss:0.000486
Epoch:34, Train loss:0.000603, valid loss:0.000476
Epoch:35, Train loss:0.000591, valid loss:0.000544
Epoch:36, Train loss:0.000606, valid loss:0.000523
Epoch:37, Train loss:0.000596, valid loss:0.000503
Epoch:38, Train loss:0.000598, valid loss:0.000470
Epoch:39, Train loss:0.000582, valid loss:0.000461
Epoch:40, Train loss:0.000598, valid loss:0.000471
Epoch:41, Train loss:0.000520, valid loss:0.000489
Epoch:42, Train loss:0.000518, valid loss:0.000453
Epoch:43, Train loss:0.000520, valid loss:0.000452
Epoch:44, Train loss:0.000522, valid loss:0.000450
Epoch:45, Train loss:0.000517, valid loss:0.000448
Epoch:46, Train loss:0.000508, valid loss:0.000444
Epoch:47, Train loss:0.000514, valid loss:0.000441
Epoch:48, Train loss:0.000511, valid loss:0.000463
Epoch:49, Train loss:0.000512, valid loss:0.000442
Epoch:50, Train loss:0.000513, valid loss:0.000462
Epoch:51, Train loss:0.000480, valid loss:0.000435
Epoch:52, Train loss:0.000477, valid loss:0.000430
Epoch:53, Train loss:0.000476, valid loss:0.000427
Epoch:54, Train loss:0.000477, valid loss:0.000428
Epoch:55, Train loss:0.000476, valid loss:0.000434
Epoch:56, Train loss:0.000473, valid loss:0.000434
Epoch:57, Train loss:0.000472, valid loss:0.000431
Epoch:58, Train loss:0.000473, valid loss:0.000429
Epoch:59, Train loss:0.000470, valid loss:0.000435
Epoch:60, Train loss:0.000470, valid loss:0.000431
training time 9055.411371707916
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.03122440112367045
plot_id,batch_id 0 1 miss% 0.026322934567177988
plot_id,batch_id 0 2 miss% 0.025328216864785244
plot_id,batch_id 0 3 miss% 0.020251816095570103
plot_id,batch_id 0 4 miss% 0.021395454747006163
plot_id,batch_id 0 5 miss% 0.026925504115846637
plot_id,batch_id 0 6 miss% 0.02110861766096385
plot_id,batch_id 0 7 miss% 0.027052398660754064
plot_id,batch_id 0 8 miss% 0.017543287798813548
plot_id,batch_id 0 9 miss% 0.018297963089836923
plot_id,batch_id 0 10 miss% 0.044198112670776446
plot_id,batch_id 0 11 miss% 0.028294604348552955
plot_id,batch_id 0 12 miss% 0.027239239936487625
plot_id,batch_id 0 13 miss% 0.016845889486342965
plot_id,batch_id 0 14 miss% 0.020988561056919198
plot_id,batch_id 0 15 miss% 0.032723658583826344
plot_id,batch_id 0 16 miss% 0.03784466952185577
plot_id,batch_id 0 17 miss% 0.03754743734225469
plot_id,batch_id 0 18 miss% 0.019060651009489806
plot_id,batch_id 0 19 miss% 0.029726224674294813
plot_id,batch_id 0 20 miss% 0.046960282217879276
plot_id,batch_id 0 21 miss% 0.017590048002848854
plot_id,batch_id 0 22 miss% 0.01962618632387135
plot_id,batch_id 0 23 miss% 0.016771749421833255
plot_id,batch_id 0 24 miss% 0.018124775088776076
plot_id,batch_id 0 25 miss% 0.031284158075044144
plot_id,batch_id 0 26 miss% 0.0253568970960593
plot_id,batch_id 0 27 miss% 0.031653914973374817
plot_id,batch_id 0 28 miss% 0.02641053936141353
plot_id,batch_id 0 29 miss% 0.01998508630012893
plot_id,batch_id 0 30 miss% 0.039582089285041755
plot_id,batch_id 0 31 miss% 0.015412976627115445
plot_id,batch_id 0 32 miss% 0.0237177202433528
plot_id,batch_id 0 33 miss% 0.01808425072973112
plot_id,batch_id 0 34 miss% 0.022571470816451514
plot_id,batch_id 0 35 miss% 0.02482408555837995
plot_id,batch_id 0 36 miss% 0.025137267413410255
plot_id,batch_id 0 37 miss% 0.031835952184413444
plot_id,batch_id 0 38 miss% 0.02554076411494617
plot_id,batch_id 0 39 miss% 0.01636427305762501
plot_id,batch_id 0 40 miss% 0.06053543677729942
plot_id,batch_id 0 41 miss% 0.02462058125638918
plot_id,batch_id 0 42 miss% 0.01732835997528128
plot_id,batch_id 0 43 miss% 0.034683791615672255
plot_id,batch_id 0 44 miss% 0.02211057296737923
plot_id,batch_id 0 45 miss% 0.03664525514733253
plot_id,batch_id 0 46 miss% 0.037437341666437564
plot_id,batch_id 0 47 miss% 0.017629902304891174
plot_id,batch_id 0 48 miss% 0.019707993428810543
plot_id,batch_id 0 49 miss% 0.020644744092634323
plot_id,batch_id 0 50 miss% 0.03671486793257868
plot_id,batch_id 0 51 miss% 0.014703926488289165
plot_id,batch_id 0 52 miss% 0.023202210503713242
plot_id,batch_id 0 53 miss% 0.0156528834279211
plot_id,batch_id 0 54 miss% 0.018389878924537813
plot_id,batch_id 0 55 miss% 0.03189239463051507
plot_id,batch_id 0 56 miss% 0.03211238547632176
plot_id,batch_id 0 57 miss% 0.02509592961476142
plot_id,batch_id 0 58 miss% 0.024642060812975552
plot_id,batch_id 0 59 miss% 0.018696507209862476
plot_id,batch_id 0 60 miss% 0.034024395725241824
plot_id,batch_id 0 61 miss% 0.020643175572808332
plot_id,batch_id 0 62 miss% 0.027920494716866563
plot_id,batch_id 0 63 miss% 0.022105249011597817
plot_id,batch_id 0 64 miss% 0.018346518099851147
plot_id,batch_id 0 65 miss% 0.03003368490257092
plot_id,batch_id 0 66 miss% 0.039946400306650316
plot_id,batch_id 0 67 miss% 0.026724755727878278
plot_id,batch_id 0 68 miss% 0.02649131413682692
plot_id,batch_id 0 69 miss% 0.02262784566980387
plot_id,batch_id 0 70 miss% 0.03627157425772997
plot_id,batch_id 0 71 miss% 0.03826556574104301
plot_id,batch_id 0 72 miss% 0.02372831031493567
plot_id,batch_id 0 73 miss% 0.028945758574293864
plot_id,batch_id 0 74 miss% 0.03500724243720374
plot_id,batch_id 0 75 miss% 0.03375834961380673
plot_id,batch_id 0 76 miss% 0.02986428986584326
plot_id,batch_id 0 77 miss% 0.03986097662235081
plot_id,batch_id 0 78 miss% 0.02962094085559923
plot_id,batch_id 0 79 miss% 0.02338835250099729
plot_id,batch_id 0 80 miss% 0.03379507981602006
plot_id,batch_id 0 81 miss% 0.016301860682962504
plot_id,batch_id 0 82 miss% 0.019602576452620445
plot_id,batch_id 0 83 miss% 0.02438364190982549
plot_id,batch_id 0 84 miss% 0.01814379832631726
plot_id,batch_id 0 85 miss% 0.044983437643898
plot_id,batch_id 0 86 miss% 0.02652428625756904
plot_id,batch_id 0 87 miss% 0.022082531205659704
plot_id,batch_id 0 88 miss% 0.01891086920204989
plot_id,batch_id 0 89 miss% 0.026788721010557338
plot_id,batch_id 0 90 miss% 0.0221396384159617
plot_id,batch_id 0 91 miss% 0.03891180362559127
plot_id,batch_id 0 92 miss% 0.0224730857867557
plot_id,batch_id 0 93 miss% 0.029668658479873496
plot_id,batch_id 0 94 miss% 0.023554975692972862
plot_id,batch_id 0 95 miss% 0.041148228134096035
plot_id,batch_id 0 96 miss% 0.03294251901222311
plot_id,batch_id 0 97 miss% 0.04593372967788159
plot_id,batch_id 0 98 miss% 0.03507486288815102
plot_id,batch_id 0 99 miss% 0.020150143337797768
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0312244  0.02632293 0.02532822 0.02025182 0.02139545 0.0269255
 0.02110862 0.0270524  0.01754329 0.01829796 0.04419811 0.0282946
 0.02723924 0.01684589 0.02098856 0.03272366 0.03784467 0.03754744
 0.01906065 0.02972622 0.04696028 0.01759005 0.01962619 0.01677175
 0.01812478 0.03128416 0.0253569  0.03165391 0.02641054 0.01998509
 0.03958209 0.01541298 0.02371772 0.01808425 0.02257147 0.02482409
 0.02513727 0.03183595 0.02554076 0.01636427 0.06053544 0.02462058
 0.01732836 0.03468379 0.02211057 0.03664526 0.03743734 0.0176299
 0.01970799 0.02064474 0.03671487 0.01470393 0.02320221 0.01565288
 0.01838988 0.03189239 0.03211239 0.02509593 0.02464206 0.01869651
 0.0340244  0.02064318 0.02792049 0.02210525 0.01834652 0.03003368
 0.0399464  0.02672476 0.02649131 0.02262785 0.03627157 0.03826557
 0.02372831 0.02894576 0.03500724 0.03375835 0.02986429 0.03986098
 0.02962094 0.02338835 0.03379508 0.01630186 0.01960258 0.02438364
 0.0181438  0.04498344 0.02652429 0.02208253 0.01891087 0.02678872
 0.02213964 0.0389118  0.02247309 0.02966866 0.02355498 0.04114823
 0.03294252 0.04593373 0.03507486 0.02015014]
for model  204 the mean error 0.027102907967132103
all id 204 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 31 out win 3 err 0.027102907967132103
Launcher: Job 205 completed in 9242 seconds.
Launcher: Task 182 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  107025
Epoch:0, Train loss:0.430855, valid loss:0.425762
Epoch:1, Train loss:0.037024, valid loss:0.006640
Epoch:2, Train loss:0.009450, valid loss:0.003904
Epoch:3, Train loss:0.006837, valid loss:0.004090
Epoch:4, Train loss:0.005499, valid loss:0.002884
Epoch:5, Train loss:0.005003, valid loss:0.003821
Epoch:6, Train loss:0.004748, valid loss:0.002002
Epoch:7, Train loss:0.004400, valid loss:0.002160
Epoch:8, Train loss:0.004109, valid loss:0.002117
Epoch:9, Train loss:0.003949, valid loss:0.001967
Epoch:10, Train loss:0.004022, valid loss:0.003390
Epoch:11, Train loss:0.002533, valid loss:0.001422
Epoch:12, Train loss:0.002520, valid loss:0.001502
Epoch:13, Train loss:0.002435, valid loss:0.001524
Epoch:14, Train loss:0.002399, valid loss:0.001448
Epoch:15, Train loss:0.002569, valid loss:0.001567
Epoch:16, Train loss:0.002297, valid loss:0.001276
Epoch:17, Train loss:0.002338, valid loss:0.002215
Epoch:18, Train loss:0.002264, valid loss:0.001557
Epoch:19, Train loss:0.002275, valid loss:0.001700
Epoch:20, Train loss:0.002170, valid loss:0.001855
Epoch:21, Train loss:0.001530, valid loss:0.000936
Epoch:22, Train loss:0.001532, valid loss:0.000914
Epoch:23, Train loss:0.001469, valid loss:0.000927
Epoch:24, Train loss:0.001573, valid loss:0.000943
Epoch:25, Train loss:0.001502, valid loss:0.000836
Epoch:26, Train loss:0.001516, valid loss:0.000935
Epoch:27, Train loss:0.001461, valid loss:0.000848
Epoch:28, Train loss:0.001453, valid loss:0.000865
Epoch:29, Train loss:0.001376, valid loss:0.000885
Epoch:30, Train loss:0.001458, valid loss:0.000885
Epoch:31, Train loss:0.001086, valid loss:0.000789
Epoch:32, Train loss:0.001065, valid loss:0.000918
Epoch:33, Train loss:0.001067, valid loss:0.000821
Epoch:34, Train loss:0.001091, valid loss:0.000699
Epoch:35, Train loss:0.001039, valid loss:0.000770
Epoch:36, Train loss:0.001059, valid loss:0.000738
Epoch:37, Train loss:0.001061, valid loss:0.000687
Epoch:38, Train loss:0.001047, valid loss:0.000772
Epoch:39, Train loss:0.001020, valid loss:0.000731
Epoch:40, Train loss:0.001056, valid loss:0.000781
Epoch:41, Train loss:0.000863, valid loss:0.000686
Epoch:42, Train loss:0.000866, valid loss:0.000637
Epoch:43, Train loss:0.000847, valid loss:0.000697
Epoch:44, Train loss:0.000851, valid loss:0.000661
Epoch:45, Train loss:0.000876, valid loss:0.000714
Epoch:46, Train loss:0.000867, valid loss:0.000667
Epoch:47, Train loss:0.000849, valid loss:0.000719
Epoch:48, Train loss:0.000825, valid loss:0.000689
Epoch:49, Train loss:0.000846, valid loss:0.000642
Epoch:50, Train loss:0.000856, valid loss:0.000619
Epoch:51, Train loss:0.000763, valid loss:0.000631
Epoch:52, Train loss:0.000758, valid loss:0.000623
Epoch:53, Train loss:0.000764, valid loss:0.000627
Epoch:54, Train loss:0.000755, valid loss:0.000615
Epoch:55, Train loss:0.000761, valid loss:0.000608
Epoch:56, Train loss:0.000751, valid loss:0.000621
Epoch:57, Train loss:0.000754, valid loss:0.000618
Epoch:58, Train loss:0.000751, valid loss:0.000626
Epoch:59, Train loss:0.000748, valid loss:0.000624
Epoch:60, Train loss:0.000742, valid loss:0.000641
training time 9072.008365869522
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.044279612961811286
plot_id,batch_id 0 1 miss% 0.0256655739332194
plot_id,batch_id 0 2 miss% 0.01795382582268548
plot_id,batch_id 0 3 miss% 0.026509687634506077
plot_id,batch_id 0 4 miss% 0.019476708944463666
plot_id,batch_id 0 5 miss% 0.026359386225600936
plot_id,batch_id 0 6 miss% 0.03735602497792271
plot_id,batch_id 0 7 miss% 0.03413632415154654
plot_id,batch_id 0 8 miss% 0.029623704965993397
plot_id,batch_id 0 9 miss% 0.023708366869038232
plot_id,batch_id 0 10 miss% 0.03629024168101189
plot_id,batch_id 0 11 miss% 0.04358259357600256
plot_id,batch_id 0 12 miss% 0.02225987191197432
plot_id,batch_id 0 13 miss% 0.02693155274805162
plot_id,batch_id 0 14 miss% 0.03532608724660975
plot_id,batch_id 0 15 miss% 0.04818996250538721
plot_id,batch_id 0 16 miss% 0.03685151128556231
plot_id,batch_id 0 17 miss% 0.027531918103452322
plot_id,batch_id 0 18 miss% 0.034547121885671675
plot_id,batch_id 0 19 miss% 0.04214302456869821
plot_id,batch_id 0 20 miss% 0.05444474744452712
plot_id,batch_id 0 21 miss% 0.018492102125902812
plot_id,batch_id 0 22 miss% 0.028214229163535092
plot_id,batch_id 0 23 miss% 0.0257843565644482
plot_id,batch_id 0 24 miss% 0.02782054568871646
plot_id,batch_id 0 25 miss% 0.0362588972401847
plot_id,batch_id 0 26 miss% 0.03212569107817935
plot_id,batch_id 0 27 miss% 0.030975640516589204
plot_id,batch_id 0 28 miss% 0.022234519556371284
plot_id,batch_id 0 29 miss% 0.024374414353406034
plot_id,batch_id 0 30 miss% 0.031981382771967826
plot_id,batch_id 0 31 miss% 0.04279145783146236
plot_id,batch_id 0 32 miss% 0.032152914062436144
plot_id,batch_id 0 33 miss% 0.025273684861675885
plot_id,batch_id 0 34 miss% 0.027358872855947833
plot_id,batch_id 0 35 miss% 0.041608312576618685
plot_id,batch_id 0 36 miss% 0.041150134254149755
plot_id,batch_id 0 37 miss% 0.03338808953466811
plot_id,batch_id 0 38 miss% 0.026382039482258958
plot_id,batch_id 0 39 miss% 0.021582591716250727
plot_id,batch_id 0 40 miss% 0.05374758442224827
plot_id,batch_id 0 41 miss% 0.01814488261763235
plot_id,batch_id 0 42 miss% 0.017963765413699075
plot_id,batch_id 0 43 miss% 0.025603453892191052
plot_id,batch_id 0 44 miss% 0.021769581470185227
plot_id,batch_id 0 45 miss% 0.02379953836371488
plot_id,batch_id 0 46 miss% 0.02258078168724522
plot_id,batch_id 0 47 miss% 0.01846040294958916
plot_id,batch_id 0 48 miss% 0.0181981499708544
plot_id,batch_id 0 49 miss% 0.015063919952074432
plot_id,batch_id 0 50 miss% 0.027253604735372507
plot_id,batch_id 0 51 miss% 0.02299565236529307
plot_id,batch_id 0 52 miss% 0.021992732148915787
plot_id,batch_id 0 53 miss% 0.009113921647288997
plot_id,batch_id 0 54 miss% 0.038544985646362724
plot_id,batch_id 0 55 miss% 0.04093134334366419
plot_id,batch_id 0 56 miss% 0.021966233764294123
plot_id,batch_id 0 57 miss% 0.023160428528195755
plot_id,batch_id 0 58 miss% 0.020198939624695444
plot_id,batch_id 0 59 miss% 0.021469347342302512
plot_id,batch_id 0 60 miss% 0.033177606196672656
plot_id,batch_id 0 61 miss% 0.02704779235483648
plot_id,batch_id 0 62 miss% 0.017763291725279885
plot_id,batch_id 0 63 miss% 0.04089378946742112
plot_id,batch_id 0 64 miss% 0.02732187826440867
plot_id,batch_id 0 65 miss% 0.032743928236340834
plot_id,batch_id 0 66 miss% 0.035965404739181875
plot_id,batch_id 0 67 miss% 0.03790826718447574
plot_id,batch_id 0 68 miss% 0.03144926667293927
plot_id,batch_id 0 69 miss% 0.034443703936866944
plot_id,batch_id 0 70 miss% 0.03401456332396854
plot_id,batch_id 0 71 miss% 0.05752085789064477
plot_id,batch_id 0 72 miss% 0.030838023870417626
plot_id,batch_id 0 73 miss% 0.02836793270254682
plot_id,batch_id 0 74 miss% 0.039674528765836825
plot_id,batch_id 0 75 miss% 0.04082135195401195
plot_id,batch_id 0 76 miss% 0.03767268055184549
plot_id,batch_id 0 77 miss% 0.029616305480406335
plot_id,batch_id 0 78 miss% 0.03743221394040099
plot_id,batch_id 0 79 miss% 0.05081879700882257
plot_id,batch_id 0 80 miss% 0.028354462176647083
plot_id,batch_id 0 81 miss% 0.028860380906840746
plot_id,batch_id 0 82 miss% 0.016432984440202362
plot_id,batch_id 0 83 miss% 0.026865661649399384
plot_id,batch_id 0 84 miss% 0.039133723303308734
plot_id,batch_id 0 85 miss% 0.04285877248633659
plot_id,batch_id 0 86 miss% 0.031642369052736735
plot_id,batch_id 0 87 miss% 0.03133789026250824
plot_id,batch_id 0 88 miss% 0.04187960421621659
plot_id,batch_id 0 89 miss% 0.031477340713234936
plot_id,batch_id 0 90 miss% 0.03911663750261805
plot_id,batch_id 0 91 miss% 0.04132508589545011
plot_id,batch_id 0 92 miss% 0.03428140830594186
plot_id,batch_id 0 93 miss% 0.02483490852464936
plot_id,batch_id 0 94 miss% 0.03681601680735534
plot_id,batch_id 0 95 miss% 0.04991458261758333
plot_id,batch_id 0 96 miss% 0.024384718096991834
plot_id,batch_id 0 97 miss% 0.05023142338527331
plot_id,batch_id 0 98 miss% 0.03227138327327373
plot_id,batch_id 0 99 miss% 0.03580577596030319
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04427961 0.02566557 0.01795383 0.02650969 0.01947671 0.02635939
 0.03735602 0.03413632 0.0296237  0.02370837 0.03629024 0.04358259
 0.02225987 0.02693155 0.03532609 0.04818996 0.03685151 0.02753192
 0.03454712 0.04214302 0.05444475 0.0184921  0.02821423 0.02578436
 0.02782055 0.0362589  0.03212569 0.03097564 0.02223452 0.02437441
 0.03198138 0.04279146 0.03215291 0.02527368 0.02735887 0.04160831
 0.04115013 0.03338809 0.02638204 0.02158259 0.05374758 0.01814488
 0.01796377 0.02560345 0.02176958 0.02379954 0.02258078 0.0184604
 0.01819815 0.01506392 0.0272536  0.02299565 0.02199273 0.00911392
 0.03854499 0.04093134 0.02196623 0.02316043 0.02019894 0.02146935
 0.03317761 0.02704779 0.01776329 0.04089379 0.02732188 0.03274393
 0.0359654  0.03790827 0.03144927 0.0344437  0.03401456 0.05752086
 0.03083802 0.02836793 0.03967453 0.04082135 0.03767268 0.02961631
 0.03743221 0.0508188  0.02835446 0.02886038 0.01643298 0.02686566
 0.03913372 0.04285877 0.03164237 0.03133789 0.0418796  0.03147734
 0.03911664 0.04132509 0.03428141 0.02483491 0.03681602 0.04991458
 0.02438472 0.05023142 0.03227138 0.03580578]
for model  151 the mean error 0.031393942894085224
all id 151 hidden_dim 32 learning_rate 0.02 num_layers 4 frames 25 out win 4 err 0.031393942894085224
Launcher: Job 152 completed in 9271 seconds.
Launcher: Task 129 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  77489
Epoch:0, Train loss:0.489063, valid loss:0.488345
Epoch:1, Train loss:0.041291, valid loss:0.006591
Epoch:2, Train loss:0.010697, valid loss:0.004371
Epoch:3, Train loss:0.007728, valid loss:0.004011
Epoch:4, Train loss:0.006272, valid loss:0.003314
Epoch:5, Train loss:0.005265, valid loss:0.002527
Epoch:6, Train loss:0.004526, valid loss:0.002329
Epoch:7, Train loss:0.003981, valid loss:0.003115
Epoch:8, Train loss:0.003720, valid loss:0.001793
Epoch:9, Train loss:0.003461, valid loss:0.001583
Epoch:10, Train loss:0.003354, valid loss:0.001991
Epoch:11, Train loss:0.002458, valid loss:0.001671
Epoch:12, Train loss:0.002356, valid loss:0.001445
Epoch:13, Train loss:0.002320, valid loss:0.001158
Epoch:14, Train loss:0.002318, valid loss:0.001330
Epoch:15, Train loss:0.002268, valid loss:0.001291
Epoch:16, Train loss:0.002184, valid loss:0.001188
Epoch:17, Train loss:0.002147, valid loss:0.001314
Epoch:18, Train loss:0.002162, valid loss:0.001224
Epoch:19, Train loss:0.002130, valid loss:0.001665
Epoch:20, Train loss:0.002066, valid loss:0.001769
Epoch:21, Train loss:0.001525, valid loss:0.000926
Epoch:22, Train loss:0.001492, valid loss:0.000877
Epoch:23, Train loss:0.001544, valid loss:0.000921
Epoch:24, Train loss:0.001475, valid loss:0.000946
Epoch:25, Train loss:0.001498, valid loss:0.001117
Epoch:26, Train loss:0.001455, valid loss:0.000840
Epoch:27, Train loss:0.001414, valid loss:0.001082
Epoch:28, Train loss:0.001427, valid loss:0.000877
Epoch:29, Train loss:0.001416, valid loss:0.000877
Epoch:30, Train loss:0.001407, valid loss:0.000957
Epoch:31, Train loss:0.001132, valid loss:0.000801
Epoch:32, Train loss:0.001126, valid loss:0.000779
Epoch:33, Train loss:0.001142, valid loss:0.000758
Epoch:34, Train loss:0.001108, valid loss:0.000827
Epoch:35, Train loss:0.001102, valid loss:0.000766
Epoch:36, Train loss:0.001107, valid loss:0.000844
Epoch:37, Train loss:0.001083, valid loss:0.000890
Epoch:38, Train loss:0.001084, valid loss:0.000867
Epoch:39, Train loss:0.001104, valid loss:0.000766
Epoch:40, Train loss:0.001066, valid loss:0.000716
Epoch:41, Train loss:0.000947, valid loss:0.000715
Epoch:42, Train loss:0.000935, valid loss:0.000745
Epoch:43, Train loss:0.000951, valid loss:0.000714
Epoch:44, Train loss:0.000932, valid loss:0.000698
Epoch:45, Train loss:0.000948, valid loss:0.000684
Epoch:46, Train loss:0.000935, valid loss:0.000730
Epoch:47, Train loss:0.000932, valid loss:0.000776
Epoch:48, Train loss:0.000932, valid loss:0.000711
Epoch:49, Train loss:0.000912, valid loss:0.000706
Epoch:50, Train loss:0.000920, valid loss:0.000724
Epoch:51, Train loss:0.000857, valid loss:0.000676
Epoch:52, Train loss:0.000848, valid loss:0.000687
Epoch:53, Train loss:0.000848, valid loss:0.000685
Epoch:54, Train loss:0.000847, valid loss:0.000704
Epoch:55, Train loss:0.000845, valid loss:0.000702
Epoch:56, Train loss:0.000845, valid loss:0.000710
Epoch:57, Train loss:0.000847, valid loss:0.000672
Epoch:58, Train loss:0.000839, valid loss:0.000686
Epoch:59, Train loss:0.000840, valid loss:0.000695
Epoch:60, Train loss:0.000839, valid loss:0.000680
training time 9085.049803256989
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.01599176272900668
plot_id,batch_id 0 1 miss% 0.026809114872669383
plot_id,batch_id 0 2 miss% 0.02292431996984196
plot_id,batch_id 0 3 miss% 0.024548582055043452
plot_id,batch_id 0 4 miss% 0.027442310852575064
plot_id,batch_id 0 5 miss% 0.028755456768114904
plot_id,batch_id 0 6 miss% 0.034459954154565736
plot_id,batch_id 0 7 miss% 0.025752266598264493
plot_id,batch_id 0 8 miss% 0.023314353542079846
plot_id,batch_id 0 9 miss% 0.02959532569833201
plot_id,batch_id 0 10 miss% 0.05011602688740798
plot_id,batch_id 0 11 miss% 0.03771339031057605
plot_id,batch_id 0 12 miss% 0.03174478114497115
plot_id,batch_id 0 13 miss% 0.022035672561674657
plot_id,batch_id 0 14 miss% 0.02618712725420857
plot_id,batch_id 0 15 miss% 0.040260911838009075
plot_id,batch_id 0 16 miss% 0.027358365282922112
plot_id,batch_id 0 17 miss% 0.031248751844355892
plot_id,batch_id 0 18 miss% 0.0293018961160789
plot_id,batch_id 0 19 miss% 0.03665619366950895
plot_id,batch_id 0 20 miss% 0.047780162233141646
plot_id,batch_id 0 21 miss% 0.02472763855644068
plot_id,batch_id 0 22 miss% 0.018829845823049828
plot_id,batch_id 0 23 miss% 0.021964274862552494
plot_id,batch_id 0 24 miss% 0.02311443415592395
plot_id,batch_id 0 25 miss% 0.04572336328148354
plot_id,batch_id 0 26 miss% 0.02352018507088076
plot_id,batch_id 0 27 miss% 0.02664977853867658
plot_id,batch_id 0 28 miss% 0.020592663775602472
plot_id,batch_id 0 29 miss% 0.024468471539670945
plot_id,batch_id 0 30 miss% 0.02771368217565912
plot_id,batch_id 0 31 miss% 0.034797895989740105
plot_id,batch_id 0 32 miss% 0.029511555462967534
plot_id,batch_id 0 33 miss% 0.02544124783631958
plot_id,batch_id 0 34 miss% 0.02932470480374772
plot_id,batch_id 0 35 miss% 0.050100454838026276
plot_id,batch_id 0 36 miss% 0.02953744246008971
plot_id,batch_id 0 37 miss% 0.027212932648296804
plot_id,batch_id 0 38 miss% 0.024719616249853155
plot_id,batch_id 0 39 miss% 0.021255726375296154
plot_id,batch_id 0 40 miss% 0.07083819124943651
plot_id,batch_id 0 41 miss% 0.024508746941321317
plot_id,batch_id 0 42 miss% 0.01876414022214922
plot_id,batch_id 0 43 miss% 0.025351649861229866
plot_id,batch_id 0 44 miss% 0.024562635690170493
plot_id,batch_id 0 45 miss% 0.023321245263007023
plot_id,batch_id 0 46 miss% 0.02370214356683612
plot_id,batch_id 0 47 miss% 0.024048852469067104
plot_id,batch_id 0 48 miss% 0.025801694809741534
plot_id,batch_id 0 49 miss% 0.019605662965790704
plot_id,batch_id 0 50 miss% 0.032247622073012294
plot_id,batch_id 0 51 miss% 0.02371369638665545
plot_id,batch_id 0 52 miss% 0.024176250920638218
plot_id,batch_id 0 53 miss% 0.019457337263725127
plot_id,batch_id 0 54 miss% 0.023070710990674616
plot_id,batch_id 0 55 miss% 0.04499749587669833
plot_id,batch_id 0 56 miss% 0.02945590841968766
plot_id,batch_id 0 57 miss% 0.024453511973031923
plot_id,batch_id 0 58 miss% 0.02967343193280463
plot_id,batch_id 0 59 miss% 0.01906502825821276
plot_id,batch_id 0 60 miss% 0.05015582912803167
plot_id,batch_id 0 61 miss% 0.02179806452677537
plot_id,batch_id 0 62 miss% 0.025975001389976132
plot_id,batch_id 0 63 miss% 0.03751231166379667
plot_id,batch_id 0 64 miss% 0.03594756761187485
plot_id,batch_id 0 65 miss% 0.05895677237023905
plot_id,batch_id 0 66 miss% 0.034883590068976865
plot_id,batch_id 0 67 miss% 0.035966689268043035
plot_id,batch_id 0 68 miss% 0.023976939076336065
plot_id,batch_id 0 69 miss% 0.030736209946149437
plot_id,batch_id 0 70 miss% 0.05697880345185792
plot_id,batch_id 0 71 miss% 0.05764257038194164
plot_id,batch_id 0 72 miss% 0.034903947881575054
plot_id,batch_id 0 73 miss% 0.033026461062563885
plot_id,batch_id 0 74 miss% 0.039796398348654914
plot_id,batch_id 0 75 miss% 0.040364002439009825
plot_id,batch_id 0 76 miss% 0.028934945155128206
plot_id,batch_id 0 77 miss% 0.037229608895986456
plot_id,batch_id 0 78 miss% 0.031375817663512565
plot_id,batch_id 0 79 miss% 0.04670324923547969
plot_id,batch_id 0 80 miss% 0.04177239432329634
plot_id,batch_id 0 81 miss% 0.027698567254748323
plot_id,batch_id 0 82 miss% 0.03240198719513745
plot_id,batch_id 0 83 miss% 0.03108816858279929
plot_id,batch_id 0 84 miss% 0.03668922301091101
plot_id,batch_id 0 85 miss% 0.04100160644372071
plot_id,batch_id 0 86 miss% 0.02703668032030222
plot_id,batch_id 0 87 miss% 0.028348051251480665
plot_id,batch_id 0 88 miss% 0.031547710222915464
plot_id,batch_id 0 89 miss% 0.03787527628138716
plot_id,batch_id 0 90 miss% 0.03371938102666575
plot_id,batch_id 0 91 miss% 0.02963215074784318
plot_id,batch_id 0 92 miss% 0.02416303783730358
plot_id,batch_id 0 93 miss% 0.021029984052288918
plot_id,batch_id 0 94 miss% 0.04650755872474371
plot_id,batch_id 0 95 miss% 0.056689532952949095
plot_id,batch_id 0 96 miss% 0.03776519952475937
plot_id,batch_id 0 97 miss% 0.03314227242806298
plot_id,batch_id 0 98 miss% 0.03991606777520438
plot_id,batch_id 0 99 miss% 0.03839554688211818
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01599176 0.02680911 0.02292432 0.02454858 0.02744231 0.02875546
 0.03445995 0.02575227 0.02331435 0.02959533 0.05011603 0.03771339
 0.03174478 0.02203567 0.02618713 0.04026091 0.02735837 0.03124875
 0.0293019  0.03665619 0.04778016 0.02472764 0.01882985 0.02196427
 0.02311443 0.04572336 0.02352019 0.02664978 0.02059266 0.02446847
 0.02771368 0.0347979  0.02951156 0.02544125 0.0293247  0.05010045
 0.02953744 0.02721293 0.02471962 0.02125573 0.07083819 0.02450875
 0.01876414 0.02535165 0.02456264 0.02332125 0.02370214 0.02404885
 0.02580169 0.01960566 0.03224762 0.0237137  0.02417625 0.01945734
 0.02307071 0.0449975  0.02945591 0.02445351 0.02967343 0.01906503
 0.05015583 0.02179806 0.025975   0.03751231 0.03594757 0.05895677
 0.03488359 0.03596669 0.02397694 0.03073621 0.0569788  0.05764257
 0.03490395 0.03302646 0.0397964  0.040364   0.02893495 0.03722961
 0.03137582 0.04670325 0.04177239 0.02769857 0.03240199 0.03108817
 0.03668922 0.04100161 0.02703668 0.02834805 0.03154771 0.03787528
 0.03371938 0.02963215 0.02416304 0.02102998 0.04650756 0.05668953
 0.0377652  0.03314227 0.03991607 0.03839555]
for model  130 the mean error 0.03179303774366064
all id 130 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 25 out win 4 err 0.03179303774366064
Launcher: Job 131 completed in 9284 seconds.
Launcher: Task 141 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  134801
Epoch:0, Train loss:0.648029, valid loss:0.652804
Epoch:1, Train loss:0.513407, valid loss:0.519253
Epoch:2, Train loss:0.498211, valid loss:0.517376
Epoch:3, Train loss:0.495611, valid loss:0.516905
Epoch:4, Train loss:0.493598, valid loss:0.516001
Epoch:5, Train loss:0.492744, valid loss:0.515535
Epoch:6, Train loss:0.491954, valid loss:0.515161
Epoch:7, Train loss:0.491791, valid loss:0.515265
Epoch:8, Train loss:0.491340, valid loss:0.514772
Epoch:9, Train loss:0.491088, valid loss:0.515154
Epoch:10, Train loss:0.491191, valid loss:0.515083
Epoch:11, Train loss:0.489819, valid loss:0.514420
Epoch:12, Train loss:0.489811, valid loss:0.514551
Epoch:13, Train loss:0.489761, valid loss:0.514485
Epoch:14, Train loss:0.489779, valid loss:0.514303
Epoch:15, Train loss:0.489658, valid loss:0.514754
Epoch:16, Train loss:0.489716, valid loss:0.514259
Epoch:17, Train loss:0.489568, valid loss:0.514387
Epoch:18, Train loss:0.489511, valid loss:0.514154
Epoch:19, Train loss:0.489468, valid loss:0.514186
Epoch:20, Train loss:0.489425, valid loss:0.514352
Epoch:21, Train loss:0.488968, valid loss:0.514044
Epoch:22, Train loss:0.488917, valid loss:0.513964
Epoch:23, Train loss:0.488945, valid loss:0.514004
Epoch:24, Train loss:0.488887, valid loss:0.514174
Epoch:25, Train loss:0.488879, valid loss:0.514048
Epoch:26, Train loss:0.488851, valid loss:0.514034
Epoch:27, Train loss:0.488844, valid loss:0.513999
Epoch:28, Train loss:0.488825, valid loss:0.514084
Epoch:29, Train loss:0.488853, valid loss:0.514114
Epoch:30, Train loss:0.488805, valid loss:0.514013
Epoch:31, Train loss:0.488568, valid loss:0.513895
Epoch:32, Train loss:0.488545, valid loss:0.513844
Epoch:33, Train loss:0.488550, valid loss:0.513819
Epoch:34, Train loss:0.488551, valid loss:0.513890
Epoch:35, Train loss:0.488528, valid loss:0.513884
Epoch:36, Train loss:0.488551, valid loss:0.513885
Epoch:37, Train loss:0.488521, valid loss:0.513869
Epoch:38, Train loss:0.488512, valid loss:0.513976
Epoch:39, Train loss:0.488519, valid loss:0.513860
Epoch:40, Train loss:0.488485, valid loss:0.513874
Epoch:41, Train loss:0.488396, valid loss:0.513812
Epoch:42, Train loss:0.488386, valid loss:0.513827
Epoch:43, Train loss:0.488389, valid loss:0.513838
Epoch:44, Train loss:0.488376, valid loss:0.513806
Epoch:45, Train loss:0.488375, valid loss:0.513807
Epoch:46, Train loss:0.488368, valid loss:0.513897
Epoch:47, Train loss:0.488368, valid loss:0.513811
Epoch:48, Train loss:0.488361, valid loss:0.513829
Epoch:49, Train loss:0.488374, valid loss:0.513805
Epoch:50, Train loss:0.488356, valid loss:0.513816
Epoch:51, Train loss:0.488313, valid loss:0.513790
Epoch:52, Train loss:0.488305, valid loss:0.513775
Epoch:53, Train loss:0.488303, valid loss:0.513791
Epoch:54, Train loss:0.488304, valid loss:0.513787
Epoch:55, Train loss:0.488303, valid loss:0.513800
Epoch:56, Train loss:0.488300, valid loss:0.513765
Epoch:57, Train loss:0.488297, valid loss:0.513769
Epoch:58, Train loss:0.488295, valid loss:0.513787
Epoch:59, Train loss:0.488295, valid loss:0.513774
Epoch:60, Train loss:0.488289, valid loss:0.513767
training time 9117.150401353836
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.7742169193439425
plot_id,batch_id 0 1 miss% 0.8219170729791272
plot_id,batch_id 0 2 miss% 0.8305142322746956
plot_id,batch_id 0 3 miss% 0.8329674571624546
plot_id,batch_id 0 4 miss% 0.8349633849206822
plot_id,batch_id 0 5 miss% 0.7660555982810048
plot_id,batch_id 0 6 miss% 0.815583241312885
plot_id,batch_id 0 7 miss% 0.8285213626812626
plot_id,batch_id 0 8 miss% 0.8330014266523141
plot_id,batch_id 0 9 miss% 0.8373714961462414
plot_id,batch_id 0 10 miss% 0.7459040130989798
plot_id,batch_id 0 11 miss% 0.8150022716478377
plot_id,batch_id 0 12 miss% 0.8252717719473363
plot_id,batch_id 0 13 miss% 0.8304063482371329
plot_id,batch_id 0 14 miss% 0.8337315325755907
plot_id,batch_id 0 15 miss% 0.759736291391429
plot_id,batch_id 0 16 miss% 0.8120928864354563
plot_id,batch_id 0 17 miss% 0.825467321129035
plot_id,batch_id 0 18 miss% 0.8338319012614213
plot_id,batch_id 0 19 miss% 0.8322845033280588
plot_id,batch_id 0 20 miss% 0.7933118038490904
plot_id,batch_id 0 21 miss% 0.8279974400449902
plot_id,batch_id 0 22 miss% 0.8356010541489298
plot_id,batch_id 0 23 miss% 0.8350387019637533
plot_id,batch_id 0 24 miss% 0.8417760810320843
plot_id,batch_id 0 25 miss% 0.796140407895312
plot_id,batch_id 0 26 miss% 0.8276167317423626
plot_id,batch_id 0 27 miss% 0.8322219591944884
plot_id,batch_id 0 28 miss% 0.8342718589025108
plot_id,batch_id 0 29 miss% 0.8363120588323714
plot_id,batch_id 0 30 miss% 0.7884769505552514
plot_id,batch_id 0 31 miss% 0.8241136916586078
plot_id,batch_id 0 32 miss% 0.8330610523613163
plot_id,batch_id 0 33 miss% 0.8356627917829315
plot_id,batch_id 0 34 miss% 0.8365221326303725
plot_id,batch_id 0 35 miss% 0.7824088173000556
plot_id,batch_id 0 36 miss% 0.8289858874820516
plot_id,batch_id 0 37 miss% 0.8325170172220513
plot_id,batch_id 0 38 miss% 0.8362466197625885
plot_id,batch_id 0 39 miss% 0.836587452588411
plot_id,batch_id 0 40 miss% 0.8154104694672744
plot_id,batch_id 0 41 miss% 0.8324545527144438
plot_id,batch_id 0 42 miss% 0.8377931689162017
plot_id,batch_id 0 43 miss% 0.8379615443856393
plot_id,batch_id 0 44 miss% 0.8441046101308041
plot_id,batch_id 0 45 miss% 0.8120959486347167
plot_id,batch_id 0 46 miss% 0.8340228917254774
plot_id,batch_id 0 47 miss% 0.8386596530640759
plot_id,batch_id 0 48 miss% 0.8384005518273691
plot_id,batch_id 0 49 miss% 0.8415631240633351
plot_id,batch_id 0 50 miss% 0.8166566865697085
plot_id,batch_id 0 51 miss% 0.8334663752906272
plot_id,batch_id 0 52 miss% 0.8344397391212773
plot_id,batch_id 0 53 miss% 0.837978360180569
plot_id,batch_id 0 54 miss% 0.8414713024945347
plot_id,batch_id 0 55 miss% 0.8125977982951929
plot_id,batch_id 0 56 miss% 0.8323734054290588
plot_id,batch_id 0 57 miss% 0.8354940345847706
plot_id,batch_id 0 58 miss% 0.8383351216922513
plot_id,batch_id 0 59 miss% 0.8427226719248735
plot_id,batch_id 0 60 miss% 0.7130006683795989
plot_id,batch_id 0 61 miss% 0.801347612167013
plot_id,batch_id 0 62 miss% 0.8153736775016921
plot_id,batch_id 0 63 miss% 0.8270335815765122
plot_id,batch_id 0 64 miss% 0.8285134812440856
plot_id,batch_id 0 65 miss% 0.7051247232738718
plot_id,batch_id 0 66 miss% 0.7923325196397529
plot_id,batch_id 0 67 miss% 0.8083793219646799
plot_id,batch_id 0 68 miss% 0.82341912525503
plot_id,batch_id 0 69 miss% 0.8258480922961325
plot_id,batch_id 0 70 miss% 0.6694842362039515
plot_id,batch_id 0 71 miss% 0.785003727040748
plot_id,batch_id 0 72 miss% 0.8008444467499043
plot_id,batch_id 0 73 miss% 0.8136608903883661
plot_id,batch_id 0 74 miss% 0.8206084305128546
plot_id,batch_id 0 75 miss% 0.6678789541145177
plot_id,batch_id 0 76 miss% 0.797788910298804
plot_id,batch_id 0 77 miss% 0.7928986380191287
plot_id,batch_id 0 78 miss% 0.806379853387439
plot_id,batch_id 0 79 miss% 0.8144510453637219
plot_id,batch_id 0 80 miss% 0.7302319380680667
plot_id,batch_id 0 81 miss% 0.8178773996623944
plot_id,batch_id 0 82 miss% 0.8239767687394168
plot_id,batch_id 0 83 miss% 0.8306594615015215
plot_id,batch_id 0 84 miss% 0.8306811639143602
plot_id,batch_id 0 85 miss% 0.7241641056545541
plot_id,batch_id 0 86 miss% 0.8023263625000678
plot_id,batch_id 0 87 miss% 0.820211153531925
plot_id,batch_id 0 88 miss% 0.8291224214381131
plot_id,batch_id 0 89 miss% 0.8323456880034904
plot_id,batch_id 0 90 miss% 0.6919415409546025
plot_id,batch_id 0 91 miss% 0.7991597279161373
plot_id,batch_id 0 92 miss% 0.808732785860088
plot_id,batch_id 0 93 miss% 0.8174041999617286
plot_id,batch_id 0 94 miss% 0.8295537029702672
plot_id,batch_id 0 95 miss% 0.7122138347445814
plot_id,batch_id 0 96 miss% 0.7897489312452324
plot_id,batch_id 0 97 miss% 0.8071401670241953
plot_id,batch_id 0 98 miss% 0.8180638831497666
plot_id,batch_id 0 99 miss% 0.824294236221866
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.77421692 0.82191707 0.83051423 0.83296746 0.83496338 0.7660556
 0.81558324 0.82852136 0.83300143 0.8373715  0.74590401 0.81500227
 0.82527177 0.83040635 0.83373153 0.75973629 0.81209289 0.82546732
 0.8338319  0.8322845  0.7933118  0.82799744 0.83560105 0.8350387
 0.84177608 0.79614041 0.82761673 0.83222196 0.83427186 0.83631206
 0.78847695 0.82411369 0.83306105 0.83566279 0.83652213 0.78240882
 0.82898589 0.83251702 0.83624662 0.83658745 0.81541047 0.83245455
 0.83779317 0.83796154 0.84410461 0.81209595 0.83402289 0.83865965
 0.83840055 0.84156312 0.81665669 0.83346638 0.83443974 0.83797836
 0.8414713  0.8125978  0.83237341 0.83549403 0.83833512 0.84272267
 0.71300067 0.80134761 0.81537368 0.82703358 0.82851348 0.70512472
 0.79233252 0.80837932 0.82341913 0.82584809 0.66948424 0.78500373
 0.80084445 0.81366089 0.82060843 0.66787895 0.79778891 0.79289864
 0.80637985 0.81445105 0.73023194 0.8178774  0.82397677 0.83065946
 0.83068116 0.72416411 0.80232636 0.82021115 0.82912242 0.83234569
 0.69194154 0.79915973 0.80873279 0.8174042  0.8295537  0.71221383
 0.78974893 0.80714017 0.81806388 0.82429424]
for model  25 the mean error 0.8109093496270878
all id 25 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 21 out win 4 err 0.8109093496270878
Launcher: Job 26 completed in 9298 seconds.
Launcher: Task 206 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  35921
Epoch:0, Train loss:0.447953, valid loss:0.429300
Epoch:1, Train loss:0.030378, valid loss:0.005410
Epoch:2, Train loss:0.008325, valid loss:0.003586
Epoch:3, Train loss:0.005743, valid loss:0.002769
Epoch:4, Train loss:0.004872, valid loss:0.002253
Epoch:5, Train loss:0.004244, valid loss:0.002278
Epoch:6, Train loss:0.003902, valid loss:0.002091
Epoch:7, Train loss:0.003517, valid loss:0.001970
Epoch:8, Train loss:0.003358, valid loss:0.001983
Epoch:9, Train loss:0.003138, valid loss:0.001429
Epoch:10, Train loss:0.002915, valid loss:0.001729
Epoch:11, Train loss:0.002312, valid loss:0.001248
Epoch:12, Train loss:0.002241, valid loss:0.001275
Epoch:13, Train loss:0.002201, valid loss:0.001240
Epoch:14, Train loss:0.002147, valid loss:0.001218
Epoch:15, Train loss:0.002089, valid loss:0.001174
Epoch:16, Train loss:0.002049, valid loss:0.001273
Epoch:17, Train loss:0.002003, valid loss:0.001060
Epoch:18, Train loss:0.001998, valid loss:0.001107
Epoch:19, Train loss:0.001965, valid loss:0.001086
Epoch:20, Train loss:0.001900, valid loss:0.001151
Epoch:21, Train loss:0.001599, valid loss:0.000956
Epoch:22, Train loss:0.001583, valid loss:0.000980
Epoch:23, Train loss:0.001571, valid loss:0.000968
Epoch:24, Train loss:0.001554, valid loss:0.001039
Epoch:25, Train loss:0.001533, valid loss:0.000977
Epoch:26, Train loss:0.001508, valid loss:0.000937
Epoch:27, Train loss:0.001533, valid loss:0.000962
Epoch:28, Train loss:0.001516, valid loss:0.000954
Epoch:29, Train loss:0.001483, valid loss:0.000991
Epoch:30, Train loss:0.001477, valid loss:0.000875
Epoch:31, Train loss:0.001301, valid loss:0.000893
Epoch:32, Train loss:0.001301, valid loss:0.000782
Epoch:33, Train loss:0.001290, valid loss:0.000829
Epoch:34, Train loss:0.001296, valid loss:0.000830
Epoch:35, Train loss:0.001279, valid loss:0.000841
Epoch:36, Train loss:0.001294, valid loss:0.000848
Epoch:37, Train loss:0.001275, valid loss:0.000828
Epoch:38, Train loss:0.001272, valid loss:0.000859
Epoch:39, Train loss:0.001261, valid loss:0.000818
Epoch:40, Train loss:0.001251, valid loss:0.000884
Epoch:41, Train loss:0.001169, valid loss:0.000788
Epoch:42, Train loss:0.001156, valid loss:0.000792
Epoch:43, Train loss:0.001155, valid loss:0.000780
Epoch:44, Train loss:0.001153, valid loss:0.000834
Epoch:45, Train loss:0.001147, valid loss:0.000795
Epoch:46, Train loss:0.001144, valid loss:0.000784
Epoch:47, Train loss:0.001135, valid loss:0.000792
Epoch:48, Train loss:0.001134, valid loss:0.000755
Epoch:49, Train loss:0.001139, valid loss:0.000787
Epoch:50, Train loss:0.001126, valid loss:0.000783
Epoch:51, Train loss:0.001083, valid loss:0.000753
Epoch:52, Train loss:0.001082, valid loss:0.000767
Epoch:53, Train loss:0.001079, valid loss:0.000771
Epoch:54, Train loss:0.001078, valid loss:0.000747
Epoch:55, Train loss:0.001071, valid loss:0.000744
Epoch:56, Train loss:0.001076, valid loss:0.000745
Epoch:57, Train loss:0.001068, valid loss:0.000748
Epoch:58, Train loss:0.001069, valid loss:0.000735
Epoch:59, Train loss:0.001077, valid loss:0.000750
Epoch:60, Train loss:0.001065, valid loss:0.000748
training time 9105.22449374199
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.041963158773055
plot_id,batch_id 0 1 miss% 0.03134069334413083
plot_id,batch_id 0 2 miss% 0.024252327101982513
plot_id,batch_id 0 3 miss% 0.02068806044009269
plot_id,batch_id 0 4 miss% 0.034681209294020174
plot_id,batch_id 0 5 miss% 0.03920962543775838
plot_id,batch_id 0 6 miss% 0.02370534388658014
plot_id,batch_id 0 7 miss% 0.033438723467145125
plot_id,batch_id 0 8 miss% 0.03661438185713673
plot_id,batch_id 0 9 miss% 0.020176879464074853
plot_id,batch_id 0 10 miss% 0.03754012119875214
plot_id,batch_id 0 11 miss% 0.03897476267867269
plot_id,batch_id 0 12 miss% 0.031226237255724727
plot_id,batch_id 0 13 miss% 0.02748369410212457
plot_id,batch_id 0 14 miss% 0.03587847114854404
plot_id,batch_id 0 15 miss% 0.047757694390440655
plot_id,batch_id 0 16 miss% 0.037128493666009996
plot_id,batch_id 0 17 miss% 0.03535758980520315
plot_id,batch_id 0 18 miss% 0.039334368098178286
plot_id,batch_id 0 19 miss% 0.03674585786513542
plot_id,batch_id 0 20 miss% 0.03207228119048011
plot_id,batch_id 0 21 miss% 0.01806346591023426
plot_id,batch_id 0 22 miss% 0.02028186505948211
plot_id,batch_id 0 23 miss% 0.03203258269721273
plot_id,batch_id 0 24 miss% 0.03272403305378717
plot_id,batch_id 0 25 miss% 0.03763817130842622
plot_id,batch_id 0 26 miss% 0.02135432825873011
plot_id,batch_id 0 27 miss% 0.028507351818996236
plot_id,batch_id 0 28 miss% 0.03230694488880193
plot_id,batch_id 0 29 miss% 0.028596558050715638
plot_id,batch_id 0 30 miss% 0.03388218049377662
plot_id,batch_id 0 31 miss% 0.032360767171389915
plot_id,batch_id 0 32 miss% 0.03656000811203302
plot_id,batch_id 0 33 miss% 0.017750164630821828
plot_id,batch_id 0 34 miss% 0.03033606536953004
plot_id,batch_id 0 35 miss% 0.03233377675698998
plot_id,batch_id 0 36 miss% 0.05294467747385514
plot_id,batch_id 0 37 miss% 0.0291794166586205
plot_id,batch_id 0 38 miss% 0.02912799548213926
plot_id,batch_id 0 39 miss% 0.02922900315258581
plot_id,batch_id 0 40 miss% 0.06373869837082989
plot_id,batch_id 0 41 miss% 0.02809974344934833
plot_id,batch_id 0 42 miss% 0.017784977926592685
plot_id,batch_id 0 43 miss% 0.019023998697930245
plot_id,batch_id 0 44 miss% 0.02553953879497182
plot_id,batch_id 0 45 miss% 0.028376800043768323
plot_id,batch_id 0 46 miss% 0.025266335481101437
plot_id,batch_id 0 47 miss% 0.018119401604184016
plot_id,batch_id 0 48 miss% 0.02085471082358761
plot_id,batch_id 0 49 miss% 0.02385679893612867
plot_id,batch_id 0 50 miss% 0.03550583953258663
plot_id,batch_id 0 51 miss% 0.029250907237860207
plot_id,batch_id 0 52 miss% 0.025739473657950766
plot_id,batch_id 0 53 miss% 0.02049285368765819
plot_id,batch_id 0 54 miss% 0.03603202202602911
plot_id,batch_id 0 55 miss% 0.04552610843221057
plot_id,batch_id 0 56 miss% 0.02932119441525294
plot_id,batch_id 0 57 miss% 0.022089083035370036
plot_id,batch_id 0 58 miss% 0.024239385469247456
plot_id,batch_id 0 59 miss% 0.022054456837703795
plot_id,batch_id 0 60 miss% 0.034322484680571114
plot_id,batch_id 0 61 miss% 0.01733272497164062
plot_id,batch_id 0 62 miss% 0.02557169156722938
plot_id,batch_id 0 63 miss% 0.030909244603394538
plot_id,batch_id 0 64 miss% 0.018805064575897607
plot_id,batch_id 0 65 miss% 0.03884585700738329
plot_id,batch_id 0 66 miss% 0.024235599936216882
plot_id,batch_id 0 67 miss% 0.030623226663696386
plot_id,batch_id 0 68 miss% 0.026141444556270106
plot_id,batch_id 0 69 miss% 0.01704144564964756
plot_id,batch_id 0 70 miss% 0.026204392151891385
plot_id,batch_id 0 71 miss% 0.032822809475662656
plot_id,batch_id 0 72 miss% 0.04021361442295189
plot_id,batch_id 0 73 miss% 0.035095790162421675
plot_id,batch_id 0 74 miss% 0.03006210136415032
plot_id,batch_id 0 75 miss% 0.0431080907041203
plot_id,batch_id 0 76 miss% 0.04202752669494825
plot_id,batch_id 0 77 miss% 0.025739303540414066
plot_id,batch_id 0 78 miss% 0.04836112938558829
plot_id,batch_id 0 79 miss% 0.04192326820108447
plot_id,batch_id 0 80 miss% 0.04580047119861458
plot_id,batch_id 0 81 miss% 0.022035015318050154
plot_id,batch_id 0 82 miss% 0.020341596715480637
plot_id,batch_id 0 83 miss% 0.036999415621383425
plot_id,batch_id 0 84 miss% 0.03679574339558673
plot_id,batch_id 0 85 miss% 0.05506987313334291
plot_id,batch_id 0 86 miss% 0.03141614434275433
plot_id,batch_id 0 87 miss% 0.028968037436231112
plot_id,batch_id 0 88 miss% 0.0425779943136579
plot_id,batch_id 0 89 miss% 0.041394763338536776
plot_id,batch_id 0 90 miss% 0.039352838403476226
plot_id,batch_id 0 91 miss% 0.038191932713966856
plot_id,batch_id 0 92 miss% 0.035977416038039604
plot_id,batch_id 0 93 miss% 0.02958132694390007
plot_id,batch_id 0 94 miss% 0.050891632231384376
plot_id,batch_id 0 95 miss% 0.06010372313694957
plot_id,batch_id 0 96 miss% 0.03799150046992295
plot_id,batch_id 0 97 miss% 0.06508344671326155
plot_id,batch_id 0 98 miss% 0.046765070985942715
plot_id,batch_id 0 99 miss% 0.03826696757839524
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.04196316 0.03134069 0.02425233 0.02068806 0.03468121 0.03920963
 0.02370534 0.03343872 0.03661438 0.02017688 0.03754012 0.03897476
 0.03122624 0.02748369 0.03587847 0.04775769 0.03712849 0.03535759
 0.03933437 0.03674586 0.03207228 0.01806347 0.02028187 0.03203258
 0.03272403 0.03763817 0.02135433 0.02850735 0.03230694 0.02859656
 0.03388218 0.03236077 0.03656001 0.01775016 0.03033607 0.03233378
 0.05294468 0.02917942 0.029128   0.029229   0.0637387  0.02809974
 0.01778498 0.019024   0.02553954 0.0283768  0.02526634 0.0181194
 0.02085471 0.0238568  0.03550584 0.02925091 0.02573947 0.02049285
 0.03603202 0.04552611 0.02932119 0.02208908 0.02423939 0.02205446
 0.03432248 0.01733272 0.02557169 0.03090924 0.01880506 0.03884586
 0.0242356  0.03062323 0.02614144 0.01704145 0.02620439 0.03282281
 0.04021361 0.03509579 0.0300621  0.04310809 0.04202753 0.0257393
 0.04836113 0.04192327 0.04580047 0.02203502 0.0203416  0.03699942
 0.03679574 0.05506987 0.03141614 0.02896804 0.04257799 0.04139476
 0.03935284 0.03819193 0.03597742 0.02958133 0.05089163 0.06010372
 0.0379915  0.06508345 0.04676507 0.03826697]
for model  182 the mean error 0.03274683379617742
all id 182 hidden_dim 16 learning_rate 0.005 num_layers 5 frames 31 out win 5 err 0.03274683379617742
Launcher: Job 183 completed in 9302 seconds.
Launcher: Task 185 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  107025
Epoch:0, Train loss:0.430855, valid loss:0.425762
Epoch:1, Train loss:0.034712, valid loss:0.005975
Epoch:2, Train loss:0.009163, valid loss:0.004675
Epoch:3, Train loss:0.006832, valid loss:0.003379
Epoch:4, Train loss:0.005293, valid loss:0.003420
Epoch:5, Train loss:0.004521, valid loss:0.002512
Epoch:6, Train loss:0.003938, valid loss:0.002438
Epoch:7, Train loss:0.003676, valid loss:0.002021
Epoch:8, Train loss:0.003516, valid loss:0.001690
Epoch:9, Train loss:0.003259, valid loss:0.001458
Epoch:10, Train loss:0.002995, valid loss:0.001987
Epoch:11, Train loss:0.002168, valid loss:0.001235
Epoch:12, Train loss:0.002091, valid loss:0.001376
Epoch:13, Train loss:0.002094, valid loss:0.001439
Epoch:14, Train loss:0.002058, valid loss:0.001409
Epoch:15, Train loss:0.002003, valid loss:0.001230
Epoch:16, Train loss:0.001967, valid loss:0.001199
Epoch:17, Train loss:0.001886, valid loss:0.001350
Epoch:18, Train loss:0.001863, valid loss:0.001149
Epoch:19, Train loss:0.001785, valid loss:0.001311
Epoch:20, Train loss:0.001824, valid loss:0.001050
Epoch:21, Train loss:0.001336, valid loss:0.001057
Epoch:22, Train loss:0.001360, valid loss:0.001001
Epoch:23, Train loss:0.001357, valid loss:0.000955
Epoch:24, Train loss:0.001327, valid loss:0.000912
Epoch:25, Train loss:0.001335, valid loss:0.000895
Epoch:26, Train loss:0.001299, valid loss:0.000868
Epoch:27, Train loss:0.001278, valid loss:0.000876
Epoch:28, Train loss:0.001264, valid loss:0.001180
Epoch:29, Train loss:0.001308, valid loss:0.000842
Epoch:30, Train loss:0.001231, valid loss:0.000917
Epoch:31, Train loss:0.001027, valid loss:0.000843
Epoch:32, Train loss:0.001034, valid loss:0.000859
Epoch:33, Train loss:0.001019, valid loss:0.000845
Epoch:34, Train loss:0.001016, valid loss:0.000793
Epoch:35, Train loss:0.001029, valid loss:0.000856
Epoch:36, Train loss:0.000992, valid loss:0.000824
Epoch:37, Train loss:0.000989, valid loss:0.000845
Epoch:38, Train loss:0.001004, valid loss:0.000759
Epoch:39, Train loss:0.000986, valid loss:0.000785
Epoch:40, Train loss:0.001018, valid loss:0.000811
Epoch:41, Train loss:0.000879, valid loss:0.000756
Epoch:42, Train loss:0.000872, valid loss:0.000773
Epoch:43, Train loss:0.000871, valid loss:0.000780
Epoch:44, Train loss:0.000882, valid loss:0.000787
Epoch:45, Train loss:0.000861, valid loss:0.000792
Epoch:46, Train loss:0.000856, valid loss:0.000786
Epoch:47, Train loss:0.000868, valid loss:0.000772
Epoch:48, Train loss:0.000860, valid loss:0.000774
Epoch:49, Train loss:0.000857, valid loss:0.000752
Epoch:50, Train loss:0.000853, valid loss:0.000754
Epoch:51, Train loss:0.000808, valid loss:0.000748
Epoch:52, Train loss:0.000805, valid loss:0.000744
Epoch:53, Train loss:0.000804, valid loss:0.000739
Epoch:54, Train loss:0.000798, valid loss:0.000741
Epoch:55, Train loss:0.000799, valid loss:0.000731
Epoch:56, Train loss:0.000795, valid loss:0.000733
Epoch:57, Train loss:0.000796, valid loss:0.000740
Epoch:58, Train loss:0.000792, valid loss:0.000742
Epoch:59, Train loss:0.000795, valid loss:0.000763
Epoch:60, Train loss:0.000785, valid loss:0.000785
training time 9119.264971971512
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02402271903065364
plot_id,batch_id 0 1 miss% 0.018508062307749602
plot_id,batch_id 0 2 miss% 0.02313422104401884
plot_id,batch_id 0 3 miss% 0.026984810920573702
plot_id,batch_id 0 4 miss% 0.02195940432254223
plot_id,batch_id 0 5 miss% 0.03150912653375471
plot_id,batch_id 0 6 miss% 0.023951458014212376
plot_id,batch_id 0 7 miss% 0.02651694241213309
plot_id,batch_id 0 8 miss% 0.028287924098577444
plot_id,batch_id 0 9 miss% 0.021650429580189458
plot_id,batch_id 0 10 miss% 0.043700843253332664
plot_id,batch_id 0 11 miss% 0.03536917407728054
plot_id,batch_id 0 12 miss% 0.028973565394955367
plot_id,batch_id 0 13 miss% 0.029848360173828133
plot_id,batch_id 0 14 miss% 0.031574579869975555
plot_id,batch_id 0 15 miss% 0.03302303074312442
plot_id,batch_id 0 16 miss% 0.037378795136976346
plot_id,batch_id 0 17 miss% 0.02670802005581712
plot_id,batch_id 0 18 miss% 0.039864397944078596
plot_id,batch_id 0 19 miss% 0.030466288335768397
plot_id,batch_id 0 20 miss% 0.043561081413023756
plot_id,batch_id 0 21 miss% 0.015783284505412002
plot_id,batch_id 0 22 miss% 0.0312524064674805
plot_id,batch_id 0 23 miss% 0.030514526272117422
plot_id,batch_id 0 24 miss% 0.03488357228177058
plot_id,batch_id 0 25 miss% 0.045994859462521596
plot_id,batch_id 0 26 miss% 0.029632218976993818
plot_id,batch_id 0 27 miss% 0.025310705639984516
plot_id,batch_id 0 28 miss% 0.026677737927357517
plot_id,batch_id 0 29 miss% 0.028263667000290427
plot_id,batch_id 0 30 miss% 0.022562874095335465
plot_id,batch_id 0 31 miss% 0.03154213736107932
plot_id,batch_id 0 32 miss% 0.027443000366881222
plot_id,batch_id 0 33 miss% 0.03288196769901134
plot_id,batch_id 0 34 miss% 0.026653699452591013
plot_id,batch_id 0 35 miss% 0.038855824843645234
plot_id,batch_id 0 36 miss% 0.040656876446293955
plot_id,batch_id 0 37 miss% 0.02748922582483136
plot_id,batch_id 0 38 miss% 0.024692084032959014
plot_id,batch_id 0 39 miss% 0.02275705359201111
plot_id,batch_id 0 40 miss% 0.09163154748028379
plot_id,batch_id 0 41 miss% 0.023808860819666735
plot_id,batch_id 0 42 miss% 0.021394589951480577
plot_id,batch_id 0 43 miss% 0.03141021261498545
plot_id,batch_id 0 44 miss% 0.026431357575578275
plot_id,batch_id 0 45 miss% 0.04036715546365792
plot_id,batch_id 0 46 miss% 0.020060655003402185
plot_id,batch_id 0 47 miss% 0.02227021489879062
plot_id,batch_id 0 48 miss% 0.029281363210250426
plot_id,batch_id 0 49 miss% 0.023672937872178033
plot_id,batch_id 0 50 miss% 0.026171096103557823
plot_id,batch_id 0 51 miss% 0.021583108890069334
plot_id,batch_id 0 52 miss% 0.025762506430424978
plot_id,batch_id 0 53 miss% 0.018319541140816804
plot_id,batch_id 0 54 miss% 0.034245290674751315
plot_id,batch_id 0 55 miss% 0.04974313290360633
plot_id,batch_id 0 56 miss% 0.02804336240351214
plot_id,batch_id 0 57 miss% 0.017980977359104137
plot_id,batch_id 0 58 miss% 0.023448386288627422
plot_id,batch_id 0 59 miss% 0.023863263861512853
plot_id,batch_id 0 60 miss% 0.028417063711643317
plot_id,batch_id 0 61 miss% 0.03036823010393723
plot_id,batch_id 0 62 miss% 0.02645227855931829
plot_id,batch_id 0 63 miss% 0.0356718222357121
plot_id,batch_id 0 64 miss% 0.0397154914667132
plot_id,batch_id 0 65 miss% 0.032390699836877535
plot_id,batch_id 0 66 miss% 0.0382661455999859
plot_id,batch_id 0 67 miss% 0.03324416510240106
plot_id,batch_id 0 68 miss% 0.025909428479900763
plot_id,batch_id 0 69 miss% 0.020764390817903294
plot_id,batch_id 0 70 miss% 0.044247027339458934
plot_id,batch_id 0 71 miss% 0.0450832975466966
plot_id,batch_id 0 72 miss% 0.029866004484672113
plot_id,batch_id 0 73 miss% 0.03182141918538854
plot_id,batch_id 0 74 miss% 0.03061882417747748
plot_id,batch_id 0 75 miss% 0.022850742023664602
plot_id,batch_id 0 76 miss% 0.05195545543791711
plot_id,batch_id 0 77 miss% 0.041419156353835085
plot_id,batch_id 0 78 miss% 0.037781160920016
plot_id,batch_id 0 79 miss% 0.03328048738561263
plot_id,batch_id 0 80 miss% 0.04392056631910788
plot_id,batch_id 0 81 miss% 0.019984694737090882
plot_id,batch_id 0 82 miss% 0.035681429461625745
plot_id,batch_id 0 83 miss% 0.022833077745885427
plot_id,batch_id 0 84 miss% 0.03163627269012035
plot_id,batch_id 0 85 miss% 0.04383149776493338
plot_id,batch_id 0 86 miss% 0.02372804762346926
plot_id,batch_id 0 87 miss% 0.03589045245386289
plot_id,batch_id 0 88 miss% 0.0406298012099434
plot_id,batch_id 0 89 miss% 0.02019729656500785
plot_id,batch_id 0 90 miss% 0.04094241528100518
plot_id,batch_id 0 91 miss% 0.04357076438784566
plot_id,batch_id 0 92 miss% 0.03111030552274221
plot_id,batch_id 0 93 miss% 0.01937354379280665
plot_id,batch_id 0 94 miss% 0.02941023237979052
plot_id,batch_id 0 95 miss% 0.0565617098261773
plot_id,batch_id 0 96 miss% 0.04706890888055992
plot_id,batch_id 0 97 miss% 0.044482023302570256
plot_id,batch_id 0 98 miss% 0.0316881292174939
plot_id,batch_id 0 99 miss% 0.032536826697554484
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02402272 0.01850806 0.02313422 0.02698481 0.0219594  0.03150913
 0.02395146 0.02651694 0.02828792 0.02165043 0.04370084 0.03536917
 0.02897357 0.02984836 0.03157458 0.03302303 0.0373788  0.02670802
 0.0398644  0.03046629 0.04356108 0.01578328 0.03125241 0.03051453
 0.03488357 0.04599486 0.02963222 0.02531071 0.02667774 0.02826367
 0.02256287 0.03154214 0.027443   0.03288197 0.0266537  0.03885582
 0.04065688 0.02748923 0.02469208 0.02275705 0.09163155 0.02380886
 0.02139459 0.03141021 0.02643136 0.04036716 0.02006066 0.02227021
 0.02928136 0.02367294 0.0261711  0.02158311 0.02576251 0.01831954
 0.03424529 0.04974313 0.02804336 0.01798098 0.02344839 0.02386326
 0.02841706 0.03036823 0.02645228 0.03567182 0.03971549 0.0323907
 0.03826615 0.03324417 0.02590943 0.02076439 0.04424703 0.0450833
 0.029866   0.03182142 0.03061882 0.02285074 0.05195546 0.04141916
 0.03778116 0.03328049 0.04392057 0.01998469 0.03568143 0.02283308
 0.03163627 0.0438315  0.02372805 0.03589045 0.0406298  0.0201973
 0.04094242 0.04357076 0.03111031 0.01937354 0.02941023 0.05656171
 0.04706891 0.04448202 0.03168813 0.03253683]
for model  97 the mean error 0.03165531804486116
all id 97 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 25 out win 4 err 0.03165531804486116
Launcher: Job 98 completed in 9320 seconds.
Launcher: Task 146 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  134801
Epoch:0, Train loss:0.648029, valid loss:0.652804
Epoch:1, Train loss:0.512986, valid loss:0.518933
Epoch:2, Train loss:0.498162, valid loss:0.517377
Epoch:3, Train loss:0.495388, valid loss:0.516980
Epoch:4, Train loss:0.493652, valid loss:0.515992
Epoch:5, Train loss:0.492780, valid loss:0.515552
Epoch:6, Train loss:0.492358, valid loss:0.514944
Epoch:7, Train loss:0.491824, valid loss:0.516057
Epoch:8, Train loss:0.491882, valid loss:0.515502
Epoch:9, Train loss:0.491699, valid loss:0.515656
Epoch:10, Train loss:0.491241, valid loss:0.515256
Epoch:11, Train loss:0.490118, valid loss:0.514420
Epoch:12, Train loss:0.489962, valid loss:0.514354
Epoch:13, Train loss:0.489986, valid loss:0.514367
Epoch:14, Train loss:0.489886, valid loss:0.514564
Epoch:15, Train loss:0.489963, valid loss:0.514464
Epoch:16, Train loss:0.489860, valid loss:0.514554
Epoch:17, Train loss:0.489739, valid loss:0.514798
Epoch:18, Train loss:0.489747, valid loss:0.514155
Epoch:19, Train loss:0.489722, valid loss:0.514551
Epoch:20, Train loss:0.489758, valid loss:0.514465
Epoch:21, Train loss:0.489049, valid loss:0.514110
Epoch:22, Train loss:0.489024, valid loss:0.514056
Epoch:23, Train loss:0.489018, valid loss:0.514284
Epoch:24, Train loss:0.489036, valid loss:0.514011
Epoch:25, Train loss:0.489029, valid loss:0.514243
Epoch:26, Train loss:0.488966, valid loss:0.514277
Epoch:27, Train loss:0.489204, valid loss:0.514116
Epoch:28, Train loss:0.488957, valid loss:0.514175
Epoch:29, Train loss:0.488917, valid loss:0.513999
Epoch:30, Train loss:0.488933, valid loss:0.514176
Epoch:31, Train loss:0.488620, valid loss:0.513905
Epoch:32, Train loss:0.488610, valid loss:0.513878
Epoch:33, Train loss:0.488615, valid loss:0.513922
Epoch:34, Train loss:0.488593, valid loss:0.513964
Epoch:35, Train loss:0.488604, valid loss:0.513894
Epoch:36, Train loss:0.488607, valid loss:0.513893
Epoch:37, Train loss:0.488576, valid loss:0.513983
Epoch:38, Train loss:0.488587, valid loss:0.513868
Epoch:39, Train loss:0.488584, valid loss:0.514012
Epoch:40, Train loss:0.488557, valid loss:0.513958
Epoch:41, Train loss:0.488430, valid loss:0.513829
Epoch:42, Train loss:0.488426, valid loss:0.513877
Epoch:43, Train loss:0.488416, valid loss:0.513860
Epoch:44, Train loss:0.488412, valid loss:0.513829
Epoch:45, Train loss:0.488421, valid loss:0.513891
Epoch:46, Train loss:0.488400, valid loss:0.513856
Epoch:47, Train loss:0.488410, valid loss:0.513875
Epoch:48, Train loss:0.488396, valid loss:0.513844
Epoch:49, Train loss:0.488402, valid loss:0.513797
Epoch:50, Train loss:0.488386, valid loss:0.513823
Epoch:51, Train loss:0.488339, valid loss:0.513828
Epoch:52, Train loss:0.488329, valid loss:0.513817
Epoch:53, Train loss:0.488324, valid loss:0.513843
Epoch:54, Train loss:0.488325, valid loss:0.513814
Epoch:55, Train loss:0.488325, valid loss:0.513830
Epoch:56, Train loss:0.488319, valid loss:0.513828
Epoch:57, Train loss:0.488316, valid loss:0.513814
Epoch:58, Train loss:0.488317, valid loss:0.513854
Epoch:59, Train loss:0.488314, valid loss:0.513827
Epoch:60, Train loss:0.488317, valid loss:0.513787
training time 9171.661073923111
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.7759811511171376
plot_id,batch_id 0 1 miss% 0.8223820725966825
plot_id,batch_id 0 2 miss% 0.8314432928018831
plot_id,batch_id 0 3 miss% 0.8336055617284939
plot_id,batch_id 0 4 miss% 0.8357824055848556
plot_id,batch_id 0 5 miss% 0.769476021090714
plot_id,batch_id 0 6 miss% 0.8196262342755399
plot_id,batch_id 0 7 miss% 0.8293506316596759
plot_id,batch_id 0 8 miss% 0.8336597071925441
plot_id,batch_id 0 9 miss% 0.8378337703999288
plot_id,batch_id 0 10 miss% 0.7483979736293371
plot_id,batch_id 0 11 miss% 0.8147106032946739
plot_id,batch_id 0 12 miss% 0.8274472513201412
plot_id,batch_id 0 13 miss% 0.8314846507933189
plot_id,batch_id 0 14 miss% 0.8334260999992874
plot_id,batch_id 0 15 miss% 0.7618947696099069
plot_id,batch_id 0 16 miss% 0.8139617556105363
plot_id,batch_id 0 17 miss% 0.8266878748232727
plot_id,batch_id 0 18 miss% 0.8351925695748359
plot_id,batch_id 0 19 miss% 0.8338368960151505
plot_id,batch_id 0 20 miss% 0.7937468638467388
plot_id,batch_id 0 21 miss% 0.8299843684216962
plot_id,batch_id 0 22 miss% 0.8366139959684277
plot_id,batch_id 0 23 miss% 0.8373299360764677
plot_id,batch_id 0 24 miss% 0.8423528562328794
plot_id,batch_id 0 25 miss% 0.799013244517737
plot_id,batch_id 0 26 miss% 0.8277214577906796
plot_id,batch_id 0 27 miss% 0.8322162659137817
plot_id,batch_id 0 28 miss% 0.8358870184304956
plot_id,batch_id 0 29 miss% 0.8365073039018672
plot_id,batch_id 0 30 miss% 0.787782476435866
plot_id,batch_id 0 31 miss% 0.8232130969136379
plot_id,batch_id 0 32 miss% 0.8325811286822064
plot_id,batch_id 0 33 miss% 0.8357240486962539
plot_id,batch_id 0 34 miss% 0.8368090771685227
plot_id,batch_id 0 35 miss% 0.7845988165800992
plot_id,batch_id 0 36 miss% 0.8280893351942804
plot_id,batch_id 0 37 miss% 0.8316489985540068
plot_id,batch_id 0 38 miss% 0.8398152708877175
plot_id,batch_id 0 39 miss% 0.8379989156014775
plot_id,batch_id 0 40 miss% 0.8133323822101375
plot_id,batch_id 0 41 miss% 0.8338252433156098
plot_id,batch_id 0 42 miss% 0.8365806359615794
plot_id,batch_id 0 43 miss% 0.8402788139374607
plot_id,batch_id 0 44 miss% 0.8448066408403065
plot_id,batch_id 0 45 miss% 0.8122940468547148
plot_id,batch_id 0 46 miss% 0.8341020214178845
plot_id,batch_id 0 47 miss% 0.8392257487705151
plot_id,batch_id 0 48 miss% 0.8399362457679772
plot_id,batch_id 0 49 miss% 0.8435103723284446
plot_id,batch_id 0 50 miss% 0.8214350632112908
plot_id,batch_id 0 51 miss% 0.8331578669252712
plot_id,batch_id 0 52 miss% 0.8346544563802729
plot_id,batch_id 0 53 miss% 0.8395919779509722
plot_id,batch_id 0 54 miss% 0.844361030486195
plot_id,batch_id 0 55 miss% 0.8124882173640253
plot_id,batch_id 0 56 miss% 0.8322192416039442
plot_id,batch_id 0 57 miss% 0.8366748109506731
plot_id,batch_id 0 58 miss% 0.8399729766834051
plot_id,batch_id 0 59 miss% 0.8444158593814756
plot_id,batch_id 0 60 miss% 0.714639140576767
plot_id,batch_id 0 61 miss% 0.8033513481916548
plot_id,batch_id 0 62 miss% 0.815504635598833
plot_id,batch_id 0 63 miss% 0.8260438926632371
plot_id,batch_id 0 64 miss% 0.8277911770655599
plot_id,batch_id 0 65 miss% 0.7110760140252858
plot_id,batch_id 0 66 miss% 0.7865482007283227
plot_id,batch_id 0 67 miss% 0.8086462970673771
plot_id,batch_id 0 68 miss% 0.8240741973105279
plot_id,batch_id 0 69 miss% 0.8267031724334417
plot_id,batch_id 0 70 miss% 0.6686310865170075
plot_id,batch_id 0 71 miss% 0.7949192331590107
plot_id,batch_id 0 72 miss% 0.8011002025888122
plot_id,batch_id 0 73 miss% 0.811934522601794
plot_id,batch_id 0 74 miss% 0.8211406817299474
plot_id,batch_id 0 75 miss% 0.6717161296932506
plot_id,batch_id 0 76 miss% 0.7889082493056404
plot_id,batch_id 0 77 miss% 0.7938432416161502
plot_id,batch_id 0 78 miss% 0.8096875171684244
plot_id,batch_id 0 79 miss% 0.8145778316594975
plot_id,batch_id 0 80 miss% 0.7316323256355387
plot_id,batch_id 0 81 miss% 0.816076548783123
plot_id,batch_id 0 82 miss% 0.8251262180544121
plot_id,batch_id 0 83 miss% 0.830561851912186
plot_id,batch_id 0 84 miss% 0.8326622648809896
plot_id,batch_id 0 85 miss% 0.7259726584847392
plot_id,batch_id 0 86 miss% 0.8018632268702744
plot_id,batch_id 0 87 miss% 0.8180311277252148
plot_id,batch_id 0 88 miss% 0.8285064598111361
plot_id,batch_id 0 89 miss% 0.831182526573151
plot_id,batch_id 0 90 miss% 0.690776117357661
plot_id,batch_id 0 91 miss% 0.7969460430915976
plot_id,batch_id 0 92 miss% 0.810093388532552
plot_id,batch_id 0 93 miss% 0.8198182394445347
plot_id,batch_id 0 94 miss% 0.8300731829640188
plot_id,batch_id 0 95 miss% 0.7165711713818399
plot_id,batch_id 0 96 miss% 0.7853645644907672
plot_id,batch_id 0 97 miss% 0.8100054956085163
plot_id,batch_id 0 98 miss% 0.8188889123651777
plot_id,batch_id 0 99 miss% 0.8227150913257063
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.77598115 0.82238207 0.83144329 0.83360556 0.83578241 0.76947602
 0.81962623 0.82935063 0.83365971 0.83783377 0.74839797 0.8147106
 0.82744725 0.83148465 0.8334261  0.76189477 0.81396176 0.82668787
 0.83519257 0.8338369  0.79374686 0.82998437 0.836614   0.83732994
 0.84235286 0.79901324 0.82772146 0.83221627 0.83588702 0.8365073
 0.78778248 0.8232131  0.83258113 0.83572405 0.83680908 0.78459882
 0.82808934 0.831649   0.83981527 0.83799892 0.81333238 0.83382524
 0.83658064 0.84027881 0.84480664 0.81229405 0.83410202 0.83922575
 0.83993625 0.84351037 0.82143506 0.83315787 0.83465446 0.83959198
 0.84436103 0.81248822 0.83221924 0.83667481 0.83997298 0.84441586
 0.71463914 0.80335135 0.81550464 0.82604389 0.82779118 0.71107601
 0.7865482  0.8086463  0.8240742  0.82670317 0.66863109 0.79491923
 0.8011002  0.81193452 0.82114068 0.67171613 0.78890825 0.79384324
 0.80968752 0.81457783 0.73163233 0.81607655 0.82512622 0.83056185
 0.83266226 0.72597266 0.80186323 0.81803113 0.82850646 0.83118253
 0.69077612 0.79694604 0.81009339 0.81981824 0.83007318 0.71657117
 0.78536456 0.8100055  0.81888891 0.82271509]
for model  52 the mean error 0.811663836082686
all id 52 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 21 out win 4 err 0.811663836082686
Launcher: Job 53 completed in 9351 seconds.
Launcher: Task 213 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  107025
Epoch:0, Train loss:0.304599, valid loss:0.297865
Epoch:1, Train loss:0.021968, valid loss:0.004460
Epoch:2, Train loss:0.005775, valid loss:0.002494
Epoch:3, Train loss:0.003941, valid loss:0.001981
Epoch:4, Train loss:0.003206, valid loss:0.002483
Epoch:5, Train loss:0.003028, valid loss:0.002211
Epoch:6, Train loss:0.002662, valid loss:0.001441
Epoch:7, Train loss:0.002589, valid loss:0.001572
Epoch:8, Train loss:0.002437, valid loss:0.001373
Epoch:9, Train loss:0.002436, valid loss:0.001329
Epoch:10, Train loss:0.002265, valid loss:0.001204
Epoch:11, Train loss:0.001500, valid loss:0.001105
Epoch:12, Train loss:0.001504, valid loss:0.001154
Epoch:13, Train loss:0.001470, valid loss:0.001052
Epoch:14, Train loss:0.001395, valid loss:0.001004
Epoch:15, Train loss:0.001441, valid loss:0.000963
Epoch:16, Train loss:0.001356, valid loss:0.000861
Epoch:17, Train loss:0.001376, valid loss:0.000837
Epoch:18, Train loss:0.001332, valid loss:0.000981
Epoch:19, Train loss:0.001372, valid loss:0.000668
Epoch:20, Train loss:0.001303, valid loss:0.000837
Epoch:21, Train loss:0.000933, valid loss:0.000756
Epoch:22, Train loss:0.000935, valid loss:0.000656
Epoch:23, Train loss:0.000945, valid loss:0.000546
Epoch:24, Train loss:0.000928, valid loss:0.000653
Epoch:25, Train loss:0.000898, valid loss:0.000612
Epoch:26, Train loss:0.001076, valid loss:0.000794
Epoch:27, Train loss:0.001027, valid loss:0.000604
Epoch:28, Train loss:0.000948, valid loss:0.000627
Epoch:29, Train loss:0.001577, valid loss:0.000656
Epoch:30, Train loss:0.001043, valid loss:0.000673
Epoch:31, Train loss:0.000823, valid loss:0.000551
Epoch:32, Train loss:0.000876, valid loss:0.000560
Epoch:33, Train loss:0.000785, valid loss:0.000655
Epoch:34, Train loss:0.000760, valid loss:0.000579
Epoch:35, Train loss:0.000750, valid loss:0.000653
Epoch:36, Train loss:0.000739, valid loss:0.000540
Epoch:37, Train loss:0.000717, valid loss:0.000530
Epoch:38, Train loss:0.000716, valid loss:0.000527
Epoch:39, Train loss:0.000705, valid loss:0.000503
Epoch:40, Train loss:0.000719, valid loss:0.000622
Epoch:41, Train loss:0.000604, valid loss:0.000492
Epoch:42, Train loss:0.000597, valid loss:0.000497
Epoch:43, Train loss:0.000594, valid loss:0.000485
Epoch:44, Train loss:0.000601, valid loss:0.000478
Epoch:45, Train loss:0.000591, valid loss:0.000486
Epoch:46, Train loss:0.000587, valid loss:0.000514
Epoch:47, Train loss:0.000579, valid loss:0.000493
Epoch:48, Train loss:0.000583, valid loss:0.000479
Epoch:49, Train loss:0.000578, valid loss:0.000519
Epoch:50, Train loss:0.000576, valid loss:0.000490
Epoch:51, Train loss:0.000534, valid loss:0.000473
Epoch:52, Train loss:0.000533, valid loss:0.000469
Epoch:53, Train loss:0.000529, valid loss:0.000475
Epoch:54, Train loss:0.000529, valid loss:0.000472
Epoch:55, Train loss:0.000527, valid loss:0.000481
Epoch:56, Train loss:0.000526, valid loss:0.000470
Epoch:57, Train loss:0.000523, valid loss:0.000463
Epoch:58, Train loss:0.000522, valid loss:0.000481
Epoch:59, Train loss:0.000519, valid loss:0.000487
Epoch:60, Train loss:0.000520, valid loss:0.000484
training time 9185.516278266907
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.033752886454890954
plot_id,batch_id 0 1 miss% 0.02056087877970407
plot_id,batch_id 0 2 miss% 0.021082838657289486
plot_id,batch_id 0 3 miss% 0.023559896968705807
plot_id,batch_id 0 4 miss% 0.019940394465014118
plot_id,batch_id 0 5 miss% 0.034911424007152456
plot_id,batch_id 0 6 miss% 0.02812891456430506
plot_id,batch_id 0 7 miss% 0.028679699037463268
plot_id,batch_id 0 8 miss% 0.021444374083292646
plot_id,batch_id 0 9 miss% 0.0302132722453016
plot_id,batch_id 0 10 miss% 0.03770709150348595
plot_id,batch_id 0 11 miss% 0.03814253353556111
plot_id,batch_id 0 12 miss% 0.024191173748922257
plot_id,batch_id 0 13 miss% 0.020129315242838224
plot_id,batch_id 0 14 miss% 0.02553973287140824
plot_id,batch_id 0 15 miss% 0.039908352562781184
plot_id,batch_id 0 16 miss% 0.03890986941750662
plot_id,batch_id 0 17 miss% 0.037298849804606
plot_id,batch_id 0 18 miss% 0.035488065615033794
plot_id,batch_id 0 19 miss% 0.03261312906983512
plot_id,batch_id 0 20 miss% 0.049139730299540656
plot_id,batch_id 0 21 miss% 0.02906714634948468
plot_id,batch_id 0 22 miss% 0.01599403123541796
plot_id,batch_id 0 23 miss% 0.0373095870660797
plot_id,batch_id 0 24 miss% 0.025192986184580258
plot_id,batch_id 0 25 miss% 0.024424725654890497
plot_id,batch_id 0 26 miss% 0.025803486600614236
plot_id,batch_id 0 27 miss% 0.017981740497641025
plot_id,batch_id 0 28 miss% 0.025033401368118213
plot_id,batch_id 0 29 miss% 0.023997128762247386
plot_id,batch_id 0 30 miss% 0.05296408725321628
plot_id,batch_id 0 31 miss% 0.02223680836676343
plot_id,batch_id 0 32 miss% 0.02601813932812
plot_id,batch_id 0 33 miss% 0.02083279269828541
plot_id,batch_id 0 34 miss% 0.02607426315112264
plot_id,batch_id 0 35 miss% 0.03875850335230592
plot_id,batch_id 0 36 miss% 0.024180357148865954
plot_id,batch_id 0 37 miss% 0.0438534078501552
plot_id,batch_id 0 38 miss% 0.033185438194463554
plot_id,batch_id 0 39 miss% 0.015088564341072037
plot_id,batch_id 0 40 miss% 0.057061423519646645
plot_id,batch_id 0 41 miss% 0.031079617178039046
plot_id,batch_id 0 42 miss% 0.02093104711220974
plot_id,batch_id 0 43 miss% 0.03446801655629408
plot_id,batch_id 0 44 miss% 0.027136033229597324
plot_id,batch_id 0 45 miss% 0.023610621093810933
plot_id,batch_id 0 46 miss% 0.02498272647698641
plot_id,batch_id 0 47 miss% 0.021638298780577086
plot_id,batch_id 0 48 miss% 0.0213427993699151
plot_id,batch_id 0 49 miss% 0.022756749381216298
plot_id,batch_id 0 50 miss% 0.04254706305481006
plot_id,batch_id 0 51 miss% 0.02275895427042652
plot_id,batch_id 0 52 miss% 0.021178845949728412
plot_id,batch_id 0 53 miss% 0.01610719198046813
plot_id,batch_id 0 54 miss% 0.02802252218582051
plot_id,batch_id 0 55 miss% 0.03906570749695485
plot_id,batch_id 0 56 miss% 0.018156981403353083
plot_id,batch_id 0 57 miss% 0.025573619240964966
plot_id,batch_id 0 58 miss% 0.023670033399178336
plot_id,batch_id 0 59 miss% 0.017142914198535548
plot_id,batch_id 0 60 miss% 0.0557374420273008
plot_id,batch_id 0 61 miss% 0.04010031427979474
plot_id,batch_id 0 62 miss% 0.041152325284044874
plot_id,batch_id 0 63 miss% 0.02537009635570752
plot_id,batch_id 0 64 miss% 0.030729416872236032
plot_id,batch_id 0 65 miss% 0.043096717231157065
plot_id,batch_id 0 66 miss% 0.026016430732307683
plot_id,batch_id 0 67 miss% 0.03008295136650866
plot_id,batch_id 0 68 miss% 0.030885583909404914
plot_id,batch_id 0 69 miss% 0.023914850057674075
plot_id,batch_id 0 70 miss% 0.04402756368017481
plot_id,batch_id 0 71 miss% 0.03641402843150881
plot_id,batch_id 0 72 miss% 0.022824746471892615
plot_id,batch_id 0 73 miss% 0.022507328707541394
plot_id,batch_id 0 74 miss% 0.0394402589941035
plot_id,batch_id 0 75 miss% 0.046096043098865544
plot_id,batch_id 0 76 miss% 0.023072024180640555
plot_id,batch_id 0 77 miss% 0.02083901521298062
plot_id,batch_id 0 78 miss% 0.04039215134432564
plot_id,batch_id 0 79 miss% 0.034657703340035685
plot_id,batch_id 0 80 miss% 0.04287498524278538
plot_id,batch_id 0 81 miss% 0.023741985480655148
plot_id,batch_id 0 82 miss% 0.031630681175130564
plot_id,batch_id 0 83 miss% 0.02142094555587709
plot_id,batch_id 0 84 miss% 0.020919490880519725
plot_id,batch_id 0 85 miss% 0.04981682366788848
plot_id,batch_id 0 86 miss% 0.03160061666771433
plot_id,batch_id 0 87 miss% 0.027295989367397695
plot_id,batch_id 0 88 miss% 0.027103314344214657
plot_id,batch_id 0 89 miss% 0.02258592990310504
plot_id,batch_id 0 90 miss% 0.03509269876219189
plot_id,batch_id 0 91 miss% 0.029036611492387322
plot_id,batch_id 0 92 miss% 0.03624075535828864
plot_id,batch_id 0 93 miss% 0.03248895499870727
plot_id,batch_id 0 94 miss% 0.02778755087625972
plot_id,batch_id 0 95 miss% 0.03100543868096799
plot_id,batch_id 0 96 miss% 0.0227173648837426
plot_id,batch_id 0 97 miss% 0.036158901353061906
plot_id,batch_id 0 98 miss% 0.02958051190223285
plot_id,batch_id 0 99 miss% 0.02357648058196484
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03375289 0.02056088 0.02108284 0.0235599  0.01994039 0.03491142
 0.02812891 0.0286797  0.02144437 0.03021327 0.03770709 0.03814253
 0.02419117 0.02012932 0.02553973 0.03990835 0.03890987 0.03729885
 0.03548807 0.03261313 0.04913973 0.02906715 0.01599403 0.03730959
 0.02519299 0.02442473 0.02580349 0.01798174 0.0250334  0.02399713
 0.05296409 0.02223681 0.02601814 0.02083279 0.02607426 0.0387585
 0.02418036 0.04385341 0.03318544 0.01508856 0.05706142 0.03107962
 0.02093105 0.03446802 0.02713603 0.02361062 0.02498273 0.0216383
 0.0213428  0.02275675 0.04254706 0.02275895 0.02117885 0.01610719
 0.02802252 0.03906571 0.01815698 0.02557362 0.02367003 0.01714291
 0.05573744 0.04010031 0.04115233 0.0253701  0.03072942 0.04309672
 0.02601643 0.03008295 0.03088558 0.02391485 0.04402756 0.03641403
 0.02282475 0.02250733 0.03944026 0.04609604 0.02307202 0.02083902
 0.04039215 0.0346577  0.04287499 0.02374199 0.03163068 0.02142095
 0.02091949 0.04981682 0.03160062 0.02729599 0.02710331 0.02258593
 0.0350927  0.02903661 0.03624076 0.03248895 0.02778755 0.03100544
 0.02271736 0.0361589  0.02958051 0.02357648]
for model  231 the mean error 0.029886032069699146
all id 231 hidden_dim 32 learning_rate 0.02 num_layers 4 frames 31 out win 3 err 0.029886032069699146
Launcher: Job 232 completed in 9371 seconds.
Launcher: Task 160 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  134801
Epoch:0, Train loss:0.366169, valid loss:0.370888
Epoch:1, Train loss:0.226389, valid loss:0.229324
Epoch:2, Train loss:0.218040, valid loss:0.228481
Epoch:3, Train loss:0.217016, valid loss:0.228083
Epoch:4, Train loss:0.216517, valid loss:0.228055
Epoch:5, Train loss:0.216233, valid loss:0.228017
Epoch:6, Train loss:0.216157, valid loss:0.227910
Epoch:7, Train loss:0.216029, valid loss:0.228051
Epoch:8, Train loss:0.215892, valid loss:0.228149
Epoch:9, Train loss:0.215822, valid loss:0.227653
Epoch:10, Train loss:0.215837, valid loss:0.227697
Epoch:11, Train loss:0.215268, valid loss:0.227457
Epoch:12, Train loss:0.215295, valid loss:0.227438
Epoch:13, Train loss:0.215263, valid loss:0.227965
Epoch:14, Train loss:0.215228, valid loss:0.227636
Epoch:15, Train loss:0.215222, valid loss:0.227631
Epoch:16, Train loss:0.215228, valid loss:0.227456
Epoch:17, Train loss:0.215158, valid loss:0.227519
Epoch:18, Train loss:0.215169, valid loss:0.227699
Epoch:19, Train loss:0.215189, valid loss:0.227626
Epoch:20, Train loss:0.215187, valid loss:0.227497
Epoch:21, Train loss:0.214882, valid loss:0.227327
Epoch:22, Train loss:0.214887, valid loss:0.227444
Epoch:23, Train loss:0.214896, valid loss:0.227490
Epoch:24, Train loss:0.214890, valid loss:0.227420
Epoch:25, Train loss:0.214895, valid loss:0.227340
Epoch:26, Train loss:0.214858, valid loss:0.227336
Epoch:27, Train loss:0.214875, valid loss:0.227366
Epoch:28, Train loss:0.214854, valid loss:0.227556
Epoch:29, Train loss:0.214850, valid loss:0.227339
Epoch:30, Train loss:0.214845, valid loss:0.227347
Epoch:31, Train loss:0.214729, valid loss:0.227340
Epoch:32, Train loss:0.214720, valid loss:0.227339
Epoch:33, Train loss:0.214719, valid loss:0.227325
Epoch:34, Train loss:0.214728, valid loss:0.227295
Epoch:35, Train loss:0.214726, valid loss:0.227316
Epoch:36, Train loss:0.214707, valid loss:0.227291
Epoch:37, Train loss:0.214707, valid loss:0.227332
Epoch:38, Train loss:0.214720, valid loss:0.227337
Epoch:39, Train loss:0.214701, valid loss:0.227323
Epoch:40, Train loss:0.214724, valid loss:0.227324
Epoch:41, Train loss:0.214649, valid loss:0.227282
Epoch:42, Train loss:0.214643, valid loss:0.227295
Epoch:43, Train loss:0.214642, valid loss:0.227266
Epoch:44, Train loss:0.214648, valid loss:0.227290
Epoch:45, Train loss:0.214640, valid loss:0.227283
Epoch:46, Train loss:0.214639, valid loss:0.227268
Epoch:47, Train loss:0.214639, valid loss:0.227283
Epoch:48, Train loss:0.214635, valid loss:0.227298
Epoch:49, Train loss:0.214637, valid loss:0.227308
Epoch:50, Train loss:0.214634, valid loss:0.227282
Epoch:51, Train loss:0.214609, valid loss:0.227260
Epoch:52, Train loss:0.214608, valid loss:0.227259
Epoch:53, Train loss:0.214608, valid loss:0.227269
Epoch:54, Train loss:0.214606, valid loss:0.227271
Epoch:55, Train loss:0.214609, valid loss:0.227271
Epoch:56, Train loss:0.214605, valid loss:0.227286
Epoch:57, Train loss:0.214604, valid loss:0.227259
Epoch:58, Train loss:0.214603, valid loss:0.227277
Epoch:59, Train loss:0.214603, valid loss:0.227262
Epoch:60, Train loss:0.214603, valid loss:0.227247
training time 9242.927656888962
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.5493740785108016
plot_id,batch_id 0 1 miss% 0.6803424028571189
plot_id,batch_id 0 2 miss% 0.6941935344921905
plot_id,batch_id 0 3 miss% 0.7103878461108257
plot_id,batch_id 0 4 miss% 0.70814167899086
plot_id,batch_id 0 5 miss% 0.5521668092460976
plot_id,batch_id 0 6 miss% 0.668603474677928
plot_id,batch_id 0 7 miss% 0.6955406387972237
plot_id,batch_id 0 8 miss% 0.703119396105986
plot_id,batch_id 0 9 miss% 0.711454883515859
plot_id,batch_id 0 10 miss% 0.5278887045035158
plot_id,batch_id 0 11 miss% 0.6631712926003587
plot_id,batch_id 0 12 miss% 0.6884520832735457
plot_id,batch_id 0 13 miss% 0.7012554247209923
plot_id,batch_id 0 14 miss% 0.7103522891650714
plot_id,batch_id 0 15 miss% 0.5248721768535778
plot_id,batch_id 0 16 miss% 0.667411932038185
plot_id,batch_id 0 17 miss% 0.6982137685811756
plot_id,batch_id 0 18 miss% 0.7044423578666393
plot_id,batch_id 0 19 miss% 0.7048491242931009
plot_id,batch_id 0 20 miss% 0.6138875236532274
plot_id,batch_id 0 21 miss% 0.698375849609467
plot_id,batch_id 0 22 miss% 0.7086220791667883
plot_id,batch_id 0 23 miss% 0.716424991241659
plot_id,batch_id 0 24 miss% 0.7206157760116035
plot_id,batch_id 0 25 miss% 0.6085107963401066
plot_id,batch_id 0 26 miss% 0.6922678793615368
plot_id,batch_id 0 27 miss% 0.7075570013814855
plot_id,batch_id 0 28 miss% 0.71568360444459
plot_id,batch_id 0 29 miss% 0.7201358689790999
plot_id,batch_id 0 30 miss% 0.6093884551004699
plot_id,batch_id 0 31 miss% 0.6883800760724468
plot_id,batch_id 0 32 miss% 0.7014010035229176
plot_id,batch_id 0 33 miss% 0.7102702343401811
plot_id,batch_id 0 34 miss% 0.7127019488400631
plot_id,batch_id 0 35 miss% 0.5913287735007333
plot_id,batch_id 0 36 miss% 0.6914911885955214
plot_id,batch_id 0 37 miss% 0.7023114309733627
plot_id,batch_id 0 38 miss% 0.7113329761699141
plot_id,batch_id 0 39 miss% 0.714450831270711
plot_id,batch_id 0 40 miss% 0.6579228637164525
plot_id,batch_id 0 41 miss% 0.7081981110613245
plot_id,batch_id 0 42 miss% 0.7140741878161596
plot_id,batch_id 0 43 miss% 0.7226103980108196
plot_id,batch_id 0 44 miss% 0.7271417701997916
plot_id,batch_id 0 45 miss% 0.6573173931570351
plot_id,batch_id 0 46 miss% 0.7093252678582225
plot_id,batch_id 0 47 miss% 0.7162314676951852
plot_id,batch_id 0 48 miss% 0.7216570471563938
plot_id,batch_id 0 49 miss% 0.7242772337974844
plot_id,batch_id 0 50 miss% 0.6731325476485611
plot_id,batch_id 0 51 miss% 0.7043208162963414
plot_id,batch_id 0 52 miss% 0.7141957237356853
plot_id,batch_id 0 53 miss% 0.7189059225250963
plot_id,batch_id 0 54 miss% 0.7299676194766836
plot_id,batch_id 0 55 miss% 0.6674159278403409
plot_id,batch_id 0 56 miss% 0.7040448960379938
plot_id,batch_id 0 57 miss% 0.7135173910652127
plot_id,batch_id 0 58 miss% 0.7211743322899544
plot_id,batch_id 0 59 miss% 0.7198073940732772
plot_id,batch_id 0 60 miss% 0.44053868152223763
plot_id,batch_id 0 61 miss% 0.6126471442158651
plot_id,batch_id 0 62 miss% 0.6637239691710373
plot_id,batch_id 0 63 miss% 0.6840031495371969
plot_id,batch_id 0 64 miss% 0.6918844519263284
plot_id,batch_id 0 65 miss% 0.435802716046991
plot_id,batch_id 0 66 miss% 0.6072587697448607
plot_id,batch_id 0 67 miss% 0.6419230647702434
plot_id,batch_id 0 68 miss% 0.6840432065549178
plot_id,batch_id 0 69 miss% 0.6871546863639648
plot_id,batch_id 0 70 miss% 0.4059050513922542
plot_id,batch_id 0 71 miss% 0.5887548593640735
plot_id,batch_id 0 72 miss% 0.6286209976929595
plot_id,batch_id 0 73 miss% 0.6591016858634451
plot_id,batch_id 0 74 miss% 0.6918858455208196
plot_id,batch_id 0 75 miss% 0.3924812756239502
plot_id,batch_id 0 76 miss% 0.5666525151339102
plot_id,batch_id 0 77 miss% 0.6213632659395079
plot_id,batch_id 0 78 miss% 0.6706169365622432
plot_id,batch_id 0 79 miss% 0.6704885939793188
plot_id,batch_id 0 80 miss% 0.47466061806943294
plot_id,batch_id 0 81 miss% 0.6383702141884368
plot_id,batch_id 0 82 miss% 0.6761470714778774
plot_id,batch_id 0 83 miss% 0.6930099674459377
plot_id,batch_id 0 84 miss% 0.6978919405027846
plot_id,batch_id 0 85 miss% 0.4697117954514826
plot_id,batch_id 0 86 miss% 0.6318592496195485
plot_id,batch_id 0 87 miss% 0.6654936133570212
plot_id,batch_id 0 88 miss% 0.6908359667332883
plot_id,batch_id 0 89 miss% 0.6983233755724311
plot_id,batch_id 0 90 miss% 0.44933847340678673
plot_id,batch_id 0 91 miss% 0.6278549115294897
plot_id,batch_id 0 92 miss% 0.6636954460872243
plot_id,batch_id 0 93 miss% 0.6849947098524087
plot_id,batch_id 0 94 miss% 0.6975732419626762
plot_id,batch_id 0 95 miss% 0.4245546076502487
plot_id,batch_id 0 96 miss% 0.6064155085418674
plot_id,batch_id 0 97 miss% 0.6564515327127006
plot_id,batch_id 0 98 miss% 0.6702343005760678
plot_id,batch_id 0 99 miss% 0.6868845367089645
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.54937408 0.6803424  0.69419353 0.71038785 0.70814168 0.55216681
 0.66860347 0.69554064 0.7031194  0.71145488 0.5278887  0.66317129
 0.68845208 0.70125542 0.71035229 0.52487218 0.66741193 0.69821377
 0.70444236 0.70484912 0.61388752 0.69837585 0.70862208 0.71642499
 0.72061578 0.6085108  0.69226788 0.707557   0.7156836  0.72013587
 0.60938846 0.68838008 0.701401   0.71027023 0.71270195 0.59132877
 0.69149119 0.70231143 0.71133298 0.71445083 0.65792286 0.70819811
 0.71407419 0.7226104  0.72714177 0.65731739 0.70932527 0.71623147
 0.72165705 0.72427723 0.67313255 0.70432082 0.71419572 0.71890592
 0.72996762 0.66741593 0.7040449  0.71351739 0.72117433 0.71980739
 0.44053868 0.61264714 0.66372397 0.68400315 0.69188445 0.43580272
 0.60725877 0.64192306 0.68404321 0.68715469 0.40590505 0.58875486
 0.628621   0.65910169 0.69188585 0.39248128 0.56665252 0.62136327
 0.67061694 0.67048859 0.47466062 0.63837021 0.67614707 0.69300997
 0.69789194 0.4697118  0.63185925 0.66549361 0.69083597 0.69832338
 0.44933847 0.62785491 0.66369545 0.68499471 0.69757324 0.42455461
 0.60641551 0.65645153 0.6702343  0.68688454]
for model  213 the mean error 0.6567376244560775
all id 213 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 31 out win 3 err 0.6567376244560775
Launcher: Job 214 completed in 9416 seconds.
Launcher: Task 101 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  35921
Epoch:0, Train loss:0.447953, valid loss:0.429300
Epoch:1, Train loss:0.023453, valid loss:0.004227
Epoch:2, Train loss:0.007285, valid loss:0.002942
Epoch:3, Train loss:0.005575, valid loss:0.002690
Epoch:4, Train loss:0.004747, valid loss:0.002327
Epoch:5, Train loss:0.004264, valid loss:0.002183
Epoch:6, Train loss:0.004125, valid loss:0.001911
Epoch:7, Train loss:0.003973, valid loss:0.001818
Epoch:8, Train loss:0.003798, valid loss:0.001893
Epoch:9, Train loss:0.003645, valid loss:0.001982
Epoch:10, Train loss:0.003486, valid loss:0.001453
Epoch:11, Train loss:0.002549, valid loss:0.001600
Epoch:12, Train loss:0.002506, valid loss:0.001350
Epoch:13, Train loss:0.002527, valid loss:0.001385
Epoch:14, Train loss:0.002497, valid loss:0.001494
Epoch:15, Train loss:0.002409, valid loss:0.001438
Epoch:16, Train loss:0.002334, valid loss:0.001221
Epoch:17, Train loss:0.002296, valid loss:0.001182
Epoch:18, Train loss:0.002314, valid loss:0.001325
Epoch:19, Train loss:0.003324, valid loss:0.001606
Epoch:20, Train loss:0.002554, valid loss:0.001452
Epoch:21, Train loss:0.001962, valid loss:0.000979
Epoch:22, Train loss:0.001915, valid loss:0.000911
Epoch:23, Train loss:0.001864, valid loss:0.001168
Epoch:24, Train loss:0.001817, valid loss:0.001084
Epoch:25, Train loss:0.001808, valid loss:0.001046
Epoch:26, Train loss:0.001766, valid loss:0.001003
Epoch:27, Train loss:0.001751, valid loss:0.000961
Epoch:28, Train loss:0.001771, valid loss:0.000933
Epoch:29, Train loss:0.001679, valid loss:0.001040
Epoch:30, Train loss:0.001733, valid loss:0.001040
Epoch:31, Train loss:0.001410, valid loss:0.000887
Epoch:32, Train loss:0.001419, valid loss:0.000892
Epoch:33, Train loss:0.001432, valid loss:0.000862
Epoch:34, Train loss:0.001386, valid loss:0.000878
Epoch:35, Train loss:0.001392, valid loss:0.000828
Epoch:36, Train loss:0.001391, valid loss:0.000940
Epoch:37, Train loss:0.001394, valid loss:0.000775
Epoch:38, Train loss:0.001372, valid loss:0.000895
Epoch:39, Train loss:0.001369, valid loss:0.000889
Epoch:40, Train loss:0.001367, valid loss:0.000839
Epoch:41, Train loss:0.001204, valid loss:0.000774
Epoch:42, Train loss:0.001208, valid loss:0.000734
Epoch:43, Train loss:0.001204, valid loss:0.000810
Epoch:44, Train loss:0.001195, valid loss:0.000821
Epoch:45, Train loss:0.001186, valid loss:0.000793
Epoch:46, Train loss:0.001193, valid loss:0.000780
Epoch:47, Train loss:0.001189, valid loss:0.000776
Epoch:48, Train loss:0.001180, valid loss:0.000788
Epoch:49, Train loss:0.001168, valid loss:0.000792
Epoch:50, Train loss:0.001168, valid loss:0.000813
Epoch:51, Train loss:0.001096, valid loss:0.000744
Epoch:52, Train loss:0.001090, valid loss:0.000730
Epoch:53, Train loss:0.001082, valid loss:0.000748
Epoch:54, Train loss:0.001087, valid loss:0.000718
Epoch:55, Train loss:0.001078, valid loss:0.000716
Epoch:56, Train loss:0.001080, valid loss:0.000742
Epoch:57, Train loss:0.001082, valid loss:0.000723
Epoch:58, Train loss:0.001066, valid loss:0.000733
Epoch:59, Train loss:0.001067, valid loss:0.000747
Epoch:60, Train loss:0.001076, valid loss:0.000735
training time 9245.10315990448
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.015892835623719532
plot_id,batch_id 0 1 miss% 0.03230117340190217
plot_id,batch_id 0 2 miss% 0.030310267377737392
plot_id,batch_id 0 3 miss% 0.03306160274460365
plot_id,batch_id 0 4 miss% 0.026526464721203503
plot_id,batch_id 0 5 miss% 0.04539733252433282
plot_id,batch_id 0 6 miss% 0.04188286951355717
plot_id,batch_id 0 7 miss% 0.02737417507955476
plot_id,batch_id 0 8 miss% 0.03009998880515816
plot_id,batch_id 0 9 miss% 0.027322850351557154
plot_id,batch_id 0 10 miss% 0.04834485638121874
plot_id,batch_id 0 11 miss% 0.042659100541911035
plot_id,batch_id 0 12 miss% 0.03840944929964989
plot_id,batch_id 0 13 miss% 0.03158206344633625
plot_id,batch_id 0 14 miss% 0.04044461688848814
plot_id,batch_id 0 15 miss% 0.04690313674537969
plot_id,batch_id 0 16 miss% 0.03576332675830233
plot_id,batch_id 0 17 miss% 0.04498777133027774
plot_id,batch_id 0 18 miss% 0.0397959888575441
plot_id,batch_id 0 19 miss% 0.037794344899186
plot_id,batch_id 0 20 miss% 0.041203296751654885
plot_id,batch_id 0 21 miss% 0.03183394528547575
plot_id,batch_id 0 22 miss% 0.03184045352147251
plot_id,batch_id 0 23 miss% 0.02732330710574299
plot_id,batch_id 0 24 miss% 0.03434395516724597
plot_id,batch_id 0 25 miss% 0.046729906357524054
plot_id,batch_id 0 26 miss% 0.03542849658825758
plot_id,batch_id 0 27 miss% 0.03611523527638806
plot_id,batch_id 0 28 miss% 0.03304565796948441
plot_id,batch_id 0 29 miss% 0.04042084652735324
plot_id,batch_id 0 30 miss% 0.04139983231881518
plot_id,batch_id 0 31 miss% 0.05184204343833507
plot_id,batch_id 0 32 miss% 0.033411675675045445
plot_id,batch_id 0 33 miss% 0.030185042719099654
plot_id,batch_id 0 34 miss% 0.03131698260352568
plot_id,batch_id 0 35 miss% 0.03913096553235312
plot_id,batch_id 0 36 miss% 0.05099314375165423
plot_id,batch_id 0 37 miss% 0.03906675052946121
plot_id,batch_id 0 38 miss% 0.03424569609890901
plot_id,batch_id 0 39 miss% 0.026618329588092498
plot_id,batch_id 0 40 miss% 0.06884987030254675
plot_id,batch_id 0 41 miss% 0.032004774993572155
plot_id,batch_id 0 42 miss% 0.03912232958855741
plot_id,batch_id 0 43 miss% 0.026454019400382195
plot_id,batch_id 0 44 miss% 0.03243379902006531
plot_id,batch_id 0 45 miss% 0.048476281142392885
plot_id,batch_id 0 46 miss% 0.0293137453606851
plot_id,batch_id 0 47 miss% 0.03313471003954311
plot_id,batch_id 0 48 miss% 0.03183384892319256
plot_id,batch_id 0 49 miss% 0.025706385856926802
plot_id,batch_id 0 50 miss% 0.03927104968777875
plot_id,batch_id 0 51 miss% 0.036884831297686646
plot_id,batch_id 0 52 miss% 0.04431645409541591
plot_id,batch_id 0 53 miss% 0.033160222926318074
plot_id,batch_id 0 54 miss% 0.035864111799060405
plot_id,batch_id 0 55 miss% 0.038678673953404505
plot_id,batch_id 0 56 miss% 0.042076336982249336
plot_id,batch_id 0 57 miss% 0.03205079238151234
plot_id,batch_id 0 58 miss% 0.0312958970613257
plot_id,batch_id 0 59 miss% 0.03199486581129057
plot_id,batch_id 0 60 miss% 0.04029641271136353
plot_id,batch_id 0 61 miss% 0.02876939256750985
plot_id,batch_id 0 62 miss% 0.025263283452916126
plot_id,batch_id 0 63 miss% 0.043398413695063594
plot_id,batch_id 0 64 miss% 0.04423151938464745
plot_id,batch_id 0 65 miss% 0.04466673000042853
plot_id,batch_id 0 66 miss% 0.03070451649467131
plot_id,batch_id 0 67 miss% 0.0382674137960767
plot_id,batch_id 0 68 miss% 0.0350975160987589
plot_id,batch_id 0 69 miss% 0.031178858353175465
plot_id,batch_id 0 70 miss% 0.040329997885281486
plot_id,batch_id 0 71 miss% 0.042759634938708234
plot_id,batch_id 0 72 miss% 0.041089428623947565
plot_id,batch_id 0 73 miss% 0.04502227329578416
plot_id,batch_id 0 74 miss% 0.03291951541116702
plot_id,batch_id 0 75 miss% 0.05969441162318869
plot_id,batch_id 0 76 miss% 0.06086963820212913
plot_id,batch_id 0 77 miss% 0.0459495474352916
plot_id,batch_id 0 78 miss% 0.0354862275757667
plot_id,batch_id 0 79 miss% 0.05798566025459816
plot_id,batch_id 0 80 miss% 0.04973137722977387
plot_id,batch_id 0 81 miss% 0.02795799935340267
plot_id,batch_id 0 82 miss% 0.02975222777037863
plot_id,batch_id 0 83 miss% 0.03417463215223753
plot_id,batch_id 0 84 miss% 0.02995870884003081
plot_id,batch_id 0 85 miss% 0.046466263297328766
plot_id,batch_id 0 86 miss% 0.039844912216033465
plot_id,batch_id 0 87 miss% 0.037829807775586796
plot_id,batch_id 0 88 miss% 0.04126608949855223
plot_id,batch_id 0 89 miss% 0.0401192857779482
plot_id,batch_id 0 90 miss% 0.05567359031061224
plot_id,batch_id 0 91 miss% 0.0468269127176266
plot_id,batch_id 0 92 miss% 0.031342050455841895
plot_id,batch_id 0 93 miss% 0.03134330384160529
plot_id,batch_id 0 94 miss% 0.05266039882637832
plot_id,batch_id 0 95 miss% 0.06079195609418018
plot_id,batch_id 0 96 miss% 0.059608273546096616
plot_id,batch_id 0 97 miss% 0.04910419067568964
plot_id,batch_id 0 98 miss% 0.046439100508827665
plot_id,batch_id 0 99 miss% 0.057792797613324565
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01589284 0.03230117 0.03031027 0.0330616  0.02652646 0.04539733
 0.04188287 0.02737418 0.03009999 0.02732285 0.04834486 0.0426591
 0.03840945 0.03158206 0.04044462 0.04690314 0.03576333 0.04498777
 0.03979599 0.03779434 0.0412033  0.03183395 0.03184045 0.02732331
 0.03434396 0.04672991 0.0354285  0.03611524 0.03304566 0.04042085
 0.04139983 0.05184204 0.03341168 0.03018504 0.03131698 0.03913097
 0.05099314 0.03906675 0.0342457  0.02661833 0.06884987 0.03200477
 0.03912233 0.02645402 0.0324338  0.04847628 0.02931375 0.03313471
 0.03183385 0.02570639 0.03927105 0.03688483 0.04431645 0.03316022
 0.03586411 0.03867867 0.04207634 0.03205079 0.0312959  0.03199487
 0.04029641 0.02876939 0.02526328 0.04339841 0.04423152 0.04466673
 0.03070452 0.03826741 0.03509752 0.03117886 0.04033    0.04275963
 0.04108943 0.04502227 0.03291952 0.05969441 0.06086964 0.04594955
 0.03548623 0.05798566 0.04973138 0.027958   0.02975223 0.03417463
 0.02995871 0.04646626 0.03984491 0.03782981 0.04126609 0.04011929
 0.05567359 0.04682691 0.03134205 0.0313433  0.0526604  0.06079196
 0.05960827 0.04910419 0.0464391  0.0577928 ]
for model  236 the mean error 0.038789370490233475
all id 236 hidden_dim 16 learning_rate 0.02 num_layers 5 frames 31 out win 5 err 0.038789370490233475
Launcher: Job 237 completed in 9440 seconds.
Launcher: Task 177 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  134801
Epoch:0, Train loss:0.497999, valid loss:0.504844
Epoch:1, Train loss:0.352950, valid loss:0.358529
Epoch:2, Train loss:0.341087, valid loss:0.357388
Epoch:3, Train loss:0.339378, valid loss:0.357176
Epoch:4, Train loss:0.338685, valid loss:0.356244
Epoch:5, Train loss:0.338496, valid loss:0.357065
Epoch:6, Train loss:0.338196, valid loss:0.356441
Epoch:7, Train loss:0.338080, valid loss:0.356440
Epoch:8, Train loss:0.337923, valid loss:0.356211
Epoch:9, Train loss:0.337869, valid loss:0.355980
Epoch:10, Train loss:0.337686, valid loss:0.355850
Epoch:11, Train loss:0.336912, valid loss:0.355718
Epoch:12, Train loss:0.336886, valid loss:0.355770
Epoch:13, Train loss:0.336881, valid loss:0.355795
Epoch:14, Train loss:0.336872, valid loss:0.355682
Epoch:15, Train loss:0.336836, valid loss:0.355648
Epoch:16, Train loss:0.336785, valid loss:0.355682
Epoch:17, Train loss:0.336794, valid loss:0.355457
Epoch:18, Train loss:0.336728, valid loss:0.355512
Epoch:19, Train loss:0.336741, valid loss:0.355486
Epoch:20, Train loss:0.336700, valid loss:0.355446
Epoch:21, Train loss:0.336339, valid loss:0.355437
Epoch:22, Train loss:0.336359, valid loss:0.355446
Epoch:23, Train loss:0.336335, valid loss:0.355313
Epoch:24, Train loss:0.336312, valid loss:0.355468
Epoch:25, Train loss:0.336295, valid loss:0.355480
Epoch:26, Train loss:0.336296, valid loss:0.355539
Epoch:27, Train loss:0.336313, valid loss:0.355357
Epoch:28, Train loss:0.336281, valid loss:0.355322
Epoch:29, Train loss:0.336272, valid loss:0.355494
Epoch:30, Train loss:0.336268, valid loss:0.355389
Epoch:31, Train loss:0.336110, valid loss:0.355357
Epoch:32, Train loss:0.336092, valid loss:0.355331
Epoch:33, Train loss:0.336092, valid loss:0.355294
Epoch:34, Train loss:0.336099, valid loss:0.355418
Epoch:35, Train loss:0.336095, valid loss:0.355311
Epoch:36, Train loss:0.336065, valid loss:0.355297
Epoch:37, Train loss:0.336075, valid loss:0.355332
Epoch:38, Train loss:0.336085, valid loss:0.355345
Epoch:39, Train loss:0.336067, valid loss:0.355357
Epoch:40, Train loss:0.336068, valid loss:0.355591
Epoch:41, Train loss:0.335993, valid loss:0.355278
Epoch:42, Train loss:0.335995, valid loss:0.355305
Epoch:43, Train loss:0.335995, valid loss:0.355287
Epoch:44, Train loss:0.335985, valid loss:0.355279
Epoch:45, Train loss:0.335981, valid loss:0.355240
Epoch:46, Train loss:0.335982, valid loss:0.355344
Epoch:47, Train loss:0.335989, valid loss:0.355294
Epoch:48, Train loss:0.335972, valid loss:0.355305
Epoch:49, Train loss:0.335972, valid loss:0.355275
Epoch:50, Train loss:0.335979, valid loss:0.355280
Epoch:51, Train loss:0.335942, valid loss:0.355250
Epoch:52, Train loss:0.335941, valid loss:0.355266
Epoch:53, Train loss:0.335941, valid loss:0.355264
Epoch:54, Train loss:0.335941, valid loss:0.355254
Epoch:55, Train loss:0.335936, valid loss:0.355273
Epoch:56, Train loss:0.335939, valid loss:0.355264
Epoch:57, Train loss:0.335937, valid loss:0.355265
Epoch:58, Train loss:0.335932, valid loss:0.355268
Epoch:59, Train loss:0.335936, valid loss:0.355271
Epoch:60, Train loss:0.335930, valid loss:0.355253
training time 9298.495401144028
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.6465183035897103
plot_id,batch_id 0 1 miss% 0.7331816642945554
plot_id,batch_id 0 2 miss% 0.7442894422602385
plot_id,batch_id 0 3 miss% 0.7546912801856853
plot_id,batch_id 0 4 miss% 0.7602725106023172
plot_id,batch_id 0 5 miss% 0.6337868988159718
plot_id,batch_id 0 6 miss% 0.7260575953131917
plot_id,batch_id 0 7 miss% 0.7459597120956487
plot_id,batch_id 0 8 miss% 0.7558542730457932
plot_id,batch_id 0 9 miss% 0.7579314168325839
plot_id,batch_id 0 10 miss% 0.6086301822262202
plot_id,batch_id 0 11 miss% 0.7273699442347377
plot_id,batch_id 0 12 miss% 0.7380443636867828
plot_id,batch_id 0 13 miss% 0.7484299184741188
plot_id,batch_id 0 14 miss% 0.7570398734340587
plot_id,batch_id 0 15 miss% 0.6197197088054666
plot_id,batch_id 0 16 miss% 0.7204883791319414
plot_id,batch_id 0 17 miss% 0.7447790517985036
plot_id,batch_id 0 18 miss% 0.7481491989400082
plot_id,batch_id 0 19 miss% 0.7499027734015369
plot_id,batch_id 0 20 miss% 0.6850627357136994
plot_id,batch_id 0 21 miss% 0.7466558219743129
plot_id,batch_id 0 22 miss% 0.7547498009439846
plot_id,batch_id 0 23 miss% 0.7618687651198672
plot_id,batch_id 0 24 miss% 0.7636259169449856
plot_id,batch_id 0 25 miss% 0.6823372396285794
plot_id,batch_id 0 26 miss% 0.7421531057623332
plot_id,batch_id 0 27 miss% 0.7537758617547898
plot_id,batch_id 0 28 miss% 0.7605434130434199
plot_id,batch_id 0 29 miss% 0.7635660856525599
plot_id,batch_id 0 30 miss% 0.6631521043396253
plot_id,batch_id 0 31 miss% 0.7378534126782211
plot_id,batch_id 0 32 miss% 0.7488493789696676
plot_id,batch_id 0 33 miss% 0.7623453029693706
plot_id,batch_id 0 34 miss% 0.762801205830993
plot_id,batch_id 0 35 miss% 0.6700851644672801
plot_id,batch_id 0 36 miss% 0.7421496007359222
plot_id,batch_id 0 37 miss% 0.7486216825784733
plot_id,batch_id 0 38 miss% 0.757195675431119
plot_id,batch_id 0 39 miss% 0.7582225550052993
plot_id,batch_id 0 40 miss% 0.7104491492293038
plot_id,batch_id 0 41 miss% 0.7532050051924141
plot_id,batch_id 0 42 miss% 0.7613192302699004
plot_id,batch_id 0 43 miss% 0.7648455046842865
plot_id,batch_id 0 44 miss% 0.7667314227900185
plot_id,batch_id 0 45 miss% 0.7225665440215947
plot_id,batch_id 0 46 miss% 0.7528906485193783
plot_id,batch_id 0 47 miss% 0.7621270307961987
plot_id,batch_id 0 48 miss% 0.7624142238518737
plot_id,batch_id 0 49 miss% 0.7673969779215437
plot_id,batch_id 0 50 miss% 0.724367980323498
plot_id,batch_id 0 51 miss% 0.7532073360815087
plot_id,batch_id 0 52 miss% 0.7556432546296158
plot_id,batch_id 0 53 miss% 0.7604382288723778
plot_id,batch_id 0 54 miss% 0.7685444186790853
plot_id,batch_id 0 55 miss% 0.7237750112128495
plot_id,batch_id 0 56 miss% 0.7504029417916829
plot_id,batch_id 0 57 miss% 0.7568424071085106
plot_id,batch_id 0 58 miss% 0.7631732596374241
plot_id,batch_id 0 59 miss% 0.7684541424004053
plot_id,batch_id 0 60 miss% 0.541123101894504
plot_id,batch_id 0 61 miss% 0.68502683277116
plot_id,batch_id 0 62 miss% 0.7216250998699045
plot_id,batch_id 0 63 miss% 0.7387315015655995
plot_id,batch_id 0 64 miss% 0.7420638705018908
plot_id,batch_id 0 65 miss% 0.5229051926105386
plot_id,batch_id 0 66 miss% 0.6777632104614723
plot_id,batch_id 0 67 miss% 0.7056363793720674
plot_id,batch_id 0 68 miss% 0.7328159819649744
plot_id,batch_id 0 69 miss% 0.7388837906668777
plot_id,batch_id 0 70 miss% 0.4962194095048504
plot_id,batch_id 0 71 miss% 0.6828914442787946
plot_id,batch_id 0 72 miss% 0.6933639311220622
plot_id,batch_id 0 73 miss% 0.7173951027662664
plot_id,batch_id 0 74 miss% 0.7320650447289115
plot_id,batch_id 0 75 miss% 0.4866534890761332
plot_id,batch_id 0 76 miss% 0.6366760972455664
plot_id,batch_id 0 77 miss% 0.6860687728398703
plot_id,batch_id 0 78 miss% 0.7180863064815833
plot_id,batch_id 0 79 miss% 0.73555968561435
plot_id,batch_id 0 80 miss% 0.5616164302608586
plot_id,batch_id 0 81 miss% 0.7081058148268483
plot_id,batch_id 0 82 miss% 0.7312321750592595
plot_id,batch_id 0 83 miss% 0.7459979138551747
plot_id,batch_id 0 84 miss% 0.7470945855959201
plot_id,batch_id 0 85 miss% 0.5596354939727669
plot_id,batch_id 0 86 miss% 0.7016982221292145
plot_id,batch_id 0 87 miss% 0.7240031164942936
plot_id,batch_id 0 88 miss% 0.7421107680441417
plot_id,batch_id 0 89 miss% 0.7454060841431315
plot_id,batch_id 0 90 miss% 0.5368488645716976
plot_id,batch_id 0 91 miss% 0.6980751905259421
plot_id,batch_id 0 92 miss% 0.7206380133691044
plot_id,batch_id 0 93 miss% 0.7354394942264407
plot_id,batch_id 0 94 miss% 0.74339194559012
plot_id,batch_id 0 95 miss% 0.5238163113333704
plot_id,batch_id 0 96 miss% 0.6768356196353607
plot_id,batch_id 0 97 miss% 0.7147440734296179
plot_id,batch_id 0 98 miss% 0.7265391600478056
plot_id,batch_id 0 99 miss% 0.7366048653550283
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.6465183  0.73318166 0.74428944 0.75469128 0.76027251 0.6337869
 0.7260576  0.74595971 0.75585427 0.75793142 0.60863018 0.72736994
 0.73804436 0.74842992 0.75703987 0.61971971 0.72048838 0.74477905
 0.7481492  0.74990277 0.68506274 0.74665582 0.7547498  0.76186877
 0.76362592 0.68233724 0.74215311 0.75377586 0.76054341 0.76356609
 0.6631521  0.73785341 0.74884938 0.7623453  0.76280121 0.67008516
 0.7421496  0.74862168 0.75719568 0.75822256 0.71044915 0.75320501
 0.76131923 0.7648455  0.76673142 0.72256654 0.75289065 0.76212703
 0.76241422 0.76739698 0.72436798 0.75320734 0.75564325 0.76043823
 0.76854442 0.72377501 0.75040294 0.75684241 0.76317326 0.76845414
 0.5411231  0.68502683 0.7216251  0.7387315  0.74206387 0.52290519
 0.67776321 0.70563638 0.73281598 0.73888379 0.49621941 0.68289144
 0.69336393 0.7173951  0.73206504 0.48665349 0.6366761  0.68606877
 0.71808631 0.73555969 0.56161643 0.70810581 0.73123218 0.74599791
 0.74709459 0.55963549 0.70169822 0.72400312 0.74211077 0.74540608
 0.53684886 0.69807519 0.72063801 0.73543949 0.74339195 0.52381631
 0.67683562 0.71474407 0.72653916 0.73660487]
for model  132 the mean error 0.7140689442653051
all id 132 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 25 out win 3 err 0.7140689442653051
Launcher: Job 133 completed in 9475 seconds.
Launcher: Task 158 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  134801
Epoch:0, Train loss:0.497999, valid loss:0.504844
Epoch:1, Train loss:0.355430, valid loss:0.358957
Epoch:2, Train loss:0.342698, valid loss:0.358008
Epoch:3, Train loss:0.340789, valid loss:0.358039
Epoch:4, Train loss:0.339764, valid loss:0.356764
Epoch:5, Train loss:0.339353, valid loss:0.357217
Epoch:6, Train loss:0.339030, valid loss:0.356602
Epoch:7, Train loss:0.338801, valid loss:0.357263
Epoch:8, Train loss:0.338588, valid loss:0.356178
Epoch:9, Train loss:0.338508, valid loss:0.355874
Epoch:10, Train loss:0.338365, valid loss:0.356276
Epoch:11, Train loss:0.337247, valid loss:0.355841
Epoch:12, Train loss:0.337255, valid loss:0.355997
Epoch:13, Train loss:0.337273, valid loss:0.355763
Epoch:14, Train loss:0.337190, valid loss:0.355814
Epoch:15, Train loss:0.337127, valid loss:0.355568
Epoch:16, Train loss:0.337094, valid loss:0.356011
Epoch:17, Train loss:0.337134, valid loss:0.355587
Epoch:18, Train loss:0.337015, valid loss:0.356099
Epoch:19, Train loss:0.337070, valid loss:0.356060
Epoch:20, Train loss:0.337018, valid loss:0.355813
Epoch:21, Train loss:0.336505, valid loss:0.355459
Epoch:22, Train loss:0.336518, valid loss:0.355502
Epoch:23, Train loss:0.336520, valid loss:0.355404
Epoch:24, Train loss:0.336492, valid loss:0.355437
Epoch:25, Train loss:0.336452, valid loss:0.355442
Epoch:26, Train loss:0.336448, valid loss:0.355419
Epoch:27, Train loss:0.336426, valid loss:0.355425
Epoch:28, Train loss:0.336410, valid loss:0.355463
Epoch:29, Train loss:0.336459, valid loss:0.355460
Epoch:30, Train loss:0.336421, valid loss:0.355312
Epoch:31, Train loss:0.336190, valid loss:0.355331
Epoch:32, Train loss:0.336172, valid loss:0.355336
Epoch:33, Train loss:0.336168, valid loss:0.355298
Epoch:34, Train loss:0.336178, valid loss:0.355283
Epoch:35, Train loss:0.336159, valid loss:0.355374
Epoch:36, Train loss:0.336191, valid loss:0.355256
Epoch:37, Train loss:0.336158, valid loss:0.355382
Epoch:38, Train loss:0.336143, valid loss:0.355299
Epoch:39, Train loss:0.336156, valid loss:0.355327
Epoch:40, Train loss:0.336136, valid loss:0.355404
Epoch:41, Train loss:0.336040, valid loss:0.355264
Epoch:42, Train loss:0.336030, valid loss:0.355265
Epoch:43, Train loss:0.336017, valid loss:0.355236
Epoch:44, Train loss:0.336030, valid loss:0.355266
Epoch:45, Train loss:0.336028, valid loss:0.355255
Epoch:46, Train loss:0.336013, valid loss:0.355312
Epoch:47, Train loss:0.336032, valid loss:0.355244
Epoch:48, Train loss:0.336008, valid loss:0.355216
Epoch:49, Train loss:0.336001, valid loss:0.355220
Epoch:50, Train loss:0.336015, valid loss:0.355289
Epoch:51, Train loss:0.335965, valid loss:0.355229
Epoch:52, Train loss:0.335966, valid loss:0.355233
Epoch:53, Train loss:0.335959, valid loss:0.355235
Epoch:54, Train loss:0.335958, valid loss:0.355221
Epoch:55, Train loss:0.335956, valid loss:0.355233
Epoch:56, Train loss:0.335955, valid loss:0.355226
Epoch:57, Train loss:0.335948, valid loss:0.355214
Epoch:58, Train loss:0.335948, valid loss:0.355253
Epoch:59, Train loss:0.335957, valid loss:0.355222
Epoch:60, Train loss:0.335945, valid loss:0.355213
training time 9377.817039251328
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.6362860556006279
plot_id,batch_id 0 1 miss% 0.732930164487329
plot_id,batch_id 0 2 miss% 0.744926710091788
plot_id,batch_id 0 3 miss% 0.7554917042751794
plot_id,batch_id 0 4 miss% 0.7581838700230192
plot_id,batch_id 0 5 miss% 0.6357538407549724
plot_id,batch_id 0 6 miss% 0.72654542476199
plot_id,batch_id 0 7 miss% 0.7492818635219113
plot_id,batch_id 0 8 miss% 0.7559519880027341
plot_id,batch_id 0 9 miss% 0.7573320320307565
plot_id,batch_id 0 10 miss% 0.6100091779263493
plot_id,batch_id 0 11 miss% 0.7249961102901551
plot_id,batch_id 0 12 miss% 0.7380582470123466
plot_id,batch_id 0 13 miss% 0.7484776997760728
plot_id,batch_id 0 14 miss% 0.7577220457139023
plot_id,batch_id 0 15 miss% 0.6274077234522285
plot_id,batch_id 0 16 miss% 0.7159812764868454
plot_id,batch_id 0 17 miss% 0.7446366927102734
plot_id,batch_id 0 18 miss% 0.747033890009535
plot_id,batch_id 0 19 miss% 0.7515249041491903
plot_id,batch_id 0 20 miss% 0.6897328817035013
plot_id,batch_id 0 21 miss% 0.7465705733364632
plot_id,batch_id 0 22 miss% 0.7544879564100224
plot_id,batch_id 0 23 miss% 0.7594681213508885
plot_id,batch_id 0 24 miss% 0.7618952240646738
plot_id,batch_id 0 25 miss% 0.6800976118402745
plot_id,batch_id 0 26 miss% 0.7442051107681162
plot_id,batch_id 0 27 miss% 0.7550353639339342
plot_id,batch_id 0 28 miss% 0.7597020507457
plot_id,batch_id 0 29 miss% 0.7617254153480589
plot_id,batch_id 0 30 miss% 0.6680774916143593
plot_id,batch_id 0 31 miss% 0.7406381346016048
plot_id,batch_id 0 32 miss% 0.7529938039115324
plot_id,batch_id 0 33 miss% 0.7604309907293563
plot_id,batch_id 0 34 miss% 0.761317238871118
plot_id,batch_id 0 35 miss% 0.6708552345752183
plot_id,batch_id 0 36 miss% 0.7414600145357005
plot_id,batch_id 0 37 miss% 0.7500428159386392
plot_id,batch_id 0 38 miss% 0.7569286229074366
plot_id,batch_id 0 39 miss% 0.75778721973743
plot_id,batch_id 0 40 miss% 0.7140678095894035
plot_id,batch_id 0 41 miss% 0.7534823406996849
plot_id,batch_id 0 42 miss% 0.7610940822812439
plot_id,batch_id 0 43 miss% 0.7641377635297028
plot_id,batch_id 0 44 miss% 0.7660803881651146
plot_id,batch_id 0 45 miss% 0.7216511670421097
plot_id,batch_id 0 46 miss% 0.7542139859177603
plot_id,batch_id 0 47 miss% 0.7603774040713691
plot_id,batch_id 0 48 miss% 0.7630725318014944
plot_id,batch_id 0 49 miss% 0.7655254191828986
plot_id,batch_id 0 50 miss% 0.7263533760843048
plot_id,batch_id 0 51 miss% 0.754185905011813
plot_id,batch_id 0 52 miss% 0.758421133631992
plot_id,batch_id 0 53 miss% 0.7621069792719991
plot_id,batch_id 0 54 miss% 0.7660266039800496
plot_id,batch_id 0 55 miss% 0.7232103203119247
plot_id,batch_id 0 56 miss% 0.7514594640591304
plot_id,batch_id 0 57 miss% 0.7581942964493316
plot_id,batch_id 0 58 miss% 0.7614817117202877
plot_id,batch_id 0 59 miss% 0.7643631520181613
plot_id,batch_id 0 60 miss% 0.5352172294317136
plot_id,batch_id 0 61 miss% 0.6839959466220369
plot_id,batch_id 0 62 miss% 0.719121195186682
plot_id,batch_id 0 63 miss% 0.740532647547062
plot_id,batch_id 0 64 miss% 0.7438670015217138
plot_id,batch_id 0 65 miss% 0.5275134426421194
plot_id,batch_id 0 66 miss% 0.6829876458590297
plot_id,batch_id 0 67 miss% 0.7059284052708641
plot_id,batch_id 0 68 miss% 0.731262226977375
plot_id,batch_id 0 69 miss% 0.7393522475698405
plot_id,batch_id 0 70 miss% 0.49950022743575584
plot_id,batch_id 0 71 miss% 0.6902084705447664
plot_id,batch_id 0 72 miss% 0.6934099201367827
plot_id,batch_id 0 73 miss% 0.7176066219421503
plot_id,batch_id 0 74 miss% 0.7302913583297498
plot_id,batch_id 0 75 miss% 0.49148454194494234
plot_id,batch_id 0 76 miss% 0.6381565640577693
plot_id,batch_id 0 77 miss% 0.6848107568358732
plot_id,batch_id 0 78 miss% 0.7210217244067779
plot_id,batch_id 0 79 miss% 0.7265565952538587
plot_id,batch_id 0 80 miss% 0.5663181020920054
plot_id,batch_id 0 81 miss% 0.7057745849984708
plot_id,batch_id 0 82 miss% 0.7319503592159087
plot_id,batch_id 0 83 miss% 0.7454794056748756
plot_id,batch_id 0 84 miss% 0.747634243024117
plot_id,batch_id 0 85 miss% 0.5621868909377064
plot_id,batch_id 0 86 miss% 0.6991308103062094
plot_id,batch_id 0 87 miss% 0.7260862793359895
plot_id,batch_id 0 88 miss% 0.7414072457649601
plot_id,batch_id 0 89 miss% 0.7487895528700443
plot_id,batch_id 0 90 miss% 0.529066467010992
plot_id,batch_id 0 91 miss% 0.6969758735165609
plot_id,batch_id 0 92 miss% 0.7153206740847785
plot_id,batch_id 0 93 miss% 0.7361158769022418
plot_id,batch_id 0 94 miss% 0.7438217821747038
plot_id,batch_id 0 95 miss% 0.5266065227676161
plot_id,batch_id 0 96 miss% 0.6739024109114137
plot_id,batch_id 0 97 miss% 0.7139594037928644
plot_id,batch_id 0 98 miss% 0.726764168145113
plot_id,batch_id 0 99 miss% 0.7364659464653525
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.63628606 0.73293016 0.74492671 0.7554917  0.75818387 0.63575384
 0.72654542 0.74928186 0.75595199 0.75733203 0.61000918 0.72499611
 0.73805825 0.7484777  0.75772205 0.62740772 0.71598128 0.74463669
 0.74703389 0.7515249  0.68973288 0.74657057 0.75448796 0.75946812
 0.76189522 0.68009761 0.74420511 0.75503536 0.75970205 0.76172542
 0.66807749 0.74063813 0.7529938  0.76043099 0.76131724 0.67085523
 0.74146001 0.75004282 0.75692862 0.75778722 0.71406781 0.75348234
 0.76109408 0.76413776 0.76608039 0.72165117 0.75421399 0.7603774
 0.76307253 0.76552542 0.72635338 0.75418591 0.75842113 0.76210698
 0.7660266  0.72321032 0.75145946 0.7581943  0.76148171 0.76436315
 0.53521723 0.68399595 0.7191212  0.74053265 0.743867   0.52751344
 0.68298765 0.70592841 0.73126223 0.73935225 0.49950023 0.69020847
 0.69340992 0.71760662 0.73029136 0.49148454 0.63815656 0.68481076
 0.72102172 0.7265566  0.5663181  0.70577458 0.73195036 0.74547941
 0.74763424 0.56218689 0.69913081 0.72608628 0.74140725 0.74878955
 0.52906647 0.69697587 0.71532067 0.73611588 0.74382178 0.52660652
 0.67390241 0.7139594  0.72676417 0.73646595]
for model  159 the mean error 0.714180705023298
all id 159 hidden_dim 32 learning_rate 0.02 num_layers 5 frames 25 out win 3 err 0.714180705023298
Launcher: Job 160 completed in 9557 seconds.
Launcher: Task 150 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 52800
total number of trained parameters  134801
Epoch:0, Train loss:0.497999, valid loss:0.504844
Epoch:1, Train loss:0.354002, valid loss:0.359155
Epoch:2, Train loss:0.341625, valid loss:0.357270
Epoch:3, Train loss:0.339520, valid loss:0.356557
Epoch:4, Train loss:0.338682, valid loss:0.356357
Epoch:5, Train loss:0.338335, valid loss:0.356732
Epoch:6, Train loss:0.338065, valid loss:0.356227
Epoch:7, Train loss:0.337855, valid loss:0.356153
Epoch:8, Train loss:0.337789, valid loss:0.356069
Epoch:9, Train loss:0.337680, valid loss:0.356316
Epoch:10, Train loss:0.337585, valid loss:0.356172
Epoch:11, Train loss:0.336928, valid loss:0.355684
Epoch:12, Train loss:0.336878, valid loss:0.355624
Epoch:13, Train loss:0.336838, valid loss:0.356028
Epoch:14, Train loss:0.336834, valid loss:0.355601
Epoch:15, Train loss:0.336808, valid loss:0.355755
Epoch:16, Train loss:0.336764, valid loss:0.355599
Epoch:17, Train loss:0.336756, valid loss:0.355563
Epoch:18, Train loss:0.336688, valid loss:0.355504
Epoch:19, Train loss:0.336709, valid loss:0.355665
Epoch:20, Train loss:0.336675, valid loss:0.355459
Epoch:21, Train loss:0.336362, valid loss:0.355444
Epoch:22, Train loss:0.336365, valid loss:0.355487
Epoch:23, Train loss:0.336380, valid loss:0.355426
Epoch:24, Train loss:0.336357, valid loss:0.355436
Epoch:25, Train loss:0.336350, valid loss:0.355451
Epoch:26, Train loss:0.336316, valid loss:0.355466
Epoch:27, Train loss:0.336318, valid loss:0.355450
Epoch:28, Train loss:0.336322, valid loss:0.355421
Epoch:29, Train loss:0.336316, valid loss:0.355430
Epoch:30, Train loss:0.336302, valid loss:0.355538
Epoch:31, Train loss:0.336153, valid loss:0.355375
Epoch:32, Train loss:0.336157, valid loss:0.355394
Epoch:33, Train loss:0.336144, valid loss:0.355363
Epoch:34, Train loss:0.336153, valid loss:0.355391
Epoch:35, Train loss:0.336132, valid loss:0.355369
Epoch:36, Train loss:0.336134, valid loss:0.355344
Epoch:37, Train loss:0.336128, valid loss:0.355440
Epoch:38, Train loss:0.336118, valid loss:0.355373
Epoch:39, Train loss:0.336125, valid loss:0.355429
Epoch:40, Train loss:0.336129, valid loss:0.355463
Epoch:41, Train loss:0.336060, valid loss:0.355336
Epoch:42, Train loss:0.336054, valid loss:0.355345
Epoch:43, Train loss:0.336047, valid loss:0.355367
Epoch:44, Train loss:0.336048, valid loss:0.355352
Epoch:45, Train loss:0.336042, valid loss:0.355351
Epoch:46, Train loss:0.336042, valid loss:0.355354
Epoch:47, Train loss:0.336038, valid loss:0.355327
Epoch:48, Train loss:0.336040, valid loss:0.355344
Epoch:49, Train loss:0.336033, valid loss:0.355360
Epoch:50, Train loss:0.336033, valid loss:0.355359
Epoch:51, Train loss:0.336004, valid loss:0.355335
Epoch:52, Train loss:0.336009, valid loss:0.355352
Epoch:53, Train loss:0.336003, valid loss:0.355341
Epoch:54, Train loss:0.336002, valid loss:0.355348
Epoch:55, Train loss:0.335999, valid loss:0.355352
Epoch:56, Train loss:0.335998, valid loss:0.355344
Epoch:57, Train loss:0.335996, valid loss:0.355346
Epoch:58, Train loss:0.336001, valid loss:0.355345
Epoch:59, Train loss:0.335993, valid loss:0.355337
Epoch:60, Train loss:0.335993, valid loss:0.355333
training time 9392.175175189972
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.6393227259184101
plot_id,batch_id 0 1 miss% 0.7334280565559181
plot_id,batch_id 0 2 miss% 0.7447053190229871
plot_id,batch_id 0 3 miss% 0.753665348319508
plot_id,batch_id 0 4 miss% 0.7604155832179715
plot_id,batch_id 0 5 miss% 0.6382168212833743
plot_id,batch_id 0 6 miss% 0.7259178436847357
plot_id,batch_id 0 7 miss% 0.7461257369034563
plot_id,batch_id 0 8 miss% 0.7552965963178
plot_id,batch_id 0 9 miss% 0.7568549038512341
plot_id,batch_id 0 10 miss% 0.6111153606743912
plot_id,batch_id 0 11 miss% 0.7266131452702124
plot_id,batch_id 0 12 miss% 0.736305569926879
plot_id,batch_id 0 13 miss% 0.7474674194501213
plot_id,batch_id 0 14 miss% 0.7556993113575297
plot_id,batch_id 0 15 miss% 0.6269066829625312
plot_id,batch_id 0 16 miss% 0.7158496488154327
plot_id,batch_id 0 17 miss% 0.7465461896664171
plot_id,batch_id 0 18 miss% 0.7497048745857666
plot_id,batch_id 0 19 miss% 0.7513030922360824
plot_id,batch_id 0 20 miss% 0.6851077622077147
plot_id,batch_id 0 21 miss% 0.7489003331074588
plot_id,batch_id 0 22 miss% 0.7572740174784311
plot_id,batch_id 0 23 miss% 0.761075229256625
plot_id,batch_id 0 24 miss% 0.7638221421897483
plot_id,batch_id 0 25 miss% 0.6807163233077871
plot_id,batch_id 0 26 miss% 0.7445077947518423
plot_id,batch_id 0 27 miss% 0.7540830690076122
plot_id,batch_id 0 28 miss% 0.7603184038185011
plot_id,batch_id 0 29 miss% 0.76273392963359
plot_id,batch_id 0 30 miss% 0.6624751646875714
plot_id,batch_id 0 31 miss% 0.7385998885411038
plot_id,batch_id 0 32 miss% 0.7511456842940926
plot_id,batch_id 0 33 miss% 0.7603318032716233
plot_id,batch_id 0 34 miss% 0.7622564476909685
plot_id,batch_id 0 35 miss% 0.6667235181933276
plot_id,batch_id 0 36 miss% 0.7442804411527791
plot_id,batch_id 0 37 miss% 0.7483772597720102
plot_id,batch_id 0 38 miss% 0.7574152784959436
plot_id,batch_id 0 39 miss% 0.7580619403127947
plot_id,batch_id 0 40 miss% 0.7166840165179983
plot_id,batch_id 0 41 miss% 0.754698835693086
plot_id,batch_id 0 42 miss% 0.7617905979231548
plot_id,batch_id 0 43 miss% 0.7645125737994477
plot_id,batch_id 0 44 miss% 0.7667908743487625
plot_id,batch_id 0 45 miss% 0.7212549749440865
plot_id,batch_id 0 46 miss% 0.7534093972194715
plot_id,batch_id 0 47 miss% 0.7629852999625824
plot_id,batch_id 0 48 miss% 0.7629621087681848
plot_id,batch_id 0 49 miss% 0.7658899466930651
plot_id,batch_id 0 50 miss% 0.7277525188810138
plot_id,batch_id 0 51 miss% 0.7527292559858185
plot_id,batch_id 0 52 miss% 0.7570208551379183
plot_id,batch_id 0 53 miss% 0.7619027497197044
plot_id,batch_id 0 54 miss% 0.7684379824653056
plot_id,batch_id 0 55 miss% 0.7245399408950783
plot_id,batch_id 0 56 miss% 0.7504172762431103
plot_id,batch_id 0 57 miss% 0.7576495800196139
plot_id,batch_id 0 58 miss% 0.76302303707201
plot_id,batch_id 0 59 miss% 0.7667381841760836
plot_id,batch_id 0 60 miss% 0.5384752865906645
plot_id,batch_id 0 61 miss% 0.6873604643563656
plot_id,batch_id 0 62 miss% 0.7195277869625242
plot_id,batch_id 0 63 miss% 0.7375199222657363
plot_id,batch_id 0 64 miss% 0.741784962514899
plot_id,batch_id 0 65 miss% 0.5273181125897141
plot_id,batch_id 0 66 miss% 0.6830022345193344
plot_id,batch_id 0 67 miss% 0.709597856332627
plot_id,batch_id 0 68 miss% 0.7348612048928369
plot_id,batch_id 0 69 miss% 0.7410275370648902
plot_id,batch_id 0 70 miss% 0.5036800139054222
plot_id,batch_id 0 71 miss% 0.6809593471250156
plot_id,batch_id 0 72 miss% 0.698024410752665
plot_id,batch_id 0 73 miss% 0.7197752586095627
plot_id,batch_id 0 74 miss% 0.7263462099962346
plot_id,batch_id 0 75 miss% 0.4847911458785264
plot_id,batch_id 0 76 miss% 0.639939670880259
plot_id,batch_id 0 77 miss% 0.6898067658026727
plot_id,batch_id 0 78 miss% 0.7161029869234528
plot_id,batch_id 0 79 miss% 0.7306997873389988
plot_id,batch_id 0 80 miss% 0.5621798965077567
plot_id,batch_id 0 81 miss% 0.7095848997737779
plot_id,batch_id 0 82 miss% 0.7332415446149516
plot_id,batch_id 0 83 miss% 0.7450133649481717
plot_id,batch_id 0 84 miss% 0.745561974946481
plot_id,batch_id 0 85 miss% 0.5616691370057667
plot_id,batch_id 0 86 miss% 0.7010049970371139
plot_id,batch_id 0 87 miss% 0.7247032404223064
plot_id,batch_id 0 88 miss% 0.7434443549036448
plot_id,batch_id 0 89 miss% 0.7485057501070748
plot_id,batch_id 0 90 miss% 0.5336163979506667
plot_id,batch_id 0 91 miss% 0.6958709158911164
plot_id,batch_id 0 92 miss% 0.7141713003905987
plot_id,batch_id 0 93 miss% 0.7368812544862112
plot_id,batch_id 0 94 miss% 0.7481058769825876
plot_id,batch_id 0 95 miss% 0.5254214990546424
plot_id,batch_id 0 96 miss% 0.6746516420073883
plot_id,batch_id 0 97 miss% 0.7132143233322532
plot_id,batch_id 0 98 miss% 0.7267492973081442
plot_id,batch_id 0 99 miss% 0.7364473409587704
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.63932273 0.73342806 0.74470532 0.75366535 0.76041558 0.63821682
 0.72591784 0.74612574 0.7552966  0.7568549  0.61111536 0.72661315
 0.73630557 0.74746742 0.75569931 0.62690668 0.71584965 0.74654619
 0.74970487 0.75130309 0.68510776 0.74890033 0.75727402 0.76107523
 0.76382214 0.68071632 0.74450779 0.75408307 0.7603184  0.76273393
 0.66247516 0.73859989 0.75114568 0.7603318  0.76225645 0.66672352
 0.74428044 0.74837726 0.75741528 0.75806194 0.71668402 0.75469884
 0.7617906  0.76451257 0.76679087 0.72125497 0.7534094  0.7629853
 0.76296211 0.76588995 0.72775252 0.75272926 0.75702086 0.76190275
 0.76843798 0.72453994 0.75041728 0.75764958 0.76302304 0.76673818
 0.53847529 0.68736046 0.71952779 0.73751992 0.74178496 0.52731811
 0.68300223 0.70959786 0.7348612  0.74102754 0.50368001 0.68095935
 0.69802441 0.71977526 0.72634621 0.48479115 0.63993967 0.68980677
 0.71610299 0.73069979 0.5621799  0.7095849  0.73324154 0.74501336
 0.74556197 0.56166914 0.701005   0.72470324 0.74344435 0.74850575
 0.5336164  0.69587092 0.7141713  0.73688125 0.74810588 0.5254215
 0.67465164 0.71321432 0.7267493  0.73644734]
for model  105 the mean error 0.7144149664158961
all id 105 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 25 out win 3 err 0.7144149664158961
Launcher: Job 106 completed in 9573 seconds.
Launcher: Task 248 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 40800
total number of trained parameters  134801
Epoch:0, Train loss:0.648029, valid loss:0.652804
Epoch:1, Train loss:0.516163, valid loss:0.519927
Epoch:2, Train loss:0.499795, valid loss:0.517719
Epoch:3, Train loss:0.496895, valid loss:0.517776
Epoch:4, Train loss:0.494610, valid loss:0.516722
Epoch:5, Train loss:0.493809, valid loss:0.516175
Epoch:6, Train loss:0.493601, valid loss:0.516652
Epoch:7, Train loss:0.492934, valid loss:0.516071
Epoch:8, Train loss:0.492430, valid loss:0.516259
Epoch:9, Train loss:0.492647, valid loss:0.516007
Epoch:10, Train loss:0.492074, valid loss:0.515827
Epoch:11, Train loss:0.490903, valid loss:0.515033
Epoch:12, Train loss:0.490548, valid loss:0.515051
Epoch:13, Train loss:0.490577, valid loss:0.514648
Epoch:14, Train loss:0.490637, valid loss:0.514826
Epoch:15, Train loss:0.490220, valid loss:0.514623
Epoch:16, Train loss:0.490360, valid loss:0.515402
Epoch:17, Train loss:0.490175, valid loss:0.514753
Epoch:18, Train loss:0.490273, valid loss:0.514750
Epoch:19, Train loss:0.490027, valid loss:0.514627
Epoch:20, Train loss:0.489960, valid loss:0.514506
Epoch:21, Train loss:0.489299, valid loss:0.514234
Epoch:22, Train loss:0.489244, valid loss:0.514067
Epoch:23, Train loss:0.489264, valid loss:0.514144
Epoch:24, Train loss:0.489228, valid loss:0.514258
Epoch:25, Train loss:0.489212, valid loss:0.514218
Epoch:26, Train loss:0.489179, valid loss:0.514625
Epoch:27, Train loss:0.489207, valid loss:0.514147
Epoch:28, Train loss:0.489075, valid loss:0.514750
Epoch:29, Train loss:0.489088, valid loss:0.514137
Epoch:30, Train loss:0.489118, valid loss:0.514264
Epoch:31, Train loss:0.488723, valid loss:0.513993
Epoch:32, Train loss:0.488643, valid loss:0.514083
Epoch:33, Train loss:0.488690, valid loss:0.514046
Epoch:34, Train loss:0.488712, valid loss:0.513965
Epoch:35, Train loss:0.488670, valid loss:0.513969
Epoch:36, Train loss:0.488632, valid loss:0.513996
Epoch:37, Train loss:0.488704, valid loss:0.514057
Epoch:38, Train loss:0.488645, valid loss:0.514138
Epoch:39, Train loss:0.488621, valid loss:0.513951
Epoch:40, Train loss:0.488628, valid loss:0.514186
Epoch:41, Train loss:0.488418, valid loss:0.513922
Epoch:42, Train loss:0.488400, valid loss:0.513959
Epoch:43, Train loss:0.488427, valid loss:0.513990
Epoch:44, Train loss:0.488417, valid loss:0.513916
Epoch:45, Train loss:0.488413, valid loss:0.513905
Epoch:46, Train loss:0.488406, valid loss:0.513993
Epoch:47, Train loss:0.488418, valid loss:0.514009
Epoch:48, Train loss:0.488381, valid loss:0.513952
Epoch:49, Train loss:0.488374, valid loss:0.513903
Epoch:50, Train loss:0.488386, valid loss:0.513935
Epoch:51, Train loss:0.488299, valid loss:0.513899
Epoch:52, Train loss:0.488296, valid loss:0.513883
Epoch:53, Train loss:0.488288, valid loss:0.514020
Epoch:54, Train loss:0.488291, valid loss:0.513903
Epoch:55, Train loss:0.488284, valid loss:0.513946
Epoch:56, Train loss:0.488290, valid loss:0.513872
Epoch:57, Train loss:0.488289, valid loss:0.513910
Epoch:58, Train loss:0.488276, valid loss:0.513927
Epoch:59, Train loss:0.488275, valid loss:0.513907
Epoch:60, Train loss:0.488276, valid loss:0.513918
training time 9431.784673452377
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.7730727051090337
plot_id,batch_id 0 1 miss% 0.8224892215695891
plot_id,batch_id 0 2 miss% 0.8320305790787489
plot_id,batch_id 0 3 miss% 0.8334602866830376
plot_id,batch_id 0 4 miss% 0.8359158900402365
plot_id,batch_id 0 5 miss% 0.7686619079867653
plot_id,batch_id 0 6 miss% 0.8160981132167848
plot_id,batch_id 0 7 miss% 0.8302470973363446
plot_id,batch_id 0 8 miss% 0.8343062531549836
plot_id,batch_id 0 9 miss% 0.8368051118508089
plot_id,batch_id 0 10 miss% 0.7488737948649997
plot_id,batch_id 0 11 miss% 0.8165025155638582
plot_id,batch_id 0 12 miss% 0.8250804708230325
plot_id,batch_id 0 13 miss% 0.8316882697770142
plot_id,batch_id 0 14 miss% 0.8336039532021058
plot_id,batch_id 0 15 miss% 0.7628335213079819
plot_id,batch_id 0 16 miss% 0.8139596993156596
plot_id,batch_id 0 17 miss% 0.8269348819696292
plot_id,batch_id 0 18 miss% 0.8319299451020227
plot_id,batch_id 0 19 miss% 0.8353852294983535
plot_id,batch_id 0 20 miss% 0.7985649949944146
plot_id,batch_id 0 21 miss% 0.8304522567375663
plot_id,batch_id 0 22 miss% 0.8362270344987016
plot_id,batch_id 0 23 miss% 0.8367069554013781
plot_id,batch_id 0 24 miss% 0.8415092451748772
plot_id,batch_id 0 25 miss% 0.796774605644741
plot_id,batch_id 0 26 miss% 0.8273369964681997
plot_id,batch_id 0 27 miss% 0.8320818668143594
plot_id,batch_id 0 28 miss% 0.836184685174859
plot_id,batch_id 0 29 miss% 0.836673271307622
plot_id,batch_id 0 30 miss% 0.7886485066320627
plot_id,batch_id 0 31 miss% 0.8247511074114168
plot_id,batch_id 0 32 miss% 0.8321121847602095
plot_id,batch_id 0 33 miss% 0.8364197469060676
plot_id,batch_id 0 34 miss% 0.8370556997690014
plot_id,batch_id 0 35 miss% 0.7855944299229086
plot_id,batch_id 0 36 miss% 0.8292041113828156
plot_id,batch_id 0 37 miss% 0.8315984250328103
plot_id,batch_id 0 38 miss% 0.8356954165663131
plot_id,batch_id 0 39 miss% 0.836620572976586
plot_id,batch_id 0 40 miss% 0.8225545634029316
plot_id,batch_id 0 41 miss% 0.8340165399409979
plot_id,batch_id 0 42 miss% 0.8445478620483943
plot_id,batch_id 0 43 miss% 0.8392704934264859
plot_id,batch_id 0 44 miss% 0.8416182045733838
plot_id,batch_id 0 45 miss% 0.8171615995471226
plot_id,batch_id 0 46 miss% 0.8341336804917834
plot_id,batch_id 0 47 miss% 0.8382055984507576
plot_id,batch_id 0 48 miss% 0.8390311849627574
plot_id,batch_id 0 49 miss% 0.8423824689988434
plot_id,batch_id 0 50 miss% 0.8208013940923019
plot_id,batch_id 0 51 miss% 0.8332459517242655
plot_id,batch_id 0 52 miss% 0.8348028916310909
plot_id,batch_id 0 53 miss% 0.8375220901397622
plot_id,batch_id 0 54 miss% 0.8425523812918665
plot_id,batch_id 0 55 miss% 0.8107641621184668
plot_id,batch_id 0 56 miss% 0.8332446083160077
plot_id,batch_id 0 57 miss% 0.8358752653324505
plot_id,batch_id 0 58 miss% 0.838976430199038
plot_id,batch_id 0 59 miss% 0.8416123817423059
plot_id,batch_id 0 60 miss% 0.7120211909256652
plot_id,batch_id 0 61 miss% 0.8003109705288688
plot_id,batch_id 0 62 miss% 0.8164723550565328
plot_id,batch_id 0 63 miss% 0.8291526535685798
plot_id,batch_id 0 64 miss% 0.8288318359920617
plot_id,batch_id 0 65 miss% 0.7101681160913643
plot_id,batch_id 0 66 miss% 0.7928129538405476
plot_id,batch_id 0 67 miss% 0.8076144358608156
plot_id,batch_id 0 68 miss% 0.8224836363627964
plot_id,batch_id 0 69 miss% 0.8268102309568405
plot_id,batch_id 0 70 miss% 0.66266534393261
plot_id,batch_id 0 71 miss% 0.7945386836230441
plot_id,batch_id 0 72 miss% 0.7997569738313286
plot_id,batch_id 0 73 miss% 0.8155430652871499
plot_id,batch_id 0 74 miss% 0.8210776800697774
plot_id,batch_id 0 75 miss% 0.6703367638462022
plot_id,batch_id 0 76 miss% 0.7923259638957939
plot_id,batch_id 0 77 miss% 0.7919365712652899
plot_id,batch_id 0 78 miss% 0.8107498307980087
plot_id,batch_id 0 79 miss% 0.8096869788854442
plot_id,batch_id 0 80 miss% 0.7308677037753248
plot_id,batch_id 0 81 miss% 0.8150624946686073
plot_id,batch_id 0 82 miss% 0.8237545118754738
plot_id,batch_id 0 83 miss% 0.8314824174524041
plot_id,batch_id 0 84 miss% 0.831945762105686
plot_id,batch_id 0 85 miss% 0.7267846686796334
plot_id,batch_id 0 86 miss% 0.8013046954831025
plot_id,batch_id 0 87 miss% 0.8191539874388226
plot_id,batch_id 0 88 miss% 0.8295161765771487
plot_id,batch_id 0 89 miss% 0.8307868606799242
plot_id,batch_id 0 90 miss% 0.6893421599632168
plot_id,batch_id 0 91 miss% 0.7987358163132746
plot_id,batch_id 0 92 miss% 0.8108586865583605
plot_id,batch_id 0 93 miss% 0.8204451886144072
plot_id,batch_id 0 94 miss% 0.8319039923068999
plot_id,batch_id 0 95 miss% 0.7175435557634604
plot_id,batch_id 0 96 miss% 0.7944303566149761
plot_id,batch_id 0 97 miss% 0.8121359809334591
plot_id,batch_id 0 98 miss% 0.8189685319818051
plot_id,batch_id 0 99 miss% 0.8225628901328869
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.77307271 0.82248922 0.83203058 0.83346029 0.83591589 0.76866191
 0.81609811 0.8302471  0.83430625 0.83680511 0.74887379 0.81650252
 0.82508047 0.83168827 0.83360395 0.76283352 0.8139597  0.82693488
 0.83192995 0.83538523 0.79856499 0.83045226 0.83622703 0.83670696
 0.84150925 0.79677461 0.827337   0.83208187 0.83618469 0.83667327
 0.78864851 0.82475111 0.83211218 0.83641975 0.8370557  0.78559443
 0.82920411 0.83159843 0.83569542 0.83662057 0.82255456 0.83401654
 0.84454786 0.83927049 0.8416182  0.8171616  0.83413368 0.8382056
 0.83903118 0.84238247 0.82080139 0.83324595 0.83480289 0.83752209
 0.84255238 0.81076416 0.83324461 0.83587527 0.83897643 0.84161238
 0.71202119 0.80031097 0.81647236 0.82915265 0.82883184 0.71016812
 0.79281295 0.80761444 0.82248364 0.82681023 0.66266534 0.79453868
 0.79975697 0.81554307 0.82107768 0.67033676 0.79232596 0.79193657
 0.81074983 0.80968698 0.7308677  0.81506249 0.82375451 0.83148242
 0.83194576 0.72678467 0.8013047  0.81915399 0.82951618 0.83078686
 0.68934216 0.79873582 0.81085869 0.82044519 0.83190399 0.71754356
 0.79443036 0.81213598 0.81896853 0.82256289]
for model  79 the mean error 0.8117529398599652
all id 79 hidden_dim 32 learning_rate 0.02 num_layers 5 frames 21 out win 4 err 0.8117529398599652
Launcher: Job 80 completed in 9614 seconds.
Launcher: Task 234 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  107025
Epoch:0, Train loss:0.299547, valid loss:0.293055
Epoch:1, Train loss:0.018668, valid loss:0.003774
Epoch:2, Train loss:0.005188, valid loss:0.002884
Epoch:3, Train loss:0.004012, valid loss:0.002347
Epoch:4, Train loss:0.003208, valid loss:0.001593
Epoch:5, Train loss:0.002739, valid loss:0.001574
Epoch:6, Train loss:0.002432, valid loss:0.001288
Epoch:7, Train loss:0.002318, valid loss:0.001482
Epoch:8, Train loss:0.002126, valid loss:0.001247
Epoch:9, Train loss:0.002023, valid loss:0.001010
Epoch:10, Train loss:0.001981, valid loss:0.001304
Epoch:11, Train loss:0.001390, valid loss:0.000951
Epoch:12, Train loss:0.001394, valid loss:0.001019
Epoch:13, Train loss:0.001395, valid loss:0.000929
Epoch:14, Train loss:0.001348, valid loss:0.000854
Epoch:15, Train loss:0.001295, valid loss:0.000830
Epoch:16, Train loss:0.001301, valid loss:0.000985
Epoch:17, Train loss:0.001255, valid loss:0.000997
Epoch:18, Train loss:0.001229, valid loss:0.000779
Epoch:19, Train loss:0.001244, valid loss:0.000862
Epoch:20, Train loss:0.001178, valid loss:0.000740
Epoch:21, Train loss:0.000909, valid loss:0.000631
Epoch:22, Train loss:0.000911, valid loss:0.000589
Epoch:23, Train loss:0.000911, valid loss:0.000659
Epoch:24, Train loss:0.000888, valid loss:0.000676
Epoch:25, Train loss:0.000889, valid loss:0.001208
Epoch:26, Train loss:0.000882, valid loss:0.000612
Epoch:27, Train loss:0.000848, valid loss:0.000672
Epoch:28, Train loss:0.000880, valid loss:0.000718
Epoch:29, Train loss:0.000848, valid loss:0.000590
Epoch:30, Train loss:0.000848, valid loss:0.000585
Epoch:31, Train loss:0.000699, valid loss:0.000572
Epoch:32, Train loss:0.000703, valid loss:0.000584
Epoch:33, Train loss:0.000703, valid loss:0.000526
Epoch:34, Train loss:0.000708, valid loss:0.000519
Epoch:35, Train loss:0.000698, valid loss:0.000534
Epoch:36, Train loss:0.000699, valid loss:0.000505
Epoch:37, Train loss:0.000686, valid loss:0.000580
Epoch:38, Train loss:0.000671, valid loss:0.000570
Epoch:39, Train loss:0.000672, valid loss:0.000602
Epoch:40, Train loss:0.000668, valid loss:0.000515
Epoch:41, Train loss:0.000609, valid loss:0.000581
Epoch:42, Train loss:0.000604, valid loss:0.000524
Epoch:43, Train loss:0.000599, valid loss:0.000519
Epoch:44, Train loss:0.000595, valid loss:0.000503
Epoch:45, Train loss:0.000596, valid loss:0.000509
Epoch:46, Train loss:0.000594, valid loss:0.000520
Epoch:47, Train loss:0.000598, valid loss:0.000494
Epoch:48, Train loss:0.000594, valid loss:0.000499
Epoch:49, Train loss:0.000585, valid loss:0.000511
Epoch:50, Train loss:0.000588, valid loss:0.000508
Epoch:51, Train loss:0.000555, valid loss:0.000492
Epoch:52, Train loss:0.000552, valid loss:0.000499
Epoch:53, Train loss:0.000555, valid loss:0.000501
Epoch:54, Train loss:0.000557, valid loss:0.000505
Epoch:55, Train loss:0.000553, valid loss:0.000497
Epoch:56, Train loss:0.000554, valid loss:0.000489
Epoch:57, Train loss:0.000550, valid loss:0.000490
Epoch:58, Train loss:0.000548, valid loss:0.000495
Epoch:59, Train loss:0.000550, valid loss:0.000477
Epoch:60, Train loss:0.000547, valid loss:0.000506
training time 9508.152853012085
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.02169217196206137
plot_id,batch_id 0 1 miss% 0.027579346426273018
plot_id,batch_id 0 2 miss% 0.024117542300292477
plot_id,batch_id 0 3 miss% 0.015920781071074315
plot_id,batch_id 0 4 miss% 0.026247142746198396
plot_id,batch_id 0 5 miss% 0.033286699286887676
plot_id,batch_id 0 6 miss% 0.036661351400409735
plot_id,batch_id 0 7 miss% 0.028550650975932335
plot_id,batch_id 0 8 miss% 0.025355620982703303
plot_id,batch_id 0 9 miss% 0.01971495243046653
plot_id,batch_id 0 10 miss% 0.051320534277666204
plot_id,batch_id 0 11 miss% 0.044277601185230525
plot_id,batch_id 0 12 miss% 0.025526070263650318
plot_id,batch_id 0 13 miss% 0.01548987354678681
plot_id,batch_id 0 14 miss% 0.025795979204626108
plot_id,batch_id 0 15 miss% 0.04035124349577387
plot_id,batch_id 0 16 miss% 0.027868526596166187
plot_id,batch_id 0 17 miss% 0.025475621598101655
plot_id,batch_id 0 18 miss% 0.03177399287330652
plot_id,batch_id 0 19 miss% 0.031227839172520346
plot_id,batch_id 0 20 miss% 0.031281066683105964
plot_id,batch_id 0 21 miss% 0.021366898979391008
plot_id,batch_id 0 22 miss% 0.023935292911476783
plot_id,batch_id 0 23 miss% 0.019655650084129398
plot_id,batch_id 0 24 miss% 0.020104943212945967
plot_id,batch_id 0 25 miss% 0.034756146323214075
plot_id,batch_id 0 26 miss% 0.027163276962163762
plot_id,batch_id 0 27 miss% 0.0245443307614555
plot_id,batch_id 0 28 miss% 0.02683448683896706
plot_id,batch_id 0 29 miss% 0.017864700250151543
plot_id,batch_id 0 30 miss% 0.04729262428243858
plot_id,batch_id 0 31 miss% 0.030830519928680102
plot_id,batch_id 0 32 miss% 0.029468885789627437
plot_id,batch_id 0 33 miss% 0.02426590690706221
plot_id,batch_id 0 34 miss% 0.01987051156432156
plot_id,batch_id 0 35 miss% 0.036146452914906174
plot_id,batch_id 0 36 miss% 0.03165848920174263
plot_id,batch_id 0 37 miss% 0.021495773814503467
plot_id,batch_id 0 38 miss% 0.019059716041756908
plot_id,batch_id 0 39 miss% 0.021647759931675357
plot_id,batch_id 0 40 miss% 0.04678764780777092
plot_id,batch_id 0 41 miss% 0.019949721303653776
plot_id,batch_id 0 42 miss% 0.018362772716266736
plot_id,batch_id 0 43 miss% 0.02936672844151749
plot_id,batch_id 0 44 miss% 0.023643744114738765
plot_id,batch_id 0 45 miss% 0.02455278779224858
plot_id,batch_id 0 46 miss% 0.02561977789006614
plot_id,batch_id 0 47 miss% 0.020292774068490213
plot_id,batch_id 0 48 miss% 0.02332295597818622
plot_id,batch_id 0 49 miss% 0.02253580059960098
plot_id,batch_id 0 50 miss% 0.04054464932395209
plot_id,batch_id 0 51 miss% 0.024237633657938178
plot_id,batch_id 0 52 miss% 0.019455781601838605
plot_id,batch_id 0 53 miss% 0.00900684444971504
plot_id,batch_id 0 54 miss% 0.02684581343716458
plot_id,batch_id 0 55 miss% 0.03715254991149127
plot_id,batch_id 0 56 miss% 0.024473124758735105
plot_id,batch_id 0 57 miss% 0.02313289135943235
plot_id,batch_id 0 58 miss% 0.018414599001920575
plot_id,batch_id 0 59 miss% 0.02605584476776694
plot_id,batch_id 0 60 miss% 0.03323108653570127
plot_id,batch_id 0 61 miss% 0.03938866130497798
plot_id,batch_id 0 62 miss% 0.02380653116715626
plot_id,batch_id 0 63 miss% 0.03331469862662802
plot_id,batch_id 0 64 miss% 0.03160679429551729
plot_id,batch_id 0 65 miss% 0.05063237023898629
plot_id,batch_id 0 66 miss% 0.034903842042119416
plot_id,batch_id 0 67 miss% 0.03213289022827939
plot_id,batch_id 0 68 miss% 0.02618029155502463
plot_id,batch_id 0 69 miss% 0.01796149925893705
plot_id,batch_id 0 70 miss% 0.04040104115356112
plot_id,batch_id 0 71 miss% 0.04304026335495396
plot_id,batch_id 0 72 miss% 0.02742779519614229
plot_id,batch_id 0 73 miss% 0.03250803449994984
plot_id,batch_id 0 74 miss% 0.033439909494945665
plot_id,batch_id 0 75 miss% 0.032589825978963004
plot_id,batch_id 0 76 miss% 0.041463963727706506
plot_id,batch_id 0 77 miss% 0.03743772181620716
plot_id,batch_id 0 78 miss% 0.03774274027516459
plot_id,batch_id 0 79 miss% 0.03057337518408034
plot_id,batch_id 0 80 miss% 0.03723812282906175
plot_id,batch_id 0 81 miss% 0.023252576734529105
plot_id,batch_id 0 82 miss% 0.025078121534147034
plot_id,batch_id 0 83 miss% 0.030239899980981436
plot_id,batch_id 0 84 miss% 0.02499368925591448
plot_id,batch_id 0 85 miss% 0.053661756801479485
plot_id,batch_id 0 86 miss% 0.022102405514953048
plot_id,batch_id 0 87 miss% 0.024426504421091975
plot_id,batch_id 0 88 miss% 0.030135595268233437
plot_id,batch_id 0 89 miss% 0.02592304883696715
plot_id,batch_id 0 90 miss% 0.028917954480577837
plot_id,batch_id 0 91 miss% 0.0423484685351522
plot_id,batch_id 0 92 miss% 0.02590103278867619
plot_id,batch_id 0 93 miss% 0.021980238425155216
plot_id,batch_id 0 94 miss% 0.02240857754044174
plot_id,batch_id 0 95 miss% 0.04364796963471805
plot_id,batch_id 0 96 miss% 0.02742519952899209
plot_id,batch_id 0 97 miss% 0.04541544597588515
plot_id,batch_id 0 98 miss% 0.02030424197681231
plot_id,batch_id 0 99 miss% 0.03194861076314386
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02169217 0.02757935 0.02411754 0.01592078 0.02624714 0.0332867
 0.03666135 0.02855065 0.02535562 0.01971495 0.05132053 0.0442776
 0.02552607 0.01548987 0.02579598 0.04035124 0.02786853 0.02547562
 0.03177399 0.03122784 0.03128107 0.0213669  0.02393529 0.01965565
 0.02010494 0.03475615 0.02716328 0.02454433 0.02683449 0.0178647
 0.04729262 0.03083052 0.02946889 0.02426591 0.01987051 0.03614645
 0.03165849 0.02149577 0.01905972 0.02164776 0.04678765 0.01994972
 0.01836277 0.02936673 0.02364374 0.02455279 0.02561978 0.02029277
 0.02332296 0.0225358  0.04054465 0.02423763 0.01945578 0.00900684
 0.02684581 0.03715255 0.02447312 0.02313289 0.0184146  0.02605584
 0.03323109 0.03938866 0.02380653 0.0333147  0.03160679 0.05063237
 0.03490384 0.03213289 0.02618029 0.0179615  0.04040104 0.04304026
 0.0274278  0.03250803 0.03343991 0.03258983 0.04146396 0.03743772
 0.03774274 0.03057338 0.03723812 0.02325258 0.02507812 0.0302399
 0.02499369 0.05366176 0.02210241 0.0244265  0.0301356  0.02592305
 0.02891795 0.04234847 0.02590103 0.02198024 0.02240858 0.04364797
 0.0274252  0.04541545 0.02030424 0.03194861]
for model  178 the mean error 0.028983642149242535
all id 178 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 31 out win 4 err 0.028983642149242535
Launcher: Job 179 completed in 9698 seconds.
Launcher: Task 62 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  77489
Epoch:0, Train loss:0.473889, valid loss:0.470487
Epoch:1, Train loss:0.039904, valid loss:0.006900
Epoch:2, Train loss:0.011100, valid loss:0.005143
Epoch:3, Train loss:0.008205, valid loss:0.003834
Epoch:4, Train loss:0.006379, valid loss:0.003321
Epoch:5, Train loss:0.005505, valid loss:0.002629
Epoch:6, Train loss:0.004765, valid loss:0.003547
Epoch:7, Train loss:0.004675, valid loss:0.002626
Epoch:8, Train loss:0.004291, valid loss:0.002289
Epoch:9, Train loss:0.004061, valid loss:0.002013
Epoch:10, Train loss:0.003845, valid loss:0.002375
Epoch:11, Train loss:0.002727, valid loss:0.001616
Epoch:12, Train loss:0.002657, valid loss:0.001726
Epoch:13, Train loss:0.002745, valid loss:0.001786
Epoch:14, Train loss:0.002608, valid loss:0.001600
Epoch:15, Train loss:0.002562, valid loss:0.001450
Epoch:16, Train loss:0.002466, valid loss:0.001882
Epoch:17, Train loss:0.002531, valid loss:0.001448
Epoch:18, Train loss:0.002400, valid loss:0.001417
Epoch:19, Train loss:0.002362, valid loss:0.001215
Epoch:20, Train loss:0.002416, valid loss:0.001696
Epoch:21, Train loss:0.001788, valid loss:0.001183
Epoch:22, Train loss:0.001689, valid loss:0.001091
Epoch:23, Train loss:0.001667, valid loss:0.000981
Epoch:24, Train loss:0.001691, valid loss:0.001219
Epoch:25, Train loss:0.001656, valid loss:0.001057
Epoch:26, Train loss:0.001685, valid loss:0.001028
Epoch:27, Train loss:0.001594, valid loss:0.000986
Epoch:28, Train loss:0.001646, valid loss:0.001035
Epoch:29, Train loss:0.001556, valid loss:0.001143
Epoch:30, Train loss:0.001578, valid loss:0.001225
Epoch:31, Train loss:0.001284, valid loss:0.000902
Epoch:32, Train loss:0.001274, valid loss:0.000916
Epoch:33, Train loss:0.001251, valid loss:0.000872
Epoch:34, Train loss:0.001276, valid loss:0.000947
Epoch:35, Train loss:0.001266, valid loss:0.000872
Epoch:36, Train loss:0.001250, valid loss:0.000920
Epoch:37, Train loss:0.001266, valid loss:0.000888
Epoch:38, Train loss:0.001211, valid loss:0.000854
Epoch:39, Train loss:0.001211, valid loss:0.000835
Epoch:40, Train loss:0.001237, valid loss:0.000948
Epoch:41, Train loss:0.001062, valid loss:0.000805
Epoch:42, Train loss:0.001060, valid loss:0.000855
Epoch:43, Train loss:0.001045, valid loss:0.000811
Epoch:44, Train loss:0.001047, valid loss:0.000817
Epoch:45, Train loss:0.001044, valid loss:0.000862
Epoch:46, Train loss:0.001045, valid loss:0.000803
Epoch:47, Train loss:0.001039, valid loss:0.000827
Epoch:48, Train loss:0.001042, valid loss:0.000786
Epoch:49, Train loss:0.001025, valid loss:0.000796
Epoch:50, Train loss:0.001031, valid loss:0.000790
Epoch:51, Train loss:0.000959, valid loss:0.000783
Epoch:52, Train loss:0.000951, valid loss:0.000786
Epoch:53, Train loss:0.000948, valid loss:0.000782
Epoch:54, Train loss:0.000947, valid loss:0.000777
Epoch:55, Train loss:0.000942, valid loss:0.000781
Epoch:56, Train loss:0.000948, valid loss:0.000793
Epoch:57, Train loss:0.000943, valid loss:0.000754
Epoch:58, Train loss:0.000936, valid loss:0.000788
Epoch:59, Train loss:0.000940, valid loss:0.000770
Epoch:60, Train loss:0.000930, valid loss:0.000804
training time 9509.685278892517
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.02654747846223134
plot_id,batch_id 0 1 miss% 0.021103729274949405
plot_id,batch_id 0 2 miss% 0.023078552031870175
plot_id,batch_id 0 3 miss% 0.02838973506773239
plot_id,batch_id 0 4 miss% 0.02834589279973958
plot_id,batch_id 0 5 miss% 0.04515025220004261
plot_id,batch_id 0 6 miss% 0.03248757362571984
plot_id,batch_id 0 7 miss% 0.024444081540753216
plot_id,batch_id 0 8 miss% 0.026434458730157383
plot_id,batch_id 0 9 miss% 0.02854533913290875
plot_id,batch_id 0 10 miss% 0.033439783749480013
plot_id,batch_id 0 11 miss% 0.046752664473689136
plot_id,batch_id 0 12 miss% 0.03102217653732579
plot_id,batch_id 0 13 miss% 0.031184104519061298
plot_id,batch_id 0 14 miss% 0.034012437034097245
plot_id,batch_id 0 15 miss% 0.03312620813962127
plot_id,batch_id 0 16 miss% 0.03192578526564235
plot_id,batch_id 0 17 miss% 0.02645664122143993
plot_id,batch_id 0 18 miss% 0.042554323784692934
plot_id,batch_id 0 19 miss% 0.032137384079940545
plot_id,batch_id 0 20 miss% 0.037374547715042034
plot_id,batch_id 0 21 miss% 0.022723672115815125
plot_id,batch_id 0 22 miss% 0.0328579735662061
plot_id,batch_id 0 23 miss% 0.03838349672228645
plot_id,batch_id 0 24 miss% 0.03579841112378609
plot_id,batch_id 0 25 miss% 0.03005959773709392
plot_id,batch_id 0 26 miss% 0.029856270531763252
plot_id,batch_id 0 27 miss% 0.027831997460199487
plot_id,batch_id 0 28 miss% 0.03199507719279084
plot_id,batch_id 0 29 miss% 0.038980003381642606
plot_id,batch_id 0 30 miss% 0.03855750410841765
plot_id,batch_id 0 31 miss% 0.028948354605529386
plot_id,batch_id 0 32 miss% 0.03147828716021065
plot_id,batch_id 0 33 miss% 0.03079566971783484
plot_id,batch_id 0 34 miss% 0.0342257440852627
plot_id,batch_id 0 35 miss% 0.03239074229713701
plot_id,batch_id 0 36 miss% 0.029700722241121778
plot_id,batch_id 0 37 miss% 0.024779741697795253
plot_id,batch_id 0 38 miss% 0.0247487410503977
plot_id,batch_id 0 39 miss% 0.02313994722012564
plot_id,batch_id 0 40 miss% 0.0473201321387396
plot_id,batch_id 0 41 miss% 0.02437671360749755
plot_id,batch_id 0 42 miss% 0.020207951237168363
plot_id,batch_id 0 43 miss% 0.04323934193960303
plot_id,batch_id 0 44 miss% 0.030269501787965815
plot_id,batch_id 0 45 miss% 0.02042921569839105
plot_id,batch_id 0 46 miss% 0.026521840053319166
plot_id,batch_id 0 47 miss% 0.03081482691903257
plot_id,batch_id 0 48 miss% 0.038796923697372795
plot_id,batch_id 0 49 miss% 0.030432228485933986
plot_id,batch_id 0 50 miss% 0.04484234796515937
plot_id,batch_id 0 51 miss% 0.02329131371287183
plot_id,batch_id 0 52 miss% 0.02458593799207136
plot_id,batch_id 0 53 miss% 0.020007460224490257
plot_id,batch_id 0 54 miss% 0.03728719254947667
plot_id,batch_id 0 55 miss% 0.04488404518535421
plot_id,batch_id 0 56 miss% 0.028292433254969978
plot_id,batch_id 0 57 miss% 0.021220802314532654
plot_id,batch_id 0 58 miss% 0.02292847846008486
plot_id,batch_id 0 59 miss% 0.030120807471282908
plot_id,batch_id 0 60 miss% 0.03419135294698097
plot_id,batch_id 0 61 miss% 0.030771586707679873
plot_id,batch_id 0 62 miss% 0.040863872270015186
plot_id,batch_id 0 63 miss% 0.022763171711866127
plot_id,batch_id 0 64 miss% 0.025374728727957832
plot_id,batch_id 0 65 miss% 0.03353471933579248
plot_id,batch_id 0 66 miss% 0.048920995378975034
plot_id,batch_id 0 67 miss% 0.03575798028895186
plot_id,batch_id 0 68 miss% 0.02148851320370734
plot_id,batch_id 0 69 miss%the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 16 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  35921
Epoch:0, Train loss:0.447953, valid loss:0.429300
Epoch:1, Train loss:0.023219, valid loss:0.004086
Epoch:2, Train loss:0.006847, valid loss:0.003297
Epoch:3, Train loss:0.005129, valid loss:0.002578
Epoch:4, Train loss:0.004281, valid loss:0.002148
Epoch:5, Train loss:0.003857, valid loss:0.001779
Epoch:6, Train loss:0.003443, valid loss:0.001613
Epoch:7, Train loss:0.003343, valid loss:0.001729
Epoch:8, Train loss:0.003155, valid loss:0.002065
Epoch:9, Train loss:0.003076, valid loss:0.001510
Epoch:10, Train loss:0.002983, valid loss:0.001386
Epoch:11, Train loss:0.002221, valid loss:0.001411
Epoch:12, Train loss:0.002162, valid loss:0.001386
Epoch:13, Train loss:0.002140, valid loss:0.001180
Epoch:14, Train loss:0.002122, valid loss:0.001256
Epoch:15, Train loss:0.002056, valid loss:0.001122
Epoch:16, Train loss:0.002074, valid loss:0.001121
Epoch:17, Train loss:0.002011, valid loss:0.001091
Epoch:18, Train loss:0.001980, valid loss:0.001382
Epoch:19, Train loss:0.001984, valid loss:0.001234
Epoch:20, Train loss:0.001954, valid loss:0.001151
Epoch:21, Train loss:0.001549, valid loss:0.000853
Epoch:22, Train loss:0.001544, valid loss:0.000850
Epoch:23, Train loss:0.001509, valid loss:0.000968
Epoch:24, Train loss:0.001514, valid loss:0.000975
Epoch:25, Train loss:0.001505, valid loss:0.000937
Epoch:26, Train loss:0.001506, valid loss:0.000957
Epoch:27, Train loss:0.001474, valid loss:0.000896
Epoch:28, Train loss:0.001452, valid loss:0.000882
Epoch:29, Train loss:0.001454, valid loss:0.000964
Epoch:30, Train loss:0.001461, valid loss:0.000859
Epoch:31, Train loss:0.001221, valid loss:0.000780
Epoch:32, Train loss:0.001210, valid loss:0.000839
Epoch:33, Train loss:0.001216, valid loss:0.000823
Epoch:34, Train loss:0.001204, valid loss:0.000792
Epoch:35, Train loss:0.001197, valid loss:0.000727
Epoch:36, Train loss:0.001197, valid loss:0.000773
Epoch:37, Train loss:0.001190, valid loss:0.000716
Epoch:38, Train loss:0.001191, valid loss:0.000775
Epoch:39, Train loss:0.001189, valid loss:0.000716
Epoch:40, Train loss:0.001193, valid loss:0.000768
Epoch:41, Train loss:0.001057, valid loss:0.000697
Epoch:42, Train loss:0.001046, valid loss:0.000727
Epoch:43, Train loss:0.001056, valid loss:0.000725
Epoch:44, Train loss:0.001041, valid loss:0.000792
Epoch:45, Train loss:0.001049, valid loss:0.000671
Epoch:46, Train loss:0.001040, valid loss:0.000756
Epoch:47, Train loss:0.001037, valid loss:0.000719
Epoch:48, Train loss:0.001035, valid loss:0.000697
Epoch:49, Train loss:0.001033, valid loss:0.000663
Epoch:50, Train loss:0.001025, valid loss:0.000677
Epoch:51, Train loss:0.000967, valid loss:0.000690
Epoch:52, Train loss:0.000960, valid loss:0.000696
Epoch:53, Train loss:0.000971, valid loss:0.000728
Epoch:54, Train loss:0.000957, valid loss:0.000655
Epoch:55, Train loss:0.000957, valid loss:0.000699
Epoch:56, Train loss:0.000959, valid loss:0.000674
Epoch:57, Train loss:0.000955, valid loss:0.000656
Epoch:58, Train loss:0.000951, valid loss:0.000667
Epoch:59, Train loss:0.000956, valid loss:0.000665
Epoch:60, Train loss:0.000953, valid loss:0.000672
training time 9518.2319586277
total number of trained parameters for initialize model 35921
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.03647981701699007
plot_id,batch_id 0 1 miss% 0.02200808544280034
plot_id,batch_id 0 2 miss% 0.020721803796331234
plot_id,batch_id 0 3 miss% 0.026754557160736218
plot_id,batch_id 0 4 miss% 0.029949769316286998
plot_id,batch_id 0 5 miss% 0.03491341408819369
plot_id,batch_id 0 6 miss% 0.025188878644094876
plot_id,batch_id 0 7 miss% 0.024458356647481985
plot_id,batch_id 0 8 miss% 0.02220053832473137
plot_id,batch_id 0 9 miss% 0.020596294457621424
plot_id,batch_id 0 10 miss% 0.03738986870453385
plot_id,batch_id 0 11 miss% 0.045095351155596666
plot_id,batch_id 0 12 miss% 0.032121121712855305
plot_id,batch_id 0 13 miss% 0.019002073773560342
plot_id,batch_id 0 14 miss% 0.029321482146124133
plot_id,batch_id 0 15 miss% 0.04332733623485721
plot_id,batch_id 0 16 miss% 0.029941056532080065
plot_id,batch_id 0 17 miss% 0.0309446070416751
plot_id,batch_id 0 18 miss% 0.042762954161871404
plot_id,batch_id 0 19 miss% 0.043417941952253104
plot_id,batch_id 0 20 miss% 0.049511885057870325
plot_id,batch_id 0 21 miss% 0.026327480860446124
plot_id,batch_id 0 22 miss% 0.04018157349303858
plot_id,batch_id 0 23 miss% 0.026670301368374196
plot_id,batch_id 0 24 miss% 0.03397223908013321
plot_id,batch_id 0 25 miss% 0.034568868339964365
plot_id,batch_id 0 26 miss% 0.03303303090799435
plot_id,batch_id 0 27 miss% 0.022731204063134443
plot_id,batch_id 0 28 miss% 0.026470710474316886
plot_id,batch_id 0 29 miss% 0.03191788519215701
plot_id,batch_id 0 30 miss% 0.03413258013781342
plot_id,batch_id 0 31 miss% 0.027466164438118917
plot_id,batch_id 0 32 miss% 0.030244544731603193
plot_id,batch_id 0 33 miss% 0.0356315156656204
plot_id,batch_id 0 34 miss% 0.02506139611811762
plot_id,batch_id 0 35 miss% 0.07158765982847254
plot_id,batch_id 0 36 miss% 0.043270422474778414
plot_id,batch_id 0 37 miss% 0.035072538428828244
plot_id,batch_id 0 38 miss% 0.025182938388443828
plot_id,batch_id 0 39 miss% 0.022394435462358437
plot_id,batch_id 0 40 miss% 0.060073784628515356
plot_id,batch_id 0 41 miss% 0.019527939166187547
plot_id,batch_id 0 42 miss% 0.021732014587168533
plot_id,batch_id 0 43 miss% 0.04442057004007841
plot_id,batch_id 0 44 miss% 0.02489715257125251
plot_id,batch_id 0 45 miss% 0.026264526216615777
plot_id,batch_id 0 46 miss% 0.02032672801499547
plot_id,batch_id 0 47 miss% 0.0211108166633846
plot_id,batch_id 0 48 miss% 0.027332082958004274
plot_id,batch_id 0 49 miss% 0.016419781007332414
plot_id,batch_id 0 50 miss% 0.03761945637411243
plot_id,batch_id 0 51 miss% 0.028483421213826625
plot_id,batch_id 0 52 miss% 0.024064731222571977
plot_id,batch_id 0 53 miss% 0.021479778658891818
plot_id,batch_id 0 54 miss% 0.023763915273795275
plot_id,batch_id 0 55 miss% 0.04196137587975694
plot_id,batch_id 0 56 miss% 0.03260148896731698
plot_id,batch_id 0 57 miss% 0.028077876696459
plot_id,batch_id 0 58 miss% 0.027380113961469048
plot_id,batch_id 0 59 miss% 0.02522880054475367
plot_id,batch_id 0 60 miss% 0.036120427952834745
plot_id,batch_id 0 61 miss% 0.02846577336527134
plot_id,batch_id 0 62 miss% 0.02515438029011292
plot_id,batch_id 0 63 miss% 0.029168900962561944
plot_id,batch_id 0 64 miss% 0.03133723898959222
plot_id,batch_id 0 65 miss% 0.05359053989952588
plot_id,batch_id 0 66 miss% 0.041595260564042794
plot_id,batch_id 0 67 miss% 0.035923876634201456
 0.038576154001339354
plot_id,batch_id 0 70 miss% 0.038350040787343824
plot_id,batch_id 0 71 miss% 0.034782076620825844
plot_id,batch_id 0 72 miss% 0.026942954965359055
plot_id,batch_id 0 73 miss% 0.04050655006607234
plot_id,batch_id 0 74 miss% 0.04269989816693477
plot_id,batch_id 0 75 miss% 0.03996249926755644
plot_id,batch_id 0 76 miss% 0.037832402488603534
plot_id,batch_id 0 77 miss% 0.043077838346745605
plot_id,batch_id 0 78 miss% 0.034776641772158896
plot_id,batch_id 0 79 miss% 0.059041834085210694
plot_id,batch_id 0 80 miss% 0.039053467375685505
plot_id,batch_id 0 81 miss% 0.027792456036573484
plot_id,batch_id 0 82 miss% 0.03231797409825726
plot_id,batch_id 0 83 miss% 0.02892729590511694
plot_id,batch_id 0 84 miss% 0.029312172056634417
plot_id,batch_id 0 85 miss% 0.04487581575746876
plot_id,batch_id 0 86 miss% 0.02872464970327629
plot_id,batch_id 0 87 miss% 0.03959599599704456
plot_id,batch_id 0 88 miss% 0.02924273248115148
plot_id,batch_id 0 89 miss% 0.04136050350342721
plot_id,batch_id 0 90 miss% 0.036638852014209605
plot_id,batch_id 0 91 miss% 0.03967413198206769
plot_id,batch_id 0 92 miss% 0.03053342592843859
plot_id,batch_id 0 93 miss% 0.02935165923995785
plot_id,batch_id 0 94 miss% 0.029905825931820052
plot_id,batch_id 0 95 miss% 0.045896240401332195
plot_id,batch_id 0 96 miss% 0.04024142922475761
plot_id,batch_id 0 97 miss% 0.033284564480287146
plot_id,batch_id 0 98 miss% 0.027673058030327796
plot_id,batch_id 0 99 miss% 0.03502488376019941
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02654748 0.02110373 0.02307855 0.02838974 0.02834589 0.04515025
 0.03248757 0.02444408 0.02643446 0.02854534 0.03343978 0.04675266
 0.03102218 0.0311841  0.03401244 0.03312621 0.03192579 0.02645664
 0.04255432 0.03213738 0.03737455 0.02272367 0.03285797 0.0383835
 0.03579841 0.0300596  0.02985627 0.027832   0.03199508 0.03898
 0.0385575  0.02894835 0.03147829 0.03079567 0.03422574 0.03239074
 0.02970072 0.02477974 0.02474874 0.02313995 0.04732013 0.02437671
 0.02020795 0.04323934 0.0302695  0.02042922 0.02652184 0.03081483
 0.03879692 0.03043223 0.04484235 0.02329131 0.02458594 0.02000746
 0.03728719 0.04488405 0.02829243 0.0212208  0.02292848 0.03012081
 0.03419135 0.03077159 0.04086387 0.02276317 0.02537473 0.03353472
 0.048921   0.03575798 0.02148851 0.03857615 0.03835004 0.03478208
 0.02694295 0.04050655 0.0426999  0.0399625  0.0378324  0.04307784
 0.03477664 0.05904183 0.03905347 0.02779246 0.03231797 0.0289273
 0.02931217 0.04487582 0.02872465 0.039596   0.02924273 0.0413605
 0.03663885 0.03967413 0.03053343 0.02935166 0.02990583 0.04589624
 0.04024143 0.03328456 0.02767306 0.03502488]
for model  131 the mean error 0.03281277543144982
all id 131 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 25 out win 5 err 0.03281277543144982
Launcher: Job 132 completed in 9717 seconds.
Launcher: Task 170 done. Exiting.
plot_id,batch_id 0 68 miss% 0.03960117141437856
plot_id,batch_id 0 69 miss% 0.015922234381937857
plot_id,batch_id 0 70 miss% 0.04158143967966835
plot_id,batch_id 0 71 miss% 0.06317921015713292
plot_id,batch_id 0 72 miss% 0.03689048842556786
plot_id,batch_id 0 73 miss% 0.034675225822429295
plot_id,batch_id 0 74 miss% 0.04962543374794206
plot_id,batch_id 0 75 miss% 0.04371810564438539
plot_id,batch_id 0 76 miss% 0.045509716750373845
plot_id,batch_id 0 77 miss% 0.04195377397077344
plot_id,batch_id 0 78 miss% 0.04642229713579417
plot_id,batch_id 0 79 miss% 0.05380124618935406
plot_id,batch_id 0 80 miss% 0.04485673249664519
plot_id,batch_id 0 81 miss% 0.02489541701102878
plot_id,batch_id 0 82 miss% 0.0318425890879556
plot_id,batch_id 0 83 miss% 0.01851794800446491
plot_id,batch_id 0 84 miss% 0.021365125897455067
plot_id,batch_id 0 85 miss% 0.05941056903851284
plot_id,batch_id 0 86 miss% 0.019541713797645372
plot_id,batch_id 0 87 miss% 0.024613605750467078
plot_id,batch_id 0 88 miss% 0.039560073722517256
plot_id,batch_id 0 89 miss% 0.025596692711492397
plot_id,batch_id 0 90 miss% 0.039945597059013904
plot_id,batch_id 0 91 miss% 0.02467569284781481
plot_id,batch_id 0 92 miss% 0.02400297706971617
plot_id,batch_id 0 93 miss% 0.030683372820668874
plot_id,batch_id 0 94 miss% 0.04229361559087352
plot_id,batch_id 0 95 miss% 0.03622751717037079
plot_id,batch_id 0 96 miss% 0.04249282591062021
plot_id,batch_id 0 97 miss% 0.05957289642917619
plot_id,batch_id 0 98 miss% 0.04268314411209564
plot_id,batch_id 0 99 miss% 0.03876994994747312
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03647982 0.02200809 0.0207218  0.02675456 0.02994977 0.03491341
 0.02518888 0.02445836 0.02220054 0.02059629 0.03738987 0.04509535
 0.03212112 0.01900207 0.02932148 0.04332734 0.02994106 0.03094461
 0.04276295 0.04341794 0.04951189 0.02632748 0.04018157 0.0266703
 0.03397224 0.03456887 0.03303303 0.0227312  0.02647071 0.03191789
 0.03413258 0.02746616 0.03024454 0.03563152 0.0250614  0.07158766
 0.04327042 0.03507254 0.02518294 0.02239444 0.06007378 0.01952794
 0.02173201 0.04442057 0.02489715 0.02626453 0.02032673 0.02111082
 0.02733208 0.01641978 0.03761946 0.02848342 0.02406473 0.02147978
 0.02376392 0.04196138 0.03260149 0.02807788 0.02738011 0.0252288
 0.03612043 0.02846577 0.02515438 0.0291689  0.03133724 0.05359054
 0.04159526 0.03592388 0.03960117 0.01592223 0.04158144 0.06317921
 0.03689049 0.03467523 0.04962543 0.04371811 0.04550972 0.04195377
 0.0464223  0.05380125 0.04485673 0.02489542 0.03184259 0.01851795
 0.02136513 0.05941057 0.01954171 0.02461361 0.03956007 0.02559669
 0.0399456  0.02467569 0.02400298 0.03068337 0.04229362 0.03622752
 0.04249283 0.0595729  0.04268314 0.03876995]
for model  209 the mean error 0.033505758359226374
all id 209 hidden_dim 16 learning_rate 0.01 num_layers 5 frames 31 out win 5 err 0.033505758359226374
Launcher: Job 210 completed in 9715 seconds.
Launcher: Task 222 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  107025
Epoch:0, Train loss:0.430855, valid loss:0.425762
Epoch:1, Train loss:0.028403, valid loss:0.005151
Epoch:2, Train loss:0.007392, valid loss:0.003884
Epoch:3, Train loss:0.005318, valid loss:0.003309
Epoch:4, Train loss:0.004598, valid loss:0.002511
Epoch:5, Train loss:0.004019, valid loss:0.002159
Epoch:6, Train loss:0.003503, valid loss:0.002057
Epoch:7, Train loss:0.003433, valid loss:0.002020
Epoch:8, Train loss:0.003184, valid loss:0.001797
Epoch:9, Train loss:0.003106, valid loss:0.001376
Epoch:10, Train loss:0.002996, valid loss:0.001790
Epoch:11, Train loss:0.002027, valid loss:0.001331
Epoch:12, Train loss:0.002010, valid loss:0.001440
Epoch:13, Train loss:0.001995, valid loss:0.001168
Epoch:14, Train loss:0.001927, valid loss:0.001019
Epoch:15, Train loss:0.001926, valid loss:0.001045
Epoch:16, Train loss:0.001965, valid loss:0.001251
Epoch:17, Train loss:0.001868, valid loss:0.001027
Epoch:18, Train loss:0.001780, valid loss:0.001457
Epoch:19, Train loss:0.001809, valid loss:0.001450
Epoch:20, Train loss:0.001735, valid loss:0.001031
Epoch:21, Train loss:0.001234, valid loss:0.000866
Epoch:22, Train loss:0.001248, valid loss:0.000827
Epoch:23, Train loss:0.001215, valid loss:0.000804
Epoch:24, Train loss:0.001230, valid loss:0.000867
Epoch:25, Train loss:0.001237, valid loss:0.000849
Epoch:26, Train loss:0.001215, valid loss:0.000776
Epoch:27, Train loss:0.001177, valid loss:0.000821
Epoch:28, Train loss:0.001173, valid loss:0.000843
Epoch:29, Train loss:0.001166, valid loss:0.000776
Epoch:30, Train loss:0.001162, valid loss:0.000772
Epoch:31, Train loss:0.000936, valid loss:0.000711
Epoch:32, Train loss:0.000909, valid loss:0.000737
Epoch:33, Train loss:0.000901, valid loss:0.000724
Epoch:34, Train loss:0.000912, valid loss:0.000704
Epoch:35, Train loss:0.000927, valid loss:0.000740
Epoch:36, Train loss:0.000909, valid loss:0.000652
Epoch:37, Train loss:0.000897, valid loss:0.000673
Epoch:38, Train loss:0.000891, valid loss:0.000778
Epoch:39, Train loss:0.000873, valid loss:0.000687
Epoch:40, Train loss:0.000864, valid loss:0.000697
Epoch:41, Train loss:0.000766, valid loss:0.000784
Epoch:42, Train loss:0.000756, valid loss:0.000648
Epoch:43, Train loss:0.000760, valid loss:0.000657
Epoch:44, Train loss:0.000756, valid loss:0.000619
Epoch:45, Train loss:0.000752, valid loss:0.000622
Epoch:46, Train loss:0.000746, valid loss:0.000622
Epoch:47, Train loss:0.000744, valid loss:0.000640
Epoch:48, Train loss:0.000744, valid loss:0.000633
Epoch:49, Train loss:0.000758, valid loss:0.000643
Epoch:50, Train loss:0.000735, valid loss:0.000619
Epoch:51, Train loss:0.000688, valid loss:0.000612
Epoch:52, Train loss:0.000684, valid loss:0.000618
Epoch:53, Train loss:0.000689, valid loss:0.000602
Epoch:54, Train loss:0.000684, valid loss:0.000609
Epoch:55, Train loss:0.000678, valid loss:0.000616
Epoch:56, Train loss:0.000674, valid loss:0.000585
Epoch:57, Train loss:0.000683, valid loss:0.000612
Epoch:58, Train loss:0.000672, valid loss:0.000636
Epoch:59, Train loss:0.000671, valid loss:0.000626
Epoch:60, Train loss:0.000668, valid loss:0.000619
training time 9560.33166217804
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.030463448898011194
plot_id,batch_id 0 1 miss% 0.01855611093939491
plot_id,batch_id 0 2 miss% 0.02299989910105571
plot_id,batch_id 0 3 miss% 0.022651298086970396
plot_id,batch_id 0 4 miss% 0.02322795725748911
plot_id,batch_id 0 5 miss% 0.03647998008652825
plot_id,batch_id 0 6 miss% 0.028159705140852708
plot_id,batch_id 0 7 miss% 0.027584654327410228
plot_id,batch_id 0 8 miss% 0.01971800802407649
plot_id,batch_id 0 9 miss% 0.01691880501382088
plot_id,batch_id 0 10 miss% 0.03629635080415847
plot_id,batch_id 0 11 miss% 0.033464692326373155
plot_id,batch_id 0 12 miss% 0.023319710997928393
plot_id,batch_id 0 13 miss% 0.020449037566305124
plot_id,batch_id 0 14 miss% 0.026132589729147657
plot_id,batch_id 0 15 miss% 0.04608147785683706
plot_id,batch_id 0 16 miss% 0.0267915928244123
plot_id,batch_id 0 17 miss% 0.03658951937658615
plot_id,batch_id 0 18 miss% 0.024964116992723794
plot_id,batch_id 0 19 miss% 0.025239191853911984
plot_id,batch_id 0 20 miss% 0.0551737413866855
plot_id,batch_id 0 21 miss% 0.01722355768695618
plot_id,batch_id 0 22 miss% 0.021284734415793673
plot_id,batch_id 0 23 miss% 0.024993625513009342
plot_id,batch_id 0 24 miss% 0.02507242162359644
plot_id,batch_id 0 25 miss% 0.03381792573908507
plot_id,batch_id 0 26 miss% 0.027587705789822736
plot_id,batch_id 0 27 miss% 0.021512246429084458
plot_id,batch_id 0 28 miss% 0.028276836263268497
plot_id,batch_id 0 29 miss% 0.022734257463641317
plot_id,batch_id 0 30 miss% 0.03411220224339838
plot_id,batch_id 0 31 miss% 0.030062765702739042
plot_id,batch_id 0 32 miss% 0.023156718143420144
plot_id,batch_id 0 33 miss% 0.02456445258153399
plot_id,batch_id 0 34 miss% 0.022886194052904586
plot_id,batch_id 0 35 miss% 0.03337670420657026
plot_id,batch_id 0 36 miss% 0.036747967811884025
plot_id,batch_id 0 37 miss% 0.026018329642439848
plot_id,batch_id 0 38 miss% 0.027984695450581017
plot_id,batch_id 0 39 miss% 0.02457777031206935
plot_id,batch_id 0 40 miss% 0.03196944121340812
plot_id,batch_id 0 41 miss% 0.0239226634305003
plot_id,batch_id 0 42 miss% 0.01313111423613771
plot_id,batch_id 0 43 miss% 0.03192237436044054
plot_id,batch_id 0 44 miss% 0.021363348508110256
plot_id,batch_id 0 45 miss% 0.027037777258189788
plot_id,batch_id 0 46 miss% 0.022453651846773184
plot_id,batch_id 0 47 miss% 0.023137927599930424
plot_id,batch_id 0 48 miss% 0.02259626212583206
plot_id,batch_id 0 49 miss% 0.01707233145727543
plot_id,batch_id 0 50 miss% 0.03329015590115051
plot_id,batch_id 0 51 miss% 0.023816778598066572
plot_id,batch_id 0 52 miss% 0.022451771047564205
plot_id,batch_id 0 53 miss% 0.012515651461749521
plot_id,batch_id 0 54 miss% 0.03154464549557744
plot_id,batch_id 0 55 miss% 0.04772263307504113
plot_id,batch_id 0 56 miss% 0.02846003585579434
plot_id,batch_id 0 57 miss% 0.02795000237343492
plot_id,batch_id 0 58 miss% 0.02626851713683306
plot_id,batch_id 0 59 miss% 0.02160560004162397
plot_id,batch_id 0 60 miss% 0.028294294775447388
plot_id,batch_id 0 61 miss% 0.02499451965068774
plot_id,batch_id 0 62 miss% 0.023285658632637974
plot_id,batch_id 0 63 miss% 0.034687835862248786
plot_id,batch_id 0 64 miss% 0.033102185313261236
plot_id,batch_id 0 65 miss% 0.02802867173487748
plot_id,batch_id 0 66 miss% 0.03513308211977729
plot_id,batch_id 0 67 miss% 0.04344337787837748
plot_id,batch_id 0 68 miss% 0.024399600535486104
plot_id,batch_id 0 69 miss% 0.01981162346318098
plot_id,batch_id 0 70 miss% 0.03672157814481869
plot_id,batch_id 0 71 miss% 0.05638304674617794
plot_id,batch_id 0 72 miss% 0.03373308506292629
plot_id,batch_id 0 73 miss% 0.03359930944101065
plot_id,batch_id 0 74 miss% 0.028537394016158277
plot_id,batch_id 0 75 miss% 0.038624264650750334
plot_id,batch_id 0 76 miss% 0.04049010042917019
plot_id,batch_id 0 77 miss% 0.026702452769586934
plot_id,batch_id 0 78 miss% 0.030486964333460296
plot_id,batch_id 0 79 miss% 0.04330797835122011
plot_id,batch_id 0 80 miss% 0.06089820237722844
plot_id,batch_id 0 81 miss% 0.032920646046315866
plot_id,batch_id 0 82 miss% 0.03390413312096526
plot_id,batch_id 0 83 miss% 0.023885887620481654
plot_id,batch_id 0 84 miss% 0.028653211211915067
plot_id,batch_id 0 85 miss% 0.05143751368055665
plot_id,batch_id 0 86 miss% 0.021236675851856155
plot_id,batch_id 0 87 miss% 0.02827489839272527
plot_id,batch_id 0 88 miss% 0.0435998055185047
plot_id,batch_id 0 89 miss% 0.03322620062648669
plot_id,batch_id 0 90 miss% 0.035279707804078526
plot_id,batch_id 0 91 miss% 0.037584135983516134
plot_id,batch_id 0 92 miss% 0.032158173083470694
plot_id,batch_id 0 93 miss% 0.0198152214000267
plot_id,batch_id 0 94 miss% 0.03629225417028227
plot_id,batch_id 0 95 miss% 0.062498866757157226
plot_id,batch_id 0 96 miss% 0.027548462618221425
plot_id,batch_id 0 97 miss% 0.03538459796995323
plot_id,batch_id 0 98 miss% 0.03642971183667354
plot_id,batch_id 0 99 miss% 0.026832226946257795
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03046345 0.01855611 0.0229999  0.0226513  0.02322796 0.03647998
 0.02815971 0.02758465 0.01971801 0.01691881 0.03629635 0.03346469
 0.02331971 0.02044904 0.02613259 0.04608148 0.02679159 0.03658952
 0.02496412 0.02523919 0.05517374 0.01722356 0.02128473 0.02499363
 0.02507242 0.03381793 0.02758771 0.02151225 0.02827684 0.02273426
 0.0341122  0.03006277 0.02315672 0.02456445 0.02288619 0.0333767
 0.03674797 0.02601833 0.0279847  0.02457777 0.03196944 0.02392266
 0.01313111 0.03192237 0.02136335 0.02703778 0.02245365 0.02313793
 0.02259626 0.01707233 0.03329016 0.02381678 0.02245177 0.01251565
 0.03154465 0.04772263 0.02846004 0.02795    0.02626852 0.0216056
 0.02829429 0.02499452 0.02328566 0.03468784 0.03310219 0.02802867
 0.03513308 0.04344338 0.0243996  0.01981162 0.03672158 0.05638305
 0.03373309 0.03359931 0.02853739 0.03862426 0.0404901  0.02670245
 0.03048696 0.04330798 0.0608982  0.03292065 0.03390413 0.02388589
 0.02865321 0.05143751 0.02123668 0.0282749  0.04359981 0.0332262
 0.03527971 0.03758414 0.03215817 0.01981522 0.03629225 0.06249887
 0.02754846 0.0353846  0.03642971 0.02683223]
for model  124 the mean error 0.02989113239613847
all id 124 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 25 out win 4 err 0.02989113239613847
Launcher: Job 125 completed in 9760 seconds.
Launcher: Task 151 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  77489
Epoch:0, Train loss:0.473889, valid loss:0.470487
Epoch:1, Train loss:0.046815, valid loss:0.012868
Epoch:2, Train loss:0.015750, valid loss:0.005757
Epoch:3, Train loss:0.009209, valid loss:0.004341
Epoch:4, Train loss:0.007234, valid loss:0.003406
Epoch:5, Train loss:0.006229, valid loss:0.003731
Epoch:6, Train loss:0.005411, valid loss:0.002966
Epoch:7, Train loss:0.004730, valid loss:0.002579
Epoch:8, Train loss:0.004389, valid loss:0.002624
Epoch:9, Train loss:0.003952, valid loss:0.001849
Epoch:10, Train loss:0.003894, valid loss:0.001931
Epoch:11, Train loss:0.002822, valid loss:0.001604
Epoch:12, Train loss:0.002819, valid loss:0.001564
Epoch:13, Train loss:0.002681, valid loss:0.001566
Epoch:14, Train loss:0.002655, valid loss:0.001576
Epoch:15, Train loss:0.002616, valid loss:0.001617
Epoch:16, Train loss:0.002555, valid loss:0.001503
Epoch:17, Train loss:0.002518, valid loss:0.001290
Epoch:18, Train loss:0.002314, valid loss:0.001426
Epoch:19, Train loss:0.002544, valid loss:0.001571
Epoch:20, Train loss:0.002248, valid loss:0.001526
Epoch:21, Train loss:0.001866, valid loss:0.001267
Epoch:22, Train loss:0.001802, valid loss:0.001108
Epoch:23, Train loss:0.001799, valid loss:0.001134
Epoch:24, Train loss:0.001777, valid loss:0.001142
Epoch:25, Train loss:0.001722, valid loss:0.001099
Epoch:26, Train loss:0.001738, valid loss:0.001132
Epoch:27, Train loss:0.001728, valid loss:0.001165
Epoch:28, Train loss:0.001699, valid loss:0.001154
Epoch:29, Train loss:0.001664, valid loss:0.001138
Epoch:30, Train loss:0.001660, valid loss:0.001166
Epoch:31, Train loss:0.001415, valid loss:0.001050
Epoch:32, Train loss:0.001421, valid loss:0.001113
Epoch:33, Train loss:0.001410, valid loss:0.001076
Epoch:34, Train loss:0.001418, valid loss:0.001043
Epoch:35, Train loss:0.001388, valid loss:0.001055
Epoch:36, Train loss:0.001376, valid loss:0.001016
Epoch:37, Train loss:0.001382, valid loss:0.001033
Epoch:38, Train loss:0.001354, valid loss:0.001006
Epoch:39, Train loss:0.001349, valid loss:0.001012
Epoch:40, Train loss:0.001347, valid loss:0.001057
Epoch:41, Train loss:0.001207, valid loss:0.000946
Epoch:42, Train loss:0.001206, valid loss:0.000950
Epoch:43, Train loss:0.001211, valid loss:0.000955
Epoch:44, Train loss:0.001213, valid loss:0.000959
Epoch:45, Train loss:0.001196, valid loss:0.001002
Epoch:46, Train loss:0.001180, valid loss:0.000926
Epoch:47, Train loss:0.001181, valid loss:0.000948
Epoch:48, Train loss:0.001183, valid loss:0.000922
Epoch:49, Train loss:0.001171, valid loss:0.000935
Epoch:50, Train loss:0.001181, valid loss:0.000971
Epoch:51, Train loss:0.001111, valid loss:0.000911
Epoch:52, Train loss:0.001105, valid loss:0.000930
Epoch:53, Train loss:0.001102, valid loss:0.000918
Epoch:54, Train loss:0.001100, valid loss:0.000911
Epoch:55, Train loss:0.001104, valid loss:0.000911
Epoch:56, Train loss:0.001097, valid loss:0.000920
Epoch:57, Train loss:0.001096, valid loss:0.000949
Epoch:58, Train loss:0.001093, valid loss:0.000932
Epoch:59, Train loss:0.001092, valid loss:0.000933
Epoch:60, Train loss:0.001085, valid loss:0.000912
training time 9597.522639989853
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.029577961397059403
plot_id,batch_id 0 1 miss% 0.024458396169812532
plot_id,batch_id 0 2 miss% 0.024949774540391368
plot_id,batch_id 0 3 miss% 0.024909400856701815
plot_id,batch_id 0 4 miss% 0.02638217081310732
plot_id,batch_id 0 5 miss% 0.025737548026474724
plot_id,batch_id 0 6 miss% 0.038482545667440755
plot_id,batch_id 0 7 miss% 0.03484878569180149
plot_id,batch_id 0 8 miss% 0.03500862622393574
plot_id,batch_id 0 9 miss% 0.024518211316005685
plot_id,batch_id 0 10 miss% 0.0460998952352262
plot_id,batch_id 0 11 miss% 0.04810027678986743
plot_id,batch_id 0 12 miss% 0.030074602005201446
plot_id,batch_id 0 13 miss% 0.029488888395868808
plot_id,batch_id 0 14 miss% 0.030270068243861768
plot_id,batch_id 0 15 miss% 0.042943793564556805
plot_id,batch_id 0 16 miss% 0.025781222908003117
plot_id,batch_id 0 17 miss% 0.03772082487806319
plot_id,batch_id 0 18 miss% 0.04122985297280559
plot_id,batch_id 0 19 miss% 0.036766351232926216
plot_id,batch_id 0 20 miss% 0.03601419711429095
plot_id,batch_id 0 21 miss% 0.040157846095648604
plot_id,batch_id 0 22 miss% 0.03086997164393175
plot_id,batch_id 0 23 miss% 0.025677016580948755
plot_id,batch_id 0 24 miss% 0.03471912144773783
plot_id,batch_id 0 25 miss% 0.03510425058175156
plot_id,batch_id 0 26 miss% 0.04479175003589052
plot_id,batch_id 0 27 miss% 0.030899415052505746
plot_id,batch_id 0 28 miss% 0.01747393839652344
plot_id,batch_id 0 29 miss% 0.023934255318909012
plot_id,batch_id 0 30 miss% 0.046187592271683967
plot_id,batch_id 0 31 miss% 0.026842873910446336
plot_id,batch_id 0 32 miss% 0.03463924049097356
plot_id,batch_id 0 33 miss% 0.02898665895802754
plot_id,batch_id 0 34 miss% 0.03032505676679603
plot_id,batch_id 0 35 miss% 0.03835077914760054
plot_id,batch_id 0 36 miss% 0.044992838380091854
plot_id,batch_id 0 37 miss% 0.023063863582839947
plot_id,batch_id 0 38 miss% 0.032057801690196785
plot_id,batch_id 0 39 miss% 0.022745425666342845
plot_id,batch_id 0 40 miss% 0.06151163085109901
plot_id,batch_id 0 41 miss% 0.02988767419125175
plot_id,batch_id 0 42 miss% 0.02439887252808402
plot_id,batch_id 0 43 miss% 0.03431262281397573
plot_id,batch_id 0 44 miss% 0.023486822181317785
plot_id,batch_id 0 45 miss% 0.04408786115657955
plot_id,batch_id 0 46 miss% 0.027904739045967727
plot_id,batch_id 0 47 miss% 0.02870401666082391
plot_id,batch_id 0 48 miss% 0.02700219966227231
plot_id,batch_id 0 49 miss% 0.02298196374396409
plot_id,batch_id 0 50 miss% 0.0363588876131422
plot_id,batch_id 0 51 miss% 0.02196747470669248
plot_id,batch_id 0 52 miss% 0.02818061337790522
plot_id,batch_id 0 53 miss% 0.019492605066476966
plot_id,batch_id 0 54 miss% 0.029795814338166762
plot_id,batch_id 0 55 miss% 0.03296245399934073
plot_id,batch_id 0 56 miss% 0.022169597253384268
plot_id,batch_id 0 57 miss% 0.02928678259460883
plot_id,batch_id 0 58 miss% 0.03062531374891797
plot_id,batch_id 0 59 miss% 0.030528688401169093
plot_id,batch_id 0 60 miss% 0.030111675419884874
plot_id,batch_id 0 61 miss% 0.03599678764394476
plot_id,batch_id 0 62 miss% 0.04244295627656596
plot_id,batch_id 0 63 miss% 0.03304185508063028
plot_id,batch_id 0 64 miss% 0.028941295806652608
plot_id,batch_id 0 65 miss% 0.049953397587517864
plot_id,batch_id 0 66 miss% 0.05337555328377437
plot_id,batch_id 0 67 miss% 0.05233696064213956
plot_id,batch_id 0 68 miss% 0.022113794592394157
plot_id,batch_id 0 69 miss% 0.030994060300937026
plot_id,batch_id 0 70 miss% 0.05100339421636644
plot_id,batch_id 0 71 miss% 0.03738838013262904
plot_id,batch_id 0 72 miss% 0.04104324680448092
plot_id,batch_id 0 73 miss% 0.02564163463672215
plot_id,batch_id 0 74 miss% 0.03836375245153625
plot_id,batch_id 0 75 miss% 0.0868342189429406
plot_id,batch_id 0 76 miss% 0.04325482549334136
plot_id,batch_id 0 77 miss% 0.02944893014664068
plot_id,batch_id 0 78 miss% 0.03393347514052226
plot_id,batch_id 0 79 miss% 0.055326066449310214
plot_id,batch_id 0 80 miss% 0.06502749295231823
plot_id,batch_id 0 81 miss% 0.029330674211099896
plot_id,batch_id 0 82 miss% 0.04058054636040615
plot_id,batch_id 0 83 miss% 0.03441575395417569
plot_id,batch_id 0 84 miss% 0.026114926452322003
plot_id,batch_id 0 85 miss% 0.0328130302943581
plot_id,batch_id 0 86 miss% 0.0328264929011348
plot_id,batch_id 0 87 miss% 0.031808446921310844
plot_id,batch_id 0 88 miss% 0.04135562987443549
plot_id,batch_id 0 89 miss% 0.024689045573052786
plot_id,batch_id 0 90 miss% 0.042166455679829906
plot_id,batch_id 0 91 miss% 0.04494739398043253
plot_id,batch_id 0 92 miss% 0.0374084553304836
plot_id,batch_id 0 93 miss% 0.0326447629519648
plot_id,batch_id 0 94 miss% 0.054374374429623756
plot_id,batch_id 0 95 miss% 0.04724309239162216
plot_id,batch_id 0 96 miss% 0.039461666944268316
plot_id,batch_id 0 97 miss% 0.050836304269700365
plot_id,batch_id 0 98 miss% 0.03525747422116796
plot_id,batch_id 0 99 miss% 0.051398359808654155
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02957796 0.0244584  0.02494977 0.0249094  0.02638217 0.02573755
 0.03848255 0.03484879 0.03500863 0.02451821 0.0460999  0.04810028
 0.0300746  0.02948889 0.03027007 0.04294379 0.02578122 0.03772082
 0.04122985 0.03676635 0.0360142  0.04015785 0.03086997 0.02567702
 0.03471912 0.03510425 0.04479175 0.03089942 0.01747394 0.02393426
 0.04618759 0.02684287 0.03463924 0.02898666 0.03032506 0.03835078
 0.04499284 0.02306386 0.0320578  0.02274543 0.06151163 0.02988767
 0.02439887 0.03431262 0.02348682 0.04408786 0.02790474 0.02870402
 0.0270022  0.02298196 0.03635889 0.02196747 0.02818061 0.01949261
 0.02979581 0.03296245 0.0221696  0.02928678 0.03062531 0.03052869
 0.03011168 0.03599679 0.04244296 0.03304186 0.0289413  0.0499534
 0.05337555 0.05233696 0.02211379 0.03099406 0.05100339 0.03738838
 0.04104325 0.02564163 0.03836375 0.08683422 0.04325483 0.02944893
 0.03393348 0.05532607 0.06502749 0.02933067 0.04058055 0.03441575
 0.02611493 0.03281303 0.03282649 0.03180845 0.04135563 0.02468905
 0.04216646 0.04494739 0.03740846 0.03264476 0.05437437 0.04724309
 0.03946167 0.0508363  0.03525747 0.05139836]
for model  104 the mean error 0.03527076364548683
all id 104 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 25 out win 5 err 0.03527076364548683
Launcher: Job 105 completed in 9805 seconds.
Launcher: Task 252 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  77489
Epoch:0, Train loss:0.473889, valid loss:0.470487
Epoch:1, Train loss:0.045195, valid loss:0.008773
Epoch:2, Train loss:0.013407, valid loss:0.005254
Epoch:3, Train loss:0.008916, valid loss:0.003614
Epoch:4, Train loss:0.006889, valid loss:0.003527
Epoch:5, Train loss:0.006174, valid loss:0.003228
Epoch:6, Train loss:0.005497, valid loss:0.002633
Epoch:7, Train loss:0.005202, valid loss:0.003006
Epoch:8, Train loss:0.005024, valid loss:0.004645
Epoch:9, Train loss:0.004787, valid loss:0.002431
Epoch:10, Train loss:0.004761, valid loss:0.002771
Epoch:11, Train loss:0.003270, valid loss:0.001863
Epoch:12, Train loss:0.003296, valid loss:0.002125
Epoch:13, Train loss:0.003166, valid loss:0.002148
Epoch:14, Train loss:0.003050, valid loss:0.001630
Epoch:15, Train loss:0.003040, valid loss:0.001861
Epoch:16, Train loss:0.003008, valid loss:0.002870
Epoch:17, Train loss:0.002958, valid loss:0.001739
Epoch:18, Train loss:0.002924, valid loss:0.001827
Epoch:19, Train loss:0.002751, valid loss:0.001531
Epoch:20, Train loss:0.002756, valid loss:0.001978
Epoch:21, Train loss:0.001997, valid loss:0.001371
Epoch:22, Train loss:0.001996, valid loss:0.001170
Epoch:23, Train loss:0.002006, valid loss:0.001176
Epoch:24, Train loss:0.001949, valid loss:0.001308
Epoch:25, Train loss:0.001934, valid loss:0.001189
Epoch:26, Train loss:0.001908, valid loss:0.001385
Epoch:27, Train loss:0.001906, valid loss:0.001447
Epoch:28, Train loss:0.001852, valid loss:0.001226
Epoch:29, Train loss:0.001855, valid loss:0.001200
Epoch:30, Train loss:0.001859, valid loss:0.001316
Epoch:31, Train loss:0.001472, valid loss:0.001166
Epoch:32, Train loss:0.001448, valid loss:0.001121
Epoch:33, Train loss:0.001441, valid loss:0.000964
Epoch:34, Train loss:0.001430, valid loss:0.001370
Epoch:35, Train loss:0.001466, valid loss:0.000960
Epoch:36, Train loss:0.001438, valid loss:0.000980
Epoch:37, Train loss:0.001410, valid loss:0.000949
Epoch:38, Train loss:0.001440, valid loss:0.000971
Epoch:39, Train loss:0.001377, valid loss:0.000989
Epoch:40, Train loss:0.001361, valid loss:0.000967
Epoch:41, Train loss:0.001302, valid loss:0.000888
Epoch:42, Train loss:0.001183, valid loss:0.000945
Epoch:43, Train loss:0.001168, valid loss:0.000919
Epoch:44, Train loss:0.001163, valid loss:0.000964
Epoch:45, Train loss:0.001167, valid loss:0.000865
Epoch:46, Train loss:0.001149, valid loss:0.000894
Epoch:47, Train loss:0.001147, valid loss:0.000919
Epoch:48, Train loss:0.001154, valid loss:0.000900
Epoch:49, Train loss:0.001185, valid loss:0.000892
Epoch:50, Train loss:0.001112, valid loss:0.000908
Epoch:51, Train loss:0.001039, valid loss:0.000859
Epoch:52, Train loss:0.001028, valid loss:0.000863
Epoch:53, Train loss:0.001032, valid loss:0.000839
Epoch:54, Train loss:0.001025, valid loss:0.000854
Epoch:55, Train loss:0.001025, valid loss:0.000833
Epoch:56, Train loss:0.001035, valid loss:0.000883
Epoch:57, Train loss:0.001021, valid loss:0.000885
Epoch:58, Train loss:0.001012, valid loss:0.000855
Epoch:59, Train loss:0.001009, valid loss:0.000850
Epoch:60, Train loss:0.001010, valid loss:0.000837
training time 9732.735131025314
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.03861296599619783
plot_id,batch_id 0 1 miss% 0.03513752733311929
plot_id,batch_id 0 2 miss% 0.021913348248581176
plot_id,batch_id 0 3 miss% 0.025451029559145945
plot_id,batch_id 0 4 miss% 0.027204060781470047
plot_id,batch_id 0 5 miss% 0.04247216123186498
plot_id,batch_id 0 6 miss% 0.02513861520587048
plot_id,batch_id 0 7 miss% 0.024475725082787073
plot_id,batch_id 0 8 miss% 0.02453977962244902
plot_id,batch_id 0 9 miss% 0.024610828680024437
plot_id,batch_id 0 10 miss% 0.047689526372136706
plot_id,batch_id 0 11 miss% 0.04064558489647635
plot_id,batch_id 0 12 miss% 0.030804841640738666
plot_id,batch_id 0 13 miss% 0.020168274168903034
plot_id,batch_id 0 14 miss% 0.03813416217809151
plot_id,batch_id 0 15 miss% 0.05194232884539509
plot_id,batch_id 0 16 miss% 0.02256003251437779
plot_id,batch_id 0 17 miss% 0.0328250470254609
plot_id,batch_id 0 18 miss% 0.05113072092245193
plot_id,batch_id 0 19 miss% 0.029737836382946695
plot_id,batch_id 0 20 miss% 0.04908220325433983
plot_id,batch_id 0 21 miss% 0.025194994800512495
plot_id,batch_id 0 22 miss% 0.015734256023998416
plot_id,batch_id 0 23 miss% 0.030128904080046454
plot_id,batch_id 0 24 miss% 0.02604282583634217
plot_id,batch_id 0 25 miss% 0.03832647296347152
plot_id,batch_id 0 26 miss% 0.024892618759467657
plot_id,batch_id 0 27 miss% 0.025522332410480342
plot_id,batch_id 0 28 miss% 0.025168464215338353
plot_id,batch_id 0 29 miss% 0.01771803769233677
plot_id,batch_id 0 30 miss% 0.044427168263213704
plot_id,batch_id 0 31 miss% 0.02977995296415097
plot_id,batch_id 0 32 miss% 0.028041945343726914
plot_id,batch_id 0 33 miss% 0.018733939344411008
plot_id,batch_id 0 34 miss% 0.014208031553791078
plot_id,batch_id 0 35 miss% 0.04229837629253689
plot_id,batch_id 0 36 miss% 0.04341809549492292
plot_id,batch_id 0 37 miss% 0.027511747779014443
plot_id,batch_id 0 38 miss% 0.02911640932291366
plot_id,batch_id 0 39 miss% 0.024002900160028016
plot_id,batch_id 0 40 miss% 0.05366162547700711
plot_id,batch_id 0 41 miss% 0.022425631210100046
plot_id,batch_id 0 42 miss% 0.02554231864167325
plot_id,batch_id 0 43 miss% 0.03192007345114194
plot_id,batch_id 0 44 miss% 0.029253006170361508
plot_id,batch_id 0 45 miss% 0.036066921908546
plot_id,batch_id 0 46 miss% 0.02282773562877525
plot_id,batch_id 0 47 miss% 0.016478913731994393
plot_id,batch_id 0 48 miss% 0.02025313337769209
plot_id,batch_id 0 49 miss% 0.0213450318020477
plot_id,batch_id 0 50 miss% 0.03239063984398689
plot_id,batch_id 0 51 miss% 0.02439588770782931
plot_id,batch_id 0 52 miss% 0.01824847156480802
plot_id,batch_id 0 53 miss% 0.01593092371446424
plot_id,batch_id 0 54 miss% 0.018459391815151348
plot_id,batch_id 0 55 miss% 0.03730491011594927
plot_id,batch_id 0 56 miss% 0.012165025829476622
plot_id,batch_id 0 57 miss% 0.028962299562593365
plot_id,batch_id 0 58 miss% 0.019068099744886536
plot_id,batch_id 0 59 miss% 0.022543440249728146
plot_id,batch_id 0 60 miss% 0.03691548442823063
plot_id,batch_id 0 61 miss% 0.032284781511397256
plot_id,batch_id 0 62 miss% 0.03354771956886597
plot_id,batch_id 0 63 miss% 0.053799414905736705
plot_id,batch_id 0 64 miss% 0.052805028374486404
plot_id,batch_id 0 65 miss% 0.052578712851900154
plot_id,batch_id 0 66 miss% 0.04215029292376251
plot_id,batch_id 0 67 miss% 0.033834170360047607
plot_id,batch_id 0 68 miss% 0.04037841287292539
plot_id,batch_id 0 69 miss% 0.029465986519318303
plot_id,batch_id 0 70 miss% 0.04903958312583297
plot_id,batch_id 0 71 miss% 0.027098915959633377
plot_id,batch_id 0 72 miss% 0.030061343076316454
plot_id,batch_id 0 73 miss% 0.029580031923250646
plot_id,batch_id 0 74 miss% 0.02898851197783861
plot_id,batch_id 0 75 miss% 0.03707939444336512
plot_id,batch_id 0 76 miss% 0.03181317504761922
plot_id,batch_id 0 77 miss% 0.03507137758759638
plot_id,batch_id 0 78 miss% 0.03738276676044922
plot_id,batch_id 0 79 miss% 0.033522338794443884
plot_id,batch_id 0 80 miss% 0.06709442631019319
plot_id,batch_id 0 81 miss% 0.036224944713297534
plot_id,batch_id 0 82 miss% 0.05266804038033301
plot_id,batch_id 0 83 miss% 0.0299225981810541
plot_id,batch_id 0 84 miss% 0.047992473138604205
plot_id,batch_id 0 85 miss% 0.04865781855096523
plot_id,batch_id 0 86 miss% 0.03252943833520207
plot_id,batch_id 0 87 miss% 0.03829840494611076
plot_id,batch_id 0 88 miss% 0.036915132361765
plot_id,batch_id 0 89 miss% 0.041195321355604476
plot_id,batch_id 0 90 miss% 0.04757831455073266
plot_id,batch_id 0 91 miss% 0.02767842639247946
plot_id,batch_id 0 92 miss% 0.02897541043840125
plot_id,batch_id 0 93 miss% 0.034188723000487295
plot_id,batch_id 0 94 miss% 0.03901854349931669
plot_id,batch_id 0 95 miss% 0.06879598286483396
plot_id,batch_id 0 96 miss% 0.03103390397993485
plot_id,batch_id 0 97 miss% 0.04698076539382515
plot_id,batch_id 0 98 miss% 0.03162438965309459
plot_id,batch_id 0 99 miss% 0.03163183594422318
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03861297 0.03513753 0.02191335 0.02545103 0.02720406 0.04247216
 0.02513862 0.02447573 0.02453978 0.02461083 0.04768953 0.04064558
 0.03080484 0.02016827 0.03813416 0.05194233 0.02256003 0.03282505
 0.05113072 0.02973784 0.0490822  0.02519499 0.01573426 0.0301289
 0.02604283 0.03832647 0.02489262 0.02552233 0.02516846 0.01771804
 0.04442717 0.02977995 0.02804195 0.01873394 0.01420803 0.04229838
 0.0434181  0.02751175 0.02911641 0.0240029  0.05366163 0.02242563
 0.02554232 0.03192007 0.02925301 0.03606692 0.02282774 0.01647891
 0.02025313 0.02134503 0.03239064 0.02439589 0.01824847 0.01593092
 0.01845939 0.03730491 0.01216503 0.0289623  0.0190681  0.02254344
 0.03691548 0.03228478 0.03354772 0.05379941 0.05280503 0.05257871
 0.04215029 0.03383417 0.04037841 0.02946599 0.04903958 0.02709892
 0.03006134 0.02958003 0.02898851 0.03707939 0.03181318 0.03507138
 0.03738277 0.03352234 0.06709443 0.03622494 0.05266804 0.0299226
 0.04799247 0.04865782 0.03252944 0.0382984  0.03691513 0.04119532
 0.04757831 0.02767843 0.02897541 0.03418872 0.03901854 0.06879598
 0.0310339  0.04698077 0.03162439 0.03163184]
for model  158 the mean error 0.03310189891795221
all id 158 hidden_dim 24 learning_rate 0.02 num_layers 5 frames 25 out win 5 err 0.03310189891795221
Launcher: Job 159 completed in 9937 seconds.
Launcher: Task 163 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  77489
Epoch:0, Train loss:0.342961, valid loss:0.338302
Epoch:1, Train loss:0.024985, valid loss:0.004684
Epoch:2, Train loss:0.007050, valid loss:0.003006
Epoch:3, Train loss:0.005022, valid loss:0.002188
Epoch:4, Train loss:0.004092, valid loss:0.001622
Epoch:5, Train loss:0.003886, valid loss:0.002384
Epoch:6, Train loss:0.003512, valid loss:0.001832
Epoch:7, Train loss:0.003464, valid loss:0.001496
Epoch:8, Train loss:0.003276, valid loss:0.002343
Epoch:9, Train loss:0.003213, valid loss:0.002259
Epoch:10, Train loss:0.003091, valid loss:0.001714
Epoch:11, Train loss:0.002089, valid loss:0.001309
Epoch:12, Train loss:0.002083, valid loss:0.001383
Epoch:13, Train loss:0.001986, valid loss:0.001286
Epoch:14, Train loss:0.001994, valid loss:0.001614
Epoch:15, Train loss:0.001951, valid loss:0.001459
Epoch:16, Train loss:148.062231, valid loss:38.550912
Epoch:17, Train loss:157.389384, valid loss:211.773189
Epoch:18, Train loss:134.719981, valid loss:210.937714
Epoch:19, Train loss:209.616543, valid loss:203.236509
Epoch:20, Train loss:215.627938, valid loss:204.295040
Epoch:21, Train loss:195.119610, valid loss:183.056695
Epoch:22, Train loss:182.418445, valid loss:146.024290
Epoch:23, Train loss:160.415711, valid loss:146.728677
Epoch:24, Train loss:165.036578, valid loss:160.731128
Epoch:25, Train loss:154.645379, valid loss:158.213168
Epoch:26, Train loss:157.524823, valid loss:149.940759
Epoch:27, Train loss:138.707960, valid loss:133.147464
Epoch:28, Train loss:128.232489, valid loss:133.600375
Epoch:29, Train loss:114.323820, valid loss:119.065428
Epoch:30, Train loss:101.785752, valid loss:86.446578
Epoch:31, Train loss:95.984757, valid loss:96.063806
Epoch:32, Train loss:105.680362, valid loss:108.571070
Epoch:33, Train loss:109.705951, valid loss:108.022908
Epoch:34, Train loss:114.762353, valid loss:106.783776
Epoch:35, Train loss:113.944989, valid loss:112.647081
Epoch:36, Train loss:120.468731, valid loss:119.270279
Epoch:37, Train loss:122.251893, valid loss:114.565959
Epoch:38, Train loss:122.120862, valid loss:121.089985
Epoch:39, Train loss:124.556389, valid loss:118.522659
Epoch:40, Train loss:128.498589, valid loss:131.027158
Epoch:41, Train loss:139.544333, valid loss:135.001675
Epoch:42, Train loss:138.038971, valid loss:133.614936
Epoch:43, Train loss:137.354274, valid loss:129.660336
Epoch:44, Train loss:135.137291, valid loss:129.529493
Epoch:45, Train loss:129.047416, valid loss:117.574772
Epoch:46, Train loss:119.714991, valid loss:112.918980
Epoch:47, Train loss:120.184877, valid loss:114.567425
Epoch:48, Train loss:118.121926, valid loss:114.063003
Epoch:49, Train loss:113.283679, valid loss:116.474494
Epoch:50, Train loss:117.595265, valid loss:117.085119
Epoch:51, Train loss:121.473817, valid loss:119.343020
Epoch:52, Train loss:123.613430, valid loss:125.717324
Epoch:53, Train loss:128.709434, valid loss:128.718420
Epoch:54, Train loss:133.488380, valid loss:132.847075
Epoch:55, Train loss:138.491597, valid loss:138.292880
Epoch:56, Train loss:142.415040, valid loss:138.773508
Epoch:57, Train loss:144.937013, valid loss:142.920646
Epoch:58, Train loss:148.231377, valid loss:146.840034
Epoch:59, Train loss:140.825433, valid loss:137.851218
Epoch:60, Train loss:143.543850, valid loss:118.996581
training time 9789.658395767212
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 19.067779578258147
plot_id,batch_id 0 1 miss% 12.571105265458899
plot_id,batch_id 0 2 miss% 13.565326956141158
plot_id,batch_id 0 3 miss% 13.15006166394959
plot_id,batch_id 0 4 miss% 13.181666025666246
plot_id,batch_id 0 5 miss% 19.36224823767054
plot_id,batch_id 0 6 miss% 16.49405862872432
plot_id,batch_id 0 7 miss% 13.269657265161658
plot_id,batch_id 0 8 miss% 14.04241011673412
plot_id,batch_id 0 9 miss% 15.67425013463788
plot_id,batch_id 0 10 miss% 17.580328334513574
plot_id,batch_id 0 11 miss% 15.187363711003004
plot_id,batch_id 0 12 miss% 13.197576164326442
plot_id,batch_id 0 13 miss% 12.57527420151187
plot_id,batch_id 0 14 miss% 13.303038448976178
plot_id,batch_id 0 15 miss% 17.496270370158786
plot_id,batch_id 0 16 miss% 14.496078506767608
plot_id,batch_id 0 17 miss% 14.891008085388437
plot_id,batch_id 0 18 miss% 12.064414390646283
plot_id,batch_id 0 19 miss% 13.590928362830164
plot_id,batch_id 0 20 miss% 17.983624568801567
plot_id,batch_id 0 21 miss% 14.102260982323065
plot_id,batch_id 0 22 miss% 13.74609352892987
plot_id,batch_id 0 23 miss% 15.363009713354176
plot_id,batch_id 0 24 miss% 15.335042431305997
plot_id,batch_id 0 25 miss% 14.420008621851682
plot_id,batch_id 0 26 miss% 12.148572370782189
plot_id,batch_id 0 27 miss% 14.63550708399214
plot_id,batch_id 0 28 miss% 12.670076177100407
plot_id,batch_id 0 29 miss% 14.357615021334277
plot_id,batch_id 0 30 miss% 13.62553373407328
plot_id,batch_id 0 31 miss% 12.119559159339577
plot_id,batch_id 0 32 miss% 12.54917146549774
plot_id,batch_id 0 33 miss% 14.722795002121208
plot_id,batch_id 0 34 miss% 11.763621514395984
plot_id,batch_id 0 35 miss% 13.654487012800145
plot_id,batch_id 0 36 miss% 12.210676567036856
plot_id,batch_id 0 37 miss% 13.265024982741823
plot_id,batch_id 0 38 miss% 15.494562664752143
plot_id,batch_id 0 39 miss% 12.676631421227933
plot_id,batch_id 0 40 miss% 15.847888618934261
plot_id,batch_id 0 41 miss% 14.413666908435518
plot_id,batch_id 0 42 miss% 13.31445530660395
plot_id,batch_id 0 43 miss% 14.806372407849212
plot_id,batch_id 0 44 miss% 15.22905140457062
plot_id,batch_id 0 45 miss% 16.11447209657018
plot_id,batch_id 0 46 miss% 15.421366872083135
plot_id,batch_id 0 47 miss% 15.162740177418554
plot_id,batch_id 0 48 miss% 11.600469296224782
plot_id,batch_id 0 49 miss% 13.931552530154836
plot_id,batch_id 0 50 miss% 14.486827669340313
plot_id,batch_id 0 51 miss% 11.82466590141326
plot_id,batch_id 0 52 miss% 11.74820462166172
plot_id,batch_id 0 53 miss% 14.00827717831213
plot_id,batch_id 0 54 miss% 11.585721216031535
plot_id,batch_id 0 55 miss% 13.631584668065464
plot_id,batch_id 0 56 miss% 15.140830254707199
plot_id,batch_id 0 57 miss% 11.697640200261047
plot_id,batch_id 0 58 miss% 13.001025166677355
plot_id,batch_id 0 59 miss% 13.864814062221733
plot_id,batch_id 0 60 miss% 26.317721230439542
plot_id,batch_id 0 61 miss% 15.280614820227147
plot_id,batch_id 0 62 miss% 14.269071018129878
plot_id,batch_id 0 63 miss% 13.929505535880374
plot_id,batch_id 0 64 miss% 12.018635217381924
plot_id,batch_id 0 65 miss% 23.003806059171094
plot_id,batch_id 0 66 miss% 15.751542534399
plot_id,batch_id 0 67 miss% 14.842305055334469
plot_id,batch_id 0 68 miss% 15.587876969469816
plot_id,batch_id 0 69 miss% 13.343815498725276
plot_id,batch_id 0 70 miss% 21.386736919997407
plot_id,batch_id 0 71 miss% 15.690641688991844
plot_id,batch_id 0 72 miss% 12.82111361330438
plot_id,batch_id 0 73 miss% 14.964669273189648
plot_id,batch_id 0 74 miss% 13.375118081566372
plot_id,batch_id 0 75 miss% 23.884085030421918
plot_id,batch_id 0 76 miss% 15.616899880885152
plot_id,batch_id 0 77 miss% 14.555754685653161
plot_id,batch_id 0 78 miss% 14.23946276970898
plot_id,batch_id 0 79 miss% 13.99766923292009
plot_id,batch_id 0 80 miss% 19.800755825241538
plot_id,batch_id 0 81 miss% 15.908258301464988
plot_id,batch_id 0 82 miss% 13.65649786858355
plot_id,batch_id 0 83 miss% 14.156955799965232
plot_id,batch_id 0 84 miss% 13.28016843983879
plot_id,batch_id 0 85 miss% 19.901797245882065
plot_id,batch_id 0 86 miss% 13.732616197034048
plot_id,batch_id 0 87 miss% 14.410493996530795
plot_id,batch_id 0 88 miss% 14.246944757132884
plot_id,batch_id 0 89 miss% 13.243444009915832
plot_id,batch_id 0 90 miss% 20.898526633746986
plot_id,batch_id 0 91 miss% 14.582283512725791
plot_id,batch_id 0 92 miss% 13.792375925962283
plot_id,batch_id 0 93 miss% 13.190553508440221
plot_id,batch_id 0 94 miss% 13.895982807670427
plot_id,batch_id 0 95 miss% 18.622293820847737
plot_id,batch_id 0 96 miss% 15.084546926676692
plot_id,batch_id 0 97 miss% 12.795941518391917
plot_id,batch_id 0 98 miss% 14.725730801992722
plot_id,batch_id 0 99 miss% 13.830289924124177
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[19.06777958 12.57110527 13.56532696 13.15006166 13.18166603 19.36224824
 16.49405863 13.26965727 14.04241012 15.67425013 17.58032833 15.18736371
 13.19757616 12.5752742  13.30303845 17.49627037 14.49607851 14.89100809
 12.06441439 13.59092836 17.98362457 14.10226098 13.74609353 15.36300971
 15.33504243 14.42000862 12.14857237 14.63550708 12.67007618 14.35761502
 13.62553373 12.11955916 12.54917147 14.722795   11.76362151 13.65448701
 12.21067657 13.26502498 15.49456266 12.67663142 15.84788862 14.41366691
 13.31445531 14.80637241 15.2290514  16.1144721  15.42136687 15.16274018
 11.6004693  13.93155253 14.48682767 11.8246659  11.74820462 14.00827718
 11.58572122 13.63158467 15.14083025 11.6976402  13.00102517 13.86481406
 26.31772123 15.28061482 14.26907102 13.92950554 12.01863522 23.00380606
 15.75154253 14.84230506 15.58787697 13.3438155  21.38673692 15.69064169
 12.82111361 14.96466927 13.37511808 23.88408503 15.61689988 14.55575469
 14.23946277 13.99766923 19.80075583 15.9082583  13.65649787 14.1569558
 13.28016844 19.90179725 13.7326162  14.410494   14.24694476 13.24344401
 20.89852663 14.58228351 13.79237593 13.19055351 13.89598281 18.62229382
 15.08454693 12.79594152 14.7257308  13.83028992]
for model  239 the mean error 14.7706984796779
all id 239 hidden_dim 24 learning_rate 0.02 num_layers 5 frames 31 out win 5 err 14.7706984796779
Launcher: Job 240 completed in 9968 seconds.
Launcher: Task 121 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  77489
Epoch:0, Train loss:0.358366, valid loss:0.356021
Epoch:1, Train loss:0.024988, valid loss:0.004305
Epoch:2, Train loss:0.005989, valid loss:0.003258
Epoch:3, Train loss:0.004154, valid loss:0.002176
Epoch:4, Train loss:0.003275, valid loss:0.001570
Epoch:5, Train loss:0.002889, valid loss:0.001646
Epoch:6, Train loss:0.002572, valid loss:0.001581
Epoch:7, Train loss:0.002370, valid loss:0.001992
Epoch:8, Train loss:0.002276, valid loss:0.001483
Epoch:9, Train loss:0.002153, valid loss:0.001148
Epoch:10, Train loss:0.002057, valid loss:0.001428
Epoch:11, Train loss:0.001541, valid loss:0.000935
Epoch:12, Train loss:0.001480, valid loss:0.000873
Epoch:13, Train loss:0.001460, valid loss:0.001134
Epoch:14, Train loss:0.001447, valid loss:0.000936
Epoch:15, Train loss:0.001415, valid loss:0.000855
Epoch:16, Train loss:0.001359, valid loss:0.000836
Epoch:17, Train loss:0.001376, valid loss:0.000733
Epoch:18, Train loss:0.001358, valid loss:0.000843
Epoch:19, Train loss:0.001299, valid loss:0.000758
Epoch:20, Train loss:0.001290, valid loss:0.000829
Epoch:21, Train loss:0.001000, valid loss:0.000625
Epoch:22, Train loss:0.000997, valid loss:0.000714
Epoch:23, Train loss:0.000992, valid loss:0.000718
Epoch:24, Train loss:0.000976, valid loss:0.000602
Epoch:25, Train loss:0.000976, valid loss:0.000714
Epoch:26, Train loss:0.000957, valid loss:0.000664
Epoch:27, Train loss:0.000955, valid loss:0.000640
Epoch:28, Train loss:0.000937, valid loss:0.000877
Epoch:29, Train loss:0.000942, valid loss:0.000709
Epoch:30, Train loss:0.000944, valid loss:0.000655
Epoch:31, Train loss:0.000781, valid loss:0.000565
Epoch:32, Train loss:0.000789, valid loss:0.000546
Epoch:33, Train loss:0.000787, valid loss:0.000549
Epoch:34, Train loss:0.000759, valid loss:0.000550
Epoch:35, Train loss:0.000768, valid loss:0.000587
Epoch:36, Train loss:0.000763, valid loss:0.000562
Epoch:37, Train loss:0.000762, valid loss:0.000560
Epoch:38, Train loss:0.000760, valid loss:0.000565
Epoch:39, Train loss:0.000757, valid loss:0.000615
Epoch:40, Train loss:0.000738, valid loss:0.000574
Epoch:41, Train loss:0.000675, valid loss:0.000537
Epoch:42, Train loss:0.000676, valid loss:0.000525
Epoch:43, Train loss:0.000675, valid loss:0.000569
Epoch:44, Train loss:0.000673, valid loss:0.000512
Epoch:45, Train loss:0.000664, valid loss:0.000528
Epoch:46, Train loss:0.000666, valid loss:0.000528
Epoch:47, Train loss:0.000672, valid loss:0.000535
Epoch:48, Train loss:0.000666, valid loss:0.000566
Epoch:49, Train loss:0.000659, valid loss:0.000534
Epoch:50, Train loss:0.000661, valid loss:0.000558
Epoch:51, Train loss:0.000627, valid loss:0.000525
Epoch:52, Train loss:0.000622, valid loss:0.000523
Epoch:53, Train loss:0.000627, valid loss:0.000535
Epoch:54, Train loss:0.000624, valid loss:0.000537
Epoch:55, Train loss:0.000620, valid loss:0.000525
Epoch:56, Train loss:0.000624, valid loss:0.000523
Epoch:57, Train loss:0.000619, valid loss:0.000515
Epoch:58, Train loss:0.000618, valid loss:0.000532
Epoch:59, Train loss:0.000617, valid loss:0.000522
Epoch:60, Train loss:0.000614, valid loss:0.000539
training time 9827.024719715118
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.02618315678511196
plot_id,batch_id 0 1 miss% 0.015493244285389849
plot_id,batch_id 0 2 miss% 0.02816431653425676
plot_id,batch_id 0 3 miss% 0.01761538725899542
plot_id,batch_id 0 4 miss% 0.028873942733290067
plot_id,batch_id 0 5 miss% 0.042103119492785834
plot_id,batch_id 0 6 miss% 0.03022346867353208
plot_id,batch_id 0 7 miss% 0.03222873265495753
plot_id,batch_id 0 8 miss% 0.025392890322472512
plot_id,batch_id 0 9 miss% 0.015872910636561745
plot_id,batch_id 0 10 miss% 0.04033975640462177
plot_id,batch_id 0 11 miss% 0.03849826287692935
plot_id,batch_id 0 12 miss% 0.03137244503346727
plot_id,batch_id 0 13 miss% 0.027894455075930264
plot_id,batch_id 0 14 miss% 0.02639672754016713
plot_id,batch_id 0 15 miss% 0.030985342886377675
plot_id,batch_id 0 16 miss% 0.01994363356038795
plot_id,batch_id 0 17 miss% 0.030387269032494377
plot_id,batch_id 0 18 miss% 0.025582929631876805
plot_id,batch_id 0 19 miss% 0.026029477203784655
plot_id,batch_id 0 20 miss% 0.029417376218881703
plot_id,batch_id 0 21 miss% 0.01820382954299152
plot_id,batch_id 0 22 miss% 0.023635954516867362
plot_id,batch_id 0 23 miss% 0.02299993935920318
plot_id,batch_id 0 24 miss% 0.030292054587307035
plot_id,batch_id 0 25 miss% 0.02432357919870639
plot_id,batch_id 0 26 miss% 0.023205763040386743
plot_id,batch_id 0 27 miss% 0.02482984933653707
plot_id,batch_id 0 28 miss% 0.0236626259082238
plot_id,batch_id 0 29 miss% 0.022757596738987602
plot_id,batch_id 0 30 miss% 0.06218772053466215
plot_id,batch_id 0 31 miss% 0.02382461456819232
plot_id,batch_id 0 32 miss% 0.022359769830137086
plot_id,batch_id 0 33 miss% 0.025485868481613664
plot_id,batch_id 0 34 miss% 0.023663664196271507
plot_id,batch_id 0 35 miss% 0.040344339952991054
plot_id,batch_id 0 36 miss% 0.02710059080059978
plot_id,batch_id 0 37 miss% 0.010149960256989344
plot_id,batch_id 0 38 miss% 0.021693592377697708
plot_id,batch_id 0 39 miss% 0.013284056266501814
plot_id,batch_id 0 40 miss% 0.0651620173064477
plot_id,batch_id 0 41 miss% 0.0216134557040424
plot_id,batch_id 0 42 miss% 0.018585919284979276
plot_id,batch_id 0 43 miss% 0.02917765468469842
plot_id,batch_id 0 44 miss% 0.020805951925414845
plot_id,batch_id 0 45 miss% 0.015572799041453433
plot_id,batch_id 0 46 miss% 0.022916768644105894
plot_id,batch_id 0 47 miss% 0.022567854779047332
plot_id,batch_id 0 48 miss% 0.028778635683380325
plot_id,batch_id 0 49 miss% 0.02043847855609953
plot_id,batch_id 0 50 miss% 0.032324753790120085
plot_id,batch_id 0 51 miss% 0.019021757115982505
plot_id,batch_id 0 52 miss% 0.017764986205609505
plot_id,batch_id 0 53 miss% 0.010927975499426212
plot_id,batch_id 0 54 miss% 0.027515986487299988
plot_id,batch_id 0 55 miss% 0.04360588039892474
plot_id,batch_id 0 56 miss% 0.021151929529373795
plot_id,batch_id 0 57 miss% 0.024547638976226627
plot_id,batch_id 0 58 miss% 0.02466262876881716
plot_id,batch_id 0 59 miss% 0.02530038299708947
plot_id,batch_id 0 60 miss% 0.039237505973191165
plot_id,batch_id 0 61 miss% 0.03350163724171043
plot_id,batch_id 0 62 miss% 0.03493502614965369
plot_id,batch_id 0 63 miss% 0.025273183343493596
plot_id,batch_id 0 64 miss% 0.02941723944896915
plot_id,batch_id 0 65 miss% 0.024272706042496275
plot_id,batch_id 0 66 miss% 0.023333802438523035
plot_id,batch_id 0 67 miss% 0.026073990147256927
plot_id,batch_id 0 68 miss% 0.032356621095201876
plot_id,batch_id 0 69 miss% 0.02092518475437833
plot_id,batch_id 0 70 miss% 0.03239462916672381
plot_id,batch_id 0 71 miss% 0.051006227421223946
plot_id,batch_id 0 72 miss% 0.04263959804231231
plot_id,batch_id 0 73 miss% 0.029206041899007495
plot_id,batch_id 0 74 miss% 0.029763808745173474
plot_id,batch_id 0 75 miss% 0.04006445490581493
plot_id,batch_id 0 76 miss% 0.046035125228475485
plot_id,batch_id 0 77 miss% 0.026643380006282556
plot_id,batch_id 0 78 miss% 0.044796634777224016
plot_id,batch_id 0 79 miss% 0.04922864602362541
plot_id,batch_id 0 80 miss% 0.029664350165690054
plot_id,batch_id 0 81 miss% 0.02445569485269941
plot_id,batch_id 0 82 miss% 0.019163841331944265
plot_id,batch_id 0 83 miss% 0.02061402895652693
plot_id,batch_id 0 84 miss% 0.016910277735661203
plot_id,batch_id 0 85 miss% 0.04642013329846262
plot_id,batch_id 0 86 miss% 0.02797770381321765
plot_id,batch_id 0 87 miss% 0.03281765381349954
plot_id,batch_id 0 88 miss% 0.027951338846631688
plot_id,batch_id 0 89 miss% 0.02424902820185884
plot_id,batch_id 0 90 miss% 0.03893043373529487
plot_id,batch_id 0 91 miss% 0.034209304083675354
plot_id,batch_id 0 92 miss% 0.033182042552066096
plot_id,batch_id 0 93 miss% 0.02385039613126318
plot_id,batch_id 0 94 miss% 0.02164157387740418
plot_id,batch_id 0 95 miss% 0.0412748559792081
plot_id,batch_id 0 96 miss% 0.03733678182257792
plot_id,batch_id 0 97 miss% 0.048534976796905986
plot_id,batch_id 0 98 miss% 0.024212751800975874
plot_id,batch_id 0 99 miss% 0.02983770106461353
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02618316 0.01549324 0.02816432 0.01761539 0.02887394 0.04210312
 0.03022347 0.03222873 0.02539289 0.01587291 0.04033976 0.03849826
 0.03137245 0.02789446 0.02639673 0.03098534 0.01994363 0.03038727
 0.02558293 0.02602948 0.02941738 0.01820383 0.02363595 0.02299994
 0.03029205 0.02432358 0.02320576 0.02482985 0.02366263 0.0227576
 0.06218772 0.02382461 0.02235977 0.02548587 0.02366366 0.04034434
 0.02710059 0.01014996 0.02169359 0.01328406 0.06516202 0.02161346
 0.01858592 0.02917765 0.02080595 0.0155728  0.02291677 0.02256785
 0.02877864 0.02043848 0.03232475 0.01902176 0.01776499 0.01092798
 0.02751599 0.04360588 0.02115193 0.02454764 0.02466263 0.02530038
 0.03923751 0.03350164 0.03493503 0.02527318 0.02941724 0.02427271
 0.0233338  0.02607399 0.03235662 0.02092518 0.03239463 0.05100623
 0.0426396  0.02920604 0.02976381 0.04006445 0.04603513 0.02664338
 0.04479663 0.04922865 0.02966435 0.02445569 0.01916384 0.02061403
 0.01691028 0.04642013 0.0279777  0.03281765 0.02795134 0.02424903
 0.03893043 0.0342093  0.03318204 0.0238504  0.02164157 0.04127486
 0.03733678 0.04853498 0.02421275 0.0298377 ]
for model  184 the mean error 0.02869787981426595
all id 184 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 31 out win 4 err 0.02869787981426595
Launcher: Job 185 completed in 10020 seconds.
Launcher: Task 153 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (3, 3)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  79249
Epoch:0, Train loss:0.298934, valid loss:0.276258
Epoch:1, Train loss:0.016497, valid loss:0.003526
Epoch:2, Train loss:0.005308, valid loss:0.003014
Epoch:3, Train loss:0.004024, valid loss:0.001767
Epoch:4, Train loss:0.003671, valid loss:0.002024
Epoch:5, Train loss:0.003131, valid loss:0.001889
Epoch:6, Train loss:0.002964, valid loss:0.001384
Epoch:7, Train loss:0.002751, valid loss:0.001522
Epoch:8, Train loss:0.002650, valid loss:0.001377
Epoch:9, Train loss:0.002532, valid loss:0.001303
Epoch:10, Train loss:0.002502, valid loss:0.001096
Epoch:11, Train loss:0.001719, valid loss:0.001077
Epoch:12, Train loss:0.001712, valid loss:0.000917
Epoch:13, Train loss:0.001688, valid loss:0.001698
Epoch:14, Train loss:0.001652, valid loss:0.000943
Epoch:15, Train loss:0.001617, valid loss:0.000810
Epoch:16, Train loss:0.001567, valid loss:0.000872
Epoch:17, Train loss:0.001556, valid loss:0.000892
Epoch:18, Train loss:0.001459, valid loss:0.000769
Epoch:19, Train loss:0.001520, valid loss:0.000806
Epoch:20, Train loss:0.001522, valid loss:0.000997
Epoch:21, Train loss:0.001120, valid loss:0.000673
Epoch:22, Train loss:0.001145, valid loss:0.000706
Epoch:23, Train loss:0.001110, valid loss:0.000842
Epoch:24, Train loss:0.001108, valid loss:0.000725
Epoch:25, Train loss:0.001110, valid loss:0.000754
Epoch:26, Train loss:0.001111, valid loss:0.000697
Epoch:27, Train loss:0.001085, valid loss:0.000875
Epoch:28, Train loss:0.001087, valid loss:0.000716
Epoch:29, Train loss:0.001065, valid loss:0.000622
Epoch:30, Train loss:0.001056, valid loss:0.000802
Epoch:31, Train loss:0.000875, valid loss:0.000577
Epoch:32, Train loss:0.000866, valid loss:0.000770
Epoch:33, Train loss:0.000862, valid loss:0.000597
Epoch:34, Train loss:0.000853, valid loss:0.000614
Epoch:35, Train loss:0.000856, valid loss:0.000606
Epoch:36, Train loss:0.000848, valid loss:0.000700
Epoch:37, Train loss:0.000865, valid loss:0.000610
Epoch:38, Train loss:0.000852, valid loss:0.000585
Epoch:39, Train loss:0.000836, valid loss:0.000631
Epoch:40, Train loss:0.000830, valid loss:0.000609
Epoch:41, Train loss:0.000736, valid loss:0.000541
Epoch:42, Train loss:0.000722, valid loss:0.000563
Epoch:43, Train loss:0.000729, valid loss:0.000616
Epoch:44, Train loss:0.000722, valid loss:0.000542
Epoch:45, Train loss:0.000739, valid loss:0.000548
Epoch:46, Train loss:0.000717, valid loss:0.000543
Epoch:47, Train loss:0.000718, valid loss:0.000555
Epoch:48, Train loss:0.000727, valid loss:0.000624
Epoch:49, Train loss:0.000728, valid loss:0.000553
Epoch:50, Train loss:0.000711, valid loss:0.000542
Epoch:51, Train loss:0.000661, valid loss:0.000551
Epoch:52, Train loss:0.000667, valid loss:0.000532
Epoch:53, Train loss:0.000655, valid loss:0.000522
Epoch:54, Train loss:0.000660, valid loss:0.000543
Epoch:55, Train loss:0.000659, valid loss:0.000514
Epoch:56, Train loss:0.000662, valid loss:0.000521
Epoch:57, Train loss:0.000655, valid loss:0.000522
Epoch:58, Train loss:0.000655, valid loss:0.000568
Epoch:59, Train loss:0.000654, valid loss:0.000534
Epoch:60, Train loss:0.000652, valid loss:0.000519
training time 9991.407637834549
total number of trained parameters for initialize model 79249
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.029225504850534847
plot_id,batch_id 0 1 miss% 0.031553333901116046
plot_id,batch_id 0 2 miss% 0.01894806985615576
plot_id,batch_id 0 3 miss% 0.02114614100547947
plot_id,batch_id 0 4 miss% 0.022775687204161666
plot_id,batch_id 0 5 miss% 0.02772332926125824
plot_id,batch_id 0 6 miss% 0.019382232440354193
plot_id,batch_id 0 7 miss% 0.021104990943989568
plot_id,batch_id 0 8 miss% 0.02303517544461151
plot_id,batch_id 0 9 miss% 0.018530026759103306
plot_id,batch_id 0 10 miss% 0.02797062828588461
plot_id,batch_id 0 11 miss% 0.052812419343745566
plot_id,batch_id 0 12 miss% 0.0268111074481023
plot_id,batch_id 0 13 miss% 0.028359393904004857
plot_id,batch_id 0 14 miss% 0.03026521076858918
plot_id,batch_id 0 15 miss% 0.03095845326499582
plot_id,batch_id 0 16 miss% 0.034663223173533576
plot_id,batch_id 0 17 miss% 0.031433926382699366
plot_id,batch_id 0 18 miss% 0.04560924736509444
plot_id,batch_id 0 19 miss% 0.03354181259467779
plot_id,batch_id 0 20 miss% 0.041200399777997485
plot_id,batch_id 0 21 miss% 0.0181136313974017
plot_id,batch_id 0 22 miss% 0.01982724216298996
plot_id,batch_id 0 23 miss% 0.019466096623775155
plot_id,batch_id 0 24 miss% 0.024626370083890892
plot_id,batch_id 0 25 miss% 0.03716959081380133
plot_id,batch_id 0 26 miss% 0.020840585602047917
plot_id,batch_id 0 27 miss% 0.019724905753282302
plot_id,batch_id 0 28 miss% 0.025206689950841587
plot_id,batch_id 0 29 miss% 0.02127339734283696
plot_id,batch_id 0 30 miss% 0.06917886312877383
plot_id,batch_id 0 31 miss% 0.04132430846692442
plot_id,batch_id 0 32 miss% 0.029564069485802053
plot_id,batch_id 0 33 miss% 0.03071251954221513
plot_id,batch_id 0 34 miss% 0.026391695520375083
plot_id,batch_id 0 35 miss% 0.05941624221627154
plot_id,batch_id 0 36 miss% 0.03673530372353093
plot_id,batch_id 0 37 miss% 0.024441438070599838
plot_id,batch_id 0 38 miss% 0.031298455989550965
plot_id,batch_id 0 39 miss% 0.023902337943900048
plot_id,batch_id 0 40 miss% 0.04419890606798526
plot_id,batch_id 0 41 miss% 0.01998760587000404
plot_id,batch_id 0 42 miss% 0.017585111602192817
plot_id,batch_id 0 43 miss% 0.030507209258430416
plot_id,batch_id 0 44 miss% 0.01923905828431983
plot_id,batch_id 0 45 miss% 0.03486869564443874
plot_id,batch_id 0 46 miss% 0.020971810271316893
plot_id,batch_id 0 47 miss% 0.016031577953454407
plot_id,batch_id 0 48 miss% 0.02603798510964111
plot_id,batch_id 0 49 miss% 0.018087301143408317
plot_id,batch_id 0 50 miss% 0.025443308530879154
plot_id,batch_id 0 51 miss% 0.028107029691688105
plot_id,batch_id 0 52 miss% 0.0235137959313863
plot_id,batch_id 0 53 miss% 0.01115808541502343
plot_id,batch_id 0 54 miss% 0.028187124857657356
plot_id,batch_id 0 55 miss% 0.04279050595395787
plot_id,batch_id 0 56 miss% 0.021722090108143605
plot_id,batch_id 0 57 miss% 0.025020756362973164
plot_id,batch_id 0 58 miss% 0.029331630662647153
plot_id,batch_id 0 59 miss% 0.02872222408186316
plot_id,batch_id 0 60 miss% 0.03961637148315019
plot_id,batch_id 0 61 miss% 0.03304117294126466
plot_id,batch_id 0 62 miss% 0.04065787090827248
plot_id,batch_id 0 63 miss% 0.03534777190796577
plot_id,batch_id 0 64 miss% 0.028078047117709528
plot_id,batch_id 0 65 miss% 0.030672357873124913
plot_id,batch_id 0 66 miss% 0.03831082154007053
plot_id,batch_id 0 67 miss% 0.03599640220510699
plot_id,batch_id 0 68 miss% 0.03201253428493093
plot_id,batch_id 0 69 miss% 0.027263055826401535
plot_id,batch_id 0 70 miss% 0.026110976489438394
plot_id,batch_id 0 71 miss% 0.05431145841905696
plot_id,batch_id 0 72 miss% 0.03125592897513681
plot_id,batch_id 0 73 miss% 0.03960135418021485
plot_id,batch_id 0 74 miss% 0.03210792016298367
plot_id,batch_id 0 75 miss% 0.06527351080856557
plot_id,batch_id 0 76 miss% 0.04259358518216272
plot_id,batch_id 0 77 miss% 0.05419727089904546
plot_id,batch_id 0 78 miss% 0.03204629215297246
plot_id,batch_id 0 79 miss% 0.0397559740105187
plot_id,batch_id 0 80 miss% 0.04796890982932517
plot_id,batch_id 0 81 miss% 0.03565782646197783
plot_id,batch_id 0 82 miss% 0.03276043532258269
plot_id,batch_id 0 83 miss% 0.030717728590012296
plot_id,batch_id 0 84 miss% 0.0311464547286458
plot_id,batch_id 0 85 miss% 0.04361622443326358
plot_id,batch_id 0 86 miss% 0.03255505977458261
plot_id,batch_id 0 87 miss% 0.026859014015593003
plot_id,batch_id 0 88 miss% 0.037543103736011205
plot_id,batch_id 0 89 miss% 0.029911389667028516
plot_id,batch_id 0 90 miss% 0.027272309674960523
plot_id,batch_id 0 91 miss% 0.033033139842252805
plot_id,batch_id 0 92 miss% 0.02274026660098978
plot_id,batch_id 0 93 miss% 0.028080711294040203
plot_id,batch_id 0 94 miss% 0.054228716168553244
plot_id,batch_id 0 95 miss% 0.06687566246450541
plot_id,batch_id 0 96 miss% 0.0338437066273055
plot_id,batch_id 0 97 miss% 0.050150903005838
plot_id,batch_id 0 98 miss% 0.040119592420036536
plot_id,batch_id 0 99 miss% 0.03582544358895275
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0292255  0.03155333 0.01894807 0.02114614 0.02277569 0.02772333
 0.01938223 0.02110499 0.02303518 0.01853003 0.02797063 0.05281242
 0.02681111 0.02835939 0.03026521 0.03095845 0.03466322 0.03143393
 0.04560925 0.03354181 0.0412004  0.01811363 0.01982724 0.0194661
 0.02462637 0.03716959 0.02084059 0.01972491 0.02520669 0.0212734
 0.06917886 0.04132431 0.02956407 0.03071252 0.0263917  0.05941624
 0.0367353  0.02444144 0.03129846 0.02390234 0.04419891 0.01998761
 0.01758511 0.03050721 0.01923906 0.0348687  0.02097181 0.01603158
 0.02603799 0.0180873  0.02544331 0.02810703 0.0235138  0.01115809
 0.02818712 0.04279051 0.02172209 0.02502076 0.02933163 0.02872222
 0.03961637 0.03304117 0.04065787 0.03534777 0.02807805 0.03067236
 0.03831082 0.0359964  0.03201253 0.02726306 0.02611098 0.05431146
 0.03125593 0.03960135 0.03210792 0.06527351 0.04259359 0.05419727
 0.03204629 0.03975597 0.04796891 0.03565783 0.03276044 0.03071773
 0.03114645 0.04361622 0.03255506 0.02685901 0.0375431  0.02991139
 0.02727231 0.03303314 0.02274027 0.02808071 0.05422872 0.06687566
 0.03384371 0.0501509  0.04011959 0.03582544]
for model  197 the mean error 0.03206933146210863
all id 197 hidden_dim 32 learning_rate 0.01 num_layers 3 frames 31 out win 5 err 0.03206933146210863
Launcher: Job 198 completed in 10188 seconds.
Launcher: Task 130 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  107025
Epoch:0, Train loss:0.426322, valid loss:0.420939
Epoch:1, Train loss:0.039844, valid loss:0.006940
Epoch:2, Train loss:0.011014, valid loss:0.005141
Epoch:3, Train loss:0.008125, valid loss:0.004422
Epoch:4, Train loss:0.006807, valid loss:0.003311
Epoch:5, Train loss:0.005743, valid loss:0.002573
Epoch:6, Train loss:0.005073, valid loss:0.002744
Epoch:7, Train loss:0.004482, valid loss:0.002207
Epoch:8, Train loss:0.004159, valid loss:0.002454
Epoch:9, Train loss:0.003780, valid loss:0.002604
Epoch:10, Train loss:0.003528, valid loss:0.002051
Epoch:11, Train loss:0.002497, valid loss:0.001541
Epoch:12, Train loss:0.002522, valid loss:0.002373
Epoch:13, Train loss:0.002364, valid loss:0.001406
Epoch:14, Train loss:0.002344, valid loss:0.001646
Epoch:15, Train loss:0.002317, valid loss:0.001654
Epoch:16, Train loss:0.002288, valid loss:0.001468
Epoch:17, Train loss:0.002177, valid loss:0.001089
Epoch:18, Train loss:0.002127, valid loss:0.001495
Epoch:19, Train loss:0.002093, valid loss:0.001218
Epoch:20, Train loss:0.002058, valid loss:0.001168
Epoch:21, Train loss:0.001540, valid loss:0.001015
Epoch:22, Train loss:0.001540, valid loss:0.001050
Epoch:23, Train loss:0.001496, valid loss:0.001081
Epoch:24, Train loss:0.001494, valid loss:0.001548
Epoch:25, Train loss:0.001498, valid loss:0.001198
Epoch:26, Train loss:0.001484, valid loss:0.000961
Epoch:27, Train loss:0.001447, valid loss:0.000963
Epoch:28, Train loss:0.001439, valid loss:0.001109
Epoch:29, Train loss:0.001414, valid loss:0.000849
Epoch:30, Train loss:0.001397, valid loss:0.000981
Epoch:31, Train loss:0.001171, valid loss:0.000988
Epoch:32, Train loss:0.001162, valid loss:0.000817
Epoch:33, Train loss:0.001147, valid loss:0.000865
Epoch:34, Train loss:0.001134, valid loss:0.000973
Epoch:35, Train loss:0.001133, valid loss:0.000900
Epoch:36, Train loss:0.001138, valid loss:0.000915
Epoch:37, Train loss:0.001121, valid loss:0.000909
Epoch:38, Train loss:0.001107, valid loss:0.000834
Epoch:39, Train loss:0.001119, valid loss:0.001028
Epoch:40, Train loss:0.001116, valid loss:0.000860
Epoch:41, Train loss:0.000980, valid loss:0.000846
Epoch:42, Train loss:0.000963, valid loss:0.000822
Epoch:43, Train loss:0.000959, valid loss:0.000894
Epoch:44, Train loss:0.000978, valid loss:0.000793
Epoch:45, Train loss:0.000952, valid loss:0.000776
Epoch:46, Train loss:0.000974, valid loss:0.000809
Epoch:47, Train loss:0.000953, valid loss:0.000800
Epoch:48, Train loss:0.000951, valid loss:0.000777
Epoch:49, Train loss:0.000940, valid loss:0.000789
Epoch:50, Train loss:0.000937, valid loss:0.000789
Epoch:51, Train loss:0.000888, valid loss:0.000752
Epoch:52, Train loss:0.000889, valid loss:0.000765
Epoch:53, Train loss:0.000876, valid loss:0.000749
Epoch:54, Train loss:0.000879, valid loss:0.000785
Epoch:55, Train loss:0.000868, valid loss:0.000753
Epoch:56, Train loss:0.000872, valid loss:0.000785
Epoch:57, Train loss:0.000873, valid loss:0.000750
Epoch:58, Train loss:0.000869, valid loss:0.000798
Epoch:59, Train loss:0.000868, valid loss:0.000783
Epoch:60, Train loss:0.000862, valid loss:0.000753
training time 10172.110318422318
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.01907785937603986
plot_id,batch_id 0 1 miss% 0.022355630410955098
plot_id,batch_id 0 2 miss% 0.023899507204972307
plot_id,batch_id 0 3 miss% 0.02069363700699146
plot_id,batch_id 0 4 miss% 0.025765057392080595
plot_id,batch_id 0 5 miss% 0.034822206304280665
plot_id,batch_id 0 6 miss% 0.027803807764827927
plot_id,batch_id 0 7 miss% 0.029503278601169452
plot_id,batch_id 0 8 miss% 0.0326096656919946
plot_id,batch_id 0 9 miss% 0.027465632861411838
plot_id,batch_id 0 10 miss% 0.04582867641971909
plot_id,batch_id 0 11 miss% 0.045768300471960933
plot_id,batch_id 0 12 miss% 0.025667722234144156
plot_id,batch_id 0 13 miss% 0.031044936251506824
plot_id,batch_id 0 14 miss% 0.033339294666039576
plot_id,batch_id 0 15 miss% 0.04871119083185917
plot_id,batch_id 0 16 miss% 0.035603424274678004
plot_id,batch_id 0 17 miss% 0.04570942440901766
plot_id,batch_id 0 18 miss% 0.03658570885205797
plot_id,batch_id 0 19 miss% 0.03984817046710659
plot_id,batch_id 0 20 miss% 0.04419122244805135
plot_id,batch_id 0 21 miss% 0.019668305403019195
plot_id,batch_id 0 22 miss% 0.029609986500536808
plot_id,batch_id 0 23 miss% 0.030875689246352735
plot_id,batch_id 0 24 miss% 0.029158071968554994
plot_id,batch_id 0 25 miss% 0.03913261891092957
plot_id,batch_id 0 26 miss% 0.026109528649102785
plot_id,batch_id 0 27 miss% 0.022440256411030927
plot_id,batch_id 0 28 miss% 0.027382509979685273
plot_id,batch_id 0 29 miss% 0.029018781227456457
plot_id,batch_id 0 30 miss% 0.03822731484796853
plot_id,batch_id 0 31 miss% 0.030456663950431628
plot_id,batch_id 0 32 miss% 0.028866307346342827
plot_id,batch_id 0 33 miss% 0.030445583132623702
plot_id,batch_id 0 34 miss% 0.027041877285754405
plot_id,batch_id 0 35 miss% 0.03488477600763591
plot_id,batch_id 0 36 miss% 0.04255946620391529
plot_id,batch_id 0 37 miss% 0.027461022556842973
plot_id,batch_id 0 38 miss% 0.021680394879139125
plot_id,batch_id 0 39 miss% 0.014395693942112679
plot_id,batch_id 0 40 miss% 0.04062307536129238
plot_id,batch_id 0 41 miss% 0.024665833259785244
plot_id,batch_id 0 42 miss% 0.01823094811288056
plot_id,batch_id 0 43 miss% 0.02910242956182014
plot_id,batch_id 0 44 miss% 0.024832701109055107
plot_id,batch_id 0 45 miss% 0.0337036844748423
plot_id,batch_id 0 46 miss% 0.021695643728537422
plot_id,batch_id 0 47 miss% 0.02605533012486278
plot_id,batch_id 0 48 miss% 0.02542512865225426
plot_id,batch_id 0 49 miss% 0.025107839731515078
plot_id,batch_id 0 50 miss% 0.031727513360489935
plot_id,batch_id 0 51 miss% 0.02127268205093042
plot_id,batch_id 0 52 miss% 0.01648581075170543
plot_id,batch_id 0 53 miss% 0.015352098251287909
plot_id,batch_id 0 54 miss% 0.02846483332795879
plot_id,batch_id 0 55 miss% 0.032185627657836655
plot_id,batch_id 0 56 miss% 0.028352849218992223
plot_id,batch_id 0 57 miss% 0.02130498070092017
plot_id,batch_id 0 58 miss% 0.023157821664973
plot_id,batch_id 0 59 miss% 0.022209856075945295
plot_id,batch_id 0 60 miss% 0.04617010610782077
plot_id,batch_id 0 61 miss% 0.0346689149722414
plot_id,batch_id 0 62 miss% 0.022437435664102962
plot_id,batch_id 0 63 miss% 0.025924425341755184
plot_id,batch_id 0 64 miss% 0.03468347774657727
plot_id,batch_id 0 65 miss% 0.03354736803425826
plot_id,batch_id 0 66 miss% 0.03130235921833074
plot_id,batch_id 0 67 miss% 0.028123160887946436
plot_id,batch_id 0 68 miss% 0.02819794403507451
plot_id,batch_id 0 69 miss% 0.021087215503406215
plot_id,batch_id 0 70 miss% 0.04393794499764888
plot_id,batch_id 0 71 miss% 0.03737377210468862
plot_id,batch_id 0 72 miss% 0.038279555103854224
plot_id,batch_id 0 73 miss% 0.04040043738237871
plot_id,batch_id 0 74 miss% 0.0484416967786887
plot_id,batch_id 0 75 miss% 0.04101382460132846
plot_id,batch_id 0 76 miss% 0.0379598632104983
plot_id,batch_id 0 77 miss% 0.036996214216392424
plot_id,batch_id 0 78 miss% 0.03679554300691047
plot_id,batch_id 0 79 miss% 0.043855789039709284
plot_id,batch_id 0 80 miss% 0.030338632179598653
plot_id,batch_id 0 81 miss% 0.0256895693685282
plot_id,batch_id 0 82 miss% 0.028391543681755473
plot_id,batch_id 0 83 miss% 0.030790007082756657
plot_id,batch_id 0 84 miss% 0.027587810664230467
plot_id,batch_id 0 85 miss% 0.03741515820765634
plot_id,batch_id 0 86 miss% 0.03390966120192259
plot_id,batch_id 0 87 miss% 0.035026787039020915
plot_id,batch_id 0 88 miss% 0.03242708922269507
plot_id,batch_id 0 89 miss% 0.03151441946707492
plot_id,batch_id 0 90 miss% 0.03244682984776261
plot_id,batch_id 0 91 miss% 0.03390135844062029
plot_id,batch_id 0 92 miss% 0.035806140815354506
plot_id,batch_id 0 93 miss% 0.026550564420979788
plot_id,batch_id 0 94 miss% 0.031714505124691976
plot_id,batch_id 0 95 miss% 0.03047773081715798
plot_id,batch_id 0 96 miss% 0.03656571322442555
plot_id,batch_id 0 97 miss% 0.048978253762316286
plot_id,batch_id 0 98 miss% 0.02845537407228751
plot_id,batch_id 0 99 miss% 0.03641552791553172
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01907786 0.02235563 0.02389951 0.02069364 0.02576506 0.03482221
 0.02780381 0.02950328 0.03260967 0.02746563 0.04582868 0.0457683
 0.02566772 0.03104494 0.03333929 0.04871119 0.03560342 0.04570942
 0.03658571 0.03984817 0.04419122 0.01966831 0.02960999 0.03087569
 0.02915807 0.03913262 0.02610953 0.02244026 0.02738251 0.02901878
 0.03822731 0.03045666 0.02886631 0.03044558 0.02704188 0.03488478
 0.04255947 0.02746102 0.02168039 0.01439569 0.04062308 0.02466583
 0.01823095 0.02910243 0.0248327  0.03370368 0.02169564 0.02605533
 0.02542513 0.02510784 0.03172751 0.02127268 0.01648581 0.0153521
 0.02846483 0.03218563 0.02835285 0.02130498 0.02315782 0.02220986
 0.04617011 0.03466891 0.02243744 0.02592443 0.03468348 0.03354737
 0.03130236 0.02812316 0.02819794 0.02108722 0.04393794 0.03737377
 0.03827956 0.04040044 0.0484417  0.04101382 0.03795986 0.03699621
 0.03679554 0.04385579 0.03033863 0.02568957 0.02839154 0.03079001
 0.02758781 0.03741516 0.03390966 0.03502679 0.03242709 0.03151442
 0.03244683 0.03390136 0.03580614 0.02655056 0.03171451 0.03047773
 0.03656571 0.04897825 0.02845537 0.03641553]
for model  98 the mean error 0.031212656140781656
all id 98 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 25 out win 5 err 0.031212656140781656
Launcher: Job 99 completed in 10380 seconds.
Launcher: Task 196 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  77489
Epoch:0, Train loss:0.358366, valid loss:0.356021
Epoch:1, Train loss:0.021306, valid loss:0.004220
Epoch:2, Train loss:0.005550, valid loss:0.003055
Epoch:3, Train loss:0.003986, valid loss:0.002000
Epoch:4, Train loss:0.003279, valid loss:0.001661
Epoch:5, Train loss:0.002957, valid loss:0.001582
Epoch:6, Train loss:0.002684, valid loss:0.001728
Epoch:7, Train loss:0.002624, valid loss:0.001808
Epoch:8, Train loss:0.002468, valid loss:0.001471
Epoch:9, Train loss:0.002452, valid loss:0.001536
Epoch:10, Train loss:0.002316, valid loss:0.001247
Epoch:11, Train loss:0.001675, valid loss:0.000955
Epoch:12, Train loss:0.001636, valid loss:0.001017
Epoch:13, Train loss:0.001637, valid loss:0.001030
Epoch:14, Train loss:0.001553, valid loss:0.001068
Epoch:15, Train loss:0.001551, valid loss:0.001017
Epoch:16, Train loss:0.001500, valid loss:0.000810
Epoch:17, Train loss:0.001488, valid loss:0.000842
Epoch:18, Train loss:0.001511, valid loss:0.000873
Epoch:19, Train loss:0.001455, valid loss:0.000901
Epoch:20, Train loss:0.001468, valid loss:0.000798
Epoch:21, Train loss:0.001073, valid loss:0.000667
Epoch:22, Train loss:0.001072, valid loss:0.000688
Epoch:23, Train loss:0.001086, valid loss:0.000820
Epoch:24, Train loss:0.001056, valid loss:0.000657
Epoch:25, Train loss:0.001065, valid loss:0.000713
Epoch:26, Train loss:0.001039, valid loss:0.000657
Epoch:27, Train loss:0.001021, valid loss:0.000662
Epoch:28, Train loss:0.001034, valid loss:0.000679
Epoch:29, Train loss:0.001037, valid loss:0.000945
Epoch:30, Train loss:0.001010, valid loss:0.000698
Epoch:31, Train loss:0.000826, valid loss:0.000590
Epoch:32, Train loss:0.000818, valid loss:0.000551
Epoch:33, Train loss:0.000845, valid loss:0.000603
Epoch:34, Train loss:0.000822, valid loss:0.000614
Epoch:35, Train loss:0.000808, valid loss:0.000593
Epoch:36, Train loss:0.000800, valid loss:0.000644
Epoch:37, Train loss:0.000803, valid loss:0.000554
Epoch:38, Train loss:0.000799, valid loss:0.000680
Epoch:39, Train loss:0.000821, valid loss:0.000551
Epoch:40, Train loss:0.000795, valid loss:0.000576
Epoch:41, Train loss:0.000703, valid loss:0.000566
Epoch:42, Train loss:0.000703, valid loss:0.000555
Epoch:43, Train loss:0.000703, valid loss:0.000565
Epoch:44, Train loss:0.000697, valid loss:0.000542
Epoch:45, Train loss:0.000694, valid loss:0.000547
Epoch:46, Train loss:0.000685, valid loss:0.000556
Epoch:47, Train loss:0.000690, valid loss:0.000563
Epoch:48, Train loss:0.000687, valid loss:0.000573
Epoch:49, Train loss:0.000687, valid loss:0.000538
Epoch:50, Train loss:0.000683, valid loss:0.000580
Epoch:51, Train loss:0.000645, valid loss:0.000522
Epoch:52, Train loss:0.000640, valid loss:0.000528
Epoch:53, Train loss:0.000641, valid loss:0.000523
Epoch:54, Train loss:0.000640, valid loss:0.000529
Epoch:55, Train loss:0.000638, valid loss:0.000516
Epoch:56, Train loss:0.000636, valid loss:0.000526
Epoch:57, Train loss:0.000636, valid loss:0.000535
Epoch:58, Train loss:0.000632, valid loss:0.000552
Epoch:59, Train loss:0.000635, valid loss:0.000526
Epoch:60, Train loss:0.000627, valid loss:0.000522
training time 10228.196670532227
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.0483873981626108
plot_id,batch_id 0 1 miss% 0.027152352273120215
plot_id,batch_id 0 2 miss% 0.03323786460259679
plot_id,batch_id 0 3 miss% 0.03109623912130988
plot_id,batch_id 0 4 miss% 0.0394165409902785
plot_id,batch_id 0 5 miss% 0.03816443966560309
plot_id,batch_id 0 6 miss% 0.027420333680332587
plot_id,batch_id 0 7 miss% 0.030115366606315017
plot_id,batch_id 0 8 miss% 0.0297166939807524
plot_id,batch_id 0 9 miss% 0.026651623041837755
plot_id,batch_id 0 10 miss% 0.040621902840867447
plot_id,batch_id 0 11 miss% 0.034228203059777376
plot_id,batch_id 0 12 miss% 0.028259228331536646
plot_id,batch_id 0 13 miss% 0.01835283120616891
plot_id,batch_id 0 14 miss% 0.027679790579869374
plot_id,batch_id 0 15 miss% 0.03987184966443209
plot_id,batch_id 0 16 miss% 0.03214646049870893
plot_id,batch_id 0 17 miss% 0.04447500776323241
plot_id,batch_id 0 18 miss% 0.022177270665151037
plot_id,batch_id 0 19 miss% 0.031439869329025955
plot_id,batch_id 0 20 miss% 0.04329927983076672
plot_id,batch_id 0 21 miss% 0.015362102341285516
plot_id,batch_id 0 22 miss% 0.04187455219053958
plot_id,batch_id 0 23 miss% 0.03384013179474112
plot_id,batch_id 0 24 miss% 0.03717611638748529
plot_id,batch_id 0 25 miss% 0.03123348847748559
plot_id,batch_id 0 26 miss% 0.028682248842036424
plot_id,batch_id 0 27 miss% 0.0266167490063833
plot_id,batch_id 0 28 miss% 0.02207671273111985
plot_id,batch_id 0 29 miss% 0.028816377753055532
plot_id,batch_id 0 30 miss% 0.06314672133738004
plot_id,batch_id 0 31 miss% 0.03369000162582467
plot_id,batch_id 0 32 miss% 0.026607903688720902
plot_id,batch_id 0 33 miss% 0.020119155737372156
plot_id,batch_id 0 34 miss% 0.0316311306501926
plot_id,batch_id 0 35 miss% 0.055840415996804456
plot_id,batch_id 0 36 miss% 0.0385272353862468
plot_id,batch_id 0 37 miss% 0.027117819539596175
plot_id,batch_id 0 38 miss% 0.02173159776628519
plot_id,batch_id 0 39 miss% 0.02020258080551966
plot_id,batch_id 0 40 miss% 0.05481142577075989
plot_id,batch_id 0 41 miss% 0.028584727131395708
plot_id,batch_id 0 42 miss% 0.01649298478306386
plot_id,batch_id 0 43 miss% 0.025331419606144715
plot_id,batch_id 0 44 miss% 0.029256787160776704
plot_id,batch_id 0 45 miss% 0.03307296255930986
plot_id,batch_id 0 46 miss% 0.03431914677048512
plot_id,batch_id 0 47 miss% 0.019510863828785852
plot_id,batch_id 0 48 miss% 0.02419683151067087
plot_id,batch_id 0 49 miss% 0.02154095270109522
plot_id,batch_id 0 50 miss% 0.028651006139972835
plot_id,batch_id 0 51 miss% 0.026034099021628114
plot_id,batch_id 0 52 miss% 0.01865145608489608
plot_id,batch_id 0 53 miss% 0.018814299111520542
plot_id,batch_id 0 54 miss% 0.027757565227662376
plot_id,batch_id 0 55 miss% 0.03913954655779409
plot_id,batch_id 0 56 miss% 0.023532193548349574
plot_id,batch_id 0 57 miss% 0.021833873247958824
plot_id,batch_id 0 58 miss% 0.02061013602216683
plot_id,batch_id 0 59 miss% 0.026356275666649494
plot_id,batch_id 0 60 miss% 0.05170549757871616
plot_id,batch_id 0 61 miss% 0.040688946996081415
plot_id,batch_id 0 62 miss% 0.038105726637398286
plot_id,batch_id 0 63 miss% 0.03076018623928441
plot_id,batch_id 0 64 miss% 0.02336269862035793
plot_id,batch_id 0 65 miss% 0.04246109399764818
plot_id,batch_id 0 66 miss% 0.04035082615863146
plot_id,batch_id 0 67 miss% 0.027320604764493373
plot_id,batch_id 0 68 miss% 0.025258583002775958
plot_id,batch_id 0 69 miss% 0.02453447970085745
plot_id,batch_id 0 70 miss% 0.03862074915179436
plot_id,batch_id 0 71 miss% 0.04746484267332202
plot_id,batch_id 0 72 miss% 0.04148170784871622
plot_id,batch_id 0 73 miss% 0.026108780748459792
plot_id,batch_id 0 74 miss% 0.024738539650284698
plot_id,batch_id 0 75 miss% 0.026786629759431965
plot_id,batch_id 0 76 miss% 0.028207923756695896
plot_id,batch_id 0 77 miss% 0.0478813525182695
plot_id,batch_id 0 78 miss% 0.035656621405735944
plot_id,batch_id 0 79 miss% 0.035168424110781984
plot_id,batch_id 0 80 miss% 0.03890770193714401
plot_id,batch_id 0 81 miss% 0.019327013780661567
plot_id,batch_id 0 82 miss% 0.03107719783390328
plot_id,batch_id 0 83 miss% 0.028701656272433866
plot_id,batch_id 0 84 miss% 0.025011868065311423
plot_id,batch_id 0 85 miss% 0.030093683044961246
plot_id,batch_id 0 86 miss% 0.03203687746168026
plot_id,batch_id 0 87 miss% 0.028075926948871337
plot_id,batch_id 0 88 miss% 0.029216056978471933
plot_id,batch_id 0 89 miss% 0.028526976734022262
plot_id,batch_id 0 90 miss% 0.029607737919155233
plot_id,batch_id 0 91 miss% 0.044440828636780595
plot_id,batch_id 0 92 miss% 0.022223483424266506
plot_id,batch_id 0 93 miss% 0.029448807251589184
plot_id,batch_id 0 94 miss% 0.028334367681628485
plot_id,batch_id 0 95 miss% 0.02545218120398876
plot_id,batch_id 0 96 miss% 0.03721328727341072
plot_id,batch_id 0 97 miss% 0.03650133719955425
plot_id,batch_id 0 98 miss% 0.020627846091121938
plot_id,batch_id 0 99 miss% 0.02624271335247162
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.0483874  0.02715235 0.03323786 0.03109624 0.03941654 0.03816444
 0.02742033 0.03011537 0.02971669 0.02665162 0.0406219  0.0342282
 0.02825923 0.01835283 0.02767979 0.03987185 0.03214646 0.04447501
 0.02217727 0.03143987 0.04329928 0.0153621  0.04187455 0.03384013
 0.03717612 0.03123349 0.02868225 0.02661675 0.02207671 0.02881638
 0.06314672 0.03369    0.0266079  0.02011916 0.03163113 0.05584042
 0.03852724 0.02711782 0.0217316  0.02020258 0.05481143 0.02858473
 0.01649298 0.02533142 0.02925679 0.03307296 0.03431915 0.01951086
 0.02419683 0.02154095 0.02865101 0.0260341  0.01865146 0.0188143
 0.02775757 0.03913955 0.02353219 0.02183387 0.02061014 0.02635628
 0.0517055  0.04068895 0.03810573 0.03076019 0.0233627  0.04246109
 0.04035083 0.0273206  0.02525858 0.02453448 0.03862075 0.04746484
 0.04148171 0.02610878 0.02473854 0.02678663 0.02820792 0.04788135
 0.03565662 0.03516842 0.0389077  0.01932701 0.0310772  0.02870166
 0.02501187 0.03009368 0.03203688 0.02807593 0.02921606 0.02852698
 0.02960774 0.04444083 0.02222348 0.02944881 0.02833437 0.02545218
 0.03721329 0.03650134 0.02062785 0.02624271]
for model  211 the mean error 0.03124433904814621
all id 211 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 31 out win 4 err 0.03124433904814621
Launcher: Job 212 completed in 10420 seconds.
Launcher: Task 210 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  77489
Epoch:0, Train loss:0.358366, valid loss:0.356021
Epoch:1, Train loss:0.021455, valid loss:0.004468
Epoch:2, Train loss:0.006087, valid loss:0.002461
Epoch:3, Train loss:0.004564, valid loss:0.002224
Epoch:4, Train loss:0.003922, valid loss:0.001698
Epoch:5, Train loss:0.003651, valid loss:0.001810
Epoch:6, Train loss:0.003382, valid loss:0.001809
Epoch:7, Train loss:0.003440, valid loss:0.001801
Epoch:8, Train loss:0.003109, valid loss:0.002140
Epoch:9, Train loss:0.003046, valid loss:0.001527
Epoch:10, Train loss:0.002898, valid loss:0.001642
Epoch:11, Train loss:0.001976, valid loss:0.001559
Epoch:12, Train loss:0.002004, valid loss:0.001036
Epoch:13, Train loss:0.001886, valid loss:0.001212
Epoch:14, Train loss:0.001887, valid loss:0.001326
Epoch:15, Train loss:0.001853, valid loss:0.000929
Epoch:16, Train loss:0.001809, valid loss:0.001016
Epoch:17, Train loss:0.001833, valid loss:0.001138
Epoch:18, Train loss:0.001775, valid loss:0.001032
Epoch:19, Train loss:0.001737, valid loss:0.001408
Epoch:20, Train loss:0.001742, valid loss:0.000968
Epoch:21, Train loss:0.001274, valid loss:0.000803
Epoch:22, Train loss:0.001282, valid loss:0.000737
Epoch:23, Train loss:0.001261, valid loss:0.000832
Epoch:24, Train loss:0.001250, valid loss:0.001002
Epoch:25, Train loss:0.001241, valid loss:0.000750
Epoch:26, Train loss:0.001217, valid loss:0.000802
Epoch:27, Train loss:0.001230, valid loss:0.000902
Epoch:28, Train loss:0.001230, valid loss:0.000814
Epoch:29, Train loss:0.001168, valid loss:0.000810
Epoch:30, Train loss:0.001205, valid loss:0.000698
Epoch:31, Train loss:0.000939, valid loss:0.000621
Epoch:32, Train loss:0.000946, valid loss:0.000635
Epoch:33, Train loss:0.000955, valid loss:0.000671
Epoch:34, Train loss:0.000939, valid loss:0.000669
Epoch:35, Train loss:0.000945, valid loss:0.000817
Epoch:36, Train loss:0.000931, valid loss:0.000647
Epoch:37, Train loss:0.000913, valid loss:0.000634
Epoch:38, Train loss:0.000923, valid loss:0.000683
Epoch:39, Train loss:0.000912, valid loss:0.000616
Epoch:40, Train loss:0.000895, valid loss:0.000578
Epoch:41, Train loss:0.000781, valid loss:0.000578
Epoch:42, Train loss:0.000769, valid loss:0.000576
Epoch:43, Train loss:0.000779, valid loss:0.000589
Epoch:44, Train loss:0.000779, valid loss:0.000691
Epoch:45, Train loss:0.000779, valid loss:0.000588
Epoch:46, Train loss:0.000750, valid loss:0.000545
Epoch:47, Train loss:0.000762, valid loss:0.000581
Epoch:48, Train loss:0.000764, valid loss:0.000646
Epoch:49, Train loss:0.000761, valid loss:0.000653
Epoch:50, Train loss:0.000754, valid loss:0.000559
Epoch:51, Train loss:0.000691, valid loss:0.000537
Epoch:52, Train loss:0.000684, valid loss:0.000532
Epoch:53, Train loss:0.000686, valid loss:0.000535
Epoch:54, Train loss:0.000682, valid loss:0.000534
Epoch:55, Train loss:0.000678, valid loss:0.000565
Epoch:56, Train loss:0.000686, valid loss:0.000582
Epoch:57, Train loss:0.000678, valid loss:0.000584
Epoch:58, Train loss:0.000669, valid loss:0.000545
Epoch:59, Train loss:0.000670, valid loss:0.000561
Epoch:60, Train loss:0.000677, valid loss:0.000588
training time 10237.599928855896
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.021461833714498762
plot_id,batch_id 0 1 miss% 0.020731191136166314
plot_id,batch_id 0 2 miss% 0.018166752163304666
plot_id,batch_id 0 3 miss% 0.02177657845753533
plot_id,batch_id 0 4 miss% 0.029041727924192743
plot_id,batch_id 0 5 miss% 0.052348840081377336
plot_id,batch_id 0 6 miss% 0.032128562860312994
plot_id,batch_id 0 7 miss% 0.022390007465025518
plot_id,batch_id 0 8 miss% 0.03149940336793945
plot_id,batch_id 0 9 miss% 0.020332017090445475
plot_id,batch_id 0 10 miss% 0.04622499321478
plot_id,batch_id 0 11 miss% 0.051194677517851075
plot_id,batch_id 0 12 miss% 0.026578339762002738
plot_id,batch_id 0 13 miss% 0.022551846750423828
plot_id,batch_id 0 14 miss% 0.03327948588361463
plot_id,batch_id 0 15 miss% 0.03366115960404001
plot_id,batch_id 0 16 miss% 0.032094972022071566
plot_id,batch_id 0 17 miss% 0.04998810976119462
plot_id,batch_id 0 18 miss% 0.031650108500708166
plot_id,batch_id 0 19 miss% 0.03466608164610779
plot_id,batch_id 0 20 miss% 0.05360492053730055
plot_id,batch_id 0 21 miss% 0.029769266717337515
plot_id,batch_id 0 22 miss% 0.02567128221556018
plot_id,batch_id 0 23 miss% 0.02921961561279187
plot_id,batch_id 0 24 miss% 0.024324903821130645
plot_id,batch_id 0 25 miss% 0.03918721655097554
plot_id,batch_id 0 26 miss% 0.0337494663109769
plot_id,batch_id 0 27 miss% 0.02547198286502032
plot_id,batch_id 0 28 miss% 0.019170978288285782
plot_id,batch_id 0 29 miss% 0.026346102494971463
plot_id,batch_id 0 30 miss% 0.06002015056104323
plot_id,batch_id 0 31 miss% 0.022080080479755394
plot_id,batch_id 0 32 miss% 0.023220970737468722
plot_id,batch_id 0 33 miss% 0.021360986786639245
plot_id,batch_id 0 34 miss% 0.026939198199257563
plot_id,batch_id 0 35 miss% 0.040017433730209884
plot_id,batch_id 0 36 miss% 0.031439578427892086
plot_id,batch_id 0 37 miss% 0.026861456634035018
plot_id,batch_id 0 38 miss% 0.025891501467130564
plot_id,batch_id 0 39 miss% 0.021743738183870754
plot_id,batch_id 0 40 miss% 0.08854467571706885
plot_id,batch_id 0 41 miss% 0.03085068324746983
plot_id,batch_id 0 42 miss% 0.019810539360523457
plot_id,batch_id 0 43 miss% 0.04248977563464991
plot_id,batch_id 0 44 miss% 0.024789336107673064
plot_id,batch_id 0 45 miss% 0.03296003876327229
plot_id,batch_id 0 46 miss% 0.03712034443444736
plot_id,batch_id 0 47 miss% 0.026107266949321487
plot_id,batch_id 0 48 miss% 0.02527255827602882
plot_id,batch_id 0 49 miss% 0.023616787614016466
plot_id,batch_id 0 50 miss% 0.03308824674129851
plot_id,batch_id 0 51 miss% 0.027104526934117454
plot_id,batch_id 0 52 miss% 0.03028250446315915
plot_id,batch_id 0 53 miss% 0.018570685467307262
plot_id,batch_id 0 54 miss% 0.030041377803066305
plot_id,batch_id 0 55 miss% 0.04031953341063998
plot_id,batch_id 0 56 miss% 0.031303103305339204
plot_id,batch_id 0 57 miss% 0.028226280855948783
plot_id,batch_id 0 58 miss% 0.035256139556396705
plot_id,batch_id 0 59 miss% 0.03169728953683188
plot_id,batch_id 0 60 miss% 0.04318696958659642
plot_id,batch_id 0 61 miss% 0.03194033075492423
plot_id,batch_id 0 62 miss% 0.03461028486771587
plot_id,batch_id 0 63 miss% 0.03800812852893786
plot_id,batch_id 0 64 miss% 0.039401949053184296
plot_id,batch_id 0 65 miss% 0.045641080483136864
plot_id,batch_id 0 66 miss% 0.04334312309837107
plot_id,batch_id 0 67 miss% 0.04019505885333159
plot_id,batch_id 0 68 miss% 0.027284196926883497
plot_id,batch_id 0 69 miss% 0.022314519620886604
plot_id,batch_id 0 70 miss% 0.04320761422818681
plot_id,batch_id 0 71 miss% 0.052413232737766706
plot_id,batch_id 0 72 miss% 0.034761595761784836
plot_id,batch_id 0 73 miss% 0.0331449285837664
plot_id,batch_id 0 74 miss% 0.04344780562841986
plot_id,batch_id 0 75 miss% 0.04566595995414328
plot_id,batch_id 0 76 miss% 0.03527385550741271
plot_id,batch_id 0 77 miss% 0.03008304829980807
plot_id,batch_id 0 78 miss% 0.03787260874812217
plot_id,batch_id 0 79 miss% 0.03132357972460619
plot_id,batch_id 0 80 miss% 0.041331189761628126
plot_id,batch_id 0 81 miss% 0.024405034917776656
plot_id,batch_id 0 82 miss% 0.03175256034246244
plot_id,batch_id 0 83 miss% 0.025145086668407538
plot_id,batch_id 0 84 miss% 0.016554823345864625
plot_id,batch_id 0 85 miss% 0.05088407079444791
plot_id,batch_id 0 86 miss% 0.020050770008565342
plot_id,batch_id 0 87 miss% 0.03778867669045557
plot_id,batch_id 0 88 miss% 0.03870098729522232
plot_id,batch_id 0 89 miss% 0.03616670773338511
plot_id,batch_id 0 90 miss% 0.04362113394255023
plot_id,batch_id 0 91 miss% 0.03330989406696555
plot_id,batch_id 0 92 miss% 0.0402809914339864
plot_id,batch_id 0 93 miss% 0.0336187911463603
plot_id,batch_id 0 94 miss% 0.03204635105261526
plot_id,batch_id 0 95 miss% 0.03535187453821516
plot_id,batch_id 0 96 miss% 0.029607082490861743
plot_id,batch_id 0 97 miss% 0.048513405946503316
plot_id,batch_id 0 98 miss% 0.02734471572983341
plot_id,batch_id 0 99 miss% 0.028655191683701012
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02146183 0.02073119 0.01816675 0.02177658 0.02904173 0.05234884
 0.03212856 0.02239001 0.0314994  0.02033202 0.04622499 0.05119468
 0.02657834 0.02255185 0.03327949 0.03366116 0.03209497 0.04998811
 0.03165011 0.03466608 0.05360492 0.02976927 0.02567128 0.02921962
 0.0243249  0.03918722 0.03374947 0.02547198 0.01917098 0.0263461
 0.06002015 0.02208008 0.02322097 0.02136099 0.0269392  0.04001743
 0.03143958 0.02686146 0.0258915  0.02174374 0.08854468 0.03085068
 0.01981054 0.04248978 0.02478934 0.03296004 0.03712034 0.02610727
 0.02527256 0.02361679 0.03308825 0.02710453 0.0302825  0.01857069
 0.03004138 0.04031953 0.0313031  0.02822628 0.03525614 0.03169729
 0.04318697 0.03194033 0.03461028 0.03800813 0.03940195 0.04564108
 0.04334312 0.04019506 0.0272842  0.02231452 0.04320761 0.05241323
 0.0347616  0.03314493 0.04344781 0.04566596 0.03527386 0.03008305
 0.03787261 0.03132358 0.04133119 0.02440503 0.03175256 0.02514509
 0.01655482 0.05088407 0.02005077 0.03778868 0.03870099 0.03616671
 0.04362113 0.03330989 0.04028099 0.03361879 0.03204635 0.03535187
 0.02960708 0.04851341 0.02734472 0.02865519]
for model  238 the mean error 0.03323558424261616
all id 238 hidden_dim 24 learning_rate 0.02 num_layers 5 frames 31 out win 4 err 0.03323558424261616
Launcher: Job 239 completed in 10430 seconds.
Launcher: Task 232 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  134801
Epoch:0, Train loss:0.475526, valid loss:0.475468
Epoch:1, Train loss:0.364392, valid loss:0.362428
Epoch:2, Train loss:0.349298, valid loss:0.360454
Epoch:3, Train loss:0.347256, valid loss:0.360349
Epoch:4, Train loss:0.345577, valid loss:0.360596
Epoch:5, Train loss:0.345139, valid loss:0.359282
Epoch:6, Train loss:0.344261, valid loss:0.358977
Epoch:7, Train loss:0.344127, valid loss:0.359699
Epoch:8, Train loss:0.344019, valid loss:0.359275
Epoch:9, Train loss:0.343667, valid loss:0.358986
Epoch:10, Train loss:0.343431, valid loss:0.359376
Epoch:11, Train loss:0.342092, valid loss:0.358255
Epoch:12, Train loss:0.342095, valid loss:0.358338
Epoch:13, Train loss:0.342055, valid loss:0.357989
Epoch:14, Train loss:0.342001, valid loss:0.358464
Epoch:15, Train loss:0.342005, valid loss:0.358312
Epoch:16, Train loss:0.341964, valid loss:0.358239
Epoch:17, Train loss:0.342069, valid loss:0.358534
Epoch:18, Train loss:0.341860, valid loss:0.358123
Epoch:19, Train loss:0.341871, valid loss:0.358855
Epoch:20, Train loss:0.341729, valid loss:0.358321
Epoch:21, Train loss:0.341125, valid loss:0.357944
Epoch:22, Train loss:0.341106, valid loss:0.357973
Epoch:23, Train loss:0.341104, valid loss:0.357925
Epoch:24, Train loss:0.341052, valid loss:0.358316
Epoch:25, Train loss:0.341042, valid loss:0.358012
Epoch:26, Train loss:0.341018, valid loss:0.358069
Epoch:27, Train loss:0.341024, valid loss:0.358168
Epoch:28, Train loss:0.341013, valid loss:0.358076
Epoch:29, Train loss:0.340996, valid loss:0.357894
Epoch:30, Train loss:0.340945, valid loss:0.357803
Epoch:31, Train loss:0.340615, valid loss:0.357742
Epoch:32, Train loss:0.340610, valid loss:0.357918
Epoch:33, Train loss:0.340634, valid loss:0.357710
Epoch:34, Train loss:0.342782, valid loss:0.358271
Epoch:35, Train loss:0.341445, valid loss:0.357856
Epoch:36, Train loss:0.340908, valid loss:0.357819
Epoch:37, Train loss:0.340737, valid loss:0.357794
Epoch:38, Train loss:0.340650, valid loss:0.357980
Epoch:39, Train loss:0.340635, valid loss:0.357805
Epoch:40, Train loss:0.340595, valid loss:0.357762
Epoch:41, Train loss:0.340442, valid loss:0.357697
Epoch:42, Train loss:0.340442, valid loss:0.357778
Epoch:43, Train loss:0.340440, valid loss:0.357731
Epoch:44, Train loss:0.340447, valid loss:0.357729
Epoch:45, Train loss:0.340427, valid loss:0.357704
Epoch:46, Train loss:0.340532, valid loss:0.358550
Epoch:47, Train loss:0.340569, valid loss:0.357774
Epoch:48, Train loss:0.340406, valid loss:0.357684
Epoch:49, Train loss:0.340401, valid loss:0.357671
Epoch:50, Train loss:0.340401, valid loss:0.357675
Epoch:51, Train loss:0.340330, valid loss:0.357668
Epoch:52, Train loss:0.340326, valid loss:0.357667
Epoch:53, Train loss:0.340330, valid loss:0.357723
Epoch:54, Train loss:0.340322, valid loss:0.357652
Epoch:55, Train loss:0.340314, valid loss:0.357669
Epoch:56, Train loss:0.340318, valid loss:0.357670
Epoch:57, Train loss:0.340322, valid loss:0.357638
Epoch:58, Train loss:0.340315, valid loss:0.357643
Epoch:59, Train loss:0.340325, valid loss:0.357658
Epoch:60, Train loss:0.340306, valid loss:0.357654
training time 10284.913602352142
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7821151775940759
plot_id,batch_id 0 1 miss% 0.8293587593745643
plot_id,batch_id 0 2 miss% 0.8346375401364414
plot_id,batch_id 0 3 miss% 0.8394572830811151
plot_id,batch_id 0 4 miss% 0.842207574877947
plot_id,batch_id 0 5 miss% 0.774108717565255
plot_id,batch_id 0 6 miss% 0.8234399542799723
plot_id,batch_id 0 7 miss% 0.8360317383076148
plot_id,batch_id 0 8 miss% 0.8414552675301192
plot_id,batch_id 0 9 miss% 0.84579734882541
plot_id,batch_id 0 10 miss% 0.7681049355595486
plot_id,batch_id 0 11 miss% 0.8235639251463766
plot_id,batch_id 0 12 miss% 0.8336000316757858
plot_id,batch_id 0 13 miss% 0.8383369420767107
plot_id,batch_id 0 14 miss% 0.8407822992115089
plot_id,batch_id 0 15 miss% 0.7722747606868889
plot_id,batch_id 0 16 miss% 0.8200466468089553
plot_id,batch_id 0 17 miss% 0.831910150350985
plot_id,batch_id 0 18 miss% 0.841657131797111
plot_id,batch_id 0 19 miss% 0.838553344117253
plot_id,batch_id 0 20 miss% 0.8054447409286886
plot_id,batch_id 0 21 miss% 0.8355228743575752
plot_id,batch_id 0 22 miss% 0.8389076327119618
plot_id,batch_id 0 23 miss% 0.8425748167573659
plot_id,batch_id 0 24 miss% 0.8432000794616967
plot_id,batch_id 0 25 miss% 0.8070152838605329
plot_id,batch_id 0 26 miss% 0.8351450990127499
plot_id,batch_id 0 27 miss% 0.8395137066701003
plot_id,batch_id 0 28 miss% 0.8419591807542566
plot_id,batch_id 0 29 miss% 0.8428855004437695
plot_id,batch_id 0 30 miss% 0.7923538818312065
plot_id,batch_id 0 31 miss% 0.8313894085662611
plot_id,batch_id 0 32 miss% 0.8389458056123635
plot_id,batch_id 0 33 miss% 0.8415779414409932
plot_id,batch_id 0 34 miss% 0.8428224087967181
plot_id,batch_id 0 35 miss% 0.7885110154967785
plot_id,batch_id 0 36 miss% 0.8357200679818074
plot_id,batch_id 0 37 miss% 0.8357263566531934
plot_id,batch_id 0 38 miss% 0.8426088639696794
plot_id,batch_id 0 39 miss% 0.844000157904424
plot_id,batch_id 0 40 miss% 0.8213493538848268
plot_id,batch_id 0 41 miss% 0.839673860261892
plot_id,batch_id 0 42 miss% 0.839795359115081
plot_id,batch_id 0 43 miss% 0.8446863637817068
plot_id,batch_id 0 44 miss% 0.8503326617542306
plot_id,batch_id 0 45 miss% 0.8167480963720772
plot_id,batch_id 0 46 miss% 0.8399053725995606
plot_id,batch_id 0 47 miss% 0.8413388055776602
plot_id,batch_id 0 48 miss% 0.8443687632634829
plot_id,batch_id 0 49 miss% 0.8492360035391807
plot_id,batch_id 0 50 miss% 0.8306369744480453
plot_id,batch_id 0 51 miss% 0.837547480417797
plot_id,batch_id 0 52 miss% 0.8415160825634982
plot_id,batch_id 0 53 miss% 0.8442529796503213
plot_id,batch_id 0 54 miss% 0.8449522790029215
plot_id,batch_id 0 55 miss% 0.8231383382998343
plot_id,batch_id 0 56 miss% 0.8366718700550191
plot_id,batch_id 0 57 miss% 0.8406089843239605
plot_id,batch_id 0 58 miss% 0.8449167006014672
plot_id,batch_id 0 59 miss% 0.8431796368303014
plot_id,batch_id 0 60 miss% 0.7158994998118349
plot_id,batch_id 0 61 miss% 0.8136785276966193
plot_id,batch_id 0 62 miss% 0.8227090858847967
plot_id,batch_id 0 63 miss% 0.8300648466219318
plot_id,batch_id 0 64 miss% 0.8372589911373108
plot_id,batch_id 0 65 miss% 0.7168614562377656
plot_id,batch_id 0 66 miss% 0.7975433051638916
plot_id,batch_id 0 67 miss% 0.8124571069755522
plot_id,batch_id 0 68 miss% 0.8299389675885784
plot_id,batch_id 0 69 miss% 0.8304964987152731
plot_id,batch_id 0 70 miss% 0.674157656472573
plot_id,batch_id 0 71 miss% 0.8074579187753043
plot_id,batch_id 0 72 miss% 0.8046374036038856
plot_id,batch_id 0 73 miss% 0.8163212925812331
plot_id,batch_id 0 74 miss% 0.8270477765396275
plot_id,batch_id 0 75 miss% 0.6915017019530137
plot_id,batch_id 0 76 miss% 0.7946953854590091
plot_id,batch_id 0 77 miss% 0.7930488859469139
plot_id,batch_id 0 78 miss% 0.8163169249205195
plot_id,batch_id 0 79 miss% 0.821207129545211
plot_id,batch_id 0 80 miss% 0.7451731021104214
plot_id,batch_id 0 81 miss% 0.8198121749904725
plot_id,batch_id 0 82 miss% 0.827975705377732
plot_id,batch_id 0 83 miss% 0.83432978855515
plot_id,batch_id 0 84 miss% 0.8383879406526744
plot_id,batch_id 0 85 miss% 0.7330186743480368
plot_id,batch_id 0 86 miss% 0.8110233802472652
plot_id,batch_id 0 87 miss% 0.8274033428174659
plot_id,batch_id 0 88 miss% 0.83626226569867
plot_id,batch_id 0 89 miss% 0.834968633045129
plot_id,batch_id 0 90 miss% 0.7003619019632902
plot_id,batch_id 0 91 miss% 0.8026388055434537
plot_id,batch_id 0 92 miss% 0.8210310393596599
plot_id,batch_id 0 93 miss% 0.821691261029221
plot_id,batch_id 0 94 miss% 0.8392904415831233
plot_id,batch_id 0 95 miss% 0.7188653668204258
plot_id,batch_id 0 96 miss% 0.7922884965699799
plot_id,batch_id 0 97 miss% 0.8177277940406771
plot_id,batch_id 0 98 miss% 0.8269893973881791
plot_id,batch_id 0 99 miss% 0.8296436979868446
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.78211518 0.82935876 0.83463754 0.83945728 0.84220757 0.77410872
 0.82343995 0.83603174 0.84145527 0.84579735 0.76810494 0.82356393
 0.83360003 0.83833694 0.8407823  0.77227476 0.82004665 0.83191015
 0.84165713 0.83855334 0.80544474 0.83552287 0.83890763 0.84257482
 0.84320008 0.80701528 0.8351451  0.83951371 0.84195918 0.8428855
 0.79235388 0.83138941 0.83894581 0.84157794 0.84282241 0.78851102
 0.83572007 0.83572636 0.84260886 0.84400016 0.82134935 0.83967386
 0.83979536 0.84468636 0.85033266 0.8167481  0.83990537 0.84133881
 0.84436876 0.849236   0.83063697 0.83754748 0.84151608 0.84425298
 0.84495228 0.82313834 0.83667187 0.84060898 0.8449167  0.84317964
 0.7158995  0.81367853 0.82270909 0.83006485 0.83725899 0.71686146
 0.79754331 0.81245711 0.82993897 0.8304965  0.67415766 0.80745792
 0.8046374  0.81632129 0.82704778 0.6915017  0.79469539 0.79304889
 0.81631692 0.82120713 0.7451731  0.81981217 0.82797571 0.83432979
 0.83838794 0.73301867 0.81102338 0.82740334 0.83626227 0.83496863
 0.7003619  0.80263881 0.82103104 0.82169126 0.83929044 0.71886537
 0.7922885  0.81772779 0.8269894  0.8296437 ]
for model  161 the mean error 0.8179030389031733
all id 161 hidden_dim 32 learning_rate 0.02 num_layers 5 frames 25 out win 5 err 0.8179030389031733
Launcher: Job 162 completed in 10460 seconds.
Launcher: Task 120 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  134801
Epoch:0, Train loss:0.637387, valid loss:0.638600
Epoch:1, Train loss:0.516027, valid loss:0.522108
Epoch:2, Train loss:0.502873, valid loss:0.518726
Epoch:3, Train loss:0.500042, valid loss:0.518300
Epoch:4, Train loss:0.498037, valid loss:0.518367
Epoch:5, Train loss:0.497045, valid loss:0.517015
Epoch:6, Train loss:0.496108, valid loss:0.517334
Epoch:7, Train loss:0.496120, valid loss:0.517756
Epoch:8, Train loss:0.495344, valid loss:0.517330
Epoch:9, Train loss:0.495077, valid loss:0.516805
Epoch:10, Train loss:0.494797, valid loss:0.517413
Epoch:11, Train loss:0.493327, valid loss:0.516030
Epoch:12, Train loss:0.493149, valid loss:0.516183
Epoch:13, Train loss:0.493166, valid loss:0.515780
Epoch:14, Train loss:0.493169, valid loss:0.516182
Epoch:15, Train loss:0.493058, valid loss:0.516021
Epoch:16, Train loss:0.493025, valid loss:0.515843
Epoch:17, Train loss:0.493081, valid loss:0.516157
Epoch:18, Train loss:0.492847, valid loss:0.516119
Epoch:19, Train loss:0.492739, valid loss:0.515796
Epoch:20, Train loss:0.492857, valid loss:0.515820
Epoch:21, Train loss:0.492060, valid loss:0.515781
Epoch:22, Train loss:0.492003, valid loss:0.515732
Epoch:23, Train loss:0.491979, valid loss:0.515482
Epoch:24, Train loss:0.491993, valid loss:0.515587
Epoch:25, Train loss:0.491986, valid loss:0.515537
Epoch:26, Train loss:0.491982, valid loss:0.515488
Epoch:27, Train loss:0.492001, valid loss:0.516088
Epoch:28, Train loss:0.491961, valid loss:0.515611
Epoch:29, Train loss:0.491872, valid loss:0.515396
Epoch:30, Train loss:0.491955, valid loss:0.515537
Epoch:31, Train loss:0.491577, valid loss:0.515469
Epoch:32, Train loss:0.491524, valid loss:0.515372
Epoch:33, Train loss:0.491474, valid loss:0.515472
Epoch:34, Train loss:0.491543, valid loss:0.515408
Epoch:35, Train loss:0.491520, valid loss:0.515443
Epoch:36, Train loss:0.491487, valid loss:0.515549
Epoch:37, Train loss:0.491474, valid loss:0.515452
Epoch:38, Train loss:0.491466, valid loss:0.515396
Epoch:39, Train loss:0.491494, valid loss:0.515453
Epoch:40, Train loss:0.491434, valid loss:0.515346
Epoch:41, Train loss:0.491300, valid loss:0.515384
Epoch:42, Train loss:0.491266, valid loss:0.515325
Epoch:43, Train loss:0.491286, valid loss:0.515397
Epoch:44, Train loss:0.491285, valid loss:0.515344
Epoch:45, Train loss:0.491266, valid loss:0.515317
Epoch:46, Train loss:0.491255, valid loss:0.515335
Epoch:47, Train loss:0.491250, valid loss:0.515349
Epoch:48, Train loss:0.491274, valid loss:0.515345
Epoch:49, Train loss:0.491283, valid loss:0.515331
Epoch:50, Train loss:0.491242, valid loss:0.515355
Epoch:51, Train loss:0.491172, valid loss:0.515328
Epoch:52, Train loss:0.491159, valid loss:0.515305
Epoch:53, Train loss:0.491163, valid loss:0.515317
Epoch:54, Train loss:0.491167, valid loss:0.515370
Epoch:55, Train loss:0.491166, valid loss:0.515345
Epoch:56, Train loss:0.491148, valid loss:0.515323
Epoch:57, Train loss:0.491149, valid loss:0.515308
Epoch:58, Train loss:0.491151, valid loss:0.515309
Epoch:59, Train loss:0.491152, valid loss:0.515314
Epoch:60, Train loss:0.491146, valid loss:0.515299
training time 10370.72143292427
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.8165486979496129
plot_id,batch_id 0 1 miss% 0.8543561330875884
plot_id,batch_id 0 2 miss% 0.8604191455070592
plot_id,batch_id 0 3 miss% 0.8659139628115639
plot_id,batch_id 0 4 miss% 0.869896593647755
plot_id,batch_id 0 5 miss% 0.8149605078418689
plot_id,batch_id 0 6 miss% 0.8518443572682411
plot_id,batch_id 0 7 miss% 0.8628797206669598
plot_id,batch_id 0 8 miss% 0.8628882114974668
plot_id,batch_id 0 9 miss% 0.8659700026866938
plot_id,batch_id 0 10 miss% 0.8064162710267526
plot_id,batch_id 0 11 miss% 0.8491232456892539
plot_id,batch_id 0 12 miss% 0.8589967236961091
plot_id,batch_id 0 13 miss% 0.8613950194577016
plot_id,batch_id 0 14 miss% 0.8651123614221878
plot_id,batch_id 0 15 miss% 0.802980446364832
plot_id,batch_id 0 16 miss% 0.8465951101467823
plot_id,batch_id 0 17 miss% 0.8575587637457369
plot_id,batch_id 0 18 miss% 0.8623737088509498
plot_id,batch_id 0 19 miss% 0.8637018474793059
plot_id,batch_id 0 20 miss% 0.8374329918397383
plot_id,batch_id 0 21 miss% 0.8627472204472317
plot_id,batch_id 0 22 miss% 0.865077998858528
plot_id,batch_id 0 23 miss% 0.8686745078640278
plot_id,batch_id 0 24 miss% 0.8693734346250357
plot_id,batch_id 0 25 miss% 0.837436887793685
plot_id,batch_id 0 26 miss% 0.8573755487209078
plot_id,batch_id 0 27 miss% 0.8634450989270066
plot_id,batch_id 0 28 miss% 0.8673328843280683
plot_id,batch_id 0 29 miss% 0.8692502492689621
plot_id,batch_id 0 30 miss% 0.8228451012657342
plot_id,batch_id 0 31 miss% 0.8564597344981321
plot_id,batch_id 0 32 miss% 0.8617082900171867
plot_id,batch_id 0 33 miss% 0.8642227245000219
plot_id,batch_id 0 34 miss% 0.8636517206111242
plot_id,batch_id 0 35 miss% 0.8250099339416567
plot_id,batch_id 0 36 miss% 0.8605771135095494
plot_id,batch_id 0 37 miss% 0.8602707750916185
plot_id,batch_id 0 38 miss% 0.8649287116371844
plot_id,batch_id 0 39 miss% 0.866713542447602
plot_id,batch_id 0 40 miss% 0.8492649749787192
plot_id,batch_id 0 41 miss% 0.8631358454848144
plot_id,batch_id 0 42 miss% 0.8647469453944905
plot_id,batch_id 0 43 miss% 0.8691088555081912
plot_id,batch_id 0 44 miss% 0.8691195753919522
plot_id,batch_id 0 45 miss% 0.8467854975063895
plot_id,batch_id 0 46 miss% 0.8623134253372194
plot_id,batch_id 0 47 miss% 0.8648813883229395
plot_id,batch_id 0 48 miss% 0.8701618072949794
plot_id,batch_id 0 49 miss% 0.8697843897354216
plot_id,batch_id 0 50 miss% 0.8497693300196528
plot_id,batch_id 0 51 miss% 0.8628540834268508
plot_id,batch_id 0 52 miss% 0.8652981943074507
plot_id,batch_id 0 53 miss% 0.8698675762496835
plot_id,batch_id 0 54 miss% 0.8711859565454753
plot_id,batch_id 0 55 miss% 0.8476784070657549
plot_id,batch_id 0 56 miss% 0.8639707432766726
plot_id,batch_id 0 57 miss% 0.8676820030436153
plot_id,batch_id 0 58 miss% 0.8698111775371401
plot_id,batch_id 0 59 miss% 0.8682160002896977
plot_id,batch_id 0 60 miss% 0.7637987900205452
plot_id,batch_id 0 61 miss% 0.8425180572399322
plot_id,batch_id 0 62 miss% 0.8524275858258442
plot_id,batch_id 0 63 miss% 0.8573249826609348
plot_id,batch_id 0 64 miss% 0.8600921914023811
plot_id,batch_id 0 65 miss% 0.7564758165189075
plot_id,batch_id 0 66 miss% 0.8371906947130441
plot_id,batch_id 0 67 miss% 0.8425951903892893
plot_id,batch_id 0 68 miss% 0.8568999775751215
plot_id,batch_id 0 69 miss% 0.857248351024944
plot_id,batch_id 0 70 miss% 0.7242366250072257
plot_id,batch_id 0 71 miss% 0.837123923290176
plot_id,batch_id 0 72 miss% 0.8407243085662527
plot_id,batch_id 0 73 miss% 0.8484101522625389
plot_id,batch_id 0 74 miss% 0.8556422201335049
plot_id,batch_id 0 75 miss% 0.7473521036994114
plot_id,batch_id 0 76 miss% 0.8200736851282507
plot_id,batch_id 0 77 miss% 0.8266059578048356
plot_id,batch_id 0 78 miss% 0.8420250216656116
plot_id,batch_id 0 79 miss% 0.8533870990249902
plot_id,batch_id 0 80 miss% 0.7874095518973471
plot_id,batch_id 0 81 miss% 0.850126957113573
plot_id,batch_id 0 82 miss% 0.8574205372581515
plot_id,batch_id 0 83 miss% 0.857972449210915
plot_id,batch_id 0 84 miss% 0.8627727434217071
plot_id,batch_id 0 85 miss% 0.7781033086781033
plot_id,batch_id 0 86 miss% 0.843930795748964
plot_id,batch_id 0 87 miss% 0.8517838418457339
plot_id,batch_id 0 88 miss% 0.8604600120667868
plot_id,batch_id 0 89 miss% 0.8620266270011131
plot_id,batch_id 0 90 miss% 0.75084704678207
plot_id,batch_id 0 91 miss% 0.8356525817618013
plot_id,batch_id 0 92 miss% 0.846820405293465
plot_id,batch_id 0 93 miss% 0.850355912826228
plot_id,batch_id 0 94 miss% 0.8617206230642878
plot_id,batch_id 0 95 miss% 0.7614230896602706
plot_id,batch_id 0 96 miss% 0.8302989048612742
plot_id,batch_id 0 97 miss% 0.8474032989555068
plot_id,batch_id 0 98 miss% 0.8505138556506346
plot_id,batch_id 0 99 miss% 0.8553599605846045
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.8165487  0.85435613 0.86041915 0.86591396 0.86989659 0.81496051
 0.85184436 0.86287972 0.86288821 0.86597    0.80641627 0.84912325
 0.85899672 0.86139502 0.86511236 0.80298045 0.84659511 0.85755876
 0.86237371 0.86370185 0.83743299 0.86274722 0.865078   0.86867451
 0.86937343 0.83743689 0.85737555 0.8634451  0.86733288 0.86925025
 0.8228451  0.85645973 0.86170829 0.86422272 0.86365172 0.82500993
 0.86057711 0.86027078 0.86492871 0.86671354 0.84926497 0.86313585
 0.86474695 0.86910886 0.86911958 0.8467855  0.86231343 0.86488139
 0.87016181 0.86978439 0.84976933 0.86285408 0.86529819 0.86986758
 0.87118596 0.84767841 0.86397074 0.867682   0.86981118 0.868216
 0.76379879 0.84251806 0.85242759 0.85732498 0.86009219 0.75647582
 0.83719069 0.84259519 0.85689998 0.85724835 0.72423663 0.83712392
 0.84072431 0.84841015 0.85564222 0.7473521  0.82007369 0.82660596
 0.84202502 0.8533871  0.78740955 0.85012696 0.85742054 0.85797245
 0.86277274 0.77810331 0.8439308  0.85178384 0.86046001 0.86202663
 0.75084705 0.83565258 0.84682041 0.85035591 0.86172062 0.76142309
 0.8302989  0.8474033  0.85051386 0.85535996]
for model  53 the mean error 0.8463465671539083
all id 53 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 21 out win 5 err 0.8463465671539083
Launcher: Job 54 completed in 10551 seconds.
Launcher: Task 242 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  107025
Epoch:0, Train loss:0.426322, valid loss:0.420939
Epoch:1, Train loss:0.036073, valid loss:0.007616
Epoch:2, Train loss:0.011347, valid loss:0.004323
Epoch:3, Train loss:0.008579, valid loss:0.004155
Epoch:4, Train loss:0.007093, valid loss:0.003300
Epoch:5, Train loss:0.005990, valid loss:0.002844
Epoch:6, Train loss:0.005418, valid loss:0.003188
Epoch:7, Train loss:0.004961, valid loss:0.002765
Epoch:8, Train loss:0.004863, valid loss:0.002274
Epoch:9, Train loss:0.004452, valid loss:0.003081
Epoch:10, Train loss:0.004522, valid loss:0.002546
Epoch:11, Train loss:0.002852, valid loss:0.001598
Epoch:12, Train loss:0.002918, valid loss:0.001547
Epoch:13, Train loss:0.002778, valid loss:0.001762
Epoch:14, Train loss:0.002764, valid loss:0.001635
Epoch:15, Train loss:0.002743, valid loss:0.001576
Epoch:16, Train loss:0.002689, valid loss:0.001789
Epoch:17, Train loss:0.002592, valid loss:0.001673
Epoch:18, Train loss:0.002541, valid loss:0.001271
Epoch:19, Train loss:0.002602, valid loss:0.001243
Epoch:20, Train loss:0.002474, valid loss:0.001506
Epoch:21, Train loss:0.001715, valid loss:0.001051
Epoch:22, Train loss:0.001742, valid loss:0.001233
Epoch:23, Train loss:0.001740, valid loss:0.001030
Epoch:24, Train loss:0.001718, valid loss:0.001278
Epoch:25, Train loss:0.001679, valid loss:0.001083
Epoch:26, Train loss:0.001753, valid loss:0.001100
Epoch:27, Train loss:0.001685, valid loss:0.000982
Epoch:28, Train loss:0.001673, valid loss:0.001153
Epoch:29, Train loss:0.001638, valid loss:0.001179
Epoch:30, Train loss:0.001610, valid loss:0.001397
Epoch:31, Train loss:0.001260, valid loss:0.000951
Epoch:32, Train loss:0.001267, valid loss:0.000977
Epoch:33, Train loss:0.001208, valid loss:0.000929
Epoch:34, Train loss:0.001253, valid loss:0.000925
Epoch:35, Train loss:0.001217, valid loss:0.000901
Epoch:36, Train loss:0.001209, valid loss:0.000922
Epoch:37, Train loss:0.001238, valid loss:0.000918
Epoch:38, Train loss:0.001268, valid loss:0.000871
Epoch:39, Train loss:0.001167, valid loss:0.000888
Epoch:40, Train loss:0.001166, valid loss:0.000868
Epoch:41, Train loss:0.001013, valid loss:0.000871
Epoch:42, Train loss:0.000987, valid loss:0.000849
Epoch:43, Train loss:0.000991, valid loss:0.000823
Epoch:44, Train loss:0.000999, valid loss:0.000848
Epoch:45, Train loss:0.000991, valid loss:0.000804
Epoch:46, Train loss:0.000978, valid loss:0.000810
Epoch:47, Train loss:0.000963, valid loss:0.000818
Epoch:48, Train loss:0.000990, valid loss:0.000878
Epoch:49, Train loss:0.000952, valid loss:0.000781
Epoch:50, Train loss:0.000944, valid loss:0.000815
Epoch:51, Train loss:0.000872, valid loss:0.000807
Epoch:52, Train loss:0.000861, valid loss:0.000807
Epoch:53, Train loss:0.000866, valid loss:0.000776
Epoch:54, Train loss:0.000857, valid loss:0.000792
Epoch:55, Train loss:0.000852, valid loss:0.000808
Epoch:56, Train loss:0.000856, valid loss:0.000797
Epoch:57, Train loss:0.000855, valid loss:0.000909
Epoch:58, Train loss:0.000861, valid loss:0.000769
Epoch:59, Train loss:0.000845, valid loss:0.000791
Epoch:60, Train loss:0.000865, valid loss:0.000812
training time 10362.557406425476
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.031413365438033296
plot_id,batch_id 0 1 miss% 0.019022686999035036
plot_id,batch_id 0 2 miss% 0.026415158504672995
plot_id,batch_id 0 3 miss% 0.027686439301948595
plot_id,batch_id 0 4 miss% 0.022452612892252706
plot_id,batch_id 0 5 miss% 0.04624203129343684
plot_id,batch_id 0 6 miss% 0.021899605793277186
plot_id,batch_id 0 7 miss% 0.02549399983580167
plot_id,batch_id 0 8 miss% 0.020553083556879775
plot_id,batch_id 0 9 miss% 0.02856251789281535
plot_id,batch_id 0 10 miss% 0.03844913057753177
plot_id,batch_id 0 11 miss% 0.03501960293888262
plot_id,batch_id 0 12 miss% 0.027862309561204633
plot_id,batch_id 0 13 miss% 0.029249156269217706
plot_id,batch_id 0 14 miss% 0.029339591578631325
plot_id,batch_id 0 15 miss% 0.031111746301125263
plot_id,batch_id 0 16 miss% 0.035482097701395814
plot_id,batch_id 0 17 miss% 0.039720507467680584
plot_id,batch_id 0 18 miss% 0.0275853470783681
plot_id,batch_id 0 19 miss% 0.030771783431591145
plot_id,batch_id 0 20 miss% 0.03960130163431939
plot_id,batch_id 0 21 miss% 0.032769242801820746
plot_id,batch_id 0 22 miss% 0.026191908982389604
plot_id,batch_id 0 23 miss% 0.023441621036523544
plot_id,batch_id 0 24 miss% 0.01534902773499307
plot_id,batch_id 0 25 miss% 0.035496245035538276
plot_id,batch_id 0 26 miss% 0.029432702201648567
plot_id,batch_id 0 27 miss% 0.02009903954703671
plot_id,batch_id 0 28 miss% 0.019459144796723952
plot_id,batch_id 0 29 miss% 0.02275444609914687
plot_id,batch_id 0 30 miss% 0.046378668721168
plot_id,batch_id 0 31 miss% 0.029666803693581174
plot_id,batch_id 0 32 miss% 0.030644945659029827
plot_id,batch_id 0 33 miss% 0.015107117913920796
plot_id,batch_id 0 34 miss% 0.014513671587549782
plot_id,batch_id 0 35 miss% 0.03624059927600004
plot_id,batch_id 0 36 miss% 0.04075285632358725
plot_id,batch_id 0 37 miss% 0.022297496409426902
plot_id,batch_id 0 38 miss% 0.023687470673568453
plot_id,batch_id 0 39 miss% 0.02833351567931111
plot_id,batch_id 0 40 miss% 0.06578690078819173
plot_id,batch_id 0 41 miss% 0.011722722282519894
plot_id,batch_id 0 42 miss% 0.022365465732270138
plot_id,batch_id 0 43 miss% 0.015180382965753757
plot_id,batch_id 0 44 miss% 0.024317311936584814
plot_id,batch_id 0 45 miss% 0.02721439733406208
plot_id,batch_id 0 46 miss% 0.026198822589998384
plot_id,batch_id 0 47 miss% 0.01666411976088857
plot_id,batch_id 0 48 miss% 0.019899338342697467
plot_id,batch_id 0 49 miss% 0.02531076318463566
plot_id,batch_id 0 50 miss% 0.041535428523292374
plot_id,batch_id 0 51 miss% 0.026730989025179275
plot_id,batch_id 0 52 miss% 0.026296399599902084
plot_id,batch_id 0 53 miss% 0.014620955714698779
plot_id,batch_id 0 54 miss% 0.03032108075757251
plot_id,batch_id 0 55 miss% 0.03986190069534038
plot_id,batch_id 0 56 miss% 0.018805023278433232
plot_id,batch_id 0 57 miss% 0.029113768663346206
plot_id,batch_id 0 58 miss% 0.02484448101359442
plot_id,batch_id 0 59 miss% 0.027564831798354346
plot_id,batch_id 0 60 miss% 0.04166119329522423
plot_id,batch_id 0 61 miss% 0.04570453315291699
plot_id,batch_id 0 62 miss% 0.029759908649752313
plot_id,batch_id 0 63 miss% 0.029995819932806044
plot_id,batch_id 0 64 miss% 0.030862276330538306
plot_id,batch_id 0 65 miss% 0.05158373587088112
plot_id,batch_id 0 66 miss% 0.0553744291581122
plot_id,batch_id 0 67 miss% 0.01997992726880171
plot_id,batch_id 0 68 miss% 0.019280698923359044
plot_id,batch_id 0 69 miss% 0.03269063755702535
plot_id,batch_id 0 70 miss% 0.044319707881506416
plot_id,batch_id 0 71 miss% 0.03619106263192631
plot_id,batch_id 0 72 miss% 0.04162571286373961
plot_id,batch_id 0 73 miss% 0.0266305125744667
plot_id,batch_id 0 74 miss% 0.03339006212395153
plot_id,batch_id 0 75 miss% 0.029681476741327043
plot_id,batch_id 0 76 miss% 0.029160430764274103
plot_id,batch_id 0 77 miss% 0.046809811000931324
plot_id,batch_id 0 78 miss% 0.035198730021156345
plot_id,batch_id 0 79 miss% 0.037950402808521466
plot_id,batch_id 0 80 miss% 0.04094828746371385
plot_id,batch_id 0 81 miss% 0.03522328913263033
plot_id,batch_id 0 82 miss% 0.020999398208307925
plot_id,batch_id 0 83 miss% 0.02960348343978362
plot_id,batch_id 0 84 miss% 0.030151369000825105
plot_id,batch_id 0 85 miss% 0.04180116879862985
plot_id,batch_id 0 86 miss% 0.04542173074353374
plot_id,batch_id 0 87 miss% 0.030570869495250723
plot_id,batch_id 0 88 miss% 0.026106906574497483
plot_id,batch_id 0 89 miss% 0.02833407746469342
plot_id,batch_id 0 90 miss% 0.05764007124309998
plot_id,batch_id 0 91 miss% 0.04942483620589372
plot_id,batch_id 0 92 miss% 0.03606223980240453
plot_id,batch_id 0 93 miss% 0.02635586556183577
plot_id,batch_id 0 94 miss% 0.02949344833585824
plot_id,batch_id 0 95 miss% 0.03959169199780799
plot_id,batch_id 0 96 miss% 0.03881831845234148
plot_id,batch_id 0 97 miss% 0.056010087530901295
plot_id,batch_id 0 98 miss% 0.024332205199194247
plot_id,batch_id 0 99 miss% 0.04115315554939074
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03141337 0.01902269 0.02641516 0.02768644 0.02245261 0.04624203
 0.02189961 0.025494   0.02055308 0.02856252 0.03844913 0.0350196
 0.02786231 0.02924916 0.02933959 0.03111175 0.0354821  0.03972051
 0.02758535 0.03077178 0.0396013  0.03276924 0.02619191 0.02344162
 0.01534903 0.03549625 0.0294327  0.02009904 0.01945914 0.02275445
 0.04637867 0.0296668  0.03064495 0.01510712 0.01451367 0.0362406
 0.04075286 0.0222975  0.02368747 0.02833352 0.0657869  0.01172272
 0.02236547 0.01518038 0.02431731 0.0272144  0.02619882 0.01666412
 0.01989934 0.02531076 0.04153543 0.02673099 0.0262964  0.01462096
 0.03032108 0.0398619  0.01880502 0.02911377 0.02484448 0.02756483
 0.04166119 0.04570453 0.02975991 0.02999582 0.03086228 0.05158374
 0.05537443 0.01997993 0.0192807  0.03269064 0.04431971 0.03619106
 0.04162571 0.02663051 0.03339006 0.02968148 0.02916043 0.04680981
 0.03519873 0.0379504  0.04094829 0.03522329 0.0209994  0.02960348
 0.03015137 0.04180117 0.04542173 0.03057087 0.02610691 0.02833408
 0.05764007 0.04942484 0.03606224 0.02635587 0.02949345 0.03959169
 0.03881832 0.05601009 0.02433221 0.04115316]
for model  152 the mean error 0.03126799253960195
all id 152 hidden_dim 32 learning_rate 0.02 num_layers 4 frames 25 out win 5 err 0.03126799253960195
Launcher: Job 153 completed in 10567 seconds.
Launcher: Task 159 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 30
(2400, 20, 8)
maximum abs change 0.1253197193145752
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.008690576984628932
number of weird sim 0
[]
renaissance 3
renaissance points (array([ 960, 1100, 1860]), array([0, 0, 0]), array([2, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0005835583433508873
all the summation of grain fractions are 1 3.7569947153315297e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 21 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 38400
total number of trained parameters  134801
Epoch:0, Train loss:0.637387, valid loss:0.638600
Epoch:1, Train loss:0.529712, valid loss:0.524350
Epoch:2, Train loss:0.506065, valid loss:0.520143
Epoch:3, Train loss:0.501041, valid loss:0.519179
Epoch:4, Train loss:0.498577, valid loss:0.518589
Epoch:5, Train loss:0.497531, valid loss:0.518047
Epoch:6, Train loss:0.496665, valid loss:0.521068
Epoch:7, Train loss:0.496338, valid loss:0.517788
Epoch:8, Train loss:0.496269, valid loss:0.517378
Epoch:9, Train loss:0.495329, valid loss:0.516810
Epoch:10, Train loss:0.495245, valid loss:0.516697
Epoch:11, Train loss:0.493558, valid loss:0.516449
Epoch:12, Train loss:0.493290, valid loss:0.516173
Epoch:13, Train loss:0.493321, valid loss:0.515918
Epoch:14, Train loss:0.493411, valid loss:0.516079
Epoch:15, Train loss:0.493194, valid loss:0.515890
Epoch:16, Train loss:0.493394, valid loss:0.516037
Epoch:17, Train loss:0.493005, valid loss:0.515534
Epoch:18, Train loss:0.493145, valid loss:0.515914
Epoch:19, Train loss:0.493018, valid loss:0.515650
Epoch:20, Train loss:0.493154, valid loss:0.515979
Epoch:21, Train loss:0.492144, valid loss:0.515432
Epoch:22, Train loss:0.492128, valid loss:0.515673
Epoch:23, Train loss:0.492122, valid loss:0.515340
Epoch:24, Train loss:0.491995, valid loss:0.515405
Epoch:25, Train loss:0.492143, valid loss:0.515498
Epoch:26, Train loss:0.492002, valid loss:0.515628
Epoch:27, Train loss:0.492065, valid loss:0.515671
Epoch:28, Train loss:0.492093, valid loss:0.515380
Epoch:29, Train loss:0.491993, valid loss:0.515541
Epoch:30, Train loss:0.491927, valid loss:0.515325
Epoch:31, Train loss:0.491520, valid loss:0.515330
Epoch:32, Train loss:0.491549, valid loss:0.515414
Epoch:33, Train loss:0.491501, valid loss:0.515185
Epoch:34, Train loss:0.491514, valid loss:0.515285
Epoch:35, Train loss:0.491510, valid loss:0.515248
Epoch:36, Train loss:0.491510, valid loss:0.515226
Epoch:37, Train loss:0.491498, valid loss:0.515332
Epoch:38, Train loss:0.491468, valid loss:0.515149
Epoch:39, Train loss:0.491446, valid loss:0.515265
Epoch:40, Train loss:0.491439, valid loss:0.515170
Epoch:41, Train loss:0.491222, valid loss:0.515122
Epoch:42, Train loss:0.491219, valid loss:0.515106
Epoch:43, Train loss:0.491212, valid loss:0.515153
Epoch:44, Train loss:0.491236, valid loss:0.515131
Epoch:45, Train loss:0.491227, valid loss:0.515092
Epoch:46, Train loss:0.491210, valid loss:0.515138
Epoch:47, Train loss:0.491190, valid loss:0.515157
Epoch:48, Train loss:0.491202, valid loss:0.515251
Epoch:49, Train loss:0.491203, valid loss:0.515116
Epoch:50, Train loss:0.491194, valid loss:0.515142
Epoch:51, Train loss:0.491096, valid loss:0.515055
Epoch:52, Train loss:0.491079, valid loss:0.515069
Epoch:53, Train loss:0.491079, valid loss:0.515081
Epoch:54, Train loss:0.491086, valid loss:0.515035
Epoch:55, Train loss:0.491083, valid loss:0.515106
Epoch:56, Train loss:0.491082, valid loss:0.515054
Epoch:57, Train loss:0.491070, valid loss:0.515065
Epoch:58, Train loss:0.491101, valid loss:0.515066
Epoch:59, Train loss:0.491055, valid loss:0.515055
Epoch:60, Train loss:0.491049, valid loss:0.515031
training time 10416.018778800964
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.05
plot_id,batch_id 0 0 miss% 0.8179036911455894
plot_id,batch_id 0 1 miss% 0.8543281592518596
plot_id,batch_id 0 2 miss% 0.8610304033401938
plot_id,batch_id 0 3 miss% 0.8646713144552971
plot_id,batch_id 0 4 miss% 0.8679009734996951
plot_id,batch_id 0 5 miss% 0.8147347872247916
plot_id,batch_id 0 6 miss% 0.8518515284092053
plot_id,batch_id 0 7 miss% 0.8624608124746154
plot_id,batch_id 0 8 miss% 0.8630606716171153
plot_id,batch_id 0 9 miss% 0.8660830187023361
plot_id,batch_id 0 10 miss% 0.805103060044613
plot_id,batch_id 0 11 miss% 0.8498329617264
plot_id,batch_id 0 12 miss% 0.8602396658905359
plot_id,batch_id 0 13 miss% 0.8614727048042552
plot_id,batch_id 0 14 miss% 0.8643825388383659
plot_id,batch_id 0 15 miss% 0.8055095522237545
plot_id,batch_id 0 16 miss% 0.8468816661896822
plot_id,batch_id 0 17 miss% 0.8574516301480546
plot_id,batch_id 0 18 miss% 0.8615296809984203
plot_id,batch_id 0 19 miss% 0.8637543979260254
plot_id,batch_id 0 20 miss% 0.8421654985213483
plot_id,batch_id 0 21 miss% 0.8620834554609553
plot_id,batch_id 0 22 miss% 0.8647122555886646
plot_id,batch_id 0 23 miss% 0.866039933847634
plot_id,batch_id 0 24 miss% 0.8677808749461112
plot_id,batch_id 0 25 miss% 0.837656391899297
plot_id,batch_id 0 26 miss% 0.8576110913994514
plot_id,batch_id 0 27 miss% 0.8644411057162319
plot_id,batch_id 0 28 miss% 0.8663233790608361
plot_id,batch_id 0 29 miss% 0.8671168006365632
plot_id,batch_id 0 30 miss% 0.8260140220852332
plot_id,batch_id 0 31 miss% 0.8574591063815121
plot_id,batch_id 0 32 miss% 0.8638553404595711
plot_id,batch_id 0 33 miss% 0.8639718593933308
plot_id,batch_id 0 34 miss% 0.8640876399399999
plot_id,batch_id 0 35 miss% 0.8235902275928161
plot_id,batch_id 0 36 miss% 0.8580946368418882
plot_id,batch_id 0 37 miss% 0.8617630090176922
plot_id,batch_id 0 38 miss% 0.8662538958121188
plot_id,batch_id 0 39 miss% 0.868468465230014
plot_id,batch_id 0 40 miss% 0.8501008656422179
plot_id,batch_id 0 41 miss% 0.861939802849984
plot_id,batch_id 0 42 miss% 0.8642403039473584
plot_id,batch_id 0 43 miss% 0.8671183621195072
plot_id,batch_id 0 44 miss% 0.8679794042537206
plot_id,batch_id 0 45 miss% 0.8457435789864635
plot_id,batch_id 0 46 miss% 0.8616456606135913
plot_id,batch_id 0 47 miss% 0.865867567989661
plot_id,batch_id 0 48 miss% 0.8688289567919567
plot_id,batch_id 0 49 miss% 0.8675849584640999
plot_id,batch_id 0 50 miss% 0.8499073098386636
plot_id,batch_id 0 51 miss% 0.8622350316750683
plot_id,batch_id 0 52 miss% 0.8656273664327689
plot_id,batch_id 0 53 miss% 0.8677297651659182
plot_id,batch_id 0 54 miss% 0.8690438969942962
plot_id,batch_id 0 55 miss% 0.8481875026501378
plot_id,batch_id 0 56 miss% 0.8631000297488965
plot_id,batch_id 0 57 miss% 0.865768008474939
plot_id,batch_id 0 58 miss% 0.8687437128825518
plot_id,batch_id 0 59 miss% 0.8673137093654478
plot_id,batch_id 0 60 miss% 0.7709624436703042
plot_id,batch_id 0 61 miss% 0.8419053711800512
plot_id,batch_id 0 62 miss% 0.8541680899338372
plot_id,batch_id 0 63 miss% 0.857692670387725
plot_id,batch_id 0 64 miss% 0.8617374130496661
plot_id,batch_id 0 65 miss% 0.7569930626356566
plot_id,batch_id 0 66 miss% 0.8346729946471209
plot_id,batch_id 0 67 miss% 0.8422873465884864
plot_id,batch_id 0 68 miss% 0.8562978399613134
plot_id,batch_id 0 69 miss% 0.8568662433008929
plot_id,batch_id 0 70 miss% 0.7229472831270319
plot_id,batch_id 0 71 miss% 0.8324349583304005
plot_id,batch_id 0 72 miss% 0.8364769182840003
plot_id,batch_id 0 73 miss% 0.8477062869060019
plot_id,batch_id 0 74 miss% 0.8542593483688149
plot_id,batch_id 0 75 miss% 0.755483890819359
plot_id,batch_id 0 76 miss% 0.8184685851461101
plot_id,batch_id 0 77 miss% 0.8298571911536166
plot_id,batch_id 0 78 miss% 0.8411067010434415
plot_id,batch_id 0 79 miss% 0.8512307258466588
plot_id,batch_id 0 80 miss% 0.7885851563833846
plot_id,batch_id 0 81 miss% 0.848049542158269
plot_id,batch_id 0 82 miss% 0.8585204724052437
plot_id,batch_id 0 83 miss% 0.8590575643697481
plot_id,batch_id 0 84 miss% 0.8617078705853366
plot_id,batch_id 0 85 miss% 0.7766960691044325
plot_id,batch_id 0 86 miss% 0.840932913139767
plot_id,batch_id 0 87 miss% 0.8508761490967395
plot_id,batch_id 0 88 miss% 0.8606937310749434
plot_id,batch_id 0 89 miss% 0.8614602384434551
plot_id,batch_id 0 90 miss% 0.7544807520987526
plot_id,batch_id 0 91 miss% 0.8345934699794642
plot_id,batch_id 0 92 miss% 0.8468387139450053
plot_id,batch_id 0 93 miss% 0.8515734161617329
plot_id,batch_id 0 94 miss% 0.8625171305512046
plot_id,batch_id 0 95 miss% 0.7636307826978936
plot_id,batch_id 0 96 miss% 0.8273641698147265
plot_id,batch_id 0 97 miss% 0.8447401020606039
plot_id,batch_id 0 98 miss% 0.8512981218229557
plot_id,batch_id 0 99 miss% 0.8546915078867007
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.81790369 0.85432816 0.8610304  0.86467131 0.86790097 0.81473479
 0.85185153 0.86246081 0.86306067 0.86608302 0.80510306 0.84983296
 0.86023967 0.8614727  0.86438254 0.80550955 0.84688167 0.85745163
 0.86152968 0.8637544  0.8421655  0.86208346 0.86471226 0.86603993
 0.86778087 0.83765639 0.85761109 0.86444111 0.86632338 0.8671168
 0.82601402 0.85745911 0.86385534 0.86397186 0.86408764 0.82359023
 0.85809464 0.86176301 0.8662539  0.86846847 0.85010087 0.8619398
 0.8642403  0.86711836 0.8679794  0.84574358 0.86164566 0.86586757
 0.86882896 0.86758496 0.84990731 0.86223503 0.86562737 0.86772977
 0.8690439  0.8481875  0.86310003 0.86576801 0.86874371 0.86731371
 0.77096244 0.84190537 0.85416809 0.85769267 0.86173741 0.75699306
 0.83467299 0.84228735 0.85629784 0.85686624 0.72294728 0.83243496
 0.83647692 0.84770629 0.85425935 0.75548389 0.81846859 0.82985719
 0.8411067  0.85123073 0.78858516 0.84804954 0.85852047 0.85905756
 0.86170787 0.77669607 0.84093291 0.85087615 0.86069373 0.86146024
 0.75448075 0.83459347 0.84683871 0.85157342 0.86251713 0.76363078
 0.82736417 0.8447401  0.85129812 0.85469151]
for model  80 the mean error 0.8462024124179015
all id 80 hidden_dim 32 learning_rate 0.02 num_layers 5 frames 21 out win 5 err 0.8462024124179015
Launcher: Job 81 completed in 10595 seconds.
Launcher: Task 243 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  134801
Epoch:0, Train loss:0.486496, valid loss:0.489493
Epoch:1, Train loss:0.355245, valid loss:0.359844
Epoch:2, Train loss:0.344281, valid loss:0.358962
Epoch:3, Train loss:0.342597, valid loss:0.358404
Epoch:4, Train loss:0.341720, valid loss:0.358022
Epoch:5, Train loss:0.341231, valid loss:0.358046
Epoch:6, Train loss:0.340935, valid loss:0.357549
Epoch:7, Train loss:0.340617, valid loss:0.357698
Epoch:8, Train loss:0.340457, valid loss:0.357705
Epoch:9, Train loss:0.340407, valid loss:0.358172
Epoch:10, Train loss:0.340191, valid loss:0.357397
Epoch:11, Train loss:0.339513, valid loss:0.356958
Epoch:12, Train loss:0.339442, valid loss:0.357036
Epoch:13, Train loss:0.339425, valid loss:0.357158
Epoch:14, Train loss:0.339348, valid loss:0.357036
Epoch:15, Train loss:0.339374, valid loss:0.356926
Epoch:16, Train loss:0.339341, valid loss:0.356927
Epoch:17, Train loss:0.339302, valid loss:0.357050
Epoch:18, Train loss:0.339258, valid loss:0.357136
Epoch:19, Train loss:0.339239, valid loss:0.357066
Epoch:20, Train loss:0.339240, valid loss:0.356841
Epoch:21, Train loss:0.338819, valid loss:0.356761
Epoch:22, Train loss:0.338874, valid loss:0.356782
Epoch:23, Train loss:0.338819, valid loss:0.356876
Epoch:24, Train loss:0.338789, valid loss:0.356779
Epoch:25, Train loss:0.338792, valid loss:0.356761
Epoch:26, Train loss:0.338813, valid loss:0.356722
Epoch:27, Train loss:0.338770, valid loss:0.356792
Epoch:28, Train loss:0.338735, valid loss:0.356875
Epoch:29, Train loss:0.338757, valid loss:0.356907
Epoch:30, Train loss:0.338784, valid loss:0.356777
Epoch:31, Train loss:0.338556, valid loss:0.356719
Epoch:32, Train loss:0.338574, valid loss:0.356692
Epoch:33, Train loss:0.338564, valid loss:0.356680
Epoch:34, Train loss:0.338560, valid loss:0.356697
Epoch:35, Train loss:0.338534, valid loss:0.356685
Epoch:36, Train loss:0.338539, valid loss:0.356697
Epoch:37, Train loss:0.338518, valid loss:0.356740
Epoch:38, Train loss:0.338549, valid loss:0.356686
Epoch:39, Train loss:0.338529, valid loss:0.356666
Epoch:40, Train loss:0.338502, valid loss:0.356658
Epoch:41, Train loss:0.338443, valid loss:0.356632
Epoch:42, Train loss:0.338437, valid loss:0.356625
Epoch:43, Train loss:0.338433, valid loss:0.356619
Epoch:44, Train loss:0.338430, valid loss:0.356687
Epoch:45, Train loss:0.338431, valid loss:0.356609
Epoch:46, Train loss:0.338437, valid loss:0.356629
Epoch:47, Train loss:0.338427, valid loss:0.356654
Epoch:48, Train loss:0.338417, valid loss:0.356636
Epoch:49, Train loss:0.338418, valid loss:0.356600
Epoch:50, Train loss:0.338413, valid loss:0.356626
Epoch:51, Train loss:0.338381, valid loss:0.356633
Epoch:52, Train loss:0.338379, valid loss:0.356602
Epoch:53, Train loss:0.338377, valid loss:0.356610
Epoch:54, Train loss:0.338378, valid loss:0.356624
Epoch:55, Train loss:0.338378, valid loss:0.356604
Epoch:56, Train loss:0.338370, valid loss:0.356622
Epoch:57, Train loss:0.338378, valid loss:0.356625
Epoch:58, Train loss:0.338372, valid loss:0.356619
Epoch:59, Train loss:0.338371, valid loss:0.356628
Epoch:60, Train loss:0.338374, valid loss:0.356606
training time 10553.426487207413
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7312213255974054
plot_id,batch_id 0 1 miss% 0.7886894729647642
plot_id,batch_id 0 2 miss% 0.8010946722274892
plot_id,batch_id 0 3 miss% 0.8087496381377376
plot_id,batch_id 0 4 miss% 0.8078577580140016
plot_id,batch_id 0 5 miss% 0.7249857930830397
plot_id,batch_id 0 6 miss% 0.7842715374885945
plot_id,batch_id 0 7 miss% 0.7979780220948661
plot_id,batch_id 0 8 miss% 0.8056866567233829
plot_id,batch_id 0 9 miss% 0.809796162146417
plot_id,batch_id 0 10 miss% 0.7014053232627361
plot_id,batch_id 0 11 miss% 0.7818454205382216
plot_id,batch_id 0 12 miss% 0.7939725383051902
plot_id,batch_id 0 13 miss% 0.8020784953967812
plot_id,batch_id 0 14 miss% 0.80854810132756
plot_id,batch_id 0 15 miss% 0.7141458336090213
plot_id,batch_id 0 16 miss% 0.7790315419287326
plot_id,batch_id 0 17 miss% 0.797799201103539
plot_id,batch_id 0 18 miss% 0.8044617947394055
plot_id,batch_id 0 19 miss% 0.8049078188793747
plot_id,batch_id 0 20 miss% 0.763473245667056
plot_id,batch_id 0 21 miss% 0.8051724018427383
plot_id,batch_id 0 22 miss% 0.807633915818404
plot_id,batch_id 0 23 miss% 0.8135396657473015
plot_id,batch_id 0 24 miss% 0.8137516891367069
plot_id,batch_id 0 25 miss% 0.7534005283664738
plot_id,batch_id 0 26 miss% 0.7970179094111336
plot_id,batch_id 0 27 miss% 0.802961117587667
plot_id,batch_id 0 28 miss% 0.8066206020061021
plot_id,batch_id 0 29 miss% 0.8083757106171467
plot_id,batch_id 0 30 miss% 0.7507438632398338
plot_id,batch_id 0 31 miss% 0.7952772694001253
plot_id,batch_id 0 32 miss% 0.8029943745900603
plot_id,batch_id 0 33 miss% 0.8071020118905504
plot_id,batch_id 0 34 miss% 0.808044602675218
plot_id,batch_id 0 35 miss% 0.7498555049931327
plot_id,batch_id 0 36 miss% 0.799501812233177
plot_id,batch_id 0 37 miss% 0.8029790778198017
plot_id,batch_id 0 38 miss% 0.8087529681653026
plot_id,batch_id 0 39 miss% 0.8108901272934284
plot_id,batch_id 0 40 miss% 0.7794634557709497
plot_id,batch_id 0 41 miss% 0.8053557991340028
plot_id,batch_id 0 42 miss% 0.8115342332049347
plot_id,batch_id 0 43 miss% 0.8135031318197304
plot_id,batch_id 0 44 miss% 0.8171414315001748
plot_id,batch_id 0 45 miss% 0.7761133613090181
plot_id,batch_id 0 46 miss% 0.8054516959284328
plot_id,batch_id 0 47 miss% 0.810682693170725
plot_id,batch_id 0 48 miss% 0.8117108695985421
plot_id,batch_id 0 49 miss% 0.8166288769500678
plot_id,batch_id 0 50 miss% 0.7835133895078232
plot_id,batch_id 0 51 miss% 0.8037941834386089
plot_id,batch_id 0 52 miss% 0.8080668434006785
plot_id,batch_id 0 53 miss% 0.8111327975986905
plot_id,batch_id 0 54 miss% 0.8178231402995162
plot_id,batch_id 0 55 miss% 0.7614995240854682
plot_id,batch_id 0 56 miss% 0.8038118982423529
plot_id,batch_id 0 57 miss% 0.8084316994495241
plot_id,batch_id 0 58 miss% 0.812409241761013
plot_id,batch_id 0 59 miss% 0.8159490930337543
plot_id,batch_id 0 60 miss% 0.6435663179513963
plot_id,batch_id 0 61 miss% 0.7483245095578498
plot_id,batch_id 0 62 miss% 0.7854393941867828
plot_id,batch_id 0 63 miss% 0.7935895276731266
plot_id,batch_id 0 64 miss% 0.8027586266781259
plot_id,batch_id 0 65 miss% 0.6394066536527532
plot_id,batch_id 0 66 miss% 0.74849697997072
plot_id,batch_id 0 67 miss% 0.7709927665698508
plot_id,batch_id 0 68 miss% 0.7964458395840732
plot_id,batch_id 0 69 miss% 0.7937829631925135
plot_id,batch_id 0 70 miss% 0.6136179917130324
plot_id,batch_id 0 71 miss% 0.7648823741903129
plot_id,batch_id 0 72 miss% 0.759966512988702
plot_id,batch_id 0 73 miss% 0.7785897722815879
plot_id,batch_id 0 74 miss% 0.7878660413329499
plot_id,batch_id 0 75 miss% 0.6080914653163767
plot_id,batch_id 0 76 miss% 0.7093309376703251
plot_id,batch_id 0 77 miss% 0.7518293116955113
plot_id,batch_id 0 78 miss% 0.7795693727938652
plot_id,batch_id 0 79 miss% 0.7851273375526517
plot_id,batch_id 0 80 miss% 0.6762799627389113
plot_id,batch_id 0 81 miss% 0.7743365103725754
plot_id,batch_id 0 82 miss% 0.7921575431724666
plot_id,batch_id 0 83 miss% 0.799466768489574
plot_id,batch_id 0 84 miss% 0.7998419358835088
plot_id,batch_id 0 85 miss% 0.6717095495040751
plot_id,batch_id 0 86 miss% 0.7683322397370959
plot_id,batch_id 0 87 miss% 0.790449866657684
plot_id,batch_id 0 88 miss% 0.7982597156356974
plot_id,batch_id 0 89 miss% 0.7987802121774658
plot_id,batch_id 0 90 miss% 0.6466454084108266
plot_id,batch_id 0 91 miss% 0.7713906447782336
plot_id,batch_id 0 92 miss% 0.7762266054577338
plot_id,batch_id 0 93 miss% 0.7890551606109849
plot_id,batch_id 0 94 miss% 0.7993155779496273
plot_id,batch_id 0 95 miss% 0.6449911168458211
plot_id,batch_id 0 96 miss% 0.746648316614718
plot_id,batch_id 0 97 miss% 0.7771773492033498
plot_id,batch_id 0 98 miss% 0.7855785078063726
plot_id,batch_id 0 99 miss% 0.7940723760653989
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73122133 0.78868947 0.80109467 0.80874964 0.80785776 0.72498579
 0.78427154 0.79797802 0.80568666 0.80979616 0.70140532 0.78184542
 0.79397254 0.8020785  0.8085481  0.71414583 0.77903154 0.7977992
 0.80446179 0.80490782 0.76347325 0.8051724  0.80763392 0.81353967
 0.81375169 0.75340053 0.79701791 0.80296112 0.8066206  0.80837571
 0.75074386 0.79527727 0.80299437 0.80710201 0.8080446  0.7498555
 0.79950181 0.80297908 0.80875297 0.81089013 0.77946346 0.8053558
 0.81153423 0.81350313 0.81714143 0.77611336 0.8054517  0.81068269
 0.81171087 0.81662888 0.78351339 0.80379418 0.80806684 0.8111328
 0.81782314 0.76149952 0.8038119  0.8084317  0.81240924 0.81594909
 0.64356632 0.74832451 0.78543939 0.79358953 0.80275863 0.63940665
 0.74849698 0.77099277 0.79644584 0.79378296 0.61361799 0.76488237
 0.75996651 0.77858977 0.78786604 0.60809147 0.70933094 0.75182931
 0.77956937 0.78512734 0.67627996 0.77433651 0.79215754 0.79946677
 0.79984194 0.67170955 0.76833224 0.79044987 0.79825972 0.79878021
 0.64664541 0.77139064 0.77622661 0.78905516 0.79931558 0.64499112
 0.74664832 0.77717735 0.78557851 0.79407238]
for model  106 the mean error 0.7769302292993827
all id 106 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 25 out win 4 err 0.7769302292993827
Launcher: Job 107 completed in 10733 seconds.
Launcher: Task 247 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  134801
Epoch:0, Train loss:0.366169, valid loss:0.370888
Epoch:1, Train loss:0.226313, valid loss:0.229360
Epoch:2, Train loss:0.217751, valid loss:0.228429
Epoch:3, Train loss:0.216709, valid loss:0.228098
Epoch:4, Train loss:0.216314, valid loss:0.228054
Epoch:5, Train loss:0.216133, valid loss:0.228142
Epoch:6, Train loss:0.216007, valid loss:0.227960
Epoch:7, Train loss:0.215910, valid loss:0.227760
Epoch:8, Train loss:0.215783, valid loss:0.227747
Epoch:9, Train loss:0.215717, valid loss:0.227686
Epoch:10, Train loss:0.215647, valid loss:0.228042
Epoch:11, Train loss:0.215210, valid loss:0.227435
Epoch:12, Train loss:0.215211, valid loss:0.227414
Epoch:13, Train loss:0.215207, valid loss:0.227531
Epoch:14, Train loss:0.215147, valid loss:0.227464
Epoch:15, Train loss:0.215163, valid loss:0.227421
Epoch:16, Train loss:0.215130, valid loss:0.227543
Epoch:17, Train loss:0.215108, valid loss:0.227438
Epoch:18, Train loss:0.215093, valid loss:0.227553
Epoch:19, Train loss:0.215148, valid loss:0.227521
Epoch:20, Train loss:0.215043, valid loss:0.227533
Epoch:21, Train loss:0.214880, valid loss:0.227355
Epoch:22, Train loss:0.214875, valid loss:0.227348
Epoch:23, Train loss:0.214843, valid loss:0.227379
Epoch:24, Train loss:0.214841, valid loss:0.227357
Epoch:25, Train loss:0.214834, valid loss:0.227373
Epoch:26, Train loss:0.214843, valid loss:0.227337
Epoch:27, Train loss:0.214854, valid loss:0.227377
Epoch:28, Train loss:0.214825, valid loss:0.227496
Epoch:29, Train loss:0.214822, valid loss:0.227305
Epoch:30, Train loss:0.214798, valid loss:0.227370
Epoch:31, Train loss:0.214727, valid loss:0.227358
Epoch:32, Train loss:0.214714, valid loss:0.227326
Epoch:33, Train loss:0.214706, valid loss:0.227312
Epoch:34, Train loss:0.214708, valid loss:0.227364
Epoch:35, Train loss:0.214710, valid loss:0.227339
Epoch:36, Train loss:0.214700, valid loss:0.227313
Epoch:37, Train loss:0.214706, valid loss:0.227389
Epoch:38, Train loss:0.214704, valid loss:0.227307
Epoch:39, Train loss:0.214691, valid loss:0.227342
Epoch:40, Train loss:0.214696, valid loss:0.227327
Epoch:41, Train loss:0.214647, valid loss:0.227286
Epoch:42, Train loss:0.214644, valid loss:0.227280
Epoch:43, Train loss:0.214645, valid loss:0.227280
Epoch:44, Train loss:0.214643, valid loss:0.227280
Epoch:45, Train loss:0.214641, valid loss:0.227281
Epoch:46, Train loss:0.214639, valid loss:0.227287
Epoch:47, Train loss:0.214639, valid loss:0.227293
Epoch:48, Train loss:0.214638, valid loss:0.227296
Epoch:49, Train loss:0.214638, valid loss:0.227274
Epoch:50, Train loss:0.214639, valid loss:0.227276
Epoch:51, Train loss:0.214616, valid loss:0.227272
Epoch:52, Train loss:0.214613, valid loss:0.227272
Epoch:53, Train loss:0.214614, valid loss:0.227275
Epoch:54, Train loss:0.214613, valid loss:0.227272
Epoch:55, Train loss:0.214613, valid loss:0.227270
Epoch:56, Train loss:0.214611, valid loss:0.227283
Epoch:57, Train loss:0.214611, valid loss:0.227274
Epoch:58, Train loss:0.214609, valid loss:0.227277
Epoch:59, Train loss:0.214610, valid loss:0.227264
Epoch:60, Train loss:0.214609, valid loss:0.227269
training time 10599.798956155777
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.5479368405081486
plot_id,batch_id 0 1 miss% 0.6779068278446068
plot_id,batch_id 0 2 miss% 0.6962688009265955
plot_id,batch_id 0 3 miss% 0.7079181002916743
plot_id,batch_id 0 4 miss% 0.7079258853981648
plot_id,batch_id 0 5 miss% 0.5521546857844435
plot_id,batch_id 0 6 miss% 0.671545730709574
plot_id,batch_id 0 7 miss% 0.6982642356837859
plot_id,batch_id 0 8 miss% 0.7015948885617964
plot_id,batch_id 0 9 miss% 0.7109906655134502
plot_id,batch_id 0 10 miss% 0.5241653334739403
plot_id,batch_id 0 11 miss% 0.6637302116558566
plot_id,batch_id 0 12 miss% 0.6872405118269145
plot_id,batch_id 0 13 miss% 0.6996569334276816
plot_id,batch_id 0 14 miss% 0.7104546173862306
plot_id,batch_id 0 15 miss% 0.5255658837190287
plot_id,batch_id 0 16 miss% 0.6678015071023571
plot_id,batch_id 0 17 miss% 0.6961478225164163
plot_id,batch_id 0 18 miss% 0.7032393708400813
plot_id,batch_id 0 19 miss% 0.7039376740926172
plot_id,batch_id 0 20 miss% 0.6184230215991618
plot_id,batch_id 0 21 miss% 0.6965997812248007
plot_id,batch_id 0 22 miss% 0.7087613541032668
plot_id,batch_id 0 23 miss% 0.718277511998197
plot_id,batch_id 0 24 miss% 0.7210186727927158
plot_id,batch_id 0 25 miss% 0.6094657964832166
plot_id,batch_id 0 26 miss% 0.6935188272352267
plot_id,batch_id 0 27 miss% 0.7096271952506961
plot_id,batch_id 0 28 miss% 0.7171804085805465
plot_id,batch_id 0 29 miss% 0.721729108659396
plot_id,batch_id 0 30 miss% 0.6086637834079429
plot_id,batch_id 0 31 miss% 0.687177772824371
plot_id,batch_id 0 32 miss% 0.7029815833422153
plot_id,batch_id 0 33 miss% 0.7101028253665763
plot_id,batch_id 0 34 miss% 0.7118514464248277
plot_id,batch_id 0 35 miss% 0.5922688490908676
plot_id,batch_id 0 36 miss% 0.6905614585323028
plot_id,batch_id 0 37 miss% 0.7014486918548565
plot_id,batch_id 0 38 miss% 0.7120314917683338
plot_id,batch_id 0 39 miss% 0.714691564456074
plot_id,batch_id 0 40 miss% 0.6547079398647127
plot_id,batch_id 0 41 miss% 0.709312427944718
plot_id,batch_id 0 42 miss% 0.7129868302151684
plot_id,batch_id 0 43 miss% 0.7225959891431339
plot_id,batch_id 0 44 miss% 0.726709120050875
plot_id,batch_id 0 45 miss% 0.654393887965606
plot_id,batch_id 0 46 miss% 0.7072201883652826
plot_id,batch_id 0 47 miss% 0.7140006245661602
plot_id,batch_id 0 48 miss% 0.720434515525633
plot_id,batch_id 0 49 miss% 0.7277878827188601
plot_id,batch_id 0 50 miss% 0.6676818804252292
plot_id,batch_id 0 51 miss% 0.7059882860332191
plot_id,batch_id 0 52 miss% 0.714247251469562
plot_id,batch_id 0 53 miss% 0.7180285992272211
plot_id,batch_id 0 54 miss% 0.730217148542174
plot_id,batch_id 0 55 miss% 0.6711720342754165
plot_id,batch_id 0 56 miss% 0.7047752500476484
plot_id,batch_id 0 57 miss% 0.7138246579425019
plot_id,batch_id 0 58 miss% 0.7221721378320518
plot_id,batch_id 0 59 miss% 0.7196196072161867
plot_id,batch_id 0 60 miss% 0.4460890057877564
plot_id,batch_id 0 61 miss% 0.6157342512670569
plot_id,batch_id 0 62 miss% 0.6634279679202681
plot_id,batch_id 0 63 miss% 0.683376006229189
plot_id,batch_id 0 64 miss% 0.6885812649980013
plot_id,batch_id 0 65 miss% 0.43580985335356015
plot_id,batch_id 0 66 miss% 0.6012316236137243
plot_id,batch_id 0 67 miss% 0.6432674365628462
plot_id,batch_id 0 68 miss% 0.6830613492810855
plot_id,batch_id 0 69 miss% 0.6880264153195774
plot_id,batch_id 0 70 miss% 0.411303415514704
plot_id,batch_id 0 71 miss% 0.5928730930991348
plot_id,batch_id 0 72 miss% 0.6301634415911267
plot_id,batch_id 0 73 miss% 0.6616030246605914
plot_id,batch_id 0 74 miss% 0.6825365326527053
plot_id,batch_id 0 75 miss% 0.3882502384899001
plot_id,batch_id 0 76 miss% 0.5664151428241984
plot_id,batch_id 0 77 miss% 0.6208101466102584
plot_id,batch_id 0 78 miss% 0.6603113890044908
plot_id,batch_id 0 79 miss% 0.6736340610676491
plot_id,batch_id 0 80 miss% 0.46817474684688437
plot_id,batch_id 0 81 miss% 0.6392092141361476
plot_id,batch_id 0 82 miss% 0.6757979749527869
plot_id,batch_id 0 83 miss% 0.6930608941676777
plot_id,batch_id 0 84 miss% 0.6974492793838609
plot_id,batch_id 0 85 miss% 0.4650614497976343
plot_id,batch_id 0 86 miss% 0.6338964437564162
plot_id,batch_id 0 87 miss% 0.6666682504535936
plot_id,batch_id 0 88 miss% 0.6912773980901652
plot_id,batch_id 0 89 miss% 0.6987047983187091
plot_id,batch_id 0 90 miss% 0.4406711043765448
plot_id,batch_id 0 91 miss% 0.630202825959098
plot_id,batch_id 0 92 miss% 0.6630560819181414
plot_id,batch_id 0 93 miss% 0.6854593281677719
plot_id,batch_id 0 94 miss% 0.695400929069479
plot_id,batch_id 0 95 miss% 0.4314840900612525
plot_id,batch_id 0 96 miss% 0.6074619509452929
plot_id,batch_id 0 97 miss% 0.654120670983467
plot_id,batch_id 0 98 miss% 0.6752566223518667
plot_id,batch_id 0 99 miss% 0.684584041796276
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.54793684 0.67790683 0.6962688  0.7079181  0.70792589 0.55215469
 0.67154573 0.69826424 0.70159489 0.71099067 0.52416533 0.66373021
 0.68724051 0.69965693 0.71045462 0.52556588 0.66780151 0.69614782
 0.70323937 0.70393767 0.61842302 0.69659978 0.70876135 0.71827751
 0.72101867 0.6094658  0.69351883 0.7096272  0.71718041 0.72172911
 0.60866378 0.68717777 0.70298158 0.71010283 0.71185145 0.59226885
 0.69056146 0.70144869 0.71203149 0.71469156 0.65470794 0.70931243
 0.71298683 0.72259599 0.72670912 0.65439389 0.70722019 0.71400062
 0.72043452 0.72778788 0.66768188 0.70598829 0.71424725 0.7180286
 0.73021715 0.67117203 0.70477525 0.71382466 0.72217214 0.71961961
 0.44608901 0.61573425 0.66342797 0.68337601 0.68858126 0.43580985
 0.60123162 0.64326744 0.68306135 0.68802642 0.41130342 0.59287309
 0.63016344 0.66160302 0.68253653 0.38825024 0.56641514 0.62081015
 0.66031139 0.67363406 0.46817475 0.63920921 0.67579797 0.69306089
 0.69744928 0.46506145 0.63389644 0.66666825 0.6912774  0.6987048
 0.4406711  0.63020283 0.66305608 0.68545933 0.69540093 0.43148409
 0.60746195 0.65412067 0.67525662 0.68458404]
for model  186 the mean error 0.6565220768701121
all id 186 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 31 out win 3 err 0.6565220768701121
Launcher: Job 187 completed in 10775 seconds.
Launcher: Task 214 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  134801
Epoch:0, Train loss:0.486496, valid loss:0.489493
Epoch:1, Train loss:0.358336, valid loss:0.360520
Epoch:2, Train loss:0.344886, valid loss:0.358628
Epoch:3, Train loss:0.342527, valid loss:0.358283
Epoch:4, Train loss:0.341731, valid loss:0.357967
Epoch:5, Train loss:0.341362, valid loss:0.357647
Epoch:6, Train loss:0.341119, valid loss:0.357621
Epoch:7, Train loss:0.340889, valid loss:0.357484
Epoch:8, Train loss:0.340702, valid loss:0.357862
Epoch:9, Train loss:0.340745, valid loss:0.357352
Epoch:10, Train loss:0.340440, valid loss:0.357452
Epoch:11, Train loss:0.339596, valid loss:0.357031
Epoch:12, Train loss:0.339590, valid loss:0.357216
Epoch:13, Train loss:0.339559, valid loss:0.357170
Epoch:14, Train loss:0.339502, valid loss:0.357045
Epoch:15, Train loss:0.339508, valid loss:0.357177
Epoch:16, Train loss:0.339463, valid loss:0.357132
Epoch:17, Train loss:0.339414, valid loss:0.357589
Epoch:18, Train loss:0.339375, valid loss:0.357326
Epoch:19, Train loss:0.339382, valid loss:0.357012
Epoch:20, Train loss:0.339333, valid loss:0.357119
Epoch:21, Train loss:0.338911, valid loss:0.356757
Epoch:22, Train loss:0.338891, valid loss:0.356835
Epoch:23, Train loss:0.338915, valid loss:0.356833
Epoch:24, Train loss:0.338886, valid loss:0.356794
Epoch:25, Train loss:0.338880, valid loss:0.356765
Epoch:26, Train loss:0.338879, valid loss:0.356835
Epoch:27, Train loss:0.338856, valid loss:0.356804
Epoch:28, Train loss:0.338850, valid loss:0.356915
Epoch:29, Train loss:0.338802, valid loss:0.356757
Epoch:30, Train loss:0.338839, valid loss:0.356813
Epoch:31, Train loss:0.338591, valid loss:0.356718
Epoch:32, Train loss:0.338586, valid loss:0.356693
Epoch:33, Train loss:0.338588, valid loss:0.356705
Epoch:34, Train loss:0.338600, valid loss:0.356663
Epoch:35, Train loss:0.338611, valid loss:0.356734
Epoch:36, Train loss:0.338578, valid loss:0.356731
Epoch:37, Train loss:0.338598, valid loss:0.356706
Epoch:38, Train loss:0.338554, valid loss:0.356657
Epoch:39, Train loss:0.338575, valid loss:0.356729
Epoch:40, Train loss:0.338551, valid loss:0.356737
Epoch:41, Train loss:0.338455, valid loss:0.356636
Epoch:42, Train loss:0.338449, valid loss:0.356646
Epoch:43, Train loss:0.338453, valid loss:0.356651
Epoch:44, Train loss:0.338453, valid loss:0.356635
Epoch:45, Train loss:0.338439, valid loss:0.356619
Epoch:46, Train loss:0.338434, valid loss:0.356656
Epoch:47, Train loss:0.338440, valid loss:0.356657
Epoch:48, Train loss:0.338434, valid loss:0.356652
Epoch:49, Train loss:0.338428, valid loss:0.356731
Epoch:50, Train loss:0.338431, valid loss:0.356681
Epoch:51, Train loss:0.338400, valid loss:0.356653
Epoch:52, Train loss:0.338382, valid loss:0.356622
Epoch:53, Train loss:0.338381, valid loss:0.356645
Epoch:54, Train loss:0.338388, valid loss:0.356622
Epoch:55, Train loss:0.338383, valid loss:0.356651
Epoch:56, Train loss:0.338372, valid loss:0.356651
Epoch:57, Train loss:0.338374, valid loss:0.356648
Epoch:58, Train loss:0.338371, valid loss:0.356628
Epoch:59, Train loss:0.338377, valid loss:0.356622
Epoch:60, Train loss:0.338367, valid loss:0.356612
training time 10647.286382198334
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7304453513370194
plot_id,batch_id 0 1 miss% 0.78949626405924
plot_id,batch_id 0 2 miss% 0.8011839962854285
plot_id,batch_id 0 3 miss% 0.8082168576315885
plot_id,batch_id 0 4 miss% 0.8070396856721613
plot_id,batch_id 0 5 miss% 0.7239631499908253
plot_id,batch_id 0 6 miss% 0.7828812522510223
plot_id,batch_id 0 7 miss% 0.7985884067603727
plot_id,batch_id 0 8 miss% 0.8041009170216649
plot_id,batch_id 0 9 miss% 0.8088980911796917
plot_id,batch_id 0 10 miss% 0.7031044326936209
plot_id,batch_id 0 11 miss% 0.785674709546054
plot_id,batch_id 0 12 miss% 0.7948532966748182
plot_id,batch_id 0 13 miss% 0.8011755983569829
plot_id,batch_id 0 14 miss% 0.8081304397833493
plot_id,batch_id 0 15 miss% 0.7116719492867178
plot_id,batch_id 0 16 miss% 0.7734260498970947
plot_id,batch_id 0 17 miss% 0.7961259799775217
plot_id,batch_id 0 18 miss% 0.8033633455862924
plot_id,batch_id 0 19 miss% 0.8055526264813677
plot_id,batch_id 0 20 miss% 0.7625549990691192
plot_id,batch_id 0 21 miss% 0.8037509839320464
plot_id,batch_id 0 22 miss% 0.8076090558322591
plot_id,batch_id 0 23 miss% 0.8128958237487546
plot_id,batch_id 0 24 miss% 0.8143226516031381
plot_id,batch_id 0 25 miss% 0.7510371731670477
plot_id,batch_id 0 26 miss% 0.796702466179457
plot_id,batch_id 0 27 miss% 0.8043025884239385
plot_id,batch_id 0 28 miss% 0.8067732716927062
plot_id,batch_id 0 29 miss% 0.8084588299678668
plot_id,batch_id 0 30 miss% 0.7551679782808085
plot_id,batch_id 0 31 miss% 0.7960930445695855
plot_id,batch_id 0 32 miss% 0.8035528971632584
plot_id,batch_id 0 33 miss% 0.8070925649990098
plot_id,batch_id 0 34 miss% 0.807494115390582
plot_id,batch_id 0 35 miss% 0.7486916557826424
plot_id,batch_id 0 36 miss% 0.7998733828698301
plot_id,batch_id 0 37 miss% 0.8024178536614012
plot_id,batch_id 0 38 miss% 0.8079401537224781
plot_id,batch_id 0 39 miss% 0.8085437021782809
plot_id,batch_id 0 40 miss% 0.7835667603804793
plot_id,batch_id 0 41 miss% 0.8050867877068061
plot_id,batch_id 0 42 miss% 0.8101088061140258
plot_id,batch_id 0 43 miss% 0.8136862974306864
plot_id,batch_id 0 44 miss% 0.8168518378023049
plot_id,batch_id 0 45 miss% 0.7802823021764383
plot_id,batch_id 0 46 miss% 0.8051748579837771
plot_id,batch_id 0 47 miss% 0.8111836868161082
plot_id,batch_id 0 48 miss% 0.8115654364361655
plot_id,batch_id 0 49 miss% 0.8153590408351234
plot_id,batch_id 0 50 miss% 0.7877076601807488
plot_id,batch_id 0 51 miss% 0.8039089361622042
plot_id,batch_id 0 52 miss% 0.8075369284207328
plot_id,batch_id 0 53 miss% 0.8113414938224872
plot_id,batch_id 0 54 miss% 0.816520861765229
plot_id,batch_id 0 55 miss% 0.7669157937076272
plot_id,batch_id 0 56 miss% 0.8024000716248048
plot_id,batch_id 0 57 miss% 0.8080730512027936
plot_id,batch_id 0 58 miss% 0.8126488231808571
plot_id,batch_id 0 59 miss% 0.8162318132864864
plot_id,batch_id 0 60 miss% 0.6464758550232973
plot_id,batch_id 0 61 miss% 0.7473041268595727
plot_id,batch_id 0 62 miss% 0.7830557774254188
plot_id,batch_id 0 63 miss% 0.7940333932236803
plot_id,batch_id 0 64 miss% 0.8025684674973677
plot_id,batch_id 0 65 miss% 0.6420304870898081
plot_id,batch_id 0 66 miss% 0.749480267439715
plot_id,batch_id 0 67 miss% 0.7726208551536256
plot_id,batch_id 0 68 miss% 0.793391703226113
plot_id,batch_id 0 69 miss% 0.7933192573029628
plot_id,batch_id 0 70 miss% 0.6085283826411128
plot_id,batch_id 0 71 miss% 0.7578083568448853
plot_id,batch_id 0 72 miss% 0.7615494029472051
plot_id,batch_id 0 73 miss% 0.7755178800694199
plot_id,batch_id 0 74 miss% 0.789692956689356
plot_id,batch_id 0 75 miss% 0.6080774770127948
plot_id,batch_id 0 76 miss% 0.7100283760945856
plot_id,batch_id 0 77 miss% 0.755961235309946
plot_id,batch_id 0 78 miss% 0.7777293768017726
plot_id,batch_id 0 79 miss% 0.7875764243939254
plot_id,batch_id 0 80 miss% 0.6690454409483305
plot_id,batch_id 0 81 miss% 0.7758720173450231
plot_id,batch_id 0 82 miss% 0.792165780248755
plot_id,batch_id 0 83 miss% 0.7996421930842575
plot_id,batch_id 0 84 miss% 0.7991179208562901
plot_id,batch_id 0 85 miss% 0.6681607209948475
plot_id,batch_id 0 86 miss% 0.7681606739978446
plot_id,batch_id 0 87 miss% 0.7882941392141476
plot_id,batch_id 0 88 miss% 0.7956398379454341
plot_id,batch_id 0 89 miss% 0.799855636375534
plot_id,batch_id 0 90 miss% 0.6380460731321987
plot_id,batch_id 0 91 miss% 0.7658159506950447
plot_id,batch_id 0 92 miss% 0.7760290135617742
plot_id,batch_id 0 93 miss% 0.7851489714454986
plot_id,batch_id 0 94 miss% 0.7986058961885771
plot_id,batch_id 0 95 miss% 0.6362077342815607
plot_id,batch_id 0 96 miss% 0.7486077237533405
plot_id,batch_id 0 97 miss% 0.7776164170232804
plot_id,batch_id 0 98 miss% 0.7863859922106153
plot_id,batch_id 0 99 miss% 0.7938291525717959
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73044535 0.78949626 0.801184   0.80821686 0.80703969 0.72396315
 0.78288125 0.79858841 0.80410092 0.80889809 0.70310443 0.78567471
 0.7948533  0.8011756  0.80813044 0.71167195 0.77342605 0.79612598
 0.80336335 0.80555263 0.762555   0.80375098 0.80760906 0.81289582
 0.81432265 0.75103717 0.79670247 0.80430259 0.80677327 0.80845883
 0.75516798 0.79609304 0.8035529  0.80709256 0.80749412 0.74869166
 0.79987338 0.80241785 0.80794015 0.8085437  0.78356676 0.80508679
 0.81010881 0.8136863  0.81685184 0.7802823  0.80517486 0.81118369
 0.81156544 0.81535904 0.78770766 0.80390894 0.80753693 0.81134149
 0.81652086 0.76691579 0.80240007 0.80807305 0.81264882 0.81623181
 0.64647586 0.74730413 0.78305578 0.79403339 0.80256847 0.64203049
 0.74948027 0.77262086 0.7933917  0.79331926 0.60852838 0.75780836
 0.7615494  0.77551788 0.78969296 0.60807748 0.71002838 0.75596124
 0.77772938 0.78757642 0.66904544 0.77587202 0.79216578 0.79964219
 0.79911792 0.66816072 0.76816067 0.78829414 0.79563984 0.79985564
 0.63804607 0.76581595 0.77602901 0.78514897 0.7986059  0.63620773
 0.74860772 0.77761642 0.78638599 0.79382915]
for model  133 the mean error 0.7764831618466365
all id 133 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 25 out win 4 err 0.7764831618466365
Launcher: Job 134 completed in 10825 seconds.
Launcher: Task 230 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 3
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 67200
total number of trained parameters  134801
Epoch:0, Train loss:0.366169, valid loss:0.370888
Epoch:1, Train loss:0.226892, valid loss:0.230020
Epoch:2, Train loss:0.218773, valid loss:0.229029
Epoch:3, Train loss:0.217672, valid loss:0.228900
Epoch:4, Train loss:0.217209, valid loss:0.228261
Epoch:5, Train loss:0.217007, valid loss:0.228514
Epoch:6, Train loss:0.216868, valid loss:0.228184
Epoch:7, Train loss:0.216662, valid loss:0.228075
Epoch:8, Train loss:0.216543, valid loss:0.228241
Epoch:9, Train loss:0.216414, valid loss:0.228036
Epoch:10, Train loss:0.216376, valid loss:0.228346
Epoch:11, Train loss:0.215595, valid loss:0.227644
Epoch:12, Train loss:0.215592, valid loss:0.227914
Epoch:13, Train loss:0.215591, valid loss:0.227983
Epoch:14, Train loss:0.215519, valid loss:0.227880
Epoch:15, Train loss:0.215550, valid loss:0.228033
Epoch:16, Train loss:0.215477, valid loss:0.227748
Epoch:17, Train loss:0.215440, valid loss:0.227585
Epoch:18, Train loss:0.215558, valid loss:0.227728
Epoch:19, Train loss:0.215466, valid loss:0.227609
Epoch:20, Train loss:0.215394, valid loss:0.227613
Epoch:21, Train loss:0.215016, valid loss:0.227420
Epoch:22, Train loss:0.215056, valid loss:0.227540
Epoch:23, Train loss:0.215020, valid loss:0.227446
Epoch:24, Train loss:0.215038, valid loss:0.227531
Epoch:25, Train loss:0.215026, valid loss:0.227462
Epoch:26, Train loss:0.215006, valid loss:0.227417
Epoch:27, Train loss:0.214999, valid loss:0.227434
Epoch:28, Train loss:0.214984, valid loss:0.227487
Epoch:29, Train loss:0.214976, valid loss:0.227404
Epoch:30, Train loss:0.214989, valid loss:0.227428
Epoch:31, Train loss:0.214797, valid loss:0.227376
Epoch:32, Train loss:0.214790, valid loss:0.227353
Epoch:33, Train loss:0.214795, valid loss:0.227367
Epoch:34, Train loss:0.214793, valid loss:0.227386
Epoch:35, Train loss:0.214779, valid loss:0.227367
Epoch:36, Train loss:0.214791, valid loss:0.227341
Epoch:37, Train loss:0.214771, valid loss:0.227414
Epoch:38, Train loss:0.214781, valid loss:0.227410
Epoch:39, Train loss:0.214782, valid loss:0.227441
Epoch:40, Train loss:0.214766, valid loss:0.227404
Epoch:41, Train loss:0.214682, valid loss:0.227327
Epoch:42, Train loss:0.214684, valid loss:0.227366
Epoch:43, Train loss:0.214679, valid loss:0.227342
Epoch:44, Train loss:0.214682, valid loss:0.227317
Epoch:45, Train loss:0.214679, valid loss:0.227314
Epoch:46, Train loss:0.214682, valid loss:0.227330
Epoch:47, Train loss:0.214672, valid loss:0.227321
Epoch:48, Train loss:0.214679, valid loss:0.227329
Epoch:49, Train loss:0.214665, valid loss:0.227316
Epoch:50, Train loss:0.214669, valid loss:0.227322
Epoch:51, Train loss:0.214633, valid loss:0.227304
Epoch:52, Train loss:0.214631, valid loss:0.227381
Epoch:53, Train loss:0.214632, valid loss:0.227303
Epoch:54, Train loss:0.214630, valid loss:0.227302
Epoch:55, Train loss:0.214627, valid loss:0.227309
Epoch:56, Train loss:0.214627, valid loss:0.227314
Epoch:57, Train loss:0.214623, valid loss:0.227313
Epoch:58, Train loss:0.214630, valid loss:0.227312
Epoch:59, Train loss:0.214623, valid loss:0.227295
Epoch:60, Train loss:0.214621, valid loss:0.227301
training time 10712.3570394516
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.5519363657858332
plot_id,batch_id 0 1 miss% 0.6785821912037427
plot_id,batch_id 0 2 miss% 0.6956937267313507
plot_id,batch_id 0 3 miss% 0.7087047705156118
plot_id,batch_id 0 4 miss% 0.7088401388313648
plot_id,batch_id 0 5 miss% 0.5568879846847984
plot_id,batch_id 0 6 miss% 0.6708493134764271
plot_id,batch_id 0 7 miss% 0.6974175414901359
plot_id,batch_id 0 8 miss% 0.7042207135472928
plot_id,batch_id 0 9 miss% 0.712217228373363
plot_id,batch_id 0 10 miss% 0.5210008098158885
plot_id,batch_id 0 11 miss% 0.6677225447172548
plot_id,batch_id 0 12 miss% 0.6880507139771731
plot_id,batch_id 0 13 miss% 0.6990963084343222
plot_id,batch_id 0 14 miss% 0.7097108688826667
plot_id,batch_id 0 15 miss% 0.5256263913858
plot_id,batch_id 0 16 miss% 0.6711207602966635
plot_id,batch_id 0 17 miss% 0.6983559839426445
plot_id,batch_id 0 18 miss% 0.7024147543830783
plot_id,batch_id 0 19 miss% 0.7042772614717269
plot_id,batch_id 0 20 miss% 0.6122314504754506
plot_id,batch_id 0 21 miss% 0.6994841406030284
plot_id,batch_id 0 22 miss% 0.7097523557010749
plot_id,batch_id 0 23 miss% 0.7187603930678031
plot_id,batch_id 0 24 miss% 0.7218792010783194
plot_id,batch_id 0 25 miss% 0.6112156178246574
plot_id,batch_id 0 26 miss% 0.6958428146153867
plot_id,batch_id 0 27 miss% 0.7100944607471752
plot_id,batch_id 0 28 miss% 0.7180709146514891
plot_id,batch_id 0 29 miss% 0.7222422348804547
plot_id,batch_id 0 30 miss% 0.610813469693748
plot_id,batch_id 0 31 miss% 0.687667290176871
plot_id,batch_id 0 32 miss% 0.7039007488119094
plot_id,batch_id 0 33 miss% 0.7096980297695425
plot_id,batch_id 0 34 miss% 0.7136762233093757
plot_id,batch_id 0 35 miss% 0.5922928244706089
plot_id,batch_id 0 36 miss% 0.6949880545165756
plot_id,batch_id 0 37 miss% 0.7016032342010404
plot_id,batch_id 0 38 miss% 0.7113370447287879
plot_id,batch_id 0 39 miss% 0.7138159310675913
plot_id,batch_id 0 40 miss% 0.6540002295813048
plot_id,batch_id 0 41 miss% 0.7104872546207364
plot_id,batch_id 0 42 miss% 0.713169435340485
plot_id,batch_id 0 43 miss% 0.7234428979373405
plot_id,batch_id 0 44 miss% 0.7281959797173311
plot_id,batch_id 0 45 miss% 0.6537085541656
plot_id,batch_id 0 46 miss% 0.7099902632193906
plot_id,batch_id 0 47 miss% 0.7150226210350257
plot_id,batch_id 0 48 miss% 0.7214584083315704
plot_id,batch_id 0 49 miss% 0.7275197095367184
plot_id,batch_id 0 50 miss% 0.6753754790558928
plot_id,batch_id 0 51 miss% 0.706033429090552
plot_id,batch_id 0 52 miss% 0.7143352622846434
plot_id,batch_id 0 53 miss% 0.7217088393320291
plot_id,batch_id 0 54 miss% 0.7311785395952143
plot_id,batch_id 0 55 miss% 0.6672949461577955
plot_id,batch_id 0 56 miss% 0.706258067746641
plot_id,batch_id 0 57 miss% 0.7137475652718045
plot_id,batch_id 0 58 miss% 0.7209644281672316
plot_id,batch_id 0 59 miss% 0.7192577012675531
plot_id,batch_id 0 60 miss% 0.44134797124147457
plot_id,batch_id 0 61 miss% 0.6140520561868723
plot_id,batch_id 0 62 miss% 0.6635991132574564
plot_id,batch_id 0 63 miss% 0.6855748718896989
plot_id,batch_id 0 64 miss% 0.6944840862312737
plot_id,batch_id 0 65 miss% 0.43539311063169145
plot_id,batch_id 0 66 miss% 0.6056392078211229
plot_id,batch_id 0 67 miss% 0.6454648159954566
plot_id,batch_id 0 68 miss% 0.6821220057861863
plot_id,batch_id 0 69 miss% 0.6866892224115332
plot_id,batch_id 0 70 miss%the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  107025
Epoch:0, Train loss:0.294750, valid loss:0.288258
Epoch:1, Train loss:0.021607, valid loss:0.003948
Epoch:2, Train loss:0.005888, valid loss:0.002711
Epoch:3, Train loss:0.004370, valid loss:0.002136
Epoch:4, Train loss:0.003599, valid loss:0.001835
Epoch:5, Train loss:0.003160, valid loss:0.002034
Epoch:6, Train loss:0.002855, valid loss:0.001703
Epoch:7, Train loss:0.002607, valid loss:0.001406
Epoch:8, Train loss:0.002455, valid loss:0.001116
Epoch:9, Train loss:0.002277, valid loss:0.001104
Epoch:10, Train loss:0.002217, valid loss:0.001306
Epoch:11, Train loss:0.001609, valid loss:0.000956
Epoch:12, Train loss:0.001576, valid loss:0.000958
Epoch:13, Train loss:0.001542, valid loss:0.001085
Epoch:14, Train loss:0.001500, valid loss:0.000873
Epoch:15, Train loss:0.001483, valid loss:0.000831
Epoch:16, Train loss:0.001427, valid loss:0.000848
Epoch:17, Train loss:0.001471, valid loss:0.000810
Epoch:18, Train loss:0.001383, valid loss:0.000862
Epoch:19, Train loss:0.001394, valid loss:0.000874
Epoch:20, Train loss:0.001354, valid loss:0.000904
Epoch:21, Train loss:0.001030, valid loss:0.000802
Epoch:22, Train loss:0.001013, valid loss:0.000706
Epoch:23, Train loss:0.001038, valid loss:0.000862
Epoch:24, Train loss:0.001007, valid loss:0.000705
Epoch:25, Train loss:0.000992, valid loss:0.000671
Epoch:26, Train loss:0.001027, valid loss:0.000688
Epoch:27, Train loss:0.000994, valid loss:0.000648
Epoch:28, Train loss:0.000978, valid loss:0.000783
Epoch:29, Train loss:0.000986, valid loss:0.000780
Epoch:30, Train loss:0.000974, valid loss:0.000720
Epoch:31, Train loss:0.000814, valid loss:0.000670
Epoch:32, Train loss:0.000794, valid loss:0.000578
Epoch:33, Train loss:0.000801, valid loss:0.000593
Epoch:34, Train loss:0.000773, valid loss:0.000612
Epoch:35, Train loss:0.000783, valid loss:0.000707
Epoch:36, Train loss:0.000782, valid loss:0.000582
Epoch:37, Train loss:0.000789, valid loss:0.000702
Epoch:38, Train loss:0.000781, valid loss:0.000624
Epoch:39, Train loss:0.000766, valid loss:0.000599
Epoch:40, Train loss:0.000776, valid loss:0.000588
Epoch:41, Train loss:0.000684, valid loss:0.000604
Epoch:42, Train loss:0.000680, valid loss:0.000584
Epoch:43, Train loss:0.000682, valid loss:0.000561
Epoch:44, Train loss:0.000683, valid loss:0.000586
Epoch:45, Train loss:0.000689, valid loss:0.000586
Epoch:46, Train loss:0.000674, valid loss:0.000588
Epoch:47, Train loss:0.000671, valid loss:0.000601
Epoch:48, Train loss:0.000674, valid loss:0.000581
Epoch:49, Train loss:0.000661, valid loss:0.000580
Epoch:50, Train loss:0.000665, valid loss:0.000587
Epoch:51, Train loss:0.000627, valid loss:0.000571
Epoch:52, Train loss:0.000627, valid loss:0.000558
Epoch:53, Train loss:0.000626, valid loss:0.000555
Epoch:54, Train loss:0.000622, valid loss:0.000558
Epoch:55, Train loss:0.000622, valid loss:0.000569
Epoch:56, Train loss:0.000624, valid loss:0.000556
Epoch:57, Train loss:0.000619, valid loss:0.000553
Epoch:58, Train loss:0.000620, valid loss:0.000571
Epoch:59, Train loss:0.000615, valid loss:0.000563
Epoch:60, Train loss:0.000617, valid loss:0.000549
training time 10698.346695423126
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.03899844483375111
plot_id,batch_id 0 1 miss% 0.01538556697979903
plot_id,batch_id 0 2 miss% 0.02649528971053697
plot_id,batch_id 0 3 miss% 0.024693786967999985
plot_id,batch_id 0 4 miss% 0.020865409019005004
plot_id,batch_id 0 5 miss% 0.03947138343180151
plot_id,batch_id 0 6 miss% 0.02896897050222108
plot_id,batch_id 0 7 miss% 0.0320072529465313
plot_id,batch_id 0 8 miss% 0.030853740671587162
plot_id,batch_id 0 9 miss% 0.020250687204988897
plot_id,batch_id 0 10 miss% 0.05407078837982252
plot_id,batch_id 0 11 miss% 0.03755783263071914
plot_id,batch_id 0 12 miss% 0.028473317566420138
plot_id,batch_id 0 13 miss% 0.025268145838624655
plot_id,batch_id 0 14 miss% 0.030850600884230476
plot_id,batch_id 0 15 miss% 0.02731101678794187
plot_id,batch_id 0 16 miss% 0.03537665611859825
plot_id,batch_id 0 17 miss% 0.05415546718839538
plot_id,batch_id 0 18 miss% 0.033692601387768965
plot_id,batch_id 0 19 miss% 0.026573754463784145
plot_id,batch_id 0 20 miss% 0.058447213574845516
plot_id,batch_id 0 21 miss% 0.02169606785935354
plot_id,batch_id 0 22 miss% 0.01915610339058299
plot_id,batch_id 0 23 miss% 0.025226737080022283
plot_id,batch_id 0 24 miss% 0.027459354397862033
plot_id,batch_id 0 25 miss% 0.04760095030959401
plot_id,batch_id 0 26 miss% 0.028216521839320333
plot_id,batch_id 0 27 miss% 0.027204692014733627
plot_id,batch_id 0 28 miss% 0.029642784445094997
plot_id,batch_id 0 29 miss% 0.030998998119900634
plot_id,batch_id 0 30 miss% 0.05230481529972512
plot_id,batch_id 0 31 miss% 0.026737620170317146
plot_id,batch_id 0 32 miss% 0.02766160663249764
plot_id,batch_id 0 33 miss% 0.025793109034965064
plot_id,batch_id 0 34 miss% 0.030464286422947986
plot_id,batch_id 0 35 miss% 0.0556892750544866
plot_id,batch_id 0 36 miss% 0.04535798494391397
plot_id,batch_id 0 37 miss% 0.02741372625397604
plot_id,batch_id 0 38 miss% 0.0194308854849174
plot_id,batch_id 0 39 miss% 0.02312470821527848
plot_id,batch_id 0 40 miss% 0.06124464251911853
plot_id,batch_id 0 41 miss% 0.020925733850744383
plot_id,batch_id 0 42 miss% 0.021963216370265666
plot_id,batch_id 0 43 miss% 0.025007740667406686
plot_id,batch_id 0 44 miss% 0.02363602523427174
plot_id,batch_id 0 45 miss% 0.018312593900585324
plot_id,batch_id 0 46 miss% 0.024173824761927896
plot_id,batch_id 0 47 miss% 0.020825756637739567
plot_id,batch_id 0 48 miss% 0.028990174259501886
plot_id,batch_id 0 49 miss% 0.02122813160147927
plot_id,batch_id 0 50 miss% 0.02945750034098997
plot_id,batch_id 0 51 miss% 0.023671958447243718
plot_id,batch_id 0 52 miss% 0.020186853916488025
plot_id,batch_id 0 53 miss% 0.01509376382384334
plot_id,batch_id 0 54 miss% 0.036960313322617386
plot_id,batch_id 0 55 miss% 0.043140912572200325
plot_id,batch_id 0 56 miss% 0.028688759536515666
plot_id,batch_id 0 57 miss% 0.02085733226327193
plot_id,batch_id 0 58 miss% 0.026546304005058138
plot_id,batch_id 0 59 miss% 0.024190437997273065
plot_id,batch_id 0 60 miss% 0.03705226970695298
plot_id,batch_id 0 61 miss% 0.028653667353233738
plot_id,batch_id 0 62 miss% 0.025003171696690425
plot_id,batch_id 0 63 miss% 0.029425360028056808
plot_id,batch_id 0 64 miss% 0.03010894072000116
plot_id,batch_id 0 65 miss% 0.024147263105212962
plot_id,batch_id 0 66 miss% 0.028400583966439814
plot_id,batch_id 0 67 miss% 0.03186244208741101
 0.4043710439456125
plot_id,batch_id 0 71 miss% 0.5926461508473464
plot_id,batch_id 0 72 miss% 0.6316389818364982
plot_id,batch_id 0 73 miss% 0.6613015705842028
plot_id,batch_id 0 74 miss% 0.6892827940294597
plot_id,batch_id 0 75 miss% 0.40162910737523755
plot_id,batch_id 0 76 miss% 0.5719430643561767
plot_id,batch_id 0 77 miss% 0.6233767582453915
plot_id,batch_id 0 78 miss% 0.6601777103621609
plot_id,batch_id 0 79 miss% 0.6663760791845865
plot_id,batch_id 0 80 miss% 0.4690490744425154
plot_id,batch_id 0 81 miss% 0.639671410470888
plot_id,batch_id 0 82 miss% 0.6753226849780057
plot_id,batch_id 0 83 miss% 0.6943149043464587
plot_id,batch_id 0 84 miss% 0.6988244029633928
plot_id,batch_id 0 85 miss% 0.47184462354964246
plot_id,batch_id 0 86 miss% 0.6336625158264967
plot_id,batch_id 0 87 miss% 0.6660524453978723
plot_id,batch_id 0 88 miss% 0.6923158763396416
plot_id,batch_id 0 89 miss% 0.6973344553279208
plot_id,batch_id 0 90 miss% 0.4447664522270919
plot_id,batch_id 0 91 miss% 0.6294850390374102
plot_id,batch_id 0 92 miss% 0.666609830702243
plot_id,batch_id 0 93 miss% 0.6905979129804676
plot_id,batch_id 0 94 miss% 0.6937042874733698
plot_id,batch_id 0 95 miss% 0.42883390843657304
plot_id,batch_id 0 96 miss% 0.6108288423106855
plot_id,batch_id 0 97 miss% 0.6542095959078931
plot_id,batch_id 0 98 miss% 0.6726412110464446
plot_id,batch_id 0 99 miss% 0.6859863417972343
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.55193637 0.67858219 0.69569373 0.70870477 0.70884014 0.55688798
 0.67084931 0.69741754 0.70422071 0.71221723 0.52100081 0.66772254
 0.68805071 0.69909631 0.70971087 0.52562639 0.67112076 0.69835598
 0.70241475 0.70427726 0.61223145 0.69948414 0.70975236 0.71876039
 0.7218792  0.61121562 0.69584281 0.71009446 0.71807091 0.72224223
 0.61081347 0.68766729 0.70390075 0.70969803 0.71367622 0.59229282
 0.69498805 0.70160323 0.71133704 0.71381593 0.65400023 0.71048725
 0.71316944 0.7234429  0.72819598 0.65370855 0.70999026 0.71502262
 0.72145841 0.72751971 0.67537548 0.70603343 0.71433526 0.72170884
 0.73117854 0.66729495 0.70625807 0.71374757 0.72096443 0.7192577
 0.44134797 0.61405206 0.66359911 0.68557487 0.69448409 0.43539311
 0.60563921 0.64546482 0.68212201 0.68668922 0.40437104 0.59264615
 0.63163898 0.66130157 0.68928279 0.40162911 0.57194306 0.62337676
 0.66017771 0.66637608 0.46904907 0.63967141 0.67532268 0.6943149
 0.6988244  0.47184462 0.63366252 0.66605245 0.69231588 0.69733446
 0.44476645 0.62948504 0.66660983 0.69059791 0.69370429 0.42883391
 0.61082884 0.6542096  0.67264121 0.68598634]
for model  240 the mean error 0.6573840795557457
all id 240 hidden_dim 32 learning_rate 0.02 num_layers 5 frames 31 out win 3 err 0.6573840795557457
Launcher: Job 241 completed in 10885 seconds.
Launcher: Task 195 done. Exiting.
plot_id,batch_id 0 68 miss% 0.022885961182413886
plot_id,batch_id 0 69 miss% 0.029576392963250056
plot_id,batch_id 0 70 miss% 0.026591004523250413
plot_id,batch_id 0 71 miss% 0.05182600714253438
plot_id,batch_id 0 72 miss% 0.04145381235957089
plot_id,batch_id 0 73 miss% 0.02722634010371032
plot_id,batch_id 0 74 miss% 0.03309987964154639
plot_id,batch_id 0 75 miss% 0.04178670214188216
plot_id,batch_id 0 76 miss% 0.030399990591551487
plot_id,batch_id 0 77 miss% 0.027431343010215475
plot_id,batch_id 0 78 miss% 0.030364951835118652
plot_id,batch_id 0 79 miss% 0.026871752775479926
plot_id,batch_id 0 80 miss% 0.034121908261177364
plot_id,batch_id 0 81 miss% 0.021133139712521367
plot_id,batch_id 0 82 miss% 0.023397974858604814
plot_id,batch_id 0 83 miss% 0.03189129873080343
plot_id,batch_id 0 84 miss% 0.03009000650014286
plot_id,batch_id 0 85 miss% 0.04859716948804613
plot_id,batch_id 0 86 miss% 0.030089625799909914
plot_id,batch_id 0 87 miss% 0.02725205113078411
plot_id,batch_id 0 88 miss% 0.03135050132660886
plot_id,batch_id 0 89 miss% 0.02516828608645405
plot_id,batch_id 0 90 miss% 0.03364786976426016
plot_id,batch_id 0 91 miss% 0.03266657359814317
plot_id,batch_id 0 92 miss% 0.027873115352008128
plot_id,batch_id 0 93 miss% 0.026702852649651117
plot_id,batch_id 0 94 miss% 0.028196272776047433
plot_id,batch_id 0 95 miss% 0.04732511949443055
plot_id,batch_id 0 96 miss% 0.030463033942385313
plot_id,batch_id 0 97 miss% 0.050418890472508575
plot_id,batch_id 0 98 miss% 0.03863056746873177
plot_id,batch_id 0 99 miss% 0.025558173380788723
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03899844 0.01538557 0.02649529 0.02469379 0.02086541 0.03947138
 0.02896897 0.03200725 0.03085374 0.02025069 0.05407079 0.03755783
 0.02847332 0.02526815 0.0308506  0.02731102 0.03537666 0.05415547
 0.0336926  0.02657375 0.05844721 0.02169607 0.0191561  0.02522674
 0.02745935 0.04760095 0.02821652 0.02720469 0.02964278 0.030999
 0.05230482 0.02673762 0.02766161 0.02579311 0.03046429 0.05568928
 0.04535798 0.02741373 0.01943089 0.02312471 0.06124464 0.02092573
 0.02196322 0.02500774 0.02363603 0.01831259 0.02417382 0.02082576
 0.02899017 0.02122813 0.0294575  0.02367196 0.02018685 0.01509376
 0.03696031 0.04314091 0.02868876 0.02085733 0.0265463  0.02419044
 0.03705227 0.02865367 0.02500317 0.02942536 0.03010894 0.02414726
 0.02840058 0.03186244 0.02288596 0.02957639 0.026591   0.05182601
 0.04145381 0.02722634 0.03309988 0.0417867  0.03039999 0.02743134
 0.03036495 0.02687175 0.03412191 0.02113314 0.02339797 0.0318913
 0.03009001 0.04859717 0.03008963 0.02725205 0.0313505  0.02516829
 0.03364787 0.03266657 0.02787312 0.02670285 0.02819627 0.04732512
 0.03046303 0.05041889 0.03863057 0.02555817]
for model  179 the mean error 0.030847923978159365
all id 179 hidden_dim 32 learning_rate 0.005 num_layers 4 frames 31 out win 5 err 0.030847923978159365
Launcher: Job 180 completed in 10893 seconds.
Launcher: Task 52 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 50400
total number of trained parameters  134801
Epoch:0, Train loss:0.486496, valid loss:0.489493
Epoch:1, Train loss:0.359846, valid loss:0.361063
Epoch:2, Train loss:0.346373, valid loss:0.359373
Epoch:3, Train loss:0.344238, valid loss:0.359268
Epoch:4, Train loss:0.342945, valid loss:0.358357
Epoch:5, Train loss:0.342393, valid loss:0.358057
Epoch:6, Train loss:0.342163, valid loss:0.358860
Epoch:7, Train loss:0.341862, valid loss:0.358205
Epoch:8, Train loss:0.341639, valid loss:0.358338
Epoch:9, Train loss:0.341627, valid loss:0.358222
Epoch:10, Train loss:0.341230, valid loss:0.357880
Epoch:11, Train loss:0.339971, valid loss:0.357454
Epoch:12, Train loss:0.340013, valid loss:0.357199
Epoch:13, Train loss:0.339863, valid loss:0.357368
Epoch:14, Train loss:0.339975, valid loss:0.357316
Epoch:15, Train loss:0.339834, valid loss:0.357220
Epoch:16, Train loss:0.339825, valid loss:0.357202
Epoch:17, Train loss:0.339784, valid loss:0.357252
Epoch:18, Train loss:0.339846, valid loss:0.357300
Epoch:19, Train loss:0.339660, valid loss:0.357323
Epoch:20, Train loss:0.339720, valid loss:0.357120
Epoch:21, Train loss:0.339065, valid loss:0.356953
Epoch:22, Train loss:0.339108, valid loss:0.356915
Epoch:23, Train loss:0.339058, valid loss:0.357120
Epoch:24, Train loss:0.339041, valid loss:0.356941
Epoch:25, Train loss:0.339071, valid loss:0.356918
Epoch:26, Train loss:0.339070, valid loss:0.356802
Epoch:27, Train loss:0.339017, valid loss:0.356942
Epoch:28, Train loss:0.338965, valid loss:0.356959
Epoch:29, Train loss:0.338965, valid loss:0.357008
Epoch:30, Train loss:0.338980, valid loss:0.356866
Epoch:31, Train loss:0.338654, valid loss:0.356830
Epoch:32, Train loss:0.338651, valid loss:0.356794
Epoch:33, Train loss:0.338651, valid loss:0.356775
Epoch:34, Train loss:0.338651, valid loss:0.356741
Epoch:35, Train loss:0.338622, valid loss:0.356787
Epoch:36, Train loss:0.338652, valid loss:0.356820
Epoch:37, Train loss:0.338632, valid loss:0.356768
Epoch:38, Train loss:0.338633, valid loss:0.356731
Epoch:39, Train loss:0.338643, valid loss:0.356775
Epoch:40, Train loss:0.338625, valid loss:0.356717
Epoch:41, Train loss:0.338470, valid loss:0.356735
Epoch:42, Train loss:0.338475, valid loss:0.356713
Epoch:43, Train loss:0.338453, valid loss:0.356690
Epoch:44, Train loss:0.338446, valid loss:0.356735
Epoch:45, Train loss:0.338458, valid loss:0.356726
Epoch:46, Train loss:0.338479, valid loss:0.356693
Epoch:47, Train loss:0.338450, valid loss:0.356705
Epoch:48, Train loss:0.338442, valid loss:0.356697
Epoch:49, Train loss:0.338450, valid loss:0.356685
Epoch:50, Train loss:0.338426, valid loss:0.356685
Epoch:51, Train loss:0.338372, valid loss:0.356657
Epoch:52, Train loss:0.338368, valid loss:0.356668
Epoch:53, Train loss:0.338364, valid loss:0.356667
Epoch:54, Train loss:0.338362, valid loss:0.356664
Epoch:55, Train loss:0.338360, valid loss:0.356656
Epoch:56, Train loss:0.338368, valid loss:0.356680
Epoch:57, Train loss:0.338355, valid loss:0.356654
Epoch:58, Train loss:0.338356, valid loss:0.356681
Epoch:59, Train loss:0.338361, valid loss:0.356654
Epoch:60, Train loss:0.338349, valid loss:0.356651
training time 10735.720123767853
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7274005035559851
plot_id,batch_id 0 1 miss% 0.7908691650591747
plot_id,batch_id 0 2 miss% 0.8017210399239456
plot_id,batch_id 0 3 miss% 0.8074388604197195
plot_id,batch_id 0 4 miss% 0.8074816885504174
plot_id,batch_id 0 5 miss% 0.7273630605834863
plot_id,batch_id 0 6 miss% 0.7859487615391291
plot_id,batch_id 0 7 miss% 0.7982178178929386
plot_id,batch_id 0 8 miss% 0.8046501129441622
plot_id,batch_id 0 9 miss% 0.8090066949411227
plot_id,batch_id 0 10 miss% 0.7002694676280298
plot_id,batch_id 0 11 miss% 0.7828303641444521
plot_id,batch_id 0 12 miss% 0.7963898990060343
plot_id,batch_id 0 13 miss% 0.802370153715184
plot_id,batch_id 0 14 miss% 0.8086595304099939
plot_id,batch_id 0 15 miss% 0.7152582156391823
plot_id,batch_id 0 16 miss% 0.7750434888579817
plot_id,batch_id 0 17 miss% 0.7945310822873726
plot_id,batch_id 0 18 miss% 0.8036756573486722
plot_id,batch_id 0 19 miss% 0.8060404432566168
plot_id,batch_id 0 20 miss% 0.7640982861505181
plot_id,batch_id 0 21 miss% 0.803306452712185
plot_id,batch_id 0 22 miss% 0.8072230807924755
plot_id,batch_id 0 23 miss% 0.8120584273497391
plot_id,batch_id 0 24 miss% 0.8137979443509904
plot_id,batch_id 0 25 miss% 0.7550235519202637
plot_id,batch_id 0 26 miss% 0.7967780959498427
plot_id,batch_id 0 27 miss% 0.8043906899695028
plot_id,batch_id 0 28 miss% 0.8075947795259697
plot_id,batch_id 0 29 miss% 0.8095145678294123
plot_id,batch_id 0 30 miss% 0.752047204498414
plot_id,batch_id 0 31 miss% 0.7961376517531791
plot_id,batch_id 0 32 miss% 0.8031576995447324
plot_id,batch_id 0 33 miss% 0.8071886747041118
plot_id,batch_id 0 34 miss% 0.809417777498429
plot_id,batch_id 0 35 miss% 0.7492761687797949
plot_id,batch_id 0 36 miss% 0.8025738452212996
plot_id,batch_id 0 37 miss% 0.8028558320594829
plot_id,batch_id 0 38 miss% 0.8080742736152314
plot_id,batch_id 0 39 miss% 0.8098057352189225
plot_id,batch_id 0 40 miss% 0.783956864984851
plot_id,batch_id 0 41 miss% 0.8063180198003305
plot_id,batch_id 0 42 miss% 0.8092310187208284
plot_id,batch_id 0 43 miss% 0.8118569738912955
plot_id,batch_id 0 44 miss% 0.8166884625788108
plot_id,batch_id 0 45 miss% 0.7759057661437309
plot_id,batch_id 0 46 miss% 0.8056270629120739
plot_id,batch_id 0 47 miss% 0.810230309442303
plot_id,batch_id 0 48 miss% 0.8112658110453852
plot_id,batch_id 0 49 miss% 0.815518311180432
plot_id,batch_id 0 50 miss% 0.7842555885259168
plot_id,batch_id 0 51 miss% 0.803417480197281
plot_id,batch_id 0 52 miss% 0.8084190644830035
plot_id,batch_id 0 53 miss% 0.809461141331796
plot_id,batch_id 0 54 miss% 0.8165429085563549
plot_id,batch_id 0 55 miss% 0.7684719247055694
plot_id,batch_id 0 56 miss% 0.8029971192762816
plot_id,batch_id 0 57 miss% 0.8069481439940827
plot_id,batch_id 0 58 miss% 0.812382277623679
plot_id,batch_id 0 59 miss% 0.8155029037477042
plot_id,batch_id 0 60 miss% 0.6452577514720685
plot_id,batch_id 0 61 miss% 0.7482851093147893
plot_id,batch_id 0 62 miss% 0.7827664031164288
plot_id,batch_id 0 63 miss% 0.7932376230300171
plot_id,batch_id 0 64 miss% 0.8040735414208459
plot_id,batch_id 0 65 miss% 0.6377587574108029
plot_id,batch_id 0 66 miss% 0.7497008886900329
plot_id,batch_id 0 67 miss% 0.7704731551794183
plot_id,batch_id 0 68 miss% 0.7934483163577474
plot_id,batch_id 0 69 miss% 0.7941509510595317
plot_id,batch_id 0 70 miss% 0.6076544422368703
plot_id,batch_id 0 71 miss% 0.76201351048634
plot_id,batch_id 0 72 miss% 0.7618659763828155
plot_id,batch_id 0 73 miss% 0.7779779649742706
plot_id,batch_id 0 74 miss% 0.7896085793722912
plot_id,batch_id 0 75 miss% 0.6024482256424847
plot_id,batch_id 0 76 miss% 0.7083945026593225
plot_id,batch_id 0 77 miss% 0.7548635621191188
plot_id,batch_id 0 78 miss% 0.7752805493138197
plot_id,batch_id 0 79 miss% 0.7829886781001079
plot_id,batch_id 0 80 miss% 0.6678430185923092
plot_id,batch_id 0 81 miss% 0.7768911417339539
plot_id,batch_id 0 82 miss% 0.7912107640848123
plot_id,batch_id 0 83 miss% 0.7992578577725581
plot_id,batch_id 0 84 miss% 0.8003638768568545
plot_id,batch_id 0 85 miss% 0.6670880522533068
plot_id,batch_id 0 86 miss% 0.7691961853690089
plot_id,batch_id 0 87 miss% 0.7883201664844192
plot_id,batch_id 0 88 miss% 0.7969661267436124
plot_id,batch_id 0 89 miss% 0.7993514307135886
plot_id,batch_id 0 90 miss% 0.6412358593100146
plot_id,batch_id 0 91 miss% 0.7616881829016803
plot_id,batch_id 0 92 miss% 0.7754837964238537
plot_id,batch_id 0 93 miss% 0.7891617137602417
plot_id,batch_id 0 94 miss% 0.7995267611484228
plot_id,batch_id 0 95 miss% 0.631423862703822
plot_id,batch_id 0 96 miss% 0.7487472481107973
plot_id,batch_id 0 97 miss% 0.7750840990427845
plot_id,batch_id 0 98 miss% 0.787070292504036
plot_id,batch_id 0 99 miss% 0.7962629789926033
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.7274005  0.79086917 0.80172104 0.80743886 0.80748169 0.72736306
 0.78594876 0.79821782 0.80465011 0.80900669 0.70026947 0.78283036
 0.7963899  0.80237015 0.80865953 0.71525822 0.77504349 0.79453108
 0.80367566 0.80604044 0.76409829 0.80330645 0.80722308 0.81205843
 0.81379794 0.75502355 0.7967781  0.80439069 0.80759478 0.80951457
 0.7520472  0.79613765 0.8031577  0.80718867 0.80941778 0.74927617
 0.80257385 0.80285583 0.80807427 0.80980574 0.78395686 0.80631802
 0.80923102 0.81185697 0.81668846 0.77590577 0.80562706 0.81023031
 0.81126581 0.81551831 0.78425559 0.80341748 0.80841906 0.80946114
 0.81654291 0.76847192 0.80299712 0.80694814 0.81238228 0.8155029
 0.64525775 0.74828511 0.7827664  0.79323762 0.80407354 0.63775876
 0.74970089 0.77047316 0.79344832 0.79415095 0.60765444 0.76201351
 0.76186598 0.77797796 0.78960858 0.60244823 0.7083945  0.75486356
 0.77528055 0.78298868 0.66784302 0.77689114 0.79121076 0.79925786
 0.80036388 0.66708805 0.76919619 0.78832017 0.79696613 0.79935143
 0.64123586 0.76168818 0.7754838  0.78916171 0.79952676 0.63142386
 0.74874725 0.7750841  0.78707029 0.79626298]
for model  160 the mean error 0.7763890580413162
all id 160 hidden_dim 32 learning_rate 0.02 num_layers 5 frames 25 out win 4 err 0.7763890580413162
Launcher: Job 161 completed in 10912 seconds.
Launcher: Task 218 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  107025
Epoch:0, Train loss:0.299547, valid loss:0.293055
Epoch:1, Train loss:0.017424, valid loss:0.003106
Epoch:2, Train loss:0.004652, valid loss:0.002498
Epoch:3, Train loss:0.003420, valid loss:0.001850
Epoch:4, Train loss:0.002947, valid loss:0.001458
Epoch:5, Train loss:0.002686, valid loss:0.001729
Epoch:6, Train loss:0.002518, valid loss:0.001384
Epoch:7, Train loss:0.002427, valid loss:0.001337
Epoch:8, Train loss:0.002280, valid loss:0.001300
Epoch:9, Train loss:0.002180, valid loss:0.001476
Epoch:10, Train loss:0.002123, valid loss:0.001148
Epoch:11, Train loss:0.001428, valid loss:0.000854
Epoch:12, Train loss:0.001428, valid loss:0.000879
Epoch:13, Train loss:0.001415, valid loss:0.000988
Epoch:14, Train loss:0.001388, valid loss:0.000787
Epoch:15, Train loss:0.001364, valid loss:0.000743
Epoch:16, Train loss:0.001353, valid loss:0.000759
Epoch:17, Train loss:0.001315, valid loss:0.000836
Epoch:18, Train loss:0.001293, valid loss:0.000810
Epoch:19, Train loss:0.001246, valid loss:0.000861
Epoch:20, Train loss:0.001271, valid loss:0.000774
Epoch:21, Train loss:0.000929, valid loss:0.000637
Epoch:22, Train loss:0.000920, valid loss:0.000647
Epoch:23, Train loss:0.000921, valid loss:0.000762
Epoch:24, Train loss:0.000910, valid loss:0.000586
Epoch:25, Train loss:0.000916, valid loss:0.000664
Epoch:26, Train loss:0.000932, valid loss:0.000613
Epoch:27, Train loss:0.000864, valid loss:0.000688
Epoch:28, Train loss:0.000911, valid loss:0.000734
Epoch:29, Train loss:0.000853, valid loss:0.000717
Epoch:30, Train loss:0.000877, valid loss:0.000660
Epoch:31, Train loss:0.000694, valid loss:0.000553
Epoch:32, Train loss:0.000697, valid loss:0.000545
Epoch:33, Train loss:0.000699, valid loss:0.000605
Epoch:34, Train loss:0.000682, valid loss:0.000554
Epoch:35, Train loss:0.000684, valid loss:0.000593
Epoch:36, Train loss:0.000679, valid loss:0.000584
Epoch:37, Train loss:0.000679, valid loss:0.000613
Epoch:38, Train loss:0.000674, valid loss:0.000562
Epoch:39, Train loss:0.000664, valid loss:0.000606
Epoch:40, Train loss:0.000658, valid loss:0.000600
Epoch:41, Train loss:0.000582, valid loss:0.000533
Epoch:42, Train loss:0.000582, valid loss:0.000516
Epoch:43, Train loss:0.000587, valid loss:0.000519
Epoch:44, Train loss:0.000574, valid loss:0.000528
Epoch:45, Train loss:0.000573, valid loss:0.000554
Epoch:46, Train loss:0.000573, valid loss:0.000545
Epoch:47, Train loss:0.000573, valid loss:0.000518
Epoch:48, Train loss:0.000574, valid loss:0.000523
Epoch:49, Train loss:0.000570, valid loss:0.000586
Epoch:50, Train loss:0.000569, valid loss:0.000529
Epoch:51, Train loss:0.000534, valid loss:0.000520
Epoch:52, Train loss:0.000529, valid loss:0.000534
Epoch:53, Train loss:0.000533, valid loss:0.000528
Epoch:54, Train loss:0.000528, valid loss:0.000520
Epoch:55, Train loss:0.000525, valid loss:0.000513
Epoch:56, Train loss:0.000524, valid loss:0.000520
Epoch:57, Train loss:0.000523, valid loss:0.000520
Epoch:58, Train loss:0.000523, valid loss:0.000528
Epoch:59, Train loss:0.000521, valid loss:0.000536
Epoch:60, Train loss:0.000524, valid loss:0.000527
training time 10751.666461467743
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.01742853569137373
plot_id,batch_id 0 1 miss% 0.018951687681367
plot_id,batch_id 0 2 miss% 0.0263851558761778
plot_id,batch_id 0 3 miss% 0.02598372605281224
plot_id,batch_id 0 4 miss% 0.024345464721626645
plot_id,batch_id 0 5 miss% 0.04010393651354916
plot_id,batch_id 0 6 miss% 0.033794391790588796
plot_id,batch_id 0 7 miss% 0.025968676922534132
plot_id,batch_id 0 8 miss% 0.0221387228424258
plot_id,batch_id 0 9 miss% 0.025039851727839145
plot_id,batch_id 0 10 miss% 0.037156059237078355
plot_id,batch_id 0 11 miss% 0.027671456474957477
plot_id,batch_id 0 12 miss% 0.024215627059476882
plot_id,batch_id 0 13 miss% 0.01878209713735257
plot_id,batch_id 0 14 miss% 0.024205297626011275
plot_id,batch_id 0 15 miss% 0.04618861479952697
plot_id,batch_id 0 16 miss% 0.02262478941793368
plot_id,batch_id 0 17 miss% 0.038822083605398
plot_id,batch_id 0 18 miss% 0.030878939549481253
plot_id,batch_id 0 19 miss% 0.030321563957172434
plot_id,batch_id 0 20 miss% 0.03479180371049158
plot_id,batch_id 0 21 miss% 0.023355082698224176
plot_id,batch_id 0 22 miss% 0.020781727435967155
plot_id,batch_id 0 23 miss% 0.019130777503553503
plot_id,batch_id 0 24 miss% 0.025158048649638696
plot_id,batch_id 0 25 miss% 0.03285435300054817
plot_id,batch_id 0 26 miss% 0.03141920451125541
plot_id,batch_id 0 27 miss% 0.03014159828406675
plot_id,batch_id 0 28 miss% 0.019362679375613916
plot_id,batch_id 0 29 miss% 0.021325680483632273
plot_id,batch_id 0 30 miss% 0.04681685891198229
plot_id,batch_id 0 31 miss% 0.0327420106513221
plot_id,batch_id 0 32 miss% 0.026522216980892892
plot_id,batch_id 0 33 miss% 0.025565793222532218
plot_id,batch_id 0 34 miss% 0.026448643563631878
plot_id,batch_id 0 35 miss% 0.04289063221675686
plot_id,batch_id 0 36 miss% 0.03732203033796007
plot_id,batch_id 0 37 miss% 0.027764429423819445
plot_id,batch_id 0 38 miss% 0.03199068405373301
plot_id,batch_id 0 39 miss% 0.023812998432072196
plot_id,batch_id 0 40 miss% 0.07432513829349521
plot_id,batch_id 0 41 miss% 0.022413565152951467
plot_id,batch_id 0 42 miss% 0.015332840568634524
plot_id,batch_id 0 43 miss% 0.02248564072129501
plot_id,batch_id 0 44 miss% 0.02210544277301864
plot_id,batch_id 0 45 miss% 0.039969278149907056
plot_id,batch_id 0 46 miss% 0.04065946836909275
plot_id,batch_id 0 47 miss% 0.01990954043986368
plot_id,batch_id 0 48 miss% 0.02911273317856132
plot_id,batch_id 0 49 miss% 0.019498783848100207
plot_id,batch_id 0 50 miss% 0.020400308202127355
plot_id,batch_id 0 51 miss% 0.019598598212210765
plot_id,batch_id 0 52 miss% 0.023011847327094673
plot_id,batch_id 0 53 miss% 0.011268301814281836
plot_id,batch_id 0 54 miss% 0.03158009884091819
plot_id,batch_id 0 55 miss% 0.02230099181558342
plot_id,batch_id 0 56 miss% 0.021214803689669475
plot_id,batch_id 0 57 miss% 0.030014649210644395
plot_id,batch_id 0 58 miss% 0.024608861954757772
plot_id,batch_id 0 59 miss% 0.019288850335626793
plot_id,batch_id 0 60 miss% 0.026430077137649947
plot_id,batch_id 0 61 miss% 0.03228777533430588
plot_id,batch_id 0 62 miss% 0.021604336008360467
plot_id,batch_id 0 63 miss% 0.03542855829580886
plot_id,batch_id 0 64 miss% 0.027602083439344484
plot_id,batch_id 0 65 miss% 0.038244840010617516
plot_id,batch_id 0 66 miss% 0.02950700761451057
plot_id,batch_id 0 67 miss% 0.028682565293351524
plot_id,batch_id 0 68 miss% 0.027129715438286458
plot_id,batch_id 0 69 miss% 0.022064932714956527
plot_id,batch_id 0 70 miss% 0.041189846040213224
plot_id,batch_id 0 71 miss% 0.04041023724984261
plot_id,batch_id 0 72 miss% 0.031171562243227053
plot_id,batch_id 0 73 miss% 0.032938336338565265
plot_id,batch_id 0 74 miss% 0.030499970869035924
plot_id,batch_id 0 75 miss% 0.03189206428243667
plot_id,batch_id 0 76 miss% 0.049229718563952654
plot_id,batch_id 0 77 miss% 0.026258776415004906
plot_id,batch_id 0 78 miss% 0.044048272849438
plot_id,batch_id 0 79 miss% 0.055361209771847636
plot_id,batch_id 0 80 miss% 0.04416061653851011
plot_id,batch_id 0 81 miss% 0.021527437748163146
plot_id,batch_id 0 82 miss% 0.020742802872576693
plot_id,batch_id 0 83 miss% 0.02460934324034936
plot_id,batch_id 0 84 miss% 0.03421792246340884
plot_id,batch_id 0 85 miss% 0.06127053888453616
plot_id,batch_id 0 86 miss% 0.01969766279759879
plot_id,batch_id 0 87 miss% 0.02206030533196623
plot_id,batch_id 0 88 miss% 0.02757784067797676
plot_id,batch_id 0 89 miss% 0.03218895907756121
plot_id,batch_id 0 90 miss% 0.04135926227972843
plot_id,batch_id 0 91 miss% 0.029049862970388685
plot_id,batch_id 0 92 miss% 0.030581394268619857
plot_id,batch_id 0 93 miss% 0.028619793918641168
plot_id,batch_id 0 94 miss% 0.03821070637655837
plot_id,batch_id 0 95 miss% 0.041629286056539316
plot_id,batch_id 0 96 miss% 0.028909818358367288
plot_id,batch_id 0 97 miss% 0.045844603281464294
plot_id,batch_id 0 98 miss% 0.03349277029416018
plot_id,batch_id 0 99 miss% 0.028342724394178034
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.01742854 0.01895169 0.02638516 0.02598373 0.02434546 0.04010394
 0.03379439 0.02596868 0.02213872 0.02503985 0.03715606 0.02767146
 0.02421563 0.0187821  0.0242053  0.04618861 0.02262479 0.03882208
 0.03087894 0.03032156 0.0347918  0.02335508 0.02078173 0.01913078
 0.02515805 0.03285435 0.0314192  0.0301416  0.01936268 0.02132568
 0.04681686 0.03274201 0.02652222 0.02556579 0.02644864 0.04289063
 0.03732203 0.02776443 0.03199068 0.023813   0.07432514 0.02241357
 0.01533284 0.02248564 0.02210544 0.03996928 0.04065947 0.01990954
 0.02911273 0.01949878 0.02040031 0.0195986  0.02301185 0.0112683
 0.0315801  0.02230099 0.0212148  0.03001465 0.02460886 0.01928885
 0.02643008 0.03228778 0.02160434 0.03542856 0.02760208 0.03824484
 0.02950701 0.02868257 0.02712972 0.02206493 0.04118985 0.04041024
 0.03117156 0.03293834 0.03049997 0.03189206 0.04922972 0.02625878
 0.04404827 0.05536121 0.04416062 0.02152744 0.0207428  0.02460934
 0.03421792 0.06127054 0.01969766 0.02206031 0.02757784 0.03218896
 0.04135926 0.02904986 0.03058139 0.02861979 0.03821071 0.04162929
 0.02890982 0.0458446  0.03349277 0.02834272]
for model  205 the mean error 0.029963792127722613
all id 205 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 31 out win 4 err 0.029963792127722613
Launcher: Job 206 completed in 10941 seconds.
Launcher: Task 226 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  107025
Epoch:0, Train loss:0.299547, valid loss:0.293055
Epoch:1, Train loss:0.025342, valid loss:0.005708
Epoch:2, Train loss:0.007339, valid loss:0.003981
Epoch:3, Train loss:0.004655, valid loss:0.002296
Epoch:4, Train loss:0.003798, valid loss:0.002481
Epoch:5, Train loss:0.003510, valid loss:0.001728
Epoch:6, Train loss:0.003364, valid loss:0.001767
Epoch:7, Train loss:0.003118, valid loss:0.002071
Epoch:8, Train loss:0.002991, valid loss:0.001531
Epoch:9, Train loss:0.002782, valid loss:0.001522
Epoch:10, Train loss:0.002785, valid loss:0.001646
Epoch:11, Train loss:0.001789, valid loss:0.001078
Epoch:12, Train loss:0.001825, valid loss:0.001197
Epoch:13, Train loss:0.001828, valid loss:0.001077
Epoch:14, Train loss:0.005248, valid loss:0.001414
Epoch:15, Train loss:0.002344, valid loss:0.001090
Epoch:16, Train loss:0.002046, valid loss:0.000973
Epoch:17, Train loss:0.001846, valid loss:0.001177
Epoch:18, Train loss:0.001778, valid loss:0.000981
Epoch:19, Train loss:0.001712, valid loss:0.000976
Epoch:20, Train loss:0.001663, valid loss:0.001304
Epoch:21, Train loss:0.001218, valid loss:0.000933
Epoch:22, Train loss:0.001253, valid loss:0.000766
Epoch:23, Train loss:0.001173, valid loss:0.000806
Epoch:24, Train loss:0.001199, valid loss:0.000860
Epoch:25, Train loss:0.001184, valid loss:0.000730
Epoch:26, Train loss:0.001145, valid loss:0.000743
Epoch:27, Train loss:0.001130, valid loss:0.000636
Epoch:28, Train loss:0.001153, valid loss:0.000946
Epoch:29, Train loss:0.001093, valid loss:0.000855
Epoch:30, Train loss:0.001114, valid loss:0.000874
Epoch:31, Train loss:0.000854, valid loss:0.000602
Epoch:32, Train loss:0.000854, valid loss:0.000597
Epoch:33, Train loss:0.000875, valid loss:0.000632
Epoch:34, Train loss:0.000828, valid loss:0.000704
Epoch:35, Train loss:0.000858, valid loss:0.000599
Epoch:36, Train loss:0.000832, valid loss:0.000615
Epoch:37, Train loss:0.000835, valid loss:0.000662
Epoch:38, Train loss:0.000839, valid loss:0.000635
Epoch:39, Train loss:0.000813, valid loss:0.000649
Epoch:40, Train loss:0.000821, valid loss:0.000652
Epoch:41, Train loss:0.000680, valid loss:0.000554
Epoch:42, Train loss:0.000675, valid loss:0.000556
Epoch:43, Train loss:0.000684, valid loss:0.000524
Epoch:44, Train loss:0.000679, valid loss:0.000542
Epoch:45, Train loss:0.000665, valid loss:0.000540
Epoch:46, Train loss:0.000681, valid loss:0.000589
Epoch:47, Train loss:0.000664, valid loss:0.000557
Epoch:48, Train loss:0.000691, valid loss:0.000542
Epoch:49, Train loss:0.000651, valid loss:0.000530
Epoch:50, Train loss:0.000666, valid loss:0.000541
Epoch:51, Train loss:0.000601, valid loss:0.000549
Epoch:52, Train loss:0.000590, valid loss:0.000531
Epoch:53, Train loss:0.000605, valid loss:0.000511
Epoch:54, Train loss:0.000595, valid loss:0.000537
Epoch:55, Train loss:0.000593, valid loss:0.000497
Epoch:56, Train loss:0.000589, valid loss:0.000529
Epoch:57, Train loss:0.000589, valid loss:0.000500
Epoch:58, Train loss:0.000589, valid loss:0.000515
Epoch:59, Train loss:0.000585, valid loss:0.000504
Epoch:60, Train loss:0.000586, valid loss:0.000492
training time 10804.59197807312
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.02511506308602877
plot_id,batch_id 0 1 miss% 0.017999022975413684
plot_id,batch_id 0 2 miss% 0.03192122943939504
plot_id,batch_id 0 3 miss% 0.02328456340866008
plot_id,batch_id 0 4 miss% 0.025779924077120576
plot_id,batch_id 0 5 miss% 0.03631009957054599
plot_id,batch_id 0 6 miss% 0.023293127552953232
plot_id,batch_id 0 7 miss% 0.02120312925011485
plot_id,batch_id 0 8 miss% 0.02479612658620472
plot_id,batch_id 0 9 miss% 0.018791366705948817
plot_id,batch_id 0 10 miss% 0.030715831842951364
plot_id,batch_id 0 11 miss% 0.028651677273833793
plot_id,batch_id 0 12 miss% 0.03785785621395337
plot_id,batch_id 0 13 miss% 0.016824041829629703
plot_id,batch_id 0 14 miss% 0.024996943624250974
plot_id,batch_id 0 15 miss% 0.0468127482389558
plot_id,batch_id 0 16 miss% 0.032622062278445316
plot_id,batch_id 0 17 miss% 0.03868566092271526
plot_id,batch_id 0 18 miss% 0.031694959366223796
plot_id,batch_id 0 19 miss% 0.024413429913592133
plot_id,batch_id 0 20 miss% 0.048098049459983
plot_id,batch_id 0 21 miss% 0.01518345185381657
plot_id,batch_id 0 22 miss% 0.0176957704084088
plot_id,batch_id 0 23 miss% 0.01764580641984196
plot_id,batch_id 0 24 miss% 0.023445335540368536
plot_id,batch_id 0 25 miss% 0.03568179774399921
plot_id,batch_id 0 26 miss% 0.030541052268120678
plot_id,batch_id 0 27 miss% 0.02608823992263233
plot_id,batch_id 0 28 miss% 0.018164473645762045
plot_id,batch_id 0 29 miss% 0.020557918121802535
plot_id,batch_id 0 30 miss% 0.03703259023251083
plot_id,batch_id 0 31 miss% 0.03012901420246009
plot_id,batch_id 0 32 miss% 0.023510482541666055
plot_id,batch_id 0 33 miss% 0.031351116672056596
plot_id,batch_id 0 34 miss% 0.01779380798510614
plot_id,batch_id 0 35 miss% 0.03559383028370625
plot_id,batch_id 0 36 miss% 0.038146086049065456
plot_id,batch_id 0 37 miss% 0.0348797488669466
plot_id,batch_id 0 38 miss% 0.025254795797130608
plot_id,batch_id 0 39 miss% 0.022581021922194983
plot_id,batch_id 0 40 miss% 0.08098371804878411
plot_id,batch_id 0 41 miss% 0.024754812380955133
plot_id,batch_id 0 42 miss% 0.014473952836006155
plot_id,batch_id 0 43 miss% 0.029601873010467794
plot_id,batch_id 0 44 miss% 0.016510498521591303
plot_id,batch_id 0 45 miss% 0.04038720987711862
plot_id,batch_id 0 46 miss% 0.02946307516042745
plot_id,batch_id 0 47 miss% 0.01860115098853359
plot_id,batch_id 0 48 miss% 0.019078095739074652
plot_id,batch_id 0 49 miss% 0.018666537377174303
plot_id,batch_id 0 50 miss% 0.022105009979962082
plot_id,batch_id 0 51 miss% 0.019549427509862136
plot_id,batch_id 0 52 miss% 0.01727805054255569
plot_id,batch_id 0 53 miss% 0.009204776194179837
plot_id,batch_id 0 54 miss% 0.02682938869455464
plot_id,batch_id 0 55 miss% 0.03824799976406691
plot_id,batch_id 0 56 miss% 0.020414480008913823
plot_id,batch_id 0 57 miss% 0.023192363447977534
plot_id,batch_id 0 58 miss% 0.01810682370464826
plot_id,batch_id 0 59 miss% 0.018690202712795274
plot_id,batch_id 0 60 miss% 0.03926062995058131
plot_id,batch_id 0 61 miss% 0.035597987159363485
plot_id,batch_id 0 62 miss% 0.030253854661398866
plot_id,batch_id 0 63 miss% 0.031304661355455586
plot_id,batch_id 0 64 miss% 0.02251334586216575
plot_id,batch_id 0 65 miss% 0.03070925605923353
plot_id,batch_id 0 66 miss% 0.03169975694283467
plot_id,batch_id 0 67 miss% 0.026846818221949183
plot_id,batch_id 0 68 miss% 0.027183433528768338
plot_id,batch_id 0 69 miss% 0.019067279746935367
plot_id,batch_id 0 70 miss% 0.02537681069111176
plot_id,batch_id 0 71 miss% 0.035860035136461
plot_id,batch_id 0 72 miss% 0.022880590990702516
plot_id,batch_id 0 73 miss% 0.032381695392277275
plot_id,batch_id 0 74 miss% 0.03038848273370188
plot_id,batch_id 0 75 miss% 0.03441252842059333
plot_id,batch_id 0 76 miss% 0.04461080835750079
plot_id,batch_id 0 77 miss% 0.022140447358109366
plot_id,batch_id 0 78 miss% 0.03858385383263316
plot_id,batch_id 0 79 miss% 0.04770942008337416
plot_id,batch_id 0 80 miss% 0.05573886762714959
plot_id,batch_id 0 81 miss% 0.020628515505070406
plot_id,batch_id 0 82 miss% 0.027789282829515084
plot_id,batch_id 0 83 miss% 0.027865686226936668
plot_id,batch_id 0 84 miss% 0.023525083811181822
plot_id,batch_id 0 85 miss% 0.06748203535319217
plot_id,batch_id 0 86 miss% 0.02023642584051867
plot_id,batch_id 0 87 miss% 0.02751137544771572
plot_id,batch_id 0 88 miss% 0.03656999167235246
plot_id,batch_id 0 89 miss% 0.020477027850009367
plot_id,batch_id 0 90 miss% 0.053127325407672135
plot_id,batch_id 0 91 miss% 0.035971635246868736
plot_id,batch_id 0 92 miss% 0.02826632996780047
plot_id,batch_id 0 93 miss% 0.02800180998759049
plot_id,batch_id 0 94 miss% 0.02920600340822449
plot_id,batch_id 0 95 miss% 0.04629110586120537
plot_id,batch_id 0 96 miss% 0.02953320566341265
plot_id,batch_id 0 97 miss% 0.04639625113138987
plot_id,batch_id 0 98 miss% 0.0317888618894082
plot_id,batch_id 0 99 miss% 0.028770023227237883
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02511506 0.01799902 0.03192123 0.02328456 0.02577992 0.0363101
 0.02329313 0.02120313 0.02479613 0.01879137 0.03071583 0.02865168
 0.03785786 0.01682404 0.02499694 0.04681275 0.03262206 0.03868566
 0.03169496 0.02441343 0.04809805 0.01518345 0.01769577 0.01764581
 0.02344534 0.0356818  0.03054105 0.02608824 0.01816447 0.02055792
 0.03703259 0.03012901 0.02351048 0.03135112 0.01779381 0.03559383
 0.03814609 0.03487975 0.0252548  0.02258102 0.08098372 0.02475481
 0.01447395 0.02960187 0.0165105  0.04038721 0.02946308 0.01860115
 0.0190781  0.01866654 0.02210501 0.01954943 0.01727805 0.00920478
 0.02682939 0.038248   0.02041448 0.02319236 0.01810682 0.0186902
 0.03926063 0.03559799 0.03025385 0.03130466 0.02251335 0.03070926
 0.03169976 0.02684682 0.02718343 0.01906728 0.02537681 0.03586004
 0.02288059 0.0323817  0.03038848 0.03441253 0.04461081 0.02214045
 0.03858385 0.04770942 0.05573887 0.02062852 0.02778928 0.02786569
 0.02352508 0.06748204 0.02023643 0.02751138 0.03656999 0.02047703
 0.05312733 0.03597164 0.02826633 0.02800181 0.029206   0.04629111
 0.02953321 0.04639625 0.03178886 0.02877002]
for model  232 the mean error 0.029312412410337697
all id 232 hidden_dim 32 learning_rate 0.02 num_layers 4 frames 31 out win 4 err 0.029312412410337697
Launcher: Job 233 completed in 10992 seconds.
Launcher: Task 175 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  134801
Epoch:0, Train loss:0.353995, valid loss:0.355328
Epoch:1, Train loss:0.227537, valid loss:0.230247
Epoch:2, Train loss:0.219603, valid loss:0.229428
Epoch:3, Train loss:0.218490, valid loss:0.229409
Epoch:4, Train loss:0.218046, valid loss:0.228906
Epoch:5, Train loss:0.217835, valid loss:0.229110
Epoch:6, Train loss:0.217652, valid loss:0.229201
Epoch:7, Train loss:0.217537, valid loss:0.228739
Epoch:8, Train loss:0.217440, valid loss:0.228811
Epoch:9, Train loss:0.217343, valid loss:0.228620
Epoch:10, Train loss:0.217285, valid loss:0.228608
Epoch:11, Train loss:0.216765, valid loss:0.228328
Epoch:12, Train loss:0.216737, valid loss:0.228604
Epoch:13, Train loss:0.216728, valid loss:0.228518
Epoch:14, Train loss:0.216711, valid loss:0.228532
Epoch:15, Train loss:0.216704, valid loss:0.228369
Epoch:16, Train loss:0.216638, valid loss:0.228473
Epoch:17, Train loss:0.216671, valid loss:0.228400
Epoch:18, Train loss:0.216595, valid loss:0.228445
Epoch:19, Train loss:0.216583, valid loss:0.228341
Epoch:20, Train loss:0.216589, valid loss:0.228301
Epoch:21, Train loss:0.216341, valid loss:0.228186
Epoch:22, Train loss:0.216340, valid loss:0.228332
Epoch:23, Train loss:0.216356, valid loss:0.228295
Epoch:24, Train loss:0.216333, valid loss:0.228237
Epoch:25, Train loss:0.216324, valid loss:0.228182
Epoch:26, Train loss:0.216319, valid loss:0.228538
Epoch:27, Train loss:0.216314, valid loss:0.228173
Epoch:28, Train loss:0.216273, valid loss:0.228248
Epoch:29, Train loss:0.216288, valid loss:0.228270
Epoch:30, Train loss:0.216287, valid loss:0.228182
Epoch:31, Train loss:0.216152, valid loss:0.228147
Epoch:32, Train loss:0.216159, valid loss:0.228161
Epoch:33, Train loss:0.216156, valid loss:0.228131
Epoch:34, Train loss:0.216148, valid loss:0.228192
Epoch:35, Train loss:0.216147, valid loss:0.228119
Epoch:36, Train loss:0.216141, valid loss:0.228233
Epoch:37, Train loss:0.216148, valid loss:0.228123
Epoch:38, Train loss:0.216131, valid loss:0.228122
Epoch:39, Train loss:0.216142, valid loss:0.228129
Epoch:40, Train loss:0.216131, valid loss:0.228139
Epoch:41, Train loss:0.216074, valid loss:0.228122
Epoch:42, Train loss:0.216069, valid loss:0.228118
Epoch:43, Train loss:0.216069, valid loss:0.228133
Epoch:44, Train loss:0.216072, valid loss:0.228110
Epoch:45, Train loss:0.216068, valid loss:0.228123
Epoch:46, Train loss:0.216071, valid loss:0.228175
Epoch:47, Train loss:0.216064, valid loss:0.228115
Epoch:48, Train loss:0.216062, valid loss:0.228104
Epoch:49, Train loss:0.216066, valid loss:0.228124
Epoch:50, Train loss:0.216050, valid loss:0.228102
Epoch:51, Train loss:0.216034, valid loss:0.228096
Epoch:52, Train loss:0.216031, valid loss:0.228095
Epoch:53, Train loss:0.216032, valid loss:0.228099
Epoch:54, Train loss:0.216031, valid loss:0.228121
Epoch:55, Train loss:0.216032, valid loss:0.228103
Epoch:56, Train loss:0.216029, valid loss:0.228082
Epoch:57, Train loss:0.216028, valid loss:0.228103
Epoch:58, Train loss:0.216027, valid loss:0.228100
Epoch:59, Train loss:0.216027, valid loss:0.228104
Epoch:60, Train loss:0.216024, valid loss:0.228116
training time 10845.732947826385
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.656161967158598
plot_id,batch_id 0 1 miss% 0.7419373311729051
plot_id,batch_id 0 2 miss% 0.7587528095711186
plot_id,batch_id 0 3 miss% 0.7695181947223974
plot_id,batch_id 0 4 miss% 0.7676966152684699
plot_id,batch_id 0 5 miss% 0.6627546024413126
plot_id,batch_id 0 6 miss% 0.7426831479736049
plot_id,batch_id 0 7 miss% 0.7558934677188305
plot_id,batch_id 0 8 miss% 0.7652429396999508
plot_id,batch_id 0 9 miss% 0.7709398981455323
plot_id,batch_id 0 10 miss% 0.6326044516245237
plot_id,batch_id 0 11 miss% 0.7433315085310256
plot_id,batch_id 0 12 miss% 0.7531508553574257
plot_id,batch_id 0 13 miss% 0.7615164761141967
plot_id,batch_id 0 14 miss% 0.770394202016643
plot_id,batch_id 0 15 miss% 0.6413717890101215
plot_id,batch_id 0 16 miss% 0.7348133093627709
plot_id,batch_id 0 17 miss% 0.7581002847304205
plot_id,batch_id 0 18 miss% 0.7633375413195873
plot_id,batch_id 0 19 miss% 0.7646968611809661
plot_id,batch_id 0 20 miss% 0.710782779035342
plot_id,batch_id 0 21 miss% 0.7603414473534432
plot_id,batch_id 0 22 miss% 0.7703907659791758
plot_id,batch_id 0 23 miss% 0.775137406046557
plot_id,batch_id 0 24 miss% 0.7772112539035322
plot_id,batch_id 0 25 miss% 0.6977418829813613
plot_id,batch_id 0 26 miss% 0.7550873880319164
plot_id,batch_id 0 27 miss% 0.7686776774067128
plot_id,batch_id 0 28 miss% 0.769235588867501
plot_id,batch_id 0 29 miss% 0.7767158952269415
plot_id,batch_id 0 30 miss% 0.7004395669267215
plot_id,batch_id 0 31 miss% 0.7517883997963418
plot_id,batch_id 0 32 miss% 0.763226201661246
plot_id,batch_id 0 33 miss% 0.7695350533796352
plot_id,batch_id 0 34 miss% 0.7712413528520401
plot_id,batch_id 0 35 miss% 0.6909771382464093
plot_id,batch_id 0 36 miss% 0.7551955314690879
plot_id,batch_id 0 37 miss% 0.762646155467708
plot_id,batch_id 0 38 miss% 0.770440212238683
plot_id,batch_id 0 39 miss% 0.7702760828130741
plot_id,batch_id 0 40 miss% 0.7437246054912727
plot_id,batch_id 0 41 miss% 0.7667835407901671
plot_id,batch_id 0 42 miss% 0.7688746391417213
plot_id,batch_id 0 43 miss% 0.7758738609367049
plot_id,batch_id 0 44 miss% 0.7835720841057681
plot_id,batch_id 0 45 miss% 0.7287127363820659
plot_id,batch_id 0 46 miss% 0.7667538946403645
plot_id,batch_id 0 47 miss% 0.7705638379490349
plot_id,batch_id 0 48 miss% 0.7762005960615612
plot_id,batch_id 0 49 miss% 0.7816318837943307
plot_id,batch_id 0 50 miss% 0.7396689784615059
plot_id,batch_id 0 51 miss% 0.7645338735885021
plot_id,batch_id 0 52 miss% 0.770126605790121
plot_id,batch_id 0 53 miss% 0.7759174735786275
plot_id,batch_id 0 54 miss% 0.7843557193027697
plot_id,batch_id 0 55 miss% 0.7371364066419088
plot_id,batch_id 0 56 miss% 0.7638755843931875
plot_id,batch_id 0 57 miss% 0.7703095534131227
plot_id,batch_id 0 58 miss% 0.7760406963152264
plot_id,batch_id 0 59 miss% 0.7809732604352401
plot_id,batch_id 0 60 miss% 0.5595483245215238
plot_id,batch_id 0 61 miss% 0.7013217055317398
plot_id,batch_id 0 62 miss% 0.7300734574259388
plot_id,batch_id 0 63 miss% 0.7511560137732253
plot_id,batch_id 0 64 miss% 0.7585549172477217
plot_id,batch_id 0 65 miss% 0.5620851187227837
plot_id,batch_id 0 66 miss% 0.6939993909539126
plot_id,batch_id 0 67 miss% 0.7236129275344273
plot_id,batch_id 0 68 miss% 0.7484174444513962
plot_id,batch_id 0 69 miss% 0.7495097946813283
plot_id,batch_id 0 70 miss% 0.5272524624398717
plot_id,batch_id 0 71 miss% 0.7020330820714732
plot_id,batch_id 0 72 miss% 0.7163206184533979
plot_id,batch_id 0 73 miss% 0.7374262402641386
plot_id,batch_id 0 74 miss% 0.7449330245498934
plot_id,batch_id 0 75 miss% 0.5113178396972456
plot_id,batch_id 0 76 miss% 0.6517570728855927
plot_id,batch_id 0 77 miss% 0.6983194380053152
plot_id,batch_id 0 78 miss% 0.731351240648757
plot_id,batch_id 0 79 miss% 0.7408767256481371
plot_id,batch_id 0 80 miss% 0.5936719184702323
plot_id,batch_id 0 81 miss% 0.7219468236451428
plot_id,batch_id 0 82 miss% 0.7445895542923125
plot_id,batch_id 0 83 miss% 0.7549605054140974
plot_id,batch_id 0 84 miss% 0.7628759039134893
plot_id,batch_id 0 85 miss% 0.5864856951745115
plot_id,batch_id 0 86 miss% 0.7135617131189709
plot_id,batch_id 0 87 miss% 0.7394393050660664
plot_id,batch_id 0 88 miss% 0.7586023089858184
plot_id,batch_id 0 89 miss% 0.7618550225895676
plot_id,batch_id 0 90 miss% 0.5492759536051967
plot_id,batch_id 0 91 miss% 0.7101571628053817
plot_id,batch_id 0 92 miss% 0.7336664786921508
plot_id,batch_id 0 93 miss% 0.7442873178589885
plot_id,batch_id 0 94 miss% 0.7588152798743361
plot_id,batch_id 0 95 miss% 0.5456890176592161
plot_id,batch_id 0 96 miss% 0.6985019229492367
plot_id,batch_id 0 97 miss% 0.7268634603236915
plot_id,batch_id 0 98 miss% 0.743758761880303
plot_id,batch_id 0 99 miss% 0.750455962443882
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.65616197 0.74193733 0.75875281 0.76951819 0.76769662 0.6627546
 0.74268315 0.75589347 0.76524294 0.7709399  0.63260445 0.74333151
 0.75315086 0.76151648 0.7703942  0.64137179 0.73481331 0.75810028
 0.76333754 0.76469686 0.71078278 0.76034145 0.77039077 0.77513741
 0.77721125 0.69774188 0.75508739 0.76867768 0.76923559 0.7767159
 0.70043957 0.7517884  0.7632262  0.76953505 0.77124135 0.69097714
 0.75519553 0.76264616 0.77044021 0.77027608 0.74372461 0.76678354
 0.76887464 0.77587386 0.78357208 0.72871274 0.76675389 0.77056384
 0.7762006  0.78163188 0.73966898 0.76453387 0.77012661 0.77591747
 0.78435572 0.73713641 0.76387558 0.77030955 0.7760407  0.78097326
 0.55954832 0.70132171 0.73007346 0.75115601 0.75855492 0.56208512
 0.69399939 0.72361293 0.74841744 0.74950979 0.52725246 0.70203308
 0.71632062 0.73742624 0.74493302 0.51131784 0.65175707 0.69831944
 0.73135124 0.74087673 0.59367192 0.72194682 0.74458955 0.75496051
 0.7628759  0.5864857  0.71356171 0.73943931 0.75860231 0.76185502
 0.54927595 0.71015716 0.73366648 0.74428732 0.75881528 0.54568902
 0.69850192 0.72686346 0.74375876 0.75045596]
for model  187 the mean error 0.7294694277228743
all id 187 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 31 out win 4 err 0.7294694277228743
Launcher: Job 188 completed in 11020 seconds.
Launcher: Task 86 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  77489
Epoch:0, Train loss:0.342961, valid loss:0.338302
Epoch:1, Train loss:0.021624, valid loss:0.004655
Epoch:2, Train loss:0.005709, valid loss:0.002972
Epoch:3, Train loss:0.004277, valid loss:0.002063
Epoch:4, Train loss:0.003561, valid loss:0.001754
Epoch:5, Train loss:0.003384, valid loss:0.001889
Epoch:6, Train loss:0.003019, valid loss:0.001570
Epoch:7, Train loss:0.002838, valid loss:0.001334
Epoch:8, Train loss:0.002906, valid loss:0.001649
Epoch:9, Train loss:0.002700, valid loss:0.001405
Epoch:10, Train loss:0.002609, valid loss:0.001232
Epoch:11, Train loss:0.001823, valid loss:0.001168
Epoch:12, Train loss:0.001808, valid loss:0.001081
Epoch:13, Train loss:0.001749, valid loss:0.000906
Epoch:14, Train loss:0.001742, valid loss:0.001003
Epoch:15, Train loss:0.001709, valid loss:0.001396
Epoch:16, Train loss:0.001732, valid loss:0.000917
Epoch:17, Train loss:0.001661, valid loss:0.000917
Epoch:18, Train loss:0.001613, valid loss:0.000930
Epoch:19, Train loss:0.001640, valid loss:0.000974
Epoch:20, Train loss:0.001583, valid loss:0.001090
Epoch:21, Train loss:0.001198, valid loss:0.000911
Epoch:22, Train loss:0.001191, valid loss:0.000811
Epoch:23, Train loss:0.001192, valid loss:0.000945
Epoch:24, Train loss:0.001182, valid loss:0.000764
Epoch:25, Train loss:0.001184, valid loss:0.000711
Epoch:26, Train loss:0.001145, valid loss:0.000722
Epoch:27, Train loss:0.001159, valid loss:0.000690
Epoch:28, Train loss:0.001131, valid loss:0.000720
Epoch:29, Train loss:0.001127, valid loss:0.000689
Epoch:30, Train loss:0.001131, valid loss:0.000745
Epoch:31, Train loss:0.000905, valid loss:0.000612
Epoch:32, Train loss:0.000909, valid loss:0.000685
Epoch:33, Train loss:0.000896, valid loss:0.000666
Epoch:34, Train loss:0.000895, valid loss:0.000707
Epoch:35, Train loss:0.000882, valid loss:0.000678
Epoch:36, Train loss:0.000891, valid loss:0.000651
Epoch:37, Train loss:0.000898, valid loss:0.000636
Epoch:38, Train loss:0.000871, valid loss:0.000660
Epoch:39, Train loss:0.000876, valid loss:0.000596
Epoch:40, Train loss:0.000854, valid loss:0.000656
Epoch:41, Train loss:0.000765, valid loss:0.000608
Epoch:42, Train loss:0.000755, valid loss:0.000598
Epoch:43, Train loss:0.000742, valid loss:0.000588
Epoch:44, Train loss:0.000754, valid loss:0.000617
Epoch:45, Train loss:0.000757, valid loss:0.000595
Epoch:46, Train loss:0.000742, valid loss:0.000593
Epoch:47, Train loss:0.000744, valid loss:0.000633
Epoch:48, Train loss:0.000759, valid loss:0.000595
Epoch:49, Train loss:0.000734, valid loss:0.000599
Epoch:50, Train loss:0.000741, valid loss:0.000569
Epoch:51, Train loss:0.000677, valid loss:0.000579
Epoch:52, Train loss:0.000679, valid loss:0.000572
Epoch:53, Train loss:0.000680, valid loss:0.000585
Epoch:54, Train loss:0.000673, valid loss:0.000590
Epoch:55, Train loss:0.000677, valid loss:0.000580
Epoch:56, Train loss:0.000675, valid loss:0.000566
Epoch:57, Train loss:0.000668, valid loss:0.000594
Epoch:58, Train loss:0.000669, valid loss:0.000583
Epoch:59, Train loss:0.000667, valid loss:0.000596
Epoch:60, Train loss:0.000669, valid loss:0.000580
training time 11146.455704450607
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.02801591017219895
plot_id,batch_id 0 1 miss% 0.028318907980419524
plot_id,batch_id 0 2 miss% 0.031000500752893786
plot_id,batch_id 0 3 miss% 0.027985547409817117
plot_id,batch_id 0 4 miss% 0.028864795635511996
plot_id,batch_id 0 5 miss% 0.03412492860536919
plot_id,batch_id 0 6 miss% 0.021989757634906255
plot_id,batch_id 0 7 miss% 0.03126810518745464
plot_id,batch_id 0 8 miss% 0.023652794155113555
plot_id,batch_id 0 9 miss% 0.023537161574881567
plot_id,batch_id 0 10 miss% 0.03724694939378421
plot_id,batch_id 0 11 miss% 0.03712542525160578
plot_id,batch_id 0 12 miss% 0.03120058222447246
plot_id,batch_id 0 13 miss% 0.01742806423730556
plot_id,batch_id 0 14 miss% 0.030384210719336464
plot_id,batch_id 0 15 miss% 0.04844492116639425
plot_id,batch_id 0 16 miss% 0.024985891287896578
plot_id,batch_id 0 17 miss% 0.03889515282728933
plot_id,batch_id 0 18 miss% 0.03252350291752516
plot_id,batch_id 0 19 miss% 0.03607493094847855
plot_id,batch_id 0 20 miss% 0.04728436844458622
plot_id,batch_id 0 21 miss% 0.023802023728470997
plot_id,batch_id 0 22 miss% 0.031288654945930854
plot_id,batch_id 0 23 miss% 0.02581929683463824
plot_id,batch_id 0 24 miss% 0.03679479531051376
plot_id,batch_id 0 25 miss% 0.03950056632079601
plot_id,batch_id 0 26 miss% 0.028546135483319784
plot_id,batch_id 0 27 miss% 0.027110761929362917
plot_id,batch_id 0 28 miss% 0.03419566329717645
plot_id,batch_id 0 29 miss% 0.04052129601709432
plot_id,batch_id 0 30 miss% 0.03554674525987223
plot_id,batch_id 0 31 miss% 0.03527202639063429
plot_id,batch_id 0 32 miss% 0.024644595352711074
plot_id,batch_id 0 33 miss% 0.03035976370814185
plot_id,batch_id 0 34 miss% 0.03493576421889891
plot_id,batch_id 0 35 miss% 0.05666258937661394
plot_id,batch_id 0 36 miss% 0.034610942268496654
plot_id,batch_id 0 37 miss% 0.03559766931821399
plot_id,batch_id 0 38 miss% 0.02326434588653036
plot_id,batch_id 0 39 miss% 0.01965963790743414
plot_id,batch_id 0 40 miss% 0.05047114373079973
plot_id,batch_id 0 41 miss% 0.026983404203882427
plot_id,batch_id 0 42 miss% 0.01495097236638273
plot_id,batch_id 0 43 miss% 0.03855947657865358
plot_id,batch_id 0 44 miss% 0.031128856088438458
plot_id,batch_id 0 45 miss% 0.02298426854278845
plot_id,batch_id 0 46 miss% 0.022578817820132605
plot_id,batch_id 0 47 miss% 0.02351728580039297
plot_id,batch_id 0 48 miss% 0.03852172790022033
plot_id,batch_id 0 49 miss% 0.033771113934109816
plot_id,batch_id 0 50 miss% 0.017537078232843093
plot_id,batch_id 0 51 miss% 0.028163535445854603
plot_id,batch_id 0 52 miss% 0.02010365995930819
plot_id,batch_id 0 53 miss% 0.024192568868999426
plot_id,batch_id 0 54 miss% 0.03459908241966954
plot_id,batch_id 0 55 miss% 0.030532138642559277
plot_id,batch_id 0 56 miss% 0.028029652565930117
plot_id,batch_id 0 57 miss% 0.024366800638442633
plot_id,batch_id 0 58 miss% 0.022659307342810173
plot_id,batch_id 0 59 miss% 0.03743174648284932
plot_id,batch_id 0 60 miss% 0.05234593652529552
plot_id,batch_id 0 61 miss% 0.03082113919834679
plot_id,batch_id 0 62 miss% 0.030440548692731408
plot_id,batch_id 0 63 miss% 0.03908077722284948
plot_id,batch_id 0 64 miss% 0.032726455046446494
plot_id,batch_id 0 65 miss% 0.03293724106979912
plot_id,batch_id 0 66 miss% 0.025260929739946203
plot_id,batch_id 0 67 miss% 0.030837970208994637
plot_id,batch_id 0 68 miss% 0.02432414421907829
plot_id,batch_id 0 69 miss% 0.018619619385317865
plot_id,batch_id 0 70 miss% 0.029999437259442124
plot_id,batch_id 0 71 miss% 0.042100073206268636
plot_id,batch_id 0 72 miss% 0.028145683546645456
plot_id,batch_id 0 73 miss% 0.03302497160309847
plot_id,batch_id 0 74 miss% 0.028585642375502252
plot_id,batch_id 0 75 miss% 0.035864085993069686
plot_id,batch_id 0 76 miss% 0.03327244123525934
plot_id,batch_id 0 77 miss% 0.032449558777850225
plot_id,batch_id 0 78 miss% 0.039087039125231135
plot_id,batch_id 0 79 miss% 0.05236506215203664
plot_id,batch_id 0 80 miss% 0.04252826305709599
plot_id,batch_id 0 81 miss% 0.032370989186672604
plot_id,batch_id 0 82 miss% 0.03676375241981839
plot_id,batch_id 0 83 miss% 0.028556610099422827
plot_id,batch_id 0 84 miss% 0.04281249359449664
plot_id,batch_id 0 85 miss% 0.03495648378918283
plot_id,batch_id 0 86 miss% 0.026586162885354366
plot_id,batch_id 0 87 miss% 0.026584268514269184
plot_id,batch_id 0 88 miss% 0.03319765888227323
plot_id,batch_id 0 89 miss% 0.03798250081057711
plot_id,batch_id 0 90 miss% 0.02772420633892427
plot_id,batch_id 0 91 miss% 0.026574113351422318
plot_id,batch_id 0 92 miss% 0.03427654517130468
plot_id,batch_id 0 93 miss% 0.023813527366578685
plot_id,batch_id 0 94 miss% 0.03465672727738241
plot_id,batch_id 0 95 miss% 0.03224452369173816
plot_id,batch_id 0 96 miss% 0.028831609867562735
plot_id,batch_id 0 97 miss% 0.041049699730832064
plot_id,batch_id 0 98 miss% 0.028624668301146836
plot_id,batch_id 0 99 miss% 0.026119918136967766
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02801591 0.02831891 0.0310005  0.02798555 0.0288648  0.03412493
 0.02198976 0.03126811 0.02365279 0.02353716 0.03724695 0.03712543
 0.03120058 0.01742806 0.03038421 0.04844492 0.02498589 0.03889515
 0.0325235  0.03607493 0.04728437 0.02380202 0.03128865 0.0258193
 0.0367948  0.03950057 0.02854614 0.02711076 0.03419566 0.0405213
 0.03554675 0.03527203 0.0246446  0.03035976 0.03493576 0.05666259
 0.03461094 0.03559767 0.02326435 0.01965964 0.05047114 0.0269834
 0.01495097 0.03855948 0.03112886 0.02298427 0.02257882 0.02351729
 0.03852173 0.03377111 0.01753708 0.02816354 0.02010366 0.02419257
 0.03459908 0.03053214 0.02802965 0.0243668  0.02265931 0.03743175
 0.05234594 0.03082114 0.03044055 0.03908078 0.03272646 0.03293724
 0.02526093 0.03083797 0.02432414 0.01861962 0.02999944 0.04210007
 0.02814568 0.03302497 0.02858564 0.03586409 0.03327244 0.03244956
 0.03908704 0.05236506 0.04252826 0.03237099 0.03676375 0.02855661
 0.04281249 0.03495648 0.02658616 0.02658427 0.03319766 0.0379825
 0.02772421 0.02657411 0.03427655 0.02381353 0.03465673 0.03224452
 0.02883161 0.0410497  0.02862467 0.02611992]
for model  212 the mean error 0.03168111798703392
all id 212 hidden_dim 24 learning_rate 0.01 num_layers 5 frames 31 out win 5 err 0.03168111798703392
Launcher: Job 213 completed in 11341 seconds.
Launcher: Task 192 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 24 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  77489
Epoch:0, Train loss:0.342961, valid loss:0.338302
Epoch:1, Train loss:0.026888, valid loss:0.004175
Epoch:2, Train loss:0.006668, valid loss:0.003614
Epoch:3, Train loss:0.004481, valid loss:0.002110
Epoch:4, Train loss:0.003585, valid loss:0.001871
Epoch:5, Train loss:0.003225, valid loss:0.001589
Epoch:6, Train loss:0.002997, valid loss:0.001615
Epoch:7, Train loss:0.002748, valid loss:0.001341
Epoch:8, Train loss:0.002576, valid loss:0.001531
Epoch:9, Train loss:0.002488, valid loss:0.001189
Epoch:10, Train loss:0.002439, valid loss:0.001205
Epoch:11, Train loss:0.001795, valid loss:0.001049
Epoch:12, Train loss:0.001777, valid loss:0.000986
Epoch:13, Train loss:0.001773, valid loss:0.001377
Epoch:14, Train loss:0.001691, valid loss:0.000997
Epoch:15, Train loss:0.001685, valid loss:0.000965
Epoch:16, Train loss:0.001670, valid loss:0.000899
Epoch:17, Train loss:0.001616, valid loss:0.000988
Epoch:18, Train loss:0.001582, valid loss:0.000850
Epoch:19, Train loss:0.001570, valid loss:0.000885
Epoch:20, Train loss:0.001510, valid loss:0.001101
Epoch:21, Train loss:0.001215, valid loss:0.000902
Epoch:22, Train loss:0.001227, valid loss:0.000820
Epoch:23, Train loss:0.001212, valid loss:0.000825
Epoch:24, Train loss:0.001188, valid loss:0.000812
Epoch:25, Train loss:0.001163, valid loss:0.000719
Epoch:26, Train loss:0.001154, valid loss:0.000739
Epoch:27, Train loss:0.001152, valid loss:0.000849
Epoch:28, Train loss:0.001151, valid loss:0.000751
Epoch:29, Train loss:0.001135, valid loss:0.000836
Epoch:30, Train loss:0.001108, valid loss:0.000783
Epoch:31, Train loss:0.000954, valid loss:0.000668
Epoch:32, Train loss:0.000938, valid loss:0.000659
Epoch:33, Train loss:0.000933, valid loss:0.000657
Epoch:34, Train loss:0.000939, valid loss:0.000712
Epoch:35, Train loss:0.000924, valid loss:0.000649
Epoch:36, Train loss:0.000919, valid loss:0.000690
Epoch:37, Train loss:0.000930, valid loss:0.000651
Epoch:38, Train loss:0.000895, valid loss:0.000735
Epoch:39, Train loss:0.000919, valid loss:0.000695
Epoch:40, Train loss:0.000913, valid loss:0.000665
Epoch:41, Train loss:0.000807, valid loss:0.000636
Epoch:42, Train loss:0.000807, valid loss:0.000597
Epoch:43, Train loss:0.000817, valid loss:0.000623
Epoch:44, Train loss:0.000797, valid loss:0.000636
Epoch:45, Train loss:0.000811, valid loss:0.000661
Epoch:46, Train loss:0.000806, valid loss:0.000611
Epoch:47, Train loss:0.000798, valid loss:0.000620
Epoch:48, Train loss:0.000796, valid loss:0.000616
Epoch:49, Train loss:0.000791, valid loss:0.000616
Epoch:50, Train loss:0.000806, valid loss:0.000588
Epoch:51, Train loss:0.000743, valid loss:0.000598
Epoch:52, Train loss:0.000741, valid loss:0.000614
Epoch:53, Train loss:0.000745, valid loss:0.000596
Epoch:54, Train loss:0.000747, valid loss:0.000604
Epoch:55, Train loss:0.000743, valid loss:0.000606
Epoch:56, Train loss:0.000736, valid loss:0.000595
Epoch:57, Train loss:0.000735, valid loss:0.000597
Epoch:58, Train loss:0.000737, valid loss:0.000603
Epoch:59, Train loss:0.000732, valid loss:0.000601
Epoch:60, Train loss:0.000734, valid loss:0.000595
training time 11352.981291532516
total number of trained parameters for initialize model 77489
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.028535633575493137
plot_id,batch_id 0 1 miss% 0.02184681522691113
plot_id,batch_id 0 2 miss% 0.025008166978889128
plot_id,batch_id 0 3 miss% 0.026401376992667386
plot_id,batch_id 0 4 miss% 0.02239845771273045
plot_id,batch_id 0 5 miss% 0.029786410685521347
plot_id,batch_id 0 6 miss% 0.02964222152485291
plot_id,batch_id 0 7 miss% 0.02734480285742403
plot_id,batch_id 0 8 miss% 0.0317322429604294
plot_id,batch_id 0 9 miss% 0.017613601857825546
plot_id,batch_id 0 10 miss% 0.030088644358402344
plot_id,batch_id 0 11 miss% 0.029576665030932467
plot_id,batch_id 0 12 miss% 0.024923475577581376
plot_id,batch_id 0 13 miss% 0.02706013332421269
plot_id,batch_id 0 14 miss% 0.034500976304269106
plot_id,batch_id 0 15 miss% 0.054242812434350786
plot_id,batch_id 0 16 miss% 0.027516252891400014
plot_id,batch_id 0 17 miss% 0.03719137775092194
plot_id,batch_id 0 18 miss% 0.03608268186989082
plot_id,batch_id 0 19 miss% 0.029163804653417044
plot_id,batch_id 0 20 miss% 0.05115269652300173
plot_id,batch_id 0 21 miss% 0.01740803457306433
plot_id,batch_id 0 22 miss% 0.026607790494733302
plot_id,batch_id 0 23 miss% 0.02290508415153025
plot_id,batch_id 0 24 miss% 0.0260744411970292
plot_id,batch_id 0 25 miss% 0.045261647253781324
plot_id,batch_id 0 26 miss% 0.02308298548044482
plot_id,batch_id 0 27 miss% 0.029453579008931172
plot_id,batch_id 0 28 miss% 0.03268342527799505
plot_id,batch_id 0 29 miss% 0.03697036893288848
plot_id,batch_id 0 30 miss% 0.032195998099087096
plot_id,batch_id 0 31 miss% 0.03525483387506733
plot_id,batch_id 0 32 miss% 0.024271287750235737
plot_id,batch_id 0 33 miss% 0.037412967317497846
plot_id,batch_id 0 34 miss% 0.03192625631778519
plot_id,batch_id 0 35 miss% 0.05382443743119429
plot_id,batch_id 0 36 miss% 0.0326468734556306
plot_id,batch_id 0 37 miss% 0.02965564300387827
plot_id,batch_id 0 38 miss% 0.01961871517102751
plot_id,batch_id 0 39 miss% 0.020093492359593713
plot_id,batch_id 0 40 miss% 0.0654167117323976
plot_id,batch_id 0 41 miss% 0.02274942137708076
plot_id,batch_id 0 42 miss% 0.020802181050604607
plot_id,batch_id 0 43 miss% 0.02452280628428465
plot_id,batch_id 0 44 miss% 0.016517758224800352
plot_id,batch_id 0 45 miss% 0.027346165042270415
plot_id,batch_id 0 46 miss% 0.02317344178620075
plot_id,batch_id 0 47 miss% 0.029866786744728084
plot_id,batch_id 0 48 miss% 0.02288831082786294
plot_id,batch_id 0 49 miss% 0.02353173660907251
plot_id,batch_id 0 50 miss% 0.022478922231637676
plot_id,batch_id 0 51 miss% 0.019216884991101454
plot_id,batch_id 0 52 miss% 0.03320919424903391
plot_id,batch_id 0 53 miss% 0.01871818977609955
plot_id,batch_id 0 54 miss% 0.02352344361864869
plot_id,batch_id 0 55 miss% 0.03148347526848473
plot_id,batch_id 0 56 miss% 0.029446938982924005
plot_id,batch_id 0 57 miss% 0.033509204849618376
plot_id,batch_id 0 58 miss% 0.02536386034529496
plot_id,batch_id 0 59 miss% 0.024392007459501003
plot_id,batch_id 0 60 miss% 0.040076646258053274
plot_id,batch_id 0 61 miss% 0.01903322106895204
plot_id,batch_id 0 62 miss% 0.022926586279559822
plot_id,batch_id 0 63 miss% 0.02953824589215447
plot_id,batch_id 0 64 miss% 0.03250021553361929
plot_id,batch_id 0 65 miss% 0.02822756122952108
plot_id,batch_id 0 66 miss% 0.04595461191351114
plot_id,batch_id 0 67 miss% 0.03546668993913201
plot_id,batch_id 0 68 miss% 0.025519405200298235
plot_id,batch_id 0 69 miss% 0.021968008092810888
plot_id,batch_id 0 70 miss% 0.027864350661356892
plot_id,batch_id 0 71 miss% 0.05871818667813603
plot_id,batch_id 0 72 miss% 0.03976925367291623
plot_id,batch_id 0 73 miss% 0.03581715099784144
plot_id,batch_id 0 74 miss% 0.04405396515384113
plot_id,batch_id 0 75 miss% 0.03703120142256258
plot_id,batch_id 0 76 miss% 0.04273425192346826
plot_id,batch_id 0 77 miss% 0.040374422592558745
plot_id,batch_id 0 78 miss% 0.03542322098172638
plot_id,batch_id 0 79 miss% 0.046678650893960574
plot_id,batch_id 0 80 miss% 0.03665545088757406
plot_id,batch_id 0 81 miss% 0.029952999817882086
plot_id,batch_id 0 82 miss% 0.026077898831190414
plot_id,batch_id 0 83 miss% 0.027672516292202062
plot_id,batch_id 0 84 miss% 0.021832540296752134
plot_id,batch_id 0 85 miss% 0.05177463142031973
plot_id,batch_id 0 86 miss% 0.021734324089206968
plot_id,batch_id 0 87 miss% 0.026900940507177316
plot_id,batch_id 0 88 miss% 0.03899424862398225
plot_id,batch_id 0 89 miss% 0.03669006259851921
plot_id,batch_id 0 90 miss% 0.025513815108569038
plot_id,batch_id 0 91 miss% 0.030392719651133968
plot_id,batch_id 0 92 miss% 0.022415779816740745
plot_id,batch_id 0 93 miss% 0.03073861267163872
plot_id,batch_id 0 94 miss% 0.040624203595278754
plot_id,batch_id 0 95 miss% 0.040238213873703375
plot_id,batch_id 0 96 miss% 0.034652175728059696
plot_id,batch_id 0 97 miss% 0.05875560644505855
plot_id,batch_id 0 98 miss% 0.03578675480037528
plot_id,batch_id 0 99 miss% 0.02785670234507963
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02853563 0.02184682 0.02500817 0.02640138 0.02239846 0.02978641
 0.02964222 0.0273448  0.03173224 0.0176136  0.03008864 0.02957667
 0.02492348 0.02706013 0.03450098 0.05424281 0.02751625 0.03719138
 0.03608268 0.0291638  0.0511527  0.01740803 0.02660779 0.02290508
 0.02607444 0.04526165 0.02308299 0.02945358 0.03268343 0.03697037
 0.032196   0.03525483 0.02427129 0.03741297 0.03192626 0.05382444
 0.03264687 0.02965564 0.01961872 0.02009349 0.06541671 0.02274942
 0.02080218 0.02452281 0.01651776 0.02734617 0.02317344 0.02986679
 0.02288831 0.02353174 0.02247892 0.01921688 0.03320919 0.01871819
 0.02352344 0.03148348 0.02944694 0.0335092  0.02536386 0.02439201
 0.04007665 0.01903322 0.02292659 0.02953825 0.03250022 0.02822756
 0.04595461 0.03546669 0.02551941 0.02196801 0.02786435 0.05871819
 0.03976925 0.03581715 0.04405397 0.0370312  0.04273425 0.04037442
 0.03542322 0.04667865 0.03665545 0.029953   0.0260779  0.02767252
 0.02183254 0.05177463 0.02173432 0.02690094 0.03899425 0.03669006
 0.02551382 0.03039272 0.02241578 0.03073861 0.0406242  0.04023821
 0.03465218 0.05875561 0.03578675 0.0278567 ]
for model  185 the mean error 0.031322505974805924
all id 185 hidden_dim 24 learning_rate 0.005 num_layers 5 frames 31 out win 5 err 0.031322505974805924
Launcher: Job 186 completed in 11547 seconds.
Launcher: Task 180 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  134801
Epoch:0, Train loss:0.475526, valid loss:0.475468
Epoch:1, Train loss:0.358570, valid loss:0.361146
Epoch:2, Train loss:0.346816, valid loss:0.359818
Epoch:3, Train loss:0.344775, valid loss:0.359195
Epoch:4, Train loss:0.343785, valid loss:0.359103
Epoch:5, Train loss:0.343520, valid loss:0.358735
Epoch:6, Train loss:0.343063, valid loss:0.358783
Epoch:7, Train loss:0.342921, valid loss:0.358766
Epoch:8, Train loss:0.342729, valid loss:0.359223
Epoch:9, Train loss:0.342519, valid loss:0.358599
Epoch:10, Train loss:0.342338, valid loss:0.358355
Epoch:11, Train loss:0.341561, valid loss:0.358143
Epoch:12, Train loss:0.341530, valid loss:0.358384
Epoch:13, Train loss:0.341522, valid loss:0.358039
Epoch:14, Train loss:0.341395, valid loss:0.357909
Epoch:15, Train loss:0.341429, valid loss:0.358170
Epoch:16, Train loss:0.341391, valid loss:0.358118
Epoch:17, Train loss:0.341368, valid loss:0.358149
Epoch:18, Train loss:0.341322, valid loss:0.358129
Epoch:19, Train loss:0.341278, valid loss:0.358130
Epoch:20, Train loss:0.341268, valid loss:0.358045
Epoch:21, Train loss:0.340816, valid loss:0.357832
Epoch:22, Train loss:0.340865, valid loss:0.357835
Epoch:23, Train loss:0.340805, valid loss:0.357763
Epoch:24, Train loss:0.340811, valid loss:0.357847
Epoch:25, Train loss:0.340790, valid loss:0.357803
Epoch:26, Train loss:0.340816, valid loss:0.357867
Epoch:27, Train loss:0.340784, valid loss:0.357736
Epoch:28, Train loss:0.340775, valid loss:0.357809
Epoch:29, Train loss:0.340718, valid loss:0.357802
Epoch:30, Train loss:0.340731, valid loss:0.357831
Epoch:31, Train loss:0.340514, valid loss:0.357671
Epoch:32, Train loss:0.340521, valid loss:0.357688
Epoch:33, Train loss:0.340515, valid loss:0.357703
Epoch:34, Train loss:0.340508, valid loss:0.357706
Epoch:35, Train loss:0.340522, valid loss:0.357663
Epoch:36, Train loss:0.340485, valid loss:0.357677
Epoch:37, Train loss:0.340512, valid loss:0.357692
Epoch:38, Train loss:0.340452, valid loss:0.357744
Epoch:39, Train loss:0.340507, valid loss:0.357672
Epoch:40, Train loss:0.340476, valid loss:0.357852
Epoch:41, Train loss:0.340379, valid loss:0.357632
Epoch:42, Train loss:0.340363, valid loss:0.357658
Epoch:43, Train loss:0.340366, valid loss:0.357606
Epoch:44, Train loss:0.340370, valid loss:0.357635
Epoch:45, Train loss:0.340362, valid loss:0.357624
Epoch:46, Train loss:0.340353, valid loss:0.357652
Epoch:47, Train loss:0.340347, valid loss:0.357605
Epoch:48, Train loss:0.340350, valid loss:0.357602
Epoch:49, Train loss:0.340345, valid loss:0.357611
Epoch:50, Train loss:0.340344, valid loss:0.357631
Epoch:51, Train loss:0.340298, valid loss:0.357618
Epoch:52, Train loss:0.340293, valid loss:0.357606
Epoch:53, Train loss:0.340290, valid loss:0.357592
Epoch:54, Train loss:0.340291, valid loss:0.357620
Epoch:55, Train loss:0.340291, valid loss:0.357612
Epoch:56, Train loss:0.340291, valid loss:0.357632
Epoch:57, Train loss:0.340284, valid loss:0.357596
Epoch:58, Train loss:0.340299, valid loss:0.357593
Epoch:59, Train loss:0.340287, valid loss:0.357601
Epoch:60, Train loss:0.340278, valid loss:0.357616
training time 11667.216866016388
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7806807390485354
plot_id,batch_id 0 1 miss% 0.8298452977940955
plot_id,batch_id 0 2 miss% 0.83458866992579
plot_id,batch_id 0 3 miss% 0.8403113727665249
plot_id,batch_id 0 4 miss% 0.8427744975867328
plot_id,batch_id 0 5 miss% 0.7752839144890693
plot_id,batch_id 0 6 miss% 0.8239633218979199
plot_id,batch_id 0 7 miss% 0.8357028801003114
plot_id,batch_id 0 8 miss% 0.841530947621521
plot_id,batch_id 0 9 miss% 0.8456058702067608
plot_id,batch_id 0 10 miss% 0.7663603807492885
plot_id,batch_id 0 11 miss% 0.822129333945346
plot_id,batch_id 0 12 miss% 0.8327003490930609
plot_id,batch_id 0 13 miss% 0.8374433178576596
plot_id,batch_id 0 14 miss% 0.8399239602354674
plot_id,batch_id 0 15 miss% 0.76782704558215
plot_id,batch_id 0 16 miss% 0.8216712201012055
plot_id,batch_id 0 17 miss% 0.8314356765747771
plot_id,batch_id 0 18 miss% 0.8403315328156946
plot_id,batch_id 0 19 miss% 0.8409049194000545
plot_id,batch_id 0 20 miss% 0.813519076657179
plot_id,batch_id 0 21 miss% 0.8351121143350239
plot_id,batch_id 0 22 miss% 0.8381574944969377
plot_id,batch_id 0 23 miss% 0.8430607816193583
plot_id,batch_id 0 24 miss% 0.8439423428883881
plot_id,batch_id 0 25 miss% 0.8055099157248886
plot_id,batch_id 0 26 miss% 0.834521117747014
plot_id,batch_id 0 27 miss% 0.838827105272133
plot_id,batch_id 0 28 miss% 0.8427758686672063
plot_id,batch_id 0 29 miss% 0.8428958097713185
plot_id,batch_id 0 30 miss% 0.7919792872784611
plot_id,batch_id 0 31 miss% 0.8309530107308801
plot_id,batch_id 0 32 miss% 0.8395013861291277
plot_id,batch_id 0 33 miss% 0.8414876086129401
plot_id,batch_id 0 34 miss% 0.8439783649574927
plot_id,batch_id 0 35 miss% 0.7903877242892424
plot_id,batch_id 0 36 miss% 0.8361760097576001
plot_id,batch_id 0 37 miss% 0.8368657448502177
plot_id,batch_id 0 38 miss% 0.8422084971907344
plot_id,batch_id 0 39 miss% 0.8426675855587905
plot_id,batch_id 0 40 miss% 0.8212520772478226
plot_id,batch_id 0 41 miss% 0.8399838655412111
plot_id,batch_id 0 42 miss% 0.838904105750002
plot_id,batch_id 0 43 miss% 0.8458791110543793
plot_id,batch_id 0 44 miss% 0.8497942593920262
plot_id,batch_id 0 45 miss% 0.8171654980210777
plot_id,batch_id 0 46 miss% 0.8399510922134326
plot_id,batch_id 0 47 miss% 0.841060517622579
plot_id,batch_id 0 48 miss% 0.8449962054569126
plot_id,batch_id 0 49 miss% 0.8482971134411257
plot_id,batch_id 0 50 miss% 0.8260562713793645
plot_id,batch_id 0 51 miss% 0.8373244390350872
plot_id,batch_id 0 52 miss% 0.8415256131467913
plot_id,batch_id 0 53 miss% 0.8451001525056189
plot_id,batch_id 0 54 miss% 0.8453690501814328
plot_id,batch_id 0 55 miss% 0.8185378705855554
plot_id,batch_id 0 56 miss% 0.8387406883996903
plot_id,batch_id 0 57 miss% 0.8415459600173562
plot_id,batch_id 0 58 miss% 0.8454648480283027
plot_id,batch_id 0 59 miss% 0.843023051165008
plot_id,batch_id 0 60 miss% 0.7197380094284898
plot_id,batch_id 0 61 miss% 0.8076559152789456
plot_id,batch_id 0 62 miss% 0.8239135991728033
plot_id,batch_id 0 63 miss% 0.8305214071858923
plot_id,batch_id 0 64 miss% 0.8356411229670012
plot_id,batch_id 0 65 miss% 0.7181465199194496
plot_id,batch_id 0 66 miss% 0.795898612119037
plot_id,batch_id 0 67 miss% 0.8109948233140948
plot_id,batch_id 0 68 miss% 0.8302892147800306
plot_id,batch_id 0 69 miss% 0.8289797746939321
plot_id,batch_id 0 70 miss% 0.6777377863105728
plot_id,batch_id 0 71 miss% 0.8096145953280277
plot_id,batch_id 0 72 miss% 0.8041024750314386
plot_id,batch_id 0 73 miss% 0.8171587858961378
plot_id,batch_id 0 74 miss% 0.8267111042110268
plot_id,batch_id 0 75 miss% 0.6838465214115707
plot_id,batch_id 0 76 miss% 0.7893864896295196
plot_id,batch_id 0 77 miss% 0.7971680385549681
plot_id,batch_id 0 78 miss% 0.8169475767858414
plot_id,batch_id 0 79 miss% 0.8212435526932934
plot_id,batch_id 0 80 miss% 0.7444633476818907
plot_id,batch_id 0 81 miss% 0.8196533738368312
plot_id,batch_id 0 82 miss% 0.8279087661667617
plot_id,batch_id 0 83 miss% 0.8346896424493714
plot_id,batch_id 0 84 miss% 0.837996461862897
plot_id,batch_id 0 85 miss% 0.7324524981702522
plot_id,batch_id 0 86 miss% 0.8114741138020986
plot_id,batch_id 0 87 miss% 0.8268513891667378
plot_id,batch_id 0 88 miss% 0.8352848711137314
plot_id,batch_id 0 89 miss% 0.8355593720344734
plot_id,batch_id 0 90 miss% 0.6995635275458034
plot_id,batch_id 0 91 miss% 0.8011008539668449
plot_id,batch_id 0 92 miss% 0.8217975093612024
plot_id,batch_id 0 93 miss% 0.8224294004134768
plot_id,batch_id 0 94 miss% 0.8363056617715928
plot_id,batch_id 0 95 miss% 0.7232169587928411
plot_id,batch_id 0 96 miss% 0.7939098145391335
plot_id,batch_id 0 97 miss% 0.8142319455238106
plot_id,batch_id 0 98 miss% 0.825456294135361
plot_id,batch_id 0 99 miss% 0.8314001947771219
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.78068074 0.8298453  0.83458867 0.84031137 0.8427745  0.77528391
 0.82396332 0.83570288 0.84153095 0.84560587 0.76636038 0.82212933
 0.83270035 0.83744332 0.83992396 0.76782705 0.82167122 0.83143568
 0.84033153 0.84090492 0.81351908 0.83511211 0.83815749 0.84306078
 0.84394234 0.80550992 0.83452112 0.83882711 0.84277587 0.84289581
 0.79197929 0.83095301 0.83950139 0.84148761 0.84397836 0.79038772
 0.83617601 0.83686574 0.8422085  0.84266759 0.82125208 0.83998387
 0.83890411 0.84587911 0.84979426 0.8171655  0.83995109 0.84106052
 0.84499621 0.84829711 0.82605627 0.83732444 0.84152561 0.84510015
 0.84536905 0.81853787 0.83874069 0.84154596 0.84546485 0.84302305
 0.71973801 0.80765592 0.8239136  0.83052141 0.83564112 0.71814652
 0.79589861 0.81099482 0.83028921 0.82897977 0.67773779 0.8096146
 0.80410248 0.81715879 0.8267111  0.68384652 0.78938649 0.79716804
 0.81694758 0.82124355 0.74446335 0.81965337 0.82790877 0.83468964
 0.83799646 0.7324525  0.81147411 0.82685139 0.83528487 0.83555937
 0.69956353 0.80110085 0.82179751 0.8224294  0.83630566 0.72321696
 0.79390981 0.81423195 0.82545629 0.83140019]
for model  107 the mean error 0.8177695580633597
all id 107 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 25 out win 5 err 0.8177695580633597
Launcher: Job 108 completed in 11845 seconds.
Launcher: Task 245 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 25
(2400, 24, 8)
maximum abs change 0.13810741528868675
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.007265380738997212
number of weird sim 0
[]
renaissance 4
renaissance points (array([ 960, 1080, 1100, 1860]), array([0, 0, 0, 0]), array([2, 5, 5, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0006915132980793715
all the summation of grain fractions are 1 4.426459199180499e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 25 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 48000
total number of trained parameters  134801
Epoch:0, Train loss:0.475526, valid loss:0.475468
Epoch:1, Train loss:0.359097, valid loss:0.360866
Epoch:2, Train loss:0.347276, valid loss:0.360311
Epoch:3, Train loss:0.345248, valid loss:0.359108
Epoch:4, Train loss:0.344078, valid loss:0.359004
Epoch:5, Train loss:0.343954, valid loss:0.359109
Epoch:6, Train loss:0.343219, valid loss:0.359322
Epoch:7, Train loss:0.343119, valid loss:0.359171
Epoch:8, Train loss:0.343092, valid loss:0.359159
Epoch:9, Train loss:0.342731, valid loss:0.358556
Epoch:10, Train loss:0.342770, valid loss:0.358682
Epoch:11, Train loss:0.341651, valid loss:0.358318
Epoch:12, Train loss:0.341729, valid loss:0.358196
Epoch:13, Train loss:0.341574, valid loss:0.357962
Epoch:14, Train loss:0.341600, valid loss:0.358278
Epoch:15, Train loss:0.341578, valid loss:0.357961
Epoch:16, Train loss:0.341468, valid loss:0.358139
Epoch:17, Train loss:0.341455, valid loss:0.357985
Epoch:18, Train loss:0.341435, valid loss:0.358146
Epoch:19, Train loss:0.341486, valid loss:0.358082
Epoch:20, Train loss:0.341352, valid loss:0.358006
Epoch:21, Train loss:0.340884, valid loss:0.357978
Epoch:22, Train loss:0.340866, valid loss:0.358097
Epoch:23, Train loss:0.340841, valid loss:0.357808
Epoch:24, Train loss:0.340843, valid loss:0.357803
Epoch:25, Train loss:0.340840, valid loss:0.357881
Epoch:26, Train loss:0.340806, valid loss:0.357782
Epoch:27, Train loss:0.340807, valid loss:0.358030
Epoch:28, Train loss:0.340849, valid loss:0.357727
Epoch:29, Train loss:0.340790, valid loss:0.357743
Epoch:30, Train loss:0.340748, valid loss:0.357876
Epoch:31, Train loss:0.340532, valid loss:0.357669
Epoch:32, Train loss:0.340520, valid loss:0.357699
Epoch:33, Train loss:0.340536, valid loss:0.357714
Epoch:34, Train loss:0.340502, valid loss:0.357723
Epoch:35, Train loss:0.340495, valid loss:0.357758
Epoch:36, Train loss:0.340485, valid loss:0.357679
Epoch:37, Train loss:0.340531, valid loss:0.357683
Epoch:38, Train loss:0.340455, valid loss:0.357799
Epoch:39, Train loss:0.340502, valid loss:0.357773
Epoch:40, Train loss:0.340488, valid loss:0.357767
Epoch:41, Train loss:0.340354, valid loss:0.357645
Epoch:42, Train loss:0.340354, valid loss:0.357658
Epoch:43, Train loss:0.340342, valid loss:0.357610
Epoch:44, Train loss:0.340358, valid loss:0.357650
Epoch:45, Train loss:0.340346, valid loss:0.357675
Epoch:46, Train loss:0.340335, valid loss:0.357650
Epoch:47, Train loss:0.340339, valid loss:0.357626
Epoch:48, Train loss:0.340335, valid loss:0.357603
Epoch:49, Train loss:0.340342, valid loss:0.357633
Epoch:50, Train loss:0.340326, valid loss:0.357607
Epoch:51, Train loss:0.340279, valid loss:0.357610
Epoch:52, Train loss:0.340270, valid loss:0.357597
Epoch:53, Train loss:0.340268, valid loss:0.357611
Epoch:54, Train loss:0.340275, valid loss:0.357619
Epoch:55, Train loss:0.340267, valid loss:0.357625
Epoch:56, Train loss:0.340264, valid loss:0.357664
Epoch:57, Train loss:0.340264, valid loss:0.357612
Epoch:58, Train loss:0.340268, valid loss:0.357603
Epoch:59, Train loss:0.340257, valid loss:0.357632
Epoch:60, Train loss:0.340264, valid loss:0.357603
training time 11764.300821304321
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.041666666666666664
plot_id,batch_id 0 0 miss% 0.7835932873866824
plot_id,batch_id 0 1 miss% 0.8301190678219118
plot_id,batch_id 0 2 miss% 0.8357818284165637
plot_id,batch_id 0 3 miss% 0.8408779253192988
plot_id,batch_id 0 4 miss% 0.8416271444955498
plot_id,batch_id 0 5 miss% 0.7734010290749724
plot_id,batch_id 0 6 miss% 0.8255647219727631
plot_id,batch_id 0 7 miss% 0.836171377686966
plot_id,batch_id 0 8 miss% 0.8403700787998918
plot_id,batch_id 0 9 miss% 0.8470601450895524
plot_id,batch_id 0 10 miss% 0.7666239279582686
plot_id,batch_id 0 11 miss% 0.8217163937610859
plot_id,batch_id 0 12 miss% 0.8324699036522979
plot_id,batch_id 0 13 miss% 0.8411704398021079
plot_id,batch_id 0 14 miss% 0.8406745640572775
plot_id,batch_id 0 15 miss% 0.7661925347836687
plot_id,batch_id 0 16 miss% 0.8215720295092904
plot_id,batch_id 0 17 miss% 0.8313002503603373
plot_id,batch_id 0 18 miss% 0.8395473718300469
plot_id,batch_id 0 19 miss% 0.8423811205660803
plot_id,batch_id 0 20 miss% 0.811751519349892
plot_id,batch_id 0 21 miss% 0.8352936234824424
plot_id,batch_id 0 22 miss% 0.8398595283845027
plot_id,batch_id 0 23 miss% 0.8426620491553481
plot_id,batch_id 0 24 miss% 0.8447944017432879
plot_id,batch_id 0 25 miss% 0.8067981096714076
plot_id,batch_id 0 26 miss% 0.8351527754426191
plot_id,batch_id 0 27 miss% 0.83900841742411
plot_id,batch_id 0 28 miss% 0.8421260503361424
plot_id,batch_id 0 29 miss% 0.8439567507010313
plot_id,batch_id 0 30 miss% 0.7894509298044452
plot_id,batch_id 0 31 miss% 0.8318875825851089
plot_id,batch_id 0 32 miss% 0.8386241608877887
plot_id,batch_id 0 33 miss% 0.8419208550409731
plot_id,batch_id 0 34 miss% 0.8432674420562778
plot_id,batch_id 0 35 miss% 0.7910862435971439
plot_id,batch_id 0 36 miss% 0.836168983669921
plot_id,batch_id 0 37 miss% 0.8355522096944151
plot_id,batch_id 0 38 miss% 0.8422821086937109
plot_id,batch_id 0 39 miss% 0.8455585219895658
plot_id,batch_id 0 40 miss% 0.8204656070632278
plot_id,batch_id 0 41 miss% 0.8399221620399965
plot_id,batch_id 0 42 miss% 0.8400538762994525
plot_id,batch_id 0 43 miss% 0.8450930009313705
plot_id,batch_id 0 44 miss% 0.8506455212799622
plot_id,batch_id 0 45 miss% 0.8193835606191306
plot_id,batch_id 0 46 miss% 0.8386541336385239
plot_id,batch_id 0 47 miss% 0.8406376878942371
plot_id,batch_id 0 48 miss% 0.8451399193512537
plot_id,batch_id 0 49 miss% 0.8503589508274324
plot_id,batch_id 0 50 miss% 0.8253510015080857
plot_id,batch_id 0 51 miss% 0.8381740537243925
plot_id,batch_id 0 52 miss% 0.841100212972772
plot_id,batch_id 0 53 miss% 0.84576700146842
plot_id,batch_id 0 54 miss% 0.8449420017968226
plot_id,batch_id 0 55 miss% 0.8200979103581272
plot_id,batch_id 0 56 miss% 0.837681222983218
plot_id,batch_id 0 57 miss% 0.8413613364306046
plot_id,batch_id 0 58 miss% 0.8450573422559106
plot_id,batch_id 0 59 miss% 0.8444048607430619
plot_id,batch_id 0 60 miss% 0.7161059509955812
plot_id,batch_id 0 61 miss% 0.8083076408096451
plot_id,batch_id 0 62 miss% 0.8238043404315067
plot_id,batch_id 0 63 miss% 0.8303585105739628
plot_id,batch_id 0 64 miss% 0.8367541482704072
plot_id,batch_id 0 65 miss% 0.7197973258945642
plot_id,batch_id 0 66 miss% 0.795986426471952
plot_id,batch_id 0 67 miss% 0.8120354488953108
plot_id,batch_id 0 68 miss% 0.8283179032930091
plot_id,batch_id 0 69 miss% 0.8292377900940678
plot_id,batch_id 0 70 miss% 0.6809727036131505
plot_id,batch_id 0 71 miss% 0.8087602238124805
plot_id,batch_id 0 72 miss% 0.8029453370754457
plot_id,batch_id 0 73 miss% 0.8179333531614719
plot_id,batch_id 0 74 miss% 0.8269349349646918
plot_id,batch_id 0 75 miss% 0.6852818158798352
plot_id,batch_id 0 76 miss% 0.7898476150229046
plot_id,batch_id 0 77 miss% 0.7984296644820108
plot_id,batch_id 0 78 miss% 0.8155823212443458
plot_id,batch_id 0 79 miss% 0.8214189306178862
plot_id,batch_id 0 80 miss% 0.7413865162925697
plot_id,batch_id 0 81 miss% 0.8198264324490876
plot_id,batch_id 0 82 miss% 0.8291280641009429
plot_id,batch_id 0 83 miss% 0.8349623155333727
plot_id,batch_id 0 84 miss% 0.8383911645342448
plot_id,batch_id 0 85 miss% 0.7387036918572938
plot_id,batch_id 0 86 miss% 0.8097974545219839
plot_id,batch_id 0 87 miss% 0.8247978777653492
plot_id,batch_id 0 88 miss% 0.8383522256965651
plot_id,batch_id 0 89 miss% 0.8343260170726355
plot_id,batch_id 0 90 miss% 0.6969797593411005
plot_id,batch_id 0 91 miss% 0.8046937542201972
plot_id,batch_id 0 92 miss% 0.8212946968806242
plot_id,batch_id 0 93 miss% 0.8232228302935205
plot_id,batch_id 0 94 miss% 0.8365601449353937
plot_id,batch_id 0 95 miss% 0.7259655870995229
plot_id,batch_id 0 96 miss% 0.7939800032267594
plot_id,batch_id 0 97 miss% 0.8159143619209576
plot_id,batch_id 0 98 miss% 0.8256098663173316
plot_id,batch_id 0 99 miss% 0.8302763591379103
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.78359329 0.83011907 0.83578183 0.84087793 0.84162714 0.77340103
 0.82556472 0.83617138 0.84037008 0.84706015 0.76662393 0.82171639
 0.8324699  0.84117044 0.84067456 0.76619253 0.82157203 0.83130025
 0.83954737 0.84238112 0.81175152 0.83529362 0.83985953 0.84266205
 0.8447944  0.80679811 0.83515278 0.83900842 0.84212605 0.84395675
 0.78945093 0.83188758 0.83862416 0.84192086 0.84326744 0.79108624
 0.83616898 0.83555221 0.84228211 0.84555852 0.82046561 0.83992216
 0.84005388 0.845093   0.85064552 0.81938356 0.83865413 0.84063769
 0.84513992 0.85035895 0.825351   0.83817405 0.84110021 0.845767
 0.844942   0.82009791 0.83768122 0.84136134 0.84505734 0.84440486
 0.71610595 0.80830764 0.82380434 0.83035851 0.83675415 0.71979733
 0.79598643 0.81203545 0.8283179  0.82923779 0.6809727  0.80876022
 0.80294534 0.81793335 0.82693493 0.68528182 0.78984762 0.79842966
 0.81558232 0.82141893 0.74138652 0.81982643 0.82912806 0.83496232
 0.83839116 0.73870369 0.80979745 0.82479788 0.83835223 0.83432602
 0.69697976 0.80469375 0.8212947  0.82322283 0.83656014 0.72596559
 0.79398    0.81591436 0.82560987 0.83027636]
for model  134 the mean error 0.8180269027904236
all id 134 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 25 out win 5 err 0.8180269027904236
Launcher: Job 135 completed in 11940 seconds.
Launcher: Task 211 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  107025
Epoch:0, Train loss:0.294750, valid loss:0.288258
Epoch:1, Train loss:0.017811, valid loss:0.003084
Epoch:2, Train loss:0.005131, valid loss:0.002264
Epoch:3, Train loss:0.003835, valid loss:0.001626
Epoch:4, Train loss:0.003366, valid loss:0.001520
Epoch:5, Train loss:0.003022, valid loss:0.001661
Epoch:6, Train loss:0.002800, valid loss:0.001586
Epoch:7, Train loss:0.002666, valid loss:0.001448
Epoch:8, Train loss:0.002560, valid loss:0.001914
Epoch:9, Train loss:0.002529, valid loss:0.001810
Epoch:10, Train loss:0.002397, valid loss:0.001344
Epoch:11, Train loss:0.001702, valid loss:0.001061
Epoch:12, Train loss:0.001659, valid loss:0.000999
Epoch:13, Train loss:0.001637, valid loss:0.001254
Epoch:14, Train loss:0.001590, valid loss:0.000965
Epoch:15, Train loss:0.001536, valid loss:0.001032
Epoch:16, Train loss:0.001586, valid loss:0.001159
Epoch:17, Train loss:0.001485, valid loss:0.000935
Epoch:18, Train loss:0.001488, valid loss:0.000953
Epoch:19, Train loss:0.001507, valid loss:0.000843
Epoch:20, Train loss:0.001440, valid loss:0.001089
Epoch:21, Train loss:0.001087, valid loss:0.000800
Epoch:22, Train loss:0.001075, valid loss:0.000735
Epoch:23, Train loss:0.001074, valid loss:0.000768
Epoch:24, Train loss:0.001078, valid loss:0.000783
Epoch:25, Train loss:0.001065, valid loss:0.000774
Epoch:26, Train loss:0.001037, valid loss:0.000738
Epoch:27, Train loss:0.001037, valid loss:0.000769
Epoch:28, Train loss:0.001016, valid loss:0.000793
Epoch:29, Train loss:0.001030, valid loss:0.000724
Epoch:30, Train loss:0.001019, valid loss:0.000791
Epoch:31, Train loss:0.000831, valid loss:0.000630
Epoch:32, Train loss:0.000813, valid loss:0.000614
Epoch:33, Train loss:0.000828, valid loss:0.000624
Epoch:34, Train loss:0.000809, valid loss:0.000605
Epoch:35, Train loss:0.000807, valid loss:0.000868
Epoch:36, Train loss:0.000811, valid loss:0.000574
Epoch:37, Train loss:0.000804, valid loss:0.000623
Epoch:38, Train loss:0.000804, valid loss:0.000643
Epoch:39, Train loss:0.000787, valid loss:0.000617
Epoch:40, Train loss:0.000786, valid loss:0.000577
Epoch:41, Train loss:0.000688, valid loss:0.000608
Epoch:42, Train loss:0.000681, valid loss:0.000533
Epoch:43, Train loss:0.000686, valid loss:0.000543
Epoch:44, Train loss:0.000692, valid loss:0.000567
Epoch:45, Train loss:0.000682, valid loss:0.000557
Epoch:46, Train loss:0.000669, valid loss:0.000651
Epoch:47, Train loss:0.000679, valid loss:0.000550
Epoch:48, Train loss:0.000673, valid loss:0.000535
Epoch:49, Train loss:0.000676, valid loss:0.000584
Epoch:50, Train loss:0.000657, valid loss:0.000558
Epoch:51, Train loss:0.000623, valid loss:0.000552
Epoch:52, Train loss:0.000623, valid loss:0.000585
Epoch:53, Train loss:0.000621, valid loss:0.000550
Epoch:54, Train loss:0.000616, valid loss:0.000555
Epoch:55, Train loss:0.000616, valid loss:0.000568
Epoch:56, Train loss:0.000613, valid loss:0.000552
Epoch:57, Train loss:0.000625, valid loss:0.000547
Epoch:58, Train loss:0.000610, valid loss:0.000540
Epoch:59, Train loss:0.000614, valid loss:0.000549
Epoch:60, Train loss:0.000609, valid loss:0.000552
training time 11821.064592838287
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.034787112816928006
plot_id,batch_id 0 1 miss% 0.019442060520607662
plot_id,batch_id 0 2 miss% 0.024647900935856448
plot_id,batch_id 0 3 miss% 0.022453564216530223
plot_id,batch_id 0 4 miss% 0.0263196477682852
plot_id,batch_id 0 5 miss% 0.026017299529487503
plot_id,batch_id 0 6 miss% 0.03156996532385802
plot_id,batch_id 0 7 miss% 0.028624205536216876
plot_id,batch_id 0 8 miss% 0.02301798830010335
plot_id,batch_id 0 9 miss% 0.024424797769824588
plot_id,batch_id 0 10 miss% 0.03637004113511115
plot_id,batch_id 0 11 miss% 0.04746616148982955
plot_id,batch_id 0 12 miss% 0.022742138878654505
plot_id,batch_id 0 13 miss% 0.022793639089134683
plot_id,batch_id 0 14 miss% 0.027036926199771404
plot_id,batch_id 0 15 miss% 0.04797108041774612
plot_id,batch_id 0 16 miss% 0.03178450457269807
plot_id,batch_id 0 17 miss% 0.028616476580015964
plot_id,batch_id 0 18 miss% 0.03034441816859253
plot_id,batch_id 0 19 miss% 0.02924868005407343
plot_id,batch_id 0 20 miss% 0.04791997048450525
plot_id,batch_id 0 21 miss% 0.02230818619725548
plot_id,batch_id 0 22 miss% 0.021530629794474038
plot_id,batch_id 0 23 miss% 0.0193688252023674
plot_id,batch_id 0 24 miss% 0.02695348091711939
plot_id,batch_id 0 25 miss% 0.03237532435187631
plot_id,batch_id 0 26 miss% 0.028430681689352297
plot_id,batch_id 0 27 miss% 0.024454331026738628
plot_id,batch_id 0 28 miss% 0.018714902332830807
plot_id,batch_id 0 29 miss% 0.026944995557551508
plot_id,batch_id 0 30 miss% 0.04190268719470084
plot_id,batch_id 0 31 miss% 0.02814722485374528
plot_id,batch_id 0 32 miss% 0.02308564986701233
plot_id,batch_id 0 33 miss% 0.02390647730198857
plot_id,batch_id 0 34 miss% 0.027326944766965493
plot_id,batch_id 0 35 miss% 0.04029870123354728
plot_id,batch_id 0 36 miss% 0.032282030416176256
plot_id,batch_id 0 37 miss% 0.035166629711833486
plot_id,batch_id 0 38 miss% 0.024412570203005784
plot_id,batch_id 0 39 miss% 0.02679102363494885
plot_id,batch_id 0 40 miss% 0.07092823685866352
plot_id,batch_id 0 41 miss% 0.024416675076950528
plot_id,batch_id 0 42 miss% 0.011867329921640432
plot_id,batch_id 0 43 miss% 0.03293325091752517
plot_id,batch_id 0 44 miss% 0.020048955642903668
plot_id,batch_id 0 45 miss% 0.022696282070470013
plot_id,batch_id 0 46 miss% 0.021442962276491794
plot_id,batch_id 0 47 miss% 0.0234593530610535
plot_id,batch_id 0 48 miss% 0.022536001714189235
plot_id,batch_id 0 49 miss% 0.018028382388224636
plot_id,batch_id 0 50 miss% 0.027193370866160214
plot_id,batch_id 0 51 miss% 0.019798511904470555
plot_id,batch_id 0 52 miss% 0.01608663982688874
plot_id,batch_id 0 53 miss% 0.01793634477774993
plot_id,batch_id 0 54 miss% 0.029343494276511065
plot_id,batch_id 0 55 miss% 0.05202597071768552
plot_id,batch_id 0 56 miss% 0.02919178686675965
plot_id,batch_id 0 57 miss% 0.02168253901474351
plot_id,batch_id 0 58 miss% 0.021150724667342455
plot_id,batch_id 0 59 miss% 0.017290808275723168
plot_id,batch_id 0 60 miss% 0.04862131374470031
plot_id,batch_id 0 61 miss% 0.025336212375613357
plot_id,batch_id 0 62 miss% 0.02862910876890595
plot_id,batch_id 0 63 miss% 0.03715279625475149
plot_id,batch_id 0 64 miss% 0.0313690398363228
plot_id,batch_id 0 65 miss% 0.040017459764600964
plot_id,batch_id 0 66 miss% 0.03188069101800556
plot_id,batch_id 0 67 miss% 0.02238996482924809
plot_id,batch_id 0 68 miss% 0.03480644822304666
plot_id,batch_id 0 69 miss% 0.027130604898940328
plot_id,batch_id 0 70 miss% 0.0362035300846024
plot_id,batch_id 0 71 miss% 0.04336252780152275
plot_id,batch_id 0 72 miss% 0.032558517391485395
plot_id,batch_id 0 73 miss% 0.028071394606046365
plot_id,batch_id 0 74 miss% 0.03589271077746955
plot_id,batch_id 0 75 miss% 0.03363395524334793
plot_id,batch_id 0 76 miss% 0.045647325451798657
plot_id,batch_id 0 77 miss% 0.045046338166868656
plot_id,batch_id 0 78 miss% 0.02541356915907214
plot_id,batch_id 0 79 miss% 0.04010785896469468
plot_id,batch_id 0 80 miss% 0.030546830475251096
plot_id,batch_id 0 81 miss% 0.01995172642026551
plot_id,batch_id 0 82 miss% 0.027852577958093646
plot_id,batch_id 0 83 miss% 0.03281753626191509
plot_id,batch_id 0 84 miss% 0.02526304016529001
plot_id,batch_id 0 85 miss% 0.056957065177702784
plot_id,batch_id 0 86 miss% 0.025018499585505217
plot_id,batch_id 0 87 miss% 0.01996835167322462
plot_id,batch_id 0 88 miss% 0.03741764993947569
plot_id,batch_id 0 89 miss% 0.03007709815766707
plot_id,batch_id 0 90 miss% 0.027079976244700186
plot_id,batch_id 0 91 miss% 0.028951143872718788
plot_id,batch_id 0 92 miss% 0.02841484992403261
plot_id,batch_id 0 93 miss% 0.031883693738839085
plot_id,batch_id 0 94 miss% 0.0331252330047258
plot_id,batch_id 0 95 miss% 0.03870740712371505
plot_id,batch_id 0 96 miss% 0.03365337115998166
plot_id,batch_id 0 97 miss% 0.044112923010818164
plot_id,batch_id 0 98 miss% 0.028614114726389396
plot_id,batch_id 0 99 miss% 0.02986731592065328
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.03478711 0.01944206 0.0246479  0.02245356 0.02631965 0.0260173
 0.03156997 0.02862421 0.02301799 0.0244248  0.03637004 0.04746616
 0.02274214 0.02279364 0.02703693 0.04797108 0.0317845  0.02861648
 0.03034442 0.02924868 0.04791997 0.02230819 0.02153063 0.01936883
 0.02695348 0.03237532 0.02843068 0.02445433 0.0187149  0.026945
 0.04190269 0.02814722 0.02308565 0.02390648 0.02732694 0.0402987
 0.03228203 0.03516663 0.02441257 0.02679102 0.07092824 0.02441668
 0.01186733 0.03293325 0.02004896 0.02269628 0.02144296 0.02345935
 0.022536   0.01802838 0.02719337 0.01979851 0.01608664 0.01793634
 0.02934349 0.05202597 0.02919179 0.02168254 0.02115072 0.01729081
 0.04862131 0.02533621 0.02862911 0.0371528  0.03136904 0.04001746
 0.03188069 0.02238996 0.03480645 0.0271306  0.03620353 0.04336253
 0.03255852 0.02807139 0.03589271 0.03363396 0.04564733 0.04504634
 0.02541357 0.04010786 0.03054683 0.01995173 0.02785258 0.03281754
 0.02526304 0.05695707 0.0250185  0.01996835 0.03741765 0.0300771
 0.02707998 0.02895114 0.02841485 0.03188369 0.03312523 0.03870741
 0.03365337 0.04411292 0.02861411 0.02986732]
for model  206 the mean error 0.030016092690654767
all id 206 hidden_dim 32 learning_rate 0.01 num_layers 4 frames 31 out win 5 err 0.030016092690654767
Launcher: Job 207 completed in 12014 seconds.
Launcher: Task 189 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (4, 4)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  107025
Epoch:0, Train loss:0.294750, valid loss:0.288258
Epoch:1, Train loss:0.020471, valid loss:0.003848
Epoch:2, Train loss:0.006217, valid loss:0.002824
Epoch:3, Train loss:0.004618, valid loss:0.001744
Epoch:4, Train loss:0.003876, valid loss:0.001813
Epoch:5, Train loss:0.003623, valid loss:0.001490
Epoch:6, Train loss:0.003526, valid loss:0.002034
Epoch:7, Train loss:0.003277, valid loss:0.001461
Epoch:8, Train loss:0.003100, valid loss:0.001762
Epoch:9, Train loss:0.003003, valid loss:0.001704
Epoch:10, Train loss:0.002978, valid loss:0.001921
Epoch:11, Train loss:0.001934, valid loss:0.001462
Epoch:12, Train loss:0.001908, valid loss:0.001132
Epoch:13, Train loss:0.001882, valid loss:0.001274
Epoch:14, Train loss:0.002174, valid loss:0.001282
Epoch:15, Train loss:0.002016, valid loss:0.001107
Epoch:16, Train loss:0.002165, valid loss:0.001211
Epoch:17, Train loss:0.001874, valid loss:0.001107
Epoch:18, Train loss:0.001758, valid loss:0.000989
Epoch:19, Train loss:0.001804, valid loss:0.001389
Epoch:20, Train loss:0.001730, valid loss:0.001114
Epoch:21, Train loss:0.001314, valid loss:0.000832
Epoch:22, Train loss:0.001282, valid loss:0.000836
Epoch:23, Train loss:0.001234, valid loss:0.000870
Epoch:24, Train loss:0.001255, valid loss:0.000808
Epoch:25, Train loss:0.001234, valid loss:0.000833
Epoch:26, Train loss:0.001215, valid loss:0.000758
Epoch:27, Train loss:0.001225, valid loss:0.000889
Epoch:28, Train loss:0.001192, valid loss:0.000885
Epoch:29, Train loss:0.001215, valid loss:0.001006
Epoch:30, Train loss:0.001181, valid loss:0.000812
Epoch:31, Train loss:0.000927, valid loss:0.000652
Epoch:32, Train loss:0.000919, valid loss:0.000716
Epoch:33, Train loss:0.000909, valid loss:0.000650
Epoch:34, Train loss:0.000919, valid loss:0.000700
Epoch:35, Train loss:0.000909, valid loss:0.000681
Epoch:36, Train loss:0.000893, valid loss:0.000651
Epoch:37, Train loss:0.000869, valid loss:0.000682
Epoch:38, Train loss:0.000894, valid loss:0.000643
Epoch:39, Train loss:0.000879, valid loss:0.000673
Epoch:40, Train loss:0.000861, valid loss:0.000629
Epoch:41, Train loss:0.000737, valid loss:0.000614
Epoch:42, Train loss:0.000745, valid loss:0.000593
Epoch:43, Train loss:0.000762, valid loss:0.000580
Epoch:44, Train loss:0.000721, valid loss:0.000648
Epoch:45, Train loss:0.000737, valid loss:0.000592
Epoch:46, Train loss:0.000732, valid loss:0.000655
Epoch:47, Train loss:0.000725, valid loss:0.000596
Epoch:48, Train loss:0.000728, valid loss:0.000624
Epoch:49, Train loss:0.000724, valid loss:0.000613
Epoch:50, Train loss:0.000709, valid loss:0.000618
Epoch:51, Train loss:0.000647, valid loss:0.000644
Epoch:52, Train loss:0.000643, valid loss:0.000623
Epoch:53, Train loss:0.000642, valid loss:0.000560
Epoch:54, Train loss:0.000645, valid loss:0.000578
Epoch:55, Train loss:0.000636, valid loss:0.000625
Epoch:56, Train loss:0.000639, valid loss:0.000586
Epoch:57, Train loss:0.000640, valid loss:0.000581
Epoch:58, Train loss:0.000628, valid loss:0.000586
Epoch:59, Train loss:0.000635, valid loss:0.000592
Epoch:60, Train loss:0.000632, valid loss:0.000570
training time 11835.349041461945
total number of trained parameters for initialize model 107025
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.023689508560721308
plot_id,batch_id 0 1 miss% 0.02879307568651205
plot_id,batch_id 0 2 miss% 0.02665150821738798
plot_id,batch_id 0 3 miss% 0.0271533818953609
plot_id,batch_id 0 4 miss% 0.02270993020994506
plot_id,batch_id 0 5 miss% 0.038593156355152085
plot_id,batch_id 0 6 miss% 0.028671188115372975
plot_id,batch_id 0 7 miss% 0.025778060943967274
plot_id,batch_id 0 8 miss% 0.019048363874807198
plot_id,batch_id 0 9 miss% 0.01902416971960519
plot_id,batch_id 0 10 miss% 0.0443317613738692
plot_id,batch_id 0 11 miss% 0.036935858942157375
plot_id,batch_id 0 12 miss% 0.021209751345238552
plot_id,batch_id 0 13 miss% 0.012124361614904674
plot_id,batch_id 0 14 miss% 0.036731160523118235
plot_id,batch_id 0 15 miss% 0.028142894129171758
plot_id,batch_id 0 16 miss% 0.025938367778405826
plot_id,batch_id 0 17 miss% 0.035983953390038986
plot_id,batch_id 0 18 miss% 0.02013897802072622
plot_id,batch_id 0 19 miss% 0.036260811292837994
plot_id,batch_id 0 20 miss% 0.03859665161473095
plot_id,batch_id 0 21 miss% 0.023095797664547366
plot_id,batch_id 0 22 miss% 0.02518392649005779
plot_id,batch_id 0 23 miss% 0.02358527934517752
plot_id,batch_id 0 24 miss% 0.016480723174181932
plot_id,batch_id 0 25 miss% 0.028536270177736366
plot_id,batch_id 0 26 miss% 0.019822641601910225
plot_id,batch_id 0 27 miss% 0.02855746412444449
plot_id,batch_id 0 28 miss% 0.024907953264013794
plot_id,batch_id 0 29 miss% 0.01727223833616442
plot_id,batch_id 0 30 miss% 0.03126048914574859
plot_id,batch_id 0 31 miss% 0.029769598007193317
plot_id,batch_id 0 32 miss% 0.022962402195983035
plot_id,batch_id 0 33 miss% 0.02420123617867051
plot_id,batch_id 0 34 miss% 0.02965140757613641
plot_id,batch_id 0 35 miss% 0.0359209632883975
plot_id,batch_id 0 36 miss% 0.037405183857800735
plot_id,batch_id 0 37 miss% 0.02048581405310272
plot_id,batch_id 0 38 miss% 0.021438995709113604
plot_id,batch_id 0 39 miss% 0.01970544546834653
plot_id,batch_id 0 40 miss% 0.05047993068993182
plot_id,batch_id 0 41 miss% 0.021948717181582627
plot_id,batch_id 0 42 miss% 0.018067905158587846
plot_id,batch_id 0 43 miss% 0.028650323634329268
plot_id,batch_id 0 44 miss% 0.020705974298889672
plot_id,batch_id 0 45 miss% 0.030291875949406164
plot_id,batch_id 0 46 miss% 0.021401108660476422
plot_id,batch_id 0 47 miss% 0.019618909140356626
plot_id,batch_id 0 48 miss% 0.025146032958992634
plot_id,batch_id 0 49 miss% 0.012444608761531663
plot_id,batch_id 0 50 miss% 0.03473304975602088
plot_id,batch_id 0 51 miss% 0.028172011962962994
plot_id,batch_id 0 52 miss% 0.02285784637723794
plot_id,batch_id 0 53 miss% 0.017009054487720103
plot_id,batch_id 0 54 miss% 0.02462658305929852
plot_id,batch_id 0 55 miss% 0.0371998061521745
plot_id,batch_id 0 56 miss% 0.029929818482275994
plot_id,batch_id 0 57 miss% 0.029372024464603606
plot_id,batch_id 0 58 miss% 0.02918843174731402
plot_id,batch_id 0 59 miss% 0.018227811613211756
plot_id,batch_id 0 60 miss% 0.027080515474631542
plot_id,batch_id 0 61 miss% 0.01603221832439373
plot_id,batch_id 0 62 miss% 0.02699365687153508
plot_id,batch_id 0 63 miss% 0.036539317290836065
plot_id,batch_id 0 64 miss% 0.0358240379653843
plot_id,batch_id 0 65 miss% 0.03657033809475367
plot_id,batch_id 0 66 miss% 0.024147641706780614
plot_id,batch_id 0 67 miss% 0.028003535930334573
plot_id,batch_id 0 68 miss% 0.02535723767555637
plot_id,batch_id 0 69 miss% 0.023039136421727828
plot_id,batch_id 0 70 miss% 0.02462333892924211
plot_id,batch_id 0 71 miss% 0.04051866138354504
plot_id,batch_id 0 72 miss% 0.02607986479883985
plot_id,batch_id 0 73 miss% 0.031410656589355934
plot_id,batch_id 0 74 miss% 0.04464614867122819
plot_id,batch_id 0 75 miss% 0.04264404652511911
plot_id,batch_id 0 76 miss% 0.0463578960269994
plot_id,batch_id 0 77 miss% 0.03353614342074685
plot_id,batch_id 0 78 miss% 0.038782177358598964
plot_id,batch_id 0 79 miss% 0.05192431791983927
plot_id,batch_id 0 80 miss% 0.03656329994043079
plot_id,batch_id 0 81 miss% 0.032905733727530626
plot_id,batch_id 0 82 miss% 0.03409354603850005
plot_id,batch_id 0 83 miss% 0.020866259199570955
plot_id,batch_id 0 84 miss% 0.028392802147637786
plot_id,batch_id 0 85 miss% 0.05636494714280859
plot_id,batch_id 0 86 miss% 0.027837591131943777
plot_id,batch_id 0 87 miss% 0.03111532565295021
plot_id,batch_id 0 88 miss% 0.04292546990278373
plot_id,batch_id 0 89 miss% 0.03283704212025033
plot_id,batch_id 0 90 miss% 0.040865771653981
plot_id,batch_id 0 91 miss% 0.034829731000433864
plot_id,batch_id 0 92 miss% 0.03183639353065195
plot_id,batch_id 0 93 miss% 0.026736783330678526
plot_id,batch_id 0 94 miss% 0.04217471334414488
plot_id,batch_id 0 95 miss% 0.07061797244770363
plot_id,batch_id 0 96 miss% 0.02596381505594735
plot_id,batch_id 0 97 miss% 0.048571904427127346
plot_id,batch_id 0 98 miss% 0.03595926005158439
plot_id,batch_id 0 99 miss% 0.024555067186280657
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.02368951 0.02879308 0.02665151 0.02715338 0.02270993 0.03859316
 0.02867119 0.02577806 0.01904836 0.01902417 0.04433176 0.03693586
 0.02120975 0.01212436 0.03673116 0.02814289 0.02593837 0.03598395
 0.02013898 0.03626081 0.03859665 0.0230958  0.02518393 0.02358528
 0.01648072 0.02853627 0.01982264 0.02855746 0.02490795 0.01727224
 0.03126049 0.0297696  0.0229624  0.02420124 0.02965141 0.03592096
 0.03740518 0.02048581 0.021439   0.01970545 0.05047993 0.02194872
 0.01806791 0.02865032 0.02070597 0.03029188 0.02140111 0.01961891
 0.02514603 0.01244461 0.03473305 0.02817201 0.02285785 0.01700905
 0.02462658 0.03719981 0.02992982 0.02937202 0.02918843 0.01822781
 0.02708052 0.01603222 0.02699366 0.03653932 0.03582404 0.03657034
 0.02414764 0.02800354 0.02535724 0.02303914 0.02462334 0.04051866
 0.02607986 0.03141066 0.04464615 0.04264405 0.0463579  0.03353614
 0.03878218 0.05192432 0.0365633  0.03290573 0.03409355 0.02086626
 0.0283928  0.05636495 0.02783759 0.03111533 0.04292547 0.03283704
 0.04086577 0.03482973 0.03183639 0.02673678 0.04217471 0.07061797
 0.02596382 0.0485719  0.03595926 0.02455507]
for model  233 the mean error 0.029829468642120528
all id 233 hidden_dim 32 learning_rate 0.02 num_layers 4 frames 31 out win 5 err 0.029829468642120528
Launcher: Job 234 completed in 12029 seconds.
Launcher: Task 139 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  134801
Epoch:0, Train loss:0.353995, valid loss:0.355328
Epoch:1, Train loss:0.228343, valid loss:0.230253
Epoch:2, Train loss:0.219720, valid loss:0.229320
Epoch:3, Train loss:0.218705, valid loss:0.229653
Epoch:4, Train loss:0.218284, valid loss:0.229275
Epoch:5, Train loss:0.218143, valid loss:0.228875
Epoch:6, Train loss:0.217850, valid loss:0.229288
Epoch:7, Train loss:0.217867, valid loss:0.229093
Epoch:8, Train loss:0.217700, valid loss:0.228726
Epoch:9, Train loss:0.217693, valid loss:0.228752
Epoch:10, Train loss:0.217637, valid loss:0.228778
Epoch:11, Train loss:0.216933, valid loss:0.228544
Epoch:12, Train loss:0.216863, valid loss:0.228592
Epoch:13, Train loss:0.216887, valid loss:0.228466
Epoch:14, Train loss:0.216898, valid loss:0.228519
Epoch:15, Train loss:0.216823, valid loss:0.228504
Epoch:16, Train loss:0.216834, valid loss:0.228615
Epoch:17, Train loss:0.216844, valid loss:0.228447
Epoch:18, Train loss:0.216815, valid loss:0.228450
Epoch:19, Train loss:0.216713, valid loss:0.228507
Epoch:20, Train loss:0.216947, valid loss:0.228565
Epoch:21, Train loss:0.216466, valid loss:0.228294
Epoch:22, Train loss:0.216432, valid loss:0.228232
Epoch:23, Train loss:0.216452, valid loss:0.228226
Epoch:24, Train loss:0.216448, valid loss:0.228276
Epoch:25, Train loss:0.216424, valid loss:0.228265
Epoch:26, Train loss:0.216400, valid loss:0.228221
Epoch:27, Train loss:0.216402, valid loss:0.228191
Epoch:28, Train loss:0.216419, valid loss:0.228213
Epoch:29, Train loss:0.216379, valid loss:0.228313
Epoch:30, Train loss:0.216423, valid loss:0.228179
Epoch:31, Train loss:0.216217, valid loss:0.228404
Epoch:32, Train loss:0.216211, valid loss:0.228156
Epoch:33, Train loss:0.216214, valid loss:0.228199
Epoch:34, Train loss:0.216231, valid loss:0.228152
Epoch:35, Train loss:0.216214, valid loss:0.228333
Epoch:36, Train loss:0.216191, valid loss:0.228149
Epoch:37, Train loss:0.216197, valid loss:0.228220
Epoch:38, Train loss:0.216202, valid loss:0.228183
Epoch:39, Train loss:0.216205, valid loss:0.228109
Epoch:40, Train loss:0.216174, valid loss:0.228205
Epoch:41, Train loss:0.216115, valid loss:0.228141
Epoch:42, Train loss:0.216104, valid loss:0.228112
Epoch:43, Train loss:0.216106, valid loss:0.228104
Epoch:44, Train loss:0.216106, valid loss:0.228094
Epoch:45, Train loss:0.216099, valid loss:0.228128
Epoch:46, Train loss:0.216107, valid loss:0.228123
Epoch:47, Train loss:0.216095, valid loss:0.228105
Epoch:48, Train loss:0.216092, valid loss:0.228099
Epoch:49, Train loss:0.216091, valid loss:0.228081
Epoch:50, Train loss:0.216099, valid loss:0.228095
Epoch:51, Train loss:0.216052, valid loss:0.228104
Epoch:52, Train loss:0.216058, valid loss:0.228102
Epoch:53, Train loss:0.216053, valid loss:0.228090
Epoch:54, Train loss:0.216048, valid loss:0.228085
Epoch:55, Train loss:0.216053, valid loss:0.228098
Epoch:56, Train loss:0.216046, valid loss:0.228082
Epoch:57, Train loss:0.216047, valid loss:0.228104
Epoch:58, Train loss:0.216049, valid loss:0.228083
Epoch:59, Train loss:0.216046, valid loss:0.228093
Epoch:60, Train loss:0.216043, valid loss:0.228087
training time 12208.598084688187
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.6549871676174259
plot_id,batch_id 0 1 miss% 0.7431083367112378
plot_id,batch_id 0 2 miss% 0.7604350030951437
plot_id,batch_id 0 3 miss% 0.7678863357025452
plot_id,batch_id 0 4 miss% 0.7704637247214821
plot_id,batch_id 0 5 miss% 0.6675240473990658
plot_id,batch_id 0 6 miss% 0.7414859802880508
plot_id,batch_id 0 7 miss% 0.7563752365118529
plot_id,batch_id 0 8 miss% 0.7672278612015595
plot_id,batch_id 0 9 miss% 0.77039641202953
plot_id,batch_id 0 10 miss% 0.6295661242611101
plot_id,batch_id 0 11 miss% 0.7434821584798351
plot_id,batch_id 0 12 miss% 0.752612279355329
plot_id,batch_id 0 13 miss% 0.7622194564133556
plot_id,batch_id 0 14 miss% 0.77069178095888
plot_id,batch_id 0 15 miss% 0.6467150695477506
plot_id,batch_id 0 16 miss% 0.7362967096561734
plot_id,batch_id 0 17 miss% 0.7596256114703286
plot_id,batch_id 0 18 miss% 0.7642429359474853
plot_id,batch_id 0 19 miss% 0.7644612338106325
plot_id,batch_id 0 20 miss% 0.7085290033060104
plot_id,batch_id 0 21 miss% 0.7610928605477133
plot_id,batch_id 0 22 miss% 0.7673711527936431
plot_id,batch_id 0 23 miss% 0.7740606105747578
plot_id,batch_id 0 24 miss% 0.7763888330361657
plot_id,batch_id 0 25 miss% 0.6988293079175354
plot_id,batch_id 0 26 miss% 0.7555254677139425
plot_id,batch_id 0 27 miss% 0.7677211933829438
plot_id,batch_id 0 28 miss% 0.7684650059263616
plot_id,batch_id 0 29 miss% 0.7753860154152759
plot_id,batch_id 0 30 miss% 0.7021867434758327
plot_id,batch_id 0 31 miss% 0.7543221887727279
plot_id,batch_id 0 32 miss% 0.7641471964113359
plot_id,batch_id 0 33 miss% 0.7683122504658314
plot_id,batch_id 0 34 miss% 0.7719745738946148
plot_id,batch_id 0 35 miss% 0.6927782219904999
plot_id,batch_id 0 36 miss% 0.7586351009622788
plot_id,batch_id 0 37 miss% 0.7614252451836927
plot_id,batch_id 0 38 miss% 0.7692064552759645
plot_id,batch_id 0 39 miss% 0.773148028428789
plot_id,batch_id 0 40 miss% 0.7345191412083977
plot_id,batch_id 0 41 miss% 0.7679690415053816
plot_id,batch_id 0 42 miss% 0.767297904304334
plot_id,batch_id 0 43 miss% 0.7754865156787798
plot_id,batch_id 0 44 miss% 0.7803075917592853
plot_id,batch_id 0 45 miss% 0.7298508226166293
plot_id,batch_id 0 46 miss% 0.7664953420410321
plot_id,batch_id 0 47 miss% 0.7713545638326935
plot_id,batch_id 0 48 miss% 0.7754256338952283
plot_id,batch_id 0 49 miss% 0.7803340477717922
plot_id,batch_id 0 50 miss% 0.7386912963303505
plot_id,batch_id 0 51 miss% 0.764877991795382
plot_id,batch_id 0 52 miss% 0.7712651012281347
plot_id,batch_id 0 53 miss% 0.7749891754029926
plot_id,batch_id 0 54 miss% 0.781520186959081
plot_id,batch_id 0 55 miss% 0.7393349679159847
plot_id,batch_id 0 56 miss% 0.7649635434255829
plot_id,batch_id 0 57 miss% 0.7701243546750386
plot_id,batch_id 0 58 miss% 0.7758982694305903
plot_id,batch_id 0 59 miss% 0.7803501539219049
plot_id,batch_id 0 60 miss% 0.563079169201119
plot_id,batch_id 0 61 miss% 0.7032993007638
plot_id,batch_id 0 62 miss% 0.7296217165085005
plot_id,batch_id 0 63 miss% 0.7514411211095907
plot_id,batch_id 0 64 miss% 0.7589580259675949
plot_id,batch_id 0 65 miss% 0.5601982609715939
plot_id,batch_id 0 66 miss% 0.6996553616391479
plot_id,batch_id 0 67 miss% 0.7245340361433249
plot_id,batch_id 0 68 miss% 0.7495225497532266
plot_id,batch_id 0 69 miss% 0.7494325080301167
plot_id,batch_id 0 70 miss% 0.5268894966678811
plot_id,batch_id 0 71 miss% 0.7010227775726026
plot_id,batch_id 0 72 miss% 0.7160162752937852
plot_id,batch_id 0 73 miss% 0.7416773363412816
plot_id,batch_id 0 74 miss% 0.7422840526492965
plot_id,batch_id 0 75 miss% 0.5144978087214066
plot_id,batch_id 0 76 miss% 0.6489104725419562
plot_id,batch_id 0 77 miss% 0.7020822530757195
plot_id,batch_id 0 78 miss% 0.7326025280348193
plot_id,batch_id 0 79 miss% 0.7396299577461105
plot_id,batch_id 0 80 miss% 0.5897177681815493
plot_id,batch_id 0 81 miss% 0.722419217020296
plot_id,batch_id 0 82 miss% 0.7435093797039454
plot_id,batch_id 0 83 miss% 0.7566983708447043
plot_id,batch_id 0 84 miss% 0.7611414569057516
plot_id,batch_id 0 85 miss% 0.5918656603691032
plot_id,batch_id 0 86 miss% 0.7155050994901703
plot_id,batch_id 0 87 miss% 0.7405164431708573
plot_id,batch_id 0 88 miss% 0.7573696077280144
plot_id,batch_id 0 89 miss% 0.7625077423224094
plot_id,batch_id 0 90 miss% 0.55222510719964
plot_id,batch_id 0 91 miss% 0.7115752881742141
plot_id,batch_id 0 92 miss% 0.7360264128806587
plot_id,batch_id 0 93 miss% 0.7428912241276229
plot_id,batch_id 0 94 miss% 0.7624838967023307
plot_id,batch_id 0 95 miss% 0.548309036762199
plot_id,batch_id 0 96 miss% 0.7007865172742496
plot_id,batch_id 0 97 miss% 0.7260845419307023
plot_id,batch_id 0 98 miss% 0.7415247102957275
plot_id,batch_id 0 99 miss% 0.7517620078259148
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.65498717 0.74310834 0.760435   0.76788634 0.77046372 0.66752405
 0.74148598 0.75637524 0.76722786 0.77039641 0.62956612 0.74348216
 0.75261228 0.76221946 0.77069178 0.64671507 0.73629671 0.75962561
 0.76424294 0.76446123 0.708529   0.76109286 0.76737115 0.77406061
 0.77638883 0.69882931 0.75552547 0.76772119 0.76846501 0.77538602
 0.70218674 0.75432219 0.7641472  0.76831225 0.77197457 0.69277822
 0.7586351  0.76142525 0.76920646 0.77314803 0.73451914 0.76796904
 0.7672979  0.77548652 0.78030759 0.72985082 0.76649534 0.77135456
 0.77542563 0.78033405 0.7386913  0.76487799 0.7712651  0.77498918
 0.78152019 0.73933497 0.76496354 0.77012435 0.77589827 0.78035015
 0.56307917 0.7032993  0.72962172 0.75144112 0.75895803 0.56019826
 0.69965536 0.72453404 0.74952255 0.74943251 0.5268895  0.70102278
 0.71601628 0.74167734 0.74228405 0.51449781 0.64891047 0.70208225
 0.73260253 0.73962996 0.58971777 0.72241922 0.74350938 0.75669837
 0.76114146 0.59186566 0.7155051  0.74051644 0.75736961 0.76250774
 0.55222511 0.71157529 0.73602641 0.74289122 0.7624839  0.54830904
 0.70078652 0.72608454 0.74152471 0.75176201]
for model  214 the mean error 0.7298063906399622
all id 214 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 31 out win 4 err 0.7298063906399622
Launcher: Job 215 completed in 12381 seconds.
Launcher: Task 221 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 4
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 64800
total number of trained parameters  134801
Epoch:0, Train loss:0.353995, valid loss:0.355328
Epoch:1, Train loss:0.230717, valid loss:0.231484
Epoch:2, Train loss:0.221329, valid loss:0.230252
Epoch:3, Train loss:0.219679, valid loss:0.229586
Epoch:4, Train loss:0.219059, valid loss:0.229317
Epoch:5, Train loss:0.218718, valid loss:0.228876
Epoch:6, Train loss:0.218453, valid loss:0.229310
Epoch:7, Train loss:0.218359, valid loss:0.229034
Epoch:8, Train loss:0.218281, valid loss:0.228728
Epoch:9, Train loss:0.218279, valid loss:0.229171
Epoch:10, Train loss:0.217987, valid loss:0.228820
Epoch:11, Train loss:0.217170, valid loss:0.228482
Epoch:12, Train loss:0.217179, valid loss:0.228539
Epoch:13, Train loss:0.217152, valid loss:0.228502
Epoch:14, Train loss:0.217131, valid loss:0.228559
Epoch:15, Train loss:0.217087, valid loss:0.228664
Epoch:16, Train loss:0.217052, valid loss:0.228581
Epoch:17, Train loss:0.217018, valid loss:0.228749
Epoch:18, Train loss:0.216987, valid loss:0.228914
Epoch:19, Train loss:0.217058, valid loss:0.228363
Epoch:20, Train loss:0.216957, valid loss:0.228501
Epoch:21, Train loss:0.216548, valid loss:0.228268
Epoch:22, Train loss:0.216540, valid loss:0.228299
Epoch:23, Train loss:0.216555, valid loss:0.229462
Epoch:24, Train loss:0.216540, valid loss:0.229424
Epoch:25, Train loss:0.216591, valid loss:0.228252
Epoch:26, Train loss:0.216535, valid loss:0.228289
Epoch:27, Train loss:0.216519, valid loss:0.228303
Epoch:28, Train loss:0.216530, valid loss:0.228524
Epoch:29, Train loss:0.216491, valid loss:0.228283
Epoch:30, Train loss:0.216509, valid loss:0.228378
Epoch:31, Train loss:0.216278, valid loss:0.228180
Epoch:32, Train loss:0.216270, valid loss:0.228394
Epoch:33, Train loss:0.216254, valid loss:0.228457
Epoch:34, Train loss:0.216263, valid loss:0.228276
Epoch:35, Train loss:0.216247, valid loss:0.228215
Epoch:36, Train loss:0.216249, valid loss:0.228169
Epoch:37, Train loss:0.216254, valid loss:0.228235
Epoch:38, Train loss:0.216275, valid loss:0.228121
Epoch:39, Train loss:0.216253, valid loss:0.228142
Epoch:40, Train loss:0.216225, valid loss:0.228196
Epoch:41, Train loss:0.216130, valid loss:0.228225
Epoch:42, Train loss:0.216134, valid loss:0.228139
Epoch:43, Train loss:0.216123, valid loss:0.228193
Epoch:44, Train loss:0.216120, valid loss:0.228113
Epoch:45, Train loss:0.216121, valid loss:0.228120
Epoch:46, Train loss:0.216118, valid loss:0.228234
Epoch:47, Train loss:0.216109, valid loss:0.228116
Epoch:48, Train loss:0.216108, valid loss:0.228109
Epoch:49, Train loss:0.216122, valid loss:0.228112
Epoch:50, Train loss:0.216100, valid loss:0.228121
Epoch:51, Train loss:0.216058, valid loss:0.228094
Epoch:52, Train loss:0.216062, valid loss:0.228154
Epoch:53, Train loss:0.216056, valid loss:0.228100
Epoch:54, Train loss:0.216058, valid loss:0.228089
Epoch:55, Train loss:0.216052, valid loss:0.228105
Epoch:56, Train loss:0.216054, valid loss:0.228138
Epoch:57, Train loss:0.216052, valid loss:0.228099
Epoch:58, Train loss:0.216049, valid loss:0.228088
Epoch:59, Train loss:0.216057, valid loss:0.228104
Epoch:60, Train loss:0.216046, valid loss:0.228122
training time 12222.226041793823
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.6548047419872113
plot_id,batch_id 0 1 miss% 0.7462442507226028
plot_id,batch_id 0 2 miss% 0.7606669690161321
plot_id,batch_id 0 3 miss% 0.7692597225083172
plot_id,batch_id 0 4 miss% 0.7686325252189236
plot_id,batch_id 0 5 miss% 0.6657700141898103
plot_id,batch_id 0 6 miss% 0.745465839058566
plot_id,batch_id 0 7 miss% 0.7572187654130172
plot_id,batch_id 0 8 miss% 0.7660751862129089
plot_id,batch_id 0 9 miss% 0.7700184747306809
plot_id,batch_id 0 10 miss% 0.6328139931317472
plot_id,batch_id 0 11 miss% 0.7436369069029399
plot_id,batch_id 0 12 miss% 0.7522660452764129
plot_id,batch_id 0 13 miss% 0.7640732129695214
plot_id,batch_id 0 14 miss% 0.7715046847486042
plot_id,batch_id 0 15 miss% 0.6497827649324274
plot_id,batch_id 0 16 miss% 0.733918672434801
plot_id,batch_id 0 17 miss% 0.7579840495211939
plot_id,batch_id 0 18 miss% 0.7635898906986389
plot_id,batch_id 0 19 miss% 0.766162077445294
plot_id,batch_id 0 20 miss% 0.7151758104085479
plot_id,batch_id 0 21 miss% 0.7609587966319507
plot_id,batch_id 0 22 miss% 0.769356888237537
plot_id,batch_id 0 23 miss% 0.7750167579814421
plot_id,batch_id 0 24 miss% 0.777298522169866
plot_id,batch_id 0 25 miss% 0.6946326080454773
plot_id,batch_id 0 26 miss% 0.75793946131497
plot_id,batch_id 0 27 miss% 0.7682892579096644
plot_id,batch_id 0 28 miss% 0.769785859690495
plot_id,batch_id 0 29 miss% 0.7760865349771618
plot_id,batch_id 0 30 miss% 0.6973612483501123
plot_id,batch_id 0 31 miss% 0.7532170257361792
plot_id,batch_id 0 32 miss% 0.7638125217419846
plot_id,batch_id 0 33 miss% 0.7691689894678874
plot_id,batch_id 0 34 miss% 0.7707201709666607
plot_id,batch_id 0 35 miss% 0.6977970165995873
plot_id,batch_id 0 36 miss% 0.7578701203424495
plot_id,batch_id 0 37 miss% 0.764252018603963
plot_id,batch_id 0 38 miss% 0.7698397385301694
plot_id,batch_id 0 39 miss% 0.7710713269059702
plot_id,batch_id 0 40 miss% 0.7349321987537187
plot_id,batch_id 0 41 miss% 0.7672429991443576
plot_id,batch_id 0 42 miss% 0.7688744565822152
plot_id,batch_id 0 43 miss% 0.7788787877838037
plot_id,batch_id 0 44 miss% 0.7825465090171415
plot_id,batch_id 0 45 miss% 0.7328201868552761
plot_id,batch_id 0 46 miss% 0.7684400962030922
plot_id,batch_id 0 47 miss% 0.7705487724389879
plot_id,batch_id 0 48 miss% 0.7744212377960225
plot_id,batch_id 0 49 miss% 0.7804611477080473
plot_id,batch_id 0 50 miss% 0.7407776235391345
plot_id,batch_id 0 51 miss% 0.7638402601971208
plot_id,batch_id 0 52 miss% 0.7705046663684105
plot_id,batch_id 0 53 miss% 0.7750582851499306
plot_id,batch_id 0 54 miss% 0.781325122245807
plot_id,batch_id 0 55 miss% 0.7366705420241714
plot_id,batch_id 0 56 miss% 0.7661004220381522
plot_id,batch_id 0 57 miss% 0.7695536878207636
plot_id,batch_id 0 58 miss% 0.7752450409058391
plot_id,batch_id 0 59 miss% 0.7797990452352617
plot_id,batch_id 0 60 miss% 0.5628944995215742
plot_id,batch_id 0 61 miss% 0.7027661129589954
plot_id,batch_id 0 62 miss% 0.7314715570053149
plot_id,batch_id 0 63 miss% 0.7549289282990793
plot_id,batch_id 0 64 miss% 0.7578440733260657
plot_id,batch_id 0 65 miss% 0.5610181775063898
plot_id,batch_id 0 66 miss% 0.6949234731531067
plot_id,batch_id 0 67 miss% 0.7220723561322949
plot_id,batch_id 0 68 miss% 0.7508307989194208
plot_id,batch_id 0 69 miss% 0.7497403184778797
plot_id,batch_id 0 70 miss% 0.5327728189750027
plot_id,batch_id 0 71 miss% 0.6956008518679089
plot_id,batch_id 0 72 miss% 0.7182372994664769
plot_id,batch_id 0 73 miss% 0.7365865485124075
plot_id,batch_id 0 74 miss% 0.7468114272104759
plot_id,batch_id 0 75 miss% 0.52040048817185
plot_id,batch_id 0 76 miss% 0.6539360365175828
plot_id,batch_id 0 77 miss% 0.6998102614543096
plot_id,batch_id 0 78 miss% 0.7308058600238476
plot_id,batch_id 0 79 miss% 0.7362757652405891
plot_id,batch_id 0 80 miss% 0.5928134680278384
plot_id,batch_id 0 81 miss% 0.7219644065155766
plot_id,batch_id 0 82 miss% 0.7449573377921554
plot_id,batch_id 0 83 miss% 0.7583797818530179
plot_id,batch_id 0 84 miss% 0.7625462858676932
plot_id,batch_id 0 85 miss% 0.5908902664915879
plot_id,batch_id 0 86 miss% 0.7138519479004334
plot_id,batch_id 0 87 miss% 0.7395344432187209
plot_id,batch_id 0 88 miss% 0.7577615142115377
plot_id,batch_id 0 89 miss% 0.7615837540258625
plot_id,batch_id 0 90 miss% 0.5568560927435529
plot_id,batch_id 0 91 miss% 0.7137851158375895
plot_id,batch_id 0 92 miss% 0.733521357342679
plot_id,batch_id 0 93 miss% 0.7453229482880964
plot_id,batch_id 0 94 miss% 0.7615255701805397
plot_id,batch_id 0 95 miss% 0.5508289357099997
plot_id,batch_id 0 96 miss% 0.6937771327162544
plot_id,batch_id 0 97 miss% 0.7306016154195502
plot_id,batch_id 0 98 miss% 0.7421566092195228
plot_id,batch_id 0 99 miss% 0.7558624691190239
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.65480474 0.74624425 0.76066697 0.76925972 0.76863253 0.66577001
 0.74546584 0.75721877 0.76607519 0.77001847 0.63281399 0.74363691
 0.75226605 0.76407321 0.77150468 0.64978276 0.73391867 0.75798405
 0.76358989 0.76616208 0.71517581 0.7609588  0.76935689 0.77501676
 0.77729852 0.69463261 0.75793946 0.76828926 0.76978586 0.77608653
 0.69736125 0.75321703 0.76381252 0.76916899 0.77072017 0.69779702
 0.75787012 0.76425202 0.76983974 0.77107133 0.7349322  0.767243
 0.76887446 0.77887879 0.78254651 0.73282019 0.7684401  0.77054877
 0.77442124 0.78046115 0.74077762 0.76384026 0.77050467 0.77505829
 0.78132512 0.73667054 0.76610042 0.76955369 0.77524504 0.77979905
 0.5628945  0.70276611 0.73147156 0.75492893 0.75784407 0.56101818
 0.69492347 0.72207236 0.7508308  0.74974032 0.53277282 0.69560085
 0.7182373  0.73658655 0.74681143 0.52040049 0.65393604 0.69981026
 0.73080586 0.73627577 0.59281347 0.72196441 0.74495734 0.75837978
 0.76254629 0.59089027 0.71385195 0.73953444 0.75776151 0.76158375
 0.55685609 0.71378512 0.73352136 0.74532295 0.76152557 0.55082894
 0.69377713 0.73060162 0.74215661 0.75586247]
for model  241 the mean error 0.7302582925476088
all id 241 hidden_dim 32 learning_rate 0.02 num_layers 5 frames 31 out win 4 err 0.7302582925476088
Launcher: Job 242 completed in 12395 seconds.
Launcher: Task 190 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.02
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  134801
Epoch:0, Train loss:0.342721, valid loss:0.341336
Epoch:1, Train loss:0.230755, valid loss:0.231523
Epoch:2, Train loss:0.222746, valid loss:0.230870
Epoch:3, Train loss:0.221474, valid loss:0.230637
Epoch:4, Train loss:0.220770, valid loss:0.230516
Epoch:5, Train loss:0.220328, valid loss:0.230397
Epoch:6, Train loss:0.220151, valid loss:0.229655
Epoch:7, Train loss:0.219883, valid loss:0.229583
Epoch:8, Train loss:0.219824, valid loss:0.230112
Epoch:9, Train loss:0.219524, valid loss:0.229885
Epoch:10, Train loss:0.219409, valid loss:0.229556
Epoch:11, Train loss:0.218468, valid loss:0.229582
Epoch:12, Train loss:0.218515, valid loss:0.229350
Epoch:13, Train loss:0.218444, valid loss:0.229185
Epoch:14, Train loss:0.218405, valid loss:0.229175
Epoch:15, Train loss:0.218356, valid loss:0.229104
Epoch:16, Train loss:0.218384, valid loss:0.229339
Epoch:17, Train loss:0.218330, valid loss:0.229101
Epoch:18, Train loss:0.218284, valid loss:0.229247
Epoch:19, Train loss:0.218244, valid loss:0.229180
Epoch:20, Train loss:0.218554, valid loss:0.229189
Epoch:21, Train loss:0.217852, valid loss:0.228999
Epoch:22, Train loss:0.217822, valid loss:0.228997
Epoch:23, Train loss:0.217790, valid loss:0.229065
Epoch:24, Train loss:0.217779, valid loss:0.228918
Epoch:25, Train loss:0.217770, valid loss:0.228932
Epoch:26, Train loss:0.217749, valid loss:0.228891
Epoch:27, Train loss:0.217766, valid loss:0.229021
Epoch:28, Train loss:0.217735, valid loss:0.229016
Epoch:29, Train loss:0.217702, valid loss:0.228940
Epoch:30, Train loss:0.217720, valid loss:0.228873
Epoch:31, Train loss:0.217493, valid loss:0.228828
Epoch:32, Train loss:0.217457, valid loss:0.228791
Epoch:33, Train loss:0.217481, valid loss:0.228784
Epoch:34, Train loss:0.217461, valid loss:0.228866
Epoch:35, Train loss:0.217468, valid loss:0.228879
Epoch:36, Train loss:0.217470, valid loss:0.228910
Epoch:37, Train loss:0.217431, valid loss:0.228782
Epoch:38, Train loss:0.217443, valid loss:0.228814
Epoch:39, Train loss:0.217449, valid loss:0.228857
Epoch:40, Train loss:0.217457, valid loss:0.228771
Epoch:41, Train loss:0.217308, valid loss:0.228758
Epoch:42, Train loss:0.217308, valid loss:0.228920
Epoch:43, Train loss:0.217331, valid loss:0.228759
Epoch:44, Train loss:0.217306, valid loss:0.228760
Epoch:45, Train loss:0.217299, valid loss:0.228747
Epoch:46, Train loss:0.217302, valid loss:0.228794
Epoch:47, Train loss:0.217309, valid loss:0.228828
Epoch:48, Train loss:0.217302, valid loss:0.228814
Epoch:49, Train loss:0.217290, valid loss:0.228792
Epoch:50, Train loss:0.217291, valid loss:0.228766
Epoch:51, Train loss:0.217230, valid loss:0.228760
Epoch:52, Train loss:0.217230, valid loss:0.228762
Epoch:53, Train loss:0.217225, valid loss:0.228751
Epoch:54, Train loss:0.217225, valid loss:0.228738
Epoch:55, Train loss:0.217222, valid loss:0.228755
Epoch:56, Train loss:0.217221, valid loss:0.228754
Epoch:57, Train loss:0.217219, valid loss:0.228761
Epoch:58, Train loss:0.217213, valid loss:0.228746
Epoch:59, Train loss:0.217218, valid loss:0.228755
Epoch:60, Train loss:0.217213, valid loss:0.228740
training time 12248.421467542648
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.7302353854075626
plot_id,batch_id 0 1 miss% 0.7889239140085664
plot_id,batch_id 0 2 miss% 0.8007941700039867
plot_id,batch_id 0 3 miss% 0.8089811365468618
plot_id,batch_id 0 4 miss% 0.8060879101804296
plot_id,batch_id 0 5 miss% 0.7285678536159013
plot_id,batch_id 0 6 miss% 0.7830308492745818
plot_id,batch_id 0 7 miss% 0.7974642358167425
plot_id,batch_id 0 8 miss% 0.804177199422425
plot_id,batch_id 0 9 miss% 0.8097394934021105
plot_id,batch_id 0 10 miss% 0.6943327572429492
plot_id,batch_id 0 11 miss% 0.7818044796715979
plot_id,batch_id 0 12 miss% 0.7955239647798986
plot_id,batch_id 0 13 miss% 0.8028665638392287
plot_id,batch_id 0 14 miss% 0.8086896046436601
plot_id,batch_id 0 15 miss% 0.7184253063674471
plot_id,batch_id 0 16 miss% 0.7761082574943634
plot_id,batch_id 0 17 miss% 0.7958352439945314
plot_id,batch_id 0 18 miss% 0.8047039050675537
plot_id,batch_id 0 19 miss% 0.8044879405326372
plot_id,batch_id 0 20 miss% 0.764157350108427
plot_id,batch_id 0 21 miss% 0.8047554650789605
plot_id,batch_id 0 22 miss% 0.80745642134559
plot_id,batch_id 0 23 miss% 0.8123704038943084
plot_id,batch_id 0 24 miss% 0.8132439706668286
plot_id,batch_id 0 25 miss% 0.7521273711723391
plot_id,batch_id 0 26 miss% 0.7979621412387838
plot_id,batch_id 0 27 miss% 0.8017324267719094
plot_id,batch_id 0 28 miss% 0.8066459388430137
plot_id,batch_id 0 29 miss% 0.8087644584152168
plot_id,batch_id 0 30 miss% 0.7538309720052919
plot_id,batch_id 0 31 miss% 0.7930117054660689
plot_id,batch_id 0 32 miss% 0.8037872352086276
plot_id,batch_id 0 33 miss% 0.8064844131953864
plot_id,batch_id 0 34 miss% 0.8076439812169228
plot_id,batch_id 0 35 miss% 0.7502406630936738
plot_id,batch_id 0 36 miss% 0.7991468647755247
plot_id,batch_id 0 37 miss% 0.8017397600293564
plot_id,batch_id 0 38 miss% 0.8075674527271545
plot_id,batch_id 0 39 miss% 0.8105321205402917
plot_id,batch_id 0 40 miss% 0.7822194637281527
plot_id,batch_id 0 41 miss% 0.8058501678535293
plot_id,batch_id 0 42 miss% 0.8113575761270843
plot_id,batch_id 0 43 miss% 0.812963855048536
plot_id,batch_id 0 44 miss% 0.8159431948622133
plot_id,batch_id 0 45 miss% 0.7753817641794455
plot_id,batch_id 0 46 miss% 0.8063749997952925
plot_id,batch_id 0 47 miss% 0.8115580839988242
plot_id,batch_id 0 48 miss% 0.8111380191380606
plot_id,batch_id 0 49 miss% 0.8150482652932454
plot_id,batch_id 0 50 miss% 0.7866826326009694
plot_id,batch_id 0 51 miss% 0.80049268868783
plot_id,batch_id 0 52 miss% 0.8071766497437652
plot_id,batch_id 0 53 miss% 0.8115050832136234
plot_id,batch_id 0 54 miss% 0.8174073089526142
plot_id,batch_id 0 55 miss% 0.7683650759816366
plot_id,batch_id 0 56 miss% 0.8030895343581904
plot_id,batch_id 0 57 miss% 0.8092600581511386
plot_id,batch_id 0 58 miss% 0.8125376460468969
plot_id,batch_id 0 59 miss% 0.8163204645587689
plot_id,batch_id 0 60 miss% 0.6494091156774742
plot_id,batch_id 0 61 miss% 0.7504169333230111
plot_id,batch_id 0 62 miss% 0.783044345300541
plot_id,batch_id 0 63 miss% 0.7927187766025603
plot_id,batch_id 0 64 miss% 0.8015336129126885
plot_id,batch_id 0 65 miss% 0.6364271506935051
plot_id,batch_id 0 66 miss% 0.7527979364175226
plot_id,batch_id 0 67 miss% 0.7746426073189095
plot_id,batch_id 0 68 miss% 0.7948652621447967
plot_id,batch_id 0 69 miss% 0.79246751406914
plot_id,batch_id 0 70 miss% 0.6128359754621676
plot_id,batch_id 0 71 miss% 0.7647701020565846
plot_id,batch_id 0 72 miss% 0.7594259281697857
plot_id,batch_id 0 73 miss% 0.7793076366271317
plot_id,batch_id 0 74 miss% 0.7892545326169955
plot_id,batch_id 0 75 miss% 0.6073953749760109
plot_id,batch_id 0 76 miss% 0.7079664672712311
plot_id,batch_id 0 77 miss% 0.7480265074300151
plot_id,batch_id 0 78 miss% 0.7764292967250674
plot_id,batch_id 0 79 miss% 0.7825578734477004
plot_id,batch_id 0 80 miss% 0.6775175244963778
plot_id,batch_id 0 81 miss% 0.778956483123214
plot_id,batch_id 0 82 miss% 0.790070352888398
plot_id,batch_id 0 83 miss% 0.7997857297151285
plot_id,batch_id 0 84 miss% 0.8003667361218825
plot_id,batch_id 0 85 miss% 0.6687649302239237
plot_id,batch_id 0 86 miss% 0.7698261790767988
plot_id,batch_id 0 87 miss% 0.7876357706646392
plot_id,batch_id 0 88 miss% 0.7976580786123835
plot_id,batch_id 0 89 miss% 0.798880777463168
plot_id,batch_id 0 90 miss% 0.6356897109998947
plot_id,batch_id 0 91 miss% 0.764196990656101
plot_id,batch_id 0 92 miss% 0.775245950095778
plot_id,batch_id 0 93 miss% 0.7872079872901441
plot_id,batch_id 0 94 miss% 0.7985212384672882
plot_id,batch_id 0 95 miss% 0.638261948855543
plot_id,batch_id 0 96 miss% 0.7495525043265626
plot_id,batch_id 0 97 miss% 0.7747199905938237
plot_id,batch_id 0 98 miss% 0.7851701518317754
plot_id,batch_id 0 99 miss% 0.7915598243740006
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73023539 0.78892391 0.80079417 0.80898114 0.80608791 0.72856785
 0.78303085 0.79746424 0.8041772  0.80973949 0.69433276 0.78180448
 0.79552396 0.80286656 0.8086896  0.71842531 0.77610826 0.79583524
 0.80470391 0.80448794 0.76415735 0.80475547 0.80745642 0.8123704
 0.81324397 0.75212737 0.79796214 0.80173243 0.80664594 0.80876446
 0.75383097 0.79301171 0.80378724 0.80648441 0.80764398 0.75024066
 0.79914686 0.80173976 0.80756745 0.81053212 0.78221946 0.80585017
 0.81135758 0.81296386 0.81594319 0.77538176 0.806375   0.81155808
 0.81113802 0.81504827 0.78668263 0.80049269 0.80717665 0.81150508
 0.81740731 0.76836508 0.80308953 0.80926006 0.81253765 0.81632046
 0.64940912 0.75041693 0.78304435 0.79271878 0.80153361 0.63642715
 0.75279794 0.77464261 0.79486526 0.79246751 0.61283598 0.7647701
 0.75942593 0.77930764 0.78925453 0.60739537 0.70796647 0.74802651
 0.7764293  0.78255787 0.67751752 0.77895648 0.79007035 0.79978573
 0.80036674 0.66876493 0.76982618 0.78763577 0.79765808 0.79888078
 0.63568971 0.76419699 0.77524595 0.78720799 0.79852124 0.63826195
 0.7495525  0.77471999 0.78517015 0.79155982]
for model  242 the mean error 0.7765053562854619
all id 242 hidden_dim 32 learning_rate 0.02 num_layers 5 frames 31 out win 5 err 0.7765053562854619
Launcher: Job 243 completed in 12420 seconds.
Launcher: Task 103 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.005
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  134801
Epoch:0, Train loss:0.342721, valid loss:0.341336
Epoch:1, Train loss:0.229019, valid loss:0.230772
Epoch:2, Train loss:0.221309, valid loss:0.230207
Epoch:3, Train loss:0.220068, valid loss:0.230035
Epoch:4, Train loss:0.219617, valid loss:0.230048
Epoch:5, Train loss:0.219327, valid loss:0.229774
Epoch:6, Train loss:0.219136, valid loss:0.229868
Epoch:7, Train loss:0.218995, valid loss:0.229373
Epoch:8, Train loss:0.218839, valid loss:0.229399
Epoch:9, Train loss:0.218834, valid loss:0.229480
Epoch:10, Train loss:0.218686, valid loss:0.229439
Epoch:11, Train loss:0.218129, valid loss:0.229078
Epoch:12, Train loss:0.218082, valid loss:0.229038
Epoch:13, Train loss:0.218053, valid loss:0.229175
Epoch:14, Train loss:0.218015, valid loss:0.229119
Epoch:15, Train loss:0.218009, valid loss:0.228988
Epoch:16, Train loss:0.217945, valid loss:0.229156
Epoch:17, Train loss:0.217945, valid loss:0.229281
Epoch:18, Train loss:0.217912, valid loss:0.229174
Epoch:19, Train loss:0.217893, valid loss:0.229069
Epoch:20, Train loss:0.217883, valid loss:0.228951
Epoch:21, Train loss:0.217588, valid loss:0.228859
Epoch:22, Train loss:0.217588, valid loss:0.228840
Epoch:23, Train loss:0.217556, valid loss:0.228909
Epoch:24, Train loss:0.217564, valid loss:0.228836
Epoch:25, Train loss:0.217557, valid loss:0.228976
Epoch:26, Train loss:0.217554, valid loss:0.228853
Epoch:27, Train loss:0.217526, valid loss:0.229008
Epoch:28, Train loss:0.217535, valid loss:0.228870
Epoch:29, Train loss:0.217520, valid loss:0.228950
Epoch:30, Train loss:0.217514, valid loss:0.228864
Epoch:31, Train loss:0.217354, valid loss:0.228739
Epoch:32, Train loss:0.217348, valid loss:0.228756
Epoch:33, Train loss:0.217350, valid loss:0.228731
Epoch:34, Train loss:0.217336, valid loss:0.228807
Epoch:35, Train loss:0.217348, valid loss:0.228835
Epoch:36, Train loss:0.217339, valid loss:0.228761
Epoch:37, Train loss:0.217345, valid loss:0.228743
Epoch:38, Train loss:0.217332, valid loss:0.228766
Epoch:39, Train loss:0.217327, valid loss:0.228773
Epoch:40, Train loss:0.217333, valid loss:0.228748
Epoch:41, Train loss:0.217252, valid loss:0.228721
Epoch:42, Train loss:0.217251, valid loss:0.228776
Epoch:43, Train loss:0.217251, valid loss:0.228745
Epoch:44, Train loss:0.217243, valid loss:0.228742
Epoch:45, Train loss:0.217245, valid loss:0.228727
Epoch:46, Train loss:0.217238, valid loss:0.228721
Epoch:47, Train loss:0.217236, valid loss:0.228726
Epoch:48, Train loss:0.217238, valid loss:0.228717
Epoch:49, Train loss:0.217241, valid loss:0.228772
Epoch:50, Train loss:0.217233, valid loss:0.228741
Epoch:51, Train loss:0.217201, valid loss:0.228730
Epoch:52, Train loss:0.217196, valid loss:0.228734
Epoch:53, Train loss:0.217200, valid loss:0.228730
Epoch:54, Train loss:0.217198, valid loss:0.228713
Epoch:55, Train loss:0.217194, valid loss:0.228724
Epoch:56, Train loss:0.217192, valid loss:0.228726
Epoch:57, Train loss:0.217194, valid loss:0.228728
Epoch:58, Train loss:0.217191, valid loss:0.228723
Epoch:59, Train loss:0.217191, valid loss:0.228725
Epoch:60, Train loss:0.217192, valid loss:0.228713
training time 13386.073230028152
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.7260026583364644
plot_id,batch_id 0 1 miss% 0.7879541623919377
plot_id,batch_id 0 2 miss% 0.8010529768390974
plot_id,batch_id 0 3 miss% 0.8081713631945274
plot_id,batch_id 0 4 miss% 0.8078838817824376
plot_id,batch_id 0 5 miss% 0.727617259708363
plot_id,batch_id 0 6 miss% 0.7836388267977066
plot_id,batch_id 0 7 miss% 0.7981346186087759
plot_id,batch_id 0 8 miss% 0.8038897139512302
plot_id,batch_id 0 9 miss% 0.8078055160266787
plot_id,batch_id 0 10 miss% 0.696481627130385
plot_id,batch_id 0 11 miss% 0.7821141008774422
plot_id,batch_id 0 12 miss% 0.7943958718227364
plot_id,batch_id 0 13 miss% 0.8010488094651963
plot_id,batch_id 0 14 miss% 0.8078952532685775
plot_id,batch_id 0 15 miss% 0.7114874623116971
plot_id,batch_id 0 16 miss% 0.7734347174696977
plot_id,batch_id 0 17 miss% 0.7955131474151724
plot_id,batch_id 0 18 miss% 0.8047658489360757
plot_id,batch_id 0 19 miss% 0.8050100753355866
plot_id,batch_id 0 20 miss% 0.7620221074785248
plot_id,batch_id 0 21 miss% 0.8034335172044633
plot_id,batch_id 0 22 miss% 0.8083553581950892
plot_id,batch_id 0 23 miss% 0.8125693760878405
plot_id,batch_id 0 24 miss% 0.8141876087645377
plot_id,batch_id 0 25 miss% 0.7552317763414472
plot_id,batch_id 0 26 miss% 0.7974143229073142
plot_id,batch_id 0 27 miss% 0.8049319241039984
plot_id,batch_id 0 28 miss% 0.8065446407373011
plot_id,batch_id 0 29 miss% 0.8089199020262023
plot_id,batch_id 0 30 miss% 0.755952555418142
plot_id,batch_id 0 31 miss% 0.7963157055524157
plot_id,batch_id 0 32 miss% 0.8033588001269492
plot_id,batch_id 0 33 miss% 0.8064698378531723
plot_id,batch_id 0 34 miss% 0.8070307047621814
plot_id,batch_id 0 35 miss% 0.7453302718482879
plot_id,batch_id 0 36 miss% 0.7981950469724189
plot_id,batch_id 0 37 miss% 0.8021472348043807
plot_id,batch_id 0 38 miss% 0.8094238620746893
plot_id,batch_id 0 39 miss% 0.8090357696085725
plot_id,batch_id 0 40 miss% 0.7824442969876825
plot_id,batch_id 0 41 miss% 0.806074443484019
plot_id,batch_id 0 42 miss% 0.8097151157807805
plot_id,batch_id 0 43 miss% 0.8121228505761414
plot_id,batch_id 0 44 miss% 0.8157998418066654
plot_id,batch_id 0 45 miss% 0.7757383524244528
plot_id,batch_id 0 46 miss% 0.8053487760839739
plot_id,batch_id 0 47 miss% 0.8106490057398119
plot_id,batch_id 0 48 miss% 0.8113088877080118
plot_id,batch_id 0 49 miss% 0.8148189498522576
plot_id,batch_id 0 50 miss% 0.78598182208159
plot_id,batch_id 0 51 miss% 0.8035021901265231
plot_id,batch_id 0 52 miss% 0.8071675196818612
plot_id,batch_id 0 53 miss% 0.8101320933872332
plot_id,batch_id 0 54 miss% 0.8163037370560232
plot_id,batch_id 0 55 miss% 0.76073952003608
plot_id,batch_id 0 56 miss% 0.8030655007556102
plot_id,batch_id 0 57 miss% 0.807961897317888
plot_id,batch_id 0 58 miss% 0.8119625318883422
plot_id,batch_id 0 59 miss% 0.8162866504052037
plot_id,batch_id 0 60 miss% 0.6491504199678532
plot_id,batch_id 0 61 miss% 0.7494224961698877
plot_id,batch_id 0 62 miss% 0.7827661937217621
plot_id,batch_id 0 63 miss% 0.7935502202581755
plot_id,batch_id 0 64 miss% 0.8014378502253625
plot_id,batch_id 0 65 miss% 0.6365611179362336
plot_id,batch_id 0 66 miss% 0.7506662614131577
plot_id,batch_id 0 67 miss% 0.7692217048831456
plot_id,batch_id 0 68 miss% 0.7927376339593402
plot_id,batch_id 0 69 miss% 0.797088385187783
plot_id,batch_id 0 70 miss% 0.6114771797586237
plot_id,batch_id 0 71 miss% 0.7669894808049266
plot_id,batch_id 0 72 miss% 0.7594926423581913
plot_id,batch_id 0 73 miss% 0.7774632532050701
plot_id,batch_id 0 74 miss% 0.7909012727052326
plot_id,batch_id 0 75 miss% 0.6026890364285219
plot_id,batch_id 0 76 miss% 0.7127365898921219
plot_id,batch_id 0 77 miss% 0.7509017857079545
plot_id,batch_id 0 78 miss% 0.7764256268353105
plot_id,batch_id 0 79 miss% 0.7843402187559648
plot_id,batch_id 0 80 miss% 0.6691546473852262
plot_id,batch_id 0 81 miss% 0.7766165504504561
plot_id,batch_id 0 82 miss% 0.792036576873061
plot_id,batch_id 0 83 miss% 0.7998729290418867
plot_id,batch_id 0 84 miss% 0.8007121818797953
plot_id,batch_id 0 85 miss% 0.667542673347671
plot_id,batch_id 0 86 miss% 0.7702001871482995
plot_id,batch_id 0 87 miss% 0.7912618693210594
plot_id,batch_id 0 88 miss% 0.7963145144770726
plot_id,batch_id 0 89 miss% 0.7974925212988243
plot_id,batch_id 0 90 miss% 0.6332206825970849
plot_id,batch_id 0 91 miss% 0.7634582928953648
plot_id,batch_id 0 92 miss% 0.7751205000356151
plot_id,batch_id 0 93 miss% 0.7872451292986078
plot_id,batch_id 0 94 miss% 0.7974048425524438
plot_id,batch_id 0 95 miss% 0.6346394214563185
plot_id,batch_id 0 96 miss% 0.7487923285751352
plot_id,batch_id 0 97 miss% 0.7759824090498246
plot_id,batch_id 0 98 miss% 0.7848039328054973
plot_id,batch_id 0 99 miss% 0.7947793335694587
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.72600266 0.78795416 0.80105298 0.80817136 0.80788388 0.72761726
 0.78363883 0.79813462 0.80388971 0.80780552 0.69648163 0.7821141
 0.79439587 0.80104881 0.80789525 0.71148746 0.77343472 0.79551315
 0.80476585 0.80501008 0.76202211 0.80343352 0.80835536 0.81256938
 0.81418761 0.75523178 0.79741432 0.80493192 0.80654464 0.8089199
 0.75595256 0.79631571 0.8033588  0.80646984 0.8070307  0.74533027
 0.79819505 0.80214723 0.80942386 0.80903577 0.7824443  0.80607444
 0.80971512 0.81212285 0.81579984 0.77573835 0.80534878 0.81064901
 0.81130889 0.81481895 0.78598182 0.80350219 0.80716752 0.81013209
 0.81630374 0.76073952 0.8030655  0.8079619  0.81196253 0.81628665
 0.64915042 0.7494225  0.78276619 0.79355022 0.80143785 0.63656112
 0.75066626 0.7692217  0.79273763 0.79708839 0.61147718 0.76698948
 0.75949264 0.77746325 0.79090127 0.60268904 0.71273659 0.75090179
 0.77642563 0.78434022 0.66915465 0.77661655 0.79203658 0.79987293
 0.80071218 0.66754267 0.77020019 0.79126187 0.79631451 0.79749252
 0.63322068 0.76345829 0.7751205  0.78724513 0.79740484 0.63463942
 0.74879233 0.77598241 0.78480393 0.79477933]
for model  188 the mean error 0.776109630919512
all id 188 hidden_dim 32 learning_rate 0.005 num_layers 5 frames 31 out win 5 err 0.776109630919512
Launcher: Job 189 completed in 13571 seconds.
Launcher: Task 216 done. Exiting.
the mode is ini
device cpu
(input data) train, test 2400 100
dataset dir ../../twoD/*.h5  and size 2400
xmin 0.0 xmax 19.968 ymin 0.0 ymax 79.872
nx,ny 391 1561
the gap of two frames 20
(2400, 30, 8)
maximum abs change 0.10741687938570976
where the points are (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
weird values []
the mean of the difference 0.005836654551290445
number of weird sim 0
[]
renaissance 9
renaissance points (array([ 123,  960, 1080, 1100, 1343, 1463, 1782, 1821, 1860]), array([0, 0, 0, 0, 0, 0, 0, 0, 0]), array([5, 2, 5, 5, 4, 5, 5, 6, 4]))
min and max of training data 0.0 1.0
max diff from 1 5.587935447692871e-08
all the summation of grain fractions are 1 0.0008579958230257034
all the summation of grain fractions are 1 5.493383525845275e-13
mean and std of last y 57.80144506454468 3.8982882922680107
where they is small  (array([], dtype=int64),)
weird values  []
weird y traj []
nan (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))
throw away simulations []
actual num_train 2400
total frames 31 in_win 1 out_win 5
epoch 60 learning rate 0.01
1d grid size (number of grains) 8
param length 12
========== architecture ========
type -- LSTM
hidden dim 32 number of layers (5, 5)
convolution kernel size (3,)
========== architecture ========
truncate train len 0
train samples 62400
total number of trained parameters  134801
Epoch:0, Train loss:0.342721, valid loss:0.341336
Epoch:1, Train loss:0.229203, valid loss:0.231088
Epoch:2, Train loss:0.221439, valid loss:0.230317
Epoch:3, Train loss:0.220094, valid loss:0.229920
Epoch:4, Train loss:0.219701, valid loss:0.229998
Epoch:5, Train loss:0.219561, valid loss:0.229634
Epoch:6, Train loss:0.219271, valid loss:0.229588
Epoch:7, Train loss:0.219155, valid loss:0.229257
Epoch:8, Train loss:0.218955, valid loss:0.229675
Epoch:9, Train loss:0.219013, valid loss:0.229549
Epoch:10, Train loss:0.218833, valid loss:0.229792
Epoch:11, Train loss:0.218196, valid loss:0.229167
Epoch:12, Train loss:0.218153, valid loss:0.229132
Epoch:13, Train loss:0.218115, valid loss:0.229133
Epoch:14, Train loss:0.218104, valid loss:0.229248
Epoch:15, Train loss:0.218094, valid loss:0.229046
Epoch:16, Train loss:0.218064, valid loss:0.229100
Epoch:17, Train loss:0.218016, valid loss:0.229039
Epoch:18, Train loss:0.217991, valid loss:0.229073
Epoch:19, Train loss:0.217995, valid loss:0.229054
Epoch:20, Train loss:0.217941, valid loss:0.229102
Epoch:21, Train loss:0.217642, valid loss:0.228938
Epoch:22, Train loss:0.217615, valid loss:0.228911
Epoch:23, Train loss:0.217655, valid loss:0.228966
Epoch:24, Train loss:0.217842, valid loss:0.231606
Epoch:25, Train loss:0.217913, valid loss:0.229115
Epoch:26, Train loss:0.217597, valid loss:0.228860
Epoch:27, Train loss:0.217564, valid loss:0.228932
Epoch:28, Train loss:0.217591, valid loss:0.229103
Epoch:29, Train loss:0.217576, valid loss:0.228895
Epoch:30, Train loss:0.217573, valid loss:0.229110
Epoch:31, Train loss:0.217382, valid loss:0.228813
Epoch:32, Train loss:0.217392, valid loss:0.228792
Epoch:33, Train loss:0.217393, valid loss:0.228770
Epoch:34, Train loss:0.217386, valid loss:0.228756
Epoch:35, Train loss:0.217380, valid loss:0.228794
Epoch:36, Train loss:0.217394, valid loss:0.228810
Epoch:37, Train loss:0.217385, valid loss:0.228757
Epoch:38, Train loss:0.217358, valid loss:0.228741
Epoch:39, Train loss:0.217357, valid loss:0.228754
Epoch:40, Train loss:0.217390, valid loss:0.228751
Epoch:41, Train loss:0.217261, valid loss:0.228715
Epoch:42, Train loss:0.217263, valid loss:0.228736
Epoch:43, Train loss:0.217279, valid loss:0.228718
Epoch:44, Train loss:0.217259, valid loss:0.228735
Epoch:45, Train loss:0.217268, valid loss:0.228720
Epoch:46, Train loss:0.217260, valid loss:0.228713
Epoch:47, Train loss:0.217250, valid loss:0.228715
Epoch:48, Train loss:0.217246, valid loss:0.228702
Epoch:49, Train loss:0.217249, valid loss:0.228726
Epoch:50, Train loss:0.217254, valid loss:0.228873
Epoch:51, Train loss:0.217208, valid loss:0.228722
Epoch:52, Train loss:0.217211, valid loss:0.228715
Epoch:53, Train loss:0.217196, valid loss:0.228710
Epoch:54, Train loss:0.217200, valid loss:0.228699
Epoch:55, Train loss:0.217198, valid loss:0.228715
Epoch:56, Train loss:0.217198, valid loss:0.228709
Epoch:57, Train loss:0.217195, valid loss:0.228713
Epoch:58, Train loss:0.217192, valid loss:0.228715
Epoch:59, Train loss:0.217193, valid loss:0.228721
Epoch:60, Train loss:0.217196, valid loss:0.228710
training time 13679.586919307709
total number of trained parameters for initialize model 134801
sample [0.1713555  0.22762148 0.22250639 0.19693094 0.09974425 0.08184143
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.        ]
nondim time 0.03333333333333333
plot_id,batch_id 0 0 miss% 0.7323810621305313
plot_id,batch_id 0 1 miss% 0.7899920040931839
plot_id,batch_id 0 2 miss% 0.8006644884733468
plot_id,batch_id 0 3 miss% 0.8075444326052539
plot_id,batch_id 0 4 miss% 0.8070154133304865
plot_id,batch_id 0 5 miss% 0.726027725584566
plot_id,batch_id 0 6 miss% 0.7843217420758409
plot_id,batch_id 0 7 miss% 0.7991540191227546
plot_id,batch_id 0 8 miss% 0.8049968069194067
plot_id,batch_id 0 9 miss% 0.8097655183253706
plot_id,batch_id 0 10 miss% 0.6970954994417455
plot_id,batch_id 0 11 miss% 0.7833950701241386
plot_id,batch_id 0 12 miss% 0.7955845909999899
plot_id,batch_id 0 13 miss% 0.8024190071203536
plot_id,batch_id 0 14 miss% 0.8097799484344305
plot_id,batch_id 0 15 miss% 0.7153697343980233
plot_id,batch_id 0 16 miss% 0.7764555554092739
plot_id,batch_id 0 17 miss% 0.7947985763423396
plot_id,batch_id 0 18 miss% 0.8047458848123188
plot_id,batch_id 0 19 miss% 0.8044262516561207
plot_id,batch_id 0 20 miss% 0.7603247151703953
plot_id,batch_id 0 21 miss% 0.8045582859269845
plot_id,batch_id 0 22 miss% 0.8070937011672185
plot_id,batch_id 0 23 miss% 0.8119640693852741
plot_id,batch_id 0 24 miss% 0.813052024875825
plot_id,batch_id 0 25 miss% 0.7521965533327492
plot_id,batch_id 0 26 miss% 0.7976761429757772
plot_id,batch_id 0 27 miss% 0.8035836122395563
plot_id,batch_id 0 28 miss% 0.8082287560650517
plot_id,batch_id 0 29 miss% 0.8094558120775229
plot_id,batch_id 0 30 miss% 0.7536271014219463
plot_id,batch_id 0 31 miss% 0.7960824589091704
plot_id,batch_id 0 32 miss% 0.8028393684069001
plot_id,batch_id 0 33 miss% 0.8069537670921949
plot_id,batch_id 0 34 miss% 0.8079042980825473
plot_id,batch_id 0 35 miss% 0.7426529432565386
plot_id,batch_id 0 36 miss% 0.7990554363913658
plot_id,batch_id 0 37 miss% 0.8037310694072797
plot_id,batch_id 0 38 miss% 0.808717612054655
plot_id,batch_id 0 39 miss% 0.8102405571113719
plot_id,batch_id 0 40 miss% 0.7815276218676278
plot_id,batch_id 0 41 miss% 0.8052006459195994
plot_id,batch_id 0 42 miss% 0.8108635017645467
plot_id,batch_id 0 43 miss% 0.8120382469728912
plot_id,batch_id 0 44 miss% 0.8169062268995206
plot_id,batch_id 0 45 miss% 0.7749196985115442
plot_id,batch_id 0 46 miss% 0.8065239535139366
plot_id,batch_id 0 47 miss% 0.811656125478934
plot_id,batch_id 0 48 miss% 0.8116255321378385
plot_id,batch_id 0 49 miss% 0.8155222711595214
plot_id,batch_id 0 50 miss% 0.7853335779253499
plot_id,batch_id 0 51 miss% 0.8017310317750219
plot_id,batch_id 0 52 miss% 0.8075279724486172
plot_id,batch_id 0 53 miss% 0.8113266684941455
plot_id,batch_id 0 54 miss% 0.8166230903575427
plot_id,batch_id 0 55 miss% 0.7665002654432357
plot_id,batch_id 0 56 miss% 0.8036966703490719
plot_id,batch_id 0 57 miss% 0.8075093009036717
plot_id,batch_id 0 58 miss% 0.8124054118136732
plot_id,batch_id 0 59 miss% 0.8157287834996885
plot_id,batch_id 0 60 miss% 0.64600699516419
plot_id,batch_id 0 61 miss% 0.7477756801005274
plot_id,batch_id 0 62 miss% 0.7865365483868915
plot_id,batch_id 0 63 miss% 0.7938716048098896
plot_id,batch_id 0 64 miss% 0.8030731393020337
plot_id,batch_id 0 65 miss% 0.6417500101092936
plot_id,batch_id 0 66 miss% 0.7527859349278955
plot_id,batch_id 0 67 miss% 0.7694282288883613
plot_id,batch_id 0 68 miss% 0.7946196511120388
plot_id,batch_id 0 69 miss% 0.7932890163280393
plot_id,batch_id 0 70 miss% 0.6074801722468163
plot_id,batch_id 0 71 miss% 0.7639077443662726
plot_id,batch_id 0 72 miss% 0.7613427623882494
plot_id,batch_id 0 73 miss% 0.7805032498168258
plot_id,batch_id 0 74 miss% 0.7883645877624554
plot_id,batch_id 0 75 miss% 0.6106230210750528
plot_id,batch_id 0 76 miss% 0.7123795497307113
plot_id,batch_id 0 77 miss% 0.7542029786124365
plot_id,batch_id 0 78 miss% 0.7745872877411802
plot_id,batch_id 0 79 miss% 0.7818453093726029
plot_id,batch_id 0 80 miss% 0.6748334258499998
plot_id,batch_id 0 81 miss% 0.7791465001963926
plot_id,batch_id 0 82 miss% 0.7913163231998468
plot_id,batch_id 0 83 miss% 0.8005706906446496
plot_id,batch_id 0 84 miss% 0.8010038715425378
plot_id,batch_id 0 85 miss% 0.6693199683119541
plot_id,batch_id 0 86 miss% 0.7668117848327112
plot_id,batch_id 0 87 miss% 0.78849824229336
plot_id,batch_id 0 88 miss% 0.7977953131963007
plot_id,batch_id 0 89 miss% 0.7994802498619249
plot_id,batch_id 0 90 miss% 0.6355989558793457
plot_id,batch_id 0 91 miss% 0.7659131047645334
plot_id,batch_id 0 92 miss% 0.7744081586228087
plot_id,batch_id 0 93 miss% 0.787627615700672
plot_id,batch_id 0 94 miss% 0.8001330066522013
plot_id,batch_id 0 95 miss% 0.6356493878157605
plot_id,batch_id 0 96 miss% 0.7466321728389106
plot_id,batch_id 0 97 miss% 0.7751927571728383
plot_id,batch_id 0 98 miss% 0.7867624694110423
plot_id,batch_id 0 99 miss% 0.7939655118030419
[0.05 0.05 0.05 ... 0.3  0.3  0.3 ]
[0.5  0.55 0.6  ... 6.   7.   8.5 ]
[1.4 1.4 1.4 ... 1.6 1.6 1.6]
[0.73238106 0.789992   0.80066449 0.80754443 0.80701541 0.72602773
 0.78432174 0.79915402 0.80499681 0.80976552 0.6970955  0.78339507
 0.79558459 0.80241901 0.80977995 0.71536973 0.77645556 0.79479858
 0.80474588 0.80442625 0.76032472 0.80455829 0.8070937  0.81196407
 0.81305202 0.75219655 0.79767614 0.80358361 0.80822876 0.80945581
 0.7536271  0.79608246 0.80283937 0.80695377 0.8079043  0.74265294
 0.79905544 0.80373107 0.80871761 0.81024056 0.78152762 0.80520065
 0.8108635  0.81203825 0.81690623 0.7749197  0.80652395 0.81165613
 0.81162553 0.81552227 0.78533358 0.80173103 0.80752797 0.81132667
 0.81662309 0.76650027 0.80369667 0.8075093  0.81240541 0.81572878
 0.646007   0.74777568 0.78653655 0.7938716  0.80307314 0.64175001
 0.75278593 0.76942823 0.79461965 0.79328902 0.60748017 0.76390774
 0.76134276 0.78050325 0.78836459 0.61062302 0.71237955 0.75420298
 0.77458729 0.78184531 0.67483343 0.7791465  0.79131632 0.80057069
 0.80100387 0.66931997 0.76681178 0.78849824 0.79779531 0.79948025
 0.63559896 0.7659131  0.77440816 0.78762762 0.80013301 0.63564939
 0.74663217 0.77519276 0.78676247 0.79396551]
for model  215 the mean error 0.7765607119681838
all id 215 hidden_dim 32 learning_rate 0.01 num_layers 5 frames 31 out win 5 err 0.7765607119681838
Launcher: Job 216 completed in 13853 seconds.
Launcher: Task 255 done. Exiting.
Launcher: Done. Job exited without errors
